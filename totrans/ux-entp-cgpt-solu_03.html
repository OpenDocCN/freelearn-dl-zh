<html><head></head><body>
		<div><h1 id="_idParaDest-31" class="chapter-number"><a id="_idTextAnchor031" class="pcalibre pcalibre1 calibre6"/>2</h1>
			<h1 id="_idParaDest-32" class="calibre5"><a id="_idTextAnchor032" class="pcalibre pcalibre1 calibre6"/>Conducting Effective User Research</h1>
			<p class="calibre3">We all want to dive in and start working on things immediately—that’s human nature. There is more value in our efforts with a plan. And if that plan includes a solid understanding of users, any effort will be more than returned in value. This is where user research comes in. Let’s use expertise in this field to understand the user’s needs, listen to the problems, and then drive solutions. Explore a few essential methods to understand where to go with ChatGPT projects. There is a well-known adage in development circles that change gets exponentially more costly at every step of the development process. So, why not take the proper first steps and reduce expensive change later? This doesn’t mean you can’t adapt and improve.</p>
			<p class="calibre3"><em class="italic">The world of LLM is all about refinement and incremental improvement</em>. Let’s start on the right foot to reduce complexity and cost. We can do this by knowing more about how customers will use the product, what needs will be the most critical, and how deep the solution needs to go.</p>
			<p class="calibre3">We will not give a survey of all the possible user research methods out in the wild. But we always encourage deeper exploration. Let's cover a few main topics that give the best value for an investment in research:</p>
			<ul class="calibre7">
				<li class="calibre8">Surveying UX research methods</li>
				<li class="calibre8">Understanding user needs analysis</li>
				<li class="calibre8">Creating effective surveys</li>
				<li class="calibre8">Designing insightful interviews</li>
				<li class="calibre8">Trying conversational analysis</li>
			</ul>
			<h1 id="_idParaDest-33" class="calibre5"><a id="_idTextAnchor033" class="pcalibre pcalibre1 calibre6"/>Surveying UX research methods</h1>
			<p class="calibre3">This section will focus on some popular UX research methods.</p>
			<p class="calibre3">I want to respect all the possible <a id="_idIndexMarker051" class="pcalibre pcalibre1 calibre6"/>methods out there. We tend to pick techniques to help a specific problem, and one can’t apply all methods to all situations. But reminding yourself of options is good. There is more out there than covered in this chapter.</p>
			<p class="calibre3">Article: <a href="https://www.nngroup.com/articles/which-ux-research-methods/" class="pcalibre pcalibre1 calibre6">Overview of research methods</a> (<a href="https://www.nngroup.com/articles/which-ux-research-methods/" class="pcalibre pcalibre1 calibre6">https://www.nngroup.com/articles/which-ux-research-methods/</a>)</p>
			<p class="calibre3">The takeaway from that article is to notice where our review methods appear on Christian’s landscape map from the Nielsen Norman Group article, as in <em class="italic">Figure 2</em><em class="italic">.1</em>.</p>
			<p class="calibre3">I circled three methods we will cover and <a id="_idIndexMarker052" class="pcalibre pcalibre1 calibre6"/>highlighted <strong class="bold">usability testing</strong>. The ones we will cover in this chapter provide the most value for helping decide what to build and start building a collection of customers to engage with over time. Usability testing is also essential; it will come into play when we have something to test. Once we have a product, we will discuss methods and tools to help evaluate the quality of LLM solutions.</p>
			<div><div><img src="img/B21964_02_01.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="calibre3"> </p>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The broad landscape of user research methods</p>
			<p class="callout-heading">More on UX research methods</p>
			<p class="callout">Try some research methods books<a id="_idIndexMarker053" class="pcalibre pcalibre1 calibre6"/> for a deeper exploration that can help design generative AI solutions.</p>
			<p class="callout">Book: <a href="https://amzn.to/3zYtzN1" class="pcalibre pcalibre1 calibre6">A deeper dive into user research methods</a> (<a href="https://amzn.to/3zYtzN1" class="pcalibre pcalibre1 calibre6">https://amzn.to/3zYtzN1</a>)</p>
			<p class="calibre3">We spend the effort researching to understand the user’s needs. We don’t ask them what they want; we learn what they need and use that to drive design and feature decisions. This is a needs analysis, which we will discuss next.</p>
			<h1 id="_idParaDest-34" class="calibre5"><a id="_idTextAnchor034" class="pcalibre pcalibre1 calibre6"/>Understanding user needs analysis</h1>
			<p class="calibre3">Let’s review the basics of <strong class="bold">needs analysis</strong>. The needs<a id="_idIndexMarker054" class="pcalibre pcalibre1 calibre6"/> analysis helps us identify, evaluate, and prioritize the user’s requirements and needs. We want to understand the users’ outcomes and ensure we meet the challenges they will see. The product manager typically leads this product definition, but an interdisciplinary team with a designer and researcher helps improve the outcome. </p>
			<p class="calibre3">We want to accomplish something significant by introducing ChatGPT into our company’s resource bag. Let’s ensure we know how this will solve <em class="italic">real</em> problems for the right customers and at the right time during their user journey. Later in the book, we will go deep into ensuring that we service the right customers with a service they use with some frequency and <a id="_idIndexMarker055" class="pcalibre pcalibre1 calibre6"/>solve significant problems. For now, let’s get the context for what is needed and gather some knowledge on the gaps in our product that a ChatGPT solution might solve:</p>
			<ol class="calibre12">
				<li class="calibre8"><strong class="bold">Define the purpose</strong>: What goals do we want to achieve by introducing an LLM solution? A service example could be to reduce costs by introducing a chat assistant to solve simple problems or gather information before handing it off to human agents. But this is not a goal for the user. The user aims to solve a problem as quickly and accurately as possible. And not to make things worse by solving the wrong problem or making matters worse. That is their take from their boss’s perspective. A needs analysis for Sales might show that the time to close a deal is 30% longer than the industry average. A salesperson whose commissions are based on closing deals should want to help move their process along, so the goal of reducing close time helps them and the company.</li>
				<li class="calibre8"><strong class="bold">Identify stakeholders</strong>: Identify and involve stakeholders, including end-users, clients, decision-makers, and others interested in the product or service. This is important because the methods we discuss can be applied to customers and internal decision-makers.</li>
				<li class="calibre8"><strong class="bold">User profiling</strong>: Create user profiles or personas representing different target audience segments. Be careful not to create a caricature of a customer. Avoid definitions that distract from connecting the user and the system. For example, if the persona or profile includes that they get a coffee at a local shop on the way to work, let’s ensure this is relevant to the use case. Later, when we want to tune the messaging to the user’s individual needs based on experience level, previous use of the tools, or technical understanding, we should be able to use this knowledge in the profile to tune the dynamics of the conversational style and tone. If getting coffee is only there because you think it is cool, drop it.</li>
				<li class="calibre8"><strong class="bold">Conduct surveys and interviews</strong>: Gather information directly from users through surveys, interviews, or focus group discussions. Ask open-ended questions to understand their goals, challenges, preferences, and expectations. Ask specific questions to segment and quantify results easily. This is useful for customers and internal stakeholders.</li>
				<li class="calibre8"><strong class="bold">Observe user behavior</strong>: Conduct usability testing or observe users interacting with <em class="italic">existing</em> systems or prototypes. Analyze how they perform tasks, identify pain points, and follow any patterns in their behavior. We will see some of this in our log analysis discussion. A generative AI solution isn’t necessarily up and running, so use other channels to analyze user behavior and gather that data.</li>
				<li class="calibre8"><strong class="bold">Analyze feedback and support data</strong>: Review customer support channels, user reviews, blog posts, and any other sources of information that can provide insights into user<a id="_idIndexMarker056" class="pcalibre pcalibre1 calibre6"/> issues, concerns, or suggestions. This is a time-consuming and hard-to-quantify task. Analyzing existing log files is also time-consuming but easier to quantify. We will drill down into data sources like a chat log file.</li>
				<li class="calibre8"><strong class="bold">Define user stories</strong>: <strong class="bold">User stories</strong> are narratives that describe a user’s interaction with the system, focusing on the desired outcome and the context in which the user’s goals occur. Let’s briefly explore this and save the bulk of it for the next chapter. Using cases or case-driven design and development is prevalent in customer-centric companies. This is encouraged as a more systematic and tactical than the profiling approach discussed above in the <em class="italic">3rd item</em>. Some do cartoons or visually write up a story like a comic book. These are very popular and allow a broad audience to grasp the goals quickly. This example from Chris Spalton reminds us that we don’t always have to start with specifications. We can also use our creative and visual side to explain our story. <em class="italic">Figure 2</em><em class="italic">.2</em> shows Chris’s example of a UX <a id="_idIndexMarker057" class="pcalibre pcalibre1 calibre6"/>concept expressed as a storyboard.<p class="calibre3">Article: <a href="https://uxdesign.cc/using-comic-strips-and-storyboards-to-test-your-ux-concepts-cccad7ac7f71" class="pcalibre pcalibre1 calibre6">Using comic strips to show UX concepts</a> by Chris Spalton (<a href="https://uxdesign.cc/using-comic-strips-and-storyboards-to-test-your-ux-concepts-cccad7ac7f71" class="pcalibre pcalibre1 calibre6">https://uxdesign.cc/using-comic-strips-and-storyboards-to-test-your-ux-concepts-cccad7ac7f71</a>):</p></li>
			</ol>
			<div><div><img src="img/B21964_02_02.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Example of a UX story as a comic strip</p>
			<ol class="calibre12">
				<li value="8" class="calibre8"><strong class="bold">Prioritize user needs</strong>: Prioritize the identified needs based on their importance and impact on the user experience. This can "drive focus" during the design and development process. We will drill down into this heavily by providing a repeatable tool to address this significant issue.</li>
				<li class="calibre8"><strong class="bold">Create use cases</strong>: Develop use cases that describe the step-by-step interactions between users and the system. This helps visualize how users will accomplish specific tasks. Alistair Cockburn’s <em class="italic">Writing Effective Use Cases</em> is the definitive guide when teaching use case design (<a href="B21964_03.xhtml#_idTextAnchor058" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 3</em></a>, <em class="italic">Identifying Optimal Use Cases </em><em class="italic">for ChatGPT</em>).<p class="calibre3">Book: <a href="https://amzn.to/3YnbGSp" class="pcalibre pcalibre1 calibre6">Writing Effective Use Cases</a> (<a href="https://amzn.to/3YnbGSp" class="pcalibre pcalibre1 calibre6">https://amzn.to/3YnbGSp</a>)</p></li>
				<li class="calibre8"><strong class="bold">Document research</strong>: Compile and document results from the user needs analysis. This documentation serves as a reference for the design and development teams.</li>
				<li class="calibre8"><strong class="bold">Iterative process</strong>: User needs may evolve, so analysis should be considered an iterative process. Regularly<a id="_idIndexMarker058" class="pcalibre pcalibre1 calibre6"/> revisit and update user needs to align with changing requirements and user expectations.</li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">No matter how it is implemented, a ChatGPT solution will still require care and feeding throughout the foreseeable future, so plan on interactions at a regular cadence when in production. It could be weekly or faster. This is reviewed in <a href="B21964_11.xhtml#_idTextAnchor236" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter </em><em class="italic">11</em></a>, <em class="italic">Process</em>.</p>
			<p class="calibre3">By thoroughly understanding user needs, you can tailor products or services to meet the target audience’s expectations, improving user satisfaction and overall success. It is unlikely anyone will use or need all the steps listed. <em class="italic">This is why we can apply design thinking to our problems. Don’t reinvent the wheel for every project. Pick and choose tools to solve the most significant gaps in your </em><em class="italic">team’s understanding.</em></p>
			<p class="calibre3">Remember, this is the time to make changes. It costs exponentially more to improve quality once a product is built and shipped. The better early understanding of the problem, the better decision-making will be, and the less costly it will be to the organization. This is not to say we won’t continue improving solutions after shipping, but incremental changes at later stages cost more.</p>
			<p class="calibre3"><em class="italic">Figure 2</em><em class="italic">.3</em> shows how to consider the cost of change when moving from one development step to the next. We don’t get it all right in requirements, and with a ChatGPT solution, we expect to iterate to get it right, but it reminds us there is value in learning at every step because the cost of learning and repair grows quickly.</p>
			<div><div><img src="img/B21964_02_03.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"> Figure 2.3 – The cost of change rises at every stage</p>
			<p class="calibre3">I prefer to focus on a few essential methods and expose some additional tips and tricks to make them effective when researching a generative AI solution. Let’s dig into the first tool in our bag that <a id="_idIndexMarker059" class="pcalibre pcalibre1 calibre6"/>sometimes helps us refine our needs analysis: a survey.</p>
			<p class="calibre3">Creating effective surveys</p>
			<p class="calibre3">Of all the methods available, why choose a <strong class="bold">survey</strong>? There are challenges with building a survey, but when a <a id="_idIndexMarker060" class="pcalibre pcalibre1 calibre6"/>new project starts, there is sometimes little data to help form a design <em class="italic">space</em> around users’ needs. Instead of relying solely on customer feedback, visionary leaders and design innovators anticipate needs and create solutions that customers might not have imagined. Steve Jobs, co-founder of Apple Inc., was attributed by saying, “People don’t know what they want until you show it to them.” But for some of us, when we start, we need a clue where to go, what problems to solve, or help deciding which problems to solve have the most value. We do not suggest asking the customer what they want; we want to ask questions to help us understand what they need and where current products and services don’t meet their needs. We don’t ask them how to solve the problem. Let me give an example of how to ask the wrong question.</p>
			<p class="calibre3">Jim is cooking in the kitchen when Kelly comes in from a run. They both reach for the orange on the counter. They both want the orange. Jim suggests cutting it in half. Kelly says that won’t work. Kelly asks, “What do you want to do with the orange?” Jim replies, I am cooking and need the rind. Kelly says, “Great, you can have the entire rind, and I will take the insides for eating since I just got back from the gym.”</p>
			<p class="calibre3">This is more than just about negotiation and conflict resolution; it is essential to understand the use case. If the design team knows how someone wants to use a product, you can help design a better product. We don’t ask “what do you need” in a survey. We focus on understanding intent. Once we have some generative output to share, we might adjust the styles and tone to one that resonates better. It is common to run a survey after releasing software (or by embedding a survey within the software or service) and ask for feedback on their experience. In any case, ask quality, unbiased questions that you know <a id="_idIndexMarker061" class="pcalibre pcalibre1 calibre6"/>apriori (ahead of time) how the results should be analyzed.</p>
			<p class="calibre3">Writing surveys is a science and expertise all on its own. Even experts can benefit from the science in the field. We have resources for that.</p>
			<p class="calibre3">Book: <a href="https://amzn.to/3AemWWY" class="pcalibre pcalibre1 calibre6">Questionnaire Design</a> by Bolton and Bolton (5th Edition, 2022) (<a href="https://amzn.to/3AemWWY" class="pcalibre pcalibre1 calibre6">https://amzn.to/3AemWWY</a>).</p>
			<h2 id="_idParaDest-35" class="calibre9"><a id="_idTextAnchor035" class="pcalibre pcalibre1 calibre6"/>Surveys for conversational AI</h2>
			<p class="calibre3">Surveys are well suited for<a id="_idIndexMarker062" class="pcalibre pcalibre1 calibre6"/> broadening an understanding of customers in a new or novel space. We can only offer generic questions and sage advice. Questions specific to your space will have to come from you. But there are questions we should not ask. We cover some of this in the checklist. For example, don’t ask, “Do you want to use a conversational AI to answer questions?” or “Can I offer you recommendations based on either an algorithm or a generative AI analysis?”. These questions return us to a time when we asked our customers if they wanted a faster horse. The horse wasn’t the answer; a car, something they could not imagine, was the solution.</p>
			<p class="calibre3">Focus questions on customer problems, their frustration with solving them, or the tools and techniques they use to interact with the company. This is a method for gaining insights into how they experience the brand and go deeper into why and how. With recommendation UIs, you can explore style, tone, and content in a survey. Probe users concerning different recommendations and how they might react to them, which ones would be more valuable (by giving multiple examples and asking to pick the one that is the most useful), when the recommendation would be most helpful, or even how to share it (should we email the users when we know it, or only show it in the UI when they come to the deal page?). And this is an excellent opportunity to ask why. Have them fill in details on why they made their selections and take the time to organize and assess this feedback. Do this research before making recommendations because users might only accept future improvements if we hit the mark. Start on a sound footing. This research will shape how to form prompts, gather the data for analysis, and generate results. We can dig into issues to watch out for when<a id="_idIndexMarker063" class="pcalibre pcalibre1 calibre6"/> developing a survey.</p>
			<h2 id="_idParaDest-36" class="calibre9"><a id="_idTextAnchor036" class="pcalibre pcalibre1 calibre6"/>Survey checklist</h2>
			<p class="calibre3">We dive right into tricks of the trade, assuming some survey <a id="_idIndexMarker064" class="pcalibre pcalibre1 calibre6"/>expertise; grab our resources if needed. If you have the basics, proceed, as this is a collection of lessons we sometimes need to remember when building an effective survey. A poorly designed survey can be worse than no survey at all. It can give a sense of false hope and guide the team into wrong assumptions. The village of people it takes to build a solution will all be in an excellent position to contribute to the survey. Conversational writers, designers, researchers, product owners, and managers will all find value in the results of a well-done study. So, be sure to engage them to help create the questions. Here are some checklist items to ensure an effective survey.</p>
			<ul class="calibre7">
				<li class="calibre8"><strong class="bold">Only ask questions knowing how they will be analyzed</strong>: Plan to analyze every question. This forces a purpose behind every question, and when it comes time to trim, it is easier to judge its value.</li>
				<li class="calibre8"><strong class="bold">Don’t ask leading questions</strong>: Let the user form their opinion. Ask, “How satisfied are you with your support experience earlier today?” Don’t ask, “Considering how improved our excellent support process and staff is with handling concerns, how satisfied were you with your recent interaction?”</li>
				<li class="calibre8"><strong class="bold">The simpler the questions, the easier the user can answer:</strong><strong class="bold"> </strong>Multiple-choice (a, b, c, d) and simple-choice (a or b) questions are easier and faster to analyze. Use them and rework open-ended questions when possible.<p class="calibre3">Simple choice questions should be direct. Try to avoid yes/no questions.</p><p class="calibre3">We avoid yes/no questions in UIs because they require more cognitive effort. Here’s an example from banking:</p><pre class="source-code">
Do you wish to cancel your transfer? [ Yes ]  [ No ]</pre><p class="calibre3">Instead, we would use this:</p><pre class="source-code">Do you wish to cancel your transfer? 
[ Cancel Transfer ]   [Continue Transfer ]
Were you happy with your support call today?  [ Yes ]  [ No ]</pre><p class="calibre3">Instead, use this:</pre><pre class="source-code">Were you satisfied with your support call today? 
[ Satisfied ]  [ Not Satisfied ]</pre><p class="calibre3">This is less leading and <a id="_idIndexMarker065" class="pcalibre pcalibre1 calibre6"/>more direct. Decide what kind of results are needed from the survey.</p></li>				<li class="calibre8"><strong class="bold">Choice questions should be Venn diagram complete</strong>: This is my funny way of saying make sure question choices cover all the possible outcomes. This avoids someone attempting to answer a question and not seeing a response that matches their use case. Take a look at this:<pre class="source-code">
What is your current age?
    Younger than 20
    20 to 50
    Older than 70</pre><p class="calibre3">Oops! The question missed a group of users. This is an obvious example, but don’t be surprised how often this happens. With a long list (say, 5 to 10 choices) covering 90+ percent of the answers, including an “Other” option, they can optionally fill in a text answer for the details of the other answer. Manual entries do require more effort to analyze.</p><p class="calibre3">Results sometimes need normalization. If you asked a question about the customer’s favorite streaming service and didn’t include Disney, for example, they might choose “Other” and then type in answers such as “Disney,” “Disney Plus,” “Disney+,” The Walt Disney channel.” Results have to be tabulated into <a id="_idIndexMarker066" class="pcalibre pcalibre1 calibre6"/>one answer. Try to be complete in the answer options to avoid this problem.</p></li>				<li class="calibre8"><strong class="bold">Include a Not Applicable choice when needed to complete the possible use cases</strong>: This will help ensure better results by not forcing users to select the wrong answer to a required question.</li>
				<li class="calibre8"><strong class="bold">Consider what questions are required</strong>: Most questions are typically needed so users can’t accidentally skip entire sections.</li>
				<li class="calibre8"><strong class="bold">Create balance in the list of choice questions (</strong><strong class="bold">single select)</strong>:<p class="calibre3">Let’s use an example to ask for an age range:</p><pre class="source-code">
What is your current age?
    &lt; 20
    20 to 30
    30 to 50
    50 to 60
    60 to 70
    &gt; 70</pre><p class="calibre3">A few issues are of interest. Firstly, we might be trying to exclude a particular group (maybe only adults older than 20 are needed), so that group has 20 years in it, and that might be fine. Because we were interested in the older groups, we broke those segments down into 10-year segments but used 20 years as a group for the younger group. Be mindful of these decisions. Use consistent groups, especially in the middle of a range, and go beyond the range with those consistent groups. Let’s use a 70-80 group and then an 80+ group. So, when the data is graphed, it is segmented better. Then, the graph won’t show a significant spike in the 30-50 group, which might only happen because the group is twice the size of the other groups. Finally, it is expected to do a cross-tab on data like this (for example, comparing age range with income to verify the right audience segment is being targeted and then chart that). So, consistent group sizes will make that chart more appealing. But if the customer is 30, which choice<a id="_idIndexMarker067" class="pcalibre pcalibre1 calibre6"/> do they select? We don’t want to have overlapping values. Use these larger groups on the edges of the data, assuming they are small, insignificant groups (a few percentages or less).</p><p class="calibre3">I would revise this as follows:</p><pre class="source-code">What is your age?
    Younger than 20
    20 to 29
    30 to 39
    40 to 49
    50 to 59
    60 to 69
    70 to 79
    80 or older</pre><p class="calibre3">This is good as a drop-down menu. Everyone knows their age. They don’t need to compare answers on the screen and can quickly pick from the list. And we are not asking their age, so I changed the prompt. It takes up less space and is a cleaner user experience. I removed those greater than and less than arrows and used English instead. There is no reason to make them work hard to understand symbols like &lt; or &gt; to pick a simple answer. I selected ranges such as 20 to 29, not 21 to 30. This is consistent with how we talk (someone in their 20s or 30s).</p></li>			</ul>
			<div><div><img src="img/B21964_02_04.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<ul class="calibre7">
				<li class="calibre8"><strong class="bold">If a drop-down menu defaults to one answer, then the user is not required to select an option, which can bias results</strong>: So, in these cases, ensure the software you use defaults to an unselected state, as shown above.</li>
				<li class="calibre8"><strong class="bold">Data can always be merged later when asking a specific question (such as age). If the question uses groupings, you can’t change the groupings later</strong>: Grouping is nice when you want to correlate data. For example, with technical support customers, there is interest in knowing how many products they own. Don’t ask this if this is known from their account information. It can be correlated with the number of times they have used the site or the number of posts on each product forum. This information might help explain their expertise level. Use this information to adjust the level of detail, style, or tone in generative answers. In a GUI, the system can offer expert advice instead of a novice UI. It can also adapt further to give<a id="_idIndexMarker068" class="pcalibre pcalibre1 calibre6"/> novice generative advice only for the user’s new products and more advanced advice for products they are known to use regularly. And then adjust based on their use of that advice.</li>
				<li class="calibre8"><strong class="bold">Display all the choices for multiple selections and keep the entire list visible on the screen</strong>: With numerous options, especially if there is a limit to the number of selections, they should all be visible. If not, the user might hit their max and then see other answers, which causes them to need to rework their selection, and they might not bother. So, by ensuring choices are all visible, they can scan and decide which ones to pick.</li>
				<li class="calibre8"><strong class="bold">Keep the survey short</strong>: When fully designed, try to then reduce it by 30%. Look for places where asking the same question again is unnecessary or asking questions you don’t need. There are cases for asking the same question again, in a slightly different way, to validate the first time it was answered.</li>
				<li class="calibre8"><strong class="bold">Use branches in a survey to focus specific users on specific questions</strong>: If user answers go in one direction, we don’t need to ask them additional questions in a different direction. That is unnecessary. For example, ask questions about the iPhone <a id="_idIndexMarker069" class="pcalibre pcalibre1 calibre6"/>only to iPhone users. Please don’t ask them Android questions that are not applicable.</li>
				<li class="calibre8"><strong class="bold">Resist the urge to collect unnecessary demographics (age, location, maybe gender)</strong>: Don’t ask if it won’t add value to the analysis.</li>
				<li class="calibre8"><strong class="bold">See if a survey mechanism exists, such as a quarterly customer survey</strong>: In that case, add questions to the existing process and get results without the overhead of running a new survey. Of course, this will be on their schedule and be limited in some ways, but it could be an option.</li>
				<li class="calibre8"><strong class="bold">Use an anchored Likert scale with a middle value for ratings</strong>: Opinions, perceptions, and behaviors can be asked using a multi-point scale, typically with labels (anchors) for each or some values. I suggest a 1 to 5 range or 1 to 7 at the most (not more, except for Net Promoter Score). Anchors are the labels at each end of the scale (Poor to Great, and labels for each step if they are reasonable to define, such as for the common UX questions around ease of use: Very Difficult, Difficult, Neutral, Easy, Very Easy). We see these in post-use one-question surveys, such as “Please rate your experience today with our chat assistant.”<p class="calibre3">Survey Monkey’s primer is straightforward to digest for some basics.</p><p class="calibre3">Article: <a href="https://www.surveymonkey.com/mp/likert-scale/" class="pcalibre pcalibre1 calibre6">SurveyMonkey’s primer to Likert Scales</a> (<a href="https://www.surveymonkey.com/mp/likert-scale/" class="pcalibre pcalibre1 calibre6">https://www.surveymonkey.com/mp/likert-scale/</a>)</p></li>
				<li class="calibre8"><strong class="bold">Power tip for Likert Scales</strong>: Vary what is 1 with a collection of scales. Some 1s can be for the bad or poor concepts (Very Difficult, Never, Not Recommended), and some can mean positive or good options (Very Easy, Always, Strongly Agree). This <a id="_idIndexMarker070" class="pcalibre pcalibre1 calibre6"/>forces the user to read and review each scale. If they don’t notice, the data is invalid, but likewise, if they read and mark everything with the same choice, there is no value either.</li>
				<li class="calibre8"><strong class="bold">Test the survey with pilot users (friendlies)</strong>: Verify the wording, grammar, and flow. Check all branches and review the results to confirm they are valid. Get them to be frank. Encourage feedback.</li>
				<li class="calibre8"><strong class="bold">Consider distinct surveys for customers and internal stakeholders</strong>. Stakeholders might deeply understand customers’ needs, gather stakeholder feedback, and keep this data separate from customer results.</li>
				<li class="calibre8"><strong class="bold">Consider adding a final question about participating in future user research activities</strong>: This can generate an additional source of customers for interviews, user testing, or participatory design sessions.</li>
				<li class="calibre8"><strong class="bold">Don’t bug customers</strong>: Companies have rules. Sending one email and a follow-up is likely okay. But we don’t suggest sending more than that. Drop it if a customer doesn’t have time to reply to the second request. Also, track user responses (many tools allow this), only sending the follow-up to those who have not yet participated.</li>
				<li class="calibre8"><strong class="bold">Don’t expect more than 10-20% return rates for a survey</strong>: It is possible to incent customers if the company allows it. It is expensive and time-consuming to send every participant a gift card. It can cost less time and expense to enter customers in a raffle and distribute only a few significant incentives (I gave away iPods back in the day; a $100 gift card or more, depending on the complexity of the survey, is reasonable). This can increase the return rate somewhat (but also requires the survey to include name and contact information). If 5% return surveys by emailing 100 participants, there won’t be enough subjects for valuable results. We will cover the number of participants shortly.</li>
				<li class="calibre8"><strong class="bold">There are no good rules for how many respondents are needed for reliable data</strong>. Since some survey questions are subjective and qualitative, 30 to 200 answers could be required for a specific item. However, a more extensive survey with branching and multiple segments of an audience might need 1,000 or more respondents to get enough results on each branch. For statistically significant data for quantitative results, math found in statistics books is required to calculate the necessary sample size of respondents.</li>
				<li class="calibre8"><strong class="bold">Some survey examples</strong>: <em class="italic">Figure 2</em><em class="italic">.4</em> shows how questions and the UI interact. Learn how to use a survey tool’s capabilities to get the best results. I strongly encourage using one of the robust third-party tools that have been around for a while. Internal survey tools at large companies can lack the robustness and flexibility needed for deploying a quality survey.</li>
			</ul>
			<div><div><img src="img/B21964_02_05.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Examples of Likert survey questions</p>
			<ul class="calibre7">
				<li class="calibre8"><strong class="bold">If clarity is needed, ask ChatGPT</strong>: It has a wealth of knowledge about creating surveys. Use it to understand how to set up a survey process and how to edit questions to be clear and concise. However, as we will see in this book, the results from generative AI can also miss the mark, so make sure you understand its results. Here is an example to try.<pre class="source-code">
I want to ask in a survey how the agent performed in a support session. How would I ask this question and use an anchored Likert scale to collect responses?</pre></li>			</ul>
			<p class="calibre3">Now, let’s apply this knowledge to learn how to craft an effective survey.</p>
			<h1 id="_idParaDest-37" class="calibre5"><a id="_idTextAnchor037" class="pcalibre pcalibre1 calibre6"/>Case study on an effective survey</h1>
			<p class="calibre3">I took a survey on my phone based on an email for a product I use. They gave me about<a id="_idIndexMarker071" class="pcalibre pcalibre1 calibre6"/> 20 Likert scales. I don’t want to screenshot the vendor because there is no reason to embarrass them. I can set the stage. It was a one-page survey focused on my use of their online service. There were no questions on the page, only labels such as Internationalization, Reporting and Auditing, Performance, System of Record Integration, and Web Interface on a scale from 1 (Less) to 7 (Excellent). We won’t mention the company, but many use their products. We will call them CircleRize.</p>
			<p class="calibre3">Let’s discuss the positive and negative things learned from this case study.</p>
			<p class="calibre3">The positive things include the following:</p>
			<ul class="calibre7">
				<li class="calibre8">Before this page, they asked a Net Promoter Score question (1 to 10 scale). We will cover Net Promoter Score in more detail in <a href="B21964_10_split_000.xhtml#_idTextAnchor216" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring and Evaluation</em>. This single market research question determines whether a customer would recommend a company or product.</li>
				<li class="calibre8">They use a good range of 1 to 7 for questions.</li>
				<li class="calibre8">They labeled the 1 and the 7 anchors. It is good to know their meaning.</li>
				<li class="calibre8">They have titles for the items.</li>
			</ul>
			<p class="calibre3">The negative things are as follows:</p>
			<ul class="calibre7">
				<li class="calibre8">Those anchors? They used <em class="italic">Less</em> and <em class="italic">Excellent</em> labels. These are not opposites. They need to be balanced and clear. I don’t know what Less means as an anchor with the word Excellent. Do they mean Poor?</li>
				<li class="calibre8">The questions don’t have a <em class="italic">Not Applicable</em> option. I am forced to answer questions I don’t <a id="_idIndexMarker072" class="pcalibre pcalibre1 calibre6"/>understand, such as the one on System of Record Integration. They can do better by segmenting the 20 questions into groups and branching based on product use questions to eliminate confusion. They should also use language I understand.</li>
				<li class="calibre8">There were no questions on the actual survey page. It was a page labeled Feature Ranking. I needed clarification by looking at the page label. I first thought they wanted me to rank each item in importance (Web UI is more critical than Internationalization). Then I realized they were asking for individual ratings. It needs better wording to be clear.</li>
				<li class="calibre8">I don’t know how to answer statements such as Internationalization without a question to give it context. Do they want to see if I use it, need it, or even notice it is supported? No actual question was asked, just generic labels such as Internationalization. I have no idea what they wanted.</li>
				<li class="calibre8">What they expect to learn from this is unclear unless they plan on asking the questions regularly. For example, a survey can ask about performance to measure improvements or degradation. These questions must be repeated in subsequent surveys to see differences over time.</li>
				<li class="calibre8">The email subject for this survey was “Review your CircleRize plan, get a $25 gift card!”. I first read this as “check to see if the details of your CircleRize subscription with us are correct” instead of their intended “Provide feedback on your experience using the CircleRize plan.” I would suggest words matter. They should label the email clearly to reduce confusion and increase participation.</li>
			</ul>
			<p class="calibre3">There is more than meets the eye when throwing out some simple questions. The results from this survey, likely sent to all customers like me who were looking at various plans, are worthless. What is worse than the time and energy spent putting out this survey and wasting customers’ time is that they didn’t know better and will likely use this data to make decisions. User testing or someone with more experience could have caught some of these issues. It’s hard to tell why these issues appeared. Use your resources to create effective surveys. It is a lot of effort and won’t give a lot of return if everyone clicks on a score of four (the middle) because they don’t understand the question.</p>
			<p class="calibre3">We have discussed what to ask and what not to ask, but specific examples are more helpful. We don’t know your use case, but we can offer suggestions on how to use ChatGPT to <a id="_idIndexMarker073" class="pcalibre pcalibre1 calibre6"/>solve customer issues.</p>
			<p class="calibre3">Here are some questions typically asked in a survey and how they might be analyzed.</p>
			<ul class="calibre7">
				<li class="calibre8">Demographic:<ul class="calibre19"><li class="calibre8"><strong class="bold">Examples</strong>: Age, gender, education level, years of product use, expertise</li><li class="calibre8"><strong class="bold">Analysis</strong>: Style or tone can be associated with expectations for support, level of expertise, age, and other demographics</li></ul></li>
				<li class="calibre8">Multiple choice:<ul class="calibre19"><li class="calibre8"><strong class="bold">Examples</strong>: What three attributes of a service are most important to you?</li></ul><p class="calibre3">Which products give you the most headaches?</p><p class="calibre3">A list of competitors’ products that they already use.</p><ul class="calibre19"><li class="calibre8"><strong class="bold">Analysis</strong>: Judge user goals; accuracy is likely one, but availability in the context of their use is more important than, say, integration with another service. The proper selections are needed so a lot of data doesn’t go into the overflow <em class="italic">Other</em> category. The survey can explore other products that customers use and like to learn about possible advantages for competitors.</li></ul></li>
				<li class="calibre8">Likert Scales (1 to 5, 1 to 7 scales with labeled anchors):<ul class="calibre19"><li class="calibre8"><strong class="bold">Examples</strong>: How satisfied were you with the support call today?</li></ul><p class="calibre3">What is your level of experience with product x?</p><p class="calibre3">Comfort level with giving <a id="_idIndexMarker074" class="pcalibre pcalibre1 calibre6"/>confidential information to an agent or bot.</p><p class="calibre3">Satisfaction with the level of service, responsiveness, availability, accuracy, and cost.</p><ul class="calibre19"><li class="calibre8"><strong class="bold">Analysis</strong>: Find opportunities for improvement for support and gauge overall quality over time as improvements or changes occur. Determine tradeoffs between 24/7 support and level of service.</li></ul></li>
				<li class="calibre8">Net Promoter Score:<ul class="calibre19"><li class="calibre8"><strong class="bold">Example</strong>: On a scale of 0 to 10, how likely are you to recommend [company, product, or service] to a friend or colleague?</li><li class="calibre8"><strong class="bold">Analysis</strong>: This is a quick snapshot of customer sentiment that can easily be compared to other companies or services or used as a benchmark for improvements. <a href="B21964_10_split_000.xhtml#_idTextAnchor216" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring and Evaluation</em> will cover this in more detail.</li></ul></li>
				<li class="calibre8">Open-Ended questions:<ul class="calibre19"><li class="calibre8"><strong class="bold">Example</strong>: What do you like about your favorite streaming service?</li></ul><p class="calibre3">Tell us about your best experience using product Y.</p><p class="calibre3">Tell us any horror stories from using product X.</p><ul class="calibre19"><li class="calibre8"><strong class="bold">Analysis</strong>: This is an opportunity to benchmark against the competition; open-ended questions help go deeper into one area of interest. It could be a follow-up to the multiple-choice questions exploring what competitive products they use. Ask them specific questions. This can be correlated with the demographic question above to identify where particular demographics are<a id="_idIndexMarker075" class="pcalibre pcalibre1 calibre6"/> likelier to enjoy different services. These require more effort to analyze, classify, and correlate.</li></ul></li>
				<li class="calibre8">Quantitative data:<ul class="calibre19"><li class="calibre8"><strong class="bold">Example</strong>: How many hours a day do you spend on our site?</li></ul><p class="calibre3">How much did you pay last month for streaming services?</p><p class="calibre3">What is your monthly car payment?</p><p class="calibre3">When was the last time you emailed us for support?</p><p class="calibre3">How long is too long when on a tech-support phone call?</p><ul class="calibre19"><li class="calibre8"><strong class="bold">Analysis</strong>: The question about hours a day differs from what it seems. There can be analytics on their time spent on your site; this is more about perception. It can be helpful to compare actual usage data if they report less than they pay or if they report more. It can also be cross-tabbed with information on the types of activities they do on the site. This might reveal if some activities are more time-consuming than others and provide insights into how to invest resources to reduce activities that take a long time. With customer data, you can correlate perceived time, cost, and use with actual time, price, and use. This can explain if customers’ perceptions match expectations.</li></ul></li>
				<li class="calibre8">Trends:<ul class="calibre19"><li class="calibre8"><strong class="bold">Example</strong>: Which of the following online payment services have you used?</li><li class="calibre8"><strong class="bold">Analysis</strong>: A survey can capture competitive information. This data can be used to research partner opportunities or see how certain trends are viewed within specific demographic groups.</li></ul></li>
				<li class="calibre8">Behavioral:<ul class="calibre19"><li class="calibre8"><strong class="bold">Example</strong>: What was the primary reason for visiting our site?</li></ul><p class="calibre3">If you were to tell a friend about your experience with support today, what would you say?</p><p class="calibre3">How likely are you to post something about product Y on social media?</p><ul class="calibre19"><li class="calibre8"><strong class="bold">Analysis</strong>: Hopefully, this is a simple single-choice list, but it can branch into more details (e.g., whether they were having product issues, looking for a new product, downloading something, or providing a review).</li></ul></li>
				<li class="calibre8">Comparative:<ul class="calibre19"><li class="calibre8"><strong class="bold">Example</strong>: Place these five services in order of value to your needs.</li></ul><p class="calibre3">Which of these pieces of sales information is the most valuable?</p><p class="calibre3">Rank these attributes from most to least desirable.</p><p class="calibre3">Which phrasing of these statements feels most natural to you?</p><ul class="calibre19"><li class="calibre8"><strong class="bold">Analysis</strong>: Asking customers to rank features is better than asking them to rate them. Ranking only places features in order of importance. It doesn’t help you know how important they are. Ranking questions are hard for a user to answer. Limiting ranking to five items, but if some items are considered equal, forcing a ranking can cause frustration. This type of question can be used to try different styles and tones. See if there are correlations with demographics or other information to create customized personas for the conversational style. There is a lot of psychology in writing effectively.</li></ul></li>
			</ul>
			<p class="calibre3">Wow! I hope that wasn’t too much to process without giving all the training necessary to be a survey expert. Grow expertise, use resources, partner with other experts, and do what it takes! Please return<a id="_idIndexMarker076" class="pcalibre pcalibre1 calibre6"/> to this when it is time to survey and check off the items to ensure a reliable study with results the team can trust.</p>
			<p class="calibre3">As mentioned, surveys are a great place to meet people worthy of an interview. What a nice segue into discussing our next user research method: interviews.</p>
			<h1 id="_idParaDest-38" class="calibre5"><a id="_idTextAnchor038" class="pcalibre pcalibre1 calibre6"/>Designing insightful interviews</h1>
			<p class="calibre3"><strong class="bold">Interviews</strong> might not have been your first choice, but we encourage exploring interviews as a valuable tool for understanding use cases. Use a survey to gather general information, and do a click analysis (or log analysis) to get actual behavior. An interview is vital to get a cohesive feel for the customer. It has its downsides: it is hard to scale, results are more open to interpretation, and it is sometimes challenging to get the right mix of customers. The upside is the feeling you get for the customer and the empathy one can embody as the customer advocate during the design process. This will enrich the project. In surveys, consider finishing with questions about participating in future user research. Although this is self-selecting, it is fruitful almost all of the time.</p>
			<p class="calibre3">There might be time for five to ten interviews, but most teams will unlikely invest in more than 20. Targeting the right audience has value. The cost of setting up the interviews is likely more significant than the time required to conduct the interviews (typical sessions run for 30 to 60 minutes). Customers are busy, and incentives might not be an option when working with existing customers. They tend to need to reschedule because this is a low priority for them. Even with the downsides, I still recommend the interview process.</p>
			<p class="calibre3">Don’t expect demos or visuals to be shared to get feedback when dealing with conversational solutions. Ask them to have the existing products or web support site handy during an interview. Walking through some use cases creates a discussion around enhancement opportunities (focus on their needs, not their solutions). Don’t constrain them to what are perceived as AI issues; if they go somewhere else, take their feedback, pass it on to a team (or file a bug or enhancement request on their behalf), and then bring their discussions back into focus. Create a relationship where they trust that you are trying to help the company help them to do their job or task.</p>
			<p class="calibre3">Put together a framework for interviews that supports gathering the needed information. If you are trying to do a conversational assistant around support, focus on understanding the complexities they experience when trying to get answers. If you are building a recommendation<a id="_idIndexMarker077" class="pcalibre pcalibre1 calibre6"/> engine, look for places to discuss how they currently get recommendations, who gives them, and how much they trust the information. Maybe the idea is to replace or improve existing algorithms that offer sales information (likelihood to close, timeline for shipping custom solutions, promotional tie-in offers, product recommendations). Then, focus on understanding when they need this information, what channels help or annoy them (SMS, email, automated phone calls, or on the website), and how the placement of the information in the process or frequency of that presentation might influence their opinion.</p>
			<p class="calibre3">There are plenty of examples of scripts for interviews. Adapt these templates to inject the appropriate content into an interview script.</p>
			<p class="calibre3">Article: <a href="https://guides.18f.gov/ux-guide/interview-script/" class="pcalibre pcalibre1 calibre6">Example user Interview Script</a> (<a href="https://guides.18f.gov/ux-guide/interview-script/" class="pcalibre pcalibre1 calibre6">https://guides.18f.gov/ux-guide/interview-script/</a>)</p>
			<p class="calibre3">Let me walk through some tips and suggestions for interviews.</p>
			<h2 id="_idParaDest-39" class="calibre9"><a id="_idTextAnchor039" class="pcalibre pcalibre1 calibre6"/>Defining research objectives</h2>
			<p class="calibre3">Clearly outline the goals and objectives of<a id="_idIndexMarker078" class="pcalibre pcalibre1 calibre6"/> the interview. For example, with a survey, we want to know why we ask these questions beforehand. Keep the interview focused on the results suited for the interview process. Please don’t turn it into a live survey. If you need demographic information, get it out of the way before. This pre-interview allows tailoring the interview based on the results.</p>
			<p class="calibre3">The interview can use samples, demos, or visuals. But the goal is to get them talking and giving feedback, insight, knowledge, and wisdom. Use the opportunity to go deeper and adapt to what they say and how they say it. There can be nuance in how they answer that might reveal hidden truths. Explore existing or previous customer interactions, competitive product use and feedback, style and tone discussions, and a wealth of use case examples, like in surveys. However, unlike a survey, go deeper, easily adjust or drill down<a id="_idIndexMarker079" class="pcalibre pcalibre1 calibre6"/> the line of questioning, impromptu sign into a website, show a different example, and try to get more understanding of and empathy for the customer.</p>
			<h2 id="_idParaDest-40" class="calibre9"><a id="_idTextAnchor040" class="pcalibre pcalibre1 calibre6"/>Selecting participants</h2>
			<p class="calibre3">Define the target participant group based on research objectives. Ensure the selected participants have the relevant<a id="_idIndexMarker080" class="pcalibre pcalibre1 calibre6"/> knowledge or experience related to the research topic. If they are existing customers, consider the customer profile (long-term customers, new customers, those with lots of products, those only with a specific product, those who file a lot of trouble tickets, technical, non-technical, those who self-serve through a community website, etc.). Set up a process to find and interview the right audience. Sometimes, it is the survey that will resolve this issue. Here are some tips to help with this part of the process:</p>
			<ul class="calibre7">
				<li class="calibre8">Schedule 20-30% more than needed for interviews of 10 to 20 participants. If someone cancels, you don’t have to start the recruiting process again, and results will not be delayed. Don’t cancel if you get lucky and everyone shows up; it is not respectful. Either follow the process or look for opportunities to extend understanding with different lines of questioning. Maybe drill down further into areas exposed by subjects where there wasn’t time with previous subjects.</li>
				<li class="calibre8">It would be great if your company had someone to recruit and schedule. Expect recruiting to take longer than the actual interviews. So, account for this in the schedule. There are also outside firms that will assist in recruiting.</li>
				<li class="calibre8">Don’t book interviews back-to-back. Allow an hour in between. This allows overtime if the customer is okay with it and gives time to write up additional notes while they are fresh.</li>
				<li class="calibre8">The most interviews I have done for one project is 20. There is no rule for figuring out the correct number; with vast subject areas or many different customer profiles, more interviewees are warranted. However, as with discount usability methods, overlapping feedback will start to dominate, and you don’t want to be only 10% of the way through interviews and notice the same results over and over. If this was a user test, we have data to suggest a small number, such as five, is sufficient for some tests. However, an interview is less structured than using a piece of software, so consider the cost-benefit tradeoff. Read this article to see how to apply discount usability methods, a practice for iterative small studies useful for context even with the interview process.<p class="calibre3">Article: <a href="https://www.nngroup.com/articles/discount-usability-20-years/" class="pcalibre pcalibre1 calibre6">Discount usability methods</a> (<a href="https://www.nngroup.com/articles/discount-usability-20-years/" class="pcalibre pcalibre1 calibre6">https://www.nngroup.com/articles/discount-usability-20-years/</a>)</p></li>
			</ul>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">To improve the rate of people showing up, call (an actual phone call) the day before, reminding them of the scheduled time. They may appreciate the personal touch to confirm the meeting, and if they can’t show up, they might tell you right then so you are not sitting and waiting. When it is time, join the web conference five minutes early (I assume most sessions are via a web conference; if they are in person, good for you!), and send them a message with the link one more time. “I joined a few minutes early to make sure I am ready for you to join. I look forward to our session in a few minutes.”</p>
			<h2 id="_idParaDest-41" class="calibre9"><a id="_idTextAnchor041" class="pcalibre pcalibre1 calibre6"/>Develop a structured interview program</h2>
			<p class="calibre3">Create a <strong class="bold">detailed interview program</strong> that includes standard <a id="_idIndexMarker081" class="pcalibre pcalibre1 calibre6"/>questions and optional follow-ups. Questions should be clear, unbiased, and designed to elicit the necessary information. Please encourage them to<a id="_idIndexMarker082" class="pcalibre pcalibre1 calibre6"/> think aloud and to expand on their reasoning. Recognize that some customers don’t have anything to say on a subject. Be prepared to skip ahead or branch to the topics of interest. No rule says everyone has to answer every question or line of discussion. However, when doing the analysis, it is nice to say, “6 out of 8 customers expressed the same lack of support for XYZ without being prompted.” That is a powerful statement.</p>
			<p class="calibre3">In the script, include tips to be reminded to prompt customers to think aloud or expand their thinking. If they are already doing it, great, but when in the zone, you might forget; this is a good reminder.</p>
			<p class="calibre3">Keep biases to yourself. We can easily express where we want the customer to go with their feedback. Sometimes, there<a id="_idIndexMarker083" class="pcalibre pcalibre1 calibre6"/> are cultural issues with providing honest feedback. That is, some people will only give positive suggestions. Feel out the audience. Give them prompts that probe positive and negative discussions. This will help adjust the approach to tease out the most valuable feedback from that participant.</p>
			<h2 id="_idParaDest-42" class="calibre9"><a id="_idTextAnchor042" class="pcalibre pcalibre1 calibre6"/>Pilot the interview process and program</h2>
			<p class="calibre3">Conduct a pilot with a small group of participants to identify any issues with the interview process. This should include technology, sound, and quality checks, as well as checking the entire process to test <a id="_idIndexMarker084" class="pcalibre pcalibre1 calibre6"/>timing and scheduling issues. Ask the questions as expected to be asked. This will verify if the scheduled time is sufficient and if the questions sound natural when spoken out loud. Record the session. Take note of where something should be changed or review the recording. It is a pilot. Make and correct mistakes now. Use these learning to update and edit the program and try again:</p>
			<ul class="calibre7">
				<li class="calibre8">It is okay to pivot the program even after some interviews are conducted. You won’t get as much data for something the interview shifts away from, but if you realize that a different tactic is necessary, make the change. Focus on more concerning areas.</li>
				<li class="calibre8">Just because one user gives excellent feedback doesn’t mean it will be heard from more than one user. Proceed with caution if you pivot too soon.</li>
				<li class="calibre8">ChatGPT can edit the program to give it a more natural and conversational feel.</li>
			</ul>
			<h2 id="_idParaDest-43" class="calibre9"><a id="_idTextAnchor043" class="pcalibre pcalibre1 calibre6"/>Conduct the structured interviews</h2>
			<p class="calibre3">Follow the interview program, asking questions in the predefined order. Be neutral in tone and avoid leading questions that might bias <a id="_idIndexMarker085" class="pcalibre pcalibre1 calibre6"/>responses. Record responses accurately through note-taking, recording, a note-taker, or another method:</p>
			<ul class="calibre7">
				<li class="calibre8">Before starting the interview, ensure participants understand the purpose of the research and agree to participate voluntarily. Generally, we read a short statement that addresses the purpose, how the information will be used (video or audio recordings, quotes, and profile information), and how participant confidentiality will be respected. We also informed them that they could stop and withdraw from the study. A user researcher probably already has this boilerplate information available to them. If you are a designer or product person, use a researcher to run the study.</li>
				<li class="calibre8">If you’re screen recording, help the user hide or remove confidential or unrelated materials before starting the recording. This will help build trust and allow the recording to focus more on the project.</li>
				<li class="calibre8">If you are the interviewer, have a confederate take notes. It allows more focus while in the interview. There are a variety of user experience tools that will enable synchronized note-taking. Interviewer verbal cues such as “That is a very interesting point,” “We should make a note of that,” and “Thanks for your observation” can help the note-taker<a id="_idIndexMarker086" class="pcalibre pcalibre1 calibre6"/> know what is essential to you and engage the participant.</li>
				<li class="calibre8">Allow other insiders, owners, or advocates to watch or join the call. Be clear that they are silent observers during the interview.</li>
				<li class="calibre8">Those silent observers can chat on a private channel. They might have insights into an answer you don’t know or can provide prompts with helpful follow-up questions.</li>
				<li class="calibre8">Probe for clarification and encourage participants to elaborate on their responses. We want their insight, which is more valuable than simple answers readily available from a survey. The interview is a time to <em class="italic">fill in the blanks</em> and to understand their needs, why, when, with what, how, and who.</li>
				<li class="calibre8">Ensure consistency in how questions are asked and how responses are recorded. Audio or screen-sharing recordings are great to go back and review. Account for at least 2x the time of the recording for review.</li>
				<li class="calibre8">Review the recording to ensure the quote is correct and in context. It is wrong to quote a customer when<a id="_idIndexMarker087" class="pcalibre pcalibre1 calibre6"/> they say, “This is the best!” but it was entirely out of context (or said sarcastically).</li>
				<li class="calibre8">This is worth repeating a few times: avoid introducing personal bias into the interview process. It is ok to emphasize a specific feature or task, but let the customers speak. Don’t push them one way or another. Instead, put them at ease to address complex problems and give honest feedback. If the interviewer feels too invested in the solution and wants to defend it against scrutiny, your interviewee will sense this and not share fully.</li>
				<li class="calibre8">One way to avoid bias is to conduct the interview with a researcher who is not invested in the results. If you are the designer or product manager, let them run the interviews, take notes, and help behind the scenes. This is one of the most valuable tips we can share.</li>
				<li class="calibre8">Be an active listener. Use verbal neutral interjections such as “I understand” and “got it,” and reiterate what they said. Use nonverbal cues (nodding and eye contact) to show you are listening.</li>
			</ul>
			<h2 id="_idParaDest-44" class="calibre9"><a id="_idTextAnchor044" class="pcalibre pcalibre1 calibre6"/>Record and document findings</h2>
			<p class="calibre3">Record detailed notes or transcribe interviews for analysis. Generative AI tools are an excellent resource for transcription. Some <a id="_idIndexMarker088" class="pcalibre pcalibre1 calibre6"/>web conferencing tools have this built-in; turn it on or do it after the fact. This can make it easy to mark up critical elements of the conversation. Document any patterns, trends, or notable insights during the interviews. Once you have this annotated data, turn to the analysis. Some tools allow note-taking with timestamps. This is a great way to log events to return to later. Even if you are alone and don’t have time to take notes, type a placeholder (like ***) and return to the recording to capture that insight. The more notes, the better. Ignore off-topic stuff later. Work with an interdisciplinary team to evaluate items that might be off-topic for you but are in scope for other parts of the organization. It is better to catch and discard than not capture and then later realize there might be something to that information.</p>
			<h2 id="_idParaDest-45" class="calibre9"><a id="_idTextAnchor045" class="pcalibre pcalibre1 calibre6"/>Data analysis</h2>
			<p class="calibre3">Analyze the collected quantitative (for closed or demographic questions) and qualitative (for open-ended questions) results. Combine them to form more robust results. For example, what if three<a id="_idIndexMarker089" class="pcalibre pcalibre1 calibre6"/> out of four novice users didn’t think to ask for shipping information, while all four expert users explicitly wanted shipping information? Because this line of discussion was explored, it was revealed that the experts know that the company doesn’t ship quickly, and that has been an issue in the past, while the novice users are expecting Amazon-like shipping and didn’t think and didn’t have enough experience to know that wasn’t the case. Only by segmenting this data by the novice/expert role would we recognize how to solve this problem:</p>
			<ul class="calibre7">
				<li class="calibre8">Expect to take at least three times the clock time of the interviews to document the findings (2x) and then do the analysis (1x). If it took a week to gather 10 hours of interviews, it will take the next week to complete this analysis.</li>
				<li class="calibre8">Even small results (found by one or two customers) might be valuable, so be sure to capture all data. In <a href="B21964_04.xhtml#_idTextAnchor085" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 4</em></a>, <em class="italic">Scoring Stories</em>, we explain how to prioritize based on the number of customers who can be impacted. Even though only one or two customers reported something, it doesn’t imply that only a few would benefit. Profound insight can come from one person; you have to recognize it.</li>
			</ul>
			<h2 id="_idParaDest-46" class="calibre9"><a id="_idTextAnchor046" class="pcalibre pcalibre1 calibre6"/>Report findings</h2>
			<p class="calibre3">Present research findings in a clear and organized manner. Use the User Needs Scoring method (<a href="B21964_04.xhtml#_idTextAnchor085" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 4</em></a>, <em class="italic">Scoring Stories</em>) to prioritize the<a id="_idIndexMarker090" class="pcalibre pcalibre1 calibre6"/> results. Review the most to least valuable results in order. This way, when an item is discussed, the attendee’s attention is focused on the most valuable pieces of feedback:</p>
			<ul class="calibre7">
				<li class="calibre8">Include participant quotes and critical insights to support conclusions. Be sure to quote correctly and in the context, the participant intended.</li>
				<li class="calibre8">Share out-of-scope findings with the appropriate teams.</li>
				<li class="calibre8">File bugs or enhancements into the company tracking tool. Assign them to the appropriate person for resolution.</li>
				<li class="calibre8">Include feedback from colleagues participating in the process.</li>
				<li class="calibre8">Include issues or hiccups from the study to follow up for yourself and the team.</li>
				<li class="calibre8">Have a retrospective <a id="_idIndexMarker091" class="pcalibre pcalibre1 calibre6"/>with the team to learn and document what to do differently and better next time.</li>
			</ul>
			<h2 id="_idParaDest-47" class="calibre9"><a id="_idTextAnchor047" class="pcalibre pcalibre1 calibre6"/>Summary of the interview process</h2>
			<p class="calibre3">Conducting good interviews is a skill. It takes<a id="_idIndexMarker092" class="pcalibre pcalibre1 calibre6"/> practice, expertise, and good people skills. Especially for novice interviewers, do a few extra practice sessions before the first official interview. Ask a design researcher to participate in the pilot interviews and give feedback. There is an art and science to doing this well. The Nielsen Norman Group article has some helpful content. They even offer classes on how to perform successful user interviews.</p>
			<p class="calibre3">Article: <a href="https://www.nngroup.com/articles/user-interviews/" class="pcalibre pcalibre1 calibre6">User interviews</a> (<a href="https://www.nngroup.com/articles/user-interviews/" class="pcalibre pcalibre1 calibre6">https://www.nngroup.com/articles/user-interviews/</a>)</p>
			<p class="calibre3">Also, once we get a ChatGPT solution running, revisit this process to interview those who have used the solution and those who have not or intentionally not used the ChatGPT-based solution. This applies to any implementation, not just conversational solutions. Once a solution is available, there will be different topics to explore.</p>
			<p class="calibre3">Surveys and interviews are only two methods. We encourage using the correct methods to uncover user attitudes and expectations. Later, you might do user testing or a heuristic evaluation. More on those methods in <a href="B21964_10_split_000.xhtml#_idTextAnchor216" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring and Evaluation</em>. However, we have some context from which to work during this research phase. So, we covered two methods that directly engage customers, focused on their attitudes, with primarily qualitative results. Now, we pivot to behavioral and attitude research with clickstream analysis.</p>
			<h1 id="_idParaDest-48" class="calibre5"><a id="_idTextAnchor048" class="pcalibre pcalibre1 calibre6"/>Getting started with conversational analysis</h1>
			<p class="calibre3">The final type of research we want to cover is <strong class="bold">conversational analysis</strong>. We don’t have a ChatGPT solution yet, so how can I have anything to analyze? Maybe not, but let’s consider other sources. Are there existing trouble tickets or service requests? Maybe there are live-person chats or phone channels. Even reviewing community posts in forums is a good source. Perhaps an <a id="_idIndexMarker093" class="pcalibre pcalibre1 calibre6"/>existing conversational AI doesn’t include ChatGPT integration. This is an excellent source of existing data. Conversational analysis isn’t valuable for backend solutions or even recommendation systems. Use other research and needs analysis tools there.</p>
			<p class="calibre3">At one company, we kept metrics on the quantity and type of support calls. Unsurprisingly, password reset requests and account activations were broken out. We precisely knew the percentage of calls for these services and could then calculate savings to a change in a process that improved that task. For example, everyone has done a password reset via a web form. After providing some additional private information, and these days, a phone number to send a 2-factor authentication code to, an email is sent with a link to reset the password. It wasn’t always this way. The cost of a phone call to do this versus a web form makes it easy to calculate cost savings. A list of service types broken down by percentage is a valuable tool for use case analysis to determine possible opportunities for a ChatGPT-driven solution. As we will see, some use cases are better suited for a generative AI solution.</p>
			<p class="calibre3">Consider all sources of customer interaction; there can be data worth investigating. And yes, once a conversational AI with ChatGPT is running, that becomes the primary source of learning to continue the care and feeding process.</p>
			<p class="calibre3">Reviewing logs for clues on gaps in support, sales, service, or other features has advantages and disadvantages.</p>
			<p class="calibre3">Some upsides include<a id="_idIndexMarker094" class="pcalibre pcalibre1 calibre6"/> the following:</p>
			<ul class="calibre7">
				<li class="calibre8">It is accurate information about the current state of affairs</li>
				<li class="calibre8">It is likely already grouped and segmented, as existing people and teams support these interactions</li>
				<li class="calibre8">Existing AI tools can help triage and organize this data (we will share one example from HumanFirst in a few minutes)</li>
				<li class="calibre8">It is helpful for people new to the space to understand the support pain points</li>
				<li class="calibre8">It could be a great source of knowledge to feed the solution</li>
			</ul>
			<p class="calibre3">There are a few downsides <a id="_idIndexMarker095" class="pcalibre pcalibre1 calibre6"/>as well:</p>
			<ul class="calibre7">
				<li class="calibre8">It doesn’t represent where the solution should go</li>
				<li class="calibre8">It is a lot of data to process</li>
				<li class="calibre8">It is time-consuming (some pieces can be automated)</li>
				<li class="calibre8">It might be self-selecting and focusing the user on only what it can do, not what the user came to do</li>
			</ul>
			<p class="calibre3">Conversational analysis is<a id="_idIndexMarker096" class="pcalibre pcalibre1 calibre6"/> a valuable tool. We will work to address some of the downsides, but I also want to focus on what to do with the data and how to use this research tool to establish the organization’s needs. Let’s use these methods to drive the requirements, priorities, and a roadmap for improvements.</p>
			<p class="calibre3">The best logs to analyze would be those closest to the new ChatGPT solution. For example, existing Chatbot logs are a perfect place to look for existing needs and understand the failures of the current solution. The second best would be chat logs with human agents. These have much more conversational noise, and the interactions are not directed at solving a problem (we call this chit-chat). The analysis of these logs takes more time to filter. Other locations would be community social interactions or public support sites. They are even more challenging to parse. Sometimes, support people monitor those sites and offload challenging interactions into private channels while some community members answer common questions.</p>
			<p class="calibre3">In these cases, the correct answer is needed to analyze the logs. We call this <strong class="bold">ground truth</strong>: information we know is factually accurate. Sometimes, this is easy. The user gets the answer, tries it out, and reports back. Sometimes, you don’t have that feedback. Knowing the space or working with experts can help determine how well the current solution works. A benchmark is needed to understand how well the improved solution performs.</p>
			<p class="calibre3">There are automation tools to help organize and parse logs and usage information. Companies such as humanfirst.ai (<a href="https://www.humanfirst.ai/conversational-ai" class="pcalibre pcalibre1 calibre6">https://www.humanfirst.ai/conversational-ai</a>) provide tools to manage data. This is faster than manually tagging and labeling. It takes examples and then offers lists of similar phrases, as shown in <em class="italic">Figure 2</em><em class="italic">.5</em>. This works for many use cases. We<a id="_idIndexMarker097" class="pcalibre pcalibre1 calibre6"/> will review the manual version of these processes so you know how to tag and organize data and will understand if automation is performing well.</p>
			<div><div><img src="img/B21964_02_06.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.5 – An example of the HumanFirst tool that classifies input into collections</p>
			<p class="calibre3">Seek the necessary approvals to access and analyze the material. Sometimes, chat logs can contain <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>), which makes security teams worried about distribution. We agree. So, work<a id="_idIndexMarker098" class="pcalibre pcalibre1 calibre6"/> with them to mask this information, as we don’t need access to this PII. When appropriate, work with them to prevent this information from being stored in the first place. We have seen some reasonable solutions that use automated systems to make a payment, authenticate, or mask sensitive data, so there is never a human in the loop with access to something they should not.</p>
			<p class="calibre3">Here is my proposal for measuring and creating a benchmark to judge future improvements and a tool to identify gaps in current processes that become candidates for the ChatGPT solution.</p>
			<ol class="calibre12">
				<li class="calibre8">Collect a recent log file, hopefully a chat log file, as discussed. Gather at least 1,000 interactions (a question and a response are one interaction).</li>
				<li class="calibre8">Mask any PII as needed.</li>
				<li class="calibre8">Tag each interaction as a success or failure. Subclassify the interactions to understand the issues that need to be addressed.</li>
				<li class="calibre8">Analyze the results for improvement opportunities (or places where the ChatGPT solution might solve the existing problem).</li>
				<li class="calibre8">Score the 1,000 interactions and use this score (% success rate) as an improvement benchmark.</li>
			</ol>
			<p class="calibre3">To do all this, we want to teach skills to tag interactions and classify the opportunities for improvement. Once this is done, the score should be easy to measure. An expert in the space can, even with very technical material, judge the success or failure of each turn with about 99% accuracy. Even novices can correctly judge 90% of interactions. So, with limited expert availability, ask for their help on that missing 10%. Over time, novices will reduce the 10% gap. Let’s get started with how to tag a log file.</p>
			<h2 id="_idParaDest-49" class="calibre9"><a id="_idTextAnchor049" class="pcalibre pcalibre1 calibre6"/>Tagging a log file should focus on each interaction</h2>
			<p class="calibre3">Each back-and-forth is a single interaction. If a question is asked, it should be answered. We can analyze each <a id="_idIndexMarker099" class="pcalibre pcalibre1 calibre6"/>question if multiple questions were asked before getting an answer (like from a community forum). The answer might cover none, some, or all of the questions. Each answer might be a success or failure, each counted individually.</p>
			<p class="calibre3"><em class="italic">Figure 2</em><em class="italic">.6</em> is a sneak peek at a conversational log we will use in our drill-downs. We will provide a spreadsheet template to use with your data.</p>
			<div><div><img src="img/B21964_02_07.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.6 – A snapshot from a conversational log being analyzed</p>
			<p class="callout-heading">Masking tip</p>
			<p class="callout">Sometimes, teams are uncomfortable with pure log files because of the PII. If this data still exists in the file, one suggestion is to mask it. Typically, the data to be masked is <em class="italic">numerical</em>, such as social security or credit card numbers or people’s names and emails. Some countries might have specific rules around PII, so understand your region.</p>
			<p class="calibre3">Hopefully, you have<a id="_idIndexMarker100" class="pcalibre pcalibre1 calibre6"/> automated scripts for masking user data. Tools from vendors like IBM, Oracle, ServiceNow, and Salesforce have data masking features. There are third-party tools readily available. If not, we will help with a simple and practical approach. Do it manually and then turn it into a script:</p>
			<ol class="calibre12">
				<li class="calibre8">Numerical data can be substituted or masked by simple Find and Replace features in any spreadsheet. Look for patterns for numbers (Social Security, Phone, Amounts) and replace them with 1s.</li>
				<li class="calibre8">The <strong class="source-inline1">@</strong> sign can be found and used as a marker to replace email addresses with something generic in email patterns. I suggest keeping the same patterns in the file and not deleting the now-masked data.</li>
				<li class="calibre8">You might have data around a specific type of masked data that could be useful in automation or process improvements. For example, how often do customers provide a PIN or security code to a human agent? This can give a sense of the scale of the problem, how much effort is involved, and if there is an opportunity to resolve this with automation or a generative AI solution.</li>
				<li class="calibre8">Dates can be manipulated. Consider whether they must maintain a sequence for the data to be valid (such as when charting progress).</li>
				<li class="calibre8">Consider whether <a id="_idIndexMarker101" class="pcalibre pcalibre1 calibre6"/>masking should use random values so someone can’t quickly reverse engineer the original data.</li>
				<li class="calibre8">I have rarely found that masked data impacts understanding.<p class="calibre3"><em class="italic">Table 2.1</em> depicts some examples of masking.</p></li>
			</ol>
			<table id="table001-1" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<thead class="calibre18">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Type</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Original</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Masked</strong></p>
						</td>
					</tr>
				</thead>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Social Security # (</strong><strong class="bold">SSN)</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">123-45-6789</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">*-*-6789</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Credit </strong><strong class="bold">Card Numbers</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">1234-5678-9012-3456</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">--****-3456</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Names</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">John Doe</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">J*** D**</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Email Addresses</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">john.doe@company.ai</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">j***.d**@company.ai</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Phone Numbers</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">(555) 123-4567</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">(***) ***-4567</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Addresses</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">5622 Main St, City, Country</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">5622 Main St, C***</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">IP Addresses</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">192.168.1.1</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">192.***.*.*</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Usernames</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Johndoe123</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">J*******</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Employee IDs</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">EMP-00123</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">EMP-*****</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Date </strong><strong class="bold">of Birth</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">07/15/1975</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">01/01/1980</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Table 2.1 – Typical masking examples for PII</p>
			<p class="calibre3">Once the data is masked, consider outsourcing or using cloud analysis tools. By reviewing and analyzing the logs, we can define success and failure.</p>
			<h2 id="_idParaDest-50" class="calibre9"><a id="_idTextAnchor050" class="pcalibre pcalibre1 calibre6"/>Define success and failure categories</h2>
			<p class="calibre3">Each row in a <a id="_idIndexMarker102" class="pcalibre pcalibre1 calibre6"/>spreadsheet that represents a response (from the human agent, a bot, or someone on a community site) is either helpful or not. We classify interactions into success, failure, unclassifiable, or ignore. However, there is value in a more robust breakdown depending on the log analysis and the kinds of fixes that issues might necessitate. Problems can be aggregated and allocated to different teams for resolution. <em class="italic">Consider collapsing categories if the same team is going to resolve similar items or ignore some of these if the type of data collected makes </em><em class="italic">them unnecessary</em>.</p>
			<p class="calibre3">Think about the user’s intent, which should match to an answer. The content presented is the response to the user, and it is organized and presented in a structure, such as a bulleted list or just text. We can determine if we gave a good answer, an answer that needs some improvements, or one that missed the mark for various reasons. This forms the system of classification.</p>
			<p class="calibre3">The goal of categorization is not just to see where problems exist but because this classification can be used to evaluate the effort needed by different teams to improve the overall quality. Picking a category is intended to be done in a manageable amount of time. The time in the log file is well spent to get an overall sense of quality and then have the specific issues to address, and they also can create new test cases for future iterations.</p>
			<p class="calibre3">Do not take these categories as perfect; add or remove some and annotate others based on the value of classification and what the team will fix. Some may be done by the prompt engineers, others by the fine-tuning squad, and with a voice interface, the test-to-speech team will work on improvements. Moreover, identify when the knowledge base articles need improvement. Here are the detailed categories.</p>
			<h3 class="calibre11">Category – Success</h3>
			<p class="calibre3">The question was answered. We <a id="_idIndexMarker103" class="pcalibre pcalibre1 calibre6"/>are happy with it being complete, correct, and up to date. It is much less costly to classify successes<a id="_idIndexMarker104" class="pcalibre pcalibre1 calibre6"/> than any issues. Classifying the first 1,000 responses with a 60% success rate will take longer than later iterations with a 95% success rate. Just keep at it and judge each interaction on its merit.</p>
			<h3 class="calibre11">Category – Qualified Success &gt; Response &gt; Content</h3>
			<p class="calibre3">Qualified Success, but the response has a <em class="italic">content</em> issue.</p>
			<p class="calibre3">Even though the answer was correct, it contained additional content, which might cause some issues. The content could include unrelated or immaterial information or too much or too little content, which might relate to a knowledge base problem. For example, including the phone number when asked for an address is reasonable, but if the answer also includes the company’s <a id="_idIndexMarker105" class="pcalibre pcalibre1 calibre6"/>mission statement, then it would be immaterial. A recommender including references to something unrelated would be a content issue. This needs to be <a id="_idIndexMarker106" class="pcalibre pcalibre1 calibre6"/>addressed in the data and training.</p>
			<h3 class="calibre11">Category – Qualified Success &gt; Response &gt; Structure</h3>
			<p class="calibre3">Qualified Success, but the response from the system or agent has a <em class="italic">structure</em> issue.</p>
			<p class="calibre3">The structure includes examples where the order of results matters. The answer could repeat steps multiple times, put something in the wrong order, or contain results that should be excluded. The structure category contains formatting issues where a table or a numbered list should have been used. This could be a content team problem and be addressed with prompt engineering or more complex use of multiple models.</p>
			</div>
	

<div><h3 class="calibre11">Category – Qualified Success &gt; Response &gt; Grammar</h3>
			<p class="calibre3">Qualified Success, but the response has a <em class="italic">grammar</em> issue.</p>
			<p class="calibre3">Any spelling, grammar, or editorial issues would qualify. Typos might come from source documents, while formatting, such as correctly using no space before a colon, might be challenging because this might come from the basic model training. If they can be reproduced, point them out in a training session with the model and tell the model these are wrong and how they should be fixed. The editorial team will likely solve this.</p>
			<h3 class="calibre11">Category – Qualified Success &gt; Response &gt; Voice and Tone</h3>
			<p class="calibre3">Qualified Success, but the response has a <em class="italic">voice and </em><em class="italic">tone</em> issue.</p>
			<p class="calibre3">This includes the correct answers, but communication should be in a more appropriate tone. Long-winded answers can be classified here. If the voice is inconsistent with the context (for example, happy when talking about a sad subject or too serious when handling something light-hearted), include it here. Linguistics and design should address this issue by prompt engineering (<a href="B21964_07.xhtml#_idTextAnchor150" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 7</em></a>, <em class="italic">Prompt Engineering</em>). Review articles to see how they might have been written. Documentation is critical and covered in <a href="B21964_06_split_000.xhtml#_idTextAnchor134" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 6</em></a>, <em class="italic">Gathering Data – Content </em><em class="italic">is King</em>.</p>
			<h3 class="calibre11">Category – Qualified Success &gt; Response &gt; Provisionally Handled</h3>
			<p class="calibre3">Qualified Success, the response was <em class="italic">provisionally handled.</em></p>
			<p class="calibre3">This is a unique section for handled answers, where a better answer is planned for the future. It allows you to track things that are coming. For example, the assistant will handle real-time weather reports in the Q3 release. In that case, temporarily linking to a <a id="_idIndexMarker107" class="pcalibre pcalibre1 calibre6"/>weather service until the weather API is available is a way of provisionally handling the use case. This is for the Product Manager to track and prioritize these issues.</p>
			<h3 class="calibre11">Category – Failure &gt; Utterance &gt; Mismatched</h3>
			<p class="calibre3">Failure: the input was <em class="italic">mismatched</em> to the<a id="_idIndexMarker108" class="pcalibre pcalibre1 calibre6"/> wrong answer.</p>
			<p class="calibre3">There is a mismatch between the question and the answer. This is where we asked one question (How fast is a jaguar?), and the AI provided an answer to the <em class="italic">wrong</em> question. Did the user mean<a id="_idIndexMarker109" class="pcalibre pcalibre1 calibre6"/> the animal or the car brand? Or it didn’t have a clear answer, giving the proper instructions but for an incomplete question. For example, it answers the question of how to reboot a phone, but without confirming the model of the phone, it gives the wrong instructions. These are critically important issues that the AI team needs to address.</p>
			<h3 class="calibre11">Category – Failure &gt; Utterance &gt; Unrecognized &gt; Current</h3>
			<p class="calibre3">Failure: the input was unrecognized, but we have a <em class="italic">current</em> response.</p>
			<p class="calibre3">It means we (humans or AIs) didn’t understand the question, but we could have if we had asked another way. Sometimes, this is due to custom language or company-specific initialisms when the model isn’t fine-tuned (explained in <a href="B21964_08.xhtml#_idTextAnchor172" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 8</em></a>, <em class="italic">Fine-Tuning</em>) with these extra details. If this is generally unrecognized, log it here; if it is specific to a custom word or phrase, use the custom category. With generative AI on the front end, this will be rare. The AI is a people pleaser, so it will try to provide an answer. But if ChatGPT is inside another tool or process, the output might cause a downstream system, response, or recommender to fail. It could be a content or AI issue.</p>
			<h3 class="calibre11">Category – Failure &gt; Utterance &gt; Unrecognized &gt; Future Development</h3>
			<p class="calibre3">Failure: the input was unrecognized; we should support this in the <em class="italic">future.</em></p>
			<p class="calibre3">These are the basics of enhancements or new features. As mentioned, the ChatGPT instance will want to answer no matter what, so your system will likely return a mismatched answer. Depending on the data, it is ok if all of these get tagged as a mismatch. If you are running a traditional chat solution or logging against a human agent answering, use this for times when the system or human can’t answer because it isn’t supported. Typically, these items are for the backlog and are addressed by the Product Manager. This backlog can be reviewed, scored, and prioritized. This is usually done on a regular cadence as part of sprint planning. We will cover a process for these items in <a href="B21964_04.xhtml#_idTextAnchor085" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 4</em></a>, <em class="italic">Scoring Stories</em>.</p>
			<h3 class="calibre11">Category – Failure &gt; Utterance &gt; Unrecognized &gt; Customer Unique – Future</h3>
			<p class="calibre3">Failure: the input was unrecognized but should be handled by <em class="italic">another team</em> in the future.</p>
			<p class="calibre3">Similar to the <a id="_idIndexMarker110" class="pcalibre pcalibre1 calibre6"/>future intent, if this is something that another team or sister organization (or where multiple ChatGPT instances are joined together, and this instance doesn’t support this intent, but another one should), log the problem here. Another team’s Product Manager should handle these new requirements.</p>
			<h3 class="calibre11">Category – Failure &gt; Utterance &gt; Unrecognized (Missed) Entity</h3>
			<p class="calibre3">Failure: the input needed to be recognized. A <em class="italic">custom</em> or company-specific word needed to be understood.</p>
			<p class="calibre3">We segment out this type of issue only because this is typically a more direct issue to fix. Fine-tuning or using <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>), which we will explain in <a href="B21964_06_split_000.xhtml#_idTextAnchor134" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 6</em></a>, <em class="italic">Gathering Data – Content is King</em> can solve this problem, but <a id="_idIndexMarker111" class="pcalibre pcalibre1 calibre6"/>first, try prompt engineering. If there is a company thesaurus, put it to good use. At Oracle, there are many initialisms and shortcuts for named products (RDBMS, Oracle DB, Oracle Server Enterprise Edition, DB12, Oracle Server, Oracle SQL Database, OraDB, and other names all mean the Oracle Database). The system should understand terms that are unique to the company and culture. This is also a good reminder in general. Human support agents will likely know much of this. An AI must also trained to learn this material. The AI team should address this training issue; the writers on the team should have the data.</p>
			<h3 class="calibre11">Category – Failure &gt; Utterance &gt; False Positive</h3>
			<p class="calibre3">Failure: <em class="italic">misunderstood</em> (a false positive).</p>
			<p class="calibre3">The answer is wrong, but someone or something thinks it is correct. This is important for human answers as well. Humans tend to be overconfident, so it is essential to recognize if their truth is the ground truth. This can lead to a better understanding of the quality of the solution. There is a fine line between this and <em class="italic">mismatched</em>. It might be pedantic, so merging the two classifications is okay. The AI team typically addresses these. We<a id="_idIndexMarker112" class="pcalibre pcalibre1 calibre6"/> include this for completeness.</p>
			<h3 class="calibre11">Category – Failure &gt; Utterance &gt; Irrelevant</h3>
			<p class="calibre3">Failure: the input was unrecognized and <em class="italic">irrelevant</em>.</p>
			<p class="calibre3">If we have customer responses during live logs, such as “Are you there?” or “Hello????”, they should be cataloged, but they are primarily irrelevant from a model or use case perspective. We see them in live agent chat logs. The agent might be handling four or five chats simultaneously; thus, delays are inevitable. A conversational ChatGPT agent won’t be distracted by the thousands of other simultaneous interactions. Garbled or garbage messages will also go here.</p>
			<h3 class="calibre11">Category – Failure &gt; Interaction</h3>
			<p class="calibre3">Failure: the failure was due to the type of user experience provided or something structural in the response.</p>
			<p class="calibre3">The answer might be the correct content, but it could require a UI component to filter the choices based on the product’s version. In this case, the channel doesn’t support a drop-down menu to make the selection. Or a button is rendered in the answer to start a process (such as uploading a file), and nothing happens when the user clicks the button. I saw this recently, and the problem was browser-specific. Solutions need to understand the context of use. You need to know that it works on a robust channel with interactive elements. Conversational AI must adapt to the channel’s limitations if the same generative solution is used on a less robust channel. The engineers prompt engineering and development work to do. Or there can be unique instructions for each channel. Instructions, a form of prompt engineering, are discussed in <a href="B21964_07.xhtml#_idTextAnchor150" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 7</em></a>, <em class="italic">Prompt Engineering</em>.</p>
			<h3 class="calibre11">Category – Failure &gt; Response &gt; Content</h3>
			<p class="calibre3">Failure: The response content is wrong or is the right content but still has inaccuracies or incorrect details.</p>
			<p class="calibre3">Sometimes, these take <a id="_idIndexMarker113" class="pcalibre pcalibre1 calibre6"/>time to determine. Did we return the correct document or agent response but lack some additional context to give the proper details? Or it could be that the document is accurate, but the facts are wrong. This differs from <em class="italic">mismatched</em>, where the document might be valid but matched to the wrong question. Here, something we wrote needs correction. The content team should update the document.</p>
			<h3 class="calibre11">Category – Failure, No Engagement</h3>
			<p class="calibre3">Failure: there was no response.</p>
			<p class="calibre3">This is more common with social and community posts and less common with conversational AI, which always tries to provide an answer. For some systems, no response could have been classified as an error, but we include it for completeness. Product Managers can decide if these should be considered.</p>
			<h3 class="calibre11">Category– Failure &gt; System Error &gt; (further classifications, if needed)</h3>
			<p class="calibre3">Failure: <em class="italic">system error</em> due to timeout, error message, infinite loop, or no response.</p>
			<p class="calibre3">Typically, this is found when working with a chat solution or integrations that might be down. Of course, we <a id="_idIndexMarker114" class="pcalibre pcalibre1 calibre6"/>don’t ever want our customers to see these. They are not common, but they are worth capturing, logging, and getting fixed when they occur. A well-designed user interface might suppress some of these errors, so scan system logs for errors. For example, if a knowledge base connection is down, and a ChatGPT solution ignores that it is down, then the answers to questions might be wrong, and it might not be evident in the log of this problem due to a system error. Monitor resources or set up alerts to notify the right administrators when a downed service is detected. Development or operations should handle these failures.</p>
			<h3 class="calibre11">Category – Unclassifiable</h3>
			<p class="calibre3">Sometimes, we don’t know what the customer was trying to do, and likely, they came and went in the conversation with so few interactions we can’t piece together enough to infer their goals. Unclassifiable <a id="_idIndexMarker115" class="pcalibre pcalibre1 calibre6"/>utterances will probably not get addressed. This will only be frequent with a new analyst. Provide support from experts who have more experience in classifying this data.</p>
			<h3 class="calibre11">Category – Ignore</h3>
			<p class="calibre3">This could be chit-chat, test data, or garbage interactions. If no other category fits, and you don’t want to deal with this, put it in this dumping ground to ignore. I found that in enterprise logs, this is very rare (less than a fraction of 1% of interaction).</p>
			<p class="calibre3">We are looking for groups<a id="_idIndexMarker116" class="pcalibre pcalibre1 calibre6"/> of classifications so that we can continue to improve the models. Some of this might go back directly into existing solutions as ERs or enhancements, but it also forms a basis for what is wanted for the new solution. If the new ChatGPT solution replaces or supplements these tools, use these benchmarks to know if future iterations are better. Aggregating these into three buckets (Success, Qualified Success, and<a id="_idIndexMarker117" class="pcalibre pcalibre1 calibre6"/> Failures) makes an excellent chart for tracking progress.</p>
			<h3 class="calibre11">Developing categories for analysis</h3>
			<p class="calibre3">In this phase, we are focused on learning what is working and what is not. We use this to understand existing content sources and input them into our conversational AI process. You might also be fixing <a id="_idIndexMarker118" class="pcalibre pcalibre1 calibre6"/>an existing system while building the generative solution. Categories might be of value for that effort. When we have a generative AI log, we can use the same process, maybe with different categories, to analyze how we are doing. By collapsing interactions into groups around a specific issue, we can determine how many customers are impacted by issues. We can see priorities for our problems (and solutions) based on our understanding of the severity. We will teach how to score these findings in <a href="B21964_04.xhtml#_idTextAnchor085" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 4</em></a>, <em class="italic">Scoring Stories</em> to create and maintain a backlog of issues.</p>
			<p class="calibre3">Once we deploy our ChatGPT solution, we can re-run this analysis on its use and use the results to drive further improvements.</p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">Use categories that make sense for the data. This is a tradeoff. The more categories, the more time each classification takes. With fewer categories, more explanations and details are needed to understand the problems.</p>
			<p class="calibre3">“<em class="italic">What gets measured gets managed</em>”—Peter Drucker (quality maven and world-famous management consultant). By creating measurements, we can communicate the quality and improve it. Improvements become increasingly complex over time. It is one thing to go from <a id="_idIndexMarker119" class="pcalibre pcalibre1 calibre6"/>67% to 97% success; it is much harder to go from 97% to 99%.</p>
			<p class="calibre3">Let’s see how we can do this in practice.</p>
			<h1 id="_idParaDest-51" class="calibre5"><a id="_idTextAnchor051" class="pcalibre pcalibre1 calibre6"/>Trying conversational analysis</h1>
			<p class="calibre3">I have placed two files into GitHub to explore. The first is a case study conversational analysis for a human resource tool. One can imagine the kinds of questions that come to HR within a<a id="_idIndexMarker120" class="pcalibre pcalibre1 calibre6"/> company. When is my check coming? How much vacation do I have left? I need to change my last name. Can I get a discount at the company store? Within the file, explore our reasoning for the classifications for hundreds of examples and see how we summarize and aggregate the data. Use this as a basis for data analysis.</p>
			<p class="calibre3">GitHub: <a href="https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter2-ConversationalLogAnalysis-2024.xlsb" class="pcalibre pcalibre1 calibre6">Analysis Log File</a> (<a href="https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter2-ConversationalLogAnalysis-2024.xlsb" class="pcalibre pcalibre1 calibre6">https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter2-ConversationalLogAnalysis-2024.xlsb</a>)</p>
			<p class="calibre3">That brings us to the second file:</p>
			<p class="calibre3">GitHub: <a href="https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter2-ConversationalLogAnalysis-2024-empty.xlsx" class="pcalibre pcalibre1 calibre6">Blank Log Analysis File</a> (<a href="https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter2-ConversationalLogAnalysis-2024-empty.xlsx" class="pcalibre pcalibre1 calibre6">https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter2-ConversationalLogAnalysis-2024-empty.xlsx</a>)</p>
			<p class="calibre3">This is the empty case study file for your data (from one of the sources we discussed earlier), so run a data analysis. <em class="italic">Using a tool that automates this process won’t give you a good feel for the user’s experience.</em> Doing some of this manual analysis forces thinking from the customer’s perspective with their experiences and an eye on improving. The file contains various spreadsheet tricks, such as pivot tables, which can be hard to maintain, analyze, and generate summaries. There are videos and documentation on these subjects online that can be used to start using pivot tables. I provide the files to show what is possible with <a id="_idIndexMarker121" class="pcalibre pcalibre1 calibre6"/>standard tools. Please use them to get started or find similar support from other tools.</p>
			<h2 id="_idParaDest-52" class="calibre9"><a id="_idTextAnchor052" class="pcalibre pcalibre1 calibre6"/>Exploring the examples from the case study</h2>
			<p class="calibre3">The examples from the GitHub<a id="_idIndexMarker122" class="pcalibre pcalibre1 calibre6"/> file help us form a common understanding of how to classify interactions. Recognize that expertise around the material being classified will go a long way to doing a good job. Someone entirely new to the classification material might only be able to annotate 80% of the responses correctly. This number can rise to 99% if you are an expert.</p>
			<p class="calibre3">I selected examples from the GitHub files in <em class="italic">Table 2.2</em> as success examples and <em class="italic">Table 2.3</em> as failure examples because they should be familiar to anyone who has worked at a large enterprise. This file is what would be expected from a conversational AI log. Each prompt will have a response. Multiple questions posed at once might not get all the answers, system errors can occur, and because of the model, some entities might not be trained, thus missing. This type of log file will expose us to most of the classifications explained. Other files, like social media posts, won’t have this variety.</p>
			<table id="table002-1" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<thead class="calibre18">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Row #*</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">User Prompt</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Failure</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Reason</strong></p>
						</td>
					</tr>
				</thead>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">514</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">I can’t find the Bullying policy which explains the steps I need to take to raise a bullying grievance (sic)</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Qualified Success &gt; Response &gt; Content</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Correct response, but additional content and direct action would have been better for this instead of just a link to the policy. This is a critical but infrequent interaction. It requires immediate attention.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">1113</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">2024 holiday</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Qualified Success &gt; Response &gt; Grammar</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">The answer was not well written and asked them to say ‘Book absence’ so it knew what to do and didn’t do it.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">1546</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">how do I change a job title on HR Cloud</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Qualified Success &gt; Response &gt; Structure</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">The user can, when allowed, change their discretionary title. There were a lot of links and options, and the structure of the response could be improved.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">1588</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Raise a Service Request</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Success</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Started the SR process.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2" colspan="4">
							<p class="calibre3">* In the spreadsheet sort by Row # to see the conversations in the correct order.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Table 2.2 – Examples and explanations of successful interactions</p>
			<p class="calibre3">Order the spreadsheet by user ID, date, and time to see a user’s complete thread. This allows an entire conversation in order without other interactions being in between. To sort the file in the original order, sort by row number.</p>
			<p class="calibre3">Log files <a id="_idIndexMarker123" class="pcalibre pcalibre1 calibre6"/>follow different formats. There is no standard out there. The critical information is what the user said, and the system responds with the date, time, and user ID. Extend this with the labeling information as we did. Notice how I build a menu of classifications in the <strong class="bold">Menu</strong> and <strong class="bold">Tips</strong> tabs of the spreadsheet. Then, when we include this column in the log sheet, we can quickly and repeatably select from the list of classifications. We also further classified the data by the intent of the user. Since those intents roughly group to different teams, we can share items related to absences, HR details, benefits, and so on with the corresponding teams.</p>
			<p class="calibre3"><em class="italic">Table 2.3</em> is just a sample of the types of prompts we see. Explore the examples in the GitHub spreadsheet for many more. This is challenging science. Even when reviewing this data, I saw one place where it was better to reclassify the utterance. The more you become an expert, the easier classification becomes. The more straightforward the classification, the better one can judge issues.</p>
			<table id="table003-1" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<thead class="calibre18">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Row #</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">User Prompt</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Failure</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Reason</strong></p>
						</td>
					</tr>
				</thead>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">141</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Balance (sic)</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Failure &gt; Interaction</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">They were trying to find their vacation balance. A typo in their entry prevented success. The wrong response was returned.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">219</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">what is the service level for actioning promotion requests</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Failure &gt; System Error &gt; No Response</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">The system did not respond.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">604</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Payrise (sic) not showing on HR Cloud</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Failure &gt; Utterance &gt; Mismatched</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Should have gone to the current pay stub details for a pay raise. Could also consider if it should compare the old and new salaries to reveal if it sees a change.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">750</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">car allowance mileage claim</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Failure &gt; Interaction</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Should have started an expense, or at least reviewed if existing claims were in progress.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">915</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">what do I do if I need a sick day</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Failure &gt; Utterance &gt; Mismatched</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">The Absence answer probably isn’t correct. They needed help to actually take sick time.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">1341</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">where do I send my P45</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Failure &gt; Utterance &gt; Mismatched</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">A P45 is a UK form showing taxes paid this year. We returned a final salary result, which is not only unlikely but disconcerting if they are still working at the company.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">2578</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">I want to promote</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Failure &gt; Response &gt; Content</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">It responded the user didn’t have permission. This is interesting because it is possible that the user is not a manager, or we need more context.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">3851</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">date of birth</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Failure &gt; Utterance &gt; Unrecognized &gt; Current</p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Only by seeing the thread do we know they actually want to view a team member’s date of birth, not edit or confirm their own. We didn’t recognize this.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Table 2.3 – Examples and explanations of failed interactions</p>
			<p class="calibre3">The spreadsheet shows more than the <strong class="bold">Log Data</strong>, <strong class="bold">Menu</strong>, and <strong class="bold">Tips</strong> tabs. Explore the other tabs that <a id="_idIndexMarker124" class="pcalibre pcalibre1 calibre6"/>summarize results. <em class="italic">Table 2.4</em> explains the other tabs:</p>
			<table id="table004" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<thead class="calibre18">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold"> </strong><strong class="bold">Tab</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Purpose</strong></p>
						</td>
					</tr>
				</thead>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Success Summary</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">Charting and summarizing the overall quality of the interactions being analyzed. Useful to compare results over time to chart improvements.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Log Data</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">This is the raw data tab. Some additional summary and analysis columns are generated. We suggest at least 1,000 interactions.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Utterance Count </strong><strong class="bold">vs Success</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">This measures the quality of the interaction based on prompt length. LLMs benefit from longer prompts. With traditional conversational AI shorter prompts are more accurate. Three to four words returned the highest success rate in this example.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Daily Users</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">This is useful to trend in longer periods of time. Stickiness is valuable. People returning and using the skills more and more. Use the daily chart to mark where events occur that trigger increased usage, such as pay day, a merger, or a product announcement.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Repeat Users</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">This is of particular interest if the same users return. This means they are getting value and continue to see value. It is dependent on logs supporting unique identifiers over time. Systems with authenticated users should always have this identifier. This can track trends in repeated usage. This metric should be going up.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Unique Users</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">This gets to marketing and growth. Is the word getting out? Do new people engage? Flag dates for announcements or email campaigns here to find correlations.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Intent Classification</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">This is very specific to the data. In the case of the example, these are the baskets of important tasks and areas the log covered. This is a valuable chart to help us decide how to invest in improvements. Use the failure rate on the intents with the most usage as important. A high failure ratio for a well-used intent is worth investigating.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">System Errors</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">This is also specific to the service. Hopefully, there are no errors. If they occur, catalog them, and get them addressed by the right teams.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Conversational </strong><strong class="bold">Buttons</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">This is specific to the data. The conversational AI had sets of buttons that might appear depending on the flow. When they were presented, did users click on them? What buttons were not used? This is helpful in this case for removing buttons that didn’t have value and for creating good labels for buttons that were of value.</p>
						</td>
					</tr>
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<p class="calibre3"><strong class="bold">Menu </strong><strong class="bold">and Tips</strong></p>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">The drop-down menu that is used in log data. Some analysis suggestions and notes are also included. The drop-down menu can be customized and reduced to make sense for the type of data being analyzed.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Table 2.4 – The tabs in the conversational log analysis spreadsheet</p>
			<p class="calibre3">We will now focus on grouping issues to manage this enormous influx of issues. In this case, we explain the <a id="_idIndexMarker125" class="pcalibre pcalibre1 calibre6"/>process assuming a conversational log. However, you might need more time or energy to generate bugs if the logs are from message boards, human agent logs, or service request threads.</p>
			<h2 id="_idParaDest-53" class="calibre9"><a id="_idTextAnchor053" class="pcalibre pcalibre1 calibre6"/>Generate enhancements and bugs from groups of issues</h2>
			<p class="calibre3">So, grouping by specific issue is<a id="_idIndexMarker126" class="pcalibre pcalibre1 calibre6"/> typically straightforward but takes some time. For example, in the case study, we tagged the user interactions by the feature and content we expect to resolve that line of questioning. Utterances such as “career break” and “sabbatical” should be grouped to form one issue to resolve. If five entries around the style and tone are too negative for a line of questioning around bank balance, then it would be better to have one issue logged with five supporting examples. We group to get a sense of volume around specific topics. This also helps because no one wants to see bugs on 1,000 interactions; maybe there are 80 issues worth reporting, and each issue is a group of 2 to 50 similar interactions.</p>
			<p class="calibre3">For example, in one log analysis, we had 19 interactions around how the system handled direct deposit (automation for paychecks to go directly into their bank account). The system supported the answer, but the model didn’t recognize the indirect words (paper check, savings, new account, bank deposit) for the direct deposit interaction. So, one tag<a id="_idIndexMarker127" class="pcalibre pcalibre1 calibre6"/> for direct deposit issues allowed us to collect all the related examples and form a corpus to train the AI.</p>
			<h2 id="_idParaDest-54" class="calibre9"><a id="_idTextAnchor054" class="pcalibre pcalibre1 calibre6"/>Score results</h2>
			<p class="calibre3">A benchmark to compare against <a id="_idIndexMarker128" class="pcalibre pcalibre1 calibre6"/>when a new solution is deployed is good. Scoring is pretty straightforward. Take the total number of interactions measured, maybe a few that can’t be classified, and sum up the successes and failures. Create a percentage from the data. Use this as a benchmark and run the same analysis on the ChatGPT solution for each significant iteration. We will explore other tools for charting your success as well. But in all cases, always measure performance. And now there is a benchmark to exceed as the ChatGPT solution matures. Score results, and comparing at every milestone.</p>
			<p class="calibre3">The intent is to share progress within the organization, showing how each assistant improves. It could compare different assistants or the same assistant on different channels. Use this information to help meet quality goals. <em class="italic">Figure 2</em><em class="italic">.7</em> shows actual data from three assistants. The <strong class="bold">Minimum Viable Product</strong> (<strong class="bold">MVP</strong>) line is <a id="_idIndexMarker129" class="pcalibre pcalibre1 calibre6"/>where to enter live customer testing, and the <strong class="bold">High-Quality</strong> bar, at 97%, is where we would be happy. Product C version 3 was in development then, hence the dotted line. Product B came late to the game, so it shows the  first and second iterations starting at release four for Product A. And iterations do not have to mean releases. If quality decreases, like in Product A, don’t release that version.</p>
			<div><div><img src="img/B21964_02_08.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Comparison of assistant scores across iterations</p>
			<h2 id="_idParaDest-55" class="calibre9"><a id="_idTextAnchor055" class="pcalibre pcalibre1 calibre6"/>Results</h2>
			<p class="calibre3">With a simple spreadsheet<a id="_idIndexMarker130" class="pcalibre pcalibre1 calibre6"/> summarizing the results, a range of quality-related questions can be answered:</p>
			<ul class="calibre7">
				<li class="calibre8">What is the success and failure rate of the current solution?</li>
				<li class="calibre8">Identify and rank the issues using the highest to lowest user needs score</li>
				<li class="calibre8">Document the number of interactions for each issue</li>
				<li class="calibre8">Chart growth in usage from external events or triggers</li>
				<li class="calibre8">Recognize and calculate stickiness – the likelihood of repeat users</li>
			</ul>
			<p class="calibre3">Identify and segment issues to identify the work to be done:</p>
			<ul class="calibre7">
				<li class="calibre8">Within the scope of the current solution (it should work)</li>
				<li class="calibre8">Missing from the current solution (it should be added)</li>
				<li class="calibre8">This is unrelated to the current solution (some other solution should solve this)</li>
				<li class="calibre8">Not relevant (gibberish, out of scope)</li>
			</ul>
			<p class="calibre3">This work will form a design vision for how the ChatGPT solution should work, goals for the kinds of answers it will need to answer, and even how to establish the right style and tone for responses. This kind of analysis offers a wealth of learning beyond the numbers and data.</p>
			<h1 id="_idParaDest-56" class="calibre5"><a id="_idTextAnchor056" class="pcalibre pcalibre1 calibre6"/>Summary</h1>
			<p class="calibre3">So, through any method, we can start to understand user needs. The survey and interview processes are related but very different. Along with log analysis, this is a good start. You might realize, as we all have to at some point, that there is more work than time. So, how does a team decide what to do first? It seems apparent to do the most important things first, but that is only sometimes the case. And there is some math behind that to help you believe it. In <a href="B21964_04.xhtml#_idTextAnchor085" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 4</em></a>, <em class="italic">Scoring Stories</em> we will explore how to prioritize research and customer needs (user needs). This method works on any list of activities, projects, tasks, upgrades, patches, or new features. I suspect that to build an effective ChatGPT solution, you will likely focus on creating or curating content to include in the model. What should you do first? This is where <strong class="bold">Users Need Scoring</strong> and the concept of <strong class="bold">Weighted Shortest Job First</strong> (<strong class="bold">WSJF</strong>), an Agile method, comes to good use. We need to take all this great information collected and put it to good use. We are focused on ChatGPT, but it can’t always provide a good solution. When we use it, and for what purposes will it come into play? We have one more stop before exploring scoring. We need to identify the optimal use cases for enterprise ChatGPT solution.</p>
			<p class="calibre3">In this chapter, we tried to set up learning about what was done and what we can learn from customers about what to do. In the following chapters, we will focus on getting it done. To put your role as a UX or product leader in perspective, I would quote Peter Drucker again: “<em class="italic">Management is doing things right; leadership is doing the right things</em>.” So, now it is time to use this research and feedback from internal and external sources to determine what use cases can be identified for our solution.</p>
			<h1 id="_idParaDest-57" class="calibre5"><a id="_idTextAnchor057" class="pcalibre pcalibre1 calibre6"/>References</h1>
			<table id="table005" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<div><div><img src="img/B21964_02_QR.jpg" alt="" role="presentation" class="calibre4"/>
								</div>
							</div>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">The links, book recommendations, and GitHub files in this chapter are posted on the reference page.</p>
							<p class="calibre3">Web Page: <a href="https://uxdforai.com/references#C2" class="pcalibre pcalibre1 calibre6">Chapter 2 References</a> (<a href="https://uxdforai.com/references#C2" class="pcalibre pcalibre1 calibre6">https://uxdforai.com/references#C2</a>).</p>
						</td>
					</tr>
				</tbody>
			</table>
		</div>
	</body></html>