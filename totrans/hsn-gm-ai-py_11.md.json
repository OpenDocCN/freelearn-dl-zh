["```py\npip3 install -U 'mujoco-py<2.1,>=2.0'  #use the version appropriate for you\ncd path/to/mujoco-py/folder\npython -c \"import mujoco_py\" #force compile mujoco_py\npython setup.py install \n```", "```py\npip install gym[all]\n```", "```py\nimport gym \nfrom gym import envs\n\nenv = gym.make('FetchReach-v1')\n\nenv.reset()\nfor _ in range(1000):\n env.render()\n env.step(env.action_space.sample()) # take a random action\nenv.close()\n```", "```py\nfor iteration in range(iterations):\n s = env.reset()\n done = False\n while not done:\n   for t in range(T_horizon):\n     prob = model.pi(torch.from_numpy(s).float()) \n     m = Categorical(prob)\n     a = m.sample().item()\n     s_prime, r, done, info = env.step(a)\n\n     model.put_data((s, a, r/100.0, s_prime, prob[a].item(),done))\n     s = s_prime\n\n     score += r\n     if done:\n       if score/print_interval > min_play_reward:\n         play_game()\n       break\n\n   model.train_net()\n if iteration%print_interval==0 and iteration!=0:\n   print(\"# of episode :{}, avg score : {:.1f}\".format(iteration,\n     score/print_interval))\n   score = 0.0\n\nenv.close()\n```", "```py\nlearning_rate = 0.0005\ngamma = 0.98\nlmbda = 0.95\neps_clip = 0.1\nK_epoch = 3\nT_horizon = 20\n```", "```py\ndef __init__(self, input_shape, num_actions):\n super(PPO, self).__init__()\n self.data = []\n\n self.fc1 = nn.Linear(input_shape,256)\n self.fc_pi = nn.Linear(256,num_actions)\n self.fc_v = nn.Linear(256,1)\n self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n```", "```py\ndef train_net(self):\n s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n\nfor i in range(K_epoch):\n td_target = r + gamma * self.v(s_prime) * done_mask\n delta = td_target - self.v(s)\n delta = delta.detach().numpy()\n\n advantage_lst = []\n advantage = 0.0\n for delta_t in delta[::-1]:\n   advantage = gamma * lmbda * advantage + delta_t[0]\n   advantage_lst.append([advantage])\n advantage_lst.reverse()\n advantage = torch.tensor(advantage_lst, dtype=torch.float)\n\n pi = self.pi(s, softmax_dim=1)\n pi_a = pi.gather(1,a)\n ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a)) \n\n surr1 = ratio * advantage\n surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , \n   td_target.detach())\n\n self.optimizer.zero_grad()\n loss.mean().backward()\n self.optimizer.step()\n```", "```py\ntd_target = r + gamma * self.v(s_prime) * done_mask\ndelta = td_target - self.v(s)\ndelta = delta.detach().numpy()\n```", "```py\nfor delta_t in delta[::-1]:\n  advantage = gamma * lmbda * advantage + delta_t[0]\n  advantage_lst.append([advantage])\n```", "```py\npi = self.pi(s, softmax_dim=1)\npi_a = pi.gather(1,a)\nratio = torch.exp(torch.log(pi_a) - torch.log(prob_a)) \n```", "```py\nsurr1 = ratio * advantage\nsurr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\nloss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , \n   td_target.detach())\n```", "```py\nself.optimizer.zero_grad()\nloss.mean().backward()\nself.optimizer.step()\n```", "```py\nclass PPO(nn.Module):\n    def __init__(self, input_shape, num_actions):\n        super(PPO, self).__init__()\n        self.data = []\n\n        self.fc1 = nn.Linear(input_shape,64)\n        self.lstm = nn.LSTM(64,32)\n        self.fc_pi = nn.Linear(32,num_actions)\n        self.fc_v = nn.Linear(32,1)\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n```", "```py\ndef pi(self, x, hidden):\n        x = F.relu(self.fc1(x))\n        x = x.view(-1, 1, 64)\n        x, lstm_hidden = self.lstm(x, hidden)\n        x = self.fc_pi(x)\n        prob = F.softmax(x, dim=2)\n        return prob, lstm_hidden\n\ndef v(self, x, hidden):\n        x = F.relu(self.fc1(x))\n        x = x.view(-1, 1, 64)\n        x, lstm_hidden = self.lstm(x, hidden)\n        v = self.fc_v(x)\n        return v\n```", "```py\ndef train_net(self):\n        s,a,r,s_prime,done_mask, prob_a, (h1_in, h2_in), (h1_out, h2_out) = self.make_batch()\n        first_hidden = (h1_in.detach(), h2_in.detach())\n        second_hidden = (h1_out.detach(), h2_out.detach())\n```", "```py\nv_prime = self.v(s_prime, second_hidden).squeeze(1)\ntd_target = r + gamma * v_prime * done_mask\nv_s = self.v(s, first_hidden).squeeze(1)\ndelta = td_target - v_s\ndelta = delta.detach().numpy()\n```", "```py\nn_train_processes = 3\nlearning_rate = 0.0002\nupdate_interval = 5\ngamma = 0.98\nmax_train_steps = 60000\nPRINT_INTERVAL = update_interval * 100\nenvironment = \"LunarLander-v2\"\n```", "```py\nclass ActorCritic(nn.Module):\n    def __init__(self, input_shape, num_actions):\n        super(ActorCritic, self).__init__()\n        self.fc1 = nn.Linear(input_shape, 256)\n        self.fc_pi = nn.Linear(256, num_actions)\n        self.fc_v = nn.Linear(256, 1)\n\n    def pi(self, x, softmax_dim=1):\n        x = F.relu(self.fc1(x))\n        x = self.fc_pi(x)\n        prob = F.softmax(x, dim=softmax_dim)\n        return prob\n\n    def v(self, x):\n        x = F.relu(self.fc1(x))\n        v = self.fc_v(x)\n        return v\n```", "```py\ndef worker(worker_id, master_end, worker_end):\n    master_end.close() \n    env = gym.make(environment)\n    env.seed(worker_id)\n\n    while True:\n        cmd, data = worker_end.recv()\n        if cmd == 'step':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            worker_end.send((ob, reward, done, info))\n        elif cmd == 'reset':\n            ob = env.reset()\n            worker_end.send(ob)\n        elif cmd == 'reset_task':\n            ob = env.reset_task()\n            worker_end.send(ob)\n        elif cmd == 'close':\n            worker_end.close()\n            break\n        elif cmd == 'get_spaces':\n            worker_end.send((env.observation_space, env.action_space))\n        else:\n            raise NotImplementedError\n```", "```py\nclass ParallelEnv:\n    def __init__(self, n_train_processes):\n        self.nenvs = n_train_processes\n        self.waiting = False\n        self.closed = False\n        self.workers = list()\n\n        master_ends, worker_ends = zip(*[mp.Pipe() for _ in range(self.nenvs)])\n        self.master_ends, self.worker_ends = master_ends, worker_ends\n\n        for worker_id, (master_end, worker_end) in enumerate(zip(master_ends, worker_ends)):\n          p = mp.Process(target=worker,\n                           args=(worker_id, master_end, worker_end))\n            p.daemon = True\n            p.start()\n            self.workers.append(p)\n\n        # Forbid master to use the worker end for messaging\n        for worker_end in worker_ends:\n            worker_end.close()\n```", "```py\ndef compute_target(v_final, r_lst, mask_lst):\n    G = v_final.reshape(-1)\n    td_target = list()\n\n    for r, mask in zip(r_lst[::-1], mask_lst[::-1]):\n        G = r + gamma * G * mask\n        td_target.append(G)\n\n    return torch.tensor(td_target[::-1]).float()\n```", "```py\nif __name__ == '__main__':\n    envs = ParallelEnv(n_train_processes)\n    env = gym.make(environment)\n    state_size = env.observation_space.shape[0]\n    action_size = env.action_space.n\n    model = ActorCritic(state_size, action_size)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n```", "```py\nfor _ in range(update_interval):\n    prob = model.pi(torch.from_numpy(s).float())\n    a = Categorical(prob).sample().numpy()\n    s_prime, r, done, info = envs.step(a)\n\n    s_lst.append(s)\n    a_lst.append(a)\n    r_lst.append(r/100.0)\n    mask_lst.append(1 - done)\n\n    s = s_prime\n    step_idx += 1\n```", "```py\ns_final = torch.from_numpy(s_prime).float()\nv_final = model.v(s_final).detach().clone().numpy()\ntd_target = compute_target(v_final, r_lst, mask_lst)\n\ntd_target_vec = td_target.reshape(-1)\ns_vec = torch.tensor(s_lst).float().reshape(-1, state_size) \na_vec = torch.tensor(a_lst).reshape(-1).unsqueeze(1)\nmod = model.v(s_vec)\nadvantage = td_target_vec - mod.reshape(-1)\n\npi = model.pi(s_vec, softmax_dim=1)\npi_a = pi.gather(1, a_vec).reshape(-1)\nloss = -(torch.log(pi_a) * advantage.detach()).mean() +\\\n            F.smooth_l1_loss(model.v(s_vec).reshape(-1), td_target_vec)\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n```", "```py\nn_train_processes = 6\nlearning_rate = 0.0002\nupdate_interval = 6\ngamma = 0.98\nmax_train_ep = 3000\nmax_test_ep = 400\nenvironment = \"LunarLander-v2\"\n\nenv = gym.make(environment)\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\n```", "```py\ndef train(global_model, rank):\n    local_model = ActorCritic(state_size, action_size)\n    local_model.load_state_dict(global_model.state_dict())\n\n    optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)\n\n    env = gym.make(environment)\n\n    for n_epi in range(max_train_ep):\n        done = False\n        s = env.reset()\n        while not done:\n            s_lst, a_lst, r_lst = [], [], []\n            for t in range(update_interval):\n                prob = local_model.pi(torch.from_numpy(s).float())\n                m = Categorical(prob)\n                a = m.sample().item()\n                s_prime, r, done, info = env.step(a)\n\n                s_lst.append(s)\n                a_lst.append([a])\n                r_lst.append(r/100.0)\n\n                s = s_prime\n                if done:\n                    break\n\n            s_final = torch.tensor(s_prime, dtype=torch.float)\n            R = 0.0 if done else local_model.v(s_final).item()\n            td_target_lst = []\n            for reward in r_lst[::-1]:\n                R = gamma * R + reward\n                td_target_lst.append([R])\n            td_target_lst.reverse()\n\n            s_batch, a_batch, td_target = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n                torch.tensor(td_target_lst)\n            advantage = td_target - local_model.v(s_batch)\n\n            pi = local_model.pi(s_batch, softmax_dim=1)\n            pi_a = pi.gather(1, a_batch)\n            loss = -torch.log(pi_a) * advantage.detach() + \\\n                F.smooth_l1_loss(local_model.v(s_batch), td_target.detach())\n\n            optimizer.zero_grad()\n            loss.mean().backward()\n            for global_param, local_param in zip(global_model.parameters(), local_model.parameters()):\n                global_param._grad = local_param.grad\n            optimizer.step()\n            local_model.load_state_dict(global_model.state_dict())\n\n    env.close()\n    print(\"Training process {} reached maximum episode.\".format(rank))\n```", "```py\ndef test(global_model):\n    env = gym.make(environment)\n    score = 0.0\n    print_interval = 20\n\n    for n_epi in range(max_test_ep):\n        done = False\n        s = env.reset()\n        while not done:\n            prob = global_model.pi(torch.from_numpy(s).float())\n            a = Categorical(prob).sample().item()\n            s_prime, r, done, info = env.step(a)\n            s = s_prime\n            score += r\n\n        if n_epi % print_interval == 0 and n_epi != 0:\n            print(\"# of episode :{}, avg score : {:.1f}\".format(\n                n_epi, score/print_interval))\n            score = 0.0\n            time.sleep(1)\n    env.close()\n```", "```py\nif __name__ == '__main__': \n    global_model = ActorCritic(state_size, action_size)\n    global_model.share_memory()\n\n    processes = []\n    for rank in range(n_train_processes + 1): # + 1 for test process\n        if rank == 0:\n            p = mp.Process(target=test, args=(global_model,))\n        else:\n            p = mp.Process(target=train, args=(global_model, rank,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n```", "```py\nclass ReplayBuffer():\n    def __init__(self):\n        self.buffer = collections.deque(maxlen=buffer_limit)\n\n    def put(self, seq_data):\n        self.buffer.append(seq_data)\n\n    def sample(self, on_policy=False):\n        if on_policy:\n            mini_batch = [self.buffer[-1]]\n        else:\n            mini_batch = random.sample(self.buffer, batch_size)\n\n        s_lst, a_lst, r_lst, prob_lst, done_lst, is_first_lst = [], [], [], [], [], []\n        for seq in mini_batch:\n            is_first = True \n            for transition in seq:\n                s, a, r, prob, done = transition\n\n                s_lst.append(s)\n                a_lst.append([a])\n                r_lst.append(r)\n                prob_lst.append(prob)\n                done_mask = 0.0 if done else 1.0\n                done_lst.append(done_mask)\n                is_first_lst.append(is_first)\n                is_first = False\n\n        s,a,r,prob,done_mask,is_first = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n                                        r_lst, torch.tensor(prob_lst, dtype=torch.float), done_lst, \\\n                                        is_first_lst\n        return s,a,r,prob,done_mask,is_first\n\n    def size(self):\n        return len(self.buffer)\n```", "```py\nq = model.q(s)\nq_a = q.gather(1,a)\npi = model.pi(s, softmax_dim = 1)\npi_a = pi.gather(1,a)\nv = (q * pi).sum(1).unsqueeze(1).detach()\n\nrho = pi.detach()/prob\nrho_a = rho.gather(1,a)\nrho_bar = rho_a.clamp(max=c)\ncorrection_coeff = (1-c/rho).clamp(min=0)\n```", "```py\nloss1 = -rho_bar * torch.log(pi_a) * (q_ret - v) \nloss2 = -correction_coeff * pi.detach() * torch.log(pi) * (q.detach()-v) loss = loss1 + loss2.sum(1) + F.smooth_l1_loss(q_a, q_ret)\n```", "```py\nseq_data.append((s, a, r/100.0, prob.detach().numpy(), done))\n```"]