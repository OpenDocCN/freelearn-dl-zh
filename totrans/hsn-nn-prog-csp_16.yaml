- en: Time Series Prediction and LSTM Using CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is dedicated to helping you understand more of the Microsoft Cognitive
    Toolkit, or CNTK. The inspiration for the examples contained within this chapter
    comes from the Python version of **CNTK 106**: **Part A – Time Series prediction
    with LSTM (Basics)**. As C# developers, the Python code is not what we will be
    using (although there are several ways in which we could) so we made our own C#
    example to mirror that tutorial. To make our example easy and intuitive, we will
    use the Sine function to predict future time-series data. Specifically, and more
    concretely, we will be using a **long short-term memory recurrent neural network**,
    sometimes called an **LSTM-RNN** or just **LSTM**. There are many variants of
    the LSTM; we will be working with the original.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static and dynamic axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating mini-batches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be required to have basic knowledge of .NET development using Microsoft
    Visual Studio and C#. You will need to download the code for this chapter from
    the book's website.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see Code in Action: [http://bit.ly/2xtDTto](http://bit.ly/2xtDTto).
  prefs: []
  type: TYPE_NORMAL
- en: Long short-term memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Long short-term memory** (**LSTM**) networks are a specialized form of recurrent
    neural network. They have the ability to retain long-term memory of things they
    have encountered in the past. In an LSTM, each neuron is replaced by what is known
    as a **memory unit**. This memory unit is activated and deactivated at the appropriate
    time, and is actually what is known as a **recurrent self-connection**.'
  prefs: []
  type: TYPE_NORMAL
- en: If we step back for a second and look at the back-propagation phase of a regular
    recurrent network, the gradient signal can end up being multiplied many times
    by the weight matrix of the synapses between the neurons within the hidden layer.
    What does this mean exactly? Well, it means that the magnitude of those weights
    can then have a stronger impact on the learning process. This can be both good
    and bad.
  prefs: []
  type: TYPE_NORMAL
- en: If the weights are small they can lead to what is known as **vanishing gradients**,
    a scenario where the signal gets so small that learning slows to an unbearable
    pace or even, worse, comes to a complete stop. On the other hand, if the weights
    are large, this can lead to a situation where the signal is so large that learning
    diverges rather than converges. Both scenarios are undesirable but are handled
    by an item within the LSTM model known as a **memory cell**. Let's talk a little
    about this memory cell now.
  prefs: []
  type: TYPE_NORMAL
- en: 'A memory cell has four different parts. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: Input gate, with a constant weight of 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-recurrent connection neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forget gate, allowing cells to remember or forget its previous state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output gate, allowing the memory cell state to have an effect (or no effect)
    on other neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at this and try and make it all come together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d17878d-0f64-4358-b7b9-562bd00030b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Memory Cell
  prefs: []
  type: TYPE_NORMAL
- en: LSTM variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many variants of the LSTM network. Some of these variants include:'
  prefs: []
  type: TYPE_NORMAL
- en: Gated recurrent neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM4a
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM5a
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSMT6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d0a9a772-368f-483e-8d30-60b111bedc35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Training and Test accuracy, σ = relu, η = 1e −4
  prefs: []
  type: TYPE_NORMAL
- en: One of those variants, a slightly more dramatic version of the LSTM, is called
    the gated recurrent unit, or GRU/GRNN. It combines the forget and input gates
    of the LSTM into a single gate called an **update gate**. This makes it simpler
    than the standard LSTM and has been increasingly growing in popularity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what a LSTM looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91bf90ea-2e9b-4f36-92b3-bddbc00a713f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: LSTM
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are various memory *gates* in the LSTM that the RNN does
    not have. This allows it to effortlessly retain both long-and short-term memory.
    So, if we want to understand text and need to look ahead or behind in time, LSTM
    is made for just such a scenario. Let’s talk about the different gates for a moment.
    As we mentioned, there are 3 of them. Let’s use the following phrase to explain
    how each of these works.
  prefs: []
  type: TYPE_NORMAL
- en: '*Bob lives in New York City. John talks to people on the phone all day, and
    commutes on the train.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forget Gate**: As soon as we get to the period after the word City, the forget
    gate realizes that there may be a change of context in the works. As a result,
    the subject Bob is forgotten and the place where the subject was is now empty.
    As soon as the sentence turns to John, the subject is now John. This process is
    caused by the forget gate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Gate**: So the important facts are that Bob lives in New York City,
    and that John commutes on the train and talks to people all day. However, the
    fact that he talks to people over the phone is not as important and can be ignored.
    The process of adding new information is done via the input gate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output Gate**: If we were to have a sentence *Bob was a great man. We salute
    ____*. In this sentence we have an empty space with many possibilities. What we
    do know is that we are going to salute whatever is in this empty space, and this
    is a verb describing a noun. Therefore, we would be safe in assuming that the
    empty space will be filled with a noun. So, a good candidate could be *Bob*. The
    job of selecting what information is useful from the current cell state and showing
    it as an output is the job of the output gate.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications of LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are just a few of the applications of LSTM stacked networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwriting recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series prediction and anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business process management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robotic control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And many more...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series prediction itself can have a dramatic effect on the business's bottom
    line. We may need to predict in which day of the month, which quarter, or which
    year certain large expenses will occur. We may also have concerns over the Consumer
    Price Index over the course of time relative to our business. Increased prediction
    accuracy can lead to definite improvements to our bottom line.
  prefs: []
  type: TYPE_NORMAL
- en: CNTK terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important that we understand some of the terminology used within the
    Microsoft CNTK toolkit. Let’s look at some of that terminology now:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensors**: All CNTK inputs, outputs and parameters are organized as tensors.
    It should also be noted that *minibatches* are tensors as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rank**: Each tensor has a rank. Scalars are tensors with a rank of 0, vectors
    are tensors and have a rank of 1, and matrices are tensors with a rank of 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Static axis**: The dimensions listed in 2 are referred to as **axes**. Every
    tensor has *static* and *dynamic* axes. A Static axis has the same length throughout
    its entire life.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic axis**: Dynamic axes, however, can vary their length from instance
    to instance. Their length is typically not known before each minibatch is presented.
    Additionally, they may be ordered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minibatch**: A minibatch is also a tensor. It has a dynamic axis, which is
    called the **batch axis**. The length of this axis can change from minibatch to
    minibatch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the time of writing of CNTK supports one additional single additional dynamic
    axis, also known as the **sequence axis**. This axis allows the user to work with
    sequences in a more abstract, higher-level manner. The beauty of sequence is that
    whenever an operation is performed on a sequence, the CNTK toolkit does a type-checking
    operation to determine safety:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fd1d49a-0dba-4156-8d7b-1b9a7a32a281.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sequence Axis
  prefs: []
  type: TYPE_NORMAL
- en: Our example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So now that we have covered some basics for both CNTK and LSTM, it’s time to
    dive into our example application. You can find this project with the code that
    accompanies this book. Make sure you have it open in Microsoft Visual Studio before
    proceeding. You can follow the instructions in the upcoming *The Code* section
    if you need further instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The example we are creating uses Microsoft CNTK as a back-end and will use a
    simple sine wave as our function. The sine wave was plotted earlier and is used
    due to it’s being widely familiar to most individuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are screenshots of what our example application looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fce45926-766b-46ad-8232-31c95178763f.png)'
  prefs: []
  type: TYPE_IMG
- en: Main page – training data
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot shows our main screen, displaying our sine wave data
    points, our training data. The goal is to get our training data (blue) to match
    the red as closely as possible in shape, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5766a404-4601-41f8-99f9-1f060fd0edd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Main Page - Charts
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screen allows us to plot our loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d613e562-4775-485c-964a-8e6dca6215ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting the loss value (Loss Value tab)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screen allows us to plot our observed versus predicted values.
    The goal is to have the predicted values (blue) match as closely as possible the
    actual values (shown in red):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd00e35b-8e22-4eda-b904-39fca2b0d31e.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting Observed versus Predicted (Test Data tab)
  prefs: []
  type: TYPE_NORMAL
- en: Coding our application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now look at our code. For this you will need to reference the `LSTMTimeSeriesDemo`
    project that accompanies this book. Open the project in Microsoft Visual Studio.
    All the required CNTK libraries are already referenced in this project for you
    and included in the Debug/bin directory. The main library we will be using is
    `Cntk.Core.Managed`. We are using version 2.5.1 for this example, in case you
    were wondering!
  prefs: []
  type: TYPE_NORMAL
- en: Loading data and graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To load our data and graphs, we have two main functions that we will
  prefs: []
  type: TYPE_NORMAL
- en: 'use; `LoadTrainingData()`and `PopulateGraphs()`. Pretty straightforward, right?:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Loading training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this example, we are simply making up our test and training data on-the-fly.
    The `LoadTrainingData()` function does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Populating the graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function populates the graphs with training and test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Splitting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With this function we mirror frameworks such as Python, which make it very
    easy to split training and testing data from the main dataset. We have created
    our own function to do the same thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Running the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we click the **Run** button, we will execute the function outlined as
    follows. We first determine the number of iterations the user wants to use, as
    well as the batch size. After setting up our progress bar and some internal variables,
    we call our `TrainNetwork()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Training the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In every neural network we must train the network in order for it to recognize
    whatever we are providing it. Our `TrainNetwork()` function does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the trainer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the next minibatch amount of data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Train, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Creating a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a model, we are going to build a one-directional recurrent neural
    network that contains **long short-term memory** (**LSTM**) cells as shown in
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a LSTM cell for each input variable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After the LSTM sequence is created, return the last cell in order to continue
    generating the network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement dropout for 20%, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the last dense layer before the output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Getting the next data batch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We get our next batch of data in an enumerable fashion. We first verify parameters,
    and then call the `CreateBatch()` function, which is listed after this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Creating a batch of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a dataset, this function will create *batches* of data for use in traversing
    the total data set in more manageable segments. You can see how it is called from
    the `GetNextDataBatch()` function shown previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How well do LSTMs perform?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We tested LSTMs against predictions and historical values for sunspot data
    prediction, a very famous test in deep learning. As you can see, the red plot,
    which is our predictions, melded into the trend exactly, which is a tremendously
    encouraging sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e010f68f-0234-4e2f-b147-9b1904874f80.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Prediction versus Performance
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about long short-term memory recurrent neural networks.
    We programmed an example application showing how to use them, and we learned some
    basic terminology along the way. We covered topics such as LSTMs, tensors, static
    and dynamic axes, loading datasets, plotting data, creating models, and creating
    minibatches. In our next chapter, we will visit a very close cousin of LSTM networks:
    gated recurrent units.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation,
    9(8), 1735-1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gers, F. A., Schmidhuber, J., & Cummins, F. (2000). Learning to forget: Continual
    prediction with LSTM. Neural computation, 12(10), 2451-2471.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves, Alex. Supervised sequence labeling with recurrent neural networks. Vol.
    385\. Springer, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with
    gradient descent is difficult. IEE TRANSACTIONS ON NEURAL NETWORKS, 5, 1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F. Chollet. Keras github.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated
    recurrent neural networks on sequence modeling, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation,
    9:1735–1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q. V. Le, N. Jaitly, and H. G. E. A simple way to initialize recurrent networks
    of rectified linear units. 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Lu and F. Salem. Simplified gating in long short-term memory (lstm) recurrent
    neural networks. arXiv:1701.03441, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F. M. Salem. A basic recurrent neural network model. arXiv preprint arXiv:1612.09022,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F. M. Salem. Reduced parameterization of gated recurrent neural networks. MSU
    Memorandum, 7.11.2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
