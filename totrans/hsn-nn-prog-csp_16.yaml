- en: Time Series Prediction and LSTM Using CNTK
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNTK进行时间序列预测和LSTM
- en: 'This chapter is dedicated to helping you understand more of the Microsoft Cognitive
    Toolkit, or CNTK. The inspiration for the examples contained within this chapter
    comes from the Python version of **CNTK 106**: **Part A – Time Series prediction
    with LSTM (Basics)**. As C# developers, the Python code is not what we will be
    using (although there are several ways in which we could) so we made our own C#
    example to mirror that tutorial. To make our example easy and intuitive, we will
    use the Sine function to predict future time-series data. Specifically, and more
    concretely, we will be using a **long short-term memory recurrent neural network**,
    sometimes called an **LSTM-RNN** or just **LSTM**. There are many variants of
    the LSTM; we will be working with the original.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章致力于帮助你更好地了解Microsoft认知工具包，或CNTK。本章中包含的示例灵感来源于CNTK 106的Python版本：**A部分 – 使用LSTM进行时间序列预测（基础）**。作为C#开发者，我们不会使用Python代码（尽管有几种方法可以实现这一点），因此我们制作了自己的C#示例来模仿那个教程。为了使我们的示例简单直观，我们将使用正弦函数来预测未来的时间序列数据。具体来说，我们将使用**长短期记忆循环神经网络**，有时称为**LSTM-RNN**或简称为**LSTM**。LSTM有很多变体；我们将使用原始版本。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: LSTM
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM
- en: Tensors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量
- en: Static and dynamic axis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态和动态轴
- en: Loading datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Plotting data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制数据
- en: Creating models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建模型
- en: Creating mini-batches
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建小批量
- en: And more…
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及更多...
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be required to have basic knowledge of .NET development using Microsoft
    Visual Studio and C#. You will need to download the code for this chapter from
    the book's website.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备使用Microsoft Visual Studio和C#进行.NET开发的基本知识。你需要从本书的网站上下载本章的代码。
- en: Check out the following video to see Code in Action: [http://bit.ly/2xtDTto](http://bit.ly/2xtDTto).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际应用：[http://bit.ly/2xtDTto](http://bit.ly/2xtDTto)。
- en: Long short-term memory
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: '**Long short-term memory** (**LSTM**) networks are a specialized form of recurrent
    neural network. They have the ability to retain long-term memory of things they
    have encountered in the past. In an LSTM, each neuron is replaced by what is known
    as a **memory unit**. This memory unit is activated and deactivated at the appropriate
    time, and is actually what is known as a **recurrent self-connection**.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）网络是一种特殊的循环神经网络。它们能够保留它们过去遇到的事情的长期记忆。在LSTM中，每个神经元都被称为**记忆单元**的东西所取代。这个记忆单元在适当的时候被激活和去激活，实际上就是所谓的**循环自连接**。'
- en: If we step back for a second and look at the back-propagation phase of a regular
    recurrent network, the gradient signal can end up being multiplied many times
    by the weight matrix of the synapses between the neurons within the hidden layer.
    What does this mean exactly? Well, it means that the magnitude of those weights
    can then have a stronger impact on the learning process. This can be both good
    and bad.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们退一步，看看常规循环网络的反向传播阶段，梯度信号可能会被神经元之间隐藏层中突触的权重矩阵多次相乘。这究竟意味着什么呢？嗯，这意味着这些权重的幅度可以对学习过程产生更强的影响。这既有好的一面，也有不好的一面。
- en: If the weights are small they can lead to what is known as **vanishing gradients**,
    a scenario where the signal gets so small that learning slows to an unbearable
    pace or even, worse, comes to a complete stop. On the other hand, if the weights
    are large, this can lead to a situation where the signal is so large that learning
    diverges rather than converges. Both scenarios are undesirable but are handled
    by an item within the LSTM model known as a **memory cell**. Let's talk a little
    about this memory cell now.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果权重很小，可能会导致所谓的**梯度消失**，在这种情况下，信号变得非常小，以至于学习速度减慢到难以忍受，甚至更糟，完全停止。另一方面，如果权重很大，这可能会导致信号变得非常大，导致学习发散而不是收敛。这两种情况都是不希望的，但可以通过LSTM模型中的一个项目来处理，即**记忆单元**。现在让我们来谈谈这个记忆单元。
- en: 'A memory cell has four different parts. They are:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个记忆单元有四个不同的部分。它们是：
- en: Input gate, with a constant weight of 1.0
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门，具有恒定的权重1.0
- en: Self-recurrent connection neuron
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自循环连接神经元
- en: Forget gate, allowing cells to remember or forget its previous state
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗忘门，允许细胞记住或忘记其先前状态
- en: Output gate, allowing the memory cell state to have an effect (or no effect)
    on other neurons
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门，允许记忆单元状态对其他神经元产生影响（或没有影响）
- en: 'Let’s take a look at this and try and make it all come together:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个，并尝试将其全部整合起来：
- en: '![](img/6d17878d-0f64-4358-b7b9-562bd00030b9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d17878d-0f64-4358-b7b9-562bd00030b9.png)'
- en: Memory Cell
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 存储单元
- en: LSTM variants
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM变体
- en: 'There are many variants of the LSTM network. Some of these variants include:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络有许多变体。其中一些变体包括：
- en: Gated recurrent neural network
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控循环神经网络
- en: LSTM4
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM4
- en: LSTM4a
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM4a
- en: LSTM5
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM5
- en: LSTM5a
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM5a
- en: LSMT6
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSMT6
- en: '![](img/d0a9a772-368f-483e-8d30-60b111bedc35.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0a9a772-368f-483e-8d30-60b111bedc35.jpg)'
- en: Training and Test accuracy, σ = relu, η = 1e −4
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试准确度，σ = relu，η = 1e −4
- en: One of those variants, a slightly more dramatic version of the LSTM, is called
    the gated recurrent unit, or GRU/GRNN. It combines the forget and input gates
    of the LSTM into a single gate called an **update gate**. This makes it simpler
    than the standard LSTM and has been increasingly growing in popularity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变体中的一种，LSTM的一个稍微更戏剧化的版本，被称为门控循环单元，或GRU/GRNN。它将LSTM的忘记门和输入门合并成一个称为**更新门**的单个门。这使得它比标准的LSTM更简单，并且越来越受欢迎。
- en: 'Here is what a LSTM looks like:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个LSTM的样子：
- en: '![](img/91bf90ea-2e9b-4f36-92b3-bddbc00a713f.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91bf90ea-2e9b-4f36-92b3-bddbc00a713f.jpg)'
- en: LSTM
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM
- en: As you can see, there are various memory *gates* in the LSTM that the RNN does
    not have. This allows it to effortlessly retain both long-and short-term memory.
    So, if we want to understand text and need to look ahead or behind in time, LSTM
    is made for just such a scenario. Let’s talk about the different gates for a moment.
    As we mentioned, there are 3 of them. Let’s use the following phrase to explain
    how each of these works.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，LSTM中有各种记忆*门*，而RNN没有。这使得它能够轻松地保留长期和短期记忆。因此，如果我们想要理解文本并需要向前或向后查看时间，LSTM正是为此而生的场景。让我们暂时谈谈不同的门。正如我们提到的，有三个。让我们用以下短语来解释每个是如何工作的。
- en: '*Bob lives in New York City. John talks to people on the phone all day, and
    commutes on the train.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bob住在纽约市。John整天打电话与人交谈，并乘坐火车通勤。*'
- en: '**Forget Gate**: As soon as we get to the period after the word City, the forget
    gate realizes that there may be a change of context in the works. As a result,
    the subject Bob is forgotten and the place where the subject was is now empty.
    As soon as the sentence turns to John, the subject is now John. This process is
    caused by the forget gate.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**忘记门**：当我们到达单词City之后的时期，忘记门意识到可能存在上下文的变化。结果，主题Bob被遗忘，主题所在的位置现在为空。当句子转向John时，主题现在是John。这个过程是由忘记门引起的。'
- en: '**Input Gate**: So the important facts are that Bob lives in New York City,
    and that John commutes on the train and talks to people all day. However, the
    fact that he talks to people over the phone is not as important and can be ignored.
    The process of adding new information is done via the input gate.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入门**：所以重要的信息是Bob住在纽约市，John整天乘坐火车并与人们交谈。然而，他通过电话与人交谈的事实并不那么重要，可以忽略。添加新信息的过程是通过输入门完成的。'
- en: '**Output Gate**: If we were to have a sentence *Bob was a great man. We salute
    ____*. In this sentence we have an empty space with many possibilities. What we
    do know is that we are going to salute whatever is in this empty space, and this
    is a verb describing a noun. Therefore, we would be safe in assuming that the
    empty space will be filled with a noun. So, a good candidate could be *Bob*. The
    job of selecting what information is useful from the current cell state and showing
    it as an output is the job of the output gate.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出门**：如果我们有一个句子 *Bob是一位伟大的人。我们向他致敬____*。在这个句子中，我们有一个空格，有很多可能性。我们知道的是，我们将向这个空格中的任何东西致敬，这是一个描述名词的动词。因此，我们可以安全地假设这个空格将被名词填充。所以，一个好的候选者可以是
    *Bob*。选择从当前细胞状态中提取哪些信息有用并将其显示为输出的工作是由输出门完成的。'
- en: Applications of LSTM
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM的应用
- en: 'The following are just a few of the applications of LSTM stacked networks:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下只是LSTM堆叠网络的几个应用示例：
- en: Speech recognition
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: Handwriting recognition
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写识别
- en: Time series prediction and anomaly detection
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列预测和异常检测
- en: Business process management
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务流程管理
- en: Robotic control
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人控制
- en: And many more...
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及更多...
- en: Time series prediction itself can have a dramatic effect on the business's bottom
    line. We may need to predict in which day of the month, which quarter, or which
    year certain large expenses will occur. We may also have concerns over the Consumer
    Price Index over the course of time relative to our business. Increased prediction
    accuracy can lead to definite improvements to our bottom line.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测本身可以对企业的底线产生重大影响。我们可能需要预测某些大额支出将在哪一天、哪个季度或哪一年发生。我们可能还会对我们的业务相对于时间序列的消费者价格指数表示关注。预测精度的提高可以肯定地改善我们的底线。
- en: CNTK terminology
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNTK术语
- en: 'It is important that we understand some of the terminology used within the
    Microsoft CNTK toolkit. Let’s look at some of that terminology now:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Microsoft CNTK工具包中使用的某些术语非常重要。现在让我们看看这些术语中的一些：
- en: '**Tensors**: All CNTK inputs, outputs and parameters are organized as tensors.
    It should also be noted that *minibatches* are tensors as well.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量**：CNTK的所有输入、输出和参数都组织为张量。还应注意的是，*小批量*也是张量。'
- en: '**Rank**: Each tensor has a rank. Scalars are tensors with a rank of 0, vectors
    are tensors and have a rank of 1, and matrices are tensors with a rank of 2.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秩**：每个张量都有一个秩。标量是秩为0的张量，向量是秩为1的张量，矩阵是秩为2的张量。'
- en: '**Static axis**: The dimensions listed in 2 are referred to as **axes**. Every
    tensor has *static* and *dynamic* axes. A Static axis has the same length throughout
    its entire life.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态轴**：在2中列出的尺寸被称为**轴**。每个张量都有**静态**和**动态**轴。静态轴在其整个生命周期中长度保持不变。'
- en: '**Dynamic axis**: Dynamic axes, however, can vary their length from instance
    to instance. Their length is typically not known before each minibatch is presented.
    Additionally, they may be ordered.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态轴**：然而，动态轴的长度可以从实例到实例变化。通常在呈现每个小批量之前不知道它们的长度。此外，它们可能是有序的。'
- en: '**Minibatch**: A minibatch is also a tensor. It has a dynamic axis, which is
    called the **batch axis**. The length of this axis can change from minibatch to
    minibatch.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小批量**：小批量也是一个张量。它有一个动态轴，称为**批量轴**。这个轴的长度可以从一个小批量变化到另一个小批量。'
- en: 'At the time of writing of CNTK supports one additional single additional dynamic
    axis, also known as the **sequence axis**. This axis allows the user to work with
    sequences in a more abstract, higher-level manner. The beauty of sequence is that
    whenever an operation is performed on a sequence, the CNTK toolkit does a type-checking
    operation to determine safety:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写CNTK时，支持一个额外的单动态轴，也称为**序列轴**。这个轴允许用户以更抽象、更高级的方式处理序列。序列的美丽之处在于，每当对序列执行操作时，CNTK工具包都会进行类型检查操作以确定安全性：
- en: '![](img/6fd1d49a-0dba-4156-8d7b-1b9a7a32a281.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6fd1d49a-0dba-4156-8d7b-1b9a7a32a281.jpg)'
- en: Sequence Axis
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 序列轴
- en: Our example
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的示例
- en: So now that we have covered some basics for both CNTK and LSTM, it’s time to
    dive into our example application. You can find this project with the code that
    accompanies this book. Make sure you have it open in Microsoft Visual Studio before
    proceeding. You can follow the instructions in the upcoming *The Code* section
    if you need further instructions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经介绍了CNTK和LSTM的一些基础知识，是时候深入我们的示例应用程序了。您可以在本书附带的代码中找到这个项目。在继续之前，请确保您已经在Microsoft
    Visual Studio中将其打开。如果您需要进一步说明，可以参考即将到来的*代码*部分。
- en: The example we are creating uses Microsoft CNTK as a back-end and will use a
    simple sine wave as our function. The sine wave was plotted earlier and is used
    due to it’s being widely familiar to most individuals.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建的示例使用Microsoft CNTK作为后端，并将使用简单的正弦波作为我们的函数。正弦波之前已经绘制过，并且因其广为人知而被选用。
- en: 'Here are screenshots of what our example application looks like:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们示例应用程序的截图：
- en: '![](img/fce45926-766b-46ad-8232-31c95178763f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fce45926-766b-46ad-8232-31c95178763f.png)'
- en: Main page – training data
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 主页 – 训练数据
- en: 'The preceding screenshot shows our main screen, displaying our sine wave data
    points, our training data. The goal is to get our training data (blue) to match
    the red as closely as possible in shape, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了我们的主屏幕，显示了我们的正弦波数据点，我们的训练数据。目标是使我们的训练数据（蓝色）在形状上尽可能接近红色，如下所示：
- en: '![](img/5766a404-4601-41f8-99f9-1f060fd0edd1.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5766a404-4601-41f8-99f9-1f060fd0edd1.png)'
- en: Main Page - Charts
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 主页 - 图表
- en: 'The following screen allows us to plot our loss function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕允许我们绘制我们的损失函数：
- en: '![](img/d613e562-4775-485c-964a-8e6dca6215ac.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d613e562-4775-485c-964a-8e6dca6215ac.png)'
- en: Plotting the loss value (Loss Value tab)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制损失值（损失值选项卡）
- en: 'The following screen allows us to plot our observed versus predicted values.
    The goal is to have the predicted values (blue) match as closely as possible the
    actual values (shown in red):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕允许我们绘制观察值与预测值。目标是使预测值（蓝色）尽可能接近实际值（以红色显示）：
- en: '![](img/bd00e35b-8e22-4eda-b904-39fca2b0d31e.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd00e35b-8e22-4eda-b904-39fca2b0d31e.png)'
- en: Plotting Observed versus Predicted (Test Data tab)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据选项卡中绘制观察值与预测值
- en: Coding our application
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写我们的应用程序
- en: Let’s now look at our code. For this you will need to reference the `LSTMTimeSeriesDemo`
    project that accompanies this book. Open the project in Microsoft Visual Studio.
    All the required CNTK libraries are already referenced in this project for you
    and included in the Debug/bin directory. The main library we will be using is
    `Cntk.Core.Managed`. We are using version 2.5.1 for this example, in case you
    were wondering!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们的代码。为此，您需要参考随本书附带的`LSTMTimeSeriesDemo`项目。在Microsoft Visual Studio中打开项目。所有必需的CNTK库已经在此项目中为您引用，并包含在Debug/bin目录中。我们将使用的主要库是`Cntk.Core.Managed`。对于这个例子，我们使用的是2.5.1版本，以防您想知道！
- en: Loading data and graphs
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据和图表
- en: To load our data and graphs, we have two main functions that we will
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载数据和图表，我们有两个主要函数，我们将
- en: 'use; `LoadTrainingData()`and `PopulateGraphs()`. Pretty straightforward, right?:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用；`LoadTrainingData()`和`PopulateGraphs()`。非常直接，对吧？：
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Loading training data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载训练数据
- en: 'For this example, we are simply making up our test and training data on-the-fly.
    The `LoadTrainingData()` function does just that:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们只是即时创建我们的测试和训练数据。`LoadTrainingData()`函数正是如此：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Populating the graphs
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充图表
- en: 'This function populates the graphs with training and test data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数使用训练数据和测试数据填充图表：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Splitting data
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割数据
- en: 'With this function we mirror frameworks such as Python, which make it very
    easy to split training and testing data from the main dataset. We have created
    our own function to do the same thing:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此函数，我们模仿了Python等框架，这使得从主数据集中分割训练数据和测试数据变得非常容易。我们已创建了自己的函数来完成相同的事情：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running the application
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行应用程序
- en: 'Once we click the **Run** button, we will execute the function outlined as
    follows. We first determine the number of iterations the user wants to use, as
    well as the batch size. After setting up our progress bar and some internal variables,
    we call our `TrainNetwork()` function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们点击**运行**按钮，我们将执行以下概述的函数。我们首先确定用户想要使用的迭代次数以及批次大小。在设置进度条和一些内部变量之后，我们调用我们的`TrainNetwork()`函数：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Training the network
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'In every neural network we must train the network in order for it to recognize
    whatever we are providing it. Our `TrainNetwork()` function does just that:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个神经网络中，我们必须训练网络，以便它能够识别我们提供给它的任何内容。我们的`TrainNetwork()`函数正是如此：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create the model, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模型，如下所示：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Prepare for training:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 准备训练：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create the trainer, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 创建训练器，如下所示：
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Train the model, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型，如下所示：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Get the next minibatch amount of data, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 获取下一个minibatch数量的数据，如下所示：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Train, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 训练，如下所示：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Creating a model
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: 'To create a model, we are going to build a one-directional recurrent neural
    network that contains **long short-term memory** (**LSTM**) cells as shown in
    the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个模型，我们将构建一个包含**长短期记忆**（**LSTM**）细胞的单向循环神经网络，如下所示：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a LSTM cell for each input variable, as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个输入变量创建一个 LSTM 细胞，如下所示：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After the LSTM sequence is created, return the last cell in order to continue
    generating the network, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 LSTM 序列之后，返回最后一个细胞以便继续生成网络，如下所示：
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Implement dropout for 20%, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实现dropout为20%，如下所示：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create the last dense layer before the output, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出之前创建最后一个密集层，如下所示：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Getting the next data batch
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取下一个数据批次
- en: 'We get our next batch of data in an enumerable fashion. We first verify parameters,
    and then call the `CreateBatch()` function, which is listed after this function:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以可枚举的方式获取下一批数据。我们首先验证参数，然后调用`CreateBatch()`函数，该函数列在此函数之后：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Creating a batch of data
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据批次
- en: 'Given a dataset, this function will create *batches* of data for use in traversing
    the total data set in more manageable segments. You can see how it is called from
    the `GetNextDataBatch()` function shown previously:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据集，此函数将创建用于在更可管理的段中遍历整个数据集的*批次*。您可以从之前显示的`GetNextDataBatch()`函数中看到它是如何被调用的：
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How well do LSTMs perform?
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM的表现如何？
- en: 'We tested LSTMs against predictions and historical values for sunspot data
    prediction, a very famous test in deep learning. As you can see, the red plot,
    which is our predictions, melded into the trend exactly, which is a tremendously
    encouraging sign:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了LSTM对太阳黑子数据预测的预测和历史值，这是深度学习中的一个非常著名的测试。正如您所看到的，红色曲线，即我们的预测，与趋势完全融合，这是一个非常鼓舞人心的迹象：
- en: '![](img/e010f68f-0234-4e2f-b147-9b1904874f80.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e010f68f-0234-4e2f-b147-9b1904874f80.jpg)'
- en: Prediction versus Performance
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 预测与性能比较
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we learned about long short-term memory recurrent neural networks.
    We programmed an example application showing how to use them, and we learned some
    basic terminology along the way. We covered topics such as LSTMs, tensors, static
    and dynamic axes, loading datasets, plotting data, creating models, and creating
    minibatches. In our next chapter, we will visit a very close cousin of LSTM networks:
    gated recurrent units.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了长短期记忆循环神经网络。我们编写了一个示例应用程序，展示了如何使用它们，并且在过程中学习了一些基本术语。我们涵盖了诸如LSTM、张量、静态和动态轴、加载数据集、绘制数据、创建模型和创建小批量等主题。在我们下一章中，我们将访问LSTM网络的一个非常近的表亲：门控循环单元。
- en: References
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation,
    9(8), 1735-1780.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, S., & Schmidhuber, J. (1997). 长短期记忆。神经计算，第9卷，第8期，第1735-1780页。
- en: 'Gers, F. A., Schmidhuber, J., & Cummins, F. (2000). Learning to forget: Continual
    prediction with LSTM. Neural computation, 12(10), 2451-2471.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gers, F. A., Schmidhuber, J., & Cummins, F. (2000). 学习遗忘：使用LSTM的持续预测。神经计算，第12卷，第10期，第2451-2471页。
- en: Graves, Alex. Supervised sequence labeling with recurrent neural networks. Vol.
    385\. Springer, 2012.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves, Alex. 使用循环神经网络进行监督序列标注。Springer, 第385卷，2012年。
- en: Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with
    gradient descent is difficult. IEE TRANSACTIONS ON NEURAL NETWORKS, 5, 1994.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Bengio, P. Simard, 和 P. Frasconi. 使用梯度下降学习长期依赖性是困难的。IEEE神经网络 Transactions，第5卷，1994年。
- en: F. Chollet. Keras github.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F. Chollet. Keras github.
- en: J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated
    recurrent neural networks on sequence modeling, 2014.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Chung, C. Gulcehre, K. Cho, 和 Y. Bengio. 对序列建模的门控循环神经网络的实证评估，2014年。
- en: S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation,
    9:1735–1780, 1997.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Hochreiter 和 J. Schmidhuber. 长短期记忆。神经计算，第9卷，第1735-1780页，1997年。
- en: Q. V. Le, N. Jaitly, and H. G. E. A simple way to initialize recurrent networks
    of rectified linear units. 2015.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q. V. Le, N. Jaitly, 和 H. G. E. 矩形线性单元循环网络的一个简单初始化方法。2015年。
- en: Y. Lu and F. Salem. Simplified gating in long short-term memory (lstm) recurrent
    neural networks. arXiv:1701.03441, 2017.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Lu 和 F. Salem. 长短期记忆（LSTM）循环神经网络中的简化门控。arXiv:1701.03441，2017年。
- en: F. M. Salem. A basic recurrent neural network model. arXiv preprint arXiv:1612.09022,
    2016.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F. M. Salem. 基本循环神经网络模型。arXiv预印本 arXiv:1612.09022，2016年。
- en: F. M. Salem. Reduced parameterization of gated recurrent neural networks. MSU
    Memorandum, 7.11.2016.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F. M. Salem. 门控循环神经网络的参数化减少。MSU备忘录，2016年11月7日。
