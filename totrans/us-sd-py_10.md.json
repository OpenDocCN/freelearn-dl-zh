["```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\npipe = StableDiffusionPipeline.from_pretrained(\n    \"stablediffusionapi/deliberate-v2\",\n    torch_dtype=torch.float16).to(\"cuda\")\nprompt = \"a photo of a cat and a dog driving an aircraft \"*20\nimage = pipe(prompt = prompt).images[0]\nimage\n```", "```py\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens…\n```", "```py\na photo (cat:1.5) and a dog driving an aircraft\n```", "```py\n    prompt = \"a photo of a cat and a dog driving an aircraft \"*20\n    ```", "```py\n    tokenizer = pipe.tokenizer\n    ```", "```py\n    text_encoder = pipe.text_encoder\n    ```", "```py\n    tokens = tokenizer(\n    ```", "```py\n        prompt,\n    ```", "```py\n        truncation = False,\n    ```", "```py\n        return_tensors = 'pt'\n    ```", "```py\n    )[\"input_ids\"]\n    ```", "```py\n    print(len(tokens[0]))\n    ```", "```py\n    embeddings = pipe.text_encoder(tokens.to(\"cuda\"))[0]\n    ```", "```py\n    RuntimeError: The size of tensor a (181) must match the size of tensor b (77) at non-singleton dimension 1\n    ```", "```py\n    tensor([49406,   320,  1125,  2368,   537,  1929,  4161,   550,  7706, 49407])\n    ```", "```py\n    print(tokenizer._convert_id_to_token(49406))\n    ```", "```py\n    print(tokenizer._convert_id_to_token(49407))\n    ```", "```py\n    <|startoftext|>\n    ```", "```py\n    <|endoftext|>\n    ```", "```py\n    # step 1\\. take out the tokenizer and text encoder\n    ```", "```py\n    tokenizer = pipe.tokenizer\n    ```", "```py\n    text_encoder = pipe.text_encoder\n    ```", "```py\n    # step 2\\. encode whatever size prompt to tokens by setting \n    ```", "```py\n    # truncation = False.\n    ```", "```py\n    tokens = tokenizer(\n    ```", "```py\n        prompt,\n    ```", "```py\n        truncation = False\n    ```", "```py\n    )[\"input_ids\"]\n    ```", "```py\n    print(\"token length:\", len(tokens))\n    ```", "```py\n    # step 2.2\\. encode whatever size neg_prompt, \n    ```", "```py\n    # padding it to the size of prompt.\n    ```", "```py\n    negative_ids = pipe.tokenizer(\n    ```", "```py\n        neg_prompt,\n    ```", "```py\n        truncation    = False,\n    ```", "```py\n        padding       = \"max_length\",\n    ```", "```py\n        max_length    = len(tokens)\n    ```", "```py\n    ).input_ids\n    ```", "```py\n    print(\"neg_token length:\", len(negative_ids))\n    ```", "```py\n    tokens = tokens[1:-1]\n    ```", "```py\n    negative_ids = negative_ids[1:-1]\n    ```", "```py\n    # step 4\\. Pop out the head 77 tokens, \n    ```", "```py\n    # and encode the 77 tokens to embeddings.\n    ```", "```py\n    embeds,neg_embeds = [],[]\n    ```", "```py\n    chunk_size = 75\n    ```", "```py\n    bos = pipe.tokenizer.bos_token_id\n    ```", "```py\n    eos = pipe.tokenizer.eos_token_id\n    ```", "```py\n    for i in range(0, len(tokens), chunk_size):\n    ```", "```py\n    # Add the beginning and end token to the 75 chunked tokens to \n    ```", "```py\n    # make a 77-token list\n    ```", "```py\n        sub_tokens = [bos] + tokens[i:i + chunk_size] + [eos]\n    ```", "```py\n    # text_encoder support torch.Size([1,x]) input tensor\n    ```", "```py\n    # that is why use [sub_tokens], \n    ```", "```py\n    # instead of simply give sub_tokens.\n    ```", "```py\n        tensor_tokens = torch.tensor(\n    ```", "```py\n            [sub_tokens],\n    ```", "```py\n            dtype = torch.long,\n    ```", "```py\n            device = pipe.device\n    ```", "```py\n        )\n    ```", "```py\n        chunk_embeds = text_encoder(tensor_tokens)[0]\n    ```", "```py\n        embeds.append(chunk_embeds)\n    ```", "```py\n    # Add the begin and end token to the 75 chunked neg tokens to \n    ```", "```py\n    # make a 77 token list\n    ```", "```py\n        sub_neg_tokens = [bos] + negative_ids[i:i + chunk_size] + \\\n    ```", "```py\n            [eos]\n    ```", "```py\n        tensor_neg_tokens = torch.tensor(\n    ```", "```py\n            [sub_neg_tokens],\n    ```", "```py\n            dtype = torch.long,\n    ```", "```py\n            device = pipe.device\n    ```", "```py\n        )\n    ```", "```py\n        neg_chunk_embeds= text_encoder(tensor_neg_tokens)[0]\n    ```", "```py\n        neg_embeds.append(neg_chunk_embeds)\n    ```", "```py\n    [tensor1, tensor2...]\n    ```", "```py\n    # step 5\\. Stack the embeddings to a [1,x,768] size torch tensor.\n    ```", "```py\n    prompt_embeds = torch.cat(embeds, dim = 1)\n    ```", "```py\n    prompt_neg_embeds = torch.cat(neg_embeds, dim = 1)\n    ```", "```py\ndef long_prompt_encoding(\n    pipe:StableDiffusionPipeline,\n    prompt,\n    neg_prompt = \"\"\n):\n    bos = pipe.tokenizer.bos_token_id\n    eos = pipe.tokenizer.eos_token_id\n    chunk_size = 75\n    # step 1\\. take out the tokenizer and text encoder\n    tokenizer = pipe.tokenizer\n    text_encoder = pipe.text_encoder\n    # step 2.1\\. encode whatever size prompt to tokens by setting \n    # truncation = False.\n    tokens = tokenizer(\n        prompt.\n        truncation = False,\n        # return_tensors = 'pt'\n    )[\"input_ids\"]\n    # step 2.2\\. encode whatever size neg_prompt, \n    # padding it to the size of prompt.\n    negative_ids = pipe.tokenizer(\n        neg_prompt,\n        truncation = False,\n        # return_tensors = \"pt\",\n        Padding = \"max_length\",\n        max_length = len(tokens)\n    ).input_ids\n    # Step 3\\. remove begin and end tokens\n    tokens = tokens[1:-1]\n    negative_ids = negative_ids[1:-1]\n    # step 4\\. Pop out the head 77 tokens, \n    # and encode the 77 tokens to embeddings.\n    embeds,neg_embeds = [],[]\n    for i in range(0, len(tokens), chunk_size):\n# Add the beginning and end tokens to the 75 chunked tokens to make a \n# 77-token list\n        sub_tokens = [bos] + tokens[i:i + chunk_size] + [eos]\n# text_encoder support torch.Size([1,x]) input tensor\n# that is why use [sub_tokens], instead of simply give sub_tokens.\n        tensor_tokens = torch.tensor(\n            [sub_tokens],\n            dtype = torch.long,\n            device = pipe.device\n        )\n        chunk_embeds = text_encoder(tensor_tokens)[0]\n        embeds.append(chunk_embeds)\n# Add beginning and end token to the 75 chunked neg tokens to make a \n# 77-token list\n        sub_neg_tokens = [bos] + negative_ids[i:i + chunk_size] + \\\n            [eos]\n        tensor_neg_tokens = torch.tensor(\n            [sub_neg_tokens],\n            dtype = torch.long,\n            device = pipe.device\n        )\n        neg_chunk_embeds = text_encoder(tensor_neg_tokens)[0]\n        neg_embeds.append(neg_chunk_embeds)\n# step 5\\. Stack the embeddings to a [1,x,768] size torch tensor.\n    prompt_embeds = torch.cat(embeds, dim = 1)\n    prompt_neg_embeds = torch.cat(neg_embeds, dim = 1)\n    return prompt_embeds, prompt_neg_embeds\n```", "```py\nprompt = \"photo, cute cat running on the grass\" * 10 #<- long prompt\nprompt_embeds, prompt_neg_embeds = long_prompt_encoding(\n    pipe, prompt, neg_prompt=\"low resolution, bad anatomy\"\n)\nprint(prompt_embeds.shape)\nimage = pipe(\n    prompt_embeds = prompt_embeds,\n    negative_prompt_embeds = prompt_neg_embeds,\n    generator = torch.Generator(\"cuda\").manual_seed(1)\n).images[0]\nimage\n```", "```py\nprompt = \"photo, cute cat running on the grass\" * 10\nprompt = prompt + \",pure white cat\" * 10\n```", "```py\na (word) - increase attention to word by a factor of 1.1\na ((word)) - increase attention to word by a factor of 1.21 (= 1.1 * 1.1)\na [word] - decrease attention to word by a factor of 1.1\na (word:1.5) - increase attention to word by a factor of 1.5\na (word:0.25) - decrease attention to word by a factor of 4 (= 1 / 0.25)\na \\(word\\) - use literal () characters in prompt\n```", "```py\n    def parse_prompt_attention(text):\n    ```", "```py\n        import re\n    ```", "```py\n        re_attention = re.compile(\n    ```", "```py\n            r\"\"\"\n    ```", "```py\n                \\\\\\(|\\\\\\)|\\\\\\[|\\\\]|\\\\\\\\|\\\\|\\(|\\[|:([+-]?[.\\d]+)\\)|\n    ```", "```py\n                \\)|]|[^\\\\()\\[\\]:]+|:\n    ```", "```py\n            \"\"\"\n    ```", "```py\n            , re.X\n    ```", "```py\n        )\n    ```", "```py\n        re_break = re.compile(r\"\\s*\\bBREAK\\b\\s*\", re.S)\n    ```", "```py\n        res = []\n    ```", "```py\n        round_brackets = []\n    ```", "```py\n        square_brackets = []\n    ```", "```py\n        round_bracket_multiplier = 1.1\n    ```", "```py\n        square_bracket_multiplier = 1 / 1.1\n    ```", "```py\n        def multiply_range(start_position, multiplier):\n    ```", "```py\n            for p in range(start_position, len(res)):\n    ```", "```py\n                res[p][1] *= multiplier\n    ```", "```py\n        for m in re_attention.finditer(text):\n    ```", "```py\n            text = m.group(0)\n    ```", "```py\n            weight = m.group(1)\n    ```", "```py\n            if text.startswith('\\\\'):\n    ```", "```py\n                res.append([text[1:], 1.0])\n    ```", "```py\n            elif text == '(':\n    ```", "```py\n                round_brackets.append(len(res))\n    ```", "```py\n            elif text == '[':\n    ```", "```py\n                square_brackets.append(len(res))\n    ```", "```py\n            elif weight is not None and len(round_brackets) > 0:\n    ```", "```py\n                multiply_range(round_brackets.pop(), float(weight))\n    ```", "```py\n            elif text == ')' and len(round_brackets) > 0:\n    ```", "```py\n                multiply_range(round_brackets.pop(), \\\n    ```", "```py\n                    round_bracket_multiplier)\n    ```", "```py\n            elif text == ']' and len(square_brackets) > 0:\n    ```", "```py\n                multiply_range(square_brackets.pop(), \\\n    ```", "```py\n                    square_bracket_multiplier)\n    ```", "```py\n            else:\n    ```", "```py\n                parts = re.split(re_break, text)\n    ```", "```py\n                for i, part in enumerate(parts):\n    ```", "```py\n                    if i > 0:\n    ```", "```py\n                        res.append([\"BREAK\", -1])\n    ```", "```py\n                    res.append([part, 1.0])\n    ```", "```py\n        for pos in round_brackets:\n    ```", "```py\n            multiply_range(pos, round_bracket_multiplier)\n    ```", "```py\n        for pos in square_brackets:\n    ```", "```py\n            multiply_range(pos, square_bracket_multiplier)\n    ```", "```py\n        if len(res) == 0:\n    ```", "```py\n            res = [[\"\", 1.0]]\n    ```", "```py\n        # merge runs of identical weights\n    ```", "```py\n        i = 0\n    ```", "```py\n        while i + 1 < len(res):\n    ```", "```py\n            if res[i][1] == res[i + 1][1]:\n    ```", "```py\n                res[i][0] += res[i + 1][0]\n    ```", "```py\n                res.pop(i + 1)\n    ```", "```py\n            else:\n    ```", "```py\n                i += 1\n    ```", "```py\n        return res\n    ```", "```py\n    parse_prompt_attention(\"a (white) cat\")\n    ```", "```py\n    [['a ', 1.0], ['white', 1.1], [' cat', 1.0]]\n    ```", "```py\n    tokens: [1,2,3...]\n    ```", "```py\n    weights: [1.0, 1.0, 1.0...]\n    ```", "```py\n    # step 2\\. get prompts with weights\n    ```", "```py\n    # this function works for both prompt and negative prompt\n    ```", "```py\n    def get_prompts_tokens_with_weights(\n    ```", "```py\n        pipe: StableDiffusionPipeline,\n    ```", "```py\n        prompt: str\n    ```", "```py\n    ):\n    ```", "```py\n        texts_and_weights = parse_prompt_attention(prompt)\n    ```", "```py\n        text_tokens,text_weights = [],[]\n    ```", "```py\n        for word, weight in texts_and_weights:\n    ```", "```py\n            # tokenize and discard the starting and the ending token\n    ```", "```py\n            token = pipe.tokenizer(\n    ```", "```py\n                word,\n    ```", "```py\n                # so that tokenize whatever length prompt\n    ```", "```py\n                truncation = False\n    ```", "```py\n            ).input_ids[1:-1]\n    ```", "```py\n            # the returned token is a 1d list: [320, 1125, 539, 320]\n    ```", "```py\n            # use merge the new tokens to the all tokens holder: \n    ```", "```py\n            # text_tokens\n    ```", "```py\n            text_tokens = [*text_tokens,*token]\n    ```", "```py\n            # each token chunk will come with one weight, like ['red \n    ```", "```py\n            # cat', 2.0]\n    ```", "```py\n            # need to expand the weight for each token.\n    ```", "```py\n            chunk_weights = [weight] * len(token)\n    ```", "```py\n            # append the weight back to the weight holder: text_\n    ```", "```py\n            # weights\n    ```", "```py\n            text_weights = [*text_weights, *chunk_weights]\n    ```", "```py\n        return text_tokens,text_weights\n    ```", "```py\n    prompt = \"a (white) cat\"\n    ```", "```py\n    tokens, weights = get_prompts_tokens_with_weights(pipe, prompt)\n    ```", "```py\n    print(tokens,weights)\n    ```", "```py\n    [320, 1579, 2368] [1.0, 1.1, 1.0]\n    ```", "```py\n    # encode \"white\" only\n    ```", "```py\n    white_token = 1579\n    ```", "```py\n    white_token_tensor = torch.tensor(\n    ```", "```py\n        [[white_token]],\n    ```", "```py\n        dtype = torch.long,\n    ```", "```py\n        device = pipe.device\n    ```", "```py\n    )\n    ```", "```py\n    white_embed = pipe.text_encoder(white_token_tensor)[0]\n    ```", "```py\n    print(white_embed[0][0])\n    ```", "```py\n    # encode \"white cat\"\n    ```", "```py\n    white_token, cat_token = 1579, 2369\n    ```", "```py\n    white_cat_token_tensor = torch.tensor(\n    ```", "```py\n        [[white_token, cat_token]],\n    ```", "```py\n        dtype = torch.long,\n    ```", "```py\n        device = pipe.device\n    ```", "```py\n    )\n    ```", "```py\n    white_cat_embeds = pipe.text_encoder(white_cat_token_tensor)[0]\n    ```", "```py\n    print(white_cat_embeds[0][0])\n    ```", "```py\n    # step 3\\. padding tokens\n    ```", "```py\n    def pad_tokens_and_weights(\n    ```", "```py\n        token_ids: list,\n    ```", "```py\n        weights: list\n    ```", "```py\n    ):\n    ```", "```py\n        bos,eos = 49406,49407\n    ```", "```py\n        # this will be a 2d list\n    ```", "```py\n        new_token_ids = []\n    ```", "```py\n        new_weights   = []\n    ```", "```py\n        while len(token_ids) >= 75:\n    ```", "```py\n            # get the first 75 tokens\n    ```", "```py\n            head_75_tokens = [token_ids.pop(0) for _ in range(75)]\n    ```", "```py\n            head_75_weights = [weights.pop(0) for _ in range(75)]\n    ```", "```py\n            # extract token ids and weights\n    ```", "```py\n            temp_77_token_ids = [bos] + head_75_tokens + [eos]\n    ```", "```py\n            temp_77_weights   = [1.0] + head_75_weights + [1.0]\n    ```", "```py\n            # add 77 tokens and weights chunks to the holder list\n    ```", "```py\n            new_token_ids.append(temp_77_token_ids)\n    ```", "```py\n            new_weights.append(temp_77_weights)\n    ```", "```py\n        # padding the left\n    ```", "```py\n        if len(token_ids) > 0:\n    ```", "```py\n            padding_len = 75 - len(token_ids)\n    ```", "```py\n            padding_len = 0\n    ```", "```py\n            temp_77_token_ids = [bos] + token_ids + [eos] * \\\n    ```", "```py\n                padding_len + [eos]\n    ```", "```py\n            new_token_ids.append(temp_77_token_ids)\n    ```", "```py\n            temp_77_weights = [1.0] + weights   + [1.0] * \\\n    ```", "```py\n                padding_len + [1.0]\n    ```", "```py\n            new_weights.append(temp_77_weights)\n    ```", "```py\n        # return\n    ```", "```py\n        return new_token_ids, new_weights\n    ```", "```py\n    t,w = pad_tokens_and_weights(tokens.copy(), weights.copy())\n    ```", "```py\n    print(t)\n    ```", "```py\n    print(w)\n    ```", "```py\n    [320, 1579, 2368] [1.0, 1.1, 1.0]\n    ```", "```py\n    [[49406, 320, 1579, 2368, 49407]]\n    ```", "```py\n    [[1.0, 1.0, 1.1, 1.0, 1.0]]\n    ```", "```py\n    def get_weighted_text_embeddings(\n    ```", "```py\n        pipe: StableDiffusionPipeline,\n    ```", "```py\n        prompt : str      = \"\",\n    ```", "```py\n        neg_prompt: str   = \"\"\n    ```", "```py\n    ):\n    ```", "```py\n        eos = pipe.tokenizer.eos_token_id\n    ```", "```py\n        prompt_tokens, prompt_weights = \\ \n    ```", "```py\n            get_prompts_tokens_with_weights(\n    ```", "```py\n            pipe, prompt\n    ```", "```py\n        )\n    ```", "```py\n        neg_prompt_tokens, neg_prompt_weights = \\\n    ```", "```py\n            get_prompts_tokens_with_weights(pipe, neg_prompt)\n    ```", "```py\n        # padding the shorter one\n    ```", "```py\n        prompt_token_len        = len(prompt_tokens)\n    ```", "```py\n        neg_prompt_token_len    = len(neg_prompt_tokens)\n    ```", "```py\n        if prompt_token_len > neg_prompt_token_len:\n    ```", "```py\n            # padding the neg_prompt with eos token\n    ```", "```py\n            neg_prompt_tokens   = (\n    ```", "```py\n                neg_prompt_tokens  + \\\n    ```", "```py\n                [eos] * abs(prompt_token_len - neg_prompt_token_len)\n    ```", "```py\n            )\n    ```", "```py\n            neg_prompt_weights  = (\n    ```", "```py\n                neg_prompt_weights +\n    ```", "```py\n                [1.0] * abs(prompt_token_len - neg_prompt_token_len)\n    ```", "```py\n            )\n    ```", "```py\n        else:\n    ```", "```py\n            # padding the prompt\n    ```", "```py\n            prompt_tokens       = (\n    ```", "```py\n                prompt_tokens \\\n    ```", "```py\n                + [eos] * abs(prompt_token_len - \\\n    ```", "```py\n                neg_prompt_token_len)\n    ```", "```py\n            )\n    ```", "```py\n            prompt_weights      = (\n    ```", "```py\n                prompt_weights \\\n    ```", "```py\n                + [1.0] * abs(prompt_token_len - \\\n    ```", "```py\n                neg_prompt_token_len)\n    ```", "```py\n            )\n    ```", "```py\n        embeds = []\n    ```", "```py\n        neg_embeds = []\n    ```", "```py\n        prompt_token_groups ,prompt_weight_groups = \\\n    ```", "```py\n            pad_tokens_and_weights(\n    ```", "```py\n                prompt_tokens.copy(),\n    ```", "```py\n                prompt_weights.copy()\n    ```", "```py\n        )\n    ```", "```py\n        neg_prompt_token_groups, neg_prompt_weight_groups = \\\n    ```", "```py\n            pad_tokens_and_weights(\n    ```", "```py\n                neg_prompt_tokens.copy(),\n    ```", "```py\n                neg_prompt_weights.copy()\n    ```", "```py\n            )\n    ```", "```py\n        # get prompt embeddings one by one is not working.\n    ```", "```py\n        for i in range(len(prompt_token_groups)):\n    ```", "```py\n            # get positive prompt embeddings with weights\n    ```", "```py\n            token_tensor = torch.tensor(\n    ```", "```py\n                [prompt_token_groups[i]],\n    ```", "```py\n                dtype = torch.long, device = pipe.device\n    ```", "```py\n            )\n    ```", "```py\n            weight_tensor = torch.tensor(\n    ```", "```py\n                prompt_weight_groups[i],\n    ```", "```py\n                dtype     = torch.float16,\n    ```", "```py\n                device    = pipe.device\n    ```", "```py\n            )\n    ```", "```py\n            token_embedding = \\\n    ```", "```py\n                pipe.text_encoder(token_tensor)[0].squeeze(0)\n    ```", "```py\n            for j in range(len(weight_tensor)):\n    ```", "```py\n                token_embedding[j] = token_embedding[j] * \n    ```", "```py\n                    weight_tensor[j]\n    ```", "```py\n            token_embedding = token_embedding.unsqueeze(0)\n    ```", "```py\n            embeds.append(token_embedding)\n    ```", "```py\n            # get negative prompt embeddings with weights\n    ```", "```py\n            neg_token_tensor = torch.tensor(\n    ```", "```py\n                [neg_prompt_token_groups[i]],\n    ```", "```py\n                dtype = torch.long, device = pipe.device\n    ```", "```py\n            )\n    ```", "```py\n            neg_weight_tensor = torch.tensor(\n    ```", "```py\n                neg_prompt_weight_groups[i],\n    ```", "```py\n                dtype     = torch.float16,\n    ```", "```py\n                device    = pipe.device\n    ```", "```py\n            )\n    ```", "```py\n            neg_token_embedding = \\\n    ```", "```py\n                pipe.text_encoder(neg_token_tensor)[0].squeeze(0)\n    ```", "```py\n            for z in range(len(neg_weight_tensor)):\n    ```", "```py\n                neg_token_embedding[z] = (\n    ```", "```py\n                    neg_token_embedding[z] * neg_weight_tensor[z]\n    ```", "```py\n                )\n    ```", "```py\n            neg_token_embedding = neg_token_embedding.unsqueeze(0)\n    ```", "```py\n            neg_embeds.append(neg_token_embedding)\n    ```", "```py\n        prompt_embeds       = torch.cat(embeds, dim = 1)\n    ```", "```py\n        neg_prompt_embeds   = torch.cat(neg_embeds, dim = 1)\n    ```", "```py\n        return prompt_embeds, neg_prompt_embeds\n    ```", "```py\nprompt = \"photo, cute cat running on the grass\" * 10\nprompt = prompt + \",pure (white:1.5) cat\" * 10\nneg_prompt = \"low resolution, bad anatomy\"\nprompt_embeds, prompt_neg_embeds = get_weighted_text_embeddings(\n    pipe, prompt = prompt, neg_prompt = neg_prompt\n)\nimage = pipe(\n    prompt_embeds = prompt_embeds,\n    negative_prompt_embeds = prompt_neg_embeds,\n    generator = torch.Generator(\"cuda\").manual_seed(1)\n).images[0]\nimage\n```", "```py\n    from diffusers import DiffusionPipeline\n    ```", "```py\n    import torch\n    ```", "```py\n    model_id_or_path = \"stablediffusionapi/deliberate-v2\"\n    ```", "```py\n    pipe = DiffusionPipeline.from_pretrained(\n    ```", "```py\n        model_id_or_path,\n    ```", "```py\n        torch_dtype = torch.float16,\n    ```", "```py\n        custom_pipeline = \"lpw_stable_diffusion\"\n    ```", "```py\n    ).to(\"cuda:0\")\n    ```", "```py\n    prompt = \"photo, cute cat running on the grass\" * 10\n    ```", "```py\n    prompt = prompt + \",pure (white:1.5) cat\" * 10\n    ```", "```py\n    neg_prompt = \"low resolution, bad anatomy\"\n    ```", "```py\n    image = pipe(\n    ```", "```py\n        prompt = prompt,\n    ```", "```py\n        negative_prompt = neg_prompt,\n    ```", "```py\n        generator = torch.Generator(\"cuda\").manual_seed(1)\n    ```", "```py\n    ).images[0]\n    ```", "```py\n    image\n    ```", "```py\n    from diffusers import DiffusionPipeline\n    ```", "```py\n    import torch\n    ```", "```py\n    model_id_or_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n    ```", "```py\n    pipe = DiffusionPipeline.from_pretrained(\n    ```", "```py\n        model_id_or_path,\n    ```", "```py\n        torch_dtype = torch.float16,\n    ```", "```py\n        custom_pipeline = \"lpw_stable_diffusion_xl\",\n    ```", "```py\n    ).to(\"cuda:0\")\n    ```", "```py\n    prompt = \"photo, cute cat running on the grass\" * 10\n    ```", "```py\n    prompt = prompt + \",pure (white:1.5) cat\" * 10\n    ```", "```py\n    neg_prompt = \"low resolution, bad anatomy\"\n    ```", "```py\n    image = pipe(\n    ```", "```py\n        prompt = prompt,\n    ```", "```py\n        negative_prompt = neg_prompt,\n    ```", "```py\n        generator = torch.Generator(\"cuda\").manual_seed(7)\n    ```", "```py\n    ).images[0]\n    ```", "```py\n    image\n    ```"]