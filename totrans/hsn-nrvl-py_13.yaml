- en: Deep Neuroevolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the deep neuroevolution method, which
    can be used to train **Deep Neural Networks** (**DNNs**). DNNs are conventionally
    trained using backpropagation methods based on the descent of the error gradient,
    which is computed with respect to the weights of the connections between neural
    nodes. Although gradient-based learning is a powerful technique that conceived
    the current era of deep machine learning, it has its drawbacks, such as long training
    times and enormous computing power requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will demonstrate how deep neuroevolution methods can be
    used for reinforcement learning and how they considerably outperform traditional
    DQN, A3C gradient-based learning methods of training DNNs. By the end of this
    chapter, you will have a solid understanding of deep neuroevolution methods, and
    you'll also have practical experience with them. We will learn how to evolve agents
    so that they can play classic Atari games using deep neuroevolution. Also, you
    will learn how to use **Visual Inspector for NeuroEvolution** (**VINE**) to examine
    the results of experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep neuroevolution for deep reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolving agents to play Frostbite Atari games using deep neuroevolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an agent to play the Frostbite game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the Frostbite Atari experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the results with VINE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following technical requirements should be met so that you can complete
    the experiments described in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A modern PC with a Nvidia graphics accelerator GeForce GTX 1080Ti or better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MS Windows 10, Ubuntu Linux 16.04, or macOS 10.14 with a discrete GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda Distribution version 2019.03 or newer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter10)
  prefs: []
  type: TYPE_NORMAL
- en: Deep neuroevolution for deep reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we have already covered how the neuroevolution method can be applied
    to solve simple **reinforcement learning** (**RL**) tasks, such as single- and
    double-pole balancing in [Chapter 4](34913ccd-6aac-412a-8f54-70d1900cef41.xhtml),
    *Pole-Balancing Experiments*. However, while the pole-balancing experiment is
    exciting and easy to conduct, it is pretty simple and operates with tiny artificial
    neural networks. In this chapter, we will discuss how to apply neuroevolution
    to reinforcement learning problems that require immense ANNs to approximate the
    value function of the RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RL algorithm learns through trial and error. Almost all the variants of
    RL algorithms try to optimize the value function, which maps the current state
    of the system to the appropriate action that will be performed in the next time
    step. The most widely used classical version of the RL algorithm uses a Q-learning
    method that is built around a table of states keyed by actions, which constitute
    the policy rules to be followed by the algorithm after training is complete. The
    training consists of updating the cells of the Q-table by iteratively executing
    specific actions at particular states and collecting the reward signals afterward.
    The following formula determines the process of updating a particular cell in
    a Q-table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2b89378-75ed-435b-9129-bb0942dd3642.png)'
  prefs: []
  type: TYPE_IMG
- en: Here ![](img/a8ce4769-6a5a-41fa-a659-5774d19f3608.png) is the reward that's
    received when the system state changes from state ![](img/fb3f1125-08a8-4221-aec4-8da38d84ef4c.png)
    to state ![](img/5af7a49b-2ffc-49c6-9b42-9a2c41d56fc1.png), ![](img/d747ca3a-3547-4da6-a5b1-9093ac311bb4.png)
    is the action taken at time ![](img/038e002e-5dc9-45cf-8613-e9f369d1dd4e.png)
    leading to the state change, ![](img/8d7ff50c-3187-487b-bcd8-44f13acbe3e1.png)
    is the learning rate, and ![](img/6d098dd0-38e8-4639-b80d-abdd4c795fe7.png) is
    a discount factor that controls the importance of the future rewards. The learning
    rate determines to what extent the new information overrides existing information
    in the specific Q-table cell. If we set the learning rate to zero, then nothing
    will be learned, and if we set it to *1*, then nothing will be retained. Thus,
    the learning rate controls how fast the system is able to learn new information
    while maintaining useful, already-learned data.
  prefs: []
  type: TYPE_NORMAL
- en: The simple version of the Q-learning algorithm iterates over all possible action-state
    combinations and updates the Q-values, as we've already discussed. This approach
    works pretty well for simple tasks with a small number of action-state pairs but
    quickly fails with an increase in the number of such pairs, that is, with increased
    dimensionality of the action-state space. Most real-world tasks have profound
    dimensionality of the action-state space, which makes it infeasible for the classical
    version of Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: The method of Q-value function approximation was proposed to address the problem
    of increased dimensionality. In this method, the Q-learning policy is defined
    not by the action-state table we mentioned earlier but is instead approximated
    by a function. One of the ways to achieve this approximation is to use an ANN
    as a universal approximation function. By using an ANN, especially a deep ANN
    for Q-value approximation, it becomes possible to use RL algorithms for very complex
    problems, even for problems with a continuous state space. Thus, the DQN method was
    devised, which uses a DNN for Q-value approximation. The RL based on the DNN value-function
    approximation was named **deep reinforcement learning** (**deep RL**).
  prefs: []
  type: TYPE_NORMAL
- en: With deep RL, it is possible to learn action policies directly from the pixels
    of a video stream. This allows us to use a video stream to train agents to play
    video games, for example. However, the DQN method can be considered a gradient-based
    method. It uses error (loss) backpropagation in the DNN to optimize the Q-value
    function approximator. While being a potent technique, it has a significant drawback
    regarding the computational complexity that's involved, which requires the use
    of GPUs to perform all the matrix multiplications during the gradient descent-related
    computations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the methods that can be used to reduce computational costs is **Genetic
    Algorithms** (**GA**), such as neuroevolution. Neuroevolution allows us to evolve
    a DNN for the Q-value function approximation without any gradient-based computations
    involved. In recent studies, it was shown that gradient-free GA methods show excellent
    performance when it comes to challenging deep RL tasks and that they can even
    outperform their conventional counterparts. In the next section, we'll discuss
    how the deep neuroevolution method can be used to train successful agents to play
    one of the classic Atari games, just by reading game screen observations.
  prefs: []
  type: TYPE_NORMAL
- en: Evolving an agent to play the Frostbite Atari game using deep neuroevolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recently, classic Atari games were encapsulated by the **Atari Learning Environment**
    (**ALE**) to become a benchmark for testing different implementations of RL algorithms.
    Algorithms that are tested against the ALE are required to read the game state
    from the pixels of the game screen and devise a sophisticated control logic that
    allows the agent to win the game. Thus, the task of the algorithm is to evolve
    an understanding of the game situation in terms of the game character and its
    adversaries. Also, the algorithm needs to understand the reward signal that's
    received from the game screen in the form of the final game score at the end of
    a single game run.
  prefs: []
  type: TYPE_NORMAL
- en: The Frostbite Atari game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Frostbite is a classic Atari game where you control a game character that is
    building an igloo. The game screen is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ecfe780-eea0-4e30-b482-9a48c91801a5.png)'
  prefs: []
  type: TYPE_IMG
- en: The Frostbite game screen
  prefs: []
  type: TYPE_NORMAL
- en: The bottom part of the screen is water, with floating ice blocks arranged in
    four rows. The game character jumps from one row to another while trying to avoid
    various foes. If the game character jumps on a white ice block, this block is
    collected and used to build an igloo on the shore in the top right of the screen.
    After that, the white ice block changes its color and cannot be used anymore.
  prefs: []
  type: TYPE_NORMAL
- en: To build the igloo, the game character must collect 15 ice blocks within 45
    seconds. Otherwise, the game ends because the game character becomes frozen. When
    the igloo is complete, the game character must move inside it to complete the
    current level. The faster the game character completes a level, the more bonus
    points are awarded to the player.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll discuss how the game screen state is mapped into input parameters,
    which can be used by the neuroevolution method.
  prefs: []
  type: TYPE_NORMAL
- en: Game screen mapping into actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep ANNs can be trained to play Atari games if they can directly map the pixels
    on the screen to a system to control the game. This means that our algorithm must
    read the game screen and decide what game action to take to get the highest game
    score possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This task can be divided into two logical subtasks:'
  prefs: []
  type: TYPE_NORMAL
- en: The image analysis task, which encodes the state of the current game situation
    on the screen, including the game character's position, obstacles, and adversaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RL training task, which is used to train the Q-value approximation ANN to
    build the correct mapping between the specific game state and actions to be performed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**) are commonly used in tasks related
    to the analysis of visual imagery or other high-dimensional Euclidean data. The
    power of CNNs is based on their ability to significantly reduce the number of
    learning parameters compared to other types of ANN if they''re applied to visual
    recognition. The CNN hierarchy usually has multiple sequential convolutional layers
    combined with non-linear fully connected layers and ends with a fully connected
    layer that is followed by the loss layer. The final fully connected and loss layers
    implement the high-level reasoning in the Neural Network architecture. In the
    case of deep RL, these layers make the Q-value approximation. Next, we''ll consider
    the details of convolutional layer implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By studying the organization of the visual cortex of higher life forms (including
    humans), researchers gained inspiration for the design of CNNs. Each neuron of
    the visual cortex responds to signals that are received from a limited region
    of the visual field – the neuron''s receptive field. The receptive fields of various
    neurons overlap partially, which allows them to cover the entire visual field, as
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/560f658d-543d-4b08-b4d3-269287909c3a.png)'
  prefs: []
  type: TYPE_IMG
- en: The scheme of connections between the receptive field (on the left) and neurons
    in the convolutional layer (on the right)
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer consists of a column of neurons, where each neuron in
    a single column is connected to the same receptive field. This column represents
    a set of filters (kernels). Each filter is defined by the size of the receptive
    field and the number of channels. The number of channels defines the depth of
    the neurons column, while the size of the receptive field determines the number
    of columns in the convolutional layer. When the receptive field moves over the
    visual field, in each step, the new column of neurons is activated.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned previously, each convolutional layer is usually combined with
    a fully connected layer with non-linear activation, such as **Rectified Linear
    Unit** (**ReLU**). The ReLU activation function has the effect of filtering out
    negative values, as given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09ad5aa4-e9d6-42f3-b871-e1377f38c153.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/0b414ba6-373b-4aba-a98a-d9fd256e41f4.png) is the input to a neuron.
  prefs: []
  type: TYPE_NORMAL
- en: In the ANN architecture, several convolutional layers are connected to a number
    of fully connected layers, which performs high-level reasoning. Next, we'll discuss
    the CNN architecture that's used in our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: The CNN architecture to train the Atari playing agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our experiment, we'll use a CNN architecture consisting of three convolutional
    layers with 32, 64, and 64 channels, followed by a fully connected layer with
    512 units and the output layer with the number of layers corresponding to the
    number of game actions. The convolutional layers have 8 x 8, 4 x 4, and 3 x 3
    kernel sizes and use strides of 4, 2, and 1, respectively. The ReLU non-linearity
    follows all the convolutional and fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code to create the described network graph model using the TensorFlow
    framework is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As a result of this architecture, the CNN contains about **4 million trainable
    parameters**. Next, we'll discuss how RL training is done in our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: For complete implementation details, refer to the `dqn.py` Python script at
    [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py).
  prefs: []
  type: TYPE_NORMAL
- en: The RL training of the game agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RL training in our experiment was implemented using the neuroevolution method.
    This method is based on a simple genetic algorithm that evolves a population of
    individuals. The genotype of each individual encodes the vector of the trainable
    parameters of the controller ANN. By trainable parameters, we mean the connection
    weights between the network nodes. In every generation, each genotype is evaluated
    against a test environment by playing Frostbite and produces a specific fitness
    score. We evaluate each agent (genome) against 20,000 frames of the game. During
    the evaluation period, the game character can play multiple times, and the final
    Atari game score is the fitness score, which is a reward signal in terms of RL.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll discuss the genome encoding scheme, which allows us to encode more
    than 4 million learning parameters of the ANN that's controlling the game-solving
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: The genome encoding scheme
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deep RL neural network that we use as the controller of the game agent has
    about 4 million trainable parameters. Each trainable parameter is the weight of
    a connection between two nodes of the neural network. Traditionally, training
    neural networks is about finding the appropriate values of all the connection
    weights, allowing the neural network to approximate a function that describes
    the specifics of the modeled process.
  prefs: []
  type: TYPE_NORMAL
- en: The conventional way to estimate these trainable parameters is to use some form
    of error backpropagation based on the gradient descent of the loss value, which
    is very computationally intensive. On the other hand, the neuroevolution algorithm
    allows us to train ANNs using a nature-inspired genetic algorithm. The neuroevolution
    algorithm applies a series of mutations and recombinations to the trainable parameters
    to find the correct configuration of the ANN. However, to use the genetic algorithm,
    an appropriate scheme of encoding of the phenotype ANN should be devised. After
    that, the population of individuals (genomes encoding the phenotype ANN) can be
    created and evolved using a simple genetic algorithm, which we'll discuss later.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, the encoding scheme should produce compact genomes
    that can encode values of more than 4 million connection weights between the nodes
    of the deep RL ANN controlling the game agent. We are looking for compact genomes
    to reduce the computational costs associated with genetic algorithm evaluation.
    Next, we'll discuss the definition of the genome encoding scheme, which can be
    used to encode the large phenotype ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Genome encoding scheme definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Researchers at the Uber AI lab have proposed a coding scheme that uses the seed
    of a pseudo-random number generator to encode the phenotype ANN. In this scheme,
    the genome is represented as a list of seed values, which is applied sequentially
    to generate the values for all the weights (trainable parameters) of connections
    that are expressed between the nodes of a controller ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the first seed value in the list represents the policy initialization
    seed, which is shared between the genealogy of the descendants of a single parent.
    All the subsequent seed values represent the specific mutations that are acquired
    during the evolution process by the offspring. Each seed is applied sequentially
    to produce the ANN parameters vector of a specific phenotype. The following formula
    defines the estimation of the phenotype parameters vector for a specific individual
    (![](img/13909b80-d25b-4184-b366-f96c9f975f53.png)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/776be492-a072-455d-96d6-90bec14f7106.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/1981550c-2382-4367-8347-2dc4254fdb36.png) is the encoding of ![](img/56b74879-2d2d-47a1-a0d7-9cdaab645845.png)
    and consists of a list of mutation seeds; ![](img/62ff51bb-f7d9-4625-a301-3a06360e0676.png) is
    a deterministic Gaussian pseudo-random number generator with an input seed ![](img/8bc7ccc1-51cf-4d57-945b-d8a09fca3658.png)
    that produces a vector of length ![](img/b118aca6-1e3a-452c-a1a2-ee497c040b41.png); ![](img/41c3bd93-95fa-45d5-8bc6-04ffdbcf35df.png)
    is an initial parameters vector that's created during initialization as follows,
    ![](img/bb4d1685-94bc-4a18-9273-bc2976e7e3ee.png), where ![](img/54023dfc-394f-4c25-8c75-c12e92a9650c.png)
    is a deterministic initialization function; and ![](img/dae292b2-7070-4859-9119-f5f3a0271ccb.png)
    is a mutation power that determines the strength of the influence of all the subsequent
    parameter vectors on the initial parameters vector ![](img/9fd6b743-557c-43f2-9f3c-14ff0b5d0a4a.png).
  prefs: []
  type: TYPE_NORMAL
- en: In the current implementation, ![](img/2082ad39-de82-43c4-b07e-a9e5c6e0bea7.png) is
    a precomputed table with 250 million random vectors indexed using 28-bit seeds.
    This is done to speed up runtime processing because lookup by index is faster
    than the generation of new random numbers. Next, we'll discuss how to implement
    an encoding scheme in the Python source code.
  prefs: []
  type: TYPE_NORMAL
- en: Genome encoding scheme implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following source code implements ANN parameter estimations, as defined
    by the formula from the previous section (see the `compute_weights_from_seeds`
    function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `compute_mutation` function implements an estimation of the single step
    of the ANN parameter estimation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code takes the vector of the parent's trainable parameters and
    adds to it a random vector that's produced by a deterministic pseudo-random generator
    using a specific seed index. The mutation power parameter scales the generated
    random vector before it is added to the parent's parameters vector.
  prefs: []
  type: TYPE_NORMAL
- en: For more implementation details, refer to the `base.py` script at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/base.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/base.py).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll discuss the particulars of a simple genetic algorithm that's used
    to train the Frostbite playing agent.
  prefs: []
  type: TYPE_NORMAL
- en: The simple genetic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simple genetic algorithm that's used in our experiment evolves the population
    of *N* individuals over generations of evolution. As we mentioned previously,
    each individual genome encodes the vector of the trainable ANN parameters. Also,
    in each generation, we select the top *T* individuals to become the parents for
    the next generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of producing the next generation is implemented as follows. For
    *N-1* repetitions, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The parent is selected uniformly at random and removed from the selection list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The mutation is applied to the selected parent by applying additive Gaussian
    noise to the parameter vector that's encoded by the individual.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we add the new organism to the list of individuals for the next generation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, the best individual from the current generation is copied in its
    unmodified state to the next generation (elitism). To guarantee that the best
    individual was selected, we evaluate each of the 10 top individuals in the current
    generation against 30 additional game episodes. The individual with the highest
    mean fitness score is then selected as an elite to be copied to the next generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mutation of the parent individual is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This function receives the phenotype and genotype of the parent individual,
    the random source, along with the precomputed noise table (250 million vectors),
    and the mutation power value. The random source produces the random seed number
    (`idx`), which is used as an index so that we can select the appropriate parameters
    vector from the noise table. After that, we create the offspring genome by combining
    the list of parent seeds with a new seed. Finally, we create the phenotype of
    the offspring by combining the phenotype of the parent with Gaussian noise that's
    been extracted from the shared noise table using a randomly sampled seed index
    we obtained earlier (`idx`). In the next section, we will look at an experiment
    we can perform in order to train an agent to play the Frostbite Atari game.
  prefs: []
  type: TYPE_NORMAL
- en: Training an agent to play the Frostbite game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have discussed the theory behind the game-playing agent''s implementation,
    we are ready to start working on it. Our implementation is based on the source
    code provided by the Uber AI Lab on GitHub at [https://github.com/uber-research/deep-neuroevolution](https://github.com/uber-research/deep-neuroevolution).
    The source code in this repository contains an implementation of two methods to
    train DNNs: the CPU-based methods for multicore systems (up to 720 cores) and
    the GPU-based methods. We are interested in the GPU-based implementation because
    the majority of practitioners don''t have access to such behemoths of technology
    as a PC with 720 CPU cores. At the same time, it is pretty easy to get access
    to a modern Nvidia GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll discuss the implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: Atari Learning Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During agent training, we need to simulate actual gameplay in the Atari system.
    This can be done using the ALE, which simulates an Atari system that can run ROM
    images of the games. The ALE provides an interface that allows us to capture game
    screen frames and control the game by emulating the game controller. Here, we'll
    use the ALE modification that's available at [https://github.com/yaricom/atari-py](https://github.com/yaricom/atari-py).
  prefs: []
  type: TYPE_NORMAL
- en: Our implementation uses the TensorFlow framework to implement ANN models and
    execute them on the GPU. Thus, the corresponding bridge needs to be implemented
    between ALE and TensorFlow. This is done by implementing a custom TensorFlow operation
    using the C++ programming language for efficiency. The corresponding Python interface
    is also provided as an AtariEnv Python class at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py).
  prefs: []
  type: TYPE_NORMAL
- en: AtariEnv provides functions so that we can execute a single game step, reset
    the game, and return the current game state (observation). Next, we'll discuss
    each function.
  prefs: []
  type: TYPE_NORMAL
- en: The game step function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The game step function executes a single game step using the provided actions.
    It is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This function applies the game action that''s received from the controller
    ANN to the current game environment. Please note that this function can execute
    a single game step simultaneously in multiple game instances. The `self.batch_size`
    parameter or the length of the `indices` input tensor determines the number of
    game instances we''ll have. The function returns two tensors: one tensor with
    rewards (game score) and another with flags indicating whether the current game
    evaluation is complete after this step (solved or failed). Both tensors have a
    length equal to `self.batch_size` or the length of the `indices` input tensor.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll discuss how the game observations are created.
  prefs: []
  type: TYPE_NORMAL
- en: The game observation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function obtains the current game state from the Atari environment as
    a game screen buffer. This function is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This function acquires a screengrab from the Atari environment and wraps it
    in a tensor that can be used by the TensorFlow framework. The game observation
    function also allows us to receive the state from multiple games, which is determined
    either by the `self.batch_size` parameter or the length of the `indices` input
    parameter. The function returns screengrabs from multiple games, wrapped in a
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to implement the function to reset the Atari environment to the
    initial random state, which we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: The reset Atari environment function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train game agents, we need to implement a function that starts the Atari
    environment from a particular random state. It is vital to implement a stochastic
    Atari reset function to guarantee that our agent can play the game from any initial
    state. The function is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This function uses the `indices` input parameter to simultaneously reset multiple
    instances of Atari games in the random initial states. This function also defines
    the maximum number of frames for each game instance.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll discuss how RL evaluation is performed on GPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: RL evaluation on GPU cores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our experiment, we'll implement an RL evaluation using the TensorFlow framework
    on GPU devices. This means that all the calculations related to the propagation
    of input signals through the controller ANN are performed on the GPU. This allows
    us to effectively calculate more than 4 million training parameters – the connection
    weights between control ANN nodes – for every single time step of the game. Furthermore,
    we can concurrently simulate multiple runs of the game in parallel, each controlled
    by a different controller ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concurrent evaluation of multiple game controller ANNs is implemented by
    two Python classes: `RLEvalutionWorker` and `ConcurrentWorkers`. Next, we''ll
    discuss each class.'
  prefs: []
  type: TYPE_NORMAL
- en: For complete implementation details, refer to the `concurrent_worker.py` class
    at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/concurrent_worker.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/concurrent_worker.py).
  prefs: []
  type: TYPE_NORMAL
- en: The RLEvalutionWorker class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This class holds the configuration and the network graph of the controller ANN.
    It provides us with methods so that we can create a network graph of the controller
    ANN, run an evaluation loop over the created network graph, and put new tasks
    into the evaluation loop. Next, we'll discuss how the network graph is created
    from a network model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the network graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TensorFlow network graph is created by the `make_net` function, which receives
    the ANN model constructor, the GPU device identifier, and the batch size as input
    parameters. The network graph is created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by creating the controller ANN model and the game evaluation environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll create placeholders so that we can receive values during network
    graph evaluation. Also, we''ll create an operator to reset the game before the
    start of the new game episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, using the context of the provided GPU device, we''ll create two
    operators to receive game state observations and evaluate the game actions that
    follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `action` operator returns an array of action likelihood values, which needs
    to be filtered if the action space is discrete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The code checks whether the current game environment requires discrete actions
    and wraps an `action` operator using the built-in `tf.argmax` operator of the
    TensorFlow framework. The `tf.argmax` operator returns the index of the action
    with the largest value, which can be used to signal that a specific game action
    should be executed.
  prefs: []
  type: TYPE_NORMAL
- en: The Atari game environment is a discrete action environment, which means that
    only one action is accepted at each time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create the operator to perform a single game step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create a single game step operator, which returns operations to obtain
    rewards, `self.rew_op`, and the game completed status, `self.done_op`, after the
    execution of a single game step.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll discuss how the evaluation loop is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: The graph evaluation loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the loop that we use to evaluate the previously created network graph
    over multiple games in parallel – the number of games that can be evaluated simultaneously
    is determined by the `batch_size` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation loop is defined in the `_loop` function and is implemented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start with the creation of arrays to hold game evaluation values
    over multiple episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we start the loop and set the corresponding indices of the running array
    we just created to `True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the indices array, we are ready to execute a single game step operator
    and collect the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to test whether any of the evaluated games are done, either
    by winning or by hitting the maximum game frames limit. For all the completed
    tasks, we apply a number of operations, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses the indices of all the completed tasks and invokes the
    corresponding registered callbacks before resetting the collector variables at
    specific indices.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to discuss how to add and run the new task using our worker.
  prefs: []
  type: TYPE_NORMAL
- en: The asynchronous task runner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function registers a specific task to be evaluated by a worker in the
    GPU device context. It takes the ID of the task, the task object holder, and the
    callback to be executed on task completion as input. This function is defined
    under the name `run_async` and is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, it extracts the corresponding data from the task object and loads it
    into the current TensorFlow session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, `theta` is an array with all the connection weights in the controller
    ANN model, `extras` holds a random seeds list of the corresponding genome, and `max_frames`
    is the game frame's cutoff value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we run the TensorFlow session with `self.reset_op`, which resets a specific
    game environment at a specified index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The code runs `self.reset_op` within the TensorFlow session. Also, we register
    the current task identifier with the `reset` operator and the maximum game frame's
    cutoff value for a given task. The task identifier is used in the evaluation loop
    to associate evaluation results of the network graph with a specific genome in
    the population. Next, we'll discuss how concurrent asynchronous workers are maintained.
  prefs: []
  type: TYPE_NORMAL
- en: The ConcurrentWorkers class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `ConcurrentWorkers` class holds the configuration of the concurrent execution
    environment, which includes several evaluation workers (`RLEvalutionWorker` instances)
    and auxiliary routines to support multiple executions of concurrent tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the evaluation workers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the primary responsibilities of the `ConcurrentWorkers` class is to
    create and manage `RLEvalutionWorker` instances. This is done in the class constructor
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create the number of `RLEvalutionWorker` instances that are correlated
    to the number of GPU devices that are available in the system. After that, we
    initialize the selected ANN graph model and create auxiliary routines to manage
    multiple executions of asynchronous tasks. Next, we'll discuss how work tasks
    are scheduled for execution.
  prefs: []
  type: TYPE_NORMAL
- en: Running work tasks and monitoring results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use the RL evaluation mechanism we described earlier, we need a method to
    schedule the work task for evaluation and to monitor the results. This is implemented
    in the `monitor_eval` function, which receives the list of genomes in the population
    and evaluates them against the Atari game environment. This function has two essential
    implementation parts, both of which we''ll discuss in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we iterate over all the genomes in the list and create the asynchronous
    work task so that each genome can be evaluated against the Atari game environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code schedules each genome in the list for asynchronous evaluation
    and saves a reference to each asynchronous task for later use. Also, we periodically
    output the results of the evaluation process of already scheduled tasks. Now,
    we'll discuss how to monitor the evaluation results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block is waiting for the completion of asynchronous tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we iterate over all the references to the scheduled asynchronous tasks
    and wait for their completion. Also, we periodically output the evaluation progress.
    Next, we'll discuss how task evaluation results are collected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, after the completion of all tasks, we collect the results, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The code iterates through all the references to the scheduled asynchronous tasks
    and creates a list of the evaluation results. Next, we'll discuss the experiment
    runner implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment runner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The experiment runner implementation receives a configuration of the experiment
    defined in the JSON file and runs the neuroevolution process for the specified
    number of game time steps. In our experiment, the evaluation stops after reaching
    1.5 billion time steps of Frostbite. Next, we'll discuss the experiment configuration
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Experiment configuration file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the file that provides configuration parameters for the experiment
    runner. For our experiment, it has the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `game` parameter is the name of the game, as registered in the ALE. The
    full list of supported games is available at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `model` parameter designates the name of the network graph model to use
    for the construction of the controller ANN. The models are defined at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `num_validation_episodes` parameter defines how many game episodes are used
    for the evaluation of the top individuals in the population. After this step,
    we can select the true elite of the population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `num_test_episodes` parameter sets the number of game episodes to use to
    test the performance of the selected population elite.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `population_size` parameter determines the number of genomes in the population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `episode_cutoff_mode` parameter defines how the game evaluation stops for
    a particular genome. The game episode can stop either upon the execution of a
    particular number of time steps or by using the default stop signal of the corresponding
    game environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `timesteps` parameter sets the total number of time steps of the game to
    be executed during the neuroevolution process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `validation_threshold` parameter sets the number of top individuals that
    are selected from each generation for additional validation execution. The population
    elite is selected from these selected individuals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mutation_power` parameter defines how subsequent mutations that are added
    to the individual influence the training parameters (connection weights).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `selection_threshold` parameter determines how many parent individuals are
    allowed to produce offspring in the next generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we are ready to discuss the implementation details of the experiment runner.
  prefs: []
  type: TYPE_NORMAL
- en: The experiment configuration file can be found at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/configurations/ga_atari_config.json](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/configurations/ga_atari_config.json).
  prefs: []
  type: TYPE_NORMAL
- en: Experiment runner implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The experiment runner implementation creates the concurrent evaluation environment
    and runs the evolution loop over the population of individuals. Let''s discuss
    the essential implementation details:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by setting up the evaluation environment by loading the controller
    ANN model and creating the concurrent workers to execute the evaluation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a table with random noise values, which will be used as random
    seeds, and define the function to create offspring for the next generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, the main evolution loop starts. We use the previously defined function
    to create a population of the offspring for the current generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create the work tasks for each offspring in the population and schedule
    each task for evaluation against the game environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we finish with the evaluation of each individual in the population, we
    start the evaluation of the top individuals to select the elite:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the evaluation results of the top 10 individuals, we select the population''s
    elite and execute the final test runs over it to evaluate its performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The elite individual will be copied to the next generation as is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we select the top individuals from the current population to become
    parents for the next generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code collects the top individuals from the population to become
    the parents of the next generation. Also, it appends the current elite to the
    list of parents if it isn't in the parent list.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to discuss how to run the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Frostbite Atari experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have discussed all the particulars of the experiment's implementation,
    it is time to run the experiment. However, the first thing we need to do is create
    an appropriate work environment, which we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the work environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The work environment for training the agent to play Atari games assumes that
    a large controller ANN needs to be trained in the process. We already stated that
    the controller ANN has more than 4 million training parameters and requires a
    lot of computational resources to be able to evaluate. Fortunately, modern GPU
    accelerators allow the execution of massive parallel computations simultaneously.
    This feature is convenient for our experiment because we need to evaluate each
    individual against the game environment multiple times during the evolution process.
    Without GPU acceleration, it would either take a lot of time or require a massive
    number of processing cores (about 720).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss how to prepare the working environment:'
  prefs: []
  type: TYPE_NORMAL
- en: The working environment requires a Nvidia video accelerator (such as GeForce
    1080Ti) present in the system and the appropriate Nvidia CUDA SDK installed. More
    details about the CUDA SDK and its installation can be found at [https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we need to make sure that the CMake build tool is installed, as described
    at [https://cmake.org](https://cmake.org).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we need to create a new Python environment using Anaconda and install
    all the dependencies that are used by the experiment''s implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: These commands create and activate a new Python 3.5 environment. Next, it installs
    TensorFlow, OpenAI Gym, and the Python Imaging Library as dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, you need to clone the repository with the experiment''s source
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: After executing these commands, our current working directory becomes the directory
    that contains the experiment source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to build the ALE and integrate it into our experiment. We need
    to clone the ALE repository into the appropriate directory and build it with the
    following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a working ALE environment that's been integrated with TensorFlow.
    We can use it to evaluate the controller ANNs that are produced from a population
    of genomes against an Atari game (Frostbite, in our experiment).
  prefs: []
  type: TYPE_NORMAL
- en: 'After the ALE integration is complete, we need to build an integration between
    OpenAI Gym and TensorFlow that is specific to our experiment implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a fully defined work environment and we are ready to start our
    experiments. Next, we'll discuss how to run the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Running the experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With an adequately defined work environment, we are ready to start our experiment.
    You can start an experiment from the `Chapter10` directory by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command starts an experiment using the configuration file that
    was provided as the first parameter. The experiment's output will be stored in
    the `out` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing the experiment, the console''s output should look similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have the statistics output after a specific generation of evolution.
    You can see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum reward score that's achieved during the evaluation of a population
    is 3,470 (`PopulationEpRewMax`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum score that's achieved among the top individuals on an additional
    30 episodes of validation is 3,240 (`TruncatedPopulationRewMean`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean score of the top individual's evaluation is 2,360 (`TruncatedPopulationValidationRewMean`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean score of the elite individual that's received during an additional
    200 test runs is 3,060 (`TruncatedPopulationEliteTestRewMean`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The achieved reward scores are pretty high compared to other training methods
    if we look at the results that were published at [https://arxiv.org/abs/1712.06567v3](https://arxiv.org/abs/1712.06567v3).
  prefs: []
  type: TYPE_NORMAL
- en: Also, at the end of the outputs, you can see the genome representation of the
    population elite. The elite genome can be used to visualize playing Frostbite
    by the phenotype ANN that was created from it. Next, we'll discuss how to make
    this visualization possible.
  prefs: []
  type: TYPE_NORMAL
- en: Frostbite visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the results of the game agent''s training, it will be interesting
    to see how the solution we''ve found plays Frostbite in the Atari environment.
    To run the simulation, you need to copy the current elite genome representation
    from the output and paste it into the `seeds` field of the `display.py` file.
    After that, the simulation can be run using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command uses the provided elite genome to create a phenotype
    ANN and uses it as the controller of the Frostbite playing agent. It will open
    the game window, where you can see how the controller ANN is performing. The game
    will continue until the game character doesn''t have any lives left. The following
    image shows several captured game screens from the execution of `display.py` in
    an Ubuntu 16.04 environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4591a38f-4a6a-40df-a39a-29179b4edfeb.png)'
  prefs: []
  type: TYPE_IMG
- en: Frostbite screenshots, all of which have been taken from the elite genome game-playing
    session
  prefs: []
  type: TYPE_NORMAL
- en: It is pretty amazing to see how the trained controller ANN can learn the game
    rules solely from visual observations and is able to demonstrate such smooth gameplay.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll discuss an additional visualization method that allows us to analyze
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Visual inspector for neuroevolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the neuroevolution process, we are evolving a population of individuals.
    Each of the individuals is evaluated against the test environment (such as an
    Atari game) and reward scores are collected per individual for each generation
    of evolution. To explore the general dynamics of the neuroevolution process, we
    need to have a tool that can visualize the cloud of results for each individual
    in each generation of evolution. Also, it is interesting to see the changes in
    the fitness score of the elite individual to understand the progress of the evolution
    process.
  prefs: []
  type: TYPE_NORMAL
- en: To address these requirements, the researchers from Uber AI developed the VINE
    tool, which we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the work environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use the VINE tool, we need to install additional libraries into our virtual
    Python environment with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: These commands install all the necessary dependencies into a virtual Python
    environment that we have created for our experiment. Next, we'll discuss how to
    use the VINE tool.
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget to activate the appropriate virtual environment with the following
    command before running the preceding commands: `conda activate deep_ne`.
  prefs: []
  type: TYPE_NORMAL
- en: Using VINE for experiment visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, when we have all the dependencies installed on the Python virtual environment,
    we are ready to use the VINE tool. First, you need to clone it from the Git repository
    with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here, we cloned the deep neuroevolution repository into the current directory
    and changed the directory to the `visual_inspector` folder, where the source code
    of the VINE tool is present.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss how VINE can be used to visualize the results of the neuroevolution
    experiment using the results of the Mujoco Humanoid experiment provided by the
    Uber AI Lab. More details about the Mujoco Humanoid experiment can be found at
    [https://eng.uber.com/deep-neuroevolution/](https://eng.uber.com/deep-neuroevolution/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can run the visualization of the Mujoco Humanoid experiment results,
    which is supplied in the `sample_data` folder, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command uses the same data that was supplied by Uber AI Lab from
    their experiment for training humanoid locomotion and displays the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4769f1a-6ceb-4ddd-89d4-ad8f3d360e3e.png)'
  prefs: []
  type: TYPE_IMG
- en: The VINE tool's visualization of Humanoid locomotion results
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side of the graph, you can see the cloud of results for each
    individual in the population, starting from generation `90` and ending at generation
    `99`. On the right-hand side of the graph, you can see the fitness score of the
    population's elite per generation. In the right-hand graph, you can see that the
    evolutionary process demonstrates the positive dynamics from generation to generation
    as the fitness score of the elite is increasing.
  prefs: []
  type: TYPE_NORMAL
- en: Each point on the left-hand side graph demonstrates the behavioral characterization
    points for each individual in a population. The behavioral characterization for
    the Humanoid locomotion task is the final position of the Humanoid at the end
    of the trajectory. The farther it is from the origin coordinates (`0,0`), the
    higher the fitness score of the individual. You can see that, with the progress
    of evolution, the results cloud is moving away from the origin coordinates. This
    movement of the results cloud is also a signal of the positive learning dynamics
    because each individual was able to stay balanced for a more extended period.
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the Mujoco Humanoid locomotion experiment, please refer
    to the article at [https://eng.uber.com/deep-neuroevolution/](https://eng.uber.com/deep-neuroevolution/).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try to increase the `population_size` parameter in the experiment and see what
    happens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to create the experiment results, which can be visualized using VINE. You
    can use the  `master_extract_parent_ga` and `master_extract_cloud_ga` helper functions
    in the `ga.py` script to do this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how neuroevolution can be used to train large
    ANNs with more than 4 million trainable parameters. You learned how to apply this
    learning method to create successful agents that are able to play classic Atari
    games by learning the game rules solely from observing the game screens. By completing
    the Atari game-playing experiment that was described in this chapter, you have
    learned about CNNs and how they can be used to map high-dimensional inputs, such
    as game screen observations, into the appropriate game actions. You now have a
    solid understanding of how CNNs can be used for value-function approximations
    in the deep RL method, which is guided by the deep neuroevolution algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: With the knowledge that you've acquired from this chapter, you will be able
    to apply deep neuroevolution methods in domains with high-dimensional input data,
    such as inputs that have been acquired from cameras or other image sources.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll summarize what we have covered in this book and provide
    some hints about where you can continue your self-education.
  prefs: []
  type: TYPE_NORMAL
