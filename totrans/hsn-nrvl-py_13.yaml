- en: Deep Neuroevolution
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经进化
- en: In this chapter, you will learn about the deep neuroevolution method, which
    can be used to train **Deep Neural Networks** (**DNNs**). DNNs are conventionally
    trained using backpropagation methods based on the descent of the error gradient,
    which is computed with respect to the weights of the connections between neural
    nodes. Although gradient-based learning is a powerful technique that conceived
    the current era of deep machine learning, it has its drawbacks, such as long training
    times and enormous computing power requirements.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解深度神经进化方法，该方法可用于训练**深度神经网络（DNN**）。DNN传统上使用基于误差梯度下降的逆传播方法进行训练，该误差梯度是根据神经节点之间连接的权重计算的。尽管基于梯度的学习是一种强大的技术，它构思了当前深度机器学习的时代，但它也有其缺点，例如训练时间长和计算能力要求巨大。
- en: In this chapter, we will demonstrate how deep neuroevolution methods can be
    used for reinforcement learning and how they considerably outperform traditional
    DQN, A3C gradient-based learning methods of training DNNs. By the end of this
    chapter, you will have a solid understanding of deep neuroevolution methods, and
    you'll also have practical experience with them. We will learn how to evolve agents
    so that they can play classic Atari games using deep neuroevolution. Also, you
    will learn how to use **Visual Inspector for NeuroEvolution** (**VINE**) to examine
    the results of experiments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示如何使用深度神经进化方法进行强化学习，以及它们如何显著优于传统的DQN、A3C基于梯度的DNN训练学习方法。到本章结束时，你将深入理解深度神经进化方法，并且将获得实际操作经验。我们将学习如何通过深度神经进化进化智能体，使其能够玩经典的Atari游戏。此外，你还将学习如何使用**神经进化视觉检查器（VINE**）来检查实验结果。
- en: 'In this chapter, we''ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Deep neuroevolution for deep reinforcement learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经进化用于深度强化学习
- en: Evolving agents to play Frostbite Atari games using deep neuroevolution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度神经进化进化智能体以玩Frostbite Atari游戏
- en: Training an agent to play the Frostbite game
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练智能体玩Frostbite游戏
- en: Running the Frostbite Atari experiment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Frostbite Atari实验
- en: Examining the results with VINE
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用VINE检查结果
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following technical requirements should be met so that you can complete
    the experiments described in this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本章中描述的实验，以下技术要求必须满足：
- en: A modern PC with a Nvidia graphics accelerator GeForce GTX 1080Ti or better
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台配备Nvidia显卡加速器GeForce GTX 1080Ti或更好的现代PC
- en: MS Windows 10, Ubuntu Linux 16.04, or macOS 10.14 with a discrete GPU
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MS Windows 10、Ubuntu Linux 16.04或macOS 10.14，并配备离散GPU
- en: Anaconda Distribution version 2019.03 or newer
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda Distribution版本2019.03或更高版本
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter10)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter10)找到
- en: Deep neuroevolution for deep reinforcement learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经进化用于深度强化学习
- en: In this book, we have already covered how the neuroevolution method can be applied
    to solve simple **reinforcement learning** (**RL**) tasks, such as single- and
    double-pole balancing in [Chapter 4](34913ccd-6aac-412a-8f54-70d1900cef41.xhtml),
    *Pole-Balancing Experiments*. However, while the pole-balancing experiment is
    exciting and easy to conduct, it is pretty simple and operates with tiny artificial
    neural networks. In this chapter, we will discuss how to apply neuroevolution
    to reinforcement learning problems that require immense ANNs to approximate the
    value function of the RL algorithm.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经介绍了神经进化方法如何应用于解决简单的强化学习任务，例如在第4章中提到的单杆和双杆平衡实验，*杆平衡实验*。然而，尽管杆平衡实验令人兴奋且易于进行，但它相当简单，并且使用的是微型人工神经网络。在本章中，我们将讨论如何将神经进化应用于需要巨大人工神经网络来近似强化学习算法价值函数的强化学习问题。
- en: 'The RL algorithm learns through trial and error. Almost all the variants of
    RL algorithms try to optimize the value function, which maps the current state
    of the system to the appropriate action that will be performed in the next time
    step. The most widely used classical version of the RL algorithm uses a Q-learning
    method that is built around a table of states keyed by actions, which constitute
    the policy rules to be followed by the algorithm after training is complete. The
    training consists of updating the cells of the Q-table by iteratively executing
    specific actions at particular states and collecting the reward signals afterward.
    The following formula determines the process of updating a particular cell in
    a Q-table:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RL算法通过试错来学习。几乎所有的RL算法变体都试图优化值函数，该函数将系统的当前状态映射到下一个时间步将执行的正确动作。最广泛使用的经典RL算法版本是Q学习，它围绕一个由动作键入的状态表构建，该表构成了算法在训练完成后要遵循的策略规则。训练包括在特定状态下迭代执行特定动作，并在之后收集奖励信号，以更新Q表的单元格。以下公式决定了更新Q表中特定单元格的过程：
- en: '![](img/c2b89378-75ed-435b-9129-bb0942dd3642.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2b89378-75ed-435b-9129-bb0942dd3642.png)'
- en: Here ![](img/a8ce4769-6a5a-41fa-a659-5774d19f3608.png) is the reward that's
    received when the system state changes from state ![](img/fb3f1125-08a8-4221-aec4-8da38d84ef4c.png)
    to state ![](img/5af7a49b-2ffc-49c6-9b42-9a2c41d56fc1.png), ![](img/d747ca3a-3547-4da6-a5b1-9093ac311bb4.png)
    is the action taken at time ![](img/038e002e-5dc9-45cf-8613-e9f369d1dd4e.png)
    leading to the state change, ![](img/8d7ff50c-3187-487b-bcd8-44f13acbe3e1.png)
    is the learning rate, and ![](img/6d098dd0-38e8-4639-b80d-abdd4c795fe7.png) is
    a discount factor that controls the importance of the future rewards. The learning
    rate determines to what extent the new information overrides existing information
    in the specific Q-table cell. If we set the learning rate to zero, then nothing
    will be learned, and if we set it to *1*, then nothing will be retained. Thus,
    the learning rate controls how fast the system is able to learn new information
    while maintaining useful, already-learned data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![奖励](img/a8ce4769-6a5a-41fa-a659-5774d19f3608.png) 是当系统状态从状态 ![初始状态](img/fb3f1125-08a8-4221-aec4-8da38d84ef4c.png)
    变化到状态 ![目标状态](img/5af7a49b-2ffc-49c6-9b42-9a2c41d56fc1.png) 时所获得的奖励，![动作](img/d747ca3a-3547-4da6-a5b1-9093ac311bb4.png)
    是在时间 ![时间点](img/038e002e-5dc9-45cf-8613-e9f369d1dd4e.png) 所采取的动作，导致状态变化，![学习率](img/8d7ff50c-3187-487b-bcd8-44f13acbe3e1.png)
    是学习率，而![折扣因子](img/6d098dd0-38e8-4639-b80d-abdd4c795fe7.png) 是一个控制未来奖励重要性的折扣因子。学习率决定了新信息在特定Q表单元格中覆盖现有信息到何种程度。如果我们把学习率设为零，那么将不会学习任何东西，如果我们把它设为
    *1*，那么将不会保留任何东西。因此，学习率控制了系统学习新信息的同时保持有用、已学数据的速度。
- en: The simple version of the Q-learning algorithm iterates over all possible action-state
    combinations and updates the Q-values, as we've already discussed. This approach
    works pretty well for simple tasks with a small number of action-state pairs but
    quickly fails with an increase in the number of such pairs, that is, with increased
    dimensionality of the action-state space. Most real-world tasks have profound
    dimensionality of the action-state space, which makes it infeasible for the classical
    version of Q-learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习算法的简单版本迭代所有可能的动作-状态组合，并更新Q值，正如我们之前讨论的那样。这种方法对于具有少量动作-状态对的简单任务来说效果很好，但随着这种对的数量增加，即动作-状态空间的维度增加，这种方法很快就会失败。大多数现实世界任务都具有深刻的动作-状态空间维度，这使得经典版本的Q学习变得不可行。
- en: The method of Q-value function approximation was proposed to address the problem
    of increased dimensionality. In this method, the Q-learning policy is defined
    not by the action-state table we mentioned earlier but is instead approximated
    by a function. One of the ways to achieve this approximation is to use an ANN
    as a universal approximation function. By using an ANN, especially a deep ANN
    for Q-value approximation, it becomes possible to use RL algorithms for very complex
    problems, even for problems with a continuous state space. Thus, the DQN method was
    devised, which uses a DNN for Q-value approximation. The RL based on the DNN value-function
    approximation was named **deep reinforcement learning** (**deep RL**).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 提出Q值函数逼近方法是为了解决维度增加的问题。在这个方法中，Q学习策略不是由我们之前提到的动作-状态表定义的，而是通过一个函数来逼近。实现这种逼近的一种方法是用ANN作为通用逼近函数。通过使用ANN，特别是用于Q值逼近的深度ANN，使得使用RL算法解决非常复杂的问题成为可能，甚至可以解决具有连续状态空间的问题。因此，设计了DQN方法，它使用DNN进行Q值逼近。基于DNN值函数逼近的RL被称为**深度强化学习**（**深度RL**）。
- en: With deep RL, it is possible to learn action policies directly from the pixels
    of a video stream. This allows us to use a video stream to train agents to play
    video games, for example. However, the DQN method can be considered a gradient-based
    method. It uses error (loss) backpropagation in the DNN to optimize the Q-value
    function approximator. While being a potent technique, it has a significant drawback
    regarding the computational complexity that's involved, which requires the use
    of GPUs to perform all the matrix multiplications during the gradient descent-related
    computations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度强化学习，我们可以直接从视频流的像素中学习动作策略。这使得我们可以使用视频流来训练智能体玩电子游戏，例如。然而，DQN方法可以被认为是一种基于梯度的方法。它使用DNN中的误差（损失）反向传播来优化Q值函数近似器。虽然这是一种强大的技术，但它涉及到显著的计算复杂度，这需要使用GPU来执行梯度下降相关计算中的所有矩阵乘法。
- en: One of the methods that can be used to reduce computational costs is **Genetic
    Algorithms** (**GA**), such as neuroevolution. Neuroevolution allows us to evolve
    a DNN for the Q-value function approximation without any gradient-based computations
    involved. In recent studies, it was shown that gradient-free GA methods show excellent
    performance when it comes to challenging deep RL tasks and that they can even
    outperform their conventional counterparts. In the next section, we'll discuss
    how the deep neuroevolution method can be used to train successful agents to play
    one of the classic Atari games, just by reading game screen observations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用来减少计算成本的其中一种方法是**遗传算法**（**GA**），例如神经进化。神经进化允许我们在不涉及任何基于梯度的计算的情况下，进化一个用于Q值函数近似的DNN。在最近的研究中，已经表明，无梯度GA方法在挑战性的深度强化学习任务中表现出色，并且甚至可以超越它们的传统对应物。在下一节中，我们将讨论如何使用深度神经进化方法来训练成功的智能体，仅通过读取游戏屏幕观察来玩一款经典的Atari游戏。
- en: Evolving an agent to play the Frostbite Atari game using deep neuroevolution
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过深度神经进化使智能体学会玩Frostbite Atari游戏
- en: Recently, classic Atari games were encapsulated by the **Atari Learning Environment**
    (**ALE**) to become a benchmark for testing different implementations of RL algorithms.
    Algorithms that are tested against the ALE are required to read the game state
    from the pixels of the game screen and devise a sophisticated control logic that
    allows the agent to win the game. Thus, the task of the algorithm is to evolve
    an understanding of the game situation in terms of the game character and its
    adversaries. Also, the algorithm needs to understand the reward signal that's
    received from the game screen in the form of the final game score at the end of
    a single game run.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，经典的Atari游戏被封装在**Atari学习环境**（**ALE**）中，成为测试不同RL算法实现的基准。针对ALE测试的算法需要从游戏屏幕的像素中读取游戏状态，并设计复杂的控制逻辑，使智能体能够赢得游戏。因此，算法的任务是在游戏角色及其对手的背景下，演变对游戏情况的理解。此外，算法还需要理解从游戏屏幕接收到的奖励信号，这种信号以单次游戏运行结束时的最终游戏分数的形式出现。
- en: The Frostbite Atari game
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Frostbite Atari游戏
- en: 'Frostbite is a classic Atari game where you control a game character that is
    building an igloo. The game screen is shown in the following screenshot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Frostbite是一款经典的Atari游戏，玩家控制一个游戏角色，该角色正在建造一个冰屋。游戏屏幕在下面的屏幕截图中显示：
- en: '![](img/9ecfe780-eea0-4e30-b482-9a48c91801a5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Frostbite游戏屏幕](img/9ecfe780-eea0-4e30-b482-9a48c91801a5.png)'
- en: The Frostbite game screen
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Frostbite游戏屏幕
- en: The bottom part of the screen is water, with floating ice blocks arranged in
    four rows. The game character jumps from one row to another while trying to avoid
    various foes. If the game character jumps on a white ice block, this block is
    collected and used to build an igloo on the shore in the top right of the screen.
    After that, the white ice block changes its color and cannot be used anymore.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕底部是水，有浮冰块排列成四行。游戏角色在尝试避开各种敌人时从一个行跳到另一个行。如果游戏角色跳到一个白色冰块上，这个块就会被收集并用于在屏幕右上角的岸边建造一个冰屋。之后，白色冰块会改变颜色，不能再使用。
- en: To build the igloo, the game character must collect 15 ice blocks within 45
    seconds. Otherwise, the game ends because the game character becomes frozen. When
    the igloo is complete, the game character must move inside it to complete the
    current level. The faster the game character completes a level, the more bonus
    points are awarded to the player.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要建造冰屋，游戏角色必须在45秒内收集15个冰块。否则，游戏结束，因为游戏角色被冻住了。当冰屋完成时，游戏角色必须进入其中以完成当前关卡。游戏角色完成关卡越快，玩家获得的额外分数就越多。
- en: Next, we'll discuss how the game screen state is mapped into input parameters,
    which can be used by the neuroevolution method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何将游戏屏幕状态映射到输入参数，这些参数可以被神经进化方法使用。
- en: Game screen mapping into actions
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏屏幕映射到动作
- en: Deep ANNs can be trained to play Atari games if they can directly map the pixels
    on the screen to a system to control the game. This means that our algorithm must
    read the game screen and decide what game action to take to get the highest game
    score possible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果深度ANN能够直接将屏幕上的像素映射到控制游戏系统的系统，它们就可以被训练来玩Atari游戏。这意味着我们的算法必须读取游戏屏幕并决定采取什么游戏动作以获得尽可能高的游戏分数。
- en: 'This task can be divided into two logical subtasks:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此任务可以分为两个逻辑子任务：
- en: The image analysis task, which encodes the state of the current game situation
    on the screen, including the game character's position, obstacles, and adversaries
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分析任务，该任务将当前游戏情况在屏幕上的状态进行编码，包括游戏角色的位置、障碍物和对手
- en: The RL training task, which is used to train the Q-value approximation ANN to
    build the correct mapping between the specific game state and actions to be performed
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL训练任务，用于训练Q值近似ANN以建立特定游戏状态与要执行的动作之间的正确映射
- en: '**Convolutional Neural Networks** (**CNNs**) are commonly used in tasks related
    to the analysis of visual imagery or other high-dimensional Euclidean data. The
    power of CNNs is based on their ability to significantly reduce the number of
    learning parameters compared to other types of ANN if they''re applied to visual
    recognition. The CNN hierarchy usually has multiple sequential convolutional layers
    combined with non-linear fully connected layers and ends with a fully connected
    layer that is followed by the loss layer. The final fully connected and loss layers
    implement the high-level reasoning in the Neural Network architecture. In the
    case of deep RL, these layers make the Q-value approximation. Next, we''ll consider
    the details of convolutional layer implementation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）在分析视觉图像或其他高维欧几里得数据相关的任务中常用。CNN的强大之处在于，如果应用于视觉识别，它们能够显著减少与其他类型的ANN相比的学习参数数量。CNN层次结构通常由多个顺序卷积层与非线性的全连接层结合，并以一个全连接层结束，该层之后是损失层。最终的全连接和损失层实现了神经网络架构中的高级推理。在深度RL的情况下，这些层实现了Q值近似。接下来，我们将考虑卷积层实现的细节。'
- en: Convolutional layers
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'By studying the organization of the visual cortex of higher life forms (including
    humans), researchers gained inspiration for the design of CNNs. Each neuron of
    the visual cortex responds to signals that are received from a limited region
    of the visual field – the neuron''s receptive field. The receptive fields of various
    neurons overlap partially, which allows them to cover the entire visual field, as
    shown in the following diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过研究高等生命形式（包括人类）的视觉皮层组织，研究人员为CNN的设计获得了灵感。视觉皮层的每个神经元对来自视觉场有限区域的信号做出反应——神经元的接收场。不同神经元的接收场部分重叠，这使得它们能够覆盖整个视觉场，如下面的图所示：
- en: '![](img/560f658d-543d-4b08-b4d3-269287909c3a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/560f658d-543d-4b08-b4d3-269287909c3a.png)'
- en: The scheme of connections between the receptive field (on the left) and neurons
    in the convolutional layer (on the right)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接收场（左侧）与卷积层（右侧）中的神经元之间的连接方案
- en: The convolutional layer consists of a column of neurons, where each neuron in
    a single column is connected to the same receptive field. This column represents
    a set of filters (kernels). Each filter is defined by the size of the receptive
    field and the number of channels. The number of channels defines the depth of
    the neurons column, while the size of the receptive field determines the number
    of columns in the convolutional layer. When the receptive field moves over the
    visual field, in each step, the new column of neurons is activated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层由一列神经元组成，其中每一列中的神经元都连接到相同的接收场。这一列代表了一组过滤器（核）。每个过滤器由接收场的大小和通道数定义。通道数定义了神经元列的深度，而接收场的大小决定了卷积层中的列数。当接收场在视觉场中移动时，在每一步，新的神经元列被激活。
- en: 'As we mentioned previously, each convolutional layer is usually combined with
    a fully connected layer with non-linear activation, such as **Rectified Linear
    Unit** (**ReLU**). The ReLU activation function has the effect of filtering out
    negative values, as given by the following formula:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，每个卷积层通常与非线性的激活函数结合，例如**Rectified Linear Unit**（**ReLU**）。ReLU激活函数的作用是过滤掉负值，如下公式所示：
- en: '![](img/09ad5aa4-e9d6-42f3-b871-e1377f38c153.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09ad5aa4-e9d6-42f3-b871-e1377f38c153.png)'
- en: Here, ![](img/0b414ba6-373b-4aba-a98a-d9fd256e41f4.png) is the input to a neuron.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/0b414ba6-373b-4aba-a98a-d9fd256e41f4.png)是神经元的输入。
- en: In the ANN architecture, several convolutional layers are connected to a number
    of fully connected layers, which performs high-level reasoning. Next, we'll discuss
    the CNN architecture that's used in our experiment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在ANN架构中，几个卷积层连接到多个完全连接层，执行高级推理。接下来，我们将讨论在我们的实验中使用的CNN架构。
- en: The CNN architecture to train the Atari playing agent
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练Atari游戏代理的CNN架构
- en: In our experiment, we'll use a CNN architecture consisting of three convolutional
    layers with 32, 64, and 64 channels, followed by a fully connected layer with
    512 units and the output layer with the number of layers corresponding to the
    number of game actions. The convolutional layers have 8 x 8, 4 x 4, and 3 x 3
    kernel sizes and use strides of 4, 2, and 1, respectively. The ReLU non-linearity
    follows all the convolutional and fully connected layers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将使用由三个卷积层组成的CNN架构，这些卷积层具有32、64和64个通道，随后是一个具有512个单元的完全连接层和与游戏动作数量相对应的输出层。卷积层的核大小分别为8
    x 8、4 x 4和3 x 3，分别使用步长为4、2和1。ReLU非线性函数跟随所有的卷积层和完全连接层。
- en: 'The source code to create the described network graph model using the TensorFlow
    framework is defined as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow框架创建描述的网络图模型的源代码定义如下：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As a result of this architecture, the CNN contains about **4 million trainable
    parameters**. Next, we'll discuss how RL training is done in our experiment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种架构，CNN包含大约**400万个可训练参数**。接下来，我们将讨论在我们的实验中如何进行RL训练。
- en: For complete implementation details, refer to the `dqn.py` Python script at
    [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整的实现细节，请参阅[https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py)中的`dqn.py`
    Python脚本。
- en: The RL training of the game agent
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏代理的RL训练
- en: The RL training in our experiment was implemented using the neuroevolution method.
    This method is based on a simple genetic algorithm that evolves a population of
    individuals. The genotype of each individual encodes the vector of the trainable
    parameters of the controller ANN. By trainable parameters, we mean the connection
    weights between the network nodes. In every generation, each genotype is evaluated
    against a test environment by playing Frostbite and produces a specific fitness
    score. We evaluate each agent (genome) against 20,000 frames of the game. During
    the evaluation period, the game character can play multiple times, and the final
    Atari game score is the fitness score, which is a reward signal in terms of RL.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验中的RL训练是使用神经进化方法实现的。这种方法基于一个简单的遗传算法，该算法进化一个个体群体。每个个体的基因型编码了控制器ANN的可训练参数向量。我们所说的可训练参数是指网络节点之间的连接权重。在每一代中，每个基因型通过在Frostbite中玩游戏来与测试环境进行评估，并产生一个特定的适应度分数。我们评估每个代理（基因组）对游戏的20,000帧。在评估期间，游戏角色可以玩多次，最终Atari游戏得分是适应度分数，这是RL中的奖励信号。
- en: Next, we'll discuss the genome encoding scheme, which allows us to encode more
    than 4 million learning parameters of the ANN that's controlling the game-solving
    agent.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论基因组编码方案，它允许我们编码控制游戏解决代理的ANN的超过400万学习参数。
- en: The genome encoding scheme
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基因组编码方案
- en: The deep RL neural network that we use as the controller of the game agent has
    about 4 million trainable parameters. Each trainable parameter is the weight of
    a connection between two nodes of the neural network. Traditionally, training
    neural networks is about finding the appropriate values of all the connection
    weights, allowing the neural network to approximate a function that describes
    the specifics of the modeled process.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用作游戏代理控制器的深度强化学习神经网络大约有400万个可训练参数。每个可训练参数是神经网络中两个节点之间连接的权重。传统上，训练神经网络是关于找到所有连接权重的适当值，使神经网络能够近似描述建模过程具体情况的函数。
- en: The conventional way to estimate these trainable parameters is to use some form
    of error backpropagation based on the gradient descent of the loss value, which
    is very computationally intensive. On the other hand, the neuroevolution algorithm
    allows us to train ANNs using a nature-inspired genetic algorithm. The neuroevolution
    algorithm applies a series of mutations and recombinations to the trainable parameters
    to find the correct configuration of the ANN. However, to use the genetic algorithm,
    an appropriate scheme of encoding of the phenotype ANN should be devised. After
    that, the population of individuals (genomes encoding the phenotype ANN) can be
    created and evolved using a simple genetic algorithm, which we'll discuss later.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的估计这些可训练参数的方法是使用基于损失值梯度下降的某种形式的误差反向传播，这非常计算密集。另一方面，神经进化算法允许我们使用一种受自然界启发的遗传算法来训练ANN。神经进化算法通过对可训练参数应用一系列突变和重组来找到ANN的正确配置。然而，要使用遗传算法，应设计一种适当的表型ANN编码方案。之后，可以使用简单的遗传算法创建和进化个体（编码表型ANN的基因组），我们将在后面讨论。
- en: As we mentioned earlier, the encoding scheme should produce compact genomes
    that can encode values of more than 4 million connection weights between the nodes
    of the deep RL ANN controlling the game agent. We are looking for compact genomes
    to reduce the computational costs associated with genetic algorithm evaluation.
    Next, we'll discuss the definition of the genome encoding scheme, which can be
    used to encode the large phenotype ANN.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，编码方案应生成紧凑的基因组，能够编码控制游戏代理的深度强化学习人工神经网络（ANN）节点之间超过400万个连接权重的值。我们正在寻找紧凑的基因组以降低与遗传算法评估相关的计算成本。接下来，我们将讨论基因组编码方案的定义，该方案可用于编码大型表型ANN。
- en: Genome encoding scheme definition
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基因组编码方案定义
- en: Researchers at the Uber AI lab have proposed a coding scheme that uses the seed
    of a pseudo-random number generator to encode the phenotype ANN. In this scheme,
    the genome is represented as a list of seed values, which is applied sequentially
    to generate the values for all the weights (trainable parameters) of connections
    that are expressed between the nodes of a controller ANN.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 优步AI实验室的研究人员提出了一种编码方案，该方案使用伪随机数生成器的种子来编码表型ANN。在这个方案中，基因组表示为种子值的列表，这些种子值依次应用以生成控制器ANN节点之间表达的所有连接（可训练参数）的值。
- en: 'In other words, the first seed value in the list represents the policy initialization
    seed, which is shared between the genealogy of the descendants of a single parent.
    All the subsequent seed values represent the specific mutations that are acquired
    during the evolution process by the offspring. Each seed is applied sequentially
    to produce the ANN parameters vector of a specific phenotype. The following formula
    defines the estimation of the phenotype parameters vector for a specific individual
    (![](img/13909b80-d25b-4184-b366-f96c9f975f53.png)):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，列表中的第一个种子值代表策略初始化种子，它被单个父代的谱系所共享。所有后续的种子值代表后代在进化过程中获得的具体突变。每个种子依次应用以产生特定表型的ANN参数向量。以下公式定义了特定个体的表型参数向量估计（![图片](img/13909b80-d25b-4184-b366-f96c9f975f53.png)）：
- en: '![](img/776be492-a072-455d-96d6-90bec14f7106.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/776be492-a072-455d-96d6-90bec14f7106.png)'
- en: Here, ![](img/1981550c-2382-4367-8347-2dc4254fdb36.png) is the encoding of ![](img/56b74879-2d2d-47a1-a0d7-9cdaab645845.png)
    and consists of a list of mutation seeds; ![](img/62ff51bb-f7d9-4625-a301-3a06360e0676.png) is
    a deterministic Gaussian pseudo-random number generator with an input seed ![](img/8bc7ccc1-51cf-4d57-945b-d8a09fca3658.png)
    that produces a vector of length ![](img/b118aca6-1e3a-452c-a1a2-ee497c040b41.png); ![](img/41c3bd93-95fa-45d5-8bc6-04ffdbcf35df.png)
    is an initial parameters vector that's created during initialization as follows,
    ![](img/bb4d1685-94bc-4a18-9273-bc2976e7e3ee.png), where ![](img/54023dfc-394f-4c25-8c75-c12e92a9650c.png)
    is a deterministic initialization function; and ![](img/dae292b2-7070-4859-9119-f5f3a0271ccb.png)
    is a mutation power that determines the strength of the influence of all the subsequent
    parameter vectors on the initial parameters vector ![](img/9fd6b743-557c-43f2-9f3c-14ff0b5d0a4a.png).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/1981550c-2382-4367-8347-2dc4254fdb36.png)是![](img/56b74879-2d2d-47a1-a0d7-9cdaab645845.png)的编码，由一系列突变种子组成；![](img/62ff51bb-f7d9-4625-a301-3a06360e0676.png)是一个具有输入种子![](img/8bc7ccc1-51cf-4d57-945b-d8a09fca3658.png)的确定性高斯伪随机数生成器，它产生一个长度为![](img/b118aca6-1e3a-452c-a1a2-ee497c040b41.png)的向量；![](img/41c3bd93-95fa-45d5-8bc6-04ffdbcf35df.png)是在初始化期间创建的初始参数向量，如下所示，![](img/bb4d1685-94bc-4a18-9273-bc2976e7e3ee.png)，其中![](img/54023dfc-394f-4c25-8c75-c12e92a9650c.png)是一个确定性初始化函数；而![](img/dae292b2-7070-4859-9119-f5f3a0271ccb.png)是突变能力，它决定了所有后续参数向量对初始参数向量![](img/9fd6b743-557c-43f2-9f3c-14ff0b5d0a4a.png)的影响强度。
- en: In the current implementation, ![](img/2082ad39-de82-43c4-b07e-a9e5c6e0bea7.png) is
    a precomputed table with 250 million random vectors indexed using 28-bit seeds.
    This is done to speed up runtime processing because lookup by index is faster
    than the generation of new random numbers. Next, we'll discuss how to implement
    an encoding scheme in the Python source code.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前实现中，![](img/2082ad39-de82-43c4-b07e-a9e5c6e0bea7.png)是一个使用28位种子索引的预计算表，包含2.5亿个随机向量。这样做是为了加快运行时处理速度，因为通过索引查找比生成新的随机数要快。接下来，我们将讨论如何在Python源代码中实现编码方案。
- en: Genome encoding scheme implementation
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基因编码方案实现
- en: 'The following source code implements ANN parameter estimations, as defined
    by the formula from the previous section (see the `compute_weights_from_seeds`
    function):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的源代码实现了按前一小节中定义的公式（参见`compute_weights_from_seeds`函数）进行的ANN参数估计：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `compute_mutation` function implements an estimation of the single step
    of the ANN parameter estimation, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_mutation`函数实现了ANN参数估计的单步估计，如下所示：'
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code takes the vector of the parent's trainable parameters and
    adds to it a random vector that's produced by a deterministic pseudo-random generator
    using a specific seed index. The mutation power parameter scales the generated
    random vector before it is added to the parent's parameters vector.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将父代可训练参数的向量与由确定性伪随机生成器使用特定种子索引产生的随机向量相加。突变能力参数在将其添加到父代参数向量之前对生成的随机向量进行缩放。
- en: For more implementation details, refer to the `base.py` script at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/base.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/base.py).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更多实现细节，请参阅[https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/base.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/base.py)中的`base.py`脚本。
- en: Next, we'll discuss the particulars of a simple genetic algorithm that's used
    to train the Frostbite playing agent.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论用于训练Frostbite游戏代理的简单遗传算法的细节。
- en: The simple genetic algorithm
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单遗传算法
- en: The simple genetic algorithm that's used in our experiment evolves the population
    of *N* individuals over generations of evolution. As we mentioned previously,
    each individual genome encodes the vector of the trainable ANN parameters. Also,
    in each generation, we select the top *T* individuals to become the parents for
    the next generation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中使用的简单遗传算法通过进化代数来进化*N*个个体的种群。正如我们之前提到的，每个个体的基因组编码了可训练的ANN参数向量。此外，在每一代中，我们选择前*T*个个体成为下一代的双亲。
- en: 'The process of producing the next generation is implemented as follows. For
    *N-1* repetitions, we do the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 产生下一代的流程如下。对于*N-1*次重复，我们执行以下操作：
- en: The parent is selected uniformly at random and removed from the selection list.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个父代并从选择列表中移除。
- en: The mutation is applied to the selected parent by applying additive Gaussian
    noise to the parameter vector that's encoded by the individual.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对个体编码的参数向量应用加性高斯噪声，将突变应用于选定的父个体。
- en: Next, we add the new organism to the list of individuals for the next generation.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将新的生物体添加到下一代的个体列表中。
- en: After that, the best individual from the current generation is copied in its
    unmodified state to the next generation (elitism). To guarantee that the best
    individual was selected, we evaluate each of the 10 top individuals in the current
    generation against 30 additional game episodes. The individual with the highest
    mean fitness score is then selected as an elite to be copied to the next generation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，将当前代最佳个体以未修改的状态复制到下一代（精英主义）。为了保证最佳个体被选中，我们将当前代的10个顶级个体与30个额外的游戏关卡进行评估。然后，平均适应度分数最高的个体被选为精英，复制到下一代。
- en: 'The mutation of the parent individual is implemented as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 父个体突变的具体实现如下：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function receives the phenotype and genotype of the parent individual,
    the random source, along with the precomputed noise table (250 million vectors),
    and the mutation power value. The random source produces the random seed number
    (`idx`), which is used as an index so that we can select the appropriate parameters
    vector from the noise table. After that, we create the offspring genome by combining
    the list of parent seeds with a new seed. Finally, we create the phenotype of
    the offspring by combining the phenotype of the parent with Gaussian noise that's
    been extracted from the shared noise table using a randomly sampled seed index
    we obtained earlier (`idx`). In the next section, we will look at an experiment
    we can perform in order to train an agent to play the Frostbite Atari game.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接收父个体的表型和基因型、随机源、预计算的噪声表（2.5亿个向量）以及突变功率值。随机源生成随机种子数（`idx`），用作索引，以便我们可以从噪声表中选择适当的参数向量。之后，我们通过将父种子列表与新的种子结合来创建后代基因组。最后，我们通过将父个体的表型与从共享噪声表中使用先前获得的随机采样种子索引（`idx`）提取的高斯噪声相结合来创建后代的表型。在下一节中，我们将探讨我们可以进行的实验，以训练一个能够玩Frostbite
    Atari游戏的智能体。
- en: Training an agent to play the Frostbite game
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练智能体玩Frostbite游戏
- en: 'Now that we have discussed the theory behind the game-playing agent''s implementation,
    we are ready to start working on it. Our implementation is based on the source
    code provided by the Uber AI Lab on GitHub at [https://github.com/uber-research/deep-neuroevolution](https://github.com/uber-research/deep-neuroevolution).
    The source code in this repository contains an implementation of two methods to
    train DNNs: the CPU-based methods for multicore systems (up to 720 cores) and
    the GPU-based methods. We are interested in the GPU-based implementation because
    the majority of practitioners don''t have access to such behemoths of technology
    as a PC with 720 CPU cores. At the same time, it is pretty easy to get access
    to a modern Nvidia GPU.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了游戏智能体实现背后的理论，我们现在可以开始着手工作了。我们的实现基于GitHub上Uber AI实验室提供的源代码，网址为[https://github.com/uber-research/deep-neuroevolution](https://github.com/uber-research/deep-neuroevolution)。该存储库中的源代码包含两种训练DNN的方法：适用于多核系统的基于CPU的方法（最多720个核心）和基于GPU的方法。我们感兴趣的是基于GPU的实现，因为大多数实践者无法访问拥有720个CPU核心的PC这样的巨型技术设备。同时，获取现代Nvidia
    GPU相当容易。
- en: Next, we'll discuss the implementation details.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论实现的细节。
- en: Atari Learning Environment
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari学习环境
- en: During agent training, we need to simulate actual gameplay in the Atari system.
    This can be done using the ALE, which simulates an Atari system that can run ROM
    images of the games. The ALE provides an interface that allows us to capture game
    screen frames and control the game by emulating the game controller. Here, we'll
    use the ALE modification that's available at [https://github.com/yaricom/atari-py](https://github.com/yaricom/atari-py).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能体训练期间，我们需要在Atari系统中模拟实际游戏玩法。这可以通过使用ALE来完成，它模拟了一个可以运行游戏ROM图像的Atari系统。ALE提供了一个接口，允许我们通过模拟游戏控制器来捕获游戏屏幕帧和控制游戏。在这里，我们将使用可在[https://github.com/yaricom/atari-py](https://github.com/yaricom/atari-py)找到的ALE修改版。
- en: Our implementation uses the TensorFlow framework to implement ANN models and
    execute them on the GPU. Thus, the corresponding bridge needs to be implemented
    between ALE and TensorFlow. This is done by implementing a custom TensorFlow operation
    using the C++ programming language for efficiency. The corresponding Python interface
    is also provided as an AtariEnv Python class at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现使用 TensorFlow 框架来实现 ANN 模型并在 GPU 上执行它们。因此，需要在 ALE 和 TensorFlow 之间实现相应的桥梁。这是通过使用
    C++ 编程语言实现自定义 TensorFlow 操作来实现的，以提高效率。还提供了相应的 Python 接口，作为 AtariEnv Python 类，在
    [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py)。
- en: AtariEnv provides functions so that we can execute a single game step, reset
    the game, and return the current game state (observation). Next, we'll discuss
    each function.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: AtariEnv 提供了函数，使我们能够执行单个游戏步骤、重置游戏并返回当前游戏状态（观察）。接下来，我们将讨论每个函数。
- en: The game step function
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏步骤函数
- en: 'The game step function executes a single game step using the provided actions.
    It is implemented as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏步骤函数使用提供的动作执行单个游戏步骤。该函数的实现如下：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This function applies the game action that''s received from the controller
    ANN to the current game environment. Please note that this function can execute
    a single game step simultaneously in multiple game instances. The `self.batch_size`
    parameter or the length of the `indices` input tensor determines the number of
    game instances we''ll have. The function returns two tensors: one tensor with
    rewards (game score) and another with flags indicating whether the current game
    evaluation is complete after this step (solved or failed). Both tensors have a
    length equal to `self.batch_size` or the length of the `indices` input tensor.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将控制器 ANN 接收到的游戏动作应用于当前游戏环境。请注意，此函数可以在多个游戏实例中同时执行单个游戏步骤。`self.batch_size`
    参数或 `indices` 输入张量的长度决定了我们将拥有的游戏实例数量。该函数返回两个张量：一个包含奖励（游戏得分）的张量，另一个包含标志，指示当前游戏评估在此步骤后是否完成（解决或失败）。这两个张量的长度等于
    `self.batch_size` 或 `indices` 输入张量的长度。
- en: Next, we'll discuss how the game observations are created.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论游戏观察是如何创建的。
- en: The game observation function
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏观察函数
- en: 'This function obtains the current game state from the Atari environment as
    a game screen buffer. This function is implemented as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数从 Atari 环境获取当前游戏状态作为游戏屏幕缓冲区。此函数的实现如下：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This function acquires a screengrab from the Atari environment and wraps it
    in a tensor that can be used by the TensorFlow framework. The game observation
    function also allows us to receive the state from multiple games, which is determined
    either by the `self.batch_size` parameter or the length of the `indices` input
    parameter. The function returns screengrabs from multiple games, wrapped in a
    tensor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数从 Atari 环境获取屏幕截图，并将其包装在 TensorFlow 框架可以使用的张量中。游戏观察函数还允许我们通过 `self.batch_size`
    参数或 `indices` 输入参数的长度接收来自多个游戏的州。该函数返回多个游戏的屏幕截图，包装在张量中。
- en: We also need to implement the function to reset the Atari environment to the
    initial random state, which we'll discuss next.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还需要实现一个函数，将 Atari 环境重置到初始随机状态，我们将在下一节中讨论。 '
- en: The reset Atari environment function
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重置 Atari 环境函数
- en: 'To train game agents, we need to implement a function that starts the Atari
    environment from a particular random state. It is vital to implement a stochastic
    Atari reset function to guarantee that our agent can play the game from any initial
    state. The function is implemented as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练游戏智能体，我们需要实现一个函数，从特定的随机状态启动 Atari 环境。实现一个随机的 Atari 重置函数对于确保我们的智能体可以从任何初始状态玩游戏至关重要。该函数的实现如下：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function uses the `indices` input parameter to simultaneously reset multiple
    instances of Atari games in the random initial states. This function also defines
    the maximum number of frames for each game instance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数使用 `indices` 输入参数同时重置多个 Atari 游戏实例的随机初始状态。此函数还定义了每个游戏实例的最大帧数。
- en: Next, we'll discuss how RL evaluation is performed on GPU cores.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何在 GPU 核心上执行 RL 评估。
- en: RL evaluation on GPU cores
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 GPU 核心上进行 RL 评估
- en: In our experiment, we'll implement an RL evaluation using the TensorFlow framework
    on GPU devices. This means that all the calculations related to the propagation
    of input signals through the controller ANN are performed on the GPU. This allows
    us to effectively calculate more than 4 million training parameters – the connection
    weights between control ANN nodes – for every single time step of the game. Furthermore,
    we can concurrently simulate multiple runs of the game in parallel, each controlled
    by a different controller ANN.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将使用 TensorFlow 框架在 GPU 设备上实现一个强化学习评估。这意味着与控制器 ANN 中输入信号传播相关的所有计算都在
    GPU 上执行。这使得我们能够有效地计算超过 400 万个训练参数——控制 ANN 节点之间的连接权重——对于游戏中的每一个时间步。此外，我们还可以并行模拟多个游戏运行，每个运行由不同的控制器
    ANN 控制。
- en: 'The concurrent evaluation of multiple game controller ANNs is implemented by
    two Python classes: `RLEvalutionWorker` and `ConcurrentWorkers`. Next, we''ll
    discuss each class.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过两个 Python 类 `RLEvalutionWorker` 和 `ConcurrentWorkers` 实现多个游戏控制器 ANN 的并发评估。接下来，我们将讨论每个类。
- en: For complete implementation details, refer to the `concurrent_worker.py` class
    at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/concurrent_worker.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/concurrent_worker.py).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整的实现细节，请参阅 [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/concurrent_worker.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/concurrent_worker.py)
    中的 `concurrent_worker.py` 类。
- en: The RLEvalutionWorker class
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLEvalutionWorker 类
- en: This class holds the configuration and the network graph of the controller ANN.
    It provides us with methods so that we can create a network graph of the controller
    ANN, run an evaluation loop over the created network graph, and put new tasks
    into the evaluation loop. Next, we'll discuss how the network graph is created
    from a network model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此类包含控制器 ANN 的配置和网络图。它为我们提供了方法，以便我们可以创建控制器 ANN 的网络图，在创建的网络图中运行评估循环，并将新任务放入评估循环中。接下来，我们将讨论网络图是如何从网络模型创建的。
- en: Creating the network graph
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建网络图
- en: 'The TensorFlow network graph is created by the `make_net` function, which receives
    the ANN model constructor, the GPU device identifier, and the batch size as input
    parameters. The network graph is created as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 网络图是由 `make_net` 函数创建的，该函数接收 ANN 模型构造函数、GPU 设备标识符和批处理大小作为输入参数。网络图的创建如下：
- en: 'We''ll start by creating the controller ANN model and the game evaluation environment:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先创建控制器 ANN 模型和游戏评估环境：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we''ll create placeholders so that we can receive values during network
    graph evaluation. Also, we''ll create an operator to reset the game before the
    start of the new game episode:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建占位符，以便在网络图评估期间接收值。同时，我们还将创建一个操作员，在新游戏剧集开始前重置游戏：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After that, using the context of the provided GPU device, we''ll create two
    operators to receive game state observations and evaluate the game actions that
    follow:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，使用提供的 GPU 设备的上下文，我们将创建两个操作员来接收游戏状态观察并评估后续的游戏动作：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `action` operator returns an array of action likelihood values, which needs
    to be filtered if the action space is discrete:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`action` 操作符返回一个动作可能性值的数组，如果动作空间是离散的，则需要过滤：'
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The code checks whether the current game environment requires discrete actions
    and wraps an `action` operator using the built-in `tf.argmax` operator of the
    TensorFlow framework. The `tf.argmax` operator returns the index of the action
    with the largest value, which can be used to signal that a specific game action
    should be executed.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 代码检查当前游戏环境是否需要离散动作，并使用 TensorFlow 框架内置的 `tf.argmax` 操作符包装 `action` 操作符。`tf.argmax`
    操作符返回具有最大值的动作的索引，可以用来指示应该执行特定的游戏动作。
- en: The Atari game environment is a discrete action environment, which means that
    only one action is accepted at each time step.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Atari 游戏环境是一个离散动作环境，这意味着在每一个时间步只接受一个动作。
- en: 'Finally, we create the operator to perform a single game step:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们创建一个操作员来执行单个游戏步骤：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we create a single game step operator, which returns operations to obtain
    rewards, `self.rew_op`, and the game completed status, `self.done_op`, after the
    execution of a single game step.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建一个单个游戏步骤操作符，该操作符在执行单个游戏步骤后返回获取奖励的操作 `self.rew_op` 和游戏完成状态 `self.done_op`。
- en: Next, we'll discuss how the evaluation loop is implemented.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论评估循环是如何实现的。
- en: The graph evaluation loop
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图形评估循环
- en: This is the loop that we use to evaluate the previously created network graph
    over multiple games in parallel – the number of games that can be evaluated simultaneously
    is determined by the `batch_size` parameter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于并行评估先前创建的网络图在多个游戏中的循环——可以同时评估的游戏数量由 `batch_size` 参数确定。
- en: 'The evaluation loop is defined in the `_loop` function and is implemented as
    follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 评估循环定义在 `_loop` 函数中，并如下实现：
- en: 'First, we start with the creation of arrays to hold game evaluation values
    over multiple episodes:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从创建数组开始，这些数组用于存储多个连续游戏中的游戏评估值：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we start the loop and set the corresponding indices of the running array
    we just created to `True`:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们启动循环并将我们刚刚创建的运行数组的相应索引设置为 `True`：
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using the indices array, we are ready to execute a single game step operator
    and collect the results:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用索引数组，我们准备执行单个游戏步骤操作并收集结果：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we need to test whether any of the evaluated games are done, either
    by winning or by hitting the maximum game frames limit. For all the completed
    tasks, we apply a number of operations, as follows:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要测试是否有任何评估过的游戏已完成，无论是通过获胜还是达到最大游戏帧数限制。对于所有完成的任务，我们应用一系列操作，如下：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding code uses the indices of all the completed tasks and invokes the
    corresponding registered callbacks before resetting the collector variables at
    specific indices.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码使用所有已完成任务的索引并调用相应的注册回调，在特定索引重置收集器变量之前。
- en: Now, we are ready to discuss how to add and run the new task using our worker.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备讨论如何使用我们的工作器添加和运行新任务。
- en: The asynchronous task runner
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步任务运行器
- en: 'This function registers a specific task to be evaluated by a worker in the
    GPU device context. It takes the ID of the task, the task object holder, and the
    callback to be executed on task completion as input. This function is defined
    under the name `run_async` and is implemented as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将特定任务注册为在 GPU 设备上下文中由工作器评估的任务。它接受任务 ID、任务对象持有者和任务完成时要执行的回调作为输入。此函数定义为 `run_async`
    并如下实现：
- en: 'First, it extracts the corresponding data from the task object and loads it
    into the current TensorFlow session:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它从任务对象中提取相应的数据并将其加载到当前 TensorFlow 会话中：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, `theta` is an array with all the connection weights in the controller
    ANN model, `extras` holds a random seeds list of the corresponding genome, and `max_frames`
    is the game frame's cutoff value.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`theta` 是控制器 ANN 模型中所有连接权重的数组，`extras` 包含相应基因组的随机种子列表，而 `max_frames` 是游戏帧的截止值。
- en: 'Next, we run the TensorFlow session with `self.reset_op`, which resets a specific
    game environment at a specified index:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `self.reset_op` 运行 TensorFlow 会话，该操作在指定索引重置特定的游戏环境：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The code runs `self.reset_op` within the TensorFlow session. Also, we register
    the current task identifier with the `reset` operator and the maximum game frame's
    cutoff value for a given task. The task identifier is used in the evaluation loop
    to associate evaluation results of the network graph with a specific genome in
    the population. Next, we'll discuss how concurrent asynchronous workers are maintained.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 代码在 TensorFlow 会话中运行 `self.reset_op`。我们还使用 `reset` 操作符和给定任务的特定游戏帧的最大截止值注册当前任务标识符。任务标识符在评估循环中使用，以将网络图的评估结果与种群中特定的基因组相关联。接下来，我们将讨论如何维护并发异步工作器。
- en: The ConcurrentWorkers class
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`ConcurrentWorkers` 类'
- en: The `ConcurrentWorkers` class holds the configuration of the concurrent execution
    environment, which includes several evaluation workers (`RLEvalutionWorker` instances)
    and auxiliary routines to support multiple executions of concurrent tasks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConcurrentWorkers` 类持有并发执行环境的配置，这包括几个评估工作器（`RLEvalutionWorker` 实例）和辅助例程以支持并发任务的多次执行。'
- en: Creating the evaluation workers
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建评估工作器
- en: 'One of the primary responsibilities of the `ConcurrentWorkers` class is to
    create and manage `RLEvalutionWorker` instances. This is done in the class constructor
    as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConcurrentWorkers` 类的主要职责之一是创建和管理 `RLEvalutionWorker` 实例。这是在类构造函数中完成的，如下所示：'
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we create the number of `RLEvalutionWorker` instances that are correlated
    to the number of GPU devices that are available in the system. After that, we
    initialize the selected ANN graph model and create auxiliary routines to manage
    multiple executions of asynchronous tasks. Next, we'll discuss how work tasks
    are scheduled for execution.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建与系统中可用的 GPU 设备数量相对应的 `RLEvalutionWorker` 实例数量。之后，我们初始化选定的 ANN 图模型，并创建辅助例程来管理异步任务的多次执行。接下来，我们将讨论工作任务是如何安排执行的。
- en: Running work tasks and monitoring results
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行工作任务和监控结果
- en: 'To use the RL evaluation mechanism we described earlier, we need a method to
    schedule the work task for evaluation and to monitor the results. This is implemented
    in the `monitor_eval` function, which receives the list of genomes in the population
    and evaluates them against the Atari game environment. This function has two essential
    implementation parts, both of which we''ll discuss in this section:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用我们之前描述的 RL 评估机制，我们需要一种方法来安排工作任务进行评估并监控结果。这通过 `monitor_eval` 函数实现，它接收种群中的基因组列表并对它们进行
    Atari 游戏环境的评估。此函数有两个基本实现部分，我们将在本节中讨论这两个部分：
- en: 'First, we iterate over all the genomes in the list and create the asynchronous
    work task so that each genome can be evaluated against the Atari game environment:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们遍历列表中的所有基因组，创建异步工作任务，以便每个基因组都可以与 Atari 游戏环境进行评估：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The preceding code schedules each genome in the list for asynchronous evaluation
    and saves a reference to each asynchronous task for later use. Also, we periodically
    output the results of the evaluation process of already scheduled tasks. Now,
    we'll discuss how to monitor the evaluation results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码为列表中的每个基因组安排异步评估，并为每个异步任务保存一个引用以供以后使用。此外，我们定期输出已安排任务的评估过程结果。现在，我们将讨论如何监控评估结果。
- en: 'The following code block is waiting for the completion of asynchronous tasks:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块正在等待异步任务的完成：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we iterate over all the references to the scheduled asynchronous tasks
    and wait for their completion. Also, we periodically output the evaluation progress.
    Next, we'll discuss how task evaluation results are collected.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们遍历所有对已安排的异步任务的引用，并等待它们的完成。同时，我们定期输出评估进度。接下来，我们将讨论如何收集任务评估结果。
- en: 'Finally, after the completion of all tasks, we collect the results, as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在所有任务完成后，我们收集结果，如下所示：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The code iterates through all the references to the scheduled asynchronous tasks
    and creates a list of the evaluation results. Next, we'll discuss the experiment
    runner implementation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 代码遍历所有对已安排的异步任务的引用，并创建一个评估结果列表。接下来，我们将讨论实验运行器的实现。
- en: Experiment runner
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验运行器
- en: The experiment runner implementation receives a configuration of the experiment
    defined in the JSON file and runs the neuroevolution process for the specified
    number of game time steps. In our experiment, the evaluation stops after reaching
    1.5 billion time steps of Frostbite. Next, we'll discuss the experiment configuration
    details.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 实验运行器实现接收在 JSON 文件中定义的实验配置，并运行指定游戏时间步数的神经进化过程。在我们的实验中，当达到 1.5 亿个 Frostbite 时间步后，评估停止。接下来，我们将讨论实验配置的详细信息。
- en: Experiment configuration file
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验配置文件
- en: 'Here is the file that provides configuration parameters for the experiment
    runner. For our experiment, it has the following content:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是提供实验运行器配置参数的文件。对于我们的实验，它包含以下内容：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The configuration parameters are as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 配置参数如下：
- en: The `game` parameter is the name of the game, as registered in the ALE. The
    full list of supported games is available at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`game` 参数是游戏的名称，如 ALE 中注册的那样。支持的完整游戏列表可在[https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/gym_tensorflow/atari/tf_atari.py)找到。'
- en: The `model` parameter designates the name of the network graph model to use
    for the construction of the controller ANN. The models are defined at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` 参数指定了用于构建控制器ANN的网络图模型的名称。模型在[https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/neuroevolution/models/dqn.py)中定义。'
- en: The `num_validation_episodes` parameter defines how many game episodes are used
    for the evaluation of the top individuals in the population. After this step,
    we can select the true elite of the population.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_validation_episodes` 参数定义了用于评估群体中顶级个体的游戏剧集数量。在此步骤之后，我们可以选择群体的真正精英。'
- en: The `num_test_episodes` parameter sets the number of game episodes to use to
    test the performance of the selected population elite.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_test_episodes` 参数设置了用于测试所选群体精英性能的游戏剧集数量。'
- en: The `population_size` parameter determines the number of genomes in the population.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`population_size` 参数决定了群体中的基因组数量。'
- en: The `episode_cutoff_mode` parameter defines how the game evaluation stops for
    a particular genome. The game episode can stop either upon the execution of a
    particular number of time steps or by using the default stop signal of the corresponding
    game environment.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`episode_cutoff_mode` 参数定义了特定基因组游戏评估停止的方式。游戏剧集可以通过执行特定数量的时间步或使用相应游戏环境的默认停止信号来停止。'
- en: The `timesteps` parameter sets the total number of time steps of the game to
    be executed during the neuroevolution process.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps` 参数设置了在神经进化过程中执行的游戏的总时间步数。'
- en: The `validation_threshold` parameter sets the number of top individuals that
    are selected from each generation for additional validation execution. The population
    elite is selected from these selected individuals.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_threshold` 参数设置了从每一代中选择的顶级个体数量，这些个体将进行额外的验证执行。群体精英从这些选定的个体中选出。'
- en: The `mutation_power` parameter defines how subsequent mutations that are added
    to the individual influence the training parameters (connection weights).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mutation_power` 参数定义了后续添加到个体中的突变如何影响训练参数（连接权重）。'
- en: The `selection_threshold` parameter determines how many parent individuals are
    allowed to produce offspring in the next generation.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selection_threshold` 参数决定了下一代中允许产生后代的父个体数量。'
- en: Now, we are ready to discuss the implementation details of the experiment runner.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备讨论实验运行器的实现细节。
- en: The experiment configuration file can be found at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/configurations/ga_atari_config.json](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/configurations/ga_atari_config.json).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 实验配置文件可在[https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/configurations/ga_atari_config.json](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter10/configurations/ga_atari_config.json)找到。
- en: Experiment runner implementation
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验运行器实现
- en: 'The experiment runner implementation creates the concurrent evaluation environment
    and runs the evolution loop over the population of individuals. Let''s discuss
    the essential implementation details:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 实验运行器实现创建并发评估环境，并在个体群体上运行进化循环。让我们讨论基本实现细节：
- en: 'We start by setting up the evaluation environment by loading the controller
    ANN model and creating the concurrent workers to execute the evaluation:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过加载控制器ANN模型并创建并发工作者来设置评估环境以执行评估：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we create a table with random noise values, which will be used as random
    seeds, and define the function to create offspring for the next generation:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含随机噪声值的表格，这些值将用作随机种子，并定义用于创建下一代后代的函数：
- en: '[PRE24]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After that, the main evolution loop starts. We use the previously defined function
    to create a population of the offspring for the current generation:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，主进化循环开始。我们使用之前定义的函数来为当前代创建后代群体：
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, we create the work tasks for each offspring in the population and schedule
    each task for evaluation against the game environment.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为群体中的每个后代创建工作任务，并为每个任务安排对游戏环境的评估。
- en: 'When we finish with the evaluation of each individual in the population, we
    start the evaluation of the top individuals to select the elite:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们完成对种群中每个个体的评估后，我们开始评估顶级个体以选择精英：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Using the evaluation results of the top 10 individuals, we select the population''s
    elite and execute the final test runs over it to evaluate its performance:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前 10 个顶级个体的评估结果，我们选择种群中的精英，并对其执行最终测试运行以评估其性能：
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The elite individual will be copied to the next generation as is.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 精英个体将直接复制到下一代。
- en: 'Finally, we select the top individuals from the current population to become
    parents for the next generation:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们从当前种群中选择顶级个体作为下一代的父母：
- en: '[PRE28]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code collects the top individuals from the population to become
    the parents of the next generation. Also, it appends the current elite to the
    list of parents if it isn't in the parent list.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码从种群中收集顶级个体成为下一代的父母。如果当前精英不在父母列表中，它还会将其附加到父母列表中。
- en: Now, we are ready to discuss how to run the experiment.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好讨论如何运行实验。
- en: Running the Frostbite Atari experiment
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行 Frostbite Atari 实验
- en: Now that we have discussed all the particulars of the experiment's implementation,
    it is time to run the experiment. However, the first thing we need to do is create
    an appropriate work environment, which we'll discuss next.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了实验实施的全部细节，现在是时候运行实验了。然而，我们首先需要做的是创建一个合适的工作环境，我们将在下一节讨论这一点。
- en: Setting up the work environment
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置工作环境
- en: The work environment for training the agent to play Atari games assumes that
    a large controller ANN needs to be trained in the process. We already stated that
    the controller ANN has more than 4 million training parameters and requires a
    lot of computational resources to be able to evaluate. Fortunately, modern GPU
    accelerators allow the execution of massive parallel computations simultaneously.
    This feature is convenient for our experiment because we need to evaluate each
    individual against the game environment multiple times during the evolution process.
    Without GPU acceleration, it would either take a lot of time or require a massive
    number of processing cores (about 720).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代理玩 Atari 游戏的工作环境假设在过程中需要训练一个大型控制器人工神经网络。我们之前已经提到，控制器人工神经网络有超过 400 万个训练参数，需要大量的计算资源才能进行评估。幸运的是，现代
    GPU 加速器允许同时执行大规模并行计算。这一特性对我们实验来说很方便，因为我们需要在进化过程中多次将每个个体与游戏环境进行评估。如果没有 GPU 加速，要么会花费很多时间，要么需要大量的处理核心（大约
    720 个）。
- en: 'Let''s discuss how to prepare the working environment:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论如何准备工作环境：
- en: The working environment requires a Nvidia video accelerator (such as GeForce
    1080Ti) present in the system and the appropriate Nvidia CUDA SDK installed. More
    details about the CUDA SDK and its installation can be found at [https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit).
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作环境需要系统中有 Nvidia 视频加速器（例如 GeForce 1080Ti）以及安装了适当的 Nvidia CUDA SDK。有关 CUDA SDK
    及其安装的更多详细信息，请见 [https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit)。
- en: Next, we need to make sure that the CMake build tool is installed, as described
    at [https://cmake.org](https://cmake.org).
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要确保已安装 CMake 构建工具，具体描述请见 [https://cmake.org](https://cmake.org)。
- en: 'Now, we need to create a new Python environment using Anaconda and install
    all the dependencies that are used by the experiment''s implementation:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要使用 Anaconda 创建一个新的 Python 环境，并安装实验实现中使用的所有依赖项：
- en: '[PRE29]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: These commands create and activate a new Python 3.5 environment. Next, it installs
    TensorFlow, OpenAI Gym, and the Python Imaging Library as dependencies.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令创建并激活一个新的 Python 3.5 环境。接下来，它安装 TensorFlow、OpenAI Gym 和 Python 图像库作为依赖项。
- en: 'After that, you need to clone the repository with the experiment''s source
    code:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，您需要克隆包含实验源代码的仓库：
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: After executing these commands, our current working directory becomes the directory
    that contains the experiment source code.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这些命令后，我们的当前工作目录变成了包含实验源代码的目录。
- en: 'Now, we need to build the ALE and integrate it into our experiment. We need
    to clone the ALE repository into the appropriate directory and build it with the
    following commands:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要构建 ALE 并将其集成到我们的实验中。我们需要将 ALE 仓库克隆到适当的目录，并使用以下命令进行构建：
- en: '[PRE31]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, we have a working ALE environment that's been integrated with TensorFlow.
    We can use it to evaluate the controller ANNs that are produced from a population
    of genomes against an Atari game (Frostbite, in our experiment).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有一个与TensorFlow集成的可工作的ALE环境。我们可以用它来评估从基因组种群中产生的控制器ANN，与Atari游戏（在我们的实验中为Frostbite）进行对抗。
- en: 'After the ALE integration is complete, we need to build an integration between
    OpenAI Gym and TensorFlow that is specific to our experiment implementation:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在ALE集成完成后，我们需要构建一个针对我们实验实现的特定于OpenAI Gym和TensorFlow的集成：
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, we have a fully defined work environment and we are ready to start our
    experiments. Next, we'll discuss how to run the experiment.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完全定义了工作环境，并准备好开始我们的实验。接下来，我们将讨论如何运行实验。
- en: Running the experiment
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行实验
- en: 'With an adequately defined work environment, we are ready to start our experiment.
    You can start an experiment from the `Chapter10` directory by executing the following
    command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个充分定义的工作环境中，我们准备好开始我们的实验。您可以通过执行以下命令从`Chapter10`目录启动实验：
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The preceding command starts an experiment using the configuration file that
    was provided as the first parameter. The experiment's output will be stored in
    the `out` directory.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令使用提供的精英基因组启动了一个实验，该实验使用作为第一个参数提供的配置文件。实验的输出将存储在`out`目录中。
- en: 'After completing the experiment, the console''s output should look similar
    to the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 实验完成后，控制台输出应类似于以下内容：
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here, we have the statistics output after a specific generation of evolution.
    You can see the following results:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有特定进化代次后的统计数据输出。您可以看到以下结果：
- en: The maximum reward score that's achieved during the evaluation of a population
    is 3,470 (`PopulationEpRewMax`).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估种群时，所达到的最大奖励分数为3,470 (`PopulationEpRewMax`)。
- en: The maximum score that's achieved among the top individuals on an additional
    30 episodes of validation is 3,240 (`TruncatedPopulationRewMean`).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在额外30个验证集的顶尖个人中，所达到的最高分数为3,240 (`TruncatedPopulationRewMean`)。
- en: The mean score of the top individual's evaluation is 2,360 (`TruncatedPopulationValidationRewMean`).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶尖个人评估的平均分数为2,360 (`TruncatedPopulationValidationRewMean`)。
- en: The mean score of the elite individual that's received during an additional
    200 test runs is 3,060 (`TruncatedPopulationEliteTestRewMean`).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在额外的200次测试运行中获得的精英个人平均分数为3,060 (`TruncatedPopulationEliteTestRewMean`)。
- en: The achieved reward scores are pretty high compared to other training methods
    if we look at the results that were published at [https://arxiv.org/abs/1712.06567v3](https://arxiv.org/abs/1712.06567v3).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看在[https://arxiv.org/abs/1712.06567v3](https://arxiv.org/abs/1712.06567v3)上发布的成果，与其他训练方法相比，所获得的奖励分数相当高。
- en: Also, at the end of the outputs, you can see the genome representation of the
    population elite. The elite genome can be used to visualize playing Frostbite
    by the phenotype ANN that was created from it. Next, we'll discuss how to make
    this visualization possible.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在输出的末尾，您可以看到种群精英的基因组表示。精英基因组可以用来通过从它创建的表型ANN来可视化玩Frostbite。接下来，我们将讨论如何实现这种可视化。
- en: Frostbite visualization
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Frostbite可视化
- en: 'Now that we have the results of the game agent''s training, it will be interesting
    to see how the solution we''ve found plays Frostbite in the Atari environment.
    To run the simulation, you need to copy the current elite genome representation
    from the output and paste it into the `seeds` field of the `display.py` file.
    After that, the simulation can be run using the following command:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了游戏代理训练的结果，将很有趣地看到我们找到的解决方案如何在Atari环境中玩Frostbite。要运行模拟，您需要从输出中复制当前的精英基因组表示并将其粘贴到`display.py`文件的`seeds`字段中。之后，可以使用以下命令运行模拟：
- en: '[PRE35]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding command uses the provided elite genome to create a phenotype
    ANN and uses it as the controller of the Frostbite playing agent. It will open
    the game window, where you can see how the controller ANN is performing. The game
    will continue until the game character doesn''t have any lives left. The following
    image shows several captured game screens from the execution of `display.py` in
    an Ubuntu 16.04 environment:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令使用提供的精英基因组创建一个表型ANN，并将其用作Frostbite游戏代理的控制器。它将打开游戏窗口，您可以在其中看到控制器ANN的表现。游戏将继续进行，直到游戏角色没有剩余的生命。以下图像显示了在Ubuntu
    16.04环境中执行`display.py`时捕获的几个游戏屏幕：
- en: '![](img/4591a38f-4a6a-40df-a39a-29179b4edfeb.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4591a38f-4a6a-40df-a39a-29179b4edfeb.png)'
- en: Frostbite screenshots, all of which have been taken from the elite genome game-playing
    session
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Frostbite截图，所有截图均来自精英基因组游戏会话
- en: It is pretty amazing to see how the trained controller ANN can learn the game
    rules solely from visual observations and is able to demonstrate such smooth gameplay.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 看到训练后的控制器ANN仅从视觉观察中学习游戏规则，并能展示出如此流畅的游戏操作，真是令人惊叹。
- en: Next, we'll discuss an additional visualization method that allows us to analyze
    the results.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一种额外的可视化方法，它允许我们分析结果。
- en: Visual inspector for neuroevolution
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经进化可视化检查器
- en: During the neuroevolution process, we are evolving a population of individuals.
    Each of the individuals is evaluated against the test environment (such as an
    Atari game) and reward scores are collected per individual for each generation
    of evolution. To explore the general dynamics of the neuroevolution process, we
    need to have a tool that can visualize the cloud of results for each individual
    in each generation of evolution. Also, it is interesting to see the changes in
    the fitness score of the elite individual to understand the progress of the evolution
    process.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经进化过程中，我们正在进化一个个体群体。每个个体都会在测试环境（如Atari游戏）中进行评估，并为每个进化代收集每个个体的奖励分数。为了探索神经进化过程的一般动态，我们需要一个工具来可视化每个进化代每个个体的结果云。同时，观察精英个体的适应度分数变化，有助于理解进化过程的进展。
- en: To address these requirements, the researchers from Uber AI developed the VINE
    tool, which we'll discuss next.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些要求，Uber AI的研究人员开发了VINE工具，我们将在下一部分讨论。
- en: Setting up the work environment
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置工作环境
- en: 'To use the VINE tool, we need to install additional libraries into our virtual
    Python environment with the following commands:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用VINE工具，我们需要使用以下命令在我们的虚拟Python环境中安装额外的库：
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: These commands install all the necessary dependencies into a virtual Python
    environment that we have created for our experiment. Next, we'll discuss how to
    use the VINE tool.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将所有必要的依赖项安装到我们为实验创建的虚拟Python环境中。接下来，我们将讨论如何使用VINE工具。
- en: Don't forget to activate the appropriate virtual environment with the following
    command before running the preceding commands: `conda activate deep_ne`.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的命令之前，不要忘记使用以下命令激活适当的虚拟环境：`conda activate deep_ne`。
- en: Using VINE for experiment visualization
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用VINE进行实验可视化
- en: 'Now, when we have all the dependencies installed on the Python virtual environment,
    we are ready to use the VINE tool. First, you need to clone it from the Git repository
    with the following commands:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们在Python虚拟环境中安装了所有依赖项后，我们就可以使用VINE工具了。首先，你需要使用以下命令从Git仓库克隆它：
- en: '[PRE37]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here, we cloned the deep neuroevolution repository into the current directory
    and changed the directory to the `visual_inspector` folder, where the source code
    of the VINE tool is present.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将深度神经进化仓库克隆到当前目录，并将目录更改为包含VINE工具源代码的`visual_inspector`文件夹。
- en: Let's discuss how VINE can be used to visualize the results of the neuroevolution
    experiment using the results of the Mujoco Humanoid experiment provided by the
    Uber AI Lab. More details about the Mujoco Humanoid experiment can be found at
    [https://eng.uber.com/deep-neuroevolution/](https://eng.uber.com/deep-neuroevolution/).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下如何使用VINE工具来可视化Uber AI Lab提供的Mujoco Humanoid实验结果。更多关于Mujoco Humanoid实验的详细信息可以在[https://eng.uber.com/deep-neuroevolution/](https://eng.uber.com/deep-neuroevolution/)找到。
- en: 'Now, we can run the visualization of the Mujoco Humanoid experiment results,
    which is supplied in the `sample_data` folder, using the following command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令运行Mujoco Humanoid实验结果的可视化，这些结果包含在`sample_data`文件夹中：
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding command uses the same data that was supplied by Uber AI Lab from
    their experiment for training humanoid locomotion and displays the following graphs:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令使用了Uber AI Lab从其实验中提供的用于训练类人行走的数据，并显示了以下图表：
- en: '![](img/d4769f1a-6ceb-4ddd-89d4-ad8f3d360e3e.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4769f1a-6ceb-4ddd-89d4-ad8f3d360e3e.png)'
- en: The VINE tool's visualization of Humanoid locomotion results
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: VINE工具对类人行走结果的可视化
- en: On the left-hand side of the graph, you can see the cloud of results for each
    individual in the population, starting from generation `90` and ending at generation
    `99`. On the right-hand side of the graph, you can see the fitness score of the
    population's elite per generation. In the right-hand graph, you can see that the
    evolutionary process demonstrates the positive dynamics from generation to generation
    as the fitness score of the elite is increasing.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的左侧，你可以看到种群中每个个体在`90`代到`99`代之间的结果云。在图表的右侧，你可以看到每一代种群精英的适应度分数。在右侧的图表中，你可以看到进化过程展示了从一代到下一代适应度分数增加的积极动态。
- en: Each point on the left-hand side graph demonstrates the behavioral characterization
    points for each individual in a population. The behavioral characterization for
    the Humanoid locomotion task is the final position of the Humanoid at the end
    of the trajectory. The farther it is from the origin coordinates (`0,0`), the
    higher the fitness score of the individual. You can see that, with the progress
    of evolution, the results cloud is moving away from the origin coordinates. This
    movement of the results cloud is also a signal of the positive learning dynamics
    because each individual was able to stay balanced for a more extended period.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧图表上的每个点都代表种群中每个个体的行为特征点。人形运动任务的行为特征是轨迹结束时人形的最终位置。它离原点坐标（`0,0`）越远，个体的适应度分数就越高。你可以看到，随着进化的进展，结果云正远离原点坐标。这种结果云的移动也是积极学习动态的信号，因为每个个体都能够保持更长时间的平衡。
- en: For more details about the Mujoco Humanoid locomotion experiment, please refer
    to the article at [https://eng.uber.com/deep-neuroevolution/](https://eng.uber.com/deep-neuroevolution/).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Mujoco人形运动实验的更多详细信息，请参阅[https://eng.uber.com/deep-neuroevolution/](https://eng.uber.com/deep-neuroevolution/)上的文章。
- en: Exercises
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Try to increase the `population_size` parameter in the experiment and see what
    happens.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试增加实验中的`population_size`参数，看看会发生什么。
- en: Try to create the experiment results, which can be visualized using VINE. You
    can use the  `master_extract_parent_ga` and `master_extract_cloud_ga` helper functions
    in the `ga.py` script to do this.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试创建实验结果，这些结果可以使用VINE进行可视化。你可以使用`ga.py`脚本中的`master_extract_parent_ga`和`master_extract_cloud_ga`辅助函数来完成此操作。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how neuroevolution can be used to train large
    ANNs with more than 4 million trainable parameters. You learned how to apply this
    learning method to create successful agents that are able to play classic Atari
    games by learning the game rules solely from observing the game screens. By completing
    the Atari game-playing experiment that was described in this chapter, you have
    learned about CNNs and how they can be used to map high-dimensional inputs, such
    as game screen observations, into the appropriate game actions. You now have a
    solid understanding of how CNNs can be used for value-function approximations
    in the deep RL method, which is guided by the deep neuroevolution algorithm.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用神经进化来训练具有超过400万个可训练参数的大型ANN。你学习了如何应用这种方法来创建能够仅通过观察游戏屏幕学习游戏规则的成功代理，从而玩经典Atari游戏。通过完成本章中描述的Atari游戏实验，你了解了卷积神经网络（CNNs）以及它们如何将高维输入，如游戏屏幕观察，映射到适当的游戏动作。你现在对CNNs在深度强化学习（deep
    RL）方法中的价值函数近似有了坚实的理解，该方法由深度神经进化算法指导。
- en: With the knowledge that you've acquired from this chapter, you will be able
    to apply deep neuroevolution methods in domains with high-dimensional input data,
    such as inputs that have been acquired from cameras or other image sources.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章获得的知识，你将能够将深度神经进化方法应用于具有高维输入数据域，例如从摄像头或其他图像源获取的输入。
- en: In the next chapter, we'll summarize what we have covered in this book and provide
    some hints about where you can continue your self-education.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将总结本书所涵盖的内容，并提供一些关于你如何继续自我教育的提示。
