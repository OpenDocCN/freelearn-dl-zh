["```py\nINetwork nn = NetworkBuilder.MakeLstm(inputDimension,\nhiddenDimension,hiddenLayers,outputDimension,data.GetModelOutputUnitToUse(),\ninitParamsStdDev, rng);\n```", "```py\npublic static NeuralNetwork MakeLstm(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)\n{\nList<ILayer> layers = new List<ILayer>();\nfor (int h = 0; h<hiddenLayers; h++)\n{\n```", "```py\nlayers.Add(h == 0? new LstmLayer(inputDimension, hiddenDimension, initParamsStdDev, rng): new LstmLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));\n}\n```", "```py\nlayers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));\n```", "```py\nreturn new NeuralNetwork(layers);\n}\n```", "```py\nINetwork nn = NetworkBuilder.MakeGru(inputDimension,\nhiddenDimension,\nhiddenLayers,\noutputDimension,\ndata.GetModelOutputUnitToUse(),\ninitParamsStdDev, rng);\n```", "```py\npublic static NeuralNetwork MakeGru(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)\n{\nList<ILayer> layers = new List<ILayer>();\nfor (int h = 0; h<hiddenLayers; h++)\n {\n layers.Add(h == 0\n ? newGruLayer(inputDimension, hiddenDimension, initParamsStdDev, rng)\n : newGruLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));\n }\nlayers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));\nreturn new NeuralNetwork(layers);\n}\n```", "```py\nstatic void Main(string[] args)\n{\nConsole.WriteLine(\"Running GRU sample\", Color.Yellow);\nConsole.ReadKey();\nExampleGRU.Run();\nConsole.ReadKey();\nConsole.WriteLine(\"Running LSTM sample\", Color.Yellow);\nConsole.ReadKey();\nExampleLSTM.Run();\nConsole.ReadKey();\nConsole.WriteLine(\"Running RNN sample\", Color.Yellow);\nConsole.ReadKey();\nExampleRNN.Run();\nConsole.ReadKey();\nConsole.WriteLine(\"Running Feed Forward sample\", Color.Yellow);\nConsole.ReadKey();\nExampleFeedForward.Run();\nConsole.ReadKey();\n}\n```", "```py\npublic class ExampleGRU\n {\npublic static void Run()\n {\nRandom rng = new Random();\nDataSet data = new XorDataSetGenerator();\nint inputDimension = 2;\nint hiddenDimension = 3;\nint outputDimension = 1;\nint hiddenLayers = 1;\ndouble learningRate = 0.001;\ndouble initParamsStdDev = 0.08;\n\nINetwork nn = NetworkBuilder.MakeGru(inputDimension,\nhiddenDimension, hiddenLayers, outputDimension, newSigmoidUnit(),\ninitParamsStdDev, rng);\n\nint reportEveryNthEpoch = 10;\nint trainingEpochs = 10000; // GRU's typically need less training\nTrainer.train<NeuralNetwork>(trainingEpochs, learningRate, nn, data, reportEveryNthEpoch, rng);\nConsole.WriteLine(\"Training Completed.\", Color.Green);\nConsole.WriteLine(\"Test: 1,1\", Color.Yellow);\nMatrix input = new Matrix(new double[] { 1, 1 });\nMatrix output = nn.Activate(input, new Graph(false));\nConsole.WriteLine(\"Test: 1,1\\. Output:\" + output.W[0], Color.Yellow);\nMatrix input1 = new Matrix(new double[] { 0, 1 });\nMatrix output1 = nn.Activate(input1, new Graph(false));\nConsole.WriteLine(\"Test: 0,1\\. Output:\" + output1.W[0], Color.Yellow);\nConsole.WriteLine(\"Complete\", Color.Yellow);\n }\n }\n```", "```py\npublic static NeuralNetwork MakeLstm(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)\n{\n    List<ILayer> layers = new List<ILayer>();\n    for (int h = 0; h<hiddenLayers; h++)\n    {\n        layers.Add(h == 0\n         ? new LstmLayer(inputDimension, hiddenDimension, initParamsStdDev, rng)\n         : new LstmLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));\n    }\n    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit,         initParamsStdDev, rng));\n    return new NeuralNetwork(layers);\n}\n\npublic static NeuralNetwork MakeFeedForward(int inputDimension, int hiddenDimension, inthiddenLayers, int outputDimension, INonlinearity hiddenUnit, INonlinearity decoderUnit, double initParamsStdDev, Random rng)\n{\n    List<ILayer> layers = new List<ILayer>();\n    for (int h = 0; h<hiddenLayers; h++)\n    {\n        layers.Add(h == 0? new FeedForwardLayer(inputDimension, hiddenDimension,         hiddenUnit, initParamsStdDev, rng): new FeedForwardLayer(hiddenDimension,         hiddenDimension, hiddenUnit, initParamsStdDev, rng));\n    }\n    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension,             decoderUnit, initParamsStdDev, rng));\n    return new NeuralNetwork(layers);\n }\n\npublic static NeuralNetwork MakeGru(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)\n{\n    List<ILayer> layers = new List<ILayer>();\n    for (int h = 0; h<hiddenLayers; h++)\n    {\n        layers.Add(h == 0? new GruLayer(inputDimension, hiddenDimension, initParamsStdDev, rng): new GruLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));\n    }\n    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));\n    return new NeuralNetwork(layers);\n}\n\npublic static NeuralNetwork MakeRnn(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity hiddenUnit, INonlinearity decoderUnit, double initParamsStdDev, Random rng)\n{\n    List<ILayer> layers = new List<ILayer>();\n    for (int h = 0; h<hiddenLayers; h++)\n    {\n        layers.Add(h == 0? new RnnLayer(inputDimension, hiddenDimension, hiddenUnit, initParamsStdDev, rng)\n        : new RnnLayer(hiddenDimension, hiddenDimension, hiddenUnit, initParamsStdDev, rng));\n    }\n    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));\n    return new NeuralNetwork(layers);\n}\n```"]