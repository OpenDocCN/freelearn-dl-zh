- en: Face and Motion Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it is time for us to get into a really neat application. We''ll start off
    by using the open source package [http://www.aforgenet.com/](http://www.aforgenet.com/)
    to build a face and motion detection application. To do this, you''ll need to
    have a camera installed in your system to see live streaming video. From there,
    we will use that camera to detect faces as well as motion. In this chapter, we
    are going to show two separate examples: one for facial detection, the other for
    motion detection. We''ll show you exactly what goes on, and just how fast you
    can add these capabilities into your application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover such topics as:'
  prefs: []
  type: TYPE_NORMAL
- en: Facial detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motion detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the local video-integrated camera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image filtering/algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start out with facial detection. In our example, I'm going to use my friendly
    little French bulldog to pose for us. Before I do that, please re-read the chapter
    title. No matter how many times you read it, you'll probably miss the key point
    here. Notice it says face *DETECTION* and not face *RECOGNITION*. This is so very
    important I wanted to stop and re-stress it. We are not trying to identify Joe,
    Bob, or Sally. We are trying to verify that, out of everything we see via our
    camera, we can *detect* that there is a face there. We are not concerned with
    whose face it is, just the fact that it is a face! It is so important that we
    understand this before moving on, otherwise your expectations will be so incorrectly
    biased that you are going to make yourself confused and upset, and we don't want
    that!
  prefs: []
  type: TYPE_NORMAL
- en: Facial detection, as I will stress again later, is the first part of facial
    recognition, a much more complicated beast. If you can't identify, out of all
    the things that are on the screen, one or more faces, then you'll never be able
    to recognize whose face it is!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a prerequisite, you will need Microsoft Visual Studio(any version) installed
    on your system. You will also need to access the open source accord framework
    at [https://github.com/accord-net/framework](https://github.com/accord-net/framework).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see Code in Action: [http://bit.ly/2xH0thh](http://bit.ly/2xH0thh).
  prefs: []
  type: TYPE_NORMAL
- en: Facial detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s take a quick look at our application. You should have the sample
    solution loaded into Microsoft Visual Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e561a83d-af1a-49e3-9a0a-c869351ace7b.png)'
  prefs: []
  type: TYPE_IMG
- en: And here's a look at our sample application running. Say Hi to Frenchie everyone!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76dc37eb-27d2-48d3-9f37-61ecc39691ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have a very simple screen that is dedicated to our video
    capture device. In this case, the laptop camera is our video capture device. Frenchie
    is kindly posing in front of the camera for us, and as soon as we enable facial
    tracking, look what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5aedfcf2-47bb-416d-bdbf-5d99efa1227a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The facial features of Frenchie are now being tracked. What you see surrounding
    Frenchie are the tracking containers (white boxes), and our angle detector (red
    line) displayed. As we move Frenchie around, the tracking container and angle
    detector will track him. That''s all well and good, but what happens if we enable
    facial tracking on a real human face? As you can see in the following screenshot,
    the tracking containers and angles are tracking the face of our guest poser, just
    like it did for Frenchie:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ae5fc01-5b16-431b-a337-3dae7b8f0bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As our poser moves his head from side to side, the camera tracks this, and
    you can see the angle detectors adjusting to what it recognizes as the angle of
    the face. In this case you will notice that the color space is in black and white
    and not color. This is a histogram back projection and is an option that you can
    change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3d38715-814f-4d2c-a11f-55b668a4e872.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even as we move farther away from the camera where other objects come into
    view, the facial detector can keep track of our face among the noise. This is
    exactly how the facial recognition systems you see in movies work, albeit more
    simplistic, and within minutes you can be up and running with your own facial
    recognition application too!:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a360a506-8548-4e24-b9c5-fee01dd29139.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we've seen the outside, let's look under the hood at what is going
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to ask ourselves exactly what the problem is that we are trying to
    solve here. Well, we are trying to detect (notice I did not say recognize) facial
    images. While easy for a human, a computer needs very detailed instruction sets
    to accomplish this feat. Luckily for us there is a very famous algorithm called
    the Viola-Jones algorithm that will do the heavy lifting for us. Why did we pick
    this algorithm?:'
  prefs: []
  type: TYPE_NORMAL
- en: Very high detection rates and very low false positives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very good at real-time processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very good at detecting faces from non-faces. Detecting faces is the first step
    in facial recognition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This algorithm requires that the camera has a full-frontal upright view of the
    face. To be detected, the face will need to be pointing straight towards the camera,
    not tilted, not looking up or down. Remember, for the moment, we are just interested
    in facial detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'To delve into the technical side of things, our algorithm will require four
    stages to accomplish its job. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: Haar feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an integral image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaboost training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cascading classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must start by stating that all human faces share some similar properties,
    such as the eye being darker than the upper cheeks, the nose bridge being brighter
    than the eyes, your forehead may be lighter than the rest of your face, and so
    on. Our algorithm matches these up by using what is known as **Haar Features**.
    We can come up with matchable facial features by looking at the location and size
    of the eyes, mouth, and bridge of the nose, and so forth. However, here's our
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: In a 24x24 pixel window, there are a total of 162,336 possible features. Obviously,
    to try and evaluate them all would be prohibitively expensive, if it would work
    at all. So, we are going to work with a technique known as **adaptive boosting**,
    or more commonly, **AdaBoost**. It's another one for your buzzword list, you've
    heard it everywhere and perhaps even read about it. Our learning algorithm will
    use AdaBoost to select the best features and train classifiers to use them. Let's
    stop and talk about it for a moment.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost can be used with many types of learning algorithm and is considered
    the best out-of-the-box algorithm for many tasks. You usually won't notice how
    good and fast it is until you switch to another algorithm and time it. I have
    done this countless times, and I can tell you the difference is very noticeable.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting takes the output from other weak-learning algorithms and combines them
    with a weighted sum that is the final output of the boosted classifier. The adaptive
    part of AdaBoost comes from the fact that subsequent learners are tweaked in favor
    of those instances that have been misclassified by previous classifiers. We must
    be careful with our data preparation though, as AdaBoost is sensitive to noisy
    data and outliers (remember how we stressed those in [Chapter 1](a48527d5-838a-4f47-a4fd-505597aeed58.xhtml),
    *A Quick Refresher*). The algorithm tends to overfit the data more than other
    algorithms, which is why in our earlier chapters we stressed data preparation
    for missing data and outliers. In the end, if *weak* learning algorithms are better
    than random guessing, AdaBoost can be a valuable addition to our process.
  prefs: []
  type: TYPE_NORMAL
- en: With that brief description behind us, let's look under the covers at what's
    happening. For this example, we will again use the Accord framework and we will
    work with the Vision Face Tracking sample. You can download the latest version
    of this framework from its GitHub location: [https://github.com/accord-net/framework](https://github.com/accord-net/framework).
  prefs: []
  type: TYPE_NORMAL
- en: We start by creating a `FaceHaarCascade` object. This object holds a collection
    of Haar-like features' weak classification stages, or stages. There will be many
    stages provided, each containing a set of classifier trees that will be used in
    the decision-making process. We are now technically working with a decision tree.
    The beauty of the Accord framework is that `FaceHaarCascade` automatically creates
    all these stages and trees for us without exposing us to the details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at what a particular stage might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are building a decision tree underneath the hood by providing
    the nodes for each stage with the numeric values for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once created, we can use our cascade object to create our `HaarObjectDetector`,
    which is what we will use for detection. It takes:'
  prefs: []
  type: TYPE_NORMAL
- en: Our facial cascade objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum window size to use when searching for objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our search mode—in our case, we are searching for only a single object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The re-scaling factor to use when re-scaling our search window during the search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once created, we are ready to tackle the topic of our video collection source.
    In our examples, we will simply use the local camera to capture all images. However,
    the Accord.Net framework makes it easy to use other sources for image capture,
    such as `.avi` files, animated `.jpg` files, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'We connect to the camera, select the resolution, and are ready to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the application now running and our video source selected, our application
    will look like the following. Once again, enter Frenchie the bulldog! Please excuse
    the mess, Frenchie is not the tidiest of pets!:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d9d9b29-f1e7-4ab7-b41b-c8fde389d082.png)'
  prefs: []
  type: TYPE_IMG
- en: For this demonstration, you will notice that Frenchie is facing the camera,
    and in the background, we have 2 x 55" monitors, as well as many other items my
    wife likes to refer to as *junk* (we'll be proper and call it *noise*)! This is
    done to show how the face detection algorithm can distinguish Frenchie's face
    amongst everything else! If our detector cannot handle this, it is going to get
    lost in the noise and will be of little use to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our video source now coming in, we need to be notified when a new frame
    is received so that we can process it, apply our markers, and so on. We do this
    by attaching to the `NewFrameReceived` event handler of the video source player,
    as follows. .NET developers should be very familiar with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at what happens each time we are notified that a new video frame
    is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we need to do is `downsample` the image to make it easier
    to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With the image in a more manageable size, we will process the frame. If we
    have not found a facial region, we will stay in tracking mode waiting for a frame
    that has a detectable face. If we have found a facial region, we will reset our
    tracker, locate the face, reduce its size in order to flush away any background
    noise, initialize the tracker, and apply the marker window to the image. All of
    this is accomplished with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If a face was detected, our image frame now looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1536495b-9168-4a57-a784-517d0591fe1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If Frenchie tilts his head to the side, our image frame now looks like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee9335bd-0879-4341-bbd9-24a00d11b2ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Motion detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now make our focus a bit more wide-scale and detect any motion at all,
    not just faces. Again, we''ll use Accord.Net for this and use the `Motion detection`
    sample. As with facial recognition, you will see just how simple it is to add
    this capability to your applications and instantly become a hero at work! Let''s
    make sure you have the correct project loaded into Microsoft Visual Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb0ee117-d5bf-4cbd-bd7c-30e9feb5217c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With motion detection, anything that moves on the screen we will highlighted
    in red, so using the following screenshot you can see that the fingers are moving
    but everything else remains motionless:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b6f2db8-633c-40dd-a4ee-4ce16da9cfe4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following screenshot, you can see more movement, denoted by the red
    blocks along this anonymous hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a9bc94e-95ee-49c2-83b7-9d93747dcc8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following screenshot, you can see that the entire hand is moving:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be569970-5761-4a5a-852f-39054b3bad17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we do not wish to process the entire screen area for motion, we can define
    *motion regions*, where motion detection will occur only in those regions. In
    the following screenshot, you can see that I defined a motion region. You will
    notice in upcoming screenshots that this is the only area that motion will be
    processed from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1e77ea7-22a7-4caa-82eb-233dc2796570.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we create some motion for the camera, you will see that only motion
    from our defined region is being processed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa867f0c-b7af-419d-91f4-096416eb4758.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also see that, with a motion region defined and Peter the meditating
    Gnome in front of the region, we are still able to detect motion behind him, but
    his face is not part of the recognition. You could of course, combine both processes
    to have the best of both worlds, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e7bf3a5-7ad7-4340-a31a-aed2597fed13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another option that we can use is Grid Motion Highlighting. This highlights
    the motion detected region in red squares based upon a defined grid. Basically,
    the motion area is now a red box, as you can see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9622811a-aaa7-4bad-a7d3-b9d3b7064755.png)'
  prefs: []
  type: TYPE_IMG
- en: Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following snippet shows a simple example of all you need to do to add video
    recognition to your application. As you can see, it couldn’t be any easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We now open our video source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When we receive a new video frame, that''s when all the magic happens. The
    following are all the code it takes to make processing a new video frame a success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The key here is detecting the amount of motion that is happening in the frame,
    which is done with the following code. For this example, we are using a motion
    alarm level of 0.2, but you can use whatever you like. Once this threshold has
    been passed, you can do whatever logic you like such as send an email alert, text,
    start a video capture operation, and so forth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about image and motion detection (not recognition!).
    We used Accord.Net as an example of what open-source tools provide us with when
    we want to add power to our applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we remain with the image theme, but work on training Convolutional
    Neural Networks with the open-source package ConvNetSharp.
  prefs: []
  type: TYPE_NORMAL
