- en: Chapter 10. Current Trends in Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This final chapter shows the reader the most recent trends in neural networks.
    Although this book is introductory, it is always useful to be aware of the latest
    developments and where the science behind this theory is going to. Among the latest
    advancements is the so-called **deep learning**, a very popular research field
    for many data scientists; this type of network is briefly covered in this chapter.
    Convolutional and cognitive architectures are also in this trend and gaining popularity
    for multimedia data recognition. Hybrid systems that combine different architectures
    are a very interesting strategy for solving more complex problems, as well as
    applications that involve analytics, data visualization, and so on. Being more
    theoretical, there is no actual implementation of the architectures, although
    an example of implementation for a hybrid system is provided. Topics covered in
    this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short term memory networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neuro-Fuzzy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neuro-Genetic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of a hybrid neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the latest advancements in neural networks is the so-called deep learning.
    Nowadays it is nearly impossible to talk about neural networks without mentioning
    deep learning, because the recent research on feature extraction, data representation,
    and transformation has found that many layers of processing information are able
    to abstract and produce better representations of data for learning. Throughout
    this book we have seen that neural networks require input data in numerical form,
    no matter if the original data is categorical or binary, neural networks cannot
    process non-numerical data directly. But it turns out that in the real world most
    of the data is non-numerical or is even unstructured, such as images, videos,
    audios, texts, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this sense a deep network would have many layers that could act as data processing
    units to transform this data and provide it to the next layer for subsequent data
    processing. This is analogous to the process that happens in the brain, from the
    nerve endings to the cognitive core; in this long path the signals are processed
    by multiple layers before resulting in signals that control the human body. Currently,
    most of the research on deep learning has been on the processing of unstructured
    data, particularly image and sound recognition and natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning is still under development and much has changed since 2012\. Big
    companies such as Google and Microsoft have teams for research on this field and
    much is likely to change in the next couple of years.
  prefs: []
  type: TYPE_NORMAL
- en: 'A scheme of a deep learning architecture is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning](img/B5964_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, deep neural networks have some problems that need to be overcome.
    The main problem is overfitting. The many layers that produce new representations
    of data are very sensitive to the training data, because the deeper the signals
    reach in the neural layers, the more specific the transformation will be for the
    input data. Regularization methods and pruning are often applied to prevent overfitting.
    Computation time is another common issue in training deep networks. The standard
    backpropagation algorithm can take a very long time to train a deep neural network,
    although strategies such as selecting a smaller training dataset can speed up
    the training time. In addition, in order to train a deep neural network, it is
    often recommended to use a faster machine and parallelize the training as much
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Deep architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a great variety of deep neural architectures with both feedforward
    and feedback flows, although they are typically feedforward. Main architectures
    are, without limitation to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional neural network**'
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture, the layers may have multidimensional organization. Inspired
    by the visual cortex of animals, the typical dimensionality applied to the layers
    is three-dimensional. In **convolutional neural networks** (**CNNs**), part of
    the signals of a preceding layer is fed into another part of neurons in the following
    layer. This architecture is feedforward and is well applied for image and sound
    recognition. The main feature that distinguishes this architecture from Multilayer
    Perceptrons is the partial connectivity between layers. Considering the fact that
    not all neurons are relevant for a certain neuron in the next layer, the connectivity
    is local and respects the correlation between neurons. This prevents both long
    time training and overfitting, provided that a fully connected MLP blows up the
    number of weight as the dimension of images grows, for example. In addition, neurons
    in layers are arranged in dimensions, typically three, thereby staked in an array
    in width, height, and depth.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep architectures](img/B5964_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this architecture, the layers may have multidimensional organization. Inspired
    by the visual cortex of animals, the typical dimensionality applied to the layers
    is three-dimensional. In **convolutional neural networks** (**CNNs**), part of
    the signals of a preceding layer is fed into another part of neurons in the following
    layer. This architecture is feedforward and is well applied for image and sound
    recognition. The main feature that distinguishes this architecture from multilayer
    perceptrons is the partial connectivity between layers. Considering the fact that
    not all neurons are relevant for a certain neuron in the next layer, the connectivity
    is local and respects the correlation between neurons. This prevents both long
    time training and overfitting, provided that a fully connected MLP blows up the
    number of weight as the dimension of images grows, for example. In addition, neurons
    in layers are arranged in dimensions, typically three, thereby staked in an array
    in width, height, and depth.
  prefs: []
  type: TYPE_NORMAL
- en: '**Long short-term memory**: This is a recurrent type of neural network that
    takes into account always the last value of the hidden layer, exactly like a **hidden
    Markov model** (**HMM**). A **Long Short Time Memory network** (**LSTM**) has
    LSTM units instead of traditional neurons, and these units implement operations
    such as store and forget a value to control the flow in a deep network. This architecture
    is well applied to natural language processing, due to the capacity of retaining
    information for a long time while receiving completely unstructured data such
    as audio or text files. One way to train this type of network is the backpropagation
    through time (BPTT) algorithm, but there are also other algorithms such as reinforcement
    learning or evolution strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep architectures](img/B5964_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Deep belief network**: **Deep belief networks** (**DBN''s**) are probabilistic
    models where layers are classified into visible and hidden. This is also a type
    of recurrent neural network based on a **restricted Boltzmann machine** (**RBM**).
    It is typically used as a first step in the training of a **deep neural network**
    (**DNN**), which is further trained by other supervised algorithms such as backpropagation.
    In this architecture each layer acts like a feature detector, abstracting new
    representations of data. The visible layer acts both as an output and as an input,
    and the deepest hidden layer represents the highest level of abstraction. Applications
    of this architecture are typically the same as those of convolutional neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep architectures](img/B5964_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to implement deep learning in Java
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because this book is introductory, we are not diving into further details on
    deep learning in this chapter. However, some recommendations of code for a deep
    architecture are provided. An example on how a convolutional neural network would
    be implemented is provided here. One needs to implement a class called `ConvolutionalLayer`
    to represent a multidimensional layer, and a `CNN` class for the convolutional
    neural network itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this class, the neurons are organized in dimensions and methods for pruning
    are used to make the connections between the layers. Please see the files `ConvolutionalLayer.java`
    and `CNN.java` for further details.
  prefs: []
  type: TYPE_NORMAL
- en: Since the other architectures are recurrent and this book does not cover the
    recurrent neural networks (for simplicity purposes in an introductory book) they
    are provided only for the reader's information. We suggest the reader to take
    a look at the references provided to find out more on these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In machine learning, or even in the artificial intelligence field, there are
    many other algorithms and techniques other than neural networks. Each technique
    has its strengths and drawbacks, and that inspires many researchers to combine
    them into a single structure. Neural networks are part of the connectionist approach
    for artificial intelligence, whereby operations are performed on numerical and
    continuous values; but there are other approaches that include cognitive (rule-based
    systems) and evolutionary computation.
  prefs: []
  type: TYPE_NORMAL
- en: '| Connectionist | Cognitive | Evolutionary |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Numerical processing | Symbolic processing | Numerical and symbolic processing
    |'
  prefs: []
  type: TYPE_TB
- en: '| Large network structures | Large rule bases and premises | Large quantity
    of solutions |'
  prefs: []
  type: TYPE_TB
- en: '| Performance by statistics | Design by experts/statistics | Better solutions
    are produced every iteration |'
  prefs: []
  type: TYPE_TB
- en: '| Highly sensitive to data | Highly sensitive to theory | Local minima proof
    |'
  prefs: []
  type: TYPE_TB
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Fuzzy logic is a type of rule-based processing, where every variable is converted
    to a symbolic value according to a membership function, and then the combination
    of all variables is queried against an *IF-THEN* rule database.
  prefs: []
  type: TYPE_NORMAL
- en: '![Neuro-fuzzy](img/B5964_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A membership function usually has a Gaussian bell shape, which tells us how
    much a given value is a *member* of that class. Let's take, for example, temperature,
    which may take on three different classes (cold, normal, and warm). A membership
    value will be higher the more the temperature is closer to the bell shape centers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Neuro-fuzzy](img/B5964_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, the fuzzy processing finds which rules are fired by every input
    record and which output values are produced. A neuro-fuzzy architecture treats
    each input differently, so the first hidden layer has a set of neurons for each
    input corresponding for each membership function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neuro-fuzzy](img/B5964_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this architecture, the training finds optimal weights for the rule processing
    and weighted sum of consequent parameters only, the first hidden layer has no
    adjustable weights.
  prefs: []
  type: TYPE_NORMAL
- en: In fuzzy logic architecture, the experts define a rule database that may become
    huge as the number of variables increase. The neuro-fuzzy architecture releases
    the designer from defining the rules, and lets this task be performed by the neural
    network. The training of a neuro-fuzzy can be performed by gradient type algorithms
    such as backpropagation or matrix algebra such as least squares, both in the supervised
    mode. Neuro-fuzzy systems are suitable for control of dynamic systems and diagnostics.
  prefs: []
  type: TYPE_NORMAL
- en: Neuro-genetic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the evolutionary artificial intelligence approach, one common strategy is
    genetic algorithms. This name is inspired by natural evolution, which states that
    beings more adapted to the environment are able to produce new generations of
    better adapted beings. In the computing intelligence field, the *beings* or *individuals*
    are candidate solutions or hypotheses that can solve an optimization problem.
    Supervised neural networks are used for optimization, since there is an error
    measure that we want to minimize by adjusting the neural weights. While the training
    algorithms are able to find better weights by gradient methods, they often fall
    in local minima. Although some mechanisms, such as regularization and momentum,
    may improve the results, once the weights fall in a local minimum, it is very
    unlikely that a better weight will be found, and in this context genetic algorithms
    are very good at it.
  prefs: []
  type: TYPE_NORMAL
- en: Think of the neural weights as a genetic code (or DNA). If we could generate
    a finite number of random generated weight sets, and evaluate which produce the
    best results (smaller errors or other performance measurement), we would select
    a top N best weight, and then set and apply genetic operations on them, such as
    reproduction (interchange of weights) and mutation (random change of weights).
  prefs: []
  type: TYPE_NORMAL
- en: '![Neuro-genetic](img/B5964_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This process is repeated until some acceptable solution is found.
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy is to use genetic operations on neural network parameters,
    such as number of neurons, learning rate, activation functions, and so on. Considering
    that, there is always a need to adjust parameters or train multiple times to ensure
    we've found a good solution. So, one may code all parameters in a genetic code
    (parameter set) and generate multiple neural networks for each parameter set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scheme of a genetic algorithm is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neuro-genetic](img/B5964_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Genetic algorithms are broadly used for many optimization problems, but in this
    book we are sticking with these two classes of problems, weight and parameter
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a hybrid neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s implement a simple code that can be used in the neuro-fuzzy and
    neuro-genetic networks. First, we need to define Gaussian functions for activation
    that will be the membership functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The fuzzy sets and rules need to be represented in a way that a neural network
    can understand and drive the execution. This representation includes the quantity
    of sets per input, therefore having the information on how the neurons are connected;
    and the membership functions for each set. A simple way to represent the quantity
    is an array. The array of sets just indicates how many sets there are for each
    variable; and the array of rules is a matrix, where each row represents a rule
    and each column represents a variable; each set can be assigned a numerical integer
    value for reference in the rule array. An example of three variables, each having
    three sets, is defined in the following snippet, along with the rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The membership functions can be referenced in a serialized array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We need also to create classes for the layers of a neuro fuzzy architecture,
    such as `InputFuzzyLayer` and `RuleLayer`. They can be children of a `NeuroFuzzyLayer`
    superclass, which can inherit from `NeuralLayer`. These classes are necessary
    because they work differently from the already defined neural layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A `NeuroFuzzy` class will inherit from `NeuralNet`, having references to the
    other fuzzy layer classes. The `calc()`methods of the `NeuroFuzzyLayer` will also
    be different, taking into account the membership functions centers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For more details, see the files in the `edu.packt.neuralnet.neurofuzzy` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'To code a neuro-genetic for weight sets, one needs to define the genetic operations.
    Let''s create a class called `NeuroGenetic` to implement reproduction and mutation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define the evaluation of each weight on each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can just call a neuro-genetic algorithm by using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we gave the reader a glimpse of what to do next in this
    field. Being more theoretical, this chapter has focused more on the functionality
    and information than on practical implementation, because this would be very heavy
    for an introductory book. In every case, a simple code is provided to give a hint
    on how to further implement deep neural networks. The reader is then encouraged
    to modify the codes of the previous chapters, adapting them to the hybrid neural
    networks and comparing the results. Being a very dynamic and novel field of research,
    at every moment new approaches and algorithms are under development, and we provide
    in the references a list of publications to stay up to date on this subject.
  prefs: []
  type: TYPE_NORMAL
