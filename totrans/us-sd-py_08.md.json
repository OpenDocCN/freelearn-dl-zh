["```py\npip install PEFT\n```", "```py\n    import torch\n    ```", "```py\n    from diffusers import StableDiffusionPipeline\n    ```", "```py\n    pipeline = StableDiffusionPipeline.from_pretrained(\n    ```", "```py\n        \"runwayml/stable-diffusion-v1-5\",\n    ```", "```py\n        torch_dtype = torch.float16\n    ```", "```py\n    ).to(\"cuda:0\")\n    ```", "```py\n    prompt = \"\"\"\n    ```", "```py\n    shukezouma, shuimobysim, a branch of flower, traditional chinese ink painting\n    ```", "```py\n    \"\"\"\n    ```", "```py\n    image = pipeline(\n    ```", "```py\n        prompt = prompt,\n    ```", "```py\n        generator = torch.Generator(\"cuda:0\").manual_seed(1)\n    ```", "```py\n    ).images[0]\n    ```", "```py\n    display(image)\n    ```", "```py\n    # load LoRA to the pipeline\n    ```", "```py\n    pipeline.load_lora_weights(\n    ```", "```py\n        \"andrewzhu/MoXinV1\",\n    ```", "```py\n        weight_name   = \"MoXinV1.safetensors\",\n    ```", "```py\n        adapter_name  = \"MoXinV1\"\n    ```", "```py\n    )\n    ```", "```py\n    image = pipeline(\n    ```", "```py\n        prompt = prompt,\n    ```", "```py\n        generator = torch.Generator(\"cuda:0\").manual_seed(1)\n    ```", "```py\n    ).images[0]\n    ```", "```py\n    display(image)\n    ```", "```py\npipeline.set_adapters(\n    [\"MoXinV1\"],\n    adapter_weights=[0.5]\n)\nimage = pipeline(\n    prompt = prompt,\n    generator = torch.Generator(\"cuda:0\").manual_seed(1)\n).images[0]\ndisplay(image)\n```", "```py\n# load another LoRA to the pipeline\npipeline.load_lora_weights(\n    \"andrewzhu/civitai-light-shadow-lora\",\n    weight_name   = \"light_and_shadow.safetensors\",\n    adapter_name  = \"light_and_shadow\"\n)\n```", "```py\npipeline.set_adapters(\n    [\"MoXinV1\", \"light_and_shadow\"],\n    adapter_weights=[0.5,1.0]\n)\nprompt = \"\"\"\nshukezouma, shuimobysim ,a branch of flower, traditional chinese ink painting,STRRY LIGHT,COLORFUL\n\"\"\"\nimage = pipeline(\n    prompt = prompt,\n    generator = torch.Generator(\"cuda:0\").manual_seed(1)\n).images[0]\ndisplay(image)\n```", "```py\npipeline.disable_lora()\n```", "```py\n# load lora file\nfrom safetensors.torch import load_file\nlora_path = \"MoXinV1.safetensors\"\nstate_dict = load_file(lora_path)\nfor key in state_dict:\n    print(key)\n```", "```py\n...\nlora_te_text_model_encoder_layers_7_mlp_fc1.alpha\nlora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight\nlora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight\n...\n```", "```py\n...\nlora_unet_down_blocks_0_attentions_1_proj_in.alpha\nlora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight\nlora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight\n...\n```", "```py\n# find the layer name\nLORA_PREFIX_UNET = 'lora_unet'\nLORA_PREFIX_TEXT_ENCODER = 'lora_te'\nfor key in state_dict:\n    if 'text' in key:\n        layer_infos = key.split('.')[0].split(\n            LORA_PREFIX_TEXT_ENCODER+'_')[-1].split('_')\n        curr_layer = pipeline.text_encoder\n    else:\n        layer_infos = key.split('.')[0].split(\n            LORA_PREFIX_UNET+'_')[-1].split('_')\n        curr_layer = pipeline.unet\n```", "```py\nunet = pipeline.unet\nmodules = unet.named_modules()\nfor child_name, child_module in modules:\n    print(\"child_module:\",child_module)\n```", "```py\n...\n(down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n        (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n            (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n                (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                    (0): Linear(in_features=320, out_features=320, bias=True)\n                    (1): Dropout(p=0.0, inplace=False)\n                )\n...\n```", "```py\n# find the layer name\nfor key in state_dict:\n    # find the LoRA layer name (the same code shown above)\n    for key in state_dict:\n    if 'text' in key:\n        layer_infos = key.split('.')[0].split(\n            \"lora_unet_\")[-1].split('_')\n        curr_layer = pipeline.text_encoder\n    else:\n        layer_infos = key.split('.')[0].split(\n            \"lora_te_\")[-1].split('_')\n        curr_layer = pipeline.unet\n    # loop through the layers to find the target layer\n    temp_name = layer_infos.pop(0)\n    while len(layer_infos) > -1:\n        try:\n            curr_layer = curr_layer.__getattr__(temp_name)\n            # no exception means the layer is found\n            if len(layer_infos) > 0:\n                temp_name = layer_infos.pop(0)\n            # all names are pop out, break out from the loop\n            elif len(layer_infos) == 0:\n                break\n        except Exception:\n            # no such layer exist, pop next name and try again\n            if len(temp_name) > 0:\n                temp_name += '_'+layer_infos.pop(0)\n            else:\n                # temp_name is empty\n                temp_name = layer_infos.pop(0)\n```", "```py\n# ensure the sequence of lora_up(A) then lora_down(B)\npair_keys = []\nif 'lora_down' in key:\n    pair_keys.append(key.replace('lora_down', 'lora_up'))\n    pair_keys.append(key)\nelse:\n    pair_keys.append(key)\n    pair_keys.append(key.replace('lora_up', 'lora_down'))\n```", "```py\nalpha = 0.5\n# update weight\nif len(state_dict[pair_keys[0]].shape) == 4:\n    # squeeze(3) and squeeze(2) remove dimensions of size 1 \n    #from the tensor to make the tensor more compact\n    weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).\\\n        to(torch.float32)\n    weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).\\\n        to(torch.float32)\n    curr_layer.weight.data += alpha * torch.mm(weight_up, \n        weight_down).unsqueeze(2).unsqueeze(3)\nelse:\n    weight_up = state_dict[pair_keys[0]].to(torch.float32)\n    weight_down = state_dict[pair_keys[1]].to(torch.float32)\n    curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n```", "```py\ndef load_lora(\n    pipeline,\n    lora_path,\n    lora_weight = 0.5,\n    device = 'cpu'\n):\n    state_dict = load_file(lora_path, device=device)\n    LORA_PREFIX_UNET = 'lora_unet'\n    LORA_PREFIX_TEXT_ENCODER = 'lora_te'\n    alpha = lora_weight\n    visited = []\n    # directly update weight in diffusers model\n    for key in state_dict:\n        # as we have set the alpha beforehand, so just skip\n        if '.alpha' in key or key in visited:\n            continue\n        if 'text' in key:\n            layer_infos = key.split('.')[0].split(\n                LORA_PREFIX_TEXT_ENCODER+'_')[-1].split('_')\n            curr_layer = pipeline.text_encoder\n        else:\n            layer_infos = key.split('.')[0].split(\n                LORA_PREFIX_UNET+'_')[-1].split('_')\n            curr_layer = pipeline.unet\n        # find the target layer\n        # loop through the layers to find the target layer\n        temp_name = layer_infos.pop(0)\n        while len(layer_infos) > -1:\n            try:\n                curr_layer = curr_layer.__getattr__(temp_name)\n                # no exception means the layer is found\n                if len(layer_infos) > 0:\n                    temp_name = layer_infos.pop(0)\n                # layer found but length is 0,\n                # break the loop and curr_layer keep point to the \n                # current layer\n                elif len(layer_infos) == 0:\n                    break\n            except Exception:\n                # no such layer exist, pop next name and try again\n                if len(temp_name) > 0:\n                    temp_name += '_'+layer_infos.pop(0)\n                else:\n                    # temp_name is empty\n                    temp_name = layer_infos.pop(0)\n        # org_forward(x) + lora_up(lora_down(x)) * multiplier\n        # ensure the sequence of lora_up(A) then lora_down(B)\n        pair_keys = []\n        if 'lora_down' in key:\n            pair_keys.append(key.replace('lora_down', 'lora_up'))\n            pair_keys.append(key)\n        else:\n            pair_keys.append(key)\n            pair_keys.append(key.replace('lora_up', 'lora_down'))\n        # update weight\n        if len(state_dict[pair_keys[0]].shape) == 4:\n            # squeeze(3) and squeeze(2) remove dimensions of size 1 \n            # from the tensor to make the tensor more compact\n            weight_up = state_dict[pair_keys[0]].squeeze(3).\\\n                squeeze(2).to(torch.float32)\n            weight_down = state_dict[pair_keys[1]].squeeze(3).\\\n                squeeze(2).to(torch.float32)\n            curr_layer.weight.data += alpha * torch.mm(weight_up, \n                weight_down).unsqueeze(2).unsqueeze(3)\n        else:\n            weight_up = state_dict[pair_keys[0]].to(torch.float32)\n            weight_down = state_dict[pair_keys[1]].to(torch.float32)\n            curr_layer.weight.data += alpha * torch.mm(weight_up, \n                weight_down)\n        # update visited list, ensure no duplicated weight is \n        # processed.\n        for item in pair_keys:\n            visited.append(item)\n```", "```py\npipeline = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype = torch.bfloat16\n).to(\"cuda:0\")\nlora_path = r\"MoXinV1.safetensors\"\nload_lora(\n    pipeline = pipeline,\n    lora_path = lora_path,\n    lora_weight = 0.5,\n    device = \"cuda:0\"\n)\n```", "```py\nprompt = \"\"\"\nshukezouma, shuimobysim ,a branch of flower, traditional chinese ink painting\n\"\"\"\nimage = pipeline(\n    prompt = prompt,\n    generator = torch.Generator(\"cuda:0\").manual_seed(1)\n).images[0]\ndisplay(image)\n```", "```py\nM =  1  2  3\n     4  5  6\n     7  8  9\n     10 11 12\n     13 14 15\n```"]