<html><head></head><body>
<div epub:type="chapter" id="_idContainer109">
<h1 class="chapter-number" id="_idParaDest-43"><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.2.1">Exploring LLMs as a Powerful AI Engine</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we saw the structure of a transformer, how it is trained, and what makes it so powerful. </span><span class="koboSpan" id="kobo.3.2">The transformer is the seed of this </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.4.1">revolution in </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">natural language processing</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">NLP</span></strong><span class="koboSpan" id="kobo.8.1">), and today’s </span><strong class="bold"><span class="koboSpan" id="kobo.9.1">large language models</span></strong><span class="koboSpan" id="kobo.10.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.11.1">LLMs</span></strong><span class="koboSpan" id="kobo.12.1">) are</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.13.1"> all based on transformers trained at scale. </span><span class="koboSpan" id="kobo.13.2">In this chapter, we will see what happens when we train huge transformers (more than 100 billion parameters) with giant datasets. </span><span class="koboSpan" id="kobo.13.3">We will focus on how to enable this training at scale, how to fine-tune similar modern ones, how to get more manageable models, and how to extend them to multimodal data. </span><span class="koboSpan" id="kobo.13.4">At the same time, we will also see what the limitations of these models are and what techniques are used to try to overcome </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">these limitations.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">In this chapter, we'll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.17.1">Discovering the evolution </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">of LLMs</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Instruction tuning, fine-tuning, </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">and alignment</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Exploring smaller and more </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">efficient LLMs</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Exploring </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">multimodal models</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">Understanding hallucinations and ethical and </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">legal issues</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.27.1">Prompt engineering</span></span></li>
</ul>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.28.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.29.1">Most of this code can be run on a CPU, but it is preferable to run it on a GPU. </span><span class="koboSpan" id="kobo.29.2">The code is written in PyTorch and uses standard libraries for the most part (PyTorch, Hugging Face Transformers, and so on). </span><span class="koboSpan" id="kobo.29.3">The code can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr3"><span class="No-Break"><span class="koboSpan" id="kobo.31.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr3</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.32.1">.</span></span></p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.33.1">Discovering the evolution of LLMs</span></h1>
<p><span class="koboSpan" id="kobo.34.1">An</span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.35.1"> LLM is a transformer (although different architectures are beginning to emerge today). </span><span class="koboSpan" id="kobo.35.2">In general, an LLM is defined as a model that has more than 10 billion parameters. </span><span class="koboSpan" id="kobo.35.3">Although this number may seem arbitrary, some properties emerge with scale. </span><span class="koboSpan" id="kobo.35.4">These models are designed to understand and generate human language, and over time, they have acquired the ability to generate code and more. </span><span class="koboSpan" id="kobo.35.5">To achieve this beyond parameter size, they are trained with a huge amount of data. </span><span class="koboSpan" id="kobo.35.6">Today’s LLMs </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.36.1">are almost all </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.37.1">trained on </span><strong class="bold"><span class="koboSpan" id="kobo.38.1">next-word prediction</span></strong><span class="koboSpan" id="kobo.39.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.40.1">autoregressive </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.41.1">language modeling</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">Parameter growth has been motivated in the transformer field by </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">different aspects:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.45.1">Learnability</span></strong><span class="koboSpan" id="kobo.46.1">: According to the scaling law, more parameters should lead to greater capabilities and a greater understanding of nuances and complexities in </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">the data</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.48.1">Expressiveness</span></strong><span class="koboSpan" id="kobo.49.1">: The model can express more complex functions, thus increasing the ability to generalize and reducing the risk </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">of overfitting</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.51.1">Memory</span></strong><span class="koboSpan" id="kobo.52.1">: A larger number of parameters allows for internalizing more knowledge (information, entities, differences </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">in topics)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.54.1">In the next subsections, we will discuss in detail all these elements to explain what is happening in the transition from the transformer to </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">the LLM.</span></span></p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.56.1">The scaling law</span></h2>
<p><span class="koboSpan" id="kobo.57.1">It may </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.58.1">seem surprising that </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.59.1">such large models are trained with such a simple task as </span><strong class="bold"><span class="koboSpan" id="kobo.60.1">language modeling</span></strong><span class="koboSpan" id="kobo.61.1">. </span><span class="koboSpan" id="kobo.61.2">Many </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.62.1">practical </span><strong class="bold"><span class="koboSpan" id="kobo.63.1">natural language</span></strong><span class="koboSpan" id="kobo.64.1"> tasks</span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.65.1"> can be represented as next-word prediction. </span><span class="koboSpan" id="kobo.65.2">This flexibility allows us to use LLMs in different contexts. </span><span class="koboSpan" id="kobo.65.3">For example, sentiment analysis can be cast as a next-word prediction. </span><span class="koboSpan" id="kobo.65.4">The sentence “</span><em class="italic"><span class="koboSpan" id="kobo.66.1">The sentiment of the sentence: ‘I like Pizza’ is</span></em><span class="koboSpan" id="kobo.67.1">” can be used as input for an LLM, and we can extract the probability for the next token being </span><em class="italic"><span class="koboSpan" id="kobo.68.1">positive</span></em><span class="koboSpan" id="kobo.69.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.70.1">negative</span></em><span class="koboSpan" id="kobo.71.1">. </span><span class="koboSpan" id="kobo.71.2">We can then assign the sentiment depending on which of the two has the higher probability. </span><span class="koboSpan" id="kobo.71.3">Notice how this probability is a function </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">of context:</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">P(positive| </span><em class="italic"><span class="koboSpan" id="kobo.74.1">The sentiment of the sentence: ‘I like </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.75.1">Pizza’ is</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">)</span></span></p>
<p><span class="koboSpan" id="kobo.77.1">P(negative| </span><em class="italic"><span class="koboSpan" id="kobo.78.1">The sentiment of the sentence: ‘I like </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.79.1">Pizza’ is</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">)</span></span></p>
<p><span class="koboSpan" id="kobo.81.1">Similarly, we can use the same approach for other </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.82.1">tasks. </span><strong class="bold"><span class="koboSpan" id="kobo.83.1">Question answering</span></strong><span class="koboSpan" id="kobo.84.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.85.1">QA</span></strong><span class="koboSpan" id="kobo.86.1">) with an LLM can be thought of as generating the probability of the right answer given the question. </span><span class="koboSpan" id="kobo.86.2">In text summarization, we want to generate given the </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">original context:</span></span></p>
<p><span class="koboSpan" id="kobo.88.1">QA: </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">P(answer| question)</span></span></p>
<p><span class="koboSpan" id="kobo.90.1">Text summarization: </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">P(summary|original article)</span></span></p>
<p><span class="koboSpan" id="kobo.92.1">In the </span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.93.1">following diagram, we can see that using language modeling, we </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.94.1">can solve almost any task. </span><span class="koboSpan" id="kobo.94.2">For example, here, the answer is the most probable token given the previous sequence (</span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">the question):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.96.1"><img alt="Figure 3.1 – Rephrasing of any task as LM" src="image/B21257_03_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.97.1">Figure 3.1 – Rephrasing of any task as LM</span></p>
<p><span class="koboSpan" id="kobo.98.1">What we need is a dataset large enough for the model to both learn knowledge and use that knowledge for tasks. </span><span class="koboSpan" id="kobo.98.2">For this, specific datasets are assembled for training an LLM. </span><span class="koboSpan" id="kobo.98.3">These datasets typically consist of billions of words obtained from various sources (internet, books, articles, GitHub, different languages, and so on). </span><span class="koboSpan" id="kobo.98.4">For example, </span><strong class="bold"><span class="koboSpan" id="kobo.99.1">GPT-3</span></strong><span class="koboSpan" id="kobo.100.1"> was trained with Common </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.101.1">Crawl (web crawl data, 410 billion tokens), Books1 and Books2 (book corpora, 12 billion and 55 billion tokens, respectively), and Wikipedia (3 billion tokens). </span><span class="koboSpan" id="kobo.101.2">Such diversity provides specific knowledge but also examples </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">of tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.103.1">In parallel with the growth of training datasets (today, we are talking about more than a trillion tokens), the number of parameters has grown. </span><span class="koboSpan" id="kobo.103.2">The number of parameters in a transformer depends on </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">three factors:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.105.1">Embedding layer</span></strong><span class="koboSpan" id="kobo.106.1">: The number of parameters on the size of the vector and the vocabulary (which, especially for multi-language models, can be very large). </span><span class="koboSpan" id="kobo.106.2">Attention mechanisms are the heaviest component and hold the </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">most parameters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">Self-attention mechanism</span></strong><span class="koboSpan" id="kobo.109.1">: This component includes multiple weight matrices that can grow in size with context length. </span><span class="koboSpan" id="kobo.109.2">Also, there can be multiple heads per </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">single self-attention.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.111.1">Depth</span></strong><span class="koboSpan" id="kobo.112.1">: Transformers are composed of multiple transformer blocks, and increasing the number of these blocks directly adds more parameters to </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">the model</span></span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.115.1">GPT-3 </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.116.1">and </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.117.1">other studies have shown that the performance of LLMs depends mainly on three factors: model size (number of parameters), data size (the size of the training dataset), and computing size (amount of computing). </span><span class="koboSpan" id="kobo.117.2">So, in theory, to increase the performance of our model, we should enlarge the model (add layers or attention heads), increase the size of the pre-training dataset, and train it for more epochs. </span><span class="koboSpan" id="kobo.117.3">These factors have been related by OpenAI with the</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.118.1"> so-called </span><strong class="bold"><span class="koboSpan" id="kobo.119.1">scaling law</span></strong><span class="koboSpan" id="kobo.120.1">. </span><span class="koboSpan" id="kobo.120.2">From a model with a number of parameters </span><em class="italic"><span class="koboSpan" id="kobo.121.1">N</span></em><span class="koboSpan" id="kobo.122.1">, a dataset </span><em class="italic"><span class="koboSpan" id="kobo.123.1">D</span></em><span class="koboSpan" id="kobo.124.1">, and computing amount </span><em class="italic"><span class="koboSpan" id="kobo.125.1">C</span></em><span class="koboSpan" id="kobo.126.1">, if two parameters are constant, the loss </span><em class="italic"><span class="koboSpan" id="kobo.127.1">L</span></em><span class="koboSpan" id="kobo.128.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">the following:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>L</mi><mfenced close=")" open="("><mi>N</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>N</mi></msub></msup><mi>L</mi><mfenced close=")" open="("><mi>D</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>D</mi></msub></msup><mi>L</mi><mfenced close=")" open="("><mi>C</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mi>c</mi></msub><mi>C</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>C</mi></msub></msup></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.130.1">This is represented visually in the </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<span class="koboSpan" id="kobo.132.1"><img alt="Figure 3.2 – Language modeling performance improves smoothly with the increase of model size, dataset size, and amount of computing (https://arxiv.org/pdf/2001.08361)" src="image/B21257_03_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.133.1">Figure 3.2 – Language modeling performance improves smoothly with the increase of model size, dataset size, and amount of computing (</span><a href="https://arxiv.org/pdf/2001.08361"><span class="koboSpan" id="kobo.134.1">https://arxiv.org/pdf/2001.08361</span></a><span class="koboSpan" id="kobo.135.1">)</span></p>
<p><span class="koboSpan" id="kobo.136.1">The loss is, in this case, the cross-entropy loss. </span><span class="koboSpan" id="kobo.136.2">In successive studies, OpenAI has shown that this loss can be decomposed </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.137.1">into </span><strong class="bold"><span class="koboSpan" id="kobo.138.1">irreducible loss</span></strong><span class="koboSpan" id="kobo.139.1"> (which cannot be eliminated because it is related to data entropy) and reducible loss. </span><span class="koboSpan" id="kobo.139.2">This scaling law, in other words, allows us to calculate the desired performance of the model before training it. </span><span class="koboSpan" id="kobo.139.3">We can decide </span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.140.1">whether to invest more in enlarging the model or the dataset to reduce the loss (improve performance). </span><span class="koboSpan" id="kobo.140.2">However, these constants are dependent on the architecture and other </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">training choices:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<span class="koboSpan" id="kobo.142.1"><img alt="Figure 3.3 – Scaling law for an LLM" src="image/B21257_03_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.143.1">Figure 3.3 – Scaling law for an LLM</span></p>
<p><span class="koboSpan" id="kobo.144.1">Although this scaling law</span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.145.1"> has been taken for granted, the reality is more nuanced than it seems. </span><span class="koboSpan" id="kobo.145.2">According to DeepMind’s Chinchilla (</span><a href="https://arxiv.org/abs/2203.15556"><span class="koboSpan" id="kobo.146.1">https://arxiv.org/abs/2203.15556</span></a><span class="koboSpan" id="kobo.147.1">), performance depends much more on the number of tokens than OpenAI believes. </span><span class="koboSpan" id="kobo.147.2">So, LLMs would currently be underfitted because they are trained with fewer tokens than expected. </span><span class="koboSpan" id="kobo.147.3">Meta’s Llama also states that not just any tokens will do, but they must be of quality. </span><span class="koboSpan" id="kobo.147.4">So, not all tokens count the same, and according to other authors, using tokens produced by other models is just a more sophisticated form of distillation. </span><span class="koboSpan" id="kobo.147.5">In other words, to train a model at its best, you need a large amount of tokens, and they should be preferentially produced by humans and not synthetics. </span><span class="koboSpan" id="kobo.147.6">Different studies showed the potential risk of the model collapsing when trained with synthetic data. </span><span class="koboSpan" id="kobo.147.7">In several cases, it has been shown that the model, when trained with synthetic data, has a</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.148.1"> substantial decrease in performance (</span><strong class="bold"><span class="koboSpan" id="kobo.149.1">model collapse</span></strong><span class="koboSpan" id="kobo.150.1">) or may forget some of the skills it has </span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.151.1">learned (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.152.1">catastrophic forgetting</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">In any case, the scaling law is of great interest because it allows us to experiment with different architectures and variants on smaller models and then scale the model and training until the desired performance is achieved. </span><span class="koboSpan" id="kobo.154.2">A model with more than 100 billion parameters is expensive to train (in terms of architecture, time, and money), so it is better to experiment with a small proxy model and then leverage what has been learned for training the larger model. </span><span class="koboSpan" id="kobo.154.3">Also, training such a large model can encounter issues (such as training spikes), and being able to predict performance with an accurate scaling law is an active area </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">of research.</span></span></p>
<p><span class="koboSpan" id="kobo.156.1">This scaling law also </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.157.1">monitors performance only in terms of </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.158.1">loss. </span><span class="koboSpan" id="kobo.158.2">As mentioned previously, many tasks can be defined in terms of language modeling (LM), so intuitively, better performance in LM also means better performance in downstream tasks. </span><span class="koboSpan" id="kobo.158.3">Today, however, we try to create scaling laws that are instead specific to performance in some desired tasks (if we want a model specifically trained as a code assistant, we are more interested in its performance in these tasks than in its </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">overall performance).</span></span></p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.160.1">Emergent properties</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.161.1">Emergent properties</span></strong><span class="koboSpan" id="kobo.162.1"> of a</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.163.1"> model are the main justification </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.164.1">for why we have gone from 1 billion parameters to over 100 billion. </span><span class="koboSpan" id="kobo.164.2">Emergent abilities are defined as properties that are not present in a small model but emerge in a large model. </span><span class="koboSpan" id="kobo.164.3">The second characteristic is that they emerge abruptly at a certain scale. </span><span class="koboSpan" id="kobo.164.4">In other words, a model has random performances in a certain ability until they emerge when a certain size is reached. </span><span class="koboSpan" id="kobo.164.5">These properties cannot be predicted beforehand but only observed at a certain scale, called</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.165.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.166.1">critical scale</span></strong><span class="koboSpan" id="kobo.167.1">. </span><span class="koboSpan" id="kobo.167.2">After this critical size, performance increases linearly with the increase in size. </span><span class="koboSpan" id="kobo.167.3">Then, the model goes from near-zero performance to near-state-of-the-art after a certain critical point, thus showing a discontinuous rhythm. </span><span class="koboSpan" id="kobo.167.4">This process is also called phase transition. </span><span class="koboSpan" id="kobo.167.5">It is like a child who grows up appearing to be unable to speak, then beyond a certain age begins to articulate words, and then their skills grow linearly </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">over time.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.169.1"><img alt="Figure 3.4 – Example of an emergent property in an LLM" src="image/B21257_03_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.170.1">Figure 3.4 – Example of an emergent property in an LLM</span></p>
<p><span class="koboSpan" id="kobo.171.1">Typically, these</span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.172.1"> skills are related to complex skills </span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.173.1">such as mathematical reasoning or multistep processes. </span><span class="koboSpan" id="kobo.173.2">The fact that they emerge only beyond a certain scale justified the growth of such models, with the hope that beyond a certain scale, other properties </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">would appear:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<span class="koboSpan" id="kobo.175.1"><img alt="Figure 3.5 – Examples of emergent properties in different LLM families (https://arxiv.org/pdf/2206.07682)" src="image/B21257_03_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.176.1">Figure 3.5 – Examples of emergent properties in different LLM families (</span><a href="https://arxiv.org/pdf/2206.07682"><span class="koboSpan" id="kobo.177.1">https://arxiv.org/pdf/2206.07682</span></a><span class="koboSpan" id="kobo.178.1">)</span></p>
<p><span class="koboSpan" id="kobo.179.1">These </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.180.1">properties do not all emerge at the same model </span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.181.1">size. </span><span class="koboSpan" id="kobo.181.2">Some properties would emerge beyond 10 billion (arithmetic computation), beyond 100 billion (self-evaluation, </span><strong class="bold"><span class="koboSpan" id="kobo.182.1">figure-of-speech</span></strong><span class="koboSpan" id="kobo.183.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.184.1">FoS</span></strong><span class="koboSpan" id="kobo.185.1">) detection, logical deduction, and so on), and </span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.186.1">others even beyond 500 billion (causal judgment, geometric shapes, and </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.188.1">For the authors of </span><em class="italic"><span class="koboSpan" id="kobo.189.1">Emergent Abilities of Large Language Models</span></em><span class="koboSpan" id="kobo.190.1"> (</span><a href="https://arxiv.org/pdf/2206.07682"><span class="koboSpan" id="kobo.191.1">https://arxiv.org/pdf/2206.07682</span></a><span class="koboSpan" id="kobo.192.1">), reasoning tasks (especially those involving multiple steps) are difficult for LMs. </span><span class="koboSpan" id="kobo.192.2">These capabilities would appear naturally after 100 billion parameters. </span><span class="koboSpan" id="kobo.192.3">Similarly, beyond this threshold, the model is capable of understanding and following instructions (instruction following) without necessarily giving it examples of how to follow them. </span><span class="koboSpan" id="kobo.192.4">From this, it follows that larger models would be capable of executing programs (</span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">coding ability).</span></span></p>
<p><span class="koboSpan" id="kobo.194.1">Interest in these emergent properties has cooled, however, because subsequent studies question them. </span><span class="koboSpan" id="kobo.194.2">LLMs do indeed exhibit these capabilities, but according to further studies, it would simply be more noticeable once the LLM has reached a certain performance limit. </span><span class="koboSpan" id="kobo.194.3">Moreover, it seems that success in these tasks is </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">measured poorly.</span></span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.196.1">Context length</span></h2>
<p><span class="koboSpan" id="kobo.197.1">LLMs process text in chunks, a fixed </span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.198.1">context window of a specific number of tokens. </span><span class="koboSpan" id="kobo.198.2">The size of </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.199.1">this context length defines how much information they can process at a given time. </span><span class="koboSpan" id="kobo.199.2">The greater the context length, the more information a model can handle at a given time. </span><span class="koboSpan" id="kobo.199.3">Similarly, the computational cost grows quadratically. </span><span class="koboSpan" id="kobo.199.4">So, a model with a context length of 4,096 tokens needs to do 64 times more computation than one of 512. </span><span class="koboSpan" id="kobo.199.5">A longer context length allows for capturing long-range dependencies in a text, and this is related to performance in </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">specific tasks:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.201.1">Document summarization</span></strong><span class="koboSpan" id="kobo.202.1">: More</span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.203.1"> context allows for more consistent and concise summarization, allowing for better capture of information in the document and its relationships. </span><span class="koboSpan" id="kobo.203.2">The model captures entities and what they are related to in the </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">entire document.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.205.1">QA</span></strong><span class="koboSpan" id="kobo.206.1">: The model</span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.207.1"> can find complex relationships that underlie the right answer. </span><span class="koboSpan" id="kobo.207.2">Also, in multi-turn questions, the model is aware of previous answers </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">and questions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.209.1">Language translation</span></strong><span class="koboSpan" id="kobo.210.1">: The model</span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.211.1"> better preserves context, especially if there are long documents to be translated (especially if there are complex nuances). </span><span class="koboSpan" id="kobo.211.2">Bigger context lengths help with technical documents, technical jargon, polysemic items, </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">and acronyms.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.213.1">Conversational AI</span></strong><span class="koboSpan" id="kobo.214.1">: The </span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.215.1">model can conduct better tracking of the </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">entire conversation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.217.1">As we can see in the following </span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.218.1">figure, the larger the context length, the more data the model</span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.219.1"> can access in one prompt. </span><span class="koboSpan" id="kobo.219.2">Only one review can be seen by the model with a context length of 512, while a model with a larger context window can analyze hundreds </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">of them:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<span class="koboSpan" id="kobo.221.1"><img alt="Figure 3.6 – Number of reviews that can be fit with an increasing context length window" src="image/B21257_03_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.222.1">Figure 3.6 – Number of reviews that can be fit with an increasing context length window</span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.223.1">Mixture of experts</span></h2>
<p><span class="koboSpan" id="kobo.224.1">As we have seen, there is an intricate relationship between the amount of data, model scale, and computing budget. </span><span class="koboSpan" id="kobo.224.2">Given a fixed computing budget, it is better to train a larger model with fewer steps. </span><span class="koboSpan" id="kobo.224.3">A </span><strong class="bold"><span class="koboSpan" id="kobo.225.1">mixture of experts</span></strong><span class="koboSpan" id="kobo.226.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.227.1">MoE</span></strong><span class="koboSpan" id="kobo.228.1">) allows </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.229.1">one to train a model with less computing by scaling up the model with the same computing budget (which results in having a model as good as a dense one in less time). </span><span class="koboSpan" id="kobo.229.2">MoEs are, in general, made up of </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">two components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.231.1">Sparse MoE layers</span></strong><span class="koboSpan" id="kobo.232.1">: Each </span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.233.1">layer consists of several experts (typically eight, but can be more), and each expert is a </span><strong class="bold"><span class="koboSpan" id="kobo.234.1">neural network</span></strong><span class="koboSpan" id="kobo.235.1"> (in the simplest form, a </span><strong class="bold"><span class="koboSpan" id="kobo.236.1">feed-forward network</span></strong><span class="koboSpan" id="kobo.237.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.238.1">FFN</span></strong><span class="koboSpan" id="kobo.239.1">) layer, but they can </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.240.1">also consist of </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">multiple layers).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.242.1">A gate network or router</span></strong><span class="koboSpan" id="kobo.243.1">: This </span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.244.1">component decides what data is sent to each of the experts. </span><span class="koboSpan" id="kobo.244.2">In the case of an LLM, the router decides which tokens are seen by one or more experts. </span><span class="koboSpan" id="kobo.244.3">The router has learnable parameters that are trained during pre-training along with the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">the model.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.246.1">You can see an example of an MoE layer in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.247.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.248.1">.7</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<span class="koboSpan" id="kobo.250.1"><img alt="Figure 3.7 – Example of an MoE layer: The router decides to which expert the token is sent; in this case, the expert is a simple FFN layer (https://arxiv.org/pdf/2101.03961)" src="image/B21257_03_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.251.1">Figure 3.7 – Example of an MoE layer: The router decides to which expert the token is sent; in this case, the expert is a simple FFN layer (</span><a href="https://arxiv.org/pdf/2101.03961"><span class="koboSpan" id="kobo.252.1">https://arxiv.org/pdf/2101.03961</span></a><span class="koboSpan" id="kobo.253.1">)</span></p>
<p><span class="koboSpan" id="kobo.254.1">The idea behind </span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.255.1">MoEs is that each of the experts focuses on a different subset of the training (or, more formally, a different region of the input space) and the router learns when to recall this expertise. </span><span class="koboSpan" id="kobo.255.2">This is called sparse computation because the model is not active on all inputs to the </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">same model.</span></span></p>
<p><span class="koboSpan" id="kobo.257.1">This system has</span><a id="_idIndexMarker225"/> <span class="No-Break"><span class="koboSpan" id="kobo.258.1">several advantages:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.259.1">Pre-training is faster compared to a dense model (classic transformer). </span><span class="koboSpan" id="kobo.259.2">The model is faster in inference since not all experts are used at the same time on </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">all data.</span></span></li>
<li><span class="koboSpan" id="kobo.261.1">The system is flexible, can handle complex distribution, and each expert can specialize in </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">a subdomain.</span></span></li>
<li><span class="koboSpan" id="kobo.263.1">It is much more scalable since we can have additional experts </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">if needed.</span></span></li>
<li><span class="koboSpan" id="kobo.265.1">Better generalization, because we can average the expert predictions (wisdom of </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">the crowd).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.267.1">There are some </span><a id="_idIndexMarker226"/><span class="No-Break"><span class="koboSpan" id="kobo.268.1">disadvantages, however:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.269.1">It requires high VRAM because all the experts have to be loaded into </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">memory anyway.</span></span></li>
<li><span class="koboSpan" id="kobo.271.1">Training is more complex and can lead to overfitting. </span><span class="koboSpan" id="kobo.271.2">Also, without some accommodations, the model might use only the two or three most </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">popular experts.</span></span></li>
<li><span class="koboSpan" id="kobo.273.1">Fine-tuning is more complex, but new studies are solving the problem. </span><span class="koboSpan" id="kobo.273.2">MoE can be efficiently distilled, and we can also </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">extract subnetworks.</span></span></li>
<li><span class="koboSpan" id="kobo.275.1">More complex interpretability, since we have now </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">additional components.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.277.1">This is why many of today’s large models are MoE (for example, GPT-4 or Gemini). </span><span class="koboSpan" id="kobo.277.2">In the next section, we will see once an LLM is pre-trained how to adapt it to better interact with users or how we can fine-tune such a </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">large model.</span></span></p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.279.1">Instruction tuning, fine-tuning, and alignment</span></h1>
<p><span class="koboSpan" id="kobo.280.1">Fine-tuning such large </span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.281.1">models is potentially very expensive. </span><span class="koboSpan" id="kobo.281.2">In classical fine-tuning, the idea</span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.282.1"> is to fit the weights of a model for a task or a new domain. </span><span class="koboSpan" id="kobo.282.2">Even if it is a slight update of the weights for a few steps, for a model of more than 100 billion parameters, this means having large hardware infrastructure and significant costs. </span><span class="koboSpan" id="kobo.282.3">So, we need a method that allows us to have efficient and low-cost fine-tuning and preferentially keeping the model </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">weights frozen.</span></span></p>
<p><span class="koboSpan" id="kobo.284.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.285.1">intrinsic rank hypothesis</span></strong><span class="koboSpan" id="kobo.286.1"> suggests</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.287.1"> that we can capture significant changes that occur in a neural network using a lower-dimensional representation. </span><span class="koboSpan" id="kobo.287.2">In the case of fine-tuning, the model weights after fine-tuning can be defined in </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">this way:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mi>W</mi><mo>′</mo></mrow><mi>X</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>:</mo><mrow><mi>W</mi><mo>′</mo></mrow><mo>=</mo><mi>W</mi><mo>+</mo><mo>∆</mo><mi>W</mi></mrow></mrow></math></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.289.1">∆W</span></em><span class="koboSpan" id="kobo.290.1"> represents the update of the weights during fine-tuning. </span><span class="koboSpan" id="kobo.290.2">For the intrinsic rank hypothesis, not all of these elements of </span><em class="italic"><span class="koboSpan" id="kobo.291.1">∆W</span></em><span class="koboSpan" id="kobo.292.1"> are important, and instead, we can represent it as the product of two matrices with small dimensions </span><em class="italic"><span class="koboSpan" id="kobo.293.1">A</span></em><span class="koboSpan" id="kobo.294.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.295.1">B</span></em><span class="koboSpan" id="kobo.296.1"> (low-rank matrices). </span><span class="koboSpan" id="kobo.296.2">So, in this case, the model weights remain frozen, but we just need to train these </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">two matrices:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mi>W</mi><mo>′</mo></mrow><mi>X</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>:</mo><mrow><mi>W</mi><mo>′</mo></mrow><mo>=</mo><mi>W</mi><mo>+</mo><mi>B</mi><mi>A</mi></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.298.1">A matrix can be decomposed into two smaller matrices that, when multiplied, give the original matrix. </span><span class="koboSpan" id="kobo.298.2">Also, a matrix (especially larger ones) contains a lot of redundant information. </span><span class="koboSpan" id="kobo.298.3">A matrix can be reduced into a set of linearly independent vectors (the number of linearly independent vectors needed to define a matrix is </span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.299.1">called a </span><strong class="bold"><span class="koboSpan" id="kobo.300.1">rank</span></strong><span class="koboSpan" id="kobo.301.1">). </span><span class="koboSpan" id="kobo.301.2">With that in mind, the idea is to find two matrices that have a smaller rank than the original one and that multiplied with each other give us the same matrix update weights as if we had done fine-tuning. </span><span class="koboSpan" id="kobo.301.3">This</span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.302.1"> process is called </span><strong class="bold"><span class="koboSpan" id="kobo.303.1">Low-Rank </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.304.1">Adaptation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.305.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.306.1">LoRA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.308.1">LLMs are over-parametrized. </span><span class="koboSpan" id="kobo.308.2">Although this is beneficial during the pre-training stage, it makes fine-tuning very expensive. </span><span class="koboSpan" id="kobo.308.3">Because the weight matrices of an LLM have a lot of linear dependence, there is a lot of redundant information, which is especially useless for domain adaptation. </span><span class="koboSpan" id="kobo.308.4">So, we can learn much smaller matrices (</span><em class="italic"><span class="koboSpan" id="kobo.309.1">A</span></em><span class="koboSpan" id="kobo.310.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.311.1">B</span></em><span class="koboSpan" id="kobo.312.1">) at a much </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">lower cost.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<span class="koboSpan" id="kobo.314.1"><img alt="Figure 3.8 – Classical fine-tuning versus LoRA" src="image/B21257_03_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.315.1">Figure 3.8 – Classical fine-tuning versus LoRA</span></p>
<p><span class="koboSpan" id="kobo.316.1">In LoRA, we </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.317.1">keep the original weights of the LLM frozen. </span><span class="koboSpan" id="kobo.317.2">We then create two matrices (</span><em class="italic"><span class="koboSpan" id="kobo.318.1">A</span></em><span class="koboSpan" id="kobo.319.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.320.1">B</span></em><span class="koboSpan" id="kobo.321.1">) that, when multiplied together, will have the same dimensions</span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.322.1"> as the model’s weight matrices (</span><em class="italic"><span class="koboSpan" id="kobo.323.1">W</span></em><span class="koboSpan" id="kobo.324.1">). </span><span class="koboSpan" id="kobo.324.2">During fine-tuning, we pass the input </span><em class="italic"><span class="koboSpan" id="kobo.325.1">X</span></em><span class="koboSpan" id="kobo.326.1"> for the frozen model and the change matrix (the product </span><em class="italic"><span class="koboSpan" id="kobo.327.1">AB</span></em><span class="koboSpan" id="kobo.328.1">) and get the output. </span><span class="koboSpan" id="kobo.328.2">With this output, we calculate the loss, and using this loss, we update the matrices </span><em class="italic"><span class="koboSpan" id="kobo.329.1">A</span></em><span class="koboSpan" id="kobo.330.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.331.1">B</span></em><span class="koboSpan" id="kobo.332.1"> (via classical backpropagation). </span><span class="koboSpan" id="kobo.332.2">We continue this process until we are satisfied with </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">the result.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.334.1"><img alt="Figure 3.9 – Different-ranked matrices to obtain the change weight matrix" src="image/B21257_03_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.335.1">Figure 3.9 – Different-ranked matrices to obtain the change weight matrix</span></p>
<p><span class="koboSpan" id="kobo.336.1">In LoRA, we have a hyper-parameter </span><em class="italic"><span class="koboSpan" id="kobo.337.1">r</span></em><span class="koboSpan" id="kobo.338.1"> describing the depth of the </span><em class="italic"><span class="koboSpan" id="kobo.339.1">A</span></em><span class="koboSpan" id="kobo.340.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.341.1">B</span></em><span class="koboSpan" id="kobo.342.1"> matrices. </span><span class="koboSpan" id="kobo.342.2">The greater the </span><em class="italic"><span class="koboSpan" id="kobo.343.1">r</span></em><span class="koboSpan" id="kobo.344.1"> value, the greater the amount of information these matrices have (but also a greater number of parameters and thus computational cost). </span><span class="koboSpan" id="kobo.344.2">The results show that even low-rank matrices perform </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">quite well.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">LoRA has </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">several advantages:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.348.1">It is efficient in </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.349.1">training (for GPT-3, a model of 175 billion parameters can be used efficiently by LoRA by training only 17.5 </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">million</span></span><span class="No-Break"><span class="koboSpan" id="kobo.351.1"> parameters).</span></span></li>
<li><span class="koboSpan" id="kobo.352.1">In inference, it does not increase the computational cost (it is an addition where we add the change matrix to the original </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">model weights).</span></span></li>
<li><span class="koboSpan" id="kobo.354.1">LoRA will not alter the original capabilities of the model. </span><span class="koboSpan" id="kobo.354.2">It also reduces the memory cost associated with saving checkpoints </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">during fine-tuning.</span></span></li>
<li><span class="koboSpan" id="kobo.356.1">We can create different change matrices for different applications </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">and domains.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.358.1">Another technique that focuses on training only added parameters</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.359.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.360.1">adapters</span></strong><span class="koboSpan" id="kobo.361.1">. </span><span class="koboSpan" id="kobo.361.2">In this case, we add tunable layers within transformer blocks. </span><span class="koboSpan" id="kobo.361.3">These adapters are small layers that </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.362.1">have </span><strong class="bold"><span class="koboSpan" id="kobo.363.1">autoencoder</span></strong><span class="koboSpan" id="kobo.364.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.365.1">AE</span></strong><span class="koboSpan" id="kobo.366.1">)-like structures. </span><span class="koboSpan" id="kobo.366.2">For example, if the fully connected layers have 1024 dimensions, the adapter projects to 24 and then reprojects to 1024. </span><span class="koboSpan" id="kobo.366.3">This means that we are adding fewer than 50K parameters per adapter. </span><span class="koboSpan" id="kobo.366.4">In the original paper, the authors showed that the addition of adapters achieved the same performance as</span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.367.1"> fine-tuning </span><strong class="bold"><span class="koboSpan" id="kobo.368.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.369.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.370.1">BERT</span></strong><span class="koboSpan" id="kobo.371.1">).  </span><span class="koboSpan" id="kobo.371.2">Adapter require only the additional training of 3.6 % more parameters. </span><span class="koboSpan" id="kobo.371.3">In contrast, fine tuning a model such as BERT in the traditional way means conducting training for all model parameters. </span><span class="koboSpan" id="kobo.371.4">This means that for the same performance, this method is computationally much </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">more efficient.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.373.1"><img alt="Figure 3.10 – How adapters are added to the transformer block (left); results show that the adapters can reach the performance of regular fine-tuning with much fewer parameters (right) (https://arxiv.org/pdf/1902.00751)" src="image/B21257_03_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.374.1">Figure 3.10 – How adapters are added to the transformer block (left); results show that the adapters can reach the performance of regular fine-tuning with much fewer parameters (right) (</span><a href="https://arxiv.org/pdf/1902.00751"><span class="koboSpan" id="kobo.375.1">https://arxiv.org/pdf/1902.00751</span></a><span class="koboSpan" id="kobo.376.1">)</span></p>
<p><span class="koboSpan" id="kobo.377.1">The advantages of adapters</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.378.1"> are that you can conduct fine-tuning by training far fewer parameters (a few million parameters for an LLM) and that the model retains the </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">original capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">However, many other methods try to solve the problem of conducting fine-tuning of the model without conducting training on the original parameters. </span><span class="koboSpan" id="kobo.380.2">For example, some techniques, such as </span><strong class="bold"><span class="koboSpan" id="kobo.381.1">prompt tuning</span></strong><span class="koboSpan" id="kobo.382.1">, prepend</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.383.1"> the model input embeddings with a trainable tensor that learns details associated with the new tasks. </span><strong class="bold"><span class="koboSpan" id="kobo.384.1">Prefix tuning</span></strong><span class="koboSpan" id="kobo.385.1"> is</span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.386.1"> another technique in which we add trainable tensors to the hidden states of all layers. </span><span class="koboSpan" id="kobo.386.2">These parameters are learned with gradient descent while the rest of the parameters remain frozen. </span><span class="koboSpan" id="kobo.386.3">Prompt tuning and prefix tuning can still cause instability during training. </span><span class="koboSpan" id="kobo.386.4">LoRA and adapters remain the most widely </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">used techniques:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<span class="koboSpan" id="kobo.388.1"><img alt="Figure 3.11 – Parameter-efficient fine-tuning methods taxonomy (https://arxiv.org/pdf/2303.15647)" src="image/B21257_03_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.389.1">Figure 3.11 – Parameter-efficient fine-tuning methods taxonomy (</span><a href="https://arxiv.org/pdf/2303.15647"><span class="koboSpan" id="kobo.390.1">https://arxiv.org/pdf/2303.15647</span></a><span class="koboSpan" id="kobo.391.1">)</span></p>
<p><span class="koboSpan" id="kobo.392.1">Although technically, it can be called a fine-tuning method, </span><strong class="bold"><span class="koboSpan" id="kobo.393.1">alignment</span></strong><span class="koboSpan" id="kobo.394.1"> is a method that with additional training attempts to align an LLM with </span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.395.1">human values. </span><span class="koboSpan" id="kobo.395.2">Indeed, with increasing model capabilities, there</span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.396.1"> is an increasing fear of ethical risks (which will be described in detail in a later section). </span><span class="koboSpan" id="kobo.396.2">Alignment is meant to reduce these risks by reducing the mismatch between mathematical training and the soft skills expected of a human being (helpful, honest, </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">and harmless):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<span class="koboSpan" id="kobo.398.1"><img alt="Figure 3.12 – Example to show the difference between the outputs before and after alignment (https://arxiv.org/pdf/2308.05374)" src="image/B21257_03_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.399.1">Figure 3.12 – Example to show the difference between the outputs before and after alignment (</span><a href="https://arxiv.org/pdf/2308.05374"><span class="koboSpan" id="kobo.400.1">https://arxiv.org/pdf/2308.05374</span></a><span class="koboSpan" id="kobo.401.1">)</span></p>
<p><span class="koboSpan" id="kobo.402.1">An LLM during pre-training is trained to be nothing more than a sophisticated autocomplete model (predict the next word). </span><span class="koboSpan" id="kobo.402.2">With this simple objective, however, the model learns a vast knowledge and a wide array of skills. </span><span class="koboSpan" id="kobo.402.3">Alignment is intended to allow the model to use </span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.403.1">these skills obtained in pre-training in line with human values. </span><span class="koboSpan" id="kobo.403.2">Since </span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.404.1">human values can be subjective and difficult to encode in a mathematical objective, it was thought to use human feedback. </span><span class="koboSpan" id="kobo.404.2">Behind the </span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.405.1">success of ChatGPT is </span><strong class="bold"><span class="koboSpan" id="kobo.406.1">Reinforcement Learning from Human Feedback</span></strong><span class="koboSpan" id="kobo.407.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.408.1">RLHF</span></strong><span class="koboSpan" id="kobo.409.1">), which precisely uses </span><strong class="bold"><span class="koboSpan" id="kobo.410.1">reinforcement learning</span></strong><span class="koboSpan" id="kobo.411.1"> to optimize an LLM based on </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">human </span></span><span class="No-Break"><a id="_idIndexMarker246"/></span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">feedback.</span></span></p>
<p><span class="koboSpan" id="kobo.414.1">RLHF consists of three </span><a id="_idIndexMarker247"/><span class="No-Break"><span class="koboSpan" id="kobo.415.1">main steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.416.1">Supervised fine-tuning (SFT)</span></strong><span class="koboSpan" id="kobo.417.1">: We </span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.418.1">select a list of prompts and ask human annotators to write outputs that match these prompts (from 10,000 to 100,000 pairs). </span><span class="koboSpan" id="kobo.418.2">We take a model that is not aligned (pre-trained LLM on a large text dataset) and fine-tune it on the prompts and the corresponding human-generated outputs. </span><span class="koboSpan" id="kobo.418.3">This is the SFT LLM, a model that tries to mimic </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">annotator responses.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.420.1">Training reward model</span></strong><span class="koboSpan" id="kobo.421.1">: We select a set of prompts and generate multiple outputs for each prompt using the SFT LLM. </span><span class="koboSpan" id="kobo.421.2">We then ask human annotators to rank them from preferred to less preferred (using criteria such as helpfulness or accuracy). </span><span class="koboSpan" id="kobo.421.3">Using this ranking, we train a reward model. </span><span class="koboSpan" id="kobo.421.4">The reward model takes as input the output of an LLM and produces a scalar reward signal as a measure of how well this output is aligned with </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">human preferences.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.423.1">RLHF</span></strong><span class="koboSpan" id="kobo.424.1">: We take a prompt </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.425.1">and generate an output from the SFT LLM. </span><span class="koboSpan" id="kobo.425.2">We use the trained reward model to predict a reward on the output. </span><span class="koboSpan" id="kobo.425.3">Using a </span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.426.1">reinforcement learning algorithm (</span><strong class="bold"><span class="koboSpan" id="kobo.427.1">Proximal Policy Optimization</span></strong><span class="koboSpan" id="kobo.428.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.429.1">PPO</span></strong><span class="koboSpan" id="kobo.430.1">)), we update the SFT LLM with the predicted reward. </span><span class="koboSpan" id="kobo.430.2">Adding a penalty term based on</span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.431.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.432.1">Kullback-Leibler</span></strong><span class="koboSpan" id="kobo.433.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.434.1">KL</span></strong><span class="koboSpan" id="kobo.435.1">) divergence prevents the model from straying too far from its original distribution (in other words, the output text remains consistent </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">after RHLF).</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer083">
<span class="koboSpan" id="kobo.437.1"><img alt="Figure 3.13 – Diagram illustrating the three-step process (https://arxiv.org/pdf/2203.02155)" src="image/B21257_03_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.438.1">Figure 3.13 – Diagram illustrating the three-step process (</span><a href="https://arxiv.org/pdf/2203.02155"><span class="koboSpan" id="kobo.439.1">https://arxiv.org/pdf/2203.02155</span></a><span class="koboSpan" id="kobo.440.1">)</span></p>
<p><span class="koboSpan" id="kobo.441.1">The method is not without its problems, though. </span><span class="koboSpan" id="kobo.441.2">Collecting human preference data is quite expensive and requires hiring part-time staff as annotators. </span><span class="koboSpan" id="kobo.441.3">These annotators must also be selected to avoid variability and different quality in responses. </span><span class="koboSpan" id="kobo.441.4">Second, the process is rather complex and unstable. </span><strong class="bold"><span class="koboSpan" id="kobo.442.1">Direct Preference Optimization</span></strong><span class="koboSpan" id="kobo.443.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.444.1">DPO</span></strong><span class="koboSpan" id="kobo.445.1">) is an alternative that attempts to solve part of these </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.446.1">problems by eliminating the need to have a reward model. </span><span class="koboSpan" id="kobo.446.2">In short, the dataset is created according to this format: </span><em class="italic"><span class="koboSpan" id="kobo.447.1">&lt;prompt, worse completion, better completion&gt;</span></em><span class="koboSpan" id="kobo.448.1">. </span><span class="koboSpan" id="kobo.448.2">DPO uses a loss function to increase the probability of better completion and decrease the probability of worse completion. </span><span class="koboSpan" id="kobo.448.3">This allows us to use backpropagation and avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">reinforcement learning:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<span class="koboSpan" id="kobo.450.1"><img alt="Figure 3.14 – DPO optimizes for human preferences while avoiding RL (https://arxiv.org/pdf/2305.18290)" src="image/B21257_03_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.451.1">Figure 3.14 – DPO optimizes for human preferences while avoiding RL (</span><a href="https://arxiv.org/pdf/2305.18290"><span class="koboSpan" id="kobo.452.1">https://arxiv.org/pdf/2305.18290</span></a><span class="koboSpan" id="kobo.453.1">)</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.454.1">Instruction tuning</span></strong><span class="koboSpan" id="kobo.455.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.456.1">IT</span></strong><span class="koboSpan" id="kobo.457.1">) is a</span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.458.1"> fine-tuning technique that is used to improve the model’s </span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.459.1">capabilities for various tasks and generally in following instructions. </span><span class="koboSpan" id="kobo.459.2">The principle is similar to alignment: the pre-trained model is trained to minimize word prediction on large corpora and not to execute instructions. </span><span class="koboSpan" id="kobo.459.3">Most user interactions with LLMs are requests to perform a specific task (write a text, create a function, summarize an article, and </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">so on):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<span class="koboSpan" id="kobo.461.1"><img alt="Figure 3.15 – General pipeline of IT (https://arxiv.org/pdf/2308.10792)" src="image/B21257_03_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.462.1">Figure 3.15 – General pipeline of IT (</span><a href="https://arxiv.org/pdf/2308.10792"><span class="koboSpan" id="kobo.463.1">https://arxiv.org/pdf/2308.10792</span></a><span class="koboSpan" id="kobo.464.1">)</span></p>
<p><span class="koboSpan" id="kobo.465.1">To solve this mismatch, IT has been proposed to increase the model’s capabilities and controllability. </span><span class="koboSpan" id="kobo.465.2">The pre-trained model is further trained on a dataset that is a constituted instructions-outputs pair (instructions for the model and the desired output). </span><span class="koboSpan" id="kobo.465.3">This dataset is constructed from instructions that can be either annotated by humans or generated by other LLMs (such as GPT-4). </span><span class="koboSpan" id="kobo.465.4">Thus, the idea is to train the model to solve a task with a desired output. </span><span class="koboSpan" id="kobo.465.5">The model is evaluated with the desired output, and we use this output to optimize the model. </span><span class="koboSpan" id="kobo.465.6">These instructions usually represent NLP tasks and are of various kinds (up to 61 different tasks in some datasets), including tasks such as QA, summarization, classification, translation, creating writing, and so on). </span><span class="koboSpan" id="kobo.465.7">These instructions can </span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.466.1">then also contain additional content (for example, in </span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.467.1">summarization, we also provide the text to be summarized). </span><span class="koboSpan" id="kobo.467.2">To build such a dataset, a greater variety of tasks has greater benefit (especially tasks where the model must conduct reasoning and better if steps to follow are present in the context). </span><span class="koboSpan" id="kobo.467.3">Instruction tuning has several advantages. </span><span class="koboSpan" id="kobo.467.4">It makes the model capable of adapting even to unseen tasks (ensuring versatility) and is computationally efficient. </span><span class="koboSpan" id="kobo.467.5">It can also be used to fit the model to specific tasks for a particular domain (medical, finance, and so on). </span><span class="koboSpan" id="kobo.467.6">Also, it can be used in conjunction with other alignment techniques such </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">as RLHF.</span></span></p>
<p><span class="koboSpan" id="kobo.469.1">Despite these tuning techniques, it has enabled great advancement in the field of LLMs. </span><span class="koboSpan" id="kobo.469.2">The limitation of these techniques is that annotators can often be biased, and it is expensive to obtain quality datasets. </span><span class="koboSpan" id="kobo.469.3">In addition, it is always expensive to train a model that has billions of parameters. </span><span class="koboSpan" id="kobo.469.4">In addition, according to some authors, using AI-written instructions (or tests generated by AI) works as a kind of distillation but is less advantageous than using texts written by humans. </span><span class="koboSpan" id="kobo.469.5">In the next section, we will discuss how to obtain small LLMs when we do not want to deal with </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">large LLMs.</span></span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.471.1">Exploring smaller and more efficient LLMs</span></h1>
<p><span class="koboSpan" id="kobo.472.1">LLMs show incredible capabilities but are also associated with large costs beyond training costs. </span><span class="koboSpan" id="kobo.472.2">Expensive infrastructure is also required for deployment, not to mention the costs associated with simple inference that grows with the number of parameters. </span><span class="koboSpan" id="kobo.472.3">These large LLMs are generalist models, and for many tasks, it is not necessary to have a model that has 100 billion parameters. </span><span class="koboSpan" id="kobo.472.4">Especially for many business cases, we need a model that can accomplish a specific task well. </span><span class="koboSpan" id="kobo.472.5">So, there are many cases where a </span><strong class="bold"><span class="koboSpan" id="kobo.473.1">small language model</span></strong><span class="koboSpan" id="kobo.474.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.475.1">SLM</span></strong><span class="koboSpan" id="kobo.476.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">is</span></span><span class="No-Break"><a id="_idIndexMarker257"/></span><span class="No-Break"><span class="koboSpan" id="kobo.478.1"> sufficient.</span></span></p>
<p><span class="koboSpan" id="kobo.479.1">SLMs tend to excel in specialized domains, and may therefore lose the contextual informativeness that comes from integrating various domains of knowledge. </span><span class="koboSpan" id="kobo.479.2">SLMs may lose some of the capabilities of LLMs or otherwise exhibit fewer reasoning skills (thus being less versatile). </span><span class="koboSpan" id="kobo.479.3">On the other hand, they consume far fewer resources and can be used on a commercial GPU or even CPU (or, in extreme cases, </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">cell phones).</span></span></p>
<p><span class="koboSpan" id="kobo.481.1">More extensive studies of small models show that shallow models (with few transformer blocks) excel in grammar but have problems with consistency. </span><span class="koboSpan" id="kobo.481.2">So, a few layers are sufficient for syntactic correctness, but more layers are required for content coherence and creativity. </span><span class="koboSpan" id="kobo.481.3">Models that have hidden sizes might struggle with the continuation of a story, as this capability requires an increase in hidden size to at least a size of 128. </span><span class="koboSpan" id="kobo.481.4">Higher embedding dimensions impact the ability to generate continuations that are more accurate, relevant, and sound more natural (small embeddings lead the model to generate nonsense, contradictions, and irrelevant outputs). </span><span class="koboSpan" id="kobo.481.5">Also, models with a single layer are not capable of following instructions (such as continuing a story according to an input); at least two layers are needed, and the capacity increases almost proportionally as the layers increase (a single layer of attention does not produce a sufficient </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">global representation).</span></span></p>
<p><span class="koboSpan" id="kobo.483.1">So, there is a trade-off </span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.484.1">between capacity and model size. </span><span class="koboSpan" id="kobo.484.2">In general, we can say that there are three main possibilities for obtaining small and </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">efficient LLMs:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.486.1">Training a small LLM from scratch</span></strong><span class="koboSpan" id="kobo.487.1">: For example, Mistral 7B or LLaMA 7B have been trained </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">from scratch</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.489.1">Knowledge distillation</span></strong><span class="koboSpan" id="kobo.490.1">: One leverages a larger model to train a smaller model for a specific task (this can also be done using an LLM and a small LLM that is pre-trained; for example, using GPT-4 </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">and BERT)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.492.1">Reducing the size of a model</span></strong><span class="koboSpan" id="kobo.493.1">: For example, we can reduce the size of an LLM such as Mistral 7B using techniques such as quantization </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">or pruning</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.495.1">We have already discussed in the previous chapter knowledge distillation, and since LLMs are transformers, the process is the same. </span><strong class="bold"><span class="koboSpan" id="kobo.496.1">Quantization</span></strong><span class="koboSpan" id="kobo.497.1">, on the other hand, is a different process in which we act on the representation of parameters within the model. </span><span class="koboSpan" id="kobo.497.2">Formally, quantization is defined as the process</span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.498.1"> of mapping the model weights from higher-precision data types to lower-precision ones. </span><span class="koboSpan" id="kobo.498.2">The weights of the LLMs are saved as tensors of real numbers that can be of different types (</span><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">float64</span></strong><span class="koboSpan" id="kobo.500.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">float16</span></strong><span class="koboSpan" id="kobo.502.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">int64</span></strong><span class="koboSpan" id="kobo.504.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">int8</span></strong><span class="koboSpan" id="kobo.506.1">, and so on). </span><span class="koboSpan" id="kobo.506.2">Float formats are used to save reals, while int formats can express only integers. </span><span class="koboSpan" id="kobo.506.3">Greater precision means that a weight can express a greater range. </span><span class="koboSpan" id="kobo.506.4">This for an LLM translates into more stable and more accurate training, though with the need for more hardware, memory, </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">and cost.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<span class="koboSpan" id="kobo.508.1"><img alt="Figure 3.16 – Example of the quantization process" src="image/B21257_03_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.509.1">Figure 3.16 – Example of the quantization process</span></p>
<p><span class="koboSpan" id="kobo.510.1">The problem is that the loss of accuracy for weights can translate into a substantial drop in model performance. </span><span class="koboSpan" id="kobo.510.2">Different</span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.511.1"> quantization techniques attempt to reduce the accuracy of a model by avoiding damage to the original performance. </span><span class="koboSpan" id="kobo.511.2">One of the most popular</span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.512.1"> techniques is </span><strong class="bold"><span class="koboSpan" id="kobo.513.1">affine quantization mapping</span></strong><span class="koboSpan" id="kobo.514.1">, which allows one to go from a higher-precision number to a lower-precision number using two factors. </span><span class="koboSpan" id="kobo.514.2">Considering </span><em class="italic"><span class="koboSpan" id="kobo.515.1">x</span></em><span class="koboSpan" id="kobo.516.1"> with range [α,β], we can get its quantized version x</span><span class="subscript"><span class="koboSpan" id="kobo.517.1">q</span></span> <span class="No-Break"><span class="koboSpan" id="kobo.518.1">ϵ</span></span><span class="No-Break"><span class="koboSpan" id="kobo.519.1"> [α</span></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.520.1">q</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">,βq]:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mrow><mi>s</mi><mo>=</mo><mfrac><mrow><mi>β</mi><mo>−</mo><mi>α</mi></mrow><mrow><msub><mi>β</mi><mi>q</mi></msub><mo>−</mo><msub><mi>α</mi><mi>q</mi></msub></mrow></mfrac><mi>z</mi><mo>=</mo><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>(</mo><mfrac><mrow><mi>β</mi><msub><mi>α</mi><mi>q</mi></msub><mo>−</mo><mi>α</mi><msub><mi>β</mi><mi>q</mi></msub></mrow><mrow><mi>β</mi><mo>−</mo><mi>α</mi></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.522.1">Rounding</span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.523.1"> is used to improve mapping. </span><span class="koboSpan" id="kobo.523.2">In practice, we also need to conduct clipping because, after mapping, the obtained value might be out of range of the new </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">data type.</span></span></p>
<p><span class="koboSpan" id="kobo.525.1">Not all model parameters are useful, both because there is a lot of linear dependence and because these models are practically underfitting. </span><span class="koboSpan" id="kobo.525.2">In the context of neural networks (and LLMs), the process of removing unnecessary weights is</span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.526.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.527.1">pruning</span></strong><span class="koboSpan" id="kobo.528.1">. </span><span class="koboSpan" id="kobo.528.2">This process refers to eliminating weights, connections, or even whole layers. </span><strong class="bold"><span class="koboSpan" id="kobo.529.1">Unstructured pruning</span></strong><span class="koboSpan" id="kobo.530.1"> is a simple </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.531.1">technique in which, taking a pre-trained model, we eliminate connections or individual neurons, zeroing parameters. </span><span class="koboSpan" id="kobo.531.2">In the simplest form, this means we set to zero the connections that have a value below a certain threshold (the weights that are already near zero do not contain much information). </span><span class="koboSpan" id="kobo.531.3">Unstructured pruning can create sparse models that have suboptimal performance in inference, though. </span><strong class="bold"><span class="koboSpan" id="kobo.532.1">Structured pruning</span></strong><span class="koboSpan" id="kobo.533.1">, on the other hand, is a more sophisticated technique in which we</span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.534.1"> eliminate neurons, groups of neurons, structural components, entire layers, or blocks. </span><span class="koboSpan" id="kobo.534.2">Structural pruning seeks to preserve the performance of the original model by balancing accuracy and compression. </span><span class="koboSpan" id="kobo.534.3">Algorithms and other optimization systems have been developed for this. </span><span class="koboSpan" id="kobo.534.4">The two kinds of pruning are demonstrated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<span class="koboSpan" id="kobo.536.1"><img alt="Figure 3.17 – Schematic representation of pruning; the white elements represent pruned elements" src="image/B21257_03_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.537.1">Figure 3.17 – Schematic representation of pruning; the white elements represent pruned elements</span></p>
<p><span class="koboSpan" id="kobo.538.1">For classical neural networks, most algorithms are based on eliminating the curvature of the loss versus the weights so that we can identify which weights are most important and which are not (a method called </span><strong class="bold"><span class="koboSpan" id="kobo.539.1">optimal brain surgeon</span></strong><span class="koboSpan" id="kobo.540.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.541.1">OBS</span></strong><span class="koboSpan" id="kobo.542.1">)). </span><span class="koboSpan" id="kobo.542.2">Alternatively, several approaches involve training the </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.543.1">model, reducing connectivity, and retraining the compressed model (this process can take several cycles). </span><span class="koboSpan" id="kobo.543.2">The problem with these classical approaches is that LLMs are composed of billions of parameters, and it would be too expensive to proceed with cycles of training and pruning. </span><span class="koboSpan" id="kobo.543.3">Some have, therefore, proposed possible fine-tuning of the model after pruning, but this for large LLMs is still computationally expensive. </span><span class="koboSpan" id="kobo.543.4">So, approaches are sought that can be used with LLMs without the need for retraining. </span><span class="koboSpan" id="kobo.543.5">This is not an easy task because overly aggressive pruning often leads to LLM collapse (many algorithms fail to remove more than 10% of the weights without avoiding collapse). </span><span class="koboSpan" id="kobo.543.6">Recently, approaches such as SparseGPT using pruning masks have achieved significant results (up to 60% compression on </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">170-billion</span></span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">-parameter models).</span></span></p>
<p><span class="koboSpan" id="kobo.546.1">Since the model output can also be seen as the sum of the outputs of the model layers plus the embedding of the input, there will be terms in this sum that do not contribute much. </span><span class="koboSpan" id="kobo.546.2">The problem is that these terms are not exactly independent, so eliminating layers can create mismatches. </span><span class="koboSpan" id="kobo.546.3">You can study the contribution of each layer by looking at the output, though. </span><span class="koboSpan" id="kobo.546.4">Also, in each layer, the transformer learns a representation of the data, and in a very deep model, some layers will learn a similar representation. </span><span class="koboSpan" id="kobo.546.5">There is usually a hierarchy, where the deeper layers learn a more specialized representation than the initial layers. </span><span class="koboSpan" id="kobo.546.6">Some studies have started from these assumptions to eliminate layers, especially deeper layers that have layers with more similar representation. </span><span class="koboSpan" id="kobo.546.7">The results show that larger models have many more redundant layers than smaller models and can be efficiently compressed without altering performance </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">too much:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<span class="koboSpan" id="kobo.548.1"><img alt="Figure 3.18 – Percentage of the dropped layer before the LLM collapse (https://arxiv.org/pdf/2403.17887v1)" src="image/B21257_03_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.549.1">Figure 3.18 – Percentage of the dropped layer before the LLM collapse (</span><a href="https://arxiv.org/pdf/2403.17887v1"><span class="koboSpan" id="kobo.550.1">https://arxiv.org/pdf/2403.17887v1</span></a><span class="koboSpan" id="kobo.551.1">)</span></p>
<p><span class="koboSpan" id="kobo.552.1">Pruning</span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.553.1"> allows the memory footprint to be reduced and the inference time to be reduced. </span><span class="koboSpan" id="kobo.553.2">It is also a technique that allows us to study the importance of various structural components. </span><span class="koboSpan" id="kobo.553.3">In addition, it can be combined with other techniques such as quantization for </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">further compression.</span></span></p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.555.1">Exploring multimodal models</span></h1>
<p><span class="koboSpan" id="kobo.556.1">LLMs, as by definition, are trained with</span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.557.1"> text and to generate text. </span><span class="koboSpan" id="kobo.557.2">On the other hand, efforts have been made since the advent of the transformer to extend the model to other modalities. </span><span class="koboSpan" id="kobo.557.3">The addition of multimodal input allows the model to improve its reasoning capabilities and also to develop others. </span><span class="koboSpan" id="kobo.557.4">Human speech conveys a whole range of information that is not present in written words: voice, intonation, pauses, and facial expressions enhance communication but can also drastically change the meaning of </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">a message.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">We saw earlier that text can be transformed into a numeric vector. </span><span class="koboSpan" id="kobo.559.2">If we can transform a data type into a vector, we can then feed it to transformer blocks. </span><span class="koboSpan" id="kobo.559.3">So, the idea is to find a way to get a latent representation for each data type. </span><span class="koboSpan" id="kobo.559.4">For images, a way to adapt it to images was presented shortly after the original transformer was published: the </span><strong class="bold"><span class="koboSpan" id="kobo.560.1">Vision Transformer</span></strong><span class="koboSpan" id="kobo.561.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.562.1">ViT</span></strong><span class="koboSpan" id="kobo.563.1">). </span><span class="koboSpan" id="kobo.563.2">ViTs are</span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.564.1"> superior in several tasks to </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">convolutional networks.</span></span></p>
<p><span class="koboSpan" id="kobo.566.1">ViTs are typically built by the encoder alone. </span><span class="koboSpan" id="kobo.566.2">Having taken an image, it is divided into 16 x 16 patches (each patch can be thought of as being a token of a text). </span><span class="koboSpan" id="kobo.566.3">This is because a simple pixel does not represent much information, so it is more convenient to take a group of pixels (a patch). </span><span class="koboSpan" id="kobo.566.4">Once divided into patches, these are flattened (as if they were a sequence of patches). </span><span class="koboSpan" id="kobo.566.5">One clarification: since an image has multiple channels (color or RGB images have three channels), these must also be considered. </span><span class="koboSpan" id="kobo.566.6">Once this is done, there is usually a linear projection step to get tokens of the desired size (after this step, patches are no longer </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">visually recognizable).</span></span></p>
<p><span class="koboSpan" id="kobo.568.1">Given an image of</span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.569.1"> height </span><em class="italic"><span class="koboSpan" id="kobo.570.1">H</span></em><span class="koboSpan" id="kobo.571.1">, width </span><em class="italic"><span class="koboSpan" id="kobo.572.1">W</span></em><span class="koboSpan" id="kobo.573.1">, and channels </span><em class="italic"><span class="koboSpan" id="kobo.574.1">C</span></em><span class="koboSpan" id="kobo.575.1">, we get </span><em class="italic"><span class="koboSpan" id="kobo.576.1">N</span></em><span class="koboSpan" id="kobo.577.1"> tokens if the patch size </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">is </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.579.1">P</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>N</mi><mo>=</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><msup><mi>P</mi><mn>2</mn></msup></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.581.1">The length of the</span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.582.1"> token after linearization is P</span><span class="superscript"><span class="koboSpan" id="kobo.583.1">2</span></span><span class="koboSpan" id="kobo.584.1"> multiplied by the number of channels (3 if RGB; 1 if the image is black and white). </span><span class="koboSpan" id="kobo.584.2">Now, it is projected at a size chosen in advance (in the original version, 768, but it can also </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">be different):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<span class="koboSpan" id="kobo.586.1"><img alt="Figure 3.19 – Process of transforming images into tokens" src="image/B21257_03_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.587.1">Figure 3.19 – Process of transforming images into tokens</span></p>
<p><span class="koboSpan" id="kobo.588.1">At this point, a special token representing the class is added, and a positional encoder is added here as well so that the model is aware of the position of the patches in the image. </span><span class="koboSpan" id="kobo.588.2">At this point, it enters the encoder, and the process is the same as if they were textual tokens. </span><span class="koboSpan" id="kobo.588.3">The</span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.589.1"> encoder is constituted of transformer blocks, just as we </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">saw before:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<span class="koboSpan" id="kobo.591.1"><img alt="Figure 3.20 – ViT encoding process" src="image/B21257_03_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.592.1">Figure 3.20 – ViT encoding process</span></p>
<p><span class="koboSpan" id="kobo.593.1">ViTs can be </span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.594.1">used for many </span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.595.1">different tasks, such as image classification, object detection, </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">and segmentation:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<span class="koboSpan" id="kobo.597.1"><img alt="Figure 3.21 – Examples of computer vision (CV) tasks done with ViTs" src="image/B21257_03_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.598.1">Figure 3.21 – Examples of computer vision (CV) tasks done with ViTs</span></p>
<p><span class="koboSpan" id="kobo.599.1">Since musical sequences are also sequences, they too can be analyzed with transformers. </span><span class="koboSpan" id="kobo.599.2">There are now models that also process time series, DNA, and musical sequences. </span><span class="koboSpan" id="kobo.599.3">Considering that we have models for each of these modes, we have begun to think about combining them into a </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">single model.</span></span></p>
<p><span class="koboSpan" id="kobo.601.1">In the first chapter, we saw how embedding can be achieved using word2vec. </span><span class="koboSpan" id="kobo.601.2">Even a transformer can produce a latent representation that can be </span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.602.1">considered a vector embedding for a text. </span><span class="koboSpan" id="kobo.602.2">If we remove the last layer of a transformer, we can get a contextualized representation of a text (after all, the various layers of a transformer learn an increasingly sophisticated and contextualized representation of a text). </span><span class="koboSpan" id="kobo.602.3">This representation can be useful for many applications, and we will see this in detail later. </span><span class="koboSpan" id="kobo.602.4">Right now, we are interested in knowing that an LLM can generate a vector representing text. </span><span class="koboSpan" id="kobo.602.5">At the same time, a ViT can produce a vector representation of an image. </span><span class="koboSpan" id="kobo.602.6">Each of these models can then produce a single-mode embedding for a data type. </span><span class="koboSpan" id="kobo.602.7">A multimodal embedding, though, can capture information present in both images and text and </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">relate them.</span></span></p>
<p><span class="koboSpan" id="kobo.604.1">Since multimodal </span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.605.1">embedding would project images and text into the same space, we could exploit this embedding for tasks that were not possible before. </span><span class="koboSpan" id="kobo.605.2">For example, given a caption </span><em class="italic"><span class="koboSpan" id="kobo.606.1">x</span></em><span class="koboSpan" id="kobo.607.1">, we could search for all images that are similar to this caption (or, obviously, the reverse). </span><span class="koboSpan" id="kobo.607.2">The most famous of these is </span><strong class="bold"><span class="koboSpan" id="kobo.608.1">Contrastive Language-Image Pre-Training</span></strong><span class="koboSpan" id="kobo.609.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.610.1">CLIP</span></strong><span class="koboSpan" id="kobo.611.1">). </span><span class="koboSpan" id="kobo.611.2">CLIP was</span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.612.1"> designed as a model that generates embedding for both images and text (today, there are multimodal embeddings for other modalities </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">as well):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<span class="koboSpan" id="kobo.614.1"><img alt="Figure 3.22 – CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) (https://arxiv.org/pdf/2103.00020)" src="image/B21257_03_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.615.1">Figure 3.22 – CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) (</span><a href="https://arxiv.org/pdf/2103.00020"><span class="koboSpan" id="kobo.616.1">https://arxiv.org/pdf/2103.00020</span></a><span class="koboSpan" id="kobo.617.1">)</span></p>
<p><span class="koboSpan" id="kobo.618.1">CLIP was </span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.619.1">trained with a dataset of 400 million (image, text) pairs collected from the internet, trying to cover as many visual concepts as possible. </span><span class="koboSpan" id="kobo.619.2">CLIP attempts to create a representation for both the image and the corresponding caption, using an encoder (a transformer model) for each of the two data types. </span><span class="koboSpan" id="kobo.619.3">Once an image and a caption are embedded by the corresponding encoders, the two embeddings are compared via cosine similarity. </span><span class="koboSpan" id="kobo.619.4">The model learns to maximize the cosine similarity between an image and its corresponding caption. </span><span class="koboSpan" id="kobo.619.5">At the same time, it tries to minimize the similarity with other incorrect pairings (very similar to what we saw for a text embedding, only </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.620.1">this time it is multimodal). </span><span class="koboSpan" id="kobo.620.2">After that, we use this prediction to conduct the update of the </span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.621.1">model parameters (all two encoders). </span><span class="koboSpan" id="kobo.621.2">This learning method is called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.622.1">contrastive learning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.624.1">The training is framed as a classification task in which the model predicts the correct pair. </span><span class="koboSpan" id="kobo.624.2">Starting from these predictions, we compare them with the actual predictions and use cross-entropy loss. </span><span class="koboSpan" id="kobo.624.3">An interesting finding is that although the model is used to create an embedding, the authors used pre-trained models and combined them into a </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">new model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<span class="koboSpan" id="kobo.626.1"><img alt="Figure 3.23 – Similarity matrix between captions and images" src="image/B21257_03_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.627.1">Figure 3.23 – Similarity matrix between captions and images</span></p>
<p><span class="koboSpan" id="kobo.628.1">We can use </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.629.1">CLIP to achieve the embedding of not only images but also captions. </span><span class="koboSpan" id="kobo.629.2">Once we get these embeddings, we can use them to calculate similarity. </span><span class="koboSpan" id="kobo.629.3">We can, thus, obtain a similarity matrix. </span><span class="koboSpan" id="kobo.629.4">This is easy using the Hugging </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">Face</span></span><span class="No-Break"><a id="_idIndexMarker282"/></span><span class="No-Break"><span class="koboSpan" id="kobo.631.1"> libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.632.1">
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('clip-ViT-B-32')
image_embeddings = model.encode(images, convert_to_tensor=True)
caption_embeddings = model.encode(
    captions, convert_to_tensor=True)
similarity_matrix = util.cos_sim(
    image_embeddings, caption_embeddings)</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.633.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.634.1">We are creating an embedding for both images and captions and then computing a </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">similarity matrix.</span></span></p>
<p><span class="koboSpan" id="kobo.636.1">In the original article, one of</span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.637.1"> the first applications for which CLIP was conceived was </span><strong class="bold"><span class="koboSpan" id="kobo.638.1">zero-shot classification</span></strong><span class="koboSpan" id="kobo.639.1">. </span><span class="koboSpan" id="kobo.639.2">For</span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.640.1"> example, given a set of labels, we can ask the model to classify </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">an image:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<span class="koboSpan" id="kobo.642.1"><img alt="Figure 3.24 – Zero-shot image classification" src="image/B21257_03_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.643.1">Figure 3.24 – Zero-shot image classification</span></p>
<p><span class="koboSpan" id="kobo.644.1">CLIP </span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.645.1">can also be used for various other tasks, such as large dataset searches or conducting image clustering and then assigning keywords to these clusters. </span><span class="koboSpan" id="kobo.645.2">CLIP, though, cannot be used to generate text like generating a caption for an image. </span><span class="koboSpan" id="kobo.645.3">For this, we need a </span><strong class="bold"><span class="koboSpan" id="kobo.646.1">vision-language model</span></strong><span class="koboSpan" id="kobo.647.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.648.1">VLM</span></strong><span class="koboSpan" id="kobo.649.1">). </span><span class="koboSpan" id="kobo.649.2">A </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.650.1">VLM essentially behaves like an LLM, though it can also answer questions about an image, solving a limitation of LLMs. </span><span class="koboSpan" id="kobo.650.2">In other words, with a VLM, we can conduct reasoning in a similar way to a classical LLM</span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.651.1"> but also with images. </span><span class="koboSpan" id="kobo.651.2">An example is </span><strong class="bold"><span class="koboSpan" id="kobo.652.1">Bootstrapping Language-Image Pre-training </span></strong><span class="koboSpan" id="kobo.653.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.654.1">BLIP-2</span></strong><span class="koboSpan" id="kobo.655.1">), in which instead of creating a model from scratch, they took an LLM and ViT and connected them with a bridge (</span><strong class="bold"><span class="koboSpan" id="kobo.656.1">Q-Former</span></strong><span class="koboSpan" id="kobo.657.1">). </span><span class="koboSpan" id="kobo.657.2">The Q-Former</span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.658.1"> is an additional component to connect the image encoder with the</span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.659.1"> LLM (basically providing eyes to </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">our LLM):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<span class="koboSpan" id="kobo.661.1"><img alt="Figure 3.25 – Overview of BLIP-2’s framework (https://arxiv.org/pdf/2301.12597)" src="image/B21257_03_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.662.1">Figure 3.25 – Overview of BLIP-2’s framework (</span><a href="https://arxiv.org/pdf/2301.12597"><span class="koboSpan" id="kobo.663.1">https://arxiv.org/pdf/2301.12597</span></a><span class="koboSpan" id="kobo.664.1">)</span></p>
<p><span class="koboSpan" id="kobo.665.1">The Q-Former consists of two components (one interacting with ViT and one interacting with the LLM); it is the only part of the model that is trained. </span><span class="koboSpan" id="kobo.665.2">This process occurs in two stages, one for each mode. </span><span class="koboSpan" id="kobo.665.3">In the first stage, we use an image-captions pair to train the Q-Former to relate images and text. </span><span class="koboSpan" id="kobo.665.4">In the second step, the embeddings learned by the Q-Former are used as soft prompts to condition the LLM on textual representations of the images (make the LLM aware of the images). </span><span class="koboSpan" id="kobo.665.5">Once the Q-Former has been trained, we can use the model to generate text about </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">the images:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<span class="koboSpan" id="kobo.667.1"><img alt="Figure 3.26 – BLIP-2 captioning of the image" src="image/B21257_03_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.668.1">Figure 3.26 – BLIP-2 captioning of the image</span></p>
<p><span class="koboSpan" id="kobo.669.1">Since it is a VLM, we can also ask several questions and chat with the model about </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">the image:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<span class="koboSpan" id="kobo.671.1"><img alt="Figure 3.27 – Different rounds of questions to BLIP-2 about an image" src="image/B21257_03_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.672.1">Figure 3.27 – Different rounds of questions to BLIP-2 about an image</span></p>
<p><span class="koboSpan" id="kobo.673.1">Speaking of </span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.674.1">multimodal models, another type of model that has had a strong expansion in recent times is </span><strong class="bold"><span class="koboSpan" id="kobo.675.1">text-to-image models</span></strong><span class="koboSpan" id="kobo.676.1">. </span><span class="koboSpan" id="kobo.676.2">Stable Diffusion is considered a milestone for its quality of image </span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.677.1">generation, its performance, and its availability to the masses. </span><span class="koboSpan" id="kobo.677.2">The operation of this model can be summarized at a high level: given textual directions (a prompt), the system generates an image according to the instructions. </span><span class="koboSpan" id="kobo.677.3">Other alternatives also exist today (text-to-video, image modification guided by text, and so on), but the principle is similar. </span><span class="koboSpan" id="kobo.677.4">At a high level, we can say that there are three </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">main ones:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.679.1">A text encoder</span></strong><span class="koboSpan" id="kobo.680.1">: The</span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.681.1"> text encoder is a model (usually CLIP or another LLM specifically trained for this function) that takes a text and returns a vector representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">the text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.683.1">An image generator</span></strong><span class="koboSpan" id="kobo.684.1">: This</span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.685.1"> is a U-Net that generates the image representation. </span><span class="koboSpan" id="kobo.685.2">During this process, the generation is conditioned on </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">the text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.687.1">An image decoder</span></strong><span class="koboSpan" id="kobo.688.1">: The </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.689.1">image representation is transformed into an actual image. </span><span class="koboSpan" id="kobo.689.2">Usually, this component is a ViT or an </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">AE decoder.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.691.1">The heart of the system is the</span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.692.1"> U-Net, and in this component, the diffusion process takes </span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.693.1">place. </span><span class="koboSpan" id="kobo.693.2">The U-Net does not work directly on the image but on a compact representation called latent representation (which is basically a matrix). </span><span class="koboSpan" id="kobo.693.3">This latent representation, though, contains the information to generate an image, a process that is then conducted in the last step by </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">the decoder.</span></span></p>
<p><span class="koboSpan" id="kobo.695.1">During the diffusion process, starting from random noise, we begin to build a latent representation that acquires information about the image. </span><span class="koboSpan" id="kobo.695.2">Diffusion models are based on the idea that a model, given a large enough training set, can learn information about the contained patterns. </span><span class="koboSpan" id="kobo.695.3">During training, having taken an image, we generate some random noise and add a certain amount of noise to the image. </span><span class="koboSpan" id="kobo.695.4">This allows us to expand our image dataset widely (since we can control the amount of noise we can add to an image and thus create different versions of an image with more or less noise). </span><span class="koboSpan" id="kobo.695.5">The model is then trained to identify and predict the noise that has been added to the image (via classical backpropagation). </span><span class="koboSpan" id="kobo.695.6">The model then predicts the noise that needs to be subtracted in order to get the image (not exactly the image, but the distribution of it). </span><span class="koboSpan" id="kobo.695.7">By conducting this denoising process, we can then obtain a backward image (or, at least, its latent representation). </span><span class="koboSpan" id="kobo.695.8">So, starting from noise, we can get an image, and the model is trained to find an image in the noise. </span><span class="koboSpan" id="kobo.695.9">At this point, we use a decoder and we get an image. </span><span class="koboSpan" id="kobo.695.10">Up to this point, though, we cannot control this generation </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">with text.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<span class="koboSpan" id="kobo.697.1"><img alt="Figure 3.28 – Stable diffusion architecture (https://arxiv.org/pdf/2112.10752)" src="image/B21257_03_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.698.1">Figure 3.28 – Stable diffusion architecture (</span><a href="https://arxiv.org/pdf/2112.10752"><span class="koboSpan" id="kobo.699.1">https://arxiv.org/pdf/2112.10752</span></a><span class="koboSpan" id="kobo.700.1">)</span></p>
<p><span class="koboSpan" id="kobo.701.1">This is where the text encoder comes in. </span><span class="koboSpan" id="kobo.701.2">The choice of LLM is important; the better the LLM, the better the information this model can bring. </span><span class="koboSpan" id="kobo.701.3">As we saw earlier, CLIP has been trained on captions and corresponding images and is capable of producing textual embeddings. </span><span class="koboSpan" id="kobo.701.4">The idea behind CLIP is that the textual embeddings are close in embedding space to that of the corresponding images. </span><span class="koboSpan" id="kobo.701.5">Having arrived at textual information as an embedding, this information will be used to generate an image. </span><span class="koboSpan" id="kobo.701.6">In fact, in the U-Net, there is </span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.702.1">cross-attention that connects this textual information with the </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">generation process.</span></span></p>
<p><span class="koboSpan" id="kobo.704.1">We have seen how these models can also answer questions about images or generate images. </span><span class="koboSpan" id="kobo.704.2">These models don’t always answer questions optimally, and this can cause serious consequences. </span><span class="koboSpan" id="kobo.704.3">Or, at the same time, they can generate problematic images. </span><span class="koboSpan" id="kobo.704.4">We will discuss exactly this in the </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">next section.</span></span></p>
<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.706.1">Understanding hallucinations and ethical and legal issues</span></h1>
<p><span class="koboSpan" id="kobo.707.1">A well-known problem with LLMs is their </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.708.1">tendency to hallucinate. </span><strong class="bold"><span class="koboSpan" id="kobo.709.1">Hallucination</span></strong><span class="koboSpan" id="kobo.710.1"> is defined as the production of nonsensical or unfaithful content. </span><span class="koboSpan" id="kobo.710.2">This is classified into factuality hallucination and faithfulness hallucination. </span><strong class="bold"><span class="koboSpan" id="kobo.711.1">Factual hallucinations</span></strong><span class="koboSpan" id="kobo.712.1"> are </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.713.1">responses produced by the model that contradict real, verifiable facts. </span><strong class="bold"><span class="koboSpan" id="kobo.714.1">Faithfulness hallucination</span></strong><span class="koboSpan" id="kobo.715.1">, on the other hand, is content that is at odds with instructions </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.716.1">or context provided by the user. </span><span class="koboSpan" id="kobo.716.2">The model is trained to generate consistent text but has no way to revise its output or check that it </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">is correct:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<span class="koboSpan" id="kobo.718.1"><img alt="Figure 3.29 – Example of LLM hallucination (https://arxiv.org/pdf/2311.05232)" src="image/B21257_03_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.719.1">Figure 3.29 – Example of LLM hallucination (</span><a href="https://arxiv.org/pdf/2311.05232"><span class="koboSpan" id="kobo.720.1">https://arxiv.org/pdf/2311.05232</span></a><span class="koboSpan" id="kobo.721.1">)</span></p>
<p><span class="koboSpan" id="kobo.722.1">The model can also </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.723.1">generate toxic content and present stereotypes and negative attitudes toward specific demographic groups. </span><span class="koboSpan" id="kobo.723.2">It is important to prevent models from producing harm. </span><span class="koboSpan" id="kobo.723.3">Different studies have highlighted different instances of potential harm resulting from the use of AI in general and LLMs in particular. </span><span class="koboSpan" id="kobo.723.4">One example</span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.724.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.725.1">representational harm</span></strong><span class="koboSpan" id="kobo.726.1">, caused by a model that can perpetuate stereotypes or bias. </span><span class="koboSpan" id="kobo.726.2">This was previously seen with sentiment classifiers that assigned lower sentiment and negative emotion to particular groups of people. </span><span class="koboSpan" id="kobo.726.3">In fact, LLMs can produce offensive or derogatory language when representing minorities, or they can perpetuate society’s stereotypes about cultural norms, attitudes, and prejudices. </span><span class="koboSpan" id="kobo.726.4">This can lead to what is called </span><strong class="bold"><span class="koboSpan" id="kobo.727.1">allocational harm</span></strong><span class="koboSpan" id="kobo.728.1">, when a model </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.729.1">allocates resources unfairly. </span><span class="koboSpan" id="kobo.729.2">For example, if an LLM is used to decide the priority of access to medical treatment (or a job or credit), it might allocate access unfairly due to the biases it has inherited from </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">its training.</span></span></p>
<p><span class="koboSpan" id="kobo.731.1">Indeed, it had already been noted that embedding models can amplify biases, and these biases were reflected within the embedding space. </span><span class="koboSpan" id="kobo.731.2">The association of harmful content with groups and minorities was identified in the embedding space. </span><span class="koboSpan" id="kobo.731.3">In some cases, some LLMs used pre-trained embedding model weights as the initialization of the embedding layer. </span><span class="koboSpan" id="kobo.731.4">Some </span><strong class="bold"><span class="koboSpan" id="kobo.732.1">debiasing approaches</span></strong><span class="koboSpan" id="kobo.733.1"> (removal of bias from the model) have shown potential, but they are still far from</span><a id="_idIndexMarker304"/> <span class="No-Break"><span class="koboSpan" id="kobo.734.1">being effective.</span></span></p>
<p><span class="koboSpan" id="kobo.735.1">These biases stem from the </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.736.1">pre-training dataset, so it is important to detoxify and remove problematic content before training. </span><span class="koboSpan" id="kobo.736.2">When fine-tuning a model, it is important to check for incorrect labels derived from annotator bias. </span><span class="koboSpan" id="kobo.736.3">It is also important to vary the sources. </span><span class="koboSpan" id="kobo.736.4">There is indeed an imbalance in the content used to train the model between text produced in the US and other countries. </span><span class="koboSpan" id="kobo.736.5">The model, therefore, inherits the perspective of the dominant demographics in </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">its pre-training.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer100">
<span class="koboSpan" id="kobo.738.1"><img alt="Figure 3.30 – Risk associated with hallucination and disinformation (https://aclanthology.org/2023.findings-emnlp.97.pdf)" src="image/B21257_03_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.739.1">Figure 3.30 – Risk associated with hallucination and disinformation (</span><a href="https://aclanthology.org/2023.findings-emnlp.97.pdf"><span class="koboSpan" id="kobo.740.1">https://aclanthology.org/2023.findings-emnlp.97.pdf</span></a><span class="koboSpan" id="kobo.741.1">)</span></p>
<p><span class="koboSpan" id="kobo.742.1">Another potential risk of LLMs is their use to </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.743.1">produce </span><strong class="bold"><span class="koboSpan" id="kobo.744.1">misinformation</span></strong><span class="koboSpan" id="kobo.745.1">. </span><span class="koboSpan" id="kobo.745.2">LLMs are capable of producing credible, convincing text. </span><span class="koboSpan" id="kobo.745.3">Malicious actors could use them to automate the production of misinformation, phishing emails, rage bait, and other harmful content. </span><span class="koboSpan" id="kobo.745.4">This is why an important research topic is how to detect text generated by LLMs (or alternatively add watermarks to </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">text generation).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<span class="koboSpan" id="kobo.747.1"><img alt="Figure 3.31 – Taxonomy of LLM-generated misinformation (https://arxiv.org/pdf/2309.13788)" src="image/B21257_03_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.748.1">Figure 3.31 – Taxonomy of LLM-generated misinformation (</span><a href="https://arxiv.org/pdf/2309.13788"><span class="koboSpan" id="kobo.749.1">https://arxiv.org/pdf/2309.13788</span></a><span class="koboSpan" id="kobo.750.1">)</span></p>
<p><span class="koboSpan" id="kobo.751.1">Today, there are several </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.752.1">datasets and libraries in Python that allow one to study model bias. </span><span class="koboSpan" id="kobo.752.2">For example, one of the packages is the Hugging Face library, Evaluate. </span><span class="koboSpan" id="kobo.752.3">We can, for example, use a set of prompts and change the gender of the prompt. </span><span class="koboSpan" id="kobo.752.4">After that, we can evaluate with Evaluate how the model completes these prompts (the model used is GPT-2). </span><span class="koboSpan" id="kobo.752.5">Evaluate uses, in this case, another model trained for </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">this purpose:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.754.1">
import evaluate
toxicity = evaluate.load("toxicity")
toxicity.compute(
    predictions=model_continuations,
    aggregation="ratio"
)</span></pre> <p><span class="koboSpan" id="kobo.755.1">As we can see in the following heatmap, we have a difference in how the model completes </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">the prompts:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<span class="koboSpan" id="kobo.757.1"><img alt="Figure 3.32 – Heatmap of gendered completion and associated toxicity" src="image/B21257_03_32.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.758.1">Figure 3.32 – Heatmap of gendered completion and associated toxicity</span></p>
<p><span class="koboSpan" id="kobo.759.1">Models may also have a bias</span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.760.1"> regarding occupations. </span><span class="koboSpan" id="kobo.760.2">We can use the same library again to also evaluate the polarity of the prompts that have been completed by the model. </span><span class="koboSpan" id="kobo.760.3">In this case, we evaluate the sentiment associated with each of the completed prompts for each of the </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">two professions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.762.1">
regard = evaluate.load("regard", "compare")
regard_results = regard.compute(
    data = profession1_completions,
    references = profession2_completions
)</span></pre> <p><span class="koboSpan" id="kobo.763.1">The completed prompts for CEOs are much more positive than those generated for </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">truck drivers:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<span class="koboSpan" id="kobo.765.1"><img alt="Figure 3.33 – Bias distribution for two different professions" src="image/B21257_03_33.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.766.1">Figure 3.33 – Bias distribution for two different professions</span></p>
<p><span class="koboSpan" id="kobo.767.1">Another point of contention is the </span><strong class="bold"><span class="koboSpan" id="kobo.768.1">copyright issue</span></strong><span class="koboSpan" id="kobo.769.1">. </span><span class="koboSpan" id="kobo.769.2">These </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.770.1">models are trained on copyrighted text and can regenerate part of the text they are trained on. </span><span class="koboSpan" id="kobo.770.2">So far, the creators of these LLMs have claimed that they are covered by the fair use doctrine, which has allowed various companies to train models on text scraped from the internet even without permission. </span><span class="koboSpan" id="kobo.770.3">Today, though, some lawsuits are pending that could change the political and legal landscape. </span><span class="koboSpan" id="kobo.770.4">Some companies, therefore, are trying to sign licensing contracts with newspaper publishers or </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">social networks.</span></span></p>
<p><span class="koboSpan" id="kobo.772.1">Linked to the same problem </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.773.1">is a </span><strong class="bold"><span class="koboSpan" id="kobo.774.1">privacy issue</span></strong><span class="koboSpan" id="kobo.775.1"> risk. </span><span class="koboSpan" id="kobo.775.2">These models can leak information about their training data. </span><span class="koboSpan" id="kobo.775.3">It is possible with adversarial attacks to extract information from the model. </span><span class="koboSpan" id="kobo.775.4">The model can store a huge amount of information in its parameters, and if trained with databases that contain personal information, this can later be extracted. </span><span class="koboSpan" id="kobo.775.5">Therefore, </span><strong class="bold"><span class="koboSpan" id="kobo.776.1">machine unlearning</span></strong><span class="koboSpan" id="kobo.777.1"> methods</span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.778.1"> are being studied to make a model forget personal data. </span><span class="koboSpan" id="kobo.778.2">Legislation being studied in different countries may require a model to forget information of users who request it. </span><span class="koboSpan" id="kobo.778.3">We will discuss privacy in detail in </span><a href="B21257_06.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.779.1">Chapter 6</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.780.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.781.1">A final point is that these models are now capable of generating code, and this code can be used to produce malware and viruses. </span><span class="koboSpan" id="kobo.781.2">In addition, these models will be increasingly connected, and some studies show how these LLMs can potentially be used to spread computer viruses. </span><span class="koboSpan" id="kobo.781.3">In the next section, we will see how to efficiently use these models through </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">prompt techniques.</span></span></p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.783.1">Prompt engineering</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.784.1">In-context learning</span></strong><span class="koboSpan" id="kobo.785.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.786.1">ICL</span></strong><span class="koboSpan" id="kobo.787.1">) is one of the</span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.788.1"> most fascinating properties of LLMs. </span><span class="koboSpan" id="kobo.788.2">Traditionally, </span><strong class="bold"><span class="koboSpan" id="kobo.789.1">machine learning</span></strong><span class="koboSpan" id="kobo.790.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.791.1">ML</span></strong><span class="koboSpan" id="kobo.792.1">) models</span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.793.1"> are trained to solve specific tasks drawn</span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.794.1"> on training data. </span><span class="koboSpan" id="kobo.794.2">For example, in a classical classification task, we have input-output pairs (</span><em class="italic"><span class="koboSpan" id="kobo.795.1">X</span></em><span class="koboSpan" id="kobo.796.1">,</span><em class="italic"><span class="koboSpan" id="kobo.797.1">y</span></em><span class="koboSpan" id="kobo.798.1">), and the model learns to map the relationship that is between input X and output y. </span><span class="koboSpan" id="kobo.798.2">Any deviation from this task leads the model to have less-than-optimal results. </span><span class="koboSpan" id="kobo.798.3">If we train a model for text classification in different topics, we have to conduct fine-tuning to make it efficient in sentiment analysis. </span><span class="koboSpan" id="kobo.798.4">In contrast, ICL allows us not to have to have any model update to use the model in a new task. </span><span class="koboSpan" id="kobo.798.5">ICL is, thus, an emergent property of LLMs that allows the model to perform a new task in inference, taking advantage of the acquired knowledge to map a </span><span class="No-Break"><span class="koboSpan" id="kobo.799.1">new relationship.</span></span></p>
<p><span class="koboSpan" id="kobo.800.1">ICL was first defined in the article </span><em class="italic"><span class="koboSpan" id="kobo.801.1">Language Models are Few-Shot Learners </span></em><span class="koboSpan" id="kobo.802.1">(</span><a href="https://arxiv.org/abs/2005.14165"><span class="koboSpan" id="kobo.803.1">https://arxiv.org/abs/2005.14165</span></a><span class="koboSpan" id="kobo.804.1">). </span><span class="koboSpan" id="kobo.804.2">The authors define LLMs as few-shot learners because, given a set of examples in the prompt (textual input for an LLM), the model can map the relationship between input and output and have learned a new task. </span><span class="koboSpan" id="kobo.804.3">This new skill is “learned” in context because the LLM exploits the examples in the prompt (which then </span><span class="No-Break"><span class="koboSpan" id="kobo.805.1">provide context):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<span class="koboSpan" id="kobo.806.1"><img alt="Figure 3.34 – Example of ICL abilities (https://arxiv.org/pdf/2005.14165)" src="image/B21257_03_34.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.807.1">Figure 3.34 – Example of ICL abilities (</span><a href="https://arxiv.org/pdf/2005.14165"><span class="koboSpan" id="kobo.808.1">https://arxiv.org/pdf/2005.14165</span></a><span class="koboSpan" id="kobo.809.1">)</span></p>
<p><span class="koboSpan" id="kobo.810.1">In this case, the concept of “learning” is improper because the model is not really learning (in fact, there is no update of internal parameters), and therefore the learned skill is only transient. </span><span class="koboSpan" id="kobo.810.2">In other words, the model exploits what it has already learned (its latent representation) to perform a new task. </span><span class="koboSpan" id="kobo.810.3">The model exploits the relationships that have been learned in pre-training, to map the latent function that is between input and output present in </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">the prompt.</span></span></p>
<p><span class="koboSpan" id="kobo.812.1">ICL has </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">different</span></span><span class="No-Break"><a id="_idIndexMarker315"/></span><span class="No-Break"><span class="koboSpan" id="kobo.814.1"> advantages:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.815.1">It mirrors the human cognitive reasoning process, so it makes it easier to describe a problem and exploit </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">an LLM.</span></span></li>
<li><span class="koboSpan" id="kobo.817.1">It doesn’t require parameter upgrades, so it’s fast and can be used with a model in inference. </span><span class="koboSpan" id="kobo.817.2">It requires only a </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">few examples.</span></span></li>
<li><span class="koboSpan" id="kobo.819.1">ICL has shown that in this, the model can achieve competitive performance in </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">several benchmarks.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.821.1">At present, it is still not entirely clear how this behavior emerges. </span><span class="koboSpan" id="kobo.821.2">According to some, the root of ICL is precisely multi-head self-attention and how the various attention heads manage to create interconnected circuits between layers. </span><span class="koboSpan" id="kobo.821.3">The prompt, in general, provides several elements (format, inputs, outputs, and input-output mapping), and they are important for the model to succeed in achieving the mapping. </span><span class="koboSpan" id="kobo.821.4">Initial work suggests that the model succeeds in “locating” latent concepts that it acquired during training. </span><span class="koboSpan" id="kobo.821.5">In other words, the model infers from the examples what the task is, but the other elements of the prompt help it succeed in locating in its parameters the latent concepts it needs to do this mapping. </span><span class="koboSpan" id="kobo.821.6">Specifically, some work states that the format in which demonstrations are presented is the most important element (for example, in the form of </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">input-label pairs):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<span class="koboSpan" id="kobo.823.1"><img alt="Figure 3.35 – Prompt structure (https://arxiv.org/pdf/2202.12837)" src="image/B21257_03_35.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.824.1">Figure 3.35 – Prompt structure (</span><a href="https://arxiv.org/pdf/2202.12837"><span class="koboSpan" id="kobo.825.1">https://arxiv.org/pdf/2202.12837</span></a><span class="koboSpan" id="kobo.826.1">)</span></p>
<p><span class="koboSpan" id="kobo.827.1">The community has become excited about this ability because </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.828.1">ICL allows the model to “learn” a task in inference simply by manipulating the prompt. </span><span class="koboSpan" id="kobo.828.2">ICL has allowed specific techniques to evolve to be able to perform increasingly sophisticated tasks without the need to fine-tune </span><span class="No-Break"><span class="koboSpan" id="kobo.829.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.830.1">For clarity, we can define some terminology and elements in what are prompts (or formatting guidelines). </span><span class="koboSpan" id="kobo.830.2">First, a prompt typically contains a question </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">or instruction:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.832.1">
When was Shakespeare born?</span></pre> <p><span class="koboSpan" id="kobo.833.1">The preceding example is a prompt that contains only one question. </span><span class="koboSpan" id="kobo.833.2">By convention, it is referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.834.1">zero-shot prompting</span></strong><span class="koboSpan" id="kobo.835.1"> since it </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.836.1">contains neither examples nor demonstrations. </span><span class="koboSpan" id="kobo.836.2">Instead, we provide the model with only one question or instruction (such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.837.1">generate code for function x in Python</span></strong><span class="koboSpan" id="kobo.838.1">). </span><span class="koboSpan" id="kobo.838.2">The model that successfully responds to this type of prompt is said to have zero-shot capabilities, and this ability is enhanced by the instruction tuning of a </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">pre-trained model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.840.1">
This movie is awesome – positive
This sandwich is disgusting – negative
This TV series is meh -</span></pre> <p><span class="koboSpan" id="kobo.841.1">This is a typical case of </span><strong class="bold"><span class="koboSpan" id="kobo.842.1">few-shot prompting</span></strong><span class="koboSpan" id="kobo.843.1"> where </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.844.1">we provide examples in the prompt. </span><span class="koboSpan" id="kobo.844.2">More demonstrations usually help the LLM (3-shot, 5-shot, or even 10-shot are common cases). </span><span class="koboSpan" id="kobo.844.3">A prompt can also have context to help the model. </span><span class="koboSpan" id="kobo.844.4">We can also add the desired format for the response. </span><span class="koboSpan" id="kobo.844.5">However, these simple prompts have limitations, especially for tasks that require reasoning. </span><span class="koboSpan" id="kobo.844.6">Especially when this requires multiple reasoning steps, providing examples is not enough to guide the model in the right direction. </span><span class="koboSpan" id="kobo.844.7">Several techniques have been proposed to avoid the need </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">for fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.846.1">Especially when dealing </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.847.1">with an arithmetic problem, seeing examples and associated answers is not very helpful in learning the process. </span><span class="koboSpan" id="kobo.847.2">A student has more benefit in understanding the rationale before approaching the solution of such a problem. </span><span class="koboSpan" id="kobo.847.3">Similarly, an LLM has more benefit in getting the rationale of an answer than more examples with labels alone. </span><strong class="bold"><span class="koboSpan" id="kobo.848.1">Chain-of-thought prompting</span></strong><span class="koboSpan" id="kobo.849.1"> does</span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.850.1"> exactly that; a triplet, &lt;input, chain of thought, output&gt;, is provided in the prompt. </span><span class="koboSpan" id="kobo.850.2">A chain of thought is the different intermediate steps to solve </span><span class="No-Break"><span class="koboSpan" id="kobo.851.1">the problem:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer106">
<span class="koboSpan" id="kobo.852.1"><img alt="Figure 3.36 – Example of chain-of-thought (https://arxiv.org/pdf/2201.11903)" src="image/B21257_03_36.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.853.1">Figure 3.36 – Example of chain-of-thought (</span><a href="https://arxiv.org/pdf/2201.11903"><span class="koboSpan" id="kobo.854.1">https://arxiv.org/pdf/2201.11903</span></a><span class="koboSpan" id="kobo.855.1">)</span></p>
<p><span class="koboSpan" id="kobo.856.1">Adding these demonstrations makes it easier for the model to solve the task. </span><span class="koboSpan" id="kobo.856.2">It has the disadvantage, though, that we must have quality demonstrations for several problems, and collecting such annotated datasets </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">is expensive.</span></span></p>
<p><span class="koboSpan" id="kobo.858.1">The advantage of </span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.859.1">CoT is that it divides a task for the model into a more manageable series of steps. </span><span class="koboSpan" id="kobo.859.2">This behavior can be incentivized simply by adding “Let’s think step by step” to the prompt. </span><span class="koboSpan" id="kobo.859.3">This seemingly simple approach is </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.860.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.861.1">zero-shot CoT prompting</span></strong><span class="koboSpan" id="kobo.862.1">. </span><span class="koboSpan" id="kobo.862.2">The authors of the paper </span><em class="italic"><span class="koboSpan" id="kobo.863.1">Large Language Models are Zero-Shot Reasoners</span></em><span class="koboSpan" id="kobo.864.1"> (</span><a href="https://arxiv.org/pdf/2205.11916"><span class="koboSpan" id="kobo.865.1">https://arxiv.org/pdf/2205.11916</span></a><span class="koboSpan" id="kobo.866.1">) suggest that the model has inherent reasoning skills in zero-shot settings, and this approach is therefore versatile because it prompts the model to use the skills it has learned </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">in training:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer107">
<span class="koboSpan" id="kobo.868.1"><img alt="Figure 3.37 – Schematic diagram illustrating various approaches to problem-solving with LLMs (https://arxiv.org/pdf/2305.10601)" src="image/B21257_03_37.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.869.1">Figure 3.37 – Schematic diagram illustrating various approaches to problem-solving with LLMs (</span><a href="https://arxiv.org/pdf/2305.10601"><span class="koboSpan" id="kobo.870.1">https://arxiv.org/pdf/2305.10601</span></a><span class="koboSpan" id="kobo.871.1">)</span></p>
<p><span class="koboSpan" id="kobo.872.1">Other techniques, such as </span><strong class="bold"><span class="koboSpan" id="kobo.873.1">self-consistency</span></strong><span class="koboSpan" id="kobo.874.1">, have</span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.875.1"> also been used to improve reasoning skills. </span><span class="koboSpan" id="kobo.875.2">The idea behind it is ensembling, in which different models can come to the right solution by majority vote. </span><span class="koboSpan" id="kobo.875.3">In this case, we generate several solutions and then choose the majority solution. </span><strong class="bold"><span class="koboSpan" id="kobo.876.1">Tree of Thoughts</span></strong><span class="koboSpan" id="kobo.877.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.878.1">ToT</span></strong><span class="koboSpan" id="kobo.879.1">), on the other hand, exploits reasoning and self-evaluation </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.880.1">capabilities, where the model generates different reasoning intermediates and then evaluates them by exploiting search algorithms (breadth-first search and depth-first search). </span><span class="koboSpan" id="kobo.880.2">One usually has to choose the number of candidate paths and steps. </span><span class="koboSpan" id="kobo.880.3">These techniques allow a higher reasoning capacity of the model but have a higher computational cost since the model has to generate </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">several responses.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<span class="koboSpan" id="kobo.882.1"><img alt="Figure 3.38 – Examples of using the DSPy system (https://arxiv.org/abs/2310.03714)" src="image/B21257_03_38.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.883.1">Figure 3.38 – Examples of using the DSPy system (</span><a href="https://arxiv.org/abs/2310.03714"><span class="koboSpan" id="kobo.884.1">https://arxiv.org/abs/2310.03714</span></a><span class="koboSpan" id="kobo.885.1">)</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.886.1">Declarative Self-improving Language Programs in Python</span></strong><span class="koboSpan" id="kobo.887.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.888.1">DSPy</span></strong><span class="koboSpan" id="kobo.889.1">) is an interesting new paradigm that has</span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.890.1"> been evolving in recent times. </span><span class="koboSpan" id="kobo.890.2">Until now, it has been assumed that we have to manually create these prompts, and this requires a lot of trial and error. </span><span class="koboSpan" id="kobo.890.3">Instead, DSPy seeks to standardize this prompting process and turn it into a kind of programming. </span><span class="koboSpan" id="kobo.890.4">In short, the authors of DSPy (</span><a href="https://arxiv.org/abs/2310.03714"><span class="koboSpan" id="kobo.891.1">https://arxiv.org/abs/2310.03714</span></a><span class="koboSpan" id="kobo.892.1">) suggest that we can abstract prompts and fine-tune them into signatures while prompting techniques are used as modules. </span><span class="koboSpan" id="kobo.892.2">The result is that prompt engineering can be automated with optimizers. </span><span class="koboSpan" id="kobo.892.3">Given a dataset, we create a pipeline of DSPy containing signatures and modules (how these techniques are connected), define which metrics to optimize, and then optimize (we define what output we search for and the optimizer). </span><span class="koboSpan" id="kobo.892.4">The process is, then, iterative; DSPy leads to optimizing prompts that we can </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">then use.</span></span></p>
<p><span class="koboSpan" id="kobo.894.1">The techniques we have seen in this section are the most commonly used. </span><span class="koboSpan" id="kobo.894.2">There are many others, but they are generally variations of those described here. </span><span class="koboSpan" id="kobo.894.3">We now have all the elements to be able to successfully use </span><span class="No-Break"><span class="koboSpan" id="kobo.895.1">an LLM.</span></span></p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.896.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.897.1">In this chapter, we discussed the transition from transformers to LLMs. </span><span class="koboSpan" id="kobo.897.2">The transformer was an elegant evolution and synthesis of 20 years of research in NLP, combining the best of research up to that point. </span><span class="koboSpan" id="kobo.897.3">In itself, the transformer contained a whole series of elements that enabled its success and versatility. </span><span class="koboSpan" id="kobo.897.4">The beating heart of the model is self-attention, a key tool – but also the main limitation of the LLM. </span><span class="koboSpan" id="kobo.897.5">On the one hand, it allows for learning sophisticated representations of text that make LLMs capable of countless tasks; on the other hand, it has a huge computational cost (especially when scaling the model). </span><span class="koboSpan" id="kobo.897.6">LLMs are not only capable of solving tasks such as classification but also tasks that assume some reasoning, all simply by using text instructions. </span><span class="koboSpan" id="kobo.897.7">In addition, we have seen how to fit the transformer even with </span><span class="No-Break"><span class="koboSpan" id="kobo.898.1">multimodal data.</span></span></p>
<p><span class="koboSpan" id="kobo.899.1">So far, the model produces only text, although it can produce code as well. </span><span class="koboSpan" id="kobo.899.2">At this point, why not allow the model to be able to execute the code? </span><span class="koboSpan" id="kobo.899.3">Why not allow it to use tools that can extend its capabilities? </span><span class="koboSpan" id="kobo.899.4">This is what we will see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.900.1">next chapters.</span></span></p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.901.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.902.1">Everton et al</span><em class="italic"><span class="koboSpan" id="kobo.903.1">.</span></em><span class="koboSpan" id="kobo.904.1">, </span><em class="italic"><span class="koboSpan" id="kobo.905.1">Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy</span></em><span class="koboSpan" id="kobo.906.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.907.1">2023, </span></span><a href="https://arxiv.org/abs/2312.10549"><span class="No-Break"><span class="koboSpan" id="kobo.908.1">https://arxiv.org/abs/2312.10549</span></span></a></li>
<li><span class="koboSpan" id="kobo.909.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.910.1">Emergent Abilities in AI: Are We Chasing a Myth?</span></em><span class="koboSpan" id="kobo.911.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">2023, </span></span><a href="https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9"><span class="No-Break"><span class="koboSpan" id="kobo.913.1">https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9</span></span></a></li>
<li><span class="koboSpan" id="kobo.914.1">Rasyl et al., </span><em class="italic"><span class="koboSpan" id="kobo.915.1">Preference Tuning LLMs with Direct Preference Optimization Methods</span></em><span class="koboSpan" id="kobo.916.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">2024, </span></span><a href="https://huggingface.co/blog/pref-tuning"><span class="No-Break"><span class="koboSpan" id="kobo.918.1">https://huggingface.co/blog/pref-tuning</span></span></a></li>
<li><span class="koboSpan" id="kobo.919.1">Alemi, </span><em class="italic"><span class="koboSpan" id="kobo.920.1">KL is All You Need</span></em><span class="koboSpan" id="kobo.921.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.922.1">2024, </span></span><a href="https://blog.alexalemi.com/kl-is-all-you-need.html"><span class="No-Break"><span class="koboSpan" id="kobo.923.1">https://blog.alexalemi.com/kl-is-all-you-need.html</span></span></a></li>
<li><span class="koboSpan" id="kobo.924.1">OpenAI, </span><em class="italic"><span class="koboSpan" id="kobo.925.1">Proximal Policy </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.926.1">Optimization</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">, </span></span><a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html"><span class="No-Break"><span class="koboSpan" id="kobo.928.1">https://spinningup.openai.com/en/latest/algorithms/ppo.html</span></span></a></li>
<li><span class="koboSpan" id="kobo.929.1">Simonini, </span><em class="italic"><span class="koboSpan" id="kobo.930.1">Proximal Policy Optimization (PPO)</span></em><span class="koboSpan" id="kobo.931.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.932.1">2022, </span></span><a href="https://huggingface.co/blog/deep-rl-ppo"><span class="No-Break"><span class="koboSpan" id="kobo.933.1">https://huggingface.co/blog/deep-rl-ppo</span></span></a></li>
<li><span class="koboSpan" id="kobo.934.1">Hoffmann et al., </span><em class="italic"><span class="koboSpan" id="kobo.935.1">Training Compute-Optimal Large Language Models</span></em><span class="koboSpan" id="kobo.936.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">2022, </span></span><a href="https://arxiv.org/abs/2203.15556"><span class="No-Break"><span class="koboSpan" id="kobo.938.1">https://arxiv.org/abs/2203.15556</span></span></a></li>
<li><span class="koboSpan" id="kobo.939.1">Brown et al., </span><em class="italic"><span class="koboSpan" id="kobo.940.1">Language Models are Few-Shot Learners</span></em><span class="koboSpan" id="kobo.941.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">2020, </span></span><a href="https://arxiv.org/abs/2005.14165"><span class="No-Break"><span class="koboSpan" id="kobo.943.1">https://arxiv.org/abs/2005.14165</span></span></a></li>
</ul>
</div>


<div class="Content" id="_idContainer110">
<h1 id="_idParaDest-58" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.1.1">Part 2: 
AI Agents and Retrieval 
of Knowledge</span></h1>
<p><span class="koboSpan" id="kobo.2.1">This part focuses on extending the capabilities of LLMs by enabling them to access, retrieve, and reason over external sources of knowledge. </span><span class="koboSpan" id="kobo.2.2">It begins with the creation of AI agents that can interact with the web, retrieve live information, and execute tasks beyond simple question answering. </span><span class="koboSpan" id="kobo.2.3">The following chapters explore retrieval-augmented generation (RAG), starting from basic pipelines and advancing toward more modular and scalable systems that reduce hallucinations and improve factual accuracy. </span><span class="koboSpan" id="kobo.2.4">The use of structured knowledge through knowledge graphs (GraphRAG) is then introduced as a powerful method to represent and reason over information. </span><span class="koboSpan" id="kobo.2.5">Finally, this part discusses how reinforcement learning can be used to align agent behavior and improve decision-making through interaction with dynamic environments. </span><span class="koboSpan" id="kobo.2.6">These chapters collectively show how to build agents that are not only language-capable but also context-aware, goal-driven, and grounded in </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">external information.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B21257_04.xhtml#_idTextAnchor058"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 4</span></em></a><em class="italic"><span class="koboSpan" id="kobo.7.1">, Building a Web Scraping Agent with an LLM</span></em></li>
<li><a href="B21257_05.xhtml#_idTextAnchor077"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 5</span></em></a><em class="italic"><span class="koboSpan" id="kobo.9.1">, Extending Your Agent with RAG to Prevent Hallucinations</span></em></li>
<li><a href="B21257_06.xhtml#_idTextAnchor090"><em class="italic"><span class="koboSpan" id="kobo.10.1">Chapter 6</span></em></a><em class="italic"><span class="koboSpan" id="kobo.11.1">, Advanced RAG Techniques for Information Retrieval and Augmentation</span></em></li>
<li><a href="B21257_07.xhtml#_idTextAnchor113"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 7</span></em></a><em class="italic"><span class="koboSpan" id="kobo.13.1">, Creating and Connecting a Knowledge Graph to an AI Agent</span></em></li>
<li><a href="B21257_08.xhtml#_idTextAnchor137"><em class="italic"><span class="koboSpan" id="kobo.14.1">Chapter 8</span></em></a><em class="italic"><span class="koboSpan" id="kobo.15.1">, Reinforcement Learning and AI Agents</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer111">
</div>
</div>
<div>
<div id="_idContainer112">
</div>
</div>
<div>
<div id="_idContainer113">
</div>
</div>
<div>
<div id="_idContainer114">
</div>
</div>
<div>
<div id="_idContainer115">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer116">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer117">
</div>
</div>
<div>
<div id="_idContainer118">
</div>
</div>
<div>
<div id="_idContainer119">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer120">
</div>
</div>
</body></html>