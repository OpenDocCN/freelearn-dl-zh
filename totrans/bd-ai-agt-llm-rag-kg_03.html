<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-43"><a id="_idTextAnchor042"/>3</h1>
<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Exploring LLMs as a Powerful AI Engine</h1>
<p>In the previous chapter, we saw the structure of a transformer, how it is trained, and what makes it so powerful. The transformer is the seed of this <a id="_idIndexMarker181"/>revolution in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), and today’s <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) are<a id="_idIndexMarker182"/> all based on transformers trained at scale. In this chapter, we will see what happens when we train huge transformers (more than 100 billion parameters) with giant datasets. We will focus on how to enable this training at scale, how to fine-tune similar modern ones, how to get more manageable models, and how to extend them to multimodal data. At the same time, we will also see what the limitations of these models are and what techniques are used to try to overcome these limitations.</p>
<p>In this chapter, we'll be covering the following topics:</p>
<ul>
<li>Discovering the evolution of LLMs</li>
<li>Instruction tuning, fine-tuning, and alignment</li>
<li>Exploring smaller and more efficient LLMs</li>
<li>Exploring multimodal models</li>
<li>Understanding hallucinations and ethical and legal issues</li>
<li>Prompt engineering</li>
</ul>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Technical requirements</h1>
<p>Most of this code can be run on a CPU, but it is preferable to run it on a GPU. The code is written in PyTorch and uses standard libraries for the most part (PyTorch, Hugging Face Transformers, and so on). The code can be found on GitHub: <a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr3">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr3</a>.</p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>Discovering the evolution of LLMs</h1>
<p>An<a id="_idIndexMarker183"/> LLM is a transformer (although different architectures are beginning to emerge today). In general, an LLM is defined as a model that has more than 10 billion parameters. Although this number may seem arbitrary, some properties emerge with scale. These models are designed to understand and generate human language, and over time, they have acquired the ability to generate code and more. To achieve this beyond parameter size, they are trained with a huge amount of data. Today’s LLMs <a id="_idIndexMarker184"/>are almost all <a id="_idIndexMarker185"/>trained on <strong class="bold">next-word prediction</strong> (<strong class="bold">autoregressive </strong><strong class="bold">language modeling</strong>).</p>
<p>Parameter growth has been motivated in the transformer field by different aspects:</p>
<ul>
<li><strong class="bold">Learnability</strong>: According to the scaling law, more parameters should lead to greater capabilities and a greater understanding of nuances and complexities in the data</li>
<li><strong class="bold">Expressiveness</strong>: The model can express more complex functions, thus increasing the ability to generalize and reducing the risk of overfitting</li>
<li><strong class="bold">Memory</strong>: A larger number of parameters allows for internalizing more knowledge (information, entities, differences in topics)</li>
</ul>
<p>In the next subsections, we will discuss in detail all these elements to explain what is happening in the transition from the transformer to the LLM.</p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>The scaling law</h2>
<p>It may <a id="_idIndexMarker186"/>seem surprising that <a id="_idIndexMarker187"/>such large models are trained with such a simple task as <strong class="bold">language modeling</strong>. Many <a id="_idIndexMarker188"/>practical <strong class="bold">natural language</strong> tasks<a id="_idIndexMarker189"/> can be represented as next-word prediction. This flexibility allows us to use LLMs in different contexts. For example, sentiment analysis can be cast as a next-word prediction. The sentence “<em class="italic">The sentiment of the sentence: ‘I like Pizza’ is</em>” can be used as input for an LLM, and we can extract the probability for the next token being <em class="italic">positive</em> or <em class="italic">negative</em>. We can then assign the sentiment depending on which of the two has the higher probability. Notice how this probability is a function of context:</p>
<p>P(positive| <em class="italic">The sentiment of the sentence: ‘I like </em><em class="italic">Pizza’ is</em>)</p>
<p>P(negative| <em class="italic">The sentiment of the sentence: ‘I like </em><em class="italic">Pizza’ is</em>)</p>
<p>Similarly, we can use the same approach for other <a id="_idIndexMarker190"/>tasks. <strong class="bold">Question answering</strong> (<strong class="bold">QA</strong>) with an LLM can be thought of as generating the probability of the right answer given the question. In text summarization, we want to generate given the original context:</p>
<p>QA: P(answer| question)</p>
<p>Text summarization: P(summary|original article)</p>
<p>In the <a id="_idIndexMarker191"/>following diagram, we can see that using language modeling, we <a id="_idIndexMarker192"/>can solve almost any task. For example, here, the answer is the most probable token given the previous sequence (the question):</p>
<div><div><img alt="Figure 3.1 – Rephrasing of any task as LM" src="img/B21257_03_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Rephrasing of any task as LM</p>
<p>What we need is a dataset large enough for the model to both learn knowledge and use that knowledge for tasks. For this, specific datasets are assembled for training an LLM. These datasets typically consist of billions of words obtained from various sources (internet, books, articles, GitHub, different languages, and so on). For example, <strong class="bold">GPT-3</strong> was trained with Common <a id="_idIndexMarker193"/>Crawl (web crawl data, 410 billion tokens), Books1 and Books2 (book corpora, 12 billion and 55 billion tokens, respectively), and Wikipedia (3 billion tokens). Such diversity provides specific knowledge but also examples of tasks.</p>
<p>In parallel with the growth of training datasets (today, we are talking about more than a trillion tokens), the number of parameters has grown. The number of parameters in a transformer depends on three factors:</p>
<ul>
<li><strong class="bold">Embedding layer</strong>: The number of parameters on the size of the vector and the vocabulary (which, especially for multi-language models, can be very large). Attention mechanisms are the heaviest component and hold the most parameters.</li>
<li><strong class="bold">Self-attention mechanism</strong>: This component includes multiple weight matrices that can grow in size with context length. Also, there can be multiple heads per single self-attention.</li>
<li><strong class="bold">Depth</strong>: Transformers are composed of multiple transformer blocks, and increasing the number of these blocks directly adds more parameters to the model.</li>
</ul>
<p>GPT-3 <a id="_idIndexMarker194"/>and <a id="_idIndexMarker195"/>other studies have shown that the performance of LLMs depends mainly on three factors: model size (number of parameters), data size (the size of the training dataset), and computing size (amount of computing). So, in theory, to increase the performance of our model, we should enlarge the model (add layers or attention heads), increase the size of the pre-training dataset, and train it for more epochs. These factors have been related by OpenAI with the<a id="_idIndexMarker196"/> so-called <strong class="bold">scaling law</strong>. From a model with a number of parameters <em class="italic">N</em>, a dataset <em class="italic">D</em>, and computing amount <em class="italic">C</em>, if two parameters are constant, the loss <em class="italic">L</em> is the following:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>L</mi><mfenced close=")" open="("><mi>N</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>N</mi></msub></msup><mi>L</mi><mfenced close=")" open="("><mi>D</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>D</mi></msub></msup><mi>L</mi><mfenced close=")" open="("><mi>C</mi></mfenced><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mi>c</mi></msub><mi>C</mi></mfrac><mo>)</mo></mrow><msub><mi>α</mi><mi>C</mi></msub></msup></mrow></mrow></math></p>
<p>This is represented visually in the following diagram:</p>
<div><div><img alt="Figure 3.2 – Language modeling performance improves smoothly with the increase of model size, dataset size, and amount of computing (https://arxiv.org/pdf/2001.08361)" src="img/B21257_03_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Language modeling performance improves smoothly with the increase of model size, dataset size, and amount of computing (<a href="https://arxiv.org/pdf/2001.08361">https://arxiv.org/pdf/2001.08361</a>)</p>
<p>The loss is, in this case, the cross-entropy loss. In successive studies, OpenAI has shown that this loss can be decomposed <a id="_idIndexMarker197"/>into <strong class="bold">irreducible loss</strong> (which cannot be eliminated because it is related to data entropy) and reducible loss. This scaling law, in other words, allows us to calculate the desired performance of the model before training it. We can decide <a id="_idIndexMarker198"/>whether to invest more in enlarging the model or the dataset to reduce the loss (improve performance). However, these constants are dependent on the architecture and other training choices:</p>
<div><div><img alt="Figure 3.3 – Scaling law for an LLM" src="img/B21257_03_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Scaling law for an LLM</p>
<p>Although this scaling law<a id="_idIndexMarker199"/> has been taken for granted, the reality is more nuanced than it seems. According to DeepMind’s Chinchilla (<a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>), performance depends much more on the number of tokens than OpenAI believes. So, LLMs would currently be underfitted because they are trained with fewer tokens than expected. Meta’s Llama also states that not just any tokens will do, but they must be of quality. So, not all tokens count the same, and according to other authors, using tokens produced by other models is just a more sophisticated form of distillation. In other words, to train a model at its best, you need a large amount of tokens, and they should be preferentially produced by humans and not synthetics. Different studies showed the potential risk of the model collapsing when trained with synthetic data. In several cases, it has been shown that the model, when trained with synthetic data, has a<a id="_idIndexMarker200"/> substantial decrease in performance (<strong class="bold">model collapse</strong>) or may forget some of the skills it has <a id="_idIndexMarker201"/>learned (<strong class="bold">catastrophic forgetting</strong>).</p>
<p>In any case, the scaling law is of great interest because it allows us to experiment with different architectures and variants on smaller models and then scale the model and training until the desired performance is achieved. A model with more than 100 billion parameters is expensive to train (in terms of architecture, time, and money), so it is better to experiment with a small proxy model and then leverage what has been learned for training the larger model. Also, training such a large model can encounter issues (such as training spikes), and being able to predict performance with an accurate scaling law is an active area of research.</p>
<p>This scaling law also <a id="_idIndexMarker202"/>monitors performance only in terms of <a id="_idIndexMarker203"/>loss. As mentioned previously, many tasks can be defined in terms of language modeling (LM), so intuitively, better performance in LM also means better performance in downstream tasks. Today, however, we try to create scaling laws that are instead specific to performance in some desired tasks (if we want a model specifically trained as a code assistant, we are more interested in its performance in these tasks than in its overall performance).</p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Emergent properties</h2>
<p><strong class="bold">Emergent properties</strong> of a<a id="_idIndexMarker204"/> model are the main justification <a id="_idIndexMarker205"/>for why we have gone from 1 billion parameters to over 100 billion. Emergent abilities are defined as properties that are not present in a small model but emerge in a large model. The second characteristic is that they emerge abruptly at a certain scale. In other words, a model has random performances in a certain ability until they emerge when a certain size is reached. These properties cannot be predicted beforehand but only observed at a certain scale, called<a id="_idIndexMarker206"/> the <strong class="bold">critical scale</strong>. After this critical size, performance increases linearly with the increase in size. Then, the model goes from near-zero performance to near-state-of-the-art after a certain critical point, thus showing a discontinuous rhythm. This process is also called phase transition. It is like a child who grows up appearing to be unable to speak, then beyond a certain age begins to articulate words, and then their skills grow linearly over time.</p>
<div><div><img alt="Figure 3.4 – Example of an emergent property in an LLM" src="img/B21257_03_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Example of an emergent property in an LLM</p>
<p>Typically, these<a id="_idIndexMarker207"/> skills are related to complex skills <a id="_idIndexMarker208"/>such as mathematical reasoning or multistep processes. The fact that they emerge only beyond a certain scale justified the growth of such models, with the hope that beyond a certain scale, other properties would appear:</p>
<div><div><img alt="Figure 3.5 – Examples of emergent properties in different LLM families (https://arxiv.org/pdf/2206.07682)" src="img/B21257_03_5.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Examples of emergent properties in different LLM families (<a href="https://arxiv.org/pdf/2206.07682">https://arxiv.org/pdf/2206.07682</a>)</p>
<p>These <a id="_idIndexMarker209"/>properties do not all emerge at the same model <a id="_idIndexMarker210"/>size. Some properties would emerge beyond 10 billion (arithmetic computation), beyond 100 billion (self-evaluation, <strong class="bold">figure-of-speech</strong> (<strong class="bold">FoS</strong>) detection, logical deduction, and so on), and <a id="_idIndexMarker211"/>others even beyond 500 billion (causal judgment, geometric shapes, and so on).</p>
<p>For the authors of <em class="italic">Emergent Abilities of Large Language Models</em> (<a href="https://arxiv.org/pdf/2206.07682">https://arxiv.org/pdf/2206.07682</a>), reasoning tasks (especially those involving multiple steps) are difficult for LMs. These capabilities would appear naturally after 100 billion parameters. Similarly, beyond this threshold, the model is capable of understanding and following instructions (instruction following) without necessarily giving it examples of how to follow them. From this, it follows that larger models would be capable of executing programs (coding ability).</p>
<p>Interest in these emergent properties has cooled, however, because subsequent studies question them. LLMs do indeed exhibit these capabilities, but according to further studies, it would simply be more noticeable once the LLM has reached a certain performance limit. Moreover, it seems that success in these tasks is measured poorly.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Context length</h2>
<p>LLMs process text in chunks, a fixed <a id="_idIndexMarker212"/>context window of a specific number of tokens. The size of <a id="_idIndexMarker213"/>this context length defines how much information they can process at a given time. The greater the context length, the more information a model can handle at a given time. Similarly, the computational cost grows quadratically. So, a model with a context length of 4,096 tokens needs to do 64 times more computation than one of 512. A longer context length allows for capturing long-range dependencies in a text, and this is related to performance in specific tasks:</p>
<ul>
<li><strong class="bold">Document summarization</strong>: More<a id="_idIndexMarker214"/> context allows for more consistent and concise summarization, allowing for better capture of information in the document and its relationships. The model captures entities and what they are related to in the entire document.</li>
<li><strong class="bold">QA</strong>: The model<a id="_idIndexMarker215"/> can find complex relationships that underlie the right answer. Also, in multi-turn questions, the model is aware of previous answers and questions.</li>
<li><strong class="bold">Language translation</strong>: The model<a id="_idIndexMarker216"/> better preserves context, especially if there are long documents to be translated (especially if there are complex nuances). Bigger context lengths help with technical documents, technical jargon, polysemic items, and acronyms.</li>
<li><strong class="bold">Conversational AI</strong>: The <a id="_idIndexMarker217"/>model can conduct better tracking of the entire conversation.</li>
</ul>
<p>As we can see in the following <a id="_idIndexMarker218"/>figure, the larger the context length, the more data the model<a id="_idIndexMarker219"/> can access in one prompt. Only one review can be seen by the model with a context length of 512, while a model with a larger context window can analyze hundreds of them:</p>
<div><div><img alt="Figure 3.6 – Number of reviews that can be fit with an increasing context length window" src="img/B21257_03_6.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Number of reviews that can be fit with an increasing context length window</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Mixture of experts</h2>
<p>As we have seen, there is an intricate relationship between the amount of data, model scale, and computing budget. Given a fixed computing budget, it is better to train a larger model with fewer steps. A <strong class="bold">mixture of experts</strong> (<strong class="bold">MoE</strong>) allows <a id="_idIndexMarker220"/>one to train a model with less computing by scaling up the model with the same computing budget (which results in having a model as good as a dense one in less time). MoEs are, in general, made up of two components:</p>
<ul>
<li><strong class="bold">Sparse MoE layers</strong>: Each <a id="_idIndexMarker221"/>layer consists of several experts (typically eight, but can be more), and each expert is a <strong class="bold">neural network</strong> (in the simplest form, a <strong class="bold">feed-forward network</strong> (<strong class="bold">FFN</strong>) layer, but they can <a id="_idIndexMarker222"/>also consist of multiple layers).</li>
<li><strong class="bold">A gate network or router</strong>: This <a id="_idIndexMarker223"/>component decides what data is sent to each of the experts. In the case of an LLM, the router decides which tokens are seen by one or more experts. The router has learnable parameters that are trained during pre-training along with the rest of the model.</li>
</ul>
<p>You can see an example of an MoE layer in <em class="italic">Figure 3</em><em class="italic">.7</em>:</p>
<div><div><img alt="Figure 3.7 – Example of an MoE layer: The router decides to which expert the token is sent; in this case, the expert is a simple FFN layer (https://arxiv.org/pdf/2101.03961)" src="img/B21257_03_7.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Example of an MoE layer: The router decides to which expert the token is sent; in this case, the expert is a simple FFN layer (<a href="https://arxiv.org/pdf/2101.03961">https://arxiv.org/pdf/2101.03961</a>)</p>
<p>The idea behind <a id="_idIndexMarker224"/>MoEs is that each of the experts focuses on a different subset of the training (or, more formally, a different region of the input space) and the router learns when to recall this expertise. This is called sparse computation because the model is not active on all inputs to the same model.</p>
<p>This system has<a id="_idIndexMarker225"/> several advantages:</p>
<ul>
<li>Pre-training is faster compared to a dense model (classic transformer). The model is faster in inference since not all experts are used at the same time on all data.</li>
<li>The system is flexible, can handle complex distribution, and each expert can specialize in a subdomain.</li>
<li>It is much more scalable since we can have additional experts if needed.</li>
<li>Better generalization, because we can average the expert predictions (wisdom of the crowd).</li>
</ul>
<p>There are some <a id="_idIndexMarker226"/>disadvantages, however:</p>
<ul>
<li>It requires high VRAM because all the experts have to be loaded into memory anyway.</li>
<li>Training is more complex and can lead to overfitting. Also, without some accommodations, the model might use only the two or three most popular experts.</li>
<li>Fine-tuning is more complex, but new studies are solving the problem. MoE can be efficiently distilled, and we can also extract subnetworks.</li>
<li>More complex interpretability, since we have now additional components.</li>
</ul>
<p>This is why many of today’s large models are MoE (for example, GPT-4 or Gemini). In the next section, we will see once an LLM is pre-trained how to adapt it to better interact with users or how we can fine-tune such a large model.</p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/>Instruction tuning, fine-tuning, and alignment</h1>
<p>Fine-tuning such large <a id="_idIndexMarker227"/>models is potentially very expensive. In classical fine-tuning, the idea<a id="_idIndexMarker228"/> is to fit the weights of a model for a task or a new domain. Even if it is a slight update of the weights for a few steps, for a model of more than 100 billion parameters, this means having large hardware infrastructure and significant costs. So, we need a method that allows us to have efficient and low-cost fine-tuning and preferentially keeping the model weights frozen.</p>
<p>The <strong class="bold">intrinsic rank hypothesis</strong> suggests<a id="_idIndexMarker229"/> that we can capture significant changes that occur in a neural network using a lower-dimensional representation. In the case of fine-tuning, the model weights after fine-tuning can be defined in this way:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mi>W</mi><mo>′</mo></mrow><mi>X</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>:</mo><mrow><mi>W</mi><mo>′</mo></mrow><mo>=</mo><mi>W</mi><mo>+</mo><mo>∆</mo><mi>W</mi></mrow></mrow></math></p>
<p><em class="italic">∆W</em> represents the update of the weights during fine-tuning. For the intrinsic rank hypothesis, not all of these elements of <em class="italic">∆W</em> are important, and instead, we can represent it as the product of two matrices with small dimensions <em class="italic">A</em> and <em class="italic">B</em> (low-rank matrices). So, in this case, the model weights remain frozen, but we just need to train these two matrices:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mi>W</mi><mo>′</mo></mrow><mi>X</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mo>:</mo><mrow><mi>W</mi><mo>′</mo></mrow><mo>=</mo><mi>W</mi><mo>+</mo><mi>B</mi><mi>A</mi></mrow></mrow></math></p>
<p>A matrix can be decomposed into two smaller matrices that, when multiplied, give the original matrix. Also, a matrix (especially larger ones) contains a lot of redundant information. A matrix can be reduced into a set of linearly independent vectors (the number of linearly independent vectors needed to define a matrix is <a id="_idIndexMarker230"/>called a <strong class="bold">rank</strong>). With that in mind, the idea is to find two matrices that have a smaller rank than the original one and that multiplied with each other give us the same matrix update weights as if we had done fine-tuning. This<a id="_idIndexMarker231"/> process is called <strong class="bold">Low-Rank </strong><strong class="bold">Adaptation</strong> (<strong class="bold">LoRA</strong>).</p>
<p>LLMs are over-parametrized. Although this is beneficial during the pre-training stage, it makes fine-tuning very expensive. Because the weight matrices of an LLM have a lot of linear dependence, there is a lot of redundant information, which is especially useless for domain adaptation. So, we can learn much smaller matrices (<em class="italic">A</em> and <em class="italic">B</em>) at a much lower cost.</p>
<div><div><img alt="Figure 3.8 – Classical fine-tuning versus LoRA" src="img/B21257_03_8.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Classical fine-tuning versus LoRA</p>
<p>In LoRA, we <a id="_idIndexMarker232"/>keep the original weights of the LLM frozen. We then create two matrices (<em class="italic">A</em> and <em class="italic">B</em>) that, when multiplied together, will have the same dimensions<a id="_idIndexMarker233"/> as the model’s weight matrices (<em class="italic">W</em>). During fine-tuning, we pass the input <em class="italic">X</em> for the frozen model and the change matrix (the product <em class="italic">AB</em>) and get the output. With this output, we calculate the loss, and using this loss, we update the matrices <em class="italic">A</em> and <em class="italic">B</em> (via classical backpropagation). We continue this process until we are satisfied with the result.</p>
<div><div><img alt="Figure 3.9 – Different-ranked matrices to obtain the change weight matrix" src="img/B21257_03_9.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Different-ranked matrices to obtain the change weight matrix</p>
<p>In LoRA, we have a hyper-parameter <em class="italic">r</em> describing the depth of the <em class="italic">A</em> and <em class="italic">B</em> matrices. The greater the <em class="italic">r</em> value, the greater the amount of information these matrices have (but also a greater number of parameters and thus computational cost). The results show that even low-rank matrices perform quite well.</p>
<p>LoRA has several advantages:</p>
<ul>
<li>It is efficient in <a id="_idIndexMarker234"/>training (for GPT-3, a model of 175 billion parameters can be used efficiently by LoRA by training only 17.5 million parameters).</li>
<li>In inference, it does not increase the computational cost (it is an addition where we add the change matrix to the original model weights).</li>
<li>LoRA will not alter the original capabilities of the model. It also reduces the memory cost associated with saving checkpoints during fine-tuning.</li>
<li>We can create different change matrices for different applications and domains.</li>
</ul>
<p>Another technique that focuses on training only added parameters<a id="_idIndexMarker235"/> is <strong class="bold">adapters</strong>. In this case, we add tunable layers within transformer blocks. These adapters are small layers that <a id="_idIndexMarker236"/>have <strong class="bold">autoencoder</strong> (<strong class="bold">AE</strong>)-like structures. For example, if the fully connected layers have 1024 dimensions, the adapter projects to 24 and then reprojects to 1024. This means that we are adding fewer than 50K parameters per adapter. In the original paper, the authors showed that the addition of adapters achieved the same performance as<a id="_idIndexMarker237"/> fine-tuning <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>).  Adapter require only the additional training of 3.6 % more parameters. In contrast, fine tuning a model such as BERT in the traditional way means conducting training for all model parameters. This means that for the same performance, this method is computationally much more efficient.</p>
<div><div><img alt="Figure 3.10 – How adapters are added to the transformer block (left); results show that the adapters can reach the performance of regular fine-tuning with much fewer parameters (right) (https://arxiv.org/pdf/1902.00751)" src="img/B21257_03_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – How adapters are added to the transformer block (left); results show that the adapters can reach the performance of regular fine-tuning with much fewer parameters (right) (<a href="https://arxiv.org/pdf/1902.00751">https://arxiv.org/pdf/1902.00751</a>)</p>
<p>The advantages of adapters<a id="_idIndexMarker238"/> are that you can conduct fine-tuning by training far fewer parameters (a few million parameters for an LLM) and that the model retains the original capabilities.</p>
<p>However, many other methods try to solve the problem of conducting fine-tuning of the model without conducting training on the original parameters. For example, some techniques, such as <strong class="bold">prompt tuning</strong>, prepend<a id="_idIndexMarker239"/> the model input embeddings with a trainable tensor that learns details associated with the new tasks. <strong class="bold">Prefix tuning</strong> is<a id="_idIndexMarker240"/> another technique in which we add trainable tensors to the hidden states of all layers. These parameters are learned with gradient descent while the rest of the parameters remain frozen. Prompt tuning and prefix tuning can still cause instability during training. LoRA and adapters remain the most widely used techniques:</p>
<div><div><img alt="Figure 3.11 – Parameter-efficient fine-tuning methods taxonomy (https://arxiv.org/pdf/2303.15647)" src="img/B21257_03_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – Parameter-efficient fine-tuning methods taxonomy (<a href="https://arxiv.org/pdf/2303.15647">https://arxiv.org/pdf/2303.15647</a>)</p>
<p>Although technically, it can be called a fine-tuning method, <strong class="bold">alignment</strong> is a method that with additional training attempts to align an LLM with <a id="_idIndexMarker241"/>human values. Indeed, with increasing model capabilities, there<a id="_idIndexMarker242"/> is an increasing fear of ethical risks (which will be described in detail in a later section). Alignment is meant to reduce these risks by reducing the mismatch between mathematical training and the soft skills expected of a human being (helpful, honest, and harmless):</p>
<div><div><img alt="Figure 3.12 – Example to show the difference between the outputs before and after alignment (https://arxiv.org/pdf/2308.05374)" src="img/B21257_03_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – Example to show the difference between the outputs before and after alignment (<a href="https://arxiv.org/pdf/2308.05374">https://arxiv.org/pdf/2308.05374</a>)</p>
<p>An LLM during pre-training is trained to be nothing more than a sophisticated autocomplete model (predict the next word). With this simple objective, however, the model learns a vast knowledge and a wide array of skills. Alignment is intended to allow the model to use <a id="_idIndexMarker243"/>these skills obtained in pre-training in line with human values. Since <a id="_idIndexMarker244"/>human values can be subjective and difficult to encode in a mathematical objective, it was thought to use human feedback. Behind the <a id="_idIndexMarker245"/>success of ChatGPT is <strong class="bold">Reinforcement Learning from Human Feedback</strong> (<strong class="bold">RLHF</strong>), which precisely uses <strong class="bold">reinforcement learning</strong> to optimize an LLM based on human <a id="_idIndexMarker246"/>feedback.</p>
<p>RLHF consists of three <a id="_idIndexMarker247"/>main steps:</p>
<ol>
<li><strong class="bold">Supervised fine-tuning (SFT)</strong>: We <a id="_idIndexMarker248"/>select a list of prompts and ask human annotators to write outputs that match these prompts (from 10,000 to 100,000 pairs). We take a model that is not aligned (pre-trained LLM on a large text dataset) and fine-tune it on the prompts and the corresponding human-generated outputs. This is the SFT LLM, a model that tries to mimic annotator responses.</li>
<li><strong class="bold">Training reward model</strong>: We select a set of prompts and generate multiple outputs for each prompt using the SFT LLM. We then ask human annotators to rank them from preferred to less preferred (using criteria such as helpfulness or accuracy). Using this ranking, we train a reward model. The reward model takes as input the output of an LLM and produces a scalar reward signal as a measure of how well this output is aligned with human preferences.</li>
<li><strong class="bold">RLHF</strong>: We take a prompt <a id="_idIndexMarker249"/>and generate an output from the SFT LLM. We use the trained reward model to predict a reward on the output. Using a <a id="_idIndexMarker250"/>reinforcement learning algorithm (<strong class="bold">Proximal Policy Optimization</strong> (<strong class="bold">PPO</strong>)), we update the SFT LLM with the predicted reward. Adding a penalty term based on<a id="_idIndexMarker251"/> the <strong class="bold">Kullback-Leibler</strong> (<strong class="bold">KL</strong>) divergence prevents the model from straying too far from its original distribution (in other words, the output text remains consistent after RHLF).</li>
</ol>
<div><div><img alt="Figure 3.13 – Diagram illustrating the three-step process (https://arxiv.org/pdf/2203.02155)" src="img/B21257_03_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – Diagram illustrating the three-step process (<a href="https://arxiv.org/pdf/2203.02155">https://arxiv.org/pdf/2203.02155</a>)</p>
<p>The method is not without its problems, though. Collecting human preference data is quite expensive and requires hiring part-time staff as annotators. These annotators must also be selected to avoid variability and different quality in responses. Second, the process is rather complex and unstable. <strong class="bold">Direct Preference Optimization</strong> (<strong class="bold">DPO</strong>) is an alternative that attempts to solve part of these <a id="_idIndexMarker252"/>problems by eliminating the need to have a reward model. In short, the dataset is created according to this format: <em class="italic">&lt;prompt, worse completion, better completion&gt;</em>. DPO uses a loss function to increase the probability of better completion and decrease the probability of worse completion. This allows us to use backpropagation and avoid reinforcement learning:</p>
<div><div><img alt="Figure 3.14 – DPO optimizes for human preferences while avoiding RL (https://arxiv.org/pdf/2305.18290)" src="img/B21257_03_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – DPO optimizes for human preferences while avoiding RL (<a href="https://arxiv.org/pdf/2305.18290">https://arxiv.org/pdf/2305.18290</a>)</p>
<p><strong class="bold">Instruction tuning</strong> (<strong class="bold">IT</strong>) is a<a id="_idIndexMarker253"/> fine-tuning technique that is used to improve the model’s <a id="_idIndexMarker254"/>capabilities for various tasks and generally in following instructions. The principle is similar to alignment: the pre-trained model is trained to minimize word prediction on large corpora and not to execute instructions. Most user interactions with LLMs are requests to perform a specific task (write a text, create a function, summarize an article, and so on):</p>
<div><div><img alt="Figure 3.15 – General pipeline of IT (https://arxiv.org/pdf/2308.10792)" src="img/B21257_03_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – General pipeline of IT (<a href="https://arxiv.org/pdf/2308.10792">https://arxiv.org/pdf/2308.10792</a>)</p>
<p>To solve this mismatch, IT has been proposed to increase the model’s capabilities and controllability. The pre-trained model is further trained on a dataset that is a constituted instructions-outputs pair (instructions for the model and the desired output). This dataset is constructed from instructions that can be either annotated by humans or generated by other LLMs (such as GPT-4). Thus, the idea is to train the model to solve a task with a desired output. The model is evaluated with the desired output, and we use this output to optimize the model. These instructions usually represent NLP tasks and are of various kinds (up to 61 different tasks in some datasets), including tasks such as QA, summarization, classification, translation, creating writing, and so on). These instructions can <a id="_idIndexMarker255"/>then also contain additional content (for example, in <a id="_idIndexMarker256"/>summarization, we also provide the text to be summarized). To build such a dataset, a greater variety of tasks has greater benefit (especially tasks where the model must conduct reasoning and better if steps to follow are present in the context). Instruction tuning has several advantages. It makes the model capable of adapting even to unseen tasks (ensuring versatility) and is computationally efficient. It can also be used to fit the model to specific tasks for a particular domain (medical, finance, and so on). Also, it can be used in conjunction with other alignment techniques such as RLHF.</p>
<p>Despite these tuning techniques, it has enabled great advancement in the field of LLMs. The limitation of these techniques is that annotators can often be biased, and it is expensive to obtain quality datasets. In addition, it is always expensive to train a model that has billions of parameters. In addition, according to some authors, using AI-written instructions (or tests generated by AI) works as a kind of distillation but is less advantageous than using texts written by humans. In the next section, we will discuss how to obtain small LLMs when we do not want to deal with large LLMs.</p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Exploring smaller and more efficient LLMs</h1>
<p>LLMs show incredible capabilities but are also associated with large costs beyond training costs. Expensive infrastructure is also required for deployment, not to mention the costs associated with simple inference that grows with the number of parameters. These large LLMs are generalist models, and for many tasks, it is not necessary to have a model that has 100 billion parameters. Especially for many business cases, we need a model that can accomplish a specific task well. So, there are many cases where a <strong class="bold">small language model</strong> (<strong class="bold">SLM</strong>) is<a id="_idIndexMarker257"/> sufficient.</p>
<p>SLMs tend to excel in specialized domains, and may therefore lose the contextual informativeness that comes from integrating various domains of knowledge. SLMs may lose some of the capabilities of LLMs or otherwise exhibit fewer reasoning skills (thus being less versatile). On the other hand, they consume far fewer resources and can be used on a commercial GPU or even CPU (or, in extreme cases, cell phones).</p>
<p>More extensive studies of small models show that shallow models (with few transformer blocks) excel in grammar but have problems with consistency. So, a few layers are sufficient for syntactic correctness, but more layers are required for content coherence and creativity. Models that have hidden sizes might struggle with the continuation of a story, as this capability requires an increase in hidden size to at least a size of 128. Higher embedding dimensions impact the ability to generate continuations that are more accurate, relevant, and sound more natural (small embeddings lead the model to generate nonsense, contradictions, and irrelevant outputs). Also, models with a single layer are not capable of following instructions (such as continuing a story according to an input); at least two layers are needed, and the capacity increases almost proportionally as the layers increase (a single layer of attention does not produce a sufficient global representation).</p>
<p>So, there is a trade-off <a id="_idIndexMarker258"/>between capacity and model size. In general, we can say that there are three main possibilities for obtaining small and efficient LLMs:</p>
<ul>
<li><strong class="bold">Training a small LLM from scratch</strong>: For example, Mistral 7B or LLaMA 7B have been trained from scratch</li>
<li><strong class="bold">Knowledge distillation</strong>: One leverages a larger model to train a smaller model for a specific task (this can also be done using an LLM and a small LLM that is pre-trained; for example, using GPT-4 and BERT)</li>
<li><strong class="bold">Reducing the size of a model</strong>: For example, we can reduce the size of an LLM such as Mistral 7B using techniques such as quantization or pruning</li>
</ul>
<p>We have already discussed in the previous chapter knowledge distillation, and since LLMs are transformers, the process is the same. <code>float64</code>, <code>float16</code>, <code>int64</code>, <code>int8</code>, and so on). Float formats are used to save reals, while int formats can express only integers. Greater precision means that a weight can express a greater range. This for an LLM translates into more stable and more accurate training, though with the need for more hardware, memory, and cost.</p>
<div><div><img alt="Figure 3.16 – Example of the quantization process" src="img/B21257_03_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – Example of the quantization process</p>
<p>The problem is that the loss of accuracy for weights can translate into a substantial drop in model performance. Different<a id="_idIndexMarker260"/> quantization techniques attempt to reduce the accuracy of a model by avoiding damage to the original performance. One of the most popular<a id="_idIndexMarker261"/> techniques is <strong class="bold">affine quantization mapping</strong>, which allows one to go from a higher-precision number to a lower-precision number using two factors. Considering <em class="italic">x</em> with range [α,β], we can get its quantized version xq ϵ [αq,βq]:</p>
<p class="IMG---Figure"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math></p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><mi>s</mi><mo>=</mo><mfrac><mrow><mi>β</mi><mo>−</mo><mi>α</mi></mrow><mrow><msub><mi>β</mi><mi>q</mi></msub><mo>−</mo><msub><mi>α</mi><mi>q</mi></msub></mrow></mfrac><mi>z</mi><mo>=</mo><mi>r</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>d</mi><mo>(</mo><mfrac><mrow><mi>β</mi><msub><mi>α</mi><mi>q</mi></msub><mo>−</mo><mi>α</mi><msub><mi>β</mi><mi>q</mi></msub></mrow><mrow><mi>β</mi><mo>−</mo><mi>α</mi></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></math></p>
<p>Rounding<a id="_idIndexMarker262"/> is used to improve mapping. In practice, we also need to conduct clipping because, after mapping, the obtained value might be out of range of the new data type.</p>
<p>Not all model parameters are useful, both because there is a lot of linear dependence and because these models are practically underfitting. In the context of neural networks (and LLMs), the process of removing unnecessary weights is<a id="_idIndexMarker263"/> called <strong class="bold">pruning</strong>. This process refers to eliminating weights, connections, or even whole layers. <strong class="bold">Unstructured pruning</strong> is a simple <a id="_idIndexMarker264"/>technique in which, taking a pre-trained model, we eliminate connections or individual neurons, zeroing parameters. In the simplest form, this means we set to zero the connections that have a value below a certain threshold (the weights that are already near zero do not contain much information). Unstructured pruning can create sparse models that have suboptimal performance in inference, though. <strong class="bold">Structured pruning</strong>, on the other hand, is a more sophisticated technique in which we<a id="_idIndexMarker265"/> eliminate neurons, groups of neurons, structural components, entire layers, or blocks. Structural pruning seeks to preserve the performance of the original model by balancing accuracy and compression. Algorithms and other optimization systems have been developed for this. The two kinds of pruning are demonstrated in the following diagram:</p>
<div><div><img alt="Figure 3.17 – Schematic representation of pruning; the white elements represent pruned elements" src="img/B21257_03_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Schematic representation of pruning; the white elements represent pruned elements</p>
<p>For classical neural networks, most algorithms are based on eliminating the curvature of the loss versus the weights so that we can identify which weights are most important and which are not (a method called <strong class="bold">optimal brain surgeon</strong> (<strong class="bold">OBS</strong>)). Alternatively, several approaches involve training the <a id="_idIndexMarker266"/>model, reducing connectivity, and retraining the compressed model (this process can take several cycles). The problem with these classical approaches is that LLMs are composed of billions of parameters, and it would be too expensive to proceed with cycles of training and pruning. Some have, therefore, proposed possible fine-tuning of the model after pruning, but this for large LLMs is still computationally expensive. So, approaches are sought that can be used with LLMs without the need for retraining. This is not an easy task because overly aggressive pruning often leads to LLM collapse (many algorithms fail to remove more than 10% of the weights without avoiding collapse). Recently, approaches such as SparseGPT using pruning masks have achieved significant results (up to 60% compression on 170-billion-parameter models).</p>
<p>Since the model output can also be seen as the sum of the outputs of the model layers plus the embedding of the input, there will be terms in this sum that do not contribute much. The problem is that these terms are not exactly independent, so eliminating layers can create mismatches. You can study the contribution of each layer by looking at the output, though. Also, in each layer, the transformer learns a representation of the data, and in a very deep model, some layers will learn a similar representation. There is usually a hierarchy, where the deeper layers learn a more specialized representation than the initial layers. Some studies have started from these assumptions to eliminate layers, especially deeper layers that have layers with more similar representation. The results show that larger models have many more redundant layers than smaller models and can be efficiently compressed without altering performance too much:</p>
<div><div><img alt="Figure 3.18 – Percentage of the dropped layer before the LLM collapse (https://arxiv.org/pdf/2403.17887v1)" src="img/B21257_03_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – Percentage of the dropped layer before the LLM collapse (<a href="https://arxiv.org/pdf/2403.17887v1">https://arxiv.org/pdf/2403.17887v1</a>)</p>
<p>Pruning<a id="_idIndexMarker267"/> allows the memory footprint to be reduced and the inference time to be reduced. It is also a technique that allows us to study the importance of various structural components. In addition, it can be combined with other techniques such as quantization for further compression.</p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Exploring multimodal models</h1>
<p>LLMs, as by definition, are trained with<a id="_idIndexMarker268"/> text and to generate text. On the other hand, efforts have been made since the advent of the transformer to extend the model to other modalities. The addition of multimodal input allows the model to improve its reasoning capabilities and also to develop others. Human speech conveys a whole range of information that is not present in written words: voice, intonation, pauses, and facial expressions enhance communication but can also drastically change the meaning of a message.</p>
<p>We saw earlier that text can be transformed into a numeric vector. If we can transform a data type into a vector, we can then feed it to transformer blocks. So, the idea is to find a way to get a latent representation for each data type. For images, a way to adapt it to images was presented shortly after the original transformer was published: the <strong class="bold">Vision Transformer</strong> (<strong class="bold">ViT</strong>). ViTs are<a id="_idIndexMarker269"/> superior in several tasks to convolutional networks.</p>
<p>ViTs are typically built by the encoder alone. Having taken an image, it is divided into 16 x 16 patches (each patch can be thought of as being a token of a text). This is because a simple pixel does not represent much information, so it is more convenient to take a group of pixels (a patch). Once divided into patches, these are flattened (as if they were a sequence of patches). One clarification: since an image has multiple channels (color or RGB images have three channels), these must also be considered. Once this is done, there is usually a linear projection step to get tokens of the desired size (after this step, patches are no longer visually recognizable).</p>
<p>Given an image of<a id="_idIndexMarker270"/> height <em class="italic">H</em>, width <em class="italic">W</em>, and channels <em class="italic">C</em>, we get <em class="italic">N</em> tokens if the patch size is <em class="italic">P</em>:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>N</mi><mo>=</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><msup><mi>P</mi><mn>2</mn></msup></mfrac></mrow></mrow></math></p>
<p>The length of the<a id="_idIndexMarker271"/> token after linearization is P2 multiplied by the number of channels (3 if RGB; 1 if the image is black and white). Now, it is projected at a size chosen in advance (in the original version, 768, but it can also be different):</p>
<div><div><img alt="Figure 3.19 – Process of transforming images into tokens" src="img/B21257_03_19.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 – Process of transforming images into tokens</p>
<p>At this point, a special token representing the class is added, and a positional encoder is added here as well so that the model is aware of the position of the patches in the image. At this point, it enters the encoder, and the process is the same as if they were textual tokens. The<a id="_idIndexMarker272"/> encoder is constituted of transformer blocks, just as we saw before:</p>
<div><div><img alt="Figure 3.20 – ViT encoding process" src="img/B21257_03_20.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.20 – ViT encoding process</p>
<p>ViTs can be <a id="_idIndexMarker273"/>used for many <a id="_idIndexMarker274"/>different tasks, such as image classification, object detection, and segmentation:</p>
<div><div><img alt="Figure 3.21 – Examples of computer vision (CV) tasks done with ViTs" src="img/B21257_03_21.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.21 – Examples of computer vision (CV) tasks done with ViTs</p>
<p>Since musical sequences are also sequences, they too can be analyzed with transformers. There are now models that also process time series, DNA, and musical sequences. Considering that we have models for each of these modes, we have begun to think about combining them into a single model.</p>
<p>In the first chapter, we saw how embedding can be achieved using word2vec. Even a transformer can produce a latent representation that can be <a id="_idIndexMarker275"/>considered a vector embedding for a text. If we remove the last layer of a transformer, we can get a contextualized representation of a text (after all, the various layers of a transformer learn an increasingly sophisticated and contextualized representation of a text). This representation can be useful for many applications, and we will see this in detail later. Right now, we are interested in knowing that an LLM can generate a vector representing text. At the same time, a ViT can produce a vector representation of an image. Each of these models can then produce a single-mode embedding for a data type. A multimodal embedding, though, can capture information present in both images and text and relate them.</p>
<p>Since multimodal <a id="_idIndexMarker276"/>embedding would project images and text into the same space, we could exploit this embedding for tasks that were not possible before. For example, given a caption <em class="italic">x</em>, we could search for all images that are similar to this caption (or, obviously, the reverse). The most famous of these is <strong class="bold">Contrastive Language-Image Pre-Training</strong> (<strong class="bold">CLIP</strong>). CLIP was<a id="_idIndexMarker277"/> designed as a model that generates embedding for both images and text (today, there are multimodal embeddings for other modalities as well):</p>
<div><div><img alt="Figure 3.22 – CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) (https://arxiv.org/pdf/2103.00020)" src="img/B21257_03_22.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.22 – CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) (<a href="https://arxiv.org/pdf/2103.00020">https://arxiv.org/pdf/2103.00020</a>)</p>
<p>CLIP was <a id="_idIndexMarker278"/>trained with a dataset of 400 million (image, text) pairs collected from the internet, trying to cover as many visual concepts as possible. CLIP attempts to create a representation for both the image and the corresponding caption, using an encoder (a transformer model) for each of the two data types. Once an image and a caption are embedded by the corresponding encoders, the two embeddings are compared via cosine similarity. The model learns to maximize the cosine similarity between an image and its corresponding caption. At the same time, it tries to minimize the similarity with other incorrect pairings (very similar to what we saw for a text embedding, only <a id="_idIndexMarker279"/>this time it is multimodal). After that, we use this prediction to conduct the update of the <a id="_idIndexMarker280"/>model parameters (all two encoders). This learning method is called <strong class="bold">contrastive learning</strong>.</p>
<p>The training is framed as a classification task in which the model predicts the correct pair. Starting from these predictions, we compare them with the actual predictions and use cross-entropy loss. An interesting finding is that although the model is used to create an embedding, the authors used pre-trained models and combined them into a new model.</p>
<div><div><img alt="Figure 3.23 – Similarity matrix between captions and images" src="img/B21257_03_23.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.23 – Similarity matrix between captions and images</p>
<p>We can use <a id="_idIndexMarker281"/>CLIP to achieve the embedding of not only images but also captions. Once we get these embeddings, we can use them to calculate similarity. We can, thus, obtain a similarity matrix. This is easy using the Hugging Face<a id="_idIndexMarker282"/> libraries:</p>
<pre class="source-code">
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('clip-ViT-B-32')
image_embeddings = model.encode(images, convert_to_tensor=True)
caption_embeddings = model.encode(
    captions, convert_to_tensor=True)
similarity_matrix = util.cos_sim(
    image_embeddings, caption_embeddings)</pre> <p class="callout-heading">Important note</p>
<p class="callout">We are creating an embedding for both images and captions and then computing a similarity matrix.</p>
<p>In the original article, one of<a id="_idIndexMarker283"/> the first applications for which CLIP was conceived was <strong class="bold">zero-shot classification</strong>. For<a id="_idIndexMarker284"/> example, given a set of labels, we can ask the model to classify an image:</p>
<div><div><img alt="Figure 3.24 – Zero-shot image classification" src="img/B21257_03_24.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.24 – Zero-shot image classification</p>
<p>CLIP <a id="_idIndexMarker285"/>can also be used for various other tasks, such as large dataset searches or conducting image clustering and then assigning keywords to these clusters. CLIP, though, cannot be used to generate text like generating a caption for an image. For this, we need a <strong class="bold">vision-language model</strong> (<strong class="bold">VLM</strong>). A <a id="_idIndexMarker286"/>VLM essentially behaves like an LLM, though it can also answer questions about an image, solving a limitation of LLMs. In other words, with a VLM, we can conduct reasoning in a similar way to a classical LLM<a id="_idIndexMarker287"/> but also with images. An example is <strong class="bold">Bootstrapping Language-Image Pre-training </strong>(<strong class="bold">BLIP-2</strong>), in which instead of creating a model from scratch, they took an LLM and ViT and connected them with a bridge (<strong class="bold">Q-Former</strong>). The Q-Former<a id="_idIndexMarker288"/> is an additional component to connect the image encoder with the<a id="_idIndexMarker289"/> LLM (basically providing eyes to our LLM):</p>
<div><div><img alt="Figure 3.25 – Overview of BLIP-2’s framework (https://arxiv.org/pdf/2301.12597)" src="img/B21257_03_25.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.25 – Overview of BLIP-2’s framework (<a href="https://arxiv.org/pdf/2301.12597">https://arxiv.org/pdf/2301.12597</a>)</p>
<p>The Q-Former consists of two components (one interacting with ViT and one interacting with the LLM); it is the only part of the model that is trained. This process occurs in two stages, one for each mode. In the first stage, we use an image-captions pair to train the Q-Former to relate images and text. In the second step, the embeddings learned by the Q-Former are used as soft prompts to condition the LLM on textual representations of the images (make the LLM aware of the images). Once the Q-Former has been trained, we can use the model to generate text about the images:</p>
<div><div><img alt="Figure 3.26 – BLIP-2 captioning of the image" src="img/B21257_03_26.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.26 – BLIP-2 captioning of the image</p>
<p>Since it is a VLM, we can also ask several questions and chat with the model about the image:</p>
<div><div><img alt="Figure 3.27 – Different rounds of questions to BLIP-2 about an image" src="img/B21257_03_27.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.27 – Different rounds of questions to BLIP-2 about an image</p>
<p>Speaking of <a id="_idIndexMarker290"/>multimodal models, another type of model that has had a strong expansion in recent times is <strong class="bold">text-to-image models</strong>. Stable Diffusion is considered a milestone for its quality of image <a id="_idIndexMarker291"/>generation, its performance, and its availability to the masses. The operation of this model can be summarized at a high level: given textual directions (a prompt), the system generates an image according to the instructions. Other alternatives also exist today (text-to-video, image modification guided by text, and so on), but the principle is similar. At a high level, we can say that there are three main ones:</p>
<ul>
<li><strong class="bold">A text encoder</strong>: The<a id="_idIndexMarker292"/> text encoder is a model (usually CLIP or another LLM specifically trained for this function) that takes a text and returns a vector representation of the text.</li>
<li><strong class="bold">An image generator</strong>: This<a id="_idIndexMarker293"/> is a U-Net that generates the image representation. During this process, the generation is conditioned on the text.</li>
<li><strong class="bold">An image decoder</strong>: The <a id="_idIndexMarker294"/>image representation is transformed into an actual image. Usually, this component is a ViT or an AE decoder.</li>
</ul>
<p>The heart of the system is the<a id="_idIndexMarker295"/> U-Net, and in this component, the diffusion process takes <a id="_idIndexMarker296"/>place. The U-Net does not work directly on the image but on a compact representation called latent representation (which is basically a matrix). This latent representation, though, contains the information to generate an image, a process that is then conducted in the last step by the decoder.</p>
<p>During the diffusion process, starting from random noise, we begin to build a latent representation that acquires information about the image. Diffusion models are based on the idea that a model, given a large enough training set, can learn information about the contained patterns. During training, having taken an image, we generate some random noise and add a certain amount of noise to the image. This allows us to expand our image dataset widely (since we can control the amount of noise we can add to an image and thus create different versions of an image with more or less noise). The model is then trained to identify and predict the noise that has been added to the image (via classical backpropagation). The model then predicts the noise that needs to be subtracted in order to get the image (not exactly the image, but the distribution of it). By conducting this denoising process, we can then obtain a backward image (or, at least, its latent representation). So, starting from noise, we can get an image, and the model is trained to find an image in the noise. At this point, we use a decoder and we get an image. Up to this point, though, we cannot control this generation with text.</p>
<div><div><img alt="Figure 3.28 – Stable diffusion architecture (https://arxiv.org/pdf/2112.10752)" src="img/B21257_03_28.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.28 – Stable diffusion architecture (<a href="https://arxiv.org/pdf/2112.10752">https://arxiv.org/pdf/2112.10752</a>)</p>
<p>This is where the text encoder comes in. The choice of LLM is important; the better the LLM, the better the information this model can bring. As we saw earlier, CLIP has been trained on captions and corresponding images and is capable of producing textual embeddings. The idea behind CLIP is that the textual embeddings are close in embedding space to that of the corresponding images. Having arrived at textual information as an embedding, this information will be used to generate an image. In fact, in the U-Net, there is <a id="_idIndexMarker297"/>cross-attention that connects this textual information with the generation process.</p>
<p>We have seen how these models can also answer questions about images or generate images. These models don’t always answer questions optimally, and this can cause serious consequences. Or, at the same time, they can generate problematic images. We will discuss exactly this in the next section.</p>
<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/>Understanding hallucinations and ethical and legal issues</h1>
<p>A well-known problem with LLMs is their <a id="_idIndexMarker298"/>tendency to hallucinate. <strong class="bold">Hallucination</strong> is defined as the production of nonsensical or unfaithful content. This is classified into factuality hallucination and faithfulness hallucination. <strong class="bold">Factual hallucinations</strong> are <a id="_idIndexMarker299"/>responses produced by the model that contradict real, verifiable facts. <strong class="bold">Faithfulness hallucination</strong>, on the other hand, is content that is at odds with instructions <a id="_idIndexMarker300"/>or context provided by the user. The model is trained to generate consistent text but has no way to revise its output or check that it is correct:</p>
<div><div><img alt="Figure 3.29 – Example of LLM hallucination (https://arxiv.org/pdf/2311.05232)" src="img/B21257_03_29.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.29 – Example of LLM hallucination (<a href="https://arxiv.org/pdf/2311.05232">https://arxiv.org/pdf/2311.05232</a>)</p>
<p>The model can also <a id="_idIndexMarker301"/>generate toxic content and present stereotypes and negative attitudes toward specific demographic groups. It is important to prevent models from producing harm. Different studies have highlighted different instances of potential harm resulting from the use of AI in general and LLMs in particular. One example<a id="_idIndexMarker302"/> is <strong class="bold">representational harm</strong>, caused by a model that can perpetuate stereotypes or bias. This was previously seen with sentiment classifiers that assigned lower sentiment and negative emotion to particular groups of people. In fact, LLMs can produce offensive or derogatory language when representing minorities, or they can perpetuate society’s stereotypes about cultural norms, attitudes, and prejudices. This can lead to what is called <strong class="bold">allocational harm</strong>, when a model <a id="_idIndexMarker303"/>allocates resources unfairly. For example, if an LLM is used to decide the priority of access to medical treatment (or a job or credit), it might allocate access unfairly due to the biases it has inherited from its training.</p>
<p>Indeed, it had already been noted that embedding models can amplify biases, and these biases were reflected within the embedding space. The association of harmful content with groups and minorities was identified in the embedding space. In some cases, some LLMs used pre-trained embedding model weights as the initialization of the embedding layer. Some <strong class="bold">debiasing approaches</strong> (removal of bias from the model) have shown potential, but they are still far from<a id="_idIndexMarker304"/> being effective.</p>
<p>These biases stem from the <a id="_idIndexMarker305"/>pre-training dataset, so it is important to detoxify and remove problematic content before training. When fine-tuning a model, it is important to check for incorrect labels derived from annotator bias. It is also important to vary the sources. There is indeed an imbalance in the content used to train the model between text produced in the US and other countries. The model, therefore, inherits the perspective of the dominant demographics in its pre-training.</p>
<div><div><img alt="Figure 3.30 – Risk associated with hallucination and disinformation (https://aclanthology.org/2023.findings-emnlp.97.pdf)" src="img/B21257_03_30.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.30 – Risk associated with hallucination and disinformation (<a href="https://aclanthology.org/2023.findings-emnlp.97.pdf">https://aclanthology.org/2023.findings-emnlp.97.pdf</a>)</p>
<p>Another potential risk of LLMs is their use to <a id="_idIndexMarker306"/>produce <strong class="bold">misinformation</strong>. LLMs are capable of producing credible, convincing text. Malicious actors could use them to automate the production of misinformation, phishing emails, rage bait, and other harmful content. This is why an important research topic is how to detect text generated by LLMs (or alternatively add watermarks to text generation).</p>
<div><div><img alt="Figure 3.31 – Taxonomy of LLM-generated misinformation (https://arxiv.org/pdf/2309.13788)" src="img/B21257_03_31.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.31 – Taxonomy of LLM-generated misinformation (<a href="https://arxiv.org/pdf/2309.13788">https://arxiv.org/pdf/2309.13788</a>)</p>
<p>Today, there are several <a id="_idIndexMarker307"/>datasets and libraries in Python that allow one to study model bias. For example, one of the packages is the Hugging Face library, Evaluate. We can, for example, use a set of prompts and change the gender of the prompt. After that, we can evaluate with Evaluate how the model completes these prompts (the model used is GPT-2). Evaluate uses, in this case, another model trained for this purpose:</p>
<pre class="source-code">
import evaluate
toxicity = evaluate.load("toxicity")
toxicity.compute(
    predictions=model_continuations,
    aggregation="ratio"
)</pre> <p>As we can see in the following heatmap, we have a difference in how the model completes the prompts:</p>
<div><div><img alt="Figure 3.32 – Heatmap of gendered completion and associated toxicity" src="img/B21257_03_32.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.32 – Heatmap of gendered completion and associated toxicity</p>
<p>Models may also have a bias<a id="_idIndexMarker308"/> regarding occupations. We can use the same library again to also evaluate the polarity of the prompts that have been completed by the model. In this case, we evaluate the sentiment associated with each of the completed prompts for each of the two professions:</p>
<pre class="source-code">
regard = evaluate.load("regard", "compare")
regard_results = regard.compute(
    data = profession1_completions,
    references = profession2_completions
)</pre> <p>The completed prompts for CEOs are much more positive than those generated for truck drivers:</p>
<div><div><img alt="Figure 3.33 – Bias distribution for two different professions" src="img/B21257_03_33.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.33 – Bias distribution for two different professions</p>
<p>Another point of contention is the <strong class="bold">copyright issue</strong>. These <a id="_idIndexMarker309"/>models are trained on copyrighted text and can regenerate part of the text they are trained on. So far, the creators of these LLMs have claimed that they are covered by the fair use doctrine, which has allowed various companies to train models on text scraped from the internet even without permission. Today, though, some lawsuits are pending that could change the political and legal landscape. Some companies, therefore, are trying to sign licensing contracts with newspaper publishers or social networks.</p>
<p>Linked to the same problem <a id="_idIndexMarker310"/>is a <strong class="bold">privacy issue</strong> risk. These models can leak information about their training data. It is possible with adversarial attacks to extract information from the model. The model can store a huge amount of information in its parameters, and if trained with databases that contain personal information, this can later be extracted. Therefore, <strong class="bold">machine unlearning</strong> methods<a id="_idIndexMarker311"/> are being studied to make a model forget personal data. Legislation being studied in different countries may require a model to forget information of users who request it. We will discuss privacy in detail in <a href="B21257_06.xhtml#_idTextAnchor090"><em class="italic">Chapter 6</em></a>.</p>
<p>A final point is that these models are now capable of generating code, and this code can be used to produce malware and viruses. In addition, these models will be increasingly connected, and some studies show how these LLMs can potentially be used to spread computer viruses. In the next section, we will see how to efficiently use these models through prompt techniques.</p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>Prompt engineering</h1>
<p><strong class="bold">In-context learning</strong> (<strong class="bold">ICL</strong>) is one of the<a id="_idIndexMarker312"/> most fascinating properties of LLMs. Traditionally, <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models<a id="_idIndexMarker313"/> are trained to solve specific tasks drawn<a id="_idIndexMarker314"/> on training data. For example, in a classical classification task, we have input-output pairs (<em class="italic">X</em>,<em class="italic">y</em>), and the model learns to map the relationship that is between input X and output y. Any deviation from this task leads the model to have less-than-optimal results. If we train a model for text classification in different topics, we have to conduct fine-tuning to make it efficient in sentiment analysis. In contrast, ICL allows us not to have to have any model update to use the model in a new task. ICL is, thus, an emergent property of LLMs that allows the model to perform a new task in inference, taking advantage of the acquired knowledge to map a new relationship.</p>
<p>ICL was first defined in the article <em class="italic">Language Models are Few-Shot Learners </em>(<a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a>). The authors define LLMs as few-shot learners because, given a set of examples in the prompt (textual input for an LLM), the model can map the relationship between input and output and have learned a new task. This new skill is “learned” in context because the LLM exploits the examples in the prompt (which then provide context):</p>
<div><div><img alt="Figure 3.34 – Example of ICL abilities (https://arxiv.org/pdf/2005.14165)" src="img/B21257_03_34.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.34 – Example of ICL abilities (<a href="https://arxiv.org/pdf/2005.14165">https://arxiv.org/pdf/2005.14165</a>)</p>
<p>In this case, the concept of “learning” is improper because the model is not really learning (in fact, there is no update of internal parameters), and therefore the learned skill is only transient. In other words, the model exploits what it has already learned (its latent representation) to perform a new task. The model exploits the relationships that have been learned in pre-training, to map the latent function that is between input and output present in the prompt.</p>
<p>ICL has different<a id="_idIndexMarker315"/> advantages:</p>
<ul>
<li>It mirrors the human cognitive reasoning process, so it makes it easier to describe a problem and exploit an LLM.</li>
<li>It doesn’t require parameter upgrades, so it’s fast and can be used with a model in inference. It requires only a few examples.</li>
<li>ICL has shown that in this, the model can achieve competitive performance in several benchmarks.</li>
</ul>
<p>At present, it is still not entirely clear how this behavior emerges. According to some, the root of ICL is precisely multi-head self-attention and how the various attention heads manage to create interconnected circuits between layers. The prompt, in general, provides several elements (format, inputs, outputs, and input-output mapping), and they are important for the model to succeed in achieving the mapping. Initial work suggests that the model succeeds in “locating” latent concepts that it acquired during training. In other words, the model infers from the examples what the task is, but the other elements of the prompt help it succeed in locating in its parameters the latent concepts it needs to do this mapping. Specifically, some work states that the format in which demonstrations are presented is the most important element (for example, in the form of input-label pairs):</p>
<div><div><img alt="Figure 3.35 – Prompt structure (https://arxiv.org/pdf/2202.12837)" src="img/B21257_03_35.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.35 – Prompt structure (<a href="https://arxiv.org/pdf/2202.12837">https://arxiv.org/pdf/2202.12837</a>)</p>
<p>The community has become excited about this ability because <a id="_idIndexMarker316"/>ICL allows the model to “learn” a task in inference simply by manipulating the prompt. ICL has allowed specific techniques to evolve to be able to perform increasingly sophisticated tasks without the need to fine-tune the model.</p>
<p>For clarity, we can define some terminology and elements in what are prompts (or formatting guidelines). First, a prompt typically contains a question or instruction:</p>
<pre class="source-code">
When was Shakespeare born?</pre> <p>The preceding example is a prompt that contains only one question. By convention, it is referred to as <code>generate code for function x in Python</code>). The model that successfully responds to this type of prompt is said to have zero-shot capabilities, and this ability is enhanced by the instruction tuning of a pre-trained model:</p>
<pre class="source-code">
This movie is awesome – positive
This sandwich is disgusting – negative
This TV series is meh -</pre> <p>This is a typical case of <strong class="bold">few-shot prompting</strong> where <a id="_idIndexMarker318"/>we provide examples in the prompt. More demonstrations usually help the LLM (3-shot, 5-shot, or even 10-shot are common cases). A prompt can also have context to help the model. We can also add the desired format for the response. However, these simple prompts have limitations, especially for tasks that require reasoning. Especially when this requires multiple reasoning steps, providing examples is not enough to guide the model in the right direction. Several techniques have been proposed to avoid the need for fine-tuning.</p>
<p>Especially when dealing <a id="_idIndexMarker319"/>with an arithmetic problem, seeing examples and associated answers is not very helpful in learning the process. A student has more benefit in understanding the rationale before approaching the solution of such a problem. Similarly, an LLM has more benefit in getting the rationale of an answer than more examples with labels alone. <strong class="bold">Chain-of-thought prompting</strong> does<a id="_idIndexMarker320"/> exactly that; a triplet, &lt;input, chain of thought, output&gt;, is provided in the prompt. A chain of thought is the different intermediate steps to solve the problem:</p>
<div><div><img alt="Figure 3.36 – Example of chain-of-thought (https://arxiv.org/pdf/2201.11903)" src="img/B21257_03_36.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.36 – Example of chain-of-thought (<a href="https://arxiv.org/pdf/2201.11903">https://arxiv.org/pdf/2201.11903</a>)</p>
<p>Adding these demonstrations makes it easier for the model to solve the task. It has the disadvantage, though, that we must have quality demonstrations for several problems, and collecting such annotated datasets is expensive.</p>
<p>The advantage of <a id="_idIndexMarker321"/>CoT is that it divides a task for the model into a more manageable series of steps. This behavior can be incentivized simply by adding “Let’s think step by step” to the prompt. This seemingly simple approach is <a id="_idIndexMarker322"/>called <strong class="bold">zero-shot CoT prompting</strong>. The authors of the paper <em class="italic">Large Language Models are Zero-Shot Reasoners</em> (<a href="https://arxiv.org/pdf/2205.11916">https://arxiv.org/pdf/2205.11916</a>) suggest that the model has inherent reasoning skills in zero-shot settings, and this approach is therefore versatile because it prompts the model to use the skills it has learned in training:</p>
<div><div><img alt="Figure 3.37 – Schematic diagram illustrating various approaches to problem-solving with LLMs (https://arxiv.org/pdf/2305.10601)" src="img/B21257_03_37.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.37 – Schematic diagram illustrating various approaches to problem-solving with LLMs (<a href="https://arxiv.org/pdf/2305.10601">https://arxiv.org/pdf/2305.10601</a>)</p>
<p>Other techniques, such as <strong class="bold">self-consistency</strong>, have<a id="_idIndexMarker323"/> also been used to improve reasoning skills. The idea behind it is ensembling, in which different models can come to the right solution by majority vote. In this case, we generate several solutions and then choose the majority solution. <strong class="bold">Tree of Thoughts</strong> (<strong class="bold">ToT</strong>), on the other hand, exploits reasoning and self-evaluation <a id="_idIndexMarker324"/>capabilities, where the model generates different reasoning intermediates and then evaluates them by exploiting search algorithms (breadth-first search and depth-first search). One usually has to choose the number of candidate paths and steps. These techniques allow a higher reasoning capacity of the model but have a higher computational cost since the model has to generate several responses.</p>
<div><div><img alt="Figure 3.38 – Examples of using the DSPy system (https://arxiv.org/abs/2310.03714)" src="img/B21257_03_38.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.38 – Examples of using the DSPy system (<a href="https://arxiv.org/abs/2310.03714">https://arxiv.org/abs/2310.03714</a>)</p>
<p><strong class="bold">Declarative Self-improving Language Programs in Python</strong> (<strong class="bold">DSPy</strong>) is an interesting new paradigm that has<a id="_idIndexMarker325"/> been evolving in recent times. Until now, it has been assumed that we have to manually create these prompts, and this requires a lot of trial and error. Instead, DSPy seeks to standardize this prompting process and turn it into a kind of programming. In short, the authors of DSPy (<a href="https://arxiv.org/abs/2310.03714">https://arxiv.org/abs/2310.03714</a>) suggest that we can abstract prompts and fine-tune them into signatures while prompting techniques are used as modules. The result is that prompt engineering can be automated with optimizers. Given a dataset, we create a pipeline of DSPy containing signatures and modules (how these techniques are connected), define which metrics to optimize, and then optimize (we define what output we search for and the optimizer). The process is, then, iterative; DSPy leads to optimizing prompts that we can then use.</p>
<p>The techniques we have seen in this section are the most commonly used. There are many others, but they are generally variations of those described here. We now have all the elements to be able to successfully use an LLM.</p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Summary</h1>
<p>In this chapter, we discussed the transition from transformers to LLMs. The transformer was an elegant evolution and synthesis of 20 years of research in NLP, combining the best of research up to that point. In itself, the transformer contained a whole series of elements that enabled its success and versatility. The beating heart of the model is self-attention, a key tool – but also the main limitation of the LLM. On the one hand, it allows for learning sophisticated representations of text that make LLMs capable of countless tasks; on the other hand, it has a huge computational cost (especially when scaling the model). LLMs are not only capable of solving tasks such as classification but also tasks that assume some reasoning, all simply by using text instructions. In addition, we have seen how to fit the transformer even with multimodal data.</p>
<p>So far, the model produces only text, although it can produce code as well. At this point, why not allow the model to be able to execute the code? Why not allow it to use tools that can extend its capabilities? This is what we will see in the next chapters.</p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor056"/>Further reading</h1>
<ul>
<li>Everton et al<em class="italic">.</em>, <em class="italic">Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy</em>, 2023, <a href="https://arxiv.org/abs/2312.10549">https://arxiv.org/abs/2312.10549</a></li>
<li>Raieli, <em class="italic">Emergent Abilities in AI: Are We Chasing a Myth?</em>, 2023, <a href="https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9">https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9</a></li>
<li>Rasyl et al., <em class="italic">Preference Tuning LLMs with Direct Preference Optimization Methods</em>, 2024, <a href="https://huggingface.co/blog/pref-tuning">https://huggingface.co/blog/pref-tuning</a></li>
<li>Alemi, <em class="italic">KL is All You Need</em>, 2024, <a href="https://blog.alexalemi.com/kl-is-all-you-need.html">https://blog.alexalemi.com/kl-is-all-you-need.html</a></li>
<li>OpenAI, <em class="italic">Proximal Policy </em><em class="italic">Optimization</em>, <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">https://spinningup.openai.com/en/latest/algorithms/ppo.html</a></li>
<li>Simonini, <em class="italic">Proximal Policy Optimization (PPO)</em>, 2022, <a href="https://huggingface.co/blog/deep-rl-ppo">https://huggingface.co/blog/deep-rl-ppo</a></li>
<li>Hoffmann et al., <em class="italic">Training Compute-Optimal Large Language Models</em>, 2022, <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a></li>
<li>Brown et al., <em class="italic">Language Models are Few-Shot Learners</em>, 2020, <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></li>
</ul>
</div>


<div><h1 id="_idParaDest-58" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor057"/>Part 2: 
AI Agents and Retrieval 
of Knowledge</h1>
<p>This part focuses on extending the capabilities of LLMs by enabling them to access, retrieve, and reason over external sources of knowledge. It begins with the creation of AI agents that can interact with the web, retrieve live information, and execute tasks beyond simple question answering. The following chapters explore retrieval-augmented generation (RAG), starting from basic pipelines and advancing toward more modular and scalable systems that reduce hallucinations and improve factual accuracy. The use of structured knowledge through knowledge graphs (GraphRAG) is then introduced as a powerful method to represent and reason over information. Finally, this part discusses how reinforcement learning can be used to align agent behavior and improve decision-making through interaction with dynamic environments. These chapters collectively show how to build agents that are not only language-capable but also context-aware, goal-driven, and grounded in external information.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B21257_04.xhtml#_idTextAnchor058"><em class="italic">Chapter 4</em></a><em class="italic">, Building a Web Scraping Agent with an LLM</em></li>
<li><a href="B21257_05.xhtml#_idTextAnchor077"><em class="italic">Chapter 5</em></a><em class="italic">, Extending Your Agent with RAG to Prevent Hallucinations</em></li>
<li><a href="B21257_06.xhtml#_idTextAnchor090"><em class="italic">Chapter 6</em></a><em class="italic">, Advanced RAG Techniques for Information Retrieval and Augmentation</em></li>
<li><a href="B21257_07.xhtml#_idTextAnchor113"><em class="italic">Chapter 7</em></a><em class="italic">, Creating and Connecting a Knowledge Graph to an AI Agent</em></li>
<li><a href="B21257_08.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Reinforcement Learning and AI Agents</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>