<html><head></head><body>
		<div id="_idContainer054">
			<h1 id="_idParaDest-219" class="chapter-number"><a id="_idTextAnchor218"/><st c="0">10</st></h1>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor219"/><st c="3">Key RAG Components in LangChain</st></h1>
			<p><st c="35">This chapter takes an in-depth look at the key technical components that we have been talking about as they relate to </st><strong class="bold"><st c="154">LangChain</st></strong><st c="163"> and </st><strong class="bold"><st c="168">retrieval-augmented generation</st></strong><st c="198"> (</st><strong class="bold"><st c="200">RAG)</st></strong><st c="204">. As a refresher, the key technical components of our RAG system, in order of how they are used, are </st><strong class="bold"><st c="305">vector stores</st></strong><st c="318">, </st><strong class="bold"><st c="320">retrievers</st></strong><st c="330">, and </st><strong class="bold"><st c="336">large language models</st></strong><st c="357"> (</st><strong class="bold"><st c="359">LLMs</st></strong><st c="363">). </st><st c="367">We will step through the latest version of our code, last seen in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="433">Chapter 8</st></em></span></a><st c="442">, </st><em class="italic"><st c="444">Code lab 8.3</st></em><st c="456">. We will focus on each of these core components, and we will show the various options for each component using LangChain in the code. </st><st c="591">Naturally, a lot of this discussion will highlight differences among each option and discuss the different scenarios in which one option might be better </st><span class="No-Break"><st c="744">over another.</st></span></p>
			<p><st c="757">We start with a code lab outlining options for your </st><span class="No-Break"><st c="810">vector store.</st></span></p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor220"/><st c="823">Technical requirements</st></h1>
			<p><st c="846">The code for this chapter is placed in the following GitHub </st><span class="No-Break"><st c="907">repository: </st></span><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_10"><span class="No-Break"><st c="919">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_10</st></span></a></p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor221"/><st c="1016">Code lab 10.1 – LangChain vector store</st></h1>
			<p><st c="1055">The goal for all these code </st><a id="_idIndexMarker623"/><st c="1084">labs is to help you become more familiar with how the</st><a id="_idIndexMarker624"/><st c="1137"> options for each key component offered within the LangChain platform can enhance your RAG system. </st><st c="1236">We will dive deep into what each component does, available functions, parameters that make a difference, and ultimately, all of the options you can take advantage of for a better RAG implementation. </st><st c="1435">Starting with </st><em class="italic"><st c="1449">Code lab 8.3</st></em><st c="1461">, (skipping </st><a href="B22475_09.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic"><st c="1473">Chapter 9</st></em></span></a><st c="1482">’s evaluation code), we will step through these elements in order of how they appear in code, starting with the vector stores. </st><st c="1610">You can find this code in its entirety in the </st><a href="B22475_10.xhtml#_idTextAnchor218"><span class="No-Break"><em class="italic"><st c="1656">Chapter 10</st></em></span></a><st c="1666"> code folder on GitHub also labeled </st><span class="No-Break"><st c="1702">as </st></span><span class="No-Break"><strong class="source-inline"><st c="1705">10.1</st></strong></span><span class="No-Break"><st c="1709">.</st></span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor222"/><st c="1710">Vector stores, LangChain, and RAG</st></h2>
			<p><strong class="bold"><st c="1744">Vector stores</st></strong><st c="1758"> play </st><a id="_idIndexMarker625"/><st c="1764">a crucial role in RAG systems by efficiently storing and </st><a id="_idIndexMarker626"/><st c="1821">indexing vector representations of the knowledge base documents. </st><st c="1886">LangChain</st><a id="_idIndexMarker627"/><st c="1895"> provides seamless </st><a id="_idIndexMarker628"/><st c="1914">integration with various vector store </st><a id="_idIndexMarker629"/><st c="1952">implementations, such as </st><strong class="bold"><st c="1977">Chroma</st></strong><st c="1983">, </st><strong class="bold"><st c="1985">Weaviate</st></strong><st c="1993">, </st><strong class="bold"><st c="1995">FAISS</st></strong><st c="2000"> (</st><strong class="bold"><st c="2002">Facebook AI Similarity Search</st></strong><st c="2031">), </st><strong class="bold"><st c="2035">pgvector</st></strong><st c="2043">, and </st><strong class="bold"><st c="2049">Pinecone</st></strong><st c="2057">. For this </st><a id="_idIndexMarker630"/><st c="2068">code lab, we will show the code for adding</st><a id="_idIndexMarker631"/><st c="2110"> your data to Chroma, Weaviate, and FAISS, laying the groundwork for you to be able to integrate any vector store among the many that LangChain offers. </st><st c="2262">These vector stores offer high-performance similarity search capabilities, enabling fast retrieval of relevant documents based on the </st><span class="No-Break"><st c="2396">query vector.</st></span></p>
			<p><st c="2409">LangChain’s vector store class serves as a unified interface for interacting with different vector store backends. </st><st c="2525">It provides methods for adding documents to the vector store, performing similarity searches, and retrieving the stored documents. </st><st c="2656">This abstraction allows developers to easily switch between vector store implementations without modifying the core </st><span class="No-Break"><st c="2772">retrieval logic.</st></span></p>
			<p><st c="2788">When building a RAG system with LangChain, you can leverage the vector store class to efficiently store and retrieve document vectors. </st><st c="2924">The choice of vector store depends on factors such as scalability, search performance, and deployment requirements. </st><st c="3040">Pinecone, for example, offers a fully managed vector database with high scalability and real-time search capabilities, making it suitable for production-grade RAG systems. </st><st c="3212">On the other hand, FAISS provides an open source library for efficient similarity search, which can be used for local development and experimentation. </st><st c="3363">Chroma is a popular place for developers to start when building their first RAG pipelines due to its ease of use and its effective integration </st><span class="No-Break"><st c="3506">with LangChain.</st></span></p>
			<p><st c="3521">If you look at the code we discussed in previous chapters, we are already using Chroma. </st><st c="3610">Here is a snippet of that code showing our use of Chroma, which you can find in the code for this code lab </st><span class="No-Break"><st c="3717">as well:</st></span></p>
			<pre class="source-code"><st c="3725">
chroma_client = chromadb.Client()
collection_name = "google_environmental_report"
vectorstore = Chroma.from_documents(
               documents=dense_documents,
               embedding=embedding_function,
               collection_name=collection_name,
               client=chroma_client
)</st></pre>
			<p><st c="3957">LangChain calls this an </st><strong class="bold"><st c="3982">integration</st></strong><st c="3993"> since it is integrating with a third party called Chroma. </st><st c="4052">There are many other integrations available </st><span class="No-Break"><st c="4096">with LangChain.</st></span></p>
			<p><st c="4111">On the LangChain</st><a id="_idIndexMarker632"/><st c="4128"> website, as it currently stands, there is an </st><strong class="bold"><st c="4174">Integrations</st></strong><st c="4186"> link in </st><a id="_idIndexMarker633"/><st c="4195">the main website navigation at the top of the website page. </st><st c="4255">If you click on that, you will see a menu down the left side that stretches out pretty far and has the main categories of </st><strong class="bold"><st c="4377">Providers</st></strong><st c="4386"> and </st><strong class="bold"><st c="4391">Components</st></strong><st c="4401">. As you might have guessed, this gives you the ability to view all the integrations by either providers or components. </st><st c="4521">If you click on </st><strong class="bold"><st c="4537">Providers</st></strong><st c="4546">, you will first see </st><strong class="bold"><st c="4567">Partner Packages</st></strong><st c="4583"> and </st><strong class="bold"><st c="4588">Featured Community Providers</st></strong><st c="4616">. Chroma is not currently in either of these lists, but if you want to find out more about Chroma as a provider, click on the link near the end of the page that says </st><strong class="bold"><st c="4782">Click here to see all providers</st></strong><st c="4813">. The list is in alphabetical order. </st><st c="4850">Scroll down to the Cs and find Chroma. </st><st c="4889">This will show you useful LangChain documentation related to Chroma, particularly in creating both the vector store and </st><span class="No-Break"><st c="5009">the retriever.</st></span></p>
			<p><st c="5023">Another useful approach is to click on </st><strong class="bold"><st c="5063">Vector stores</st></strong><st c="5076"> under </st><strong class="bold"><st c="5083">Components</st></strong><st c="5093">. There are currently 49 vector store options! </st><st c="5140">The current link is here for version 0.2.0, but keep an eye out for future versions </st><span class="No-Break"><st c="5224">as well:</st></span></p>
			<p><a href="https://python.langchain.com/v0.2/docs/integrations/vectorstores/ "><span class="No-Break"><st c="5232">https://python.langchain.com/v0.2/docs/integrations/vectorstores/</st></span></a></p>
			<p><st c="5298">Another area we highly recommend you review is the LangChain vector store </st><span class="No-Break"><st c="5373">documentation here:</st></span></p>
			<p><a href="https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.vectorstores "><span class="No-Break"><st c="5392">https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.vectorstores</st></span></a></p>
			<p><st c="5494">We have already talked about our current vector store and Chroma in general in-depth in past chapters, but let’s review Chroma and discuss where it is </st><span class="No-Break"><st c="5646">most useful.</st></span></p>
			<h3><st c="5658">Chroma</st></h3>
			<p><strong class="bold"><st c="5665">Chroma</st></strong><st c="5672"> is an open</st><a id="_idIndexMarker634"/><st c="5683"> source AI-native vector database designed for developer productivity and</st><a id="_idIndexMarker635"/><st c="5756"> ease of use. </st><st c="5770">It offers fast search performance and seamless integration with LangChain through its Python SDK. </st><st c="5868">Chroma supports various deployment modes, including in-memory, persistent storage, and containerized deployment </st><span class="No-Break"><st c="5980">using Docker.</st></span></p>
			<p><st c="5993">One of the key advantages of Chroma is its simplicity and developer-friendly API. </st><st c="6076">It provides straightforward methods for adding, updating, deleting, and querying documents in the vector store. </st><st c="6188">Chroma also supports dynamic filtering of collections based on metadata, allowing for more targeted searches. </st><st c="6298">Additionally, Chroma offers built-in functionality for document chunking and indexing, making it convenient to work with large </st><span class="No-Break"><st c="6425">text datasets.</st></span></p>
			<p><st c="6439">When considering Chroma as a vector store for a RAG application, it’s important to evaluate its architecture and selection criteria. </st><st c="6573">Chroma’s architecture consists of an indexing layer for fast vector retrieval, a storage layer for efficient data management, and a processing layer for real-time operations. </st><st c="6748">Chroma integrates smoothly with LangChain, allowing developers to leverage its capabilities within the LangChain ecosystem. </st><st c="6872">The Chroma client can be easily instantiated and passed to LangChain, enabling efficient storage and retrieval of document vectors. </st><st c="7004">Chroma also supports advanced retrieval options, such </st><a id="_idIndexMarker636"/><st c="7058">as </st><strong class="bold"><st c="7061">maximum marginal relevance</st></strong><st c="7087"> (</st><strong class="bold"><st c="7089">MMR</st></strong><st c="7092">) and metadata filtering to refine </st><span class="No-Break"><st c="7128">search results.</st></span></p>
			<p><st c="7143">Overall, Chroma is</st><a id="_idIndexMarker637"/><st c="7162"> a solid choice</st><a id="_idIndexMarker638"/><st c="7177"> for developers seeking an open source, easy-to-use vector database that integrates well with LangChain. </st><st c="7282">Its simplicity, fast search performance, and built-in document processing features make it an attractive option for building RAG applications. </st><st c="7425">In fact, these are some of the reasons why we chose to feature Chroma in this book in several of the chapters. </st><st c="7536">However, it’s important to assess your specific requirements and compare Chroma with other vector store alternatives to determine the best fit for your project. </st><st c="7697">Let’s look at the code and discuss some of the other options available, starting </st><span class="No-Break"><st c="7778">with FAISS.</st></span></p>
			<h3><st c="7789">FAISS</st></h3>
			<p><st c="7795">Let’s start with how our code </st><a id="_idIndexMarker639"/><st c="7826">would </st><a id="_idIndexMarker640"/><st c="7832">change if we wanted to use FAISS as our vector store. </st><st c="7886">You would first need to </st><span class="No-Break"><st c="7910">install FAISS:</st></span></p>
			<pre class="source-code"><st c="7924">
%pip install faiss-cpu</st></pre>
			<p><st c="7947">After you have restarted the kernel (since you installed a new package), run all the code down to the vector store-related cell, and replace the Chroma-related code with the FAISS vector </st><span class="No-Break"><st c="8135">store instantiation:</st></span></p>
			<pre class="source-code"><st c="8155">
from langchain_community.vectorstores import FAISS
vectorstore = FAISS.from_documents(
               documents=dense_documents,
               embedding=embedding_function
)</st></pre>
			<p><st c="8300">The </st><strong class="source-inline"><st c="8305">Chroma.from_documents()</st></strong><st c="8328"> method call has been replaced with </st><strong class="source-inline"><st c="8364">FAISS.from_documents()</st></strong><st c="8386">. The </st><strong class="source-inline"><st c="8392">collection_name</st></strong><st c="8407"> and </st><strong class="source-inline"><st c="8412">client</st></strong><st c="8418"> parameters are not applicable to FAISS, so they have been removed from the method call. </st><st c="8507">We repeat some of the code that we saw with the Chroma vector store, such as the document generation, which allows us to show the exact equivalent in code between the two vector store options. </st><st c="8700">With these changes, the code can now use FAISS as the vector store instead </st><span class="No-Break"><st c="8775">of Chroma.</st></span></p>
			<p><strong class="bold"><st c="8785">FAISS</st></strong><st c="8791"> is an </st><a id="_idIndexMarker641"/><st c="8798">open source library developed by Facebook AI. </st><st c="8844">FAISS offers high-performance search capabilities and can handle large datasets that may not fit entirely in memory. </st><st c="8961">Much like other vector stores mentioned here, the architecture of FAISS consists of an indexing layer that organizes vectors for fast retrieval, a storage layer for efficient data management, and an optional processing layer for real-time operations. </st><st c="9212">FAISS provides various indexing techniques, such as clustering and quantization, to optimize search performance and memory usage. </st><st c="9342">It also supports GPU acceleration for even faster </st><span class="No-Break"><st c="9392">similarity search.</st></span></p>
			<p><st c="9410">If you have GPUs available, you </st><a id="_idIndexMarker642"/><st c="9443">can install this package instead of the one we </st><span class="No-Break"><st c="9490">installed previously:</st></span></p>
			<pre class="source-code"><st c="9511">
%pip install faiss-gpu</st></pre>
			<p><st c="9534">Using the GPU version of FAISS can significantly speed up the similarity search process, especially for large-scale datasets. </st><st c="9661">GPUs can handle a large number of vector comparisons in parallel, enabling faster retrieval of relevant documents in a RAG application. </st><st c="9797">If you are working in an environment that deals with massive amounts of data and requires a substantial performance boost compared to what we have already been working with (Chroma), you should definitely test FAISS GPU and see the impact it can have </st><span class="No-Break"><st c="10048">for you.</st></span></p>
			<p><st c="10056">The FAISS LangChain documentation provides detailed examples and guides on how to use FAISS within the LangChain framework. </st><st c="10181">It covers topics such as ingesting documents, querying the vector store, saving and loading indexes, and performing advanced operations, such as filtering and merging. </st><st c="10349">The documentation also highlights FAISS-specific features, such as similarity search with scores and serialization/deserialization </st><span class="No-Break"><st c="10480">of indexes.</st></span></p>
			<p><st c="10491">Overall, FAISS is </st><a id="_idIndexMarker643"/><st c="10510">a powerful and efficient vector store option for building RAG applications with LangChain. </st><st c="10601">Its high-performance search capabilities, scalability, and seamless integration with LangChain make it a compelling choice for developers seeking a robust and customizable solution for storing and retrieving </st><span class="No-Break"><st c="10809">document </st></span><span class="No-Break"><a id="_idIndexMarker644"/></span><span class="No-Break"><st c="10818">vectors.</st></span></p>
			<p><st c="10826">Those are two powerful options for your vector store needs. </st><st c="10887">Next, we will show and discuss the Weaviate vector </st><span class="No-Break"><st c="10938">store option.</st></span></p>
			<h3><st c="10951">Weaviate</st></h3>
			<p><st c="10960">There are multiple options</st><a id="_idIndexMarker645"/><st c="10987"> for how you want to use and access Weaviate. </st><st c="11033">We are going to show the</st><a id="_idIndexMarker646"/><st c="11057"> embedding version, which runs a Weaviate instance from your application code rather than from a standalone Weaviate </st><span class="No-Break"><st c="11174">server installation.</st></span></p>
			<p><st c="11194">When Embedded Weaviate starts for the first time, it creates a permanent datastore in the location set in </st><strong class="source-inline"><st c="11301">persistence_data_path</st></strong><st c="11322">. When your client exits, the Embedded Weaviate instance also exits, but the data persists. </st><st c="11414">The next time the client runs, it starts a new instance of Embedded Weaviate. </st><st c="11492">New Embedded Weaviate instances use the data that is saved in </st><span class="No-Break"><st c="11554">the datastore.</st></span></p>
			<p><st c="11568">If you are familiar with </st><strong class="bold"><st c="11594">GraphQL</st></strong><st c="11601">, you may recognize the influence it has had on Weaviate when you start looking at the code. </st><st c="11694">The query language and API are inspired by GraphQL, but </st><a id="_idIndexMarker647"/><st c="11750">Weaviate does not use GraphQL directly. </st><st c="11790">Weaviate uses a RESTful API with a query language that resembles GraphQL in terms of its structure and functionality. </st><st c="11908">Weaviate uses predefined data types for properties in the schema definition, similar to GraphQL’s scalar types. </st><st c="12020">The available data types in Weaviate include string, int, number, Boolean, date, </st><span class="No-Break"><st c="12101">and more.</st></span></p>
			<p><st c="12110">One strength of Weaviate is its support of batch operations for creating, updating, or deleting multiple data objects in a single request. </st><st c="12250">This is similar to GraphQL’s mutation operations, where you can perform multiple changes in a single request. </st><st c="12360">Weaviate uses the </st><strong class="source-inline"><st c="12378">client.batch</st></strong><st c="12390"> context manager to group multiple operations into a batch, which we will demonstrate in </st><span class="No-Break"><st c="12479">a moment.</st></span></p>
			<p><st c="12488">Let’s start with how our code would change if we wanted to use Weaviate as our vector store. </st><st c="12582">You will first need to </st><span class="No-Break"><st c="12605">install FAISS:</st></span></p>
			<pre class="source-code"><st c="12619">
%pip install weaviate-client
%pip install langchain-weaviate</st></pre>
			<p><st c="12680">After you have restarted the kernel (since you installed a new package), you run all the code down to the vector store-related cell, and update the code with the FAISS vector </st><span class="No-Break"><st c="12856">store instantiation:</st></span></p>
			<pre class="source-code"><st c="12876">
import weaviate
from langchain_weaviate.vectorstores import WeaviateVectorStore
from weaviate.embedded import EmbeddedOptions
from langchain.vectorstores import Weaviate
from tqdm import tqdm</st></pre>
			<p><st c="13068">As you can see, there are many additional packages to import for Weaviate. </st><st c="13144">We also install </st><strong class="source-inline"><st c="13160">tqdm</st></strong><st c="13164">, which is not specific to </st><a id="_idIndexMarker648"/><st c="13191">Weaviate, but it is required, as Weaviate uses </st><strong class="source-inline"><st c="13238">tqdm</st></strong><st c="13242"> to show progress bars when </st><span class="No-Break"><st c="13270">it loads.</st></span></p>
			<p><st c="13279">We must first declare </st><strong class="source-inline"><st c="13302">weaviate_client</st></strong><st c="13317"> as the </st><span class="No-Break"><st c="13325">Weaviate client:</st></span></p>
			<pre class="source-code"><st c="13341">
weaviate_client = weaviate.Client(
    embedded_options=EmbeddedOptions())</st></pre>
			<p><st c="13412">The differences between our original</st><a id="_idIndexMarker649"/><st c="13449"> Chroma vector store code and using Weaviate are more complicated than other approaches we have taken so far. </st><st c="13559">With Weaviate, we initialized with the </st><strong class="source-inline"><st c="13598">WeaviateClient</st></strong><st c="13612"> client and the embedding options to enable embedded mode, as you </st><span class="No-Break"><st c="13678">saw previously.</st></span></p>
			<p><st c="13693">Before we proceed, we need to make sure there is not already an instance of the Weaviate client in place, or our code </st><span class="No-Break"><st c="13812">will fail:</st></span></p>
			<pre class="source-code"><st c="13822">
try:
     weaviate_client.schema.delete_class(collection_name)
except:
        pass</st></pre>
			<p><st c="13893">For </st><a id="_idIndexMarker650"/><st c="13898">Weaviate, you have to</st><a id="_idIndexMarker651"/><st c="13919"> make sure you clear out any lingering schemas from past iterations since they can persist in </st><span class="No-Break"><st c="14013">the background.</st></span></p>
			<p><st c="14028">We then use the </st><strong class="source-inline"><st c="14045">weaviate</st></strong><st c="14053"> client to establish our database using a GraphQL-like </st><span class="No-Break"><st c="14108">definition schema:</st></span></p>
			<pre class="source-code"><st c="14126">
weaviate_client.schema.create_class({
               "class": collection_name,
               "description": "Google Environmental
                               Report",
               "properties": [
                             {
                                  "name": "text",
                                  "dataType": ["text"],
                                  "description": "Text
                                   content of the document"
                             },
                             {
                                  "name": "doc_id",
                                  "dataType": ["string"],
                                  "description": "Document
                                       ID"
                             },
                             {
                                  "name": "source",
                                  "dataType": ["string"],
                                  "description": "Document
                                       source"
                             }
               ]
})</st></pre>
			<p><st c="14501">This provides a full schema class that you will later pass into the vector store definition as part of the </st><strong class="source-inline"><st c="14609">weviate_client</st></strong><st c="14623"> object. </st><st c="14632">You need to define this schema for your collection using the </st><strong class="source-inline"><st c="14693">client.collections.create()</st></strong><st c="14720"> method. </st><st c="14729">The schema definition includes specifying the class name, properties, and their data types. </st><st c="14821">Properties can have different data types, such as string, integer, and Boolean. </st><st c="14901">As you can see, Weaviate</st><a id="_idIndexMarker652"/><st c="14925"> enforces a stricter schema validation compared to what we’ve used in previous labs </st><span class="No-Break"><st c="15009">with </st></span><span class="No-Break"><a id="_idIndexMarker653"/></span><span class="No-Break"><st c="15014">Chroma.</st></span></p>
			<p><st c="15021">While this GraphQL-like schema adds some complexity to establishing your vector store, it also gives you more control of your database in helpful and powerful ways. </st><st c="15187">In particular, you have more granular control over how to define </st><span class="No-Break"><st c="15252">your schema.</st></span></p>
			<p><st c="15264">You may recognize the next code, as it looks a lot like the </st><strong class="source-inline"><st c="15325">dense_documents</st></strong><st c="15340"> and </st><strong class="source-inline"><st c="15345">sparse_documents</st></strong><st c="15361"> variables we have defined in the past, but if you look closely, there is a slight difference that is important </st><span class="No-Break"><st c="15473">to Weaviate:</st></span></p>
			<pre class="source-code"><st c="15485">
dense_documents = [Document(page_content=text,
metadata={"doc_id": str(i), "source": "dense"}) for i,
          text in enumerate(splits)]
sparse_documents = [Document(page_content=text, metadata={"doc_id": str(i), "source": "sparse"}) for i,
          text in enumerate(splits)]</st></pre>
			<p><st c="15745">There is a slight change to these definitions for Weaviate when we pre-process the documents with the metadata. </st><st c="15858">We use </st><strong class="source-inline"><st c="15865">'doc_id'</st></strong><st c="15873"> rather than </st><strong class="source-inline"><st c="15886">'id'</st></strong><st c="15890"> for Weaviate. </st><st c="15905">This is because </st><strong class="source-inline"><st c="15921">'id'</st></strong><st c="15925"> is used internally and is not available for our use. </st><st c="15979">Later in the code, when you extract the ID from the metadata results, you will want to update that code to use </st><strong class="source-inline"><st c="16090">'doc_id'</st></strong> <span class="No-Break"><st c="16098">as well.</st></span></p>
			<p><st c="16107">Next, we define our vector store, similar to what we have done in the past with Chroma and FAISS, but with </st><span class="No-Break"><st c="16215">Weaviate-specific parameters:</st></span></p>
			<pre class="source-code"><st c="16244">
vectorstore = Weaviate(
               client=weaviate_client,
               embedding=embedding_function,
               index_name=collection_name,
               text_key="text",
               attributes=["doc_id", "source"],
               by_text=False
)</st></pre>
			<p><st c="16416">For the vector store initialization, Chroma uses the </st><strong class="source-inline"><st c="16470">from_documents</st></strong><st c="16484"> method to create the vector store directly from the documents, whereas, for Weaviate, we create the vector store and then add the documents after. </st><st c="16632">Weaviate also requires additional configuration, such as </st><strong class="source-inline"><st c="16689">text_key</st></strong><st c="16697">, </st><strong class="source-inline"><st c="16699">attributes</st></strong><st c="16709">, and </st><strong class="source-inline"><st c="16715">by_text</st></strong><st c="16722">. One major difference is Weaviate’s use of </st><span class="No-Break"><st c="16766">a schema.</st></span></p>
			<p><st c="16775">Lastly, we load up the</st><a id="_idIndexMarker654"/><st c="16798"> Weaviate vector store instance with our actual content, which also applies </st><a id="_idIndexMarker655"/><st c="16874">the embedding function in </st><span class="No-Break"><st c="16900">the process:</st></span></p>
			<pre class="source-code"><st c="16912">
weaviate_client.batch.configure(batch_size=100)
with weaviate_client.batch as batch:
    for doc in tqdm(dense_documents, desc="Processing
        documents"):
                    properties = {
                                  "text": doc.page_content,
                                  "doc_id":doc.metadata[
                                               "doc_id"],
                                  "source": doc.metadata[
                                                "source"]
                             }
                    vector=embedding_function.embed_query(
                               doc.page_content)
                           batch.add_data_object(
                               data_object=properties,
                               class_name=collection_name,
                               vector=vector
                           )</st></pre>
			<p><st c="17319">In summary, Chroma offers a simpler and more flexible approach to data schema definition and focuses on embedding storage and retrieval. </st><st c="17457">It can be easily embedded into your application. </st><st c="17506">On the other hand, Weaviate</st><a id="_idIndexMarker656"/><st c="17533"> provides a more structured and feature-rich vector database solution with explicit schema definition, multiple storage backends, and built-in support for various embedding models. </st><st c="17714">It can be deployed as a standalone server or hosted in the cloud. </st><st c="17780">The choice between Chroma, Weaviate, or any of the other vector stores depends on your specific requirements, such as the level of schema flexibility, deployment preferences, and the need for additional features beyond </st><span class="No-Break"><st c="17999">embedding </st></span><span class="No-Break"><a id="_idIndexMarker657"/></span><span class="No-Break"><st c="18009">storage.</st></span></p>
			<p><st c="18017">Note that you can use any one of these vector stores and the remaining code will work with the data loaded to them. </st><st c="18134">This is a strength of using LangChain, which allows you to swap components in and out. </st><st c="18221">This is particularly necessary in the world of generative AI, where new and dramatically improved technologies are launching all the time. </st><st c="18360">Using this approach, if you come across a newer and better vector store technology that makes a difference in your RAG pipeline, you can make this change relatively quickly and easily. </st><st c="18545">Let’s talk next about another key component in the LangChain arsenal that is at the center of a RAG application: </st><span class="No-Break"><st c="18658">the retriever.</st></span></p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor223"/><st c="18672">Code lab 10.2 – LangChain Retrievers</st></h1>
			<p><st c="18709">In this code lab, we will cover a few examples of the most important component in the retrieval process: the </st><strong class="bold"><st c="18819">LangChain retriever</st></strong><st c="18838">. Like the</st><a id="_idIndexMarker658"/><st c="18848"> LangChain vector store, there are too </st><a id="_idIndexMarker659"/><st c="18887">many options for LangChain retrievers to list here. </st><st c="18939">We will focus on a few popular choices that are particularly applicable to RAG applications, and we encourage you to look at all the others to see if there are better options for your specific situation. </st><st c="19143">Just like we discussed with the vector stores, there is ample documentation on the LangChain website that will help you find your best </st><a id="_idIndexMarker660"/><span class="No-Break"><st c="19278">solution: </st></span><a href="https://python.langchain.com/v0.2/docs/integrations/retrievers/ "><span class="No-Break"><st c="19288">https://python.langchain.com/v0.2/docs/integrations/retrievers/</st></span></a></p>
			<p><st c="19351">The documentation for the retriever package</st><a id="_idIndexMarker661"/><st c="19395"> can be found </st><span class="No-Break"><st c="19409">here: </st></span><a href="https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.retrievers "><span class="No-Break"><st c="19415">https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.retrievers</st></span></a></p>
			<p><st c="19514">Now, let’s get started with coding </st><span class="No-Break"><st c="19550">for retrievers!</st></span></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor224"/><st c="19565">Retrievers, LangChain, and RAG</st></h2>
			<p><strong class="bold"><st c="19596">Retrievers</st></strong><st c="19607"> are</st><a id="_idIndexMarker662"/><st c="19611"> responsible for querying the vector store and retrieving the most relevant documents based on the input query. </st><st c="19723">LangChain offers a range of retriever implementations that can be used in conjunction with different vector stores and </st><span class="No-Break"><st c="19842">query</st></span><span class="No-Break"><a id="_idIndexMarker663"/></span><span class="No-Break"><st c="19847"> encoders.</st></span></p>
			<p><st c="19857">In our code so far, we have already seen three versions of the retriever; let’s review them first, as they relate to the original Chroma-based </st><span class="No-Break"><st c="20001">vector store.</st></span></p>
			<h3><st c="20014">Basic retriever (dense embeddings)</st></h3>
			<p><st c="20049">We start with </st><a id="_idIndexMarker664"/><st c="20064">the </st><strong class="bold"><st c="20068">dense retriever</st></strong><st c="20083">. This is the code we have used in several of our code labs up to </st><span class="No-Break"><st c="20149">this</st></span><span class="No-Break"><a id="_idIndexMarker665"/></span><span class="No-Break"><st c="20153"> point:</st></span></p>
			<pre class="source-code"><st c="20160">
dense_retriever = vectorstore.as_retriever(
                      search_kwargs={"k": 10})</st></pre>
			<p><st c="20229">The dense retriever is created using the </st><strong class="source-inline"><st c="20271">vectorstore.as_retriever</st></strong><st c="20295"> function, specifying the number of top results to retrieve (</st><strong class="source-inline"><st c="20356">k=10</st></strong><st c="20361">). </st><st c="20365">Under the hood of this retriever, Chroma uses dense vector representations of the documents and performs a similarity search using cosine distance or Euclidean distance to retrieve the most relevant documents based on the </st><span class="No-Break"><st c="20587">query embedding.</st></span></p>
			<p><st c="20603">This is using the simplest type of retriever, the vector store retriever, which simply creates embeddings for each piece of text and uses those embeddings for retrieval. </st><st c="20774">The retriever is essentially a wrapper around the vector store. </st><st c="20838">Using this approach gives you access to the built-in retrieval/search functionality of the vector store, but in a way that integrates and interfaces in the LangChain ecosystem. </st><st c="21015">It is a lightweight wrapper around the vector store class that gives you a consistent interface for all of the retriever options in LangChain. </st><st c="21158">Because of this, once you construct a vector store, it’s very easy to construct a retriever. </st><st c="21251">If you need to</st><a id="_idIndexMarker666"/><st c="21265"> change your vector store or retriever, that is also very easy to do </st><span class="No-Break"><st c="21334">as </st></span><span class="No-Break"><a id="_idIndexMarker667"/></span><span class="No-Break"><st c="21337">well.</st></span></p>
			<p><st c="21342">There are two primary search capabilities that come from these types of retrievers, stemming directly from the vector stores that it wraps: similarity search </st><span class="No-Break"><st c="21501">and MMR.</st></span></p>
			<h3><st c="21509">Similarity score threshold retrieval</st></h3>
			<p><st c="21546">By default, retrievers use similarity</st><a id="_idIndexMarker668"/><st c="21584"> search. </st><st c="21593">If you want to set a threshold of similarity </st><a id="_idIndexMarker669"/><st c="21638">though, you simply need to set the search type to </st><strong class="source-inline"><st c="21688">similarity_score_threshold</st></strong><st c="21714"> and set that similarity score threshold within the </st><strong class="source-inline"><st c="21766">kwargs</st></strong><st c="21772"> function that you pass to the retriever object. </st><st c="21821">The code looks </st><span class="No-Break"><st c="21836">like this:</st></span></p>
			<pre class="source-code"><st c="21846">
dense_retriever = vectorstore.as_retriever(
               search_type="similarity_score_threshold",
               search_kwargs={"score_threshold": 0.5}
)</st></pre>
			<p><st c="21973">This is a useful upgrade to the default similarity search that can be useful in many RAG applications. </st><st c="22077">However, similarity search is not the only type of search these retrievers can support; there is </st><span class="No-Break"><st c="22174">also MMR.</st></span></p>
			<h3><st c="22183">MMR</st></h3>
			<p><strong class="bold"><st c="22187">MMR</st></strong><st c="22191"> is a technique used</st><a id="_idIndexMarker670"/><st c="22211"> to retrieve relevant items from a query while avoiding redundancy. </st><st c="22279">It balances </st><a id="_idIndexMarker671"/><st c="22291">relevancy and diversity in the items retrieved, as opposed to simply retrieving the most relevant items, which can be similar. </st><st c="22418">MMR is often used in information retrieval and can be used to summarize documents by calculating the similarity between parts of the text. </st><st c="22557">To set up your retriever to use this type of search, rather than a similarity search, you can add </st><strong class="source-inline"><st c="22655">search_type="mmr"</st></strong><st c="22672"> as a parameter when you define the retriever, </st><span class="No-Break"><st c="22719">like this:</st></span></p>
			<pre class="source-code"><st c="22729">
dense_retriever = vectorstore.as_retriever(
                      search_type="mmr"
)</st></pre>
			<p><st c="22793">Adding this to any vector store-based retriever will cause it to utilize an MMR type </st><span class="No-Break"><st c="22879">of search.</st></span></p>
			<p><st c="22889">Similarity search and</st><a id="_idIndexMarker672"/><st c="22911"> MMR can be supported by any vector stores that also </st><a id="_idIndexMarker673"/><st c="22964">support those search techniques. </st><st c="22997">Let’s next talk about the sparse search mechanism we introduced in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="23064">Chapter 8</st></em></span></a><st c="23073">, the </st><span class="No-Break"><st c="23079">BM25 retriever.</st></span></p>
			<h3><st c="23094">BM25 retriever</st></h3>
			<p><strong class="bold"><st c="23109">BM25</st></strong><st c="23114"> is a ranking</st><a id="_idIndexMarker674"/><st c="23127"> function used for sparse text retrieval, and </st><strong class="source-inline"><st c="23173">BM25Retriever</st></strong><st c="23186"> is the </st><a id="_idIndexMarker675"/><st c="23194">LangChain representation of BM25 that can be used for sparse text </st><span class="No-Break"><st c="23260">retrieval purposes.</st></span></p>
			<p><st c="23279">You have seen this retriever as well, as we used it to turn our basic search into a hybrid search in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="23381">Chapter 8</st></em></span></a><st c="23390">. We see this in our code with </st><span class="No-Break"><st c="23421">these settings:</st></span></p>
			<pre class="source-code"><st c="23436">
sparse_retriever = BM25Retriever.from_documents(
    sparse_documents, k=10)</st></pre>
			<p><st c="23509">The </st><strong class="source-inline"><st c="23514">BM25Retriever.from_documents()</st></strong><st c="23544"> method is called to create a sparse retriever from the sparse documents, specifying the number of top results to </st><span class="No-Break"><st c="23658">retrieve (</st></span><span class="No-Break"><strong class="source-inline"><st c="23668">k=10</st></strong></span><span class="No-Break"><st c="23673">).</st></span></p>
			<p><st c="23676">BM25 works by calculating a relevance score for each document based on the query terms and the document’s </st><strong class="bold"><st c="23783">term frequencies and inverse document frequencies</st></strong><st c="23832"> (</st><strong class="bold"><st c="23834">TF-IDF</st></strong><st c="23840">). </st><st c="23844">It uses a </st><a id="_idIndexMarker676"/><st c="23854">probabilistic model to estimate the relevance of documents to a given query. </st><st c="23931">The retriever returns the top-k documents with the highest </st><span class="No-Break"><st c="23990">BM25 scores.</st></span></p>
			<h3><st c="24002">Ensemble retriever</st></h3>
			<p><st c="24021">An </st><strong class="bold"><st c="24025">ensemble retriever</st></strong><st c="24043"> combines</st><a id="_idIndexMarker677"/><st c="24052"> multiple retrieval methods and uses an additional </st><a id="_idIndexMarker678"/><st c="24103">algorithm to combine their results into one set. </st><st c="24152">An ideal use of this type of retriever is when you want to combine dense and sparse retrievers to support a hybrid retriever approach like what we created in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="24310">Chapter 8</st></em></span></a><st c="24319">’s </st><em class="italic"><st c="24323">Code </st></em><span class="No-Break"><em class="italic"><st c="24328">lab 8.3</st></em></span><span class="No-Break"><st c="24335">:</st></span></p>
			<pre class="source-code"><st c="24337">
ensemble_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, sparse_retriever],
    weights=[0.5, 0.5], c=0, k=10)</st></pre>
			<p><st c="24456">In our case, the ensemble retriever combines the Chroma dense retriever and the BM25 sparse retriever to achieve better retrieval performance. </st><st c="24600">It is created using the </st><strong class="source-inline"><st c="24624">EnsembleRetriever</st></strong><st c="24641"> class, which takes the list of retrievers and their corresponding weights. </st><st c="24717">In this case, the dense retriever and sparse retriever are passed with equal weights of </st><span class="No-Break"><strong class="source-inline"><st c="24805">0.5</st></strong></span><span class="No-Break"><st c="24808"> each.</st></span></p>
			<p><st c="24814">The </st><strong class="source-inline"><st c="24819">c</st></strong><st c="24820"> parameter in the ensemble retriever is a reranking parameter that controls the balance between the original retrieval scores and the reranking scores. </st><st c="24972">It is used to adjust the influence of the reranking step on the final retrieval results. </st><st c="25061">In this case, the </st><strong class="source-inline"><st c="25079">c</st></strong><st c="25080"> parameter is set to </st><strong class="source-inline"><st c="25101">0</st></strong><st c="25102">, which means no reranking is performed. </st><st c="25143">When </st><strong class="source-inline"><st c="25148">c</st></strong><st c="25149"> is set to a non-zero value, the ensemble retriever performs an additional reranking step on the retrieved documents. </st><st c="25267">The reranking step rescores the retrieved documents based on a separate reranking model or function. </st><st c="25368">The reranking model can take into account additional features or criteria to assess the relevance of the documents to </st><span class="No-Break"><st c="25486">the query.</st></span></p>
			<p><st c="25496">In RAG applications, the quality and relevance of the retrieved documents directly impact the generated output. </st><st c="25609">By utilizing the </st><strong class="source-inline"><st c="25626">c</st></strong><st c="25627"> parameter and a suitable reranking model, you can enhance the retrieval results to better suit the specific requirements of your RAG application. </st><st c="25774">For example, you can design a reranking model that takes into account factors such as document relevance, coherence with the query, or domain-specific criteria. </st><st c="25935">By setting an appropriate value for </st><strong class="source-inline"><st c="25971">c</st></strong><st c="25972">, you can strike a balance between the original retrieval scores and the reranking scores, giving more weight to the reranking model when needed. </st><st c="26118">This can help prioritize documents that are more relevant and informative for the RAG task, leading to improved </st><span class="No-Break"><st c="26230">generated outputs.</st></span></p>
			<p><st c="26248">When a query is passed to the ensemble retriever, it sends the query to both the dense and sparse retrievers. </st><st c="26359">The ensemble retriever then combines the results from both retrievers based on their assigned weights and returns the top-k documents. </st><st c="26494">Under the hood, the ensemble retriever leverages the strengths of both dense and sparse retrieval methods. </st><st c="26601">Dense retrieval captures semantic similarity using dense vector representations, while sparse retrieval relies on keyword matching and term frequencies. </st><st c="26754">By combining their results, the ensemble retriever aims to provide more accurate and comprehensive </st><span class="No-Break"><st c="26853">search results.</st></span></p>
			<p><st c="26868">The specific classes and methods used in the code snippet may vary depending on the library or framework being used. </st><st c="26986">However, the general concepts of dense retrieval using vector similarity search, sparse</st><a id="_idIndexMarker679"/><st c="27073"> retrieval using BM25, and ensemble retrieval combining multiple retrievers </st><a id="_idIndexMarker680"/><st c="27149">remain </st><span class="No-Break"><st c="27156">the same.</st></span></p>
			<p><st c="27165">That covers the retrievers we have already seen in previous code, all drawn from the data we accessed and processed during the indexing stage. </st><st c="27309">There are numerous other retriever types that work with data you extract from your documents that you can explore on the LangChain website to suit your needs. </st><st c="27468">However, not all retrievers are designed to pull from documents you are processing. </st><st c="27552">Next, we will review an example of a retriever built off a public data </st><span class="No-Break"><st c="27623">source, Wikipedia.</st></span></p>
			<h3><st c="27641">Wikipedia retriever</st></h3>
			<p><st c="27661">As described by the creators of the</st><a id="_idIndexMarker681"/><st c="27697"> Wikipedia retriever</st><a id="_idIndexMarker682"/><st c="27717"> on the LangChain</st><a id="_idIndexMarker683"/> <span class="No-Break"><st c="27734">website (</st></span><a href="https://www.langchain.com/"><span class="No-Break"><st c="27744">https://www.langchain.com/</st></span></a><span class="No-Break"><st c="27771">):</st></span></p>
			<p class="author-quote"><st c="27774">Wikipedia is the largest and most-read reference work in history, acting as a multilingual free online encyclopedia written and maintained by a community of volunteers.</st></p>
			<p><st c="27943">That sounds like a great resource to tap for useful knowledge in your RAG applications! </st><st c="28032">We will add a new cell after our existing retriever cell where we will use this Wikipedia retriever to retrieve wiki pages from </st><strong class="source-inline"><st c="28160">wikipedia.org</st></strong><st c="28173"> into the </st><strong class="source-inline"><st c="28183">Document</st></strong><st c="28191"> format that is </st><span class="No-Break"><st c="28207">used downstream.</st></span></p>
			<p><st c="28223">We first need to install a couple of </st><span class="No-Break"><st c="28261">new packages:</st></span></p>
			<pre class="source-code"><st c="28274">
%pip install langchain_core
%pip install --upgrade --quiet        wikipedia</st></pre>
			<p><st c="28343">As always, when you install new packages, don’t forget to restart </st><span class="No-Break"><st c="28410">your kernel!</st></span></p>
			<p><st c="28422">With</st><a id="_idIndexMarker684"/><st c="28427"> the </st><strong class="source-inline"><st c="28432">WikipediaRetriever</st></strong><st c="28450"> retriever, we now have a mechanism</st><a id="_idIndexMarker685"/><st c="28485"> that can fetch data from Wikipedia as it relates to the user query we pass to it, similar to the other retrievers we have used, but using the whole of Wikipedia data </st><span class="No-Break"><st c="28652">behind it:</st></span></p>
			<pre class="source-code"><st c="28662">
from langchain_community.retrievers import WikipediaRetriever
retriever = WikipediaRetriever(load_max_docs=10)
docs = retriever.get_relevant_documents(query=
    "What defines the golden age of piracy in the
     Caribbean?")
metadata_title = docs[0].metadata['title']
metadata_summary = docs[0].metadata['summary']
metadata_source = docs[0].metadata['source']
page_content = docs[0].page_content
print(f"First document returned:\n")
print(f"Title: {metadata_title}\n")
print(f"Summary: {metadata_summary}\n")
print(f"Source: {metadata_source}\n")
print(f"Page content:\n\n{page_content}\n")</st></pre>
			<p><st c="29245">In this code, we import the </st><strong class="source-inline"><st c="29274">WikipediaRetriever</st></strong><st c="29292"> class from the </st><strong class="source-inline"><st c="29308">langchain_community.retrievers</st></strong><st c="29338"> module. </st><strong class="source-inline"><st c="29347">WikipediaRetriever</st></strong><st c="29365"> is a retriever class specifically designed to retrieve relevant documents from Wikipedia based on a given query. </st><st c="29479">We then instantiate an instance of this receiver using the </st><strong class="source-inline"><st c="29538">WikipediaRetriever</st></strong><st c="29556"> class and assign it to the variable retriever. </st><st c="29604">The </st><strong class="source-inline"><st c="29608">load_max_docs</st></strong><st c="29621"> parameter is set to </st><strong class="source-inline"><st c="29642">10</st></strong><st c="29644">, indicating that the retriever should load a maximum of 10 relevant documents. </st><st c="29724">The user query here is </st><strong class="source-inline"><st c="29747">What defines the golden age of piracy in the Caribbean?</st></strong><st c="29802">, and we can look at the response to see what Wikipedia articles are retrieved to help answer </st><span class="No-Break"><st c="29896">this question.</st></span></p>
			<p><st c="29910">We</st><a id="_idIndexMarker686"/><st c="29913"> call the </st><strong class="source-inline"><st c="29923">get_relevant_documents</st></strong><st c="29945"> method of the retriever</st><a id="_idIndexMarker687"/><st c="29969"> object, passing in a query string as an argument, and receive this as the first document in </st><span class="No-Break"><st c="30062">that response:</st></span></p>
			<pre class="source-code"><st c="30076">
First document returned:
Title: Golden Age of Piracy
Summary: The Golden Age of Piracy is a common designation for the period between the 1650s and the 1730s, when maritime piracy was a significant factor in the histories of the North Atlantic and Indian Oceans.
</st><st c="30340">Histories of piracy often subdivide the Golden Age of Piracy into three periods:
The buccaneering period (approximately 1650 to 1680)…</st></pre>
			<p><st c="30474">You can see the matching content at </st><span class="No-Break"><st c="30511">this link:</st></span></p>
			<p><a href="https://en.wikipedia.org/wiki/Golden_Age_of_Piracy "><span class="No-Break"><st c="30521">https://en.wikipedia.org/wiki/Golden_Age_of_Piracy</st></span></a></p>
			<p><st c="30572">This link was provided as the source by </st><span class="No-Break"><st c="30613">the retriever.</st></span></p>
			<p><st c="30627">In summary, this code demonstrates how to use the </st><strong class="source-inline"><st c="30678">WikipediaRetriever</st></strong><st c="30696"> class from the </st><strong class="source-inline"><st c="30712">langchain_community.retrievers</st></strong><st c="30742"> module to retrieve relevant documents from Wikipedia based on a given query. </st><st c="30820">It then extracts and prints specific metadata information (title, summary, source) and the content of the first </st><span class="No-Break"><st c="30932">retrieved document.</st></span></p>
			<p><strong class="source-inline"><st c="30951">WikipediaRetriever</st></strong><st c="30970"> internally handles the process of querying Wikipedia’s API or search functionality, retrieving the relevant documents, and returning them as a list of </st><strong class="source-inline"><st c="31122">Document</st></strong><st c="31130"> objects. </st><st c="31140">Each </st><strong class="source-inline"><st c="31145">Document</st></strong><st c="31153"> object contains metadata and the actual page content, which can be accessed and utilized as needed. </st><st c="31254">There are many other retrievers that can access public data sources similar to this but focused on specific domains. </st><st c="31371">For scientific research, there is </st><strong class="source-inline"><st c="31405">PubMedRetriever</st></strong><st c="31420">. For other fields of research, such as mathematics and computer science, there is </st><strong class="source-inline"><st c="31503">ArxivRetreiver</st></strong><st c="31517">, which accesses data from the open-access archive of more than 2 million scholarly articles about these subjects. </st><st c="31632">In the</st><a id="_idIndexMarker688"/><st c="31638"> finance world, there is a retriever called </st><strong class="source-inline"><st c="31682">KayAiRetriever</st></strong><st c="31696"> that can access </st><strong class="bold"><st c="31713">Securities and Exchange Commission</st></strong><st c="31747"> (</st><strong class="bold"><st c="31749">SEC</st></strong><st c="31752">) filings, which contain the financial statements that public companies are required to submit to the </st><span class="No-Break"><st c="31855">US SEC.</st></span></p>
			<p><st c="31862">For projects that deal with data that is not on a massive scale, we have one more retriever to highlight: the </st><span class="No-Break"><st c="31973">kNN retriever.</st></span></p>
			<h3><st c="31987">kNN retriever</st></h3>
			<p><st c="32001">The</st><a id="_idIndexMarker689"/><st c="32005"> nearest-neighbor algorithms we have</st><a id="_idIndexMarker690"/><st c="32041"> been working with up to this point, the ones responsible for finding the most closely related content to the user query, have been based</st><a id="_idIndexMarker691"/><st c="32178"> on </st><strong class="bold"><st c="32182">approximate nearest neighbor</st></strong><st c="32210"> (</st><strong class="bold"><st c="32212">ANN</st></strong><st c="32215">). </st><st c="32219">There is a more </st><em class="italic"><st c="32235">traditional</st></em><st c="32246"> and </st><em class="italic"><st c="32251">older</st></em><st c="32256"> algorithm that serves as an alternative to ANN though, and this is</st><a id="_idIndexMarker692"/><st c="32323"> the </st><strong class="bold"><st c="32328">k-nearest neighbor</st></strong><st c="32346"> (</st><strong class="bold"><st c="32348">kNN</st></strong><st c="32351">). </st><st c="32355">But kNN is based on an algorithm that dates back to 1951; why would we use this when we have a more sophisticated and powerful algorithm like ANN available? </st><st c="32512">Because kNN is </st><em class="italic"><st c="32527">still better</st></em><st c="32539"> than anything that came after it. </st><st c="32574">That is not a misprint. </st><st c="32598">kNN is still the </st><em class="italic"><st c="32615">most effective</st></em><st c="32629"> way to find the nearest neighbors. </st><st c="32665">It is better than ANN, which is touted as </st><em class="italic"><st c="32707">the</st></em><st c="32710"> solution by all of the database, vector database, and information retrieval companies that operate in this field. </st><st c="32825">ANN can come close, but kNN is still </st><span class="No-Break"><st c="32862">considered better.</st></span></p>
			<p><st c="32880">Why is ANN touted as </st><em class="italic"><st c="32902">the</st></em><st c="32905"> solution then? </st><st c="32921">Because kNN does not scale to the level you see in the large enterprises these vendors are targeting. </st><st c="33023">But this is all relative. </st><st c="33049">You may have a million data points, which sounds like a lot, with 1,536 dimension vectors, but that is still considered quite small on the global enterprise stage. </st><st c="33213">kNN can handle that pretty easily! </st><st c="33248">Many of the smaller projects that are using ANN in the field can probably benefit from using kNN instead. </st><st c="33354">The theoretical limit of kNN is going to depend on many things, such as your development environment, your data, the dimensions of your data, internet connectivity if using APIs, and many more. </st><st c="33548">So, we cannot give a specific number of data points. </st><st c="33601">You will need to test this. </st><st c="33629">But if it is smaller than the project I just described, 1 million data points with 1,536 dimension vectors, in a relatively capable development environment, you should really consider kNN! </st><st c="33818">At some point, you will notice a significant increase in processing time, and when the wait becomes too long for the usefulness of your application, switch to ANN. </st><st c="33982">But in the meantime, be sure to take full advantage of the superior search capabilities </st><span class="No-Break"><st c="34070">of kNN.</st></span></p>
			<p><st c="34077">Luckily, kNN is available in an easy-to-set-up retriever called </st><strong class="source-inline"><st c="34142">KNNRetriever</st></strong><st c="34154">. This retriever will utilize the same dense embeddings we use with our other algorithms, and therefore, we will replace </st><strong class="source-inline"><st c="34275">dense_retriever</st></strong><st c="34290"> with the kNN-based </st><strong class="source-inline"><st c="34310">KNNRetriever</st></strong><st c="34322">. Here is the code to implement that, fitting in nicely after we defined the previous version of our </st><strong class="source-inline"><st c="34423">dense_retriever</st></strong> <span class="No-Break"><st c="34438">retriever object:</st></span></p>
			<pre class="source-code"><st c="34456">
from langchain_community.retrievers import KNNRetriever
dense_retriever = KNNRetriever.from_texts(splits,
    OpenAIEmbeddings(), k=10)
ensemble_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, sparse_retriever],
    weights=[0.5, 0.5], c=0, k=10)</st></pre>
			<p><st c="34707">Run the</st><a id="_idIndexMarker693"/><st c="34715"> remaining code in the code lab to see it take the place of our</st><a id="_idIndexMarker694"/><st c="34778"> previous </st><strong class="source-inline"><st c="34788">dense_retriever</st></strong><st c="34803"> and perform in its place. </st><st c="34830">In this particular situation, with a very limited dataset, it is difficult to evaluate if it is doing better than the ANN-based algorithm we were previously using. </st><st c="34994">But, as your project scales, we highly recommend you take advantage of this approach until its scaling issues become too much of </st><span class="No-Break"><st c="35123">a burden.</st></span></p>
			<p><st c="35132">This concludes our exploration of the retrievers that can support RAG. </st><st c="35204">There are additional types of retrievers, as well as notable integrations with vector stores that support those retrievers that can be reviewed on the LangChain website. </st><st c="35374">For example, there is a time-weighted vector store retriever that allows you to incorporate recency into the retrieval process. </st><st c="35502">There is also a retriever called the Long-Context Reorder focused on improving results from long-context models that have difficulty paying attention to information in the middle of the retrieved documents. </st><st c="35709">Be sure to take a look at what is available, as they have the potential to have a significant impact on your RAG application. </st><st c="35835">We will now move on to talking about the </st><em class="italic"><st c="35876">brains</st></em><st c="35882"> of the operation and of the generation stage: </st><span class="No-Break"><st c="35929">the LLMs.</st></span></p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor225"/><st c="35938">Code lab 10.3 – LangChain LLMs</st></h1>
			<p><st c="35969">We now turn our attention </st><a id="_idIndexMarker695"/><st c="35996">to the last key component for RAG: the LLM. </st><st c="36040">Just like the retriever in the retrieval stage, without the LLM for the generation stage, there is no RAG. </st><st c="36147">The retrieval stage simply retrieves data from our data source, typically data the LLM does not know about. </st><st c="36255">However, that does not mean that the LLM does not play a vital role in our RAG implementation. </st><st c="36350">By providing the retrieved data to the LLM, we quickly catch that LLM up with what we want it to talk about, and this allows the LLM to do what it is really good at, providing a response based on that data to answer the original question posed by </st><span class="No-Break"><st c="36597">the user.</st></span></p>
			<p><st c="36606">The synergy between LLMs and RAG systems stems from the complementary strengths of these two technologies. </st><st c="36714">RAG systems enhance the capabilities of LLMs by incorporating external knowledge sources, enabling the generation of responses that are not only contextually relevant but also factually accurate and up to date. </st><st c="36925">In turn, LLMs contribute to RAG by providing a sophisticated understanding of the query context, facilitating more effective retrieval of pertinent information from the knowledge base. </st><st c="37110">This symbiotic relationship significantly improves the performance of AI systems in tasks that require both deep language understanding and access to a wide range of factual</st><a id="_idIndexMarker696"/><st c="37283"> information, leveraging the strengths of each component to create a more powerful and </st><span class="No-Break"><st c="37370">versatile system.</st></span></p>
			<p><st c="37387">In this code lab, we will cover a few examples of the most important component in the generation stage: the </st><span class="No-Break"><st c="37496">LangChain LLM.</st></span></p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor226"/><st c="37510">LLMs, LangChain, and RAG</st></h2>
			<p><st c="37535">As with the previous key components, we will first provide links to the LangChain documentation related to this major component, the </st><span class="No-Break"><st c="37669">LLMs: </st></span><a href="https://python.langchain.com/v0.2/docs/integrations/llms/ "><span class="No-Break"><st c="37675">https://python.langchain.com/v0.2/docs/integrations/llms/</st></span></a></p>
			<p><st c="37732">Here is a second helpful source for information combining LLMs with LangChain is the API </st><span class="No-Break"><st c="37822">documentation: </st></span><a href="https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.llms "><span class="No-Break"><st c="37837">https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.llms</st></span></a></p>
			<p><st c="37940">Let’s start with the API we have been using </st><span class="No-Break"><st c="37985">already: OpenAI.</st></span></p>
			<h3><st c="38001">OpenAI</st></h3>
			<p><st c="38008">We already </st><a id="_idIndexMarker697"/><st c="38020">have this code in place, but</st><a id="_idIndexMarker698"/><st c="38048"> let’s refresh the inner workings of this code by stepping through the key areas of our lab that make this </st><span class="No-Break"><st c="38155">component work:</st></span></p>
			<ol>
				<li><st c="38170">First, we must install the </st><span class="No-Break"><strong class="source-inline"><st c="38198">langchain-openai</st></strong></span><span class="No-Break"><st c="38214"> package:</st></span><pre class="source-code">
<strong class="bold"><st c="38223">%pip install langchain-openai</st></strong></pre><p class="list-inset"><st c="38253">The </st><strong class="source-inline"><st c="38258">langchain-openai</st></strong><st c="38274"> library provides integration between OpenAI’s language models </st><span class="No-Break"><st c="38337">and LangChain.</st></span></p></li>
				<li><st c="38351">Next, we import the </st><strong class="source-inline"><st c="38372">openai</st></strong><st c="38378"> library, which is the official Python library for interacting with OpenAI’s API, and will be used in this code primarily to apply the API key to the model so that we can access the paid API. </st><st c="38570">We then import the </st><strong class="source-inline"><st c="38589">ChatOpenAI</st></strong><st c="38599"> and </st><strong class="source-inline"><st c="38604">OpenAIEmbeddings</st></strong><st c="38620"> classes from the </st><span class="No-Break"><strong class="source-inline"><st c="38638">langchain_openai</st></strong></span><span class="No-Break"><st c="38654"> library:</st></span><pre class="source-code">
<strong class="bold"><st c="38663">import openai</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="38677">from langchain_openai import ChatOpenAI, OpenAIEmbeddings</st></strong></pre><p class="list-inset"><strong class="source-inline"><st c="38735">ChatOpenAI</st></strong><st c="38746"> is used to interact with OpenAI’s chat models, and </st><strong class="source-inline"><st c="38798">OpenAIEmbeddings</st></strong><st c="38814"> is used for generating embeddings </st><span class="No-Break"><st c="38849">from text.</st></span></p></li>
				<li><st c="38859">In the next line, we load the environment variables from a file named </st><strong class="source-inline"><st c="38930">env.txt</st></strong><st c="38937"> using the </st><span class="No-Break"><strong class="source-inline"><st c="38948">load_dotenv</st></strong></span><span class="No-Break"><st c="38959"> function:</st></span><pre class="source-code">
<strong class="bold"><st c="38969">_ = load_dotenv(dotenv_path='env.txt')</st></strong></pre><p class="list-inset"><st c="39008">We are using the </st><strong class="source-inline"><st c="39026">env.txt</st></strong><st c="39033"> file to store sensitive information (an API key) in a way that we can hide it from our versioning system, practicing better and more secure </st><span class="No-Break"><st c="39174">secret management.</st></span></p></li>
				<li><st c="39192">We then </st><a id="_idIndexMarker699"/><st c="39201">pass that API key into</st><a id="_idIndexMarker700"/><st c="39223"> OpenAI using the </st><span class="No-Break"><st c="39241">following code:</st></span><pre class="source-code">
<strong class="bold"><st c="39256">os.environ['OPENAI_API_KEY'] = os.getenv(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="39298">    'OPENAI_API_KEY')</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="39316">openai.api_key = os.environ['OPENAI_API_KEY']</st></strong></pre><p class="list-inset"><st c="39362">We first set up the API key as an environment variable called </st><strong class="source-inline"><st c="39425">OPENAI_API_KEY</st></strong><st c="39439">. Then, we set the OpenAI API key for the </st><strong class="source-inline"><st c="39481">openai</st></strong><st c="39487"> library using the retrieved value from the environment variable. </st><st c="39553">At this point, we can use the OpenAI integration with LangChain to call the LLM that is hosted at OpenAI with the </st><span class="No-Break"><st c="39667">proper access.</st></span></p></li>
				<li><st c="39681">Later in the code, we define the LLM we want </st><span class="No-Break"><st c="39727">to use:</st></span><pre class="source-code">
<strong class="bold"><st c="39734">llm = ChatOpenAI(model_name="gpt-4o-mini",</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="39777">    temperature=0)</st></strong></pre></li>
			</ol>
			<p><st c="39792">This line creates an instance of the </st><strong class="source-inline"><st c="39830">ChatOpenAI</st></strong><st c="39840"> class, specifying the model name as </st><strong class="source-inline"><st c="39877">gpt-4o-mini</st></strong><st c="39888"> and setting the </st><strong class="source-inline"><st c="39905">temperature</st></strong><st c="39916"> variable to </st><strong class="source-inline"><st c="39929">0</st></strong><st c="39930">. The temperature controls the randomness of the generated responses, with lower values producing more focused and deterministic outputs. </st><st c="40068">Currently, </st><strong class="source-inline"><st c="40079">gpt-4o-mini</st></strong><st c="40090"> is the newest and most capable model while also being the most cost-effective of the GPT4 series. </st><st c="40189">But even that model costs 10X more than </st><strong class="source-inline"><st c="40229">gpt-3.5-turbo</st></strong><st c="40242">, which is actually a relatively </st><span class="No-Break"><st c="40275">capable model.</st></span></p>
			<p><st c="40289">The most expensive of OpenAI’s models, </st><strong class="source-inline"><st c="40329">gpt-4-32k</st></strong><st c="40338">, is not as fast or capable as </st><strong class="source-inline"><st c="40369">gpt-4o-mini</st></strong><st c="40380"> and has a context window 4X its size. </st><st c="40419">There will likely be newer models soon, including </st><strong class="source-inline"><st c="40469">gpt-5</st></strong><st c="40474">, that may be even lower cost and more capable. </st><st c="40522">What you can take away from all of this is that you shouldn’t just assume the latest model is going to be the most expensive, and there are alternative versions that can be more capable and even more cost-effective coming out all the time. </st><st c="40762">Stay diligent in following the latest releases of the models, and for each release, weigh </st><a id="_idIndexMarker701"/><st c="40852">the benefits of cost, LLM capability, and any other related </st><a id="_idIndexMarker702"/><st c="40912">attributes to decide if a change </st><span class="No-Break"><st c="40945">is warranted.</st></span></p>
			<p><st c="40958">But in this effort, you do not need to limit yourself to just OpenAI. </st><st c="41029">Using LangChain makes it easy to switch LLMs and broaden your search for the best solution to all options within the LangChain community. </st><st c="41167">Let’s step through some other options you </st><span class="No-Break"><st c="41209">may consider.</st></span></p>
			<h3><st c="41222">Together AI</st></h3>
			<p><strong class="bold"><st c="41234">Together AI</st></strong><st c="41246"> offers a</st><a id="_idIndexMarker703"/><st c="41255"> developer-friendly set of services that give you access to numerous models. </st><st c="41332">Their</st><a id="_idIndexMarker704"/><st c="41337"> pricing for hosted LLMs is difficult to beat, and they often offer $5.00 in free credits to test out the </st><span class="No-Break"><st c="41443">different models.</st></span></p>
			<p><st c="41460">If you are new to Together API, you can use this link to set up your API key and add it to your </st><strong class="source-inline"><st c="41557">env.txt</st></strong><st c="41564"> file just like we did in the past with the OpenAI API </st><span class="No-Break"><st c="41619">key: </st></span><a href="https://api.together.ai/settings/api-keys "><span class="No-Break"><st c="41624">https://api.together.ai/settings/api-keys</st></span></a></p>
			<p><st c="41665">As you arrive at this web page, it currently offers you a $5.00 credit that will be in place after you click on the </st><strong class="bold"><st c="41782">Get started</st></strong><st c="41793"> button. </st><st c="41802">You do not have to provide a credit card to access this $</st><span class="No-Break"><st c="41859">5.00 credit.</st></span></p>
			<p><st c="41872">Be sure to add your new API key to your </st><strong class="source-inline"><st c="41913">env.txt</st></strong><st c="41920"> file </st><span class="No-Break"><st c="41926">as </st></span><span class="No-Break"><strong class="source-inline"><st c="41929">TOGETHER_API_KEY</st></strong></span><span class="No-Break"><st c="41945">.</st></span></p>
			<p><st c="41946">Once you are logged in, you can see the current costs for each LLM </st><span class="No-Break"><st c="42014">here: </st></span><a href="https://api.together.ai/models "><span class="No-Break"><st c="42020">https://api.together.ai/models</st></span></a></p>
			<p><st c="42050">For example, Meta Llama 3 70B instruct (Llama-3-70b-chat-hf) is currently listed to cost $0.90 per 1 million tokens. </st><st c="42168">This is a model that has been shown to rival ChatGPT 4, but Together AI, will</st><a id="_idIndexMarker705"/><st c="42245"> run with significantly lower inference costs than what OpenAI </st><a id="_idIndexMarker706"/><st c="42308">charges. </st><st c="42317">Another highly capable model, the Mixtral mixture of experts model costs $1.20 per 1 million Follow these steps to set up and use </st><span class="No-Break"><st c="42447">Together AI:</st></span></p>
			<ol>
				<li><st c="42459">We start with installing the package we need to use the </st><span class="No-Break"><st c="42516">Together API:</st></span><pre class="source-code">
<strong class="bold"><st c="42529">%pip install --upgrade langchain-together</st></strong></pre></li>
				<li><st c="42571">This prepares us to use the integration between the Together API </st><span class="No-Break"><st c="42637">and LangChain:</st></span><pre class="source-code">
<strong class="bold"><st c="42651">from langchain_together import ChatTogether</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="42695">_ = load_dotenv(dotenv_path='env.txt')</st></strong></pre><p class="list-inset"><st c="42734">This imports what we need in LangChain to use the </st><strong class="source-inline"><st c="42785">ChatTogether</st></strong><st c="42797"> integration and loads the API key (don’t forget to add it to the </st><strong class="source-inline"><st c="42863">env.txt</st></strong><st c="42870"> file before running this line </st><span class="No-Break"><st c="42901">of code!).</st></span></p></li>
				<li><st c="42911">Just like we did in the past with the OpenAI API key, we are going to pull in </st><strong class="source-inline"><st c="42990">TOGETHER_API_KEY</st></strong><st c="43006"> so that it can access </st><span class="No-Break"><st c="43029">your account:</st></span><pre class="source-code">
<strong class="bold"><st c="43042">os.environ['TOGETHER_API_KEY'] = os.getenv(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43086">    'TOGETHER_API_KEY')</st></strong></pre><p class="list-inset"><st c="43106">We are going to use the Llama 3 Chat model and Mistral’s Mixtral 8X22B Instruct model, but you can choose from 50+ models </st><span class="No-Break"><st c="43229">here: </st></span><a href="https://docs.together.ai/docs/inference-models"><span class="No-Break"><st c="43235">https://docs.together.ai/docs/inference-models</st></span></a></p><p class="list-inset"><st c="43281">You may find a better model for your </st><span class="No-Break"><st c="43319">particular needs!</st></span></p></li>
				<li><st c="43336">Here, we are defining </st><span class="No-Break"><st c="43359">the models:</st></span><pre class="source-code">
<strong class="bold"><st c="43370">llama3llm = ChatTogether(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43396">    together_api_key=os.environ['TOGETHER_API_KEY'],</st></strong></pre><pre class="source-code">
<strong class="bold"/><strong class="bold"><st c="43445">model="meta-llama/Llama-3-70b-chat-hf",</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43485">)</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43487">mistralexpertsllm = ChatTogether(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43520">    together_api_key=os.environ['TOGETHER_API_KEY'],</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43569">    model="mistralai/Mixtral-8x22B-Instruct-v0.1",</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43616">)</st></strong></pre><p class="list-inset"><st c="43618">In the preceding code snippet, we are establishing two different LLMs that we can run through the remainder of our code and see </st><span class="No-Break"><st c="43746">the results.</st></span></p></li>
				<li><st c="43758">Here, we</st><a id="_idIndexMarker707"/><st c="43767"> have </st><a id="_idIndexMarker708"/><st c="43773">updated the final code for using the Llama </st><span class="No-Break"><st c="43816">3 model:</st></span><pre class="source-code">
<strong class="bold"><st c="43824">llama3_rag_chain_from_docs = (</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43855">    RunnablePassthrough.assign(context=(lambda x:</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43901">        format_docs(x["context"])))</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43929">    | RunnableParallel(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43949">        {"relevance_score": (</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43971">            RunnablePassthrough()</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="43993">            | (lambda x: relevance_prompt_template.</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44033">                format(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44041">                    question=x['question'],</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44065">                    retrieved_context=x['context']))</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44098">            | llama3llm</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44110">            | StrOutputParser()</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44130">        ), "answer": (</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44145">               RunnablePassthrough()</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44167">               | prompt</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44176">               | llama3llm</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44188">               | StrOutputParser()</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44208">           )}</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44211">       )</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44213">       | RunnablePassthrough().assign(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44245">            final_answer=conditional_answer)</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44278">    )</st></strong></pre><p class="list-inset"><st c="44280">This should look familiar, as it is the RAG chain we have used in the past, but running with the Llama </st><span class="No-Break"><st c="44383">3 LLM.</st></span></p><pre class="source-code">
<strong class="bold"><st c="44389">llama3_rag_chain_with_source = RunnableParallel(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44438">    {"context": ensemble_retriever,</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44470">     "question": RunnablePassthrough()}</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44505">).assign(answer=llama3_rag_chain_from_docs)</st></strong></pre><p class="list-inset"><st c="44549">This is the final RAG chain we use, updated with the previous Llama 3-focused </st><span class="No-Break"><st c="44628">RAG chain.</st></span></p></li>
				<li><st c="44638">Next, we want</st><a id="_idIndexMarker709"/><st c="44652"> to run similar code to what we have run in the past that</st><a id="_idIndexMarker710"/><st c="44709"> invokes and runs the RAG pipeline with the Llama 3 LLM replacing the </st><span class="No-Break"><st c="44779">ChatGPT-4o-mini model:</st></span><pre class="source-code">
<strong class="bold"><st c="44801">llama3_result = llama3_rag_chain_with_source.invoke(</st></strong></pre><pre class="source-code">
<strong class="bold"/><strong class="bold"><st c="44854">user_query)</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44866">llama3_retrieved_docs = llama3_result['context']</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44915">print(f"Original Question: {user_query}\n")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44959">print(f"Relevance Score:</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="44984">    {llama3_result['answer']['relevance_score']}\n")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45033">print(f"Final Answer:</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45055">    \n{llama3_result['answer']['final_answer']}\n\n")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45105">print("Retrieved Documents:")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45135">for i, doc in enumerate(llama3_retrieved_docs,</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45182">    start=1):</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45192">print(f"Document {i}: Document ID:</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45227">    {doc.metadata['id']} source:</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45256">    {doc.metadata['source']}")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="45283">    print(f"Content:\n{doc.page_content}\n")</st></strong></pre><p class="list-inset"><st c="45324">The </st><a id="_idIndexMarker711"/><st c="45329">resulting </st><a id="_idIndexMarker712"/><st c="45339">response to the question </st><strong class="source-inline"><st c="45364">What are Google's environmental initiatives?</st></strong><st c="45408"> is </st><span class="No-Break"><st c="45412">as follows:</st></span></p><pre class="source-code"><st c="45423">
Google's environmental initiatives include:</st></pre><pre class="source-code"><st c="45467">
1. </st><st c="45471">Empowering individuals to take action: Offering sustainability features in Google products, such as eco-friendly routing in Google Maps, energy efficiency features in Google Nest thermostats, and carbon emissions information in Google Flights…</st></pre><pre class="source-code"><st c="45714">
[TRUNCATED]</st></pre><pre class="source-code"><st c="45726">
10. </st><st c="45731">Engagement with external targets and initiatives: Participating in industry-wide initiatives and partnerships to promote sustainability, such as the RE-Source Platform, iMasons Climate Accord, and World Business Council for Sustainable Development.</st></pre></li>
				<li><st c="45979">Let’s see what it looks like if we use the mixture of </st><span class="No-Break"><st c="46034">experts model:</st></span><pre class="source-code">
<strong class="bold"><st c="46048">mistralexperts_rag_chain_from_docs = (</st></strong></pre><pre class="source-code">
<strong class="bold"/><strong class="bold"><st c="46087">RunnablePassthrough.assign(context=(lambda x:</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46133">        format_docs(x["context"])))</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46161">    | RunnableParallel(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46181">        {"relevance_score": (RunnablePassthrough()</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46224">    | (lambda x: relevance_prompt_template.format(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46271">        question=x['question'],</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46295">        retrieved_context=x['context']))</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46328">     | mistralexpertsllm</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46348">     | StrOutputParser()</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46368">      ), "answer": (</st></strong></pre><pre class="source-code">
<strong class="bold"/><strong class="bold"><st c="46383">RunnablePassthrough()</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46405">          | prompt</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46414">          | mistralexpertsllm</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46434">          | StrOutputParser()</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46454">          )}</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46457">     )</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46459">    | RunnablePassthrough().assign(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46491">        final_answer=conditional_answer)</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46524">)</st></strong></pre><p class="list-inset"><st c="46526">Again, this </st><a id="_idIndexMarker713"/><st c="46538">should look familiar, as it is the RAG chain we have used in the past, but </st><a id="_idIndexMarker714"/><st c="46613">this time running with the mixture of </st><span class="No-Break"><st c="46651">experts LLM.</st></span></p><pre class="source-code">
<strong class="bold"><st c="46663">mistralexperts_rag_chain_with_source = RunnableParallel(</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46720">    {"context": ensemble_retriever, "question": RunnablePassthrough()}</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="46787">).assign(answer=mistralexperts_rag_chain_from_docs)</st></strong></pre><p class="list-inset"><st c="46839">Just as we did</st><a id="_idIndexMarker715"/><st c="46854"> before, we update the final RAG pipeline with the previous </st><a id="_idIndexMarker716"/><st c="46914">mixture of experts-focused </st><span class="No-Break"><st c="46941">RAG chain.</st></span></p><p class="list-inset"><st c="46951">This code will let us see the results of the mixture of experts replacing the </st><span class="No-Break"><st c="47030">ChatGPT-4o-mini model:</st></span></p><pre class="source-code">
<strong class="bold"><st c="47052">mistralexperts_result = mistralexperts_rag_chain_with_source.invoke(user_query)</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47132">mistralexperts_retrieved_docs = mistralexperts_result[</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47187">    'context']</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47198">print(f"Original Question: {user_query}\n")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47242">print(f"Relevance Score: {mistralexperts_result['answer']['relevance_score']}\n")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47324">print(f"Final Answer:\n{mistralexperts_result['answer']['final_answer']}\n\n")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47403">print("Retrieved Documents:")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47433">for i, doc in enumerate(mistralexperts_retrieved_docs, start=1):</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47498">             print(f"Document {i}: Document ID:</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47533">{doc.metadata['id']} source: {doc.metadata['source']}")</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="47589">              print(f"Content:\n{doc.page_content}\n")</st></strong></pre><p class="list-inset"><st c="47630">The</st><a id="_idIndexMarker717"/><st c="47634"> resulting response </st><a id="_idIndexMarker718"/><st c="47654">to </st><strong class="source-inline"><st c="47657">What are Google's environmental initiatives?</st></strong><st c="47701"> Is </st><span class="No-Break"><st c="47705">the following:</st></span></p><pre class="source-code"><st c="47719">
Google's environmental initiatives are organized around three key pillars: empowering individuals to take action, working together with partners and customers, and operating their business sustainably.</st></pre><pre class="source-code"><st c="47921">
1. </st><st c="47925">Empowering individuals: Google provides sustainability features like eco-friendly routing in Google Maps, energy efficiency features in Google Nest thermostats, and carbon emissions information in Google Flights. </st><st c="48138">Their goal is to help individuals, cities, and other partners collectively reduce 1 gigaton of carbon equivalent emissions annually by 2030.</st></pre><pre class="source-code"><st c="48278">
[TRUNCATED]</st></pre><pre class="source-code"><st c="48290">
Additionally, Google advocates for strong public policy action to create low-carbon economies, they work with the United Nations Framework Convention on Climate Change (UNFCCC) and support the Paris Agreement's goal to keep global temperature rise well below 2°C above pre-industrial levels. </st><st c="48583">They also engage with coalitions and sustainability initiatives like the RE-Source Platform and the Google.org Impact Challenge on Climate Innovation.</st></pre><p class="list-inset"><st c="48733">Compare this to the original response we saw in </st><span class="No-Break"><st c="48782">previous chapters:</st></span></p><pre class="source-code"><st c="48800">
Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, focusing on water stewardship, engaging in a circular economy, and supporting sustainable consumption of public goods. </st><st c="49108">They also engage with suppliers to reduce energy consumption and greenhouse gas emissions, report environmental data, and assess environmental criteria. </st><st c="49261">Google is involved in various sustainability initiatives, such as the iMasons Climate Accord, ReFED, and supporting projects with The Nature Conservancy. </st><st c="49415">They also work with coalitions like the RE-Source Platform and the World Business Council for Sustainable Development. </st><st c="49534">Additionally, Google invests in breakthrough innovation and collaborates with startups to tackle sustainability challenges. </st><st c="49658">They also focus on renewable energy and use data analytics tools to drive more intelligent supply chains.</st></pre></li>
			</ol>
			<p><st c="49763">The new responses from Llama 3 and the mixture of experts models show expanded responses that seem to be similar, if not more robust, compared to the original responses we were able to achieve with OpenAI’s </st><strong class="source-inline"><st c="49971">gpt-4o-mini</st></strong><st c="49982"> model at considerably fewer costs than OpenAI’s more expensive but more </st><span class="No-Break"><st c="50055">capable models.</st></span></p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor227"/><st c="50070">Extending the LLM capabilities</st></h2>
			<p><st c="50101">There are aspects of these LLM</st><a id="_idIndexMarker719"/><st c="50132"> objects that can be better utilized in your RAG application. </st><st c="50194">As described in the LangChain LLM documentation (</st><a href="https://python.langchain.com/v0.1/docs/modules/model_io/llms/streaming_llm/"><st c="50243">https://python.langchain.com/v0.1/docs/modules/model_io/llms/streaming_llm/</st></a><st c="50319">): </st></p>
			<p class="author-quote"><st c="50323">All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. </st><st c="50427">ainvoke, batch, abatch, stream, astream. </st><st c="50468">This gives all LLMs basic support for async, streaming and batch.</st></p>
			<p><st c="50533">These are key features that can significantly speed up the processing of your RAG application, particularly if you are processing multiple LLM calls at once. </st><st c="50692">In the following subsections, we will look at the key methods and how they can </st><span class="No-Break"><st c="50771">help you.</st></span></p>
			<h3><st c="50780">Async</st></h3>
			<p><st c="50786">By default, async support</st><a id="_idIndexMarker720"/><st c="50812"> runs the regular sync method in a separate thread. </st><st c="50864">This allows other parts of your async program to keep running while the language model </st><span class="No-Break"><st c="50951">is working.</st></span></p>
			<h3><st c="50962">Stream</st></h3>
			<p><st c="50969">Streaming support</st><a id="_idIndexMarker721"/><st c="50987"> typically returns </st><strong class="source-inline"><st c="51006">Iterator</st></strong><st c="51014"> (or </st><strong class="source-inline"><st c="51019">AsyncIterator</st></strong><st c="51032"> for async streaming) with just one item: the final result from the language model. </st><st c="51116">This doesn’t provide word-by-word streaming, but it ensures your code can</st><a id="_idIndexMarker722"/><st c="51189"> work with any of the LangChain language model integrations that expect a stream </st><span class="No-Break"><st c="51270">of tokens.</st></span></p>
			<h3><st c="51280">Batch</st></h3>
			<p><st c="51286">Batch support processes</st><a id="_idIndexMarker723"/><st c="51310"> multiple inputs at the same time. </st><st c="51345">For a sync batch, it uses multiple threads. </st><st c="51389">For an async batch, it uses </st><strong class="source-inline"><st c="51417">asyncio.gather</st></strong><st c="51431">. You can control how many tasks run at once using the </st><strong class="source-inline"><st c="51486">max_concurrency</st></strong><st c="51501"> setting </st><span class="No-Break"><st c="51510">in </st></span><span class="No-Break"><strong class="source-inline"><st c="51513">RunnableConfig</st></strong></span><span class="No-Break"><st c="51527">.</st></span></p>
			<p><st c="51528">Not all LLMs support all of these functions natively though. </st><st c="51590">For the two implementations we have discussed, as well as many more, LangChain provides an in-depth chart that can be found </st><span class="No-Break"><st c="51714">here: </st></span><a href="https://python.langchain.com/v0.2/docs/integrations/llms/"><span class="No-Break"><st c="51720">https://python.langchain.com/v0.2/docs/integrations/llms/</st></span></a></p>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor228"/><st c="51777">Summary</st></h1>
			<p><st c="51785">This chapter explored the key technical components of RAG systems in the context of LangChain: vector stores, retrievers, and LLMs. </st><st c="51918">It provided an in-depth look at the various options available for each component and discussed their strengths, weaknesses, and scenarios in which one option might be better </st><span class="No-Break"><st c="52092">than another.</st></span></p>
			<p><st c="52105">The chapter started by examining vector stores, which play a crucial role in efficiently storing and indexing vector representations of knowledge base documents. </st><st c="52268">LangChain integrates with various vector store implementations, such as Pinecone, Weaviate, FAISS, and PostgreSQL with vector extensions. </st><st c="52406">The choice of vector store depends on factors such as scalability, search performance, and deployment requirements. </st><st c="52522">The chapter then moved on to discuss retrievers, which are responsible for querying the vector store and retrieving the most relevant documents based on the input query. </st><st c="52692">LangChain offers a range of retriever implementations, including dense retrievers, sparse retrievers (such as BM25), and ensemble retrievers that combine the results of </st><span class="No-Break"><st c="52861">multiple retrievers.</st></span></p>
			<p><st c="52881">Finally, the chapter covered the role of LLMs in RAG systems. </st><st c="52944">LLMs contribute to RAG by providing a sophisticated understanding of the query context and facilitating more effective retrieval of pertinent information from the knowledge base. </st><st c="53123">The chapter showcased the integration of LangChain with various LLM providers, such as OpenAI and Together AI, and highlighted the capabilities and cost considerations of different models. </st><st c="53312">It also discussed the extended capabilities of LLMs in LangChain, such as async, streaming, and batch support, and provided a comparison of the native implementations offered by different </st><span class="No-Break"><st c="53500">LLM integrations.</st></span></p>
			<p><st c="53517">In the next chapter, we will continue to talk about how LangChain can be utilized to build a capable RAG application, focusing now on the smaller components that can be used in support of the key components we just discussed in </st><span class="No-Break"><st c="53746">this chapter.</st></span></p>
		</div>
	<div id="charCountTotal" value="53759"/></body></html>