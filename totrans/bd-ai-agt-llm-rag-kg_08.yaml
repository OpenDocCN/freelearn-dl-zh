- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Reinforcement Learning and AI Agents
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与AI代理
- en: In *Chapters 5–7*, we discussed how to provide our model with access to external
    memory. This memory was stored in a database of a different type (vector or graph),
    and through a search, we could look up the information needed to answer a question.
    The model would then receive all the information that was needed in context and
    then answer, providing definite and discrete real-world information.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5-7章*中，我们讨论了如何让我们的模型访问外部记忆。这种记忆存储在另一种类型的数据库中（向量或图），通过搜索，我们可以查找回答问题所需的信息。然后，模型将接收到在特定环境中所需的所有信息，然后回答，提供明确和离散的现实世界信息。
- en: However, as we saw later in [*Chapter 7*](B21257_07.xhtml#_idTextAnchor113),
    LLMs have limited knowledge and understanding of the real world (both when it
    comes to commonsense reasoning and when it comes to spatial relations).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们在[*第7章*](B21257_07.xhtml#_idTextAnchor113)中稍后看到的，LLMs对现实世界的知识和理解有限（无论是常识推理还是空间关系）。
- en: Humans learn to move in space and interact with the environment through exploration.
    In a process that is trial and error, we humans learn that we cannot touch fire
    or how to find our way home. Likewise, we learn how to relate to other human beings
    through interactions with them. Our interactions with the real world allow us
    to learn but also to modify our surroundings. The environment provides us with
    information through perception, information that we process and learn from, and
    ultimately use to modify the environment. This is a cyclical process in which
    we sense changes in the environment and respond.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过探索学习在空间中移动并与环境互动。在这个过程中，我们通过试错学习，人类知道我们不能触摸火或如何找到回家的路。同样，我们通过与他们的互动学习如何与其他人类建立联系。我们与真实世界的互动使我们能够学习并改变我们的环境。环境通过感知为我们提供信息，我们处理并从中学习这些信息，最终用于改变环境。这是一个循环过程，其中我们感知环境的变化并做出反应。
- en: We do not learn all these skills just by reading from a book; it wouldn’t be
    possible. Interaction with the environment, therefore, is critical to learn certain
    skills and knowledge. Without this, we would find it difficult to do certain tasks.
    So, we need a system that allows artificial intelligence to interact and learn
    from the environment through exploration. **reinforcement learning** (**RL**)
    is a paradigm that focuses on describing how an intelligent agent can take actions
    in a dynamic environment. RL governs the behavior of an agent, what actions to
    take in a given environment (and the state of that environment), and how to learn
    from it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能仅仅通过读书来学习所有这些技能；这是不可能的。因此，与环境互动对于学习某些技能和知识至关重要。没有这一点，我们将发现很难完成某些任务。所以，我们需要一个系统，允许人工智能通过探索与环境互动和学习。**强化学习**（**RL**）是一种范式，它专注于描述智能代理如何在动态环境中采取行动。RL控制代理的行为，在给定环境中采取哪些行动（以及该环境的状况），以及如何从中学习。
- en: In this chapter, therefore, we will discuss RL. We will start with some theory
    on the topic. We will start with a simple case in which an agent needs to understand
    how to balance exploration with exploitation in order to find the winning strategy
    to solve a problem. Once the basics are defined, we will describe how we can use
    a neural network as an agent. We will look at some of the most popular algorithms
    used nowadays for interacting and learning from the environment. In addition,
    we will show how an agent can be used to be able to explore an environment (such
    as training an agent to solve a video game). In the last section, we will discuss
    the intersection of **large language models** (**LLMs**) and RL.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将讨论RL。我们将从该主题的一些理论开始。我们将从一个简单的案例开始，其中代理需要理解如何平衡探索与利用，以找到解决问题的获胜策略。一旦定义了基础知识，我们将描述如何使用神经网络作为代理。我们将查看目前用于与环境交互和学习的最流行的一些算法。此外，我们将展示如何使用代理来探索环境（例如训练代理解决视频游戏）。在最后一节中，我们将讨论**大型语言模型**（**LLMs**）与RL的交集。
- en: 'In this chapter, we''ll be covering the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Introduction to reinforcement learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: Deep reinforcement learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: LLM interactions with RL models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM与RL模型的交互
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Most of this code can be run on a CPU, but it is preferable to run it on a
    GPU. This is especially true when we are discussing how to train an agent to learn
    how to play a video game. The code is written in PyTorch and uses standard libraries
    for the most part (PyTorch, OpenAI Gym). The code can be found on GitHub: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码可以在CPU上运行，但最好在GPU上运行。当我们讨论如何训练一个代理学习如何玩电子游戏时，这一点尤其正确。代码是用PyTorch编写的，大部分使用标准库（PyTorch、OpenAI
    Gym）。代码可以在GitHub上找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8)。
- en: Introduction to reinforcement learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: In previous chapters, we discussed a model that learns from a large amount of
    text. Humans—and increasingly, AI agents—learn best through trial and error. Imagine
    a child learning to stack blocks or riding a bike. There’s no explicit teacher
    guiding each move; instead, the child learns by acting, observing the results,
    and adjusting. This interaction with the environment—where actions lead to outcomes
    and those outcomes shape future behavior—is central to how we learn. Unlike passive
    learning from books or text, this kind of learning is goal-directed and grounded
    in experience. To enable machines to learn in a similar way, we need a new approach.
    This learning paradigm is called RL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了一个从大量文本中学习的模型。人类——以及越来越多的AI代理——通过试错学习得最好。想象一个孩子学习堆叠积木或骑自行车。没有明确的老师指导每一个动作；相反，孩子通过行动、观察结果并调整来学习。这种与环境互动——其中行动导致结果，而这些结果又塑造未来的行为——是我们学习的关键。与被动地从书籍或文本中学习不同，这种学习是有目标的，并且基于经验。为了使机器能够以类似的方式学习，我们需要一种新的方法。这种学习范式被称为强化学习（RL）。
- en: More formally, an infant learns from their interaction with the environment,
    from the consequential relationship of an action and its effect. A child’s learning
    is not simply exploratory but aimed at a specific goal; they learn what actions
    must be taken to achieve a goal. Throughout our lives, our learning is often related
    to our interaction with the environment and how it responds in response to our
    behavior. These concepts are seen as the basis of both learning theory and intelligence
    in general.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，婴儿通过与环境互动，从行为及其效果之间的因果关系中学到东西。孩子的学习不仅仅是探索性的，而是有特定目标的；他们学习必须采取哪些行动才能实现目标。在我们的一生中，我们的学习往往与我们的环境互动以及环境如何对我们的行为做出反应有关。这些概念被视为学习理论和一般智能的基础。
- en: RL is defined as a branch of machine learning, where a system must make decisions
    to maximize cumulative rewards in a given situation. Unlike in supervised learning
    (wherein a model learns from labeled examples) or unsupervised learning (wherein
    a model learns by detecting patterns in the data), in RL the model learns from
    experience. In fact, the system is not told what actions it must perform but must
    explore the environment and find out what actions allow it to have a reward. In
    more complex situations, these rewards may not be immediate but come only later
    (e.g., sacrificing a piece in chess but achieving victory later). So, on a general
    level, we can say that the basics of RL are trial and error and the possibility
    of delayed reward.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习被定义为机器学习的一个分支，其中系统必须做出决策，以在给定情况下最大化累积奖励。与监督学习（其中模型从标记的例子中学习）或无监督学习（其中模型通过在数据中检测模式来学习）不同，在强化学习中，模型从经验中学习。事实上，系统没有被告知必须执行哪些动作，而是必须探索环境并找出哪些动作可以使它获得奖励。在更复杂的情况下，这些奖励可能不是立即的，而是稍后才会到来（例如，在棋盘上牺牲一个棋子但最终获胜）。因此，在更广泛的意义上，我们可以说强化学习的基本原理是试错和延迟奖励的可能性。
- en: 'From this, we derive two important concepts that will form the basis of our
    discussion of RL:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个基础上，我们推导出两个重要的概念，这些概念将成为我们讨论强化学习（RL）的基础：
- en: '**Exploration versus exploitation**: The model must exploit previously acquired
    knowledge to achieve its goal. At the same time, it must explore the environment
    in order to make better choices in the future. A balance must be struck between
    these two aspects, because solving a problem may not be the most obvious path.
    A model therefore must test different types of actions (explore) before it can
    exploit the best action (exploitation). Even today, choosing the best balance
    is an open challenge for RL theory. A helpful way to understand this is by imagining
    someone trying different restaurants in a new city. At first, they might try a
    variety of places (exploration) to see what’s available. After discovering a few
    favorites, they might start going to the same ones more often (exploitation).
    But if they always stick to the familiar spots, they might miss out on finding
    an even better restaurant. The challenge is knowing when to try something new
    and when to stick with what works—and this is still an open question in RL.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索与利用**：模型必须利用之前获得的知识来实现其目标。同时，它必须探索环境以便在未来做出更好的选择。在这两个方面之间必须取得平衡，因为解决问题可能不是最明显的路径。因此，模型必须在利用最佳行动（利用）之前测试不同类型的行动（探索）。即使今天，选择最佳平衡仍然是强化学习理论的一个开放挑战。理解这一点的有用方法是想象有人在新的城市尝试不同的餐厅。一开始，他们可能会尝试各种地方（探索）以了解有什么可用的。在发现几个喜欢的餐厅后，他们可能会开始更频繁地去同一个地方（利用）。但如果他们总是坚持去熟悉的地方，他们可能会错过发现更好的餐厅的机会。挑战在于知道何时尝试新事物，何时坚持有效的方法——这仍然是强化学习中的一个开放问题。'
- en: '**Achieving a global goal in an uncertain environment**: RL focuses on achieving
    a goal without requiring the problem to be reframed into subproblems. Instead,
    it addresses a classic supervised machine learning challenge, which involves breaking
    a complex problem into general subproblems and devising an effective schedule.
    In the case of RL, on the other hand, one directly defines a general problem that
    an agent must solve. This does not mean that there has to be only one agent but
    there can be multiple agents with a clear goal interacting with each other. A
    relatable example would be learning to commute efficiently in a new city. At first,
    you don’t break the task into subproblems like “learn the bus schedule,” “estimate
    walking time,” or “optimize weather exposure.” Instead, you treat the goal as
    a whole: get to work on time every day. Through trial and error—taking different
    routes, trying trains versus buses, adjusting for traffic—you learn which options
    work best. Over time, you build a strategy without ever explicitly labeling every
    part of the problem. If you live with roommates or friends who are doing the same,
    you might exchange tips or compete for the fastest route, just like multiple agents
    interacting in RL.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在不确定的环境中实现全局目标**：强化学习专注于实现目标，而不需要将问题重新构造成子问题。相反，它解决了一个经典的监督机器学习挑战，这涉及到将复杂问题分解成一般子问题，并制定一个有效的计划。在强化学习的情况下，另一方面，直接定义一个代理必须解决的问题。这并不意味着必须只有一个代理，但可以有多个具有明确目标的代理相互交互。一个相关的例子是学习在新城市高效通勤。一开始，你不会将任务分解成“学习公交时刻表”、“估计步行时间”或“优化天气暴露”这样的子问题。相反，你将目标视为整体：每天按时到达工作地点。通过试错——尝试不同的路线、尝试火车与公交车的对比、调整交通状况——你学会了哪些选项最有效。随着时间的推移，你制定了一个策略，而无需明确标记问题的每个部分。如果你与室友或朋友住在一起，他们也在做同样的事情，你们可能会交换建议或为了最快的路线而竞争，就像强化学习中的多个代理相互作用一样。'
- en: 'There are several elements that are present in an RL system: an **agent**,
    the **environment**, a **state**, a **policy**, a **reward signal**, and a **value
    function**. The agent is clearly the learner or decision-maker (the model that
    interacts with the environment, makes decisions, and takes actions). The environment,
    on the other hand, is everything that the environment interacts with. A state
    represents a particular condition or configuration of the environment at a certain
    time (for example, the state of the pieces on a chessboard before a move). Given
    a state, the agent must make choices and choose an action to take. Not all space
    is always observable; our agent can only have access to a partial description
    of the state. For example, a robotic agent navigating a maze can only get information
    through the camera and thus observe only what is in front of it. The information
    obtained from the camera is an observation, so the model will use only a subset
    of the state.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习系统中存在几个元素：一个 **代理**，一个 **环境**，一个 **状态**，一个 **策略**，一个 **奖励信号**，和一个 **价值函数**。代理显然是学习者或决策者（与环境交互、做出决策并采取行动的模型）。另一方面，环境是与环境交互的一切。状态代表在特定时间环境的一个特定条件或配置（例如，在移动之前棋盘上棋子的状态）。给定一个状态，代理必须做出选择并选择一个要采取的动作。并非所有空间总是可观察的；我们的代理只能访问状态的局部描述。例如，在一个迷宫中导航的机器人代理只能通过摄像头获取信息，因此只能观察到它面前的东西。从摄像头获得的信息是观察结果，因此模型将只使用状态的一个子集。
- en: '![Figure 8.1 – Representation of elements in the RL system](img/B21257_08_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 强化学习系统中元素的表现](img/B21257_08_01.jpg)'
- en: Figure 8.1 – Representation of elements in the RL system
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 强化学习系统中元素的表现
- en: In the preceding figure, we can see how the environment (in this case, the game
    screen) is represented in vector form (this is the state). Also, the three possible
    actions are represented in this case with a scalar. This allows us to be able
    to train an algorithm.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到环境（在这种情况下，游戏屏幕）是如何以向量形式表示的（这就是状态）。此外，三种可能的行为在这个例子中以标量表示。这使我们能够训练一个算法。
- en: '**Actions** are the possible decisions or moves that an agent can conduct in
    an environment (the pieces on the chessboard can only move in certain directions:
    a bishop only diagonally, the rook vertically or horizontally, and so on). The
    action set can be discrete (movements in the maze) but also a continuous action
    space (in this case, it will be real-value vectors). These actions are part of
    a strategy to achieve a certain goal, according to the state of the environment
    and policy.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作**是代理在环境中可以执行的可能决策或移动（棋盘上的棋子只能向特定方向移动：主教只能斜着走，车可以垂直或水平移动，等等）。动作集可以是离散的（迷宫中的移动）但也可以是连续的动作空间（在这种情况下，它将是实值向量）。这些动作是实现特定目标策略的一部分，根据环境的状态和政策。'
- en: '![Figure 8.2 – Interaction of the agent with the environment selecting an action](img/B21257_08_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 代理与环境交互选择动作](img/B21257_08_02.jpg)'
- en: Figure 8.2 – Interaction of the agent with the environment selecting an action
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 代理与环境交互选择动作
- en: In the preceding figure, we can see that time 0 (*t0*) corresponds to a state
    *t0*; if our agent acts with a move, this changes the environment. At time *t1*,
    the environment will be different and therefore we will have a different state,
    *t1*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到时间 0 (*t0*) 对应于状态 *t0*；如果我们的代理执行一个动作，这将改变环境。在时间 *t1*，环境将不同，因此我们将拥有不同的状态，*t1*。
- en: 'The **policy** defines how an agent behaves at a certain time. Given, then,
    the state of the environment and the possible actions, the policy maps the action
    to the state of the system. The policy can be a set of rules, a lookup table,
    a function, or something else. The policy can also be stochastic by specifying
    a probability for each action. In a sense, policy is the heart of RL because it
    determines the agent’s behavior. In psychology, this can be defined as a set of
    stimulus-response rules. For example, a policy might be to eat an opponent’s piece
    whenever the opportunity arises. More often, a policy is parameterized: the output
    of the policy is a computable function that depends on a set of parameters. One
    of the most widely used systems is a neural network whose parameters are optimized
    with an optimization algorithm.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略**定义了代理在特定时间的行为方式。给定环境的状态和可能的行为，策略将行为映射到系统的状态。策略可以是一组规则、查找表、函数或其他东西。策略也可以是随机的，通过为每个行为指定一个概率。从某种意义上说，策略是强化学习的心脏，因为它决定了代理的行为。在心理学中，这可以定义为一系列刺激-反应规则。例如，一个策略可能是每当有机会时吃掉对手的棋子。更常见的是，策略是参数化的：策略的输出是一个可计算的函数，它依赖于一组参数。最广泛使用的系统之一是神经网络，其参数通过优化算法进行优化。'
- en: The **reward** is a positive or negative signal received from the environment.
    It is another critical factor because it provides a goal to the agent at each
    time step. This reward is used to define both the local and global objectives
    of an agent. In other words, at each time step, the agent receives a signal from
    the environment (usually a single number), and in the long run, the agent’s goal
    is to optimize this reward. The reward then allows us to determine whether the
    model is behaving correctly or not and allows us to understand the difference
    between positive and negative events, to understand our interaction with the environment
    and the appropriate response to the state of the system. For example, losing a
    piece can be considered a local negative reward, and winning the game a global
    reward. The reward is often used to change the policy and calibrate it in response
    to the environment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励**是从环境中接收到的正或负信号。它是一个关键因素，因为它为代理在每个时间步提供了目标。这个奖励被用来定义代理的局部和全局目标。换句话说，在每个时间步，代理从环境中接收一个信号（通常是一个数字），而在长期，代理的目标是优化这个奖励。奖励然后使我们能够确定模型是否表现正确，并使我们能够理解正负事件之间的差异，理解我们与环境的互动以及对于系统状态的适当响应。例如，失去一个棋子可以被视为局部负奖励，而赢得游戏则是全局奖励。奖励通常用于改变策略并对其环境进行校准。'
- en: '![Figure 8.3 – Example of positive and negative reward](img/B21257_08_03.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 正负奖励的示例](img/B21257_08_03.jpg)'
- en: Figure 8.3 – Example of positive and negative reward
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 正负奖励的示例
- en: The reward, though, gives us information about what is right in the immediate
    moment while a **value function** defines what is the best approach in the long
    run. In more technical terms, the value of a state is the total amount of reward
    that an agent can expect to get in the future, starting from that state (for example,
    how many points in a game the agent can collect starting from that position).
    In simple words, the value function helps us understand what happens if we consider
    that state and subsequent states, and what is likely to happen in the future.
    There is, however, a dependence between rewards and value; without the former,
    we cannot calculate the latter, despite that our real goal is value. For example,
    sacrificing a piece has a low reward but may ultimately be the key to winning
    the game. Clearly, establishing a reward is much easier, while it is difficult
    to establish a value function because we have to take into account not only the
    current state, but all previous observations conducted by the agent.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，奖励虽然告诉我们即时什么是正确的，而**价值函数**则定义了长期的最佳方法。用更技术性的术语来说，状态的价值是一个代理从该状态开始可以期望在未来获得的奖励总量（例如，从该位置开始，代理在游戏中可以收集多少分数）。简单来说，价值函数帮助我们理解如果我们考虑该状态及其后续状态会发生什么，以及未来可能发生什么。然而，奖励和价值之间存在依赖关系；没有前者，我们无法计算后者，尽管我们的真正目标是价值。例如，牺牲一个棋子可能有低奖励，但最终可能是赢得游戏的关键。显然，建立奖励要容易得多，而建立价值函数则很困难，因为我们不仅要考虑当前状态，还要考虑代理所做的所有先前观察。
- en: 'A classic example of RL is an agent who has to navigate a maze. A state *S*
    defines the agent’s position in the maze; this agent has a possible set of actions
    *A* (move east, west, north, or south). The policy *π* indicates what action the
    agent must take in a certain state. A reward *R* can be a penalty when the agent
    chooses an action that is not allowed (slamming on a wall, for example), and the
    value is getting out of the maze. In *Figure 8**.4*, we have a depiction of the
    interactions between an agent and its environment:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个经典例子是必须导航迷宫的智能体。状态 *S* 定义了智能体在迷宫中的位置；这个智能体有一个可能的动作集 *A*（向东、西、北或南移动）。策略
    *π* 指示智能体在特定状态下必须采取什么动作。奖励 *R* 可以是当智能体选择不允许的动作时的惩罚（例如撞墙），而价值是走出迷宫。在 *图 8**.4*
    中，我们展示了智能体与其环境之间的交互：
- en: '![Figure 8.4 – Overview model of reinforcement learning system (https://arxiv.org/pdf/2408.07712)](img/B21257_08_04.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 强化学习系统概述模型 (https://arxiv.org/pdf/2408.07712)](img/B21257_08_04.jpg)'
- en: Figure 8.4 – Overview model of reinforcement learning system ([https://arxiv.org/pdf/2408.07712](https://arxiv.org/pdf/2408.07712))
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 强化学习系统概述模型 ([https://arxiv.org/pdf/2408.07712](https://arxiv.org/pdf/2408.07712))
- en: At a given time step (*t*), an agent observes the state of the environment (*S*t),
    chooses an action (*A*t) according to policy *π*, and receives a reward (*R*t).
    At this point, the cycle repeats at the new state (*S*t+1). The policy can be
    static or updated at the end of each cycle.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的时间步 (*t*), 智能体观察到环境的当前状态 (*S*t)，根据策略 *π* 选择动作 (*A*t)，并获得奖励 (*R*t)。此时，循环在新状态
    (*S*t+1*) 上重复。策略可以是静态的，也可以在每个循环结束时更新。
- en: In the next section, we will begin to discuss an initial example of RL, starting
    with the classic multi-armed bandit.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将开始讨论强化学习的初始示例，从经典的多臂老虎机开始。
- en: The multi-armed bandit problem
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多臂老虎机问题
- en: The **k-armed bandit problem** is perhaps the most classic example to introduce
    RL. RL is needed for all those problems in which a model must learn from its actions
    rather than be instructed by a positive example. In the *k*-armed bandit problem,
    we have a slot machine with *n* independent arms (bandits), and each of these
    bandits has its own rigged probability distribution of success. Each time we pull
    an arm, we have a stochastic probability of either receiving a reward or failing.
    At each action, we have to choose which lever to pull, and the rewards are what
    we gain. The goal is to maximize our expected total reward over a certain period
    of time (e.g., 1,000 actions or time steps). In other words, we have to figure
    out which levers give us the best payoff, and we will maximize our actions on
    them (i.e., we will pull them more often).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-armed bandit problem** 可能是介绍强化学习最经典的例子。在所有那些模型必须从其动作中学习而不是从正例中接受指导的问题中，都需要强化学习。在
    *k*-armed bandit 问题中，我们有一个具有 *n* 个独立臂（带枪）的老虎机，每个带枪都有自己的成功概率分布。每次拉动臂时，我们都有随机概率要么获得奖励要么失败。在每次动作中，我们必须选择拉动哪个杠杆，奖励就是我们获得的。目标是最大化在一定时期内（例如，1,000
    个动作或时间步）的期望总奖励。换句话说，我们必须找出哪些杠杆给我们带来最好的回报，并将最大化我们的动作（即，我们将更频繁地拉动它们）。'
- en: 'The problem may appear simple, but it is far from trivial. Our agent does not
    have access to the true bandit probability distribution and must learn the most
    favorable bandits through trial and error. Moreover, as simple as this problem
    is, it has similarities to several real-world case scenarios: choosing the best
    treatment for a patient, A/B testing, social media influence, and so on. At each
    time step *t*, we can select an *A*t action and get the corresponding reward (*R*t).
    The value of an arbitrary action *a*, defined as *q**⇤**(a)*, is the expected
    reward if we selected this action at time step *t*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 问题可能看起来很简单，但它远非微不足道。我们的智能体无法访问真正的带枪概率分布，必须通过试错来学习最有利的带枪。此外，尽管这个问题很简单，但它与几个现实世界的案例场景有相似之处：为患者选择最佳治疗方案、A/B
    测试、社交媒体影响等等。在每一个时间步 *t*，我们可以选择一个 *A*t 动作并获得相应的奖励 (*R*t)。任意动作 *a* 的价值，定义为 *q**⇤**(a)*，是在时间步
    *t* 选择此动作时的期望奖励：
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>]</mml:mo></mml:math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>]</mml:mo></mml:math>
- en: If we knew the value of each action, we would have practically solved the problem
    already (we would always select the one with the highest value). However, we do
    not know the value of an action, but we can calculate an estimated value, defined
    as *Q*t(a), which we wish to be close to *q*(a)*. At each time step, we have estimated
    values *Q*t(a) that are greater than the others; selecting these actions (pulling
    the arms) is called greedy actions and exploiting the current knowledge. Conversely,
    selecting an action with a lower estimated value is referred to as exploration
    (because it allows us to explore what happens with other actions and thus improve
    our estimation of these actions).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道每个动作的价值，我们实际上已经解决了这个问题（我们会总是选择价值最高的那个）。然而，我们并不知道动作的价值，但我们可以计算一个估计值，定义为
    *Q*t(a)，我们希望它接近 *q*(a)*。在每一步，我们都有估计值 *Q*t(a)，这些值比其他值大；选择这些动作（拉动杠杆）被称为贪婪动作和利用当前知识。相反，选择估计值较低的动作被称为探索（因为它允许我们探索其他动作会发生什么，从而提高我们对这些动作的估计）。
- en: '![Figure 8.5 – Multi-arm bandit](img/B21257_08_05.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 多臂老虎机](img/B21257_08_05.jpg)'
- en: Figure 8.5 – Multi-arm bandit
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 多臂老虎机
- en: Exploring may bring a decrease in gain in later steps but guarantees a greater
    gain in the long run. This is because our estimation may not be correct. Exploring
    allows us to correct the estimated value for an action. Especially in the early
    steps, it is more important to explore so that the system can understand which
    actions are best. In the final steps, the model should exploit the best actions.
    For this, we need a system that allows us to balance exploration toward exploitation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 探索可能会在后续步骤中减少收益，但保证长期收益更大。这是因为我们的估计可能不正确。探索允许我们纠正动作的估计值。特别是在早期步骤中，探索更为重要，以便系统可以了解哪些动作最好。在最后一步，模型应该利用最好的动作。为此，我们需要一个系统，允许我们平衡探索和利用。
- en: 'To get an initial estimate of value, we can take an average of the rewards
    that are received:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到价值的初始估计，我们可以取收到的奖励的平均值：
- en: <mrow><mrow><msub><mi>Q</mi><mi>t</mi></msub><mfenced close=")" open="("><mi>a</mi></mfenced><mo>=</mo><mfrac><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mi>o</mi><mi>f</mi><mi>r</mi><mi>e</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>w</mi><mi>h</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>p</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>o</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>p</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>o</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>·</mo><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub></mrow></mfrac></mrow></mrow>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>Q</mi><mi>t</mi></msub><mfenced close=")" open="("><mi>a</mi></mfenced><mo>=</mo><mfrac><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mi>o</mi><mi>f</mi><mi>r</mi><mi>e</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>w</mi><mi>h</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>p</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>o</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>p</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>o</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>t</mi></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>·</mo><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub></mrow></mfrac></mrow></mrow>
- en: In this simple equation, *1* represents a variable that indicates whether the
    action was used at the time step (1 if used, 0 if not used).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单方程中，*1*代表一个变量，表示在时间步长中是否使用了动作（使用时为1，未使用时为0）。
- en: <mrow><mrow><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub><mo>=</mo><mfenced
    close="" open="{"><mtable columnalign="center" columnwidth="auto" rowalign="baseline
    baseline" rowspacing="1.0000ex"><mtr><mtd><mrow><mn>1</mn><mi>i</mi><mi>f</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>i</mi></mrow></mtd></mtr><mtr><mtd><mrow><mn>0</mn><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mtd></mtr></mtable></mfenced></mrow></mrow>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub><mo>=</mo><mfenced
    close="" open="{"><mtable columnalign="center" columnwidth="auto" rowalign="baseline
    baseline" rowspacing="1.0000ex"><mtr><mtd><mrow><mn>1</mn><mi>i</mi><mi>f</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>i</mi></mrow></mtd></mtr><mtr><mtd><mrow><mn>0</mn><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mtd></mtr></mtable></mfenced></mrow></mrow>
- en: 'If the action has never been used, the denominator would be zero; to avoid
    the result being infinite, we use a default value (e.g., 0). If the number of
    steps goes to infinity, the estimated value should converge to the true value.
    Once these estimated values are obtained, we can choose the action. The easiest
    way to select an action is to choose the highest value (greedy action). The `arg
    max` function does exactly that:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动作从未被使用过，分母将为零；为了避免结果为无穷大，我们使用默认值（例如，0）。如果步数趋于无穷大，估计值应收敛到真实值。一旦获得这些估计值，我们就可以选择动作。选择动作最简单的方法是选择最大值（贪婪动作）。`arg
    max`函数正是如此：
- en: <mrow><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mi>max</mi><mi>a</mi></munder><msub><mi>Q</mi><mi>t</mi></msub><mfenced
    close=")" open="("><mi>a</mi></mfenced></mrow></mrow>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mi>max</mi><mi>a</mi></munder><msub><mi>Q</mi><mi>t</mi></msub><mfenced
    close=")" open="("><mi>a</mi></mfenced></mrow></mrow>
- en: As we said before, we don’t always want to choose greedy actions, but we want
    the model to explore other actions as well. For this, we can introduce a probability
    *ε*, so that the agent will select from the other actions with equal probability.
    In simple words, the model almost always selects the greedy action, but with a
    probability *ε*, it selects one of the other actions (regardless of its value).
    By increasing the number of steps, the other actions will also be tested (at infinity,
    they will be tested an infinite number of times) ensuring the convergence of *Q*
    to *q**. Similarly, the probability of selecting the best action converges to
    *1 - ε*. This method is called the **ε-greedy** method, and it allows for some
    balancing between exploitation and exploration.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说的，我们并不总是想选择贪婪动作，但我们希望模型也能探索其他动作。为此，我们可以引入一个概率 *ε*，这样智能体将以相等的概率从其他动作中选择。简单来说，模型几乎总是选择贪婪动作，但以概率
    *ε* 选择其他动作（无论其值如何）。通过增加步骤数，其他动作也将被测试（在无限大时，它们将被测试无限多次），确保 *Q* 收敛到 *q**。同样，选择最佳动作的概率收敛到
    *1 - ε*。这种方法被称为 **ε-greedy** 方法，它允许在利用和探索之间进行一些平衡。
- en: 'To give a simple example, we can imagine a 10-armed bandit (*k*=10) where we
    have action values *q**, where we have a normal distribution to represent the
    true value of each action. Here, we have plotted 1,000 examples:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 举一个简单的例子，我们可以想象一个有10个臂的老虎机（*k*=10），其中我们有一个动作值 *q*，我们有一个正态分布来表示每个动作的真实值。在这里，我们绘制了1,000个示例：
- en: '![Figure 8.6 – Action value distribution](img/B21257_08_06.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 动作值分布](img/B21257_08_06.jpg)'
- en: Figure 8.6 – Action value distribution
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 动作值分布
- en: In the following example, we compare different *ε*-greedy methods. The rewards
    increase with agent experience and then go to plateaus. The pure greedy method
    is suboptimal in comparison to methods that also allow exploration. Similarly,
    choosing an *ε* constant that is too high (*ε*=0.5) leads to worse results than
    a pure greedy method.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们比较了不同的 *ε*-贪婪方法。奖励随着智能体经验的增加而增加，然后达到平台期。与允许探索的方法相比，纯贪婪方法次优。同样，选择一个过高的
    *ε* 常数（*ε*=0.5）会导致比纯贪婪方法更差的结果。
- en: '![Figure 8.7 – Average rewards for time step for different greedy methods](img/B21257_08_07.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 不同贪婪方法的时间步平均奖励](img/B21257_08_07.jpg)'
- en: Figure 8.7 – Average rewards for time step for different greedy methods
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 不同贪婪方法的时间步平均奖励
- en: To investigate this phenomenon, we can look at the optimal choice by the agent
    (*Figure 8**.8*).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究这种现象，我们可以查看智能体的最优选择（*图8**.8*）。
- en: '![Figure 8.8 – Percentage optimal choice for time step for different greedy
    methods](img/B21257_08_08.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 不同贪婪方法的时间步最优选择百分比](img/B21257_08_08.jpg)'
- en: Figure 8.8 – Percentage optimal choice for time step for different greedy methods
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 不同贪婪方法的时间步最优选择百分比
- en: The greedy method chooses the optimal choice only one-third of the time, while
    a method that includes some exploration selects the optimal choice 80% of the
    time (ε=0.1)%. This result shows that an agent that can also explore the environment
    achieves better results (can recognize the optimal action), while an agent that
    is greedy in the long run will choose actions that are suboptimal. In addition,
    *ε*-greedy methods find the optimal action faster than greedy methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪方法只有三分之一的时间选择最优选择，而包含一些探索的方法有80%的时间（ε=0.1）%选择最优选择。这个结果表明，能够探索环境的智能体可以达到更好的结果（可以识别最优动作），而长期贪婪的智能体会选择次优动作。此外，*ε*-贪婪方法比贪婪方法更快地找到最优动作。
- en: In this case, we explored simple methods, where *ε* is constant. In some variants,
    *ε* decreases with the number of steps, allowing the agent to shift focus from
    exploration to exploitation once the environment has been sufficiently explored.
    ε-greedy methods work best in almost all cases, especially when there is greater
    uncertainty (e.g., greater variance) or when the system is nonstationary.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们探索了简单的方法，其中 *ε* 是常数。在某些变体中，*ε* 随着步骤数的增加而减少，允许智能体在环境被充分探索后从探索转移到利用。ε-greedy
    方法在几乎所有情况下都表现最佳，尤其是在存在更大的不确定性（例如，更大的方差）或系统非平稳时。
- en: 'The system we have seen so far is not efficient when we have a large number
    of samples. So, instead of taking the average of observed rewards, we can use
    an incremental method (the one most widely used today). For an action selected
    *i* times, the reward will be *R*i, and we can calculate the estimated value *Q*n
    as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的系统在样本数量较多时并不高效。因此，我们不是取观察到的奖励的平均值，而是可以使用一种逐步方法（目前最广泛使用的方法）。对于一个被选择*i*次的动作，奖励将是*R*i*，我们可以计算估计值*Q*n*如下：
- en: <mrow><mrow><msub><mi>Q</mi><mi>n</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>R</mi><mi>n</mi></msub></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow></mrow>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>Q</mi><mi>n</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>R</mi><mi>n</mi></msub></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow></mrow>
- en: 'At this point, we do not have to recollect the average each time but can keep
    a record of what was calculated and simply conduct an update incrementally in
    this way:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们不必每次都重新收集平均值，而是可以记录计算结果，并以这种方式逐步进行更新：
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math>
- en: 'This can be seen simply as a kind of stepwise adjustment of the expected value:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以简单地看作是一种逐步调整期望值：
- en: <mrow><mrow><mrow><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>←</mo><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>+</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mo>_</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>[</mo><mi>T</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>−</mo><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mo>]</mo></mrow></mrow></mrow>
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>←</mo><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>+</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mo>_</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>[</mo><mi>T</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>−</mo><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mo>]</mo></mrow></mrow></mrow>
- en: In fact, we can see [*Target - estimated*old] as a kind of error in the estimation
    that we are trying to correct step by step and bring closer to the real target.
    The agent is trying to move the value estimation to the real value.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以将[*目标 - 估计旧值*]视为一种我们在尝试逐步纠正并使其接近真实目标的估计误差。智能体试图将价值估计移动到真实值。
- en: 'We can test this incremental implementation, and we can see how, after an initial
    exploratory phase, the agent begins to exploit optimal choice:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以测试这种逐步实现，并看到在初始的探索阶段之后，智能体开始利用最优选择：
- en: '![Figure 8.9 – Incremental implementation](img/B21257_08_09.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – 逐步实现](img/B21257_08_09.jpg)'
- en: Figure 8.9 – Incremental implementation
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 逐步实现
- en: '*1/n* can be replaced by a fixed step size parameter *α*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*1/n*可以被替换为一个固定的步长参数*α*。'
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math>
- en: Using *α* not only simplifies the calculation but also reduces the bias inherent
    in this approach. In fact, the choice of the initial value estimation for an action,
    *Q*1(a),, can significantly influence early decisions and convergence behavior
    *α* also allows you to handle non-stationary problems better (where the reward
    probability changes over time). The initial expected values are in general set
    to 0, but choose values greater than 0\. This alternative is called the optimistic
    greedy strategy; these optimistic values stimulate the agent to explore the environment
    more (even when we use a pure greedy approach with *ε*=0). The disadvantage is
    that we have to test different values for the initial *Q*, and in practice, almost
    all practitioners set it to 0 for convenience.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*α*不仅简化了计算，还减少了这种方法固有的偏差。实际上，对于动作的初始值估计*Q*1(a)，的选择可以显著影响早期决策和收敛行为。*α*还允许你更好地处理非平稳问题（其中奖励概率随时间变化）。初始期望值通常设置为0，但可以选择大于0的值。这种替代方案称为乐观贪婪策略；这些乐观值刺激智能体更多地探索环境（即使我们使用ε=0的纯贪婪方法）。缺点是我们必须测试不同的初始*Q*值，而在实践中，几乎所有从业者都将其设置为0以方便。
- en: 'By testing the optimistic greedy method, we can see that it behaves similarly
    to the *ε*-greedy method:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过测试乐观贪婪方法，我们可以看到它的行为与*ε*-贪婪方法相似：
- en: '![Figure 8.10 – Optimistic greedy versus the ε-greedy method](img/B21257_08_10.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 乐观贪婪与ε-贪婪方法的比较](img/B21257_08_10.jpg)'
- en: Figure 8.10 – Optimistic greedy versus the ε-greedy method
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – 乐观贪婪与ε-贪婪方法的比较
- en: In non-stationary problems, *α* can be set to give greater weight to recent
    rewards than to prior ones.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在非平稳问题中，*α*可以设置为给予近期奖励比先前奖励更大的权重。
- en: 'One final note: so far we have chosen greedy actions for their higher estimated
    value. In contrast, we have chosen non-greedy actions randomly. Instead of choosing
    them randomly, we can select them based on their potential optimality and uncertainty.
    This method is called **Upper Confidence Bound** (**UCB**), where an action *A*
    is selected based on:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点：到目前为止，我们选择了具有更高估计值的贪婪动作。相比之下，我们随机选择非贪婪动作。我们不仅可以随机选择它们，还可以根据它们的潜在最优性和不确定性来选择。这种方法称为**上置信界**（**UCB**），其中动作*A*的选择基于：
- en: <mrow><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mi>max</mi><mi>a</mi></munder><mrow><mrow><mo>[</mo><msub><mi>Q</mi><mi>t</mi></msub><mfenced
    close=")" open="("><mi>a</mi></mfenced><mo>+</mo><mi>c</mi><msqrt><mfrac><mrow><mi>ln</mi><mi>t</mi></mrow><mrow><msub><mi>N</mi><mi>t</mi></msub><mo>(</mo><mi>a</mi><mo>)</mo></mrow></mfrac></msqrt><mo>]</mo></mrow></mrow></mrow></mrow>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mi>max</mi><mi>a</mi></munder><mrow><mrow><mo>[</mo><msub><mi>Q</mi><mi>t</mi></msub><mfenced
    close=")" open="("><mi>a</mi></mfenced><mo>+</mo><mi>c</mi><msqrt><mfrac><mrow><mi>ln</mi><mi>t</mi></mrow><mrow><msub><mi>N</mi><mi>t</mi></msub><mo>(</mo><mi>a</mi><mo>)</mo></mrow></mfrac></msqrt><mo>]</mo></mrow></mrow></mrow></mrow>
- en: where *ln*(*t*) represents the natural logarithm of *t*, *c> 0* controls exploration,
    and *N*t is the number of times an action *A* has been tested. This approach means
    that all actions will be tested but actions that have a lower estimate value (and
    have been tested frequently) will be chosen again in a decreasing manner. Think
    of it as choosing between different restaurants. UCB helps you balance between
    going to the one you already like (high estimated value) and trying out others
    that might be better but haven’t visited much yet (high uncertainty). Over time,
    it naturally reduces exploration of poorly performing options while continuing
    to test under-explored but potentially good ones. UCB works very well, though
    it is difficult to apply in approaches other than multi-armed bandits.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *ln*(*t*) 代表 *t* 的自然对数，*c> 0* 控制探索，而 *N*t 是动作 *A* 被测试的次数。这种方法意味着所有动作都将被测试，但那些估计值较低（并且已被频繁测试）的动作将以递减的方式再次被选择。想象一下在挑选不同的餐厅。UCB
    帮助你在去你已经喜欢的餐厅（高估计值）和尝试可能更好但尚未访问过的餐厅（高不确定性）之间取得平衡。随着时间的推移，它自然会减少对表现不佳的选项的探索，同时继续测试那些尚未充分探索但可能很好的选项。UCB
    工作得非常好，尽管它在除了多臂老虎机以外的其他方法中很难应用。
- en: 'As you can see, UCB generally gives better results:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，UCB 通常给出更好的结果：
- en: '![Figure 8.11 – UCB improvements on greedy methods](img/B21257_08_11.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – UCB 对贪婪方法的改进](img/B21257_08_11.jpg)'
- en: Figure 8.11 – UCB improvements on greedy methods
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – UCB 对贪婪方法的改进
- en: Multi-armed bandit is a classic example of RL, but it allows one to begin to
    understand how RL works.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机是强化学习的一个经典例子，但它允许人们开始理解强化学习是如何工作的。
- en: Multi-armed bandit has been used for several applications, but it is a simplistic
    system that cannot be applied to different real-world situations. For example,
    in a chess game, the goal is not to eat the chess pieces but to win the game.
    Therefore, in the next subsection, we will begin to look at methods that take
    into account a purpose farther back in time than immediate gain.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机已被用于多个应用，但它是一个过于简化的系统，不能应用于不同的现实世界情况。例如，在棋局中，目标不是吃掉棋子，而是赢得比赛。因此，在下一小节中，我们将开始探讨考虑比即时收益更早目的的方法。
- en: Markov decision processes
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: '**Markov decision processes** (**MDPs**) are problems where actions impact
    not only immediate rewards but also future outcomes. In MDPs then, delayed reward
    has much more weight than what we saw in the multi-armed bandit problem, but also
    in deciding the appropriate action for different situations.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDPs**）是动作不仅影响即时奖励，还影响未来结果的问题。在 MDPs 中，延迟奖励比我们在多臂老虎机问题中看到的权重更大，而且在决定不同情况下的适当动作时也是如此。'
- en: Imagine you are navigating a maze. Each intersection or hallway you enter is
    a state, and every turn you make is an action that changes your state. Some paths
    lead you closer to the exit (the final reward), while others might take you in
    circles or into dead ends. The reward for each move may not be immediate—you only
    get the big reward when you reach the end. So, every action you take needs to
    consider how it impacts your chances of reaching the goal later on.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在穿越迷宫。你进入的每个交叉点或走廊都是一个状态，你做出的每个转弯都是一个改变你状态的行动。一些路径会带你更接近出口（最终奖励），而其他路径可能会让你绕圈子或走到死胡同。每一步的奖励可能不是即时的——你只有在到达终点时才会得到大奖励。因此，你采取的每个行动都需要考虑它如何影响你后来达到目标的机会。
- en: 'In MDPs, this idea is formalized: the agent must decide the best action in
    each state, not just for instant rewards, but for maximizing long-term success,
    which makes them more complex than simpler problems such as the multi-armed bandit,
    where only immediate rewards are considered.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫决策过程（MDP）中，这个想法被形式化：智能体必须在每个状态下决定最佳动作，而不仅仅是即时奖励，还要最大化长期成功，这使得它们比像多臂老虎机这样的简单问题更复杂，在多臂老虎机问题中，只考虑即时奖励。
- en: Previously we were just estimating *q*(a)*. Now, we want to estimate the value
    of action *a* in the presence of state *s*, *q*(s,a)*. At each time step, the
    agent receives a representation of the environment *S*t and performs an action
    *A*t, receives a reward *R*, and moves to a new state *S*t+1\. It can be seen
    that the agent’s action can change the state of the environment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们只是估计 *q*(a)*。现在，我们想要在状态 *s* 的存在下估计动作 *a* 的价值，即 *q*(s,a)*。在每个时间步，智能体接收环境的表示
    *S*t 并执行动作 *A*t，获得奖励 *R*，并移动到新的状态 *S*t+1*。可以看出，智能体的动作可以改变环境的状态。
- en: 'In a finite MDP, the set of states, actions, and rewards contains a finite
    number of elements. The variables *R* and *S* are probability distributions that
    depend on both the previous state and the action. We can describe the dynamics
    of this system using the state-transition probability function *p(s’, r |* *s,
    a)*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在有限的MDP中，状态、动作和奖励的集合包含有限数量的元素。变量 *R* 和 *S* 是依赖于先前状态和动作的概率分布。我们可以使用状态转移概率函数 *p(s’，
    r | s, a)* 来描述这个系统的动力学：
- en: <mrow><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi>Pr</mi><mfenced
    close="|" open="{"><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>=</mo><mi>r</mi></mrow></msub></mrow></mfenced><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></mrow></mrow>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi>Pr</mi><mfenced
    close="|" open="{"><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>=</mo><mi>r</mi></mrow></msub></mrow></mfenced><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></mrow></mrow>
- en: 'In other words, state and reward depend on the previous state and the previous
    action. At each time step, the new state and reward will derive from the previous
    cycle. This will repeat a finite series of events. So, each state will summarize
    all the previous information (for example, in tic-tac-toe, the new system configuration
    gives us information about the previous moves) and is said to be a Markov state
    and possess the Markov property. The advantage of a Markov state is that each
    state possesses all the information we need to predict the future. The preceding
    function describes to us how one state evolves into another as we perform actions.
    An RL problem that respects this property is called an MDP. So, from this function,
    we can derive anything we care about the environment. We can then derive state-transition
    probabilities:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，状态和奖励取决于先前的状态和动作。在每一步，新的状态和奖励将来自前一个周期。这将重复有限的一系列事件。因此，每个状态将总结所有先前信息（例如，在井字棋中，新的系统配置给我们关于先前移动的信息），并被称为马尔可夫状态，具有马尔可夫属性。马尔可夫状态的优势在于每个状态都包含我们预测未来的所有所需信息。前面的函数描述了当我们执行动作时，一个状态如何演变到另一个状态。遵循此属性的RL问题被称为MDP。因此，从这个函数中，我们可以推导出关于环境的任何我们关心的事情。然后我们可以推导出状态转移概率：
- en: <mrow><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi>Pr</mi><mfenced
    close="|" open="{"><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow></mrow></mfenced><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mo>}</mo><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mi>r</mi><mo>∈</mo><mi>R</mi></mrow></munder><mrow><mi>p</mi><mfenced
    close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow></mrow></mrow></mrow></mrow>
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi>Pr</mi><mfenced
    close="|" open="{"><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow></mrow></mfenced><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mo>}</mo><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mi>r</mi><mo>∈</mo><mi>R</mi></mrow></munder><mrow><mi>p</mi><mfenced
    close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow></mrow></mrow></mrow></mrow>
- en: 'We can also derive the expected reward for state-action pairs:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以推导出状态-动作对的预期奖励：
- en: <mml:math display="block"><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="double-struck">E</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi
    mathvariant="double-struck">E</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mi>a</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
- en: 'And we can derive the expected rewards for state-action-next-state triples:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以推导出状态-动作-下一个状态三元组的期望奖励：
- en: <mrow><mrow><mi>r</mi><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s</mi><mi
    mathvariant="normal">ʹ</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    close="]" open="["><mrow><msub><mi>R</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mo>,</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mi>r</mi><mo>∈</mo><mi>R</mi></mrow></munder><mrow><mi>r</mi><mfrac><mrow><mi>p</mi><mfenced
    close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow><mrow><mi>p</mi><mfenced
    close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow></mrow>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mi>r</mi><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s</mi><mi
    mathvariant="normal">ʹ</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    close="]" open="["><mrow><msub><mi>R</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mml:mi><mo>,</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mi>r</mi><mo>∈</mml:mi>R</mml:mi></mrow></munder><mrow><mi>r</mi><mfrac><mrow><mi>p</mi><mfenced
    close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow><mrow><mi>p</mi><mfenced
    close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow></mrow>
- en: 'This shows us the flexibility of this framework. A brief note is that *t* does
    not need to be time steps but a sequence of states (a series of decisions, movements
    of a robot, and so on) making MDP a flexible system. After all, in MDPs, every
    problem can be reduced to three signals: actions, states, and rewards. This allows
    us a better abstraction to represent goal-oriented learning.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了该框架的灵活性。简而言之，*t* 不一定是时间步，而是一系列状态（一系列决策、机器人的移动等），这使得MDP成为一个灵活的系统。毕竟，在MDP中，每个问题都可以归结为三个信号：动作、状态和奖励。这使我们能够更好地抽象化，以表示以目标为导向的学习。
- en: The goal for an agent is clearly to maximize cumulative reward over a long period
    of time (rather than immediate gain). This system has been shown to be very flexible
    because much of the problem can be formalized in this way (one just has to find
    a way to define what the rewards are so that the agent learns how to maximize
    them). One note is that agents try to maximize the reward in any way possible;
    if the goal is not well defined, it can lead to unintended results (e.g., in chess,
    the goal is to win the game; if the reward is to eat a piece and not to win the
    game, the agent will try to maximize the pieces eaten even when it might lead
    to losing the game).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个智能体来说，其目标显然是在长时间内最大化累积奖励（而不是立即收益）。这个系统已被证明非常灵活，因为许多问题都可以用这种方式形式化（只需要找到一种定义奖励的方法，以便智能体学会如何最大化它们）。有一点需要注意的是，智能体会尝试以任何可能的方式最大化奖励；如果目标定义不明确，可能会导致意想不到的结果（例如，在棋类游戏中，目标是赢得游戏；如果奖励是吃掉一个棋子而不是赢得游戏，那么智能体将尝试最大化吃掉的棋子数量，即使这可能导致输掉游戏）。
- en: 'This can be better expressed formally, where *G* is the cumulative sum of rewards
    received from step *t* and later:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以更正式地表达，其中*G*是从步骤*t*开始并在此之后收到的累积奖励总和：
- en: <mrow><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>+</mo><msub><mi>R</mi><mi>T</mi></msub></mrow></mrow>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mrow><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>+</mo><msub><mi>R</mi><mi>T</mi></msub></mrow></mrow>`'
- en: 'Our goal is thus to maximize *G*. This is easier to define for a sequence where
    we clearly must have an end (e.g., a game where only a defined number of moves
    can be made). A defined sequence of steps is called an episode, and the last state
    is called a terminal state. Note that each episode is independent of the other
    (losing one game does not affect the outcome of the next). This is, of course,
    not always possible; there are also definite continuous tasks where there is no
    net end (a robot moving in an environment), for which the previous equation does
    not work. In this case, we can use a so-called discount rate *γ*. This parameter
    allows us to decide the agent’s behavior: when *γ* is near 0, the agent will try
    to maximize immediate rewards, while approaching 1, the agent considers future
    rewards with greater weight:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的目标是最大化*G*。对于有明确结束的序列来说，这更容易定义（例如，只能进行有限次移动的游戏）。一系列定义明确的步骤被称为一个回合，最后一个状态被称为终止状态。请注意，每个回合都是相互独立的（输掉一局不会影响下一局的结果）。当然，这并不总是可能的；还有一些明确的连续任务，其中没有净结束（例如，在环境中移动的机器人），对于这些任务，前面的方程不适用。在这种情况下，我们可以使用所谓的折现率*γ*。这个参数允许我们决定智能体的行为：当*γ*接近0时，智能体将尝试最大化即时奖励，而当*γ*接近1时，智能体将更加重视未来的奖励：
- en: <mrow><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi
    mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow></mrow>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mrow><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi
    mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow></mrow>`'
- en: 'As we saw in the *multi-armed bandit problem* section, we can estimate value
    functions (how good it is for an agent to be in a given state). The value of a
    state *S* considering a policy *π*, is the expected return starting from *S* and
    following the policy *π* from that time:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*多臂老虎机问题*部分所看到的，我们可以估计价值函数（一个智能体处于某个状态的好坏）。考虑策略*π*，状态*S*的价值是从*S*开始并遵循策略*π*的期望回报：
- en: <mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi
    mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi
    mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow>
- en: 'This is called the state-value function for the policy π, and *G*t is the expected
    return. Similarly, we can define the value of taking action *A* in state *S* under
    policy *π*:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为策略 π 的状态-价值函数，*G*t 是期望回报。同样，我们可以在策略 π 下定义在状态 S 中采取动作 *A* 的价值：
- en: <mrow><mrow><msub><mi>q</mi><mi>π</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi
    mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced></mrow></mrow>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>q</mi><mi>π</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi
    mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced></mrow></mrow>
- en: This is called the action-value function for policy *π*. We can estimate these
    functions by experience (interaction with the environment), and at infinity, they
    should approach the true value. Methods like this where we conduct averaging of
    many random samples of actual returns are called Monte Carlo methods.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为策略 *π* 的动作-价值函数。我们可以通过经验（与环境交互）来估计这些函数，并且在无限远处，它们应该接近真实值。这种进行许多实际回报随机样本平均的方法被称为蒙特卡洛方法。
- en: 'For efficiency, we can rewrite it in a recursive form (using discounting):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，我们可以将其重写为递归形式（使用折现）：
- en: <mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mi>G</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mi>a</mi></munder><mrow><mi>π</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mfenced
    close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mfenced
    close="]" open="["><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mfenced
    close=")" open="("><mrow><mi>s</mi><mo>′</mo></mrow></mfenced></mrow></mfenced></mrow></mrow><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mi>G</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mi>a</mi></munder><mrow><mi>π</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mfenced
    close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mfenced
    close="]" open="["><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mfenced
    close=")" open="("><mrow><mi>s</mi><mo>′</mo></mrow></mfenced></mrow></mfenced></mrow></mrow><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow>
- en: This simplified form is called the Bellman equation. This can be represented
    as thinking forward to the next state from the previous state. From a state with
    a policy, we choose an action and get a reward (or not) with a given probability.
    The Bellman equation conducts the average of these probabilities, giving a weight
    to the possibility of their occurrence. This equation is the basis of many great
    RL algorithms.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简化的形式被称为 Bellman 方程。这可以表示为从上一个状态向前思考下一个状态。从一个具有策略的状态，我们选择一个动作，并按照给定的概率获得奖励（或不获得）。Bellman
    方程对这些概率进行平均，给它们发生的可能性赋予权重。这个方程是许多优秀强化学习算法的基础。
- en: '![Figure 8.12 – Bellman backup diagram](img/B21257_08_12.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – Bellman 回退图](img/B21257_08_12.jpg)'
- en: Figure 8.12 – Bellman backup diagram
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – Bellman 回退图
- en: Now that we have both state-value functions *vπ(s)* and action-value functions
    *qπ(s, a)*, we can evaluate policies and choose the best ones. Action-value functions
    allow us to choose the best action relative to the state. Consider, for example,
    a case of a Texas Hold’em poker game. A player has $100 and must choose the strategy
    starting from the state *π*. The strategy *π*1 has a state value function that
    returns 10, while *π*2 has a return of -2\. This means that the first strategy
    brings an expected gain of 10, while *π*2 brings an expected loss of 2\. Given
    a state *s*, the player wants to figure out which action to choose. For example,
    choosing whether to bet 10 or 5, *q*π (s, a) tells us what the expected cumulative
    reward is from this action. So, the preceding equations allow us to figure out
    which action or strategy to choose to maximize the reward.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们既有状态值函数 *vπ(s)* 和动作值函数 *qπ(s, a)*，我们可以评估策略并选择最好的策略。动作值函数允许我们根据状态选择最佳动作。例如，考虑德克萨斯扑克游戏的情况。一个玩家有
    100 美元，必须从状态 *π* 出发选择策略。策略 *π*1 的状态值函数返回 10，而 *π*2 的回报为 -2。这意味着第一个策略带来期望收益 10，而
    *π*2 带来期望损失 2。给定一个状态 *s*，玩家想要弄清楚选择哪个动作。例如，选择是否下注 10 或 5，*q*π (s, a) 告诉我们从这个动作中期望累积奖励是多少。因此，前面的方程使我们能够确定选择哪个动作或策略以最大化奖励。
- en: 'From *Figure 8**.12*, it can be understood that solving an RL task means finding
    an optimal policy that succeeds in collecting many rewards over the long run.
    For MDPs, it is possible to define an optimal policy because we can evaluate whether
    one policy is better than another if it has a higher expected return for all states
    *vπ(s)*. *π** denotes the optimal policy and is the one that has the maximum value
    function over all possible policies:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从**图8.12**可以理解，解决强化学习任务意味着找到一种最优策略，在长期内成功收集到许多奖励。对于马尔可夫决策过程（MDPs），我们可以定义一个最优策略，因为我们可以在所有状态下评估一个策略是否比另一个策略更好，如果它对所有状态的预期回报更高。*π**
    表示最优策略，它是所有可能策略中具有最大值函数的那个：
- en: <mrow><mrow><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mi>s</mi></mfenced><mo>=</mo><munder><mi>max</mi><mi>π</mi></munder><msub><mi>v</mi><mi>π</mi></msub><mfenced
    close=")" open="("><mi>s</mi></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mi>s</mi></mfenced><mo>=</mo><munder><mi>max</mi><mi>π</mi></munder><msub><mi>v</mi><mi>π</mi></msub><mfenced
    close=")" open="("><mi>s</mi></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi}s</mi><mo>∈</mo><mi>S</mi></mrow></mrow>
- en: 'The optimal policies share the same optimal action-value function *q**, which
    is defined as the maximum action-value function over all possible policies:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略共享相同的最佳动作值函数 *q**，它被定义为所有可能策略中最大动作值函数：
- en: <mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><munder><mi>max</mi><mi>π</mi></munder><msub><mi>q</mi><mi>π</mi></msub><mfenced
    close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><munder><mi>max</mi><mi>π</mi></munder><msub><mi>q</mi><mi>π</mi></msub><mfenced
    close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi}s</mi><mo>∈</mo><mi>S</mi></mrow></mrow>
- en: 'The relationship between these two functions can be summarized as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数之间的关系可以总结如下：
- en: <mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi
    mathvariant="normal">*</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced></mrow></mrow>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi
    mathvariant="normal">*</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced></mrow></mrow>
- en: 'The equation expresses the cumulative return given a state-action pair. Optimal
    value functions are an ideal state in RL, though, and it is difficult to find
    optimal policies, especially when tasks are complex or computationally expensive.
    RL therefore tries to approximate them, for example, by using **dynamic programming**
    (**DP**). The purpose of DP is to use value functions to search for good policies
    (even if not exact solutions). At this point, we can derive Bellman optimality
    equations for the optimal state-value function *v*(s)* and the optimal action-value
    function *q*∗ *(**s, a)*:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程表示给定一个状态-动作对时的累积回报。然而，在强化学习（RL）中，最优值函数是一个理想的状态，但找到最优策略尤其困难，尤其是在任务复杂或计算成本高昂时。因此，强化学习试图通过使用**动态规划**（**DP**）等方法来近似它们。动态规划的目的就是利用值函数来寻找好的策略（即使不是精确解）。在这个阶段，我们可以推导出最优状态值函数
    *v*(s)* 和最优动作值函数 *q*∗ *(**s, a)* 的贝尔曼最优方程：
- en: <mrow><mrow><mrow><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mfenced
    close=")" open="("><mi>s</mi></mfenced><mo>=</mo><munder><mi>max</mi><mi>a</mi></munder><mi
    mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi
    mathvariant="normal">*</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><munder><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mi>a</mi></munder><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi
    mathvariant="normal">*</mi></msub><mo>+</mo><mi>s</mi><mi mathvariant="normal">ʹ</mi><mo>]</mo></mrow></mrow></mrow>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mfenced
    close=")" open="("><mi>s</mi></mfenced><mo>=</mo><munder><mi>max</mi><mi>a</mi></munder><mi
    mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi
    mathvariant="normal">*</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><munder><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mi>a</mi></munder><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi
    mathvariant="normal">*</mi></msub><mo>+</mo><mi>s</mi><mi mathvariant="normal">ʹ</mi><mo>]</mo></mrow></mrow></mrow>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><munder><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mi
    mathvariant="normal">ʹ</mi></mrow></munder><msup><mi>q</mi><mi mathvariant="normal">*</mi></msup><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>a</mi><mi
    mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mfenced
    close="]" open="["><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mrow><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mo>′</mo></mrow></munder><msup><mi>q</mi><mi
    mathvariant="normal">*</mi></msup><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>a</mi><mi
    mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mfenced></mrow></mrow>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><munder><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mi
    mathvariant="normal">ʹ</mi></mrow></munder><msup><mi>q</mi><mi mathvariant="normal">*</mi></msup><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>a</mi><mi
    mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mfenced
    close="]" open="["><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mrow><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mo>′</mo></mrow></munder><msup><mi>q</mi><mi
    mathvariant="normal">*</mi></msup><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>a</mi><mi
    mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mfenced></mrow></mrow>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")"
    open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><munder><mrow><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mi
    mathvariant="normal">ʹ</mi></mrow></munder><msup><mi>q</mi><mi mathvariant="normal">*</mi></msup><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>a</mi><mi
    mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mfenced
    close="]" open="["><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mrow><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mo>′</mo></mrow></munder><msup><mi>q</mi><mi
    mathvariant="normal">*</mi></msup><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>a</mi><mi
    mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mfenced></mrow></mrow>
- en: For finite MDP, Bellman optimality equations have only one solution, and they
    can be solved if we know the dynamics of the system. Once we get *v**, it is easy
    to identify the optimal policy *q**; having an optimal *q**, we can identify the
    optimal actions. The beauty of *v** is that it allows us to choose the best actions
    at the moment while still taking into account the long-term goal. Solving these
    equations for a problem is solving the problem through RL. On the other hand,
    for many problems, solving them means calculating all possibilities and thus would
    be too computationally expensive. In other cases, we do not know the dynamics
    of the environment with certainty or the states do not have Markov properties.
    However, these equations are the basis of RL, and many methods are approximations
    of these equations, often using experience from previous states. So, these algorithms
    do not identify the best policy but an approximation. For example, many algorithms
    learn optimal actions for the most frequent states but may choose suboptimal actions
    for infrequent or rare states. The trick is that these choices should not impact
    the future amount of reward. For example, an agent might still win a game even
    if it does not make the best move in rare situations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有限马尔可夫决策过程（MDP），贝尔曼最优性方程只有一个解，如果我们知道系统的动态特性，那么它们是可以被解决的。一旦我们得到 *v*，就很容易识别出最优策略
    *q*；拥有最优的 *q*，我们就可以识别出最优动作。*v* 的美妙之处在于它允许我们在考虑长期目标的同时选择最佳动作。为一个问题求解这些方程就是通过强化学习（RL）来解决问题。另一方面，对于许多问题，解决它们意味着计算所有可能性，因此计算成本过高。在其他情况下，我们可能无法确定环境的动态特性，或者状态可能不具有马尔可夫性质。然而，这些方程是强化学习的基础，许多方法都是这些方程的近似，通常使用先前状态的经验。因此，这些算法并不识别最佳策略，而是识别一个近似。例如，许多算法学习最频繁状态的最优动作，但可能对不常见或罕见的状态选择次优动作。技巧是这些选择不应影响未来的奖励量。例如，即使代理在罕见情况下没有做出最佳动作，它仍然可能赢得游戏。
- en: 'DP refers to a collection of algorithms that are used to compute the best policy
    given a perfect model of the environment as an MDP. Now, these algorithms require
    a lot of computation and the assumption of the perfect model does not always hold.
    So, these algorithms are not practically used anymore; at the same time, one can
    define today’s algorithms as inspired by DP algorithms, with the purpose of reducing
    computation and working even when the assumption of a perfect model of the environment
    does not hold. DP algorithms, in short, are obtained from transforming Bellman
    equations into update rules to improve the approximation of desired value functions.
    This allows value functions to be used to organize the search for good policies.
    To evaluate a policy, we can use the state-value function and evaluate the expected
    return when following policy *π* from each state:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: DP 指的是一组算法，这些算法用于在给定环境的完美模型作为马尔可夫决策过程（MDP）的情况下计算最佳策略。现在，这些算法需要大量的计算，并且完美模型的假设并不总是成立。因此，这些算法不再实际使用；同时，今天可以将算法定义为受
    DP 算法启发，目的是减少计算量，即使在环境完美模型的假设不成立的情况下也能工作。简而言之，DP 算法是通过将贝尔曼方程转换为更新规则来改进所需价值函数的近似而获得的。这允许使用价值函数来组织对良好策略的搜索。为了评估策略，我们可以使用状态价值函数，并评估遵循策略
    *π* 从每个状态预期的回报：
- en: <mrow><mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><mi
    mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mi>a</mi></munder><mrow><mi>π</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>)</mo><mo>]</mo></mrow></mrow></mrow>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><mi
    mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mi>a</mi></munder><mrow><mi>π</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>)</mo><mo>]</mo></mrow></mrow></mrow>
- en: 'Calculating the value function for a policy aims to identify better policies.
    For a state *s*, we want to know whether we should keep that policy, improve it,
    or choose another. Remember that the choice of a policy decides what actions an
    agent will take. To answer the question “is it better to change policy?”, we can
    consider what happens if we choose an action in a state *s* following policy *π*:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 计算策略的价值函数旨在识别更好的策略。对于一个状态 *s*，我们想知道是否应该保持该策略，改进它，或者选择另一个策略。记住，策略的选择决定了智能体将采取哪些行动。为了回答“改变策略是否更好？”这个问题，我们可以考虑在状态
    *s* 下遵循策略 *π* 选择一个动作会发生什么：
- en: <mrow><mrow><mrow><msub><mi>q</mi><mi>π</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi
    mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>)</mo><mo>]</mo></mrow></mrow></mrow>
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><msub><mi>q</mi><mi>π</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi
    mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>)</mo><mo>]</mo></mrow></mrow></mrow>
- en: A better policy *π’* should provide us with a better value of *v*π*(s)*. If
    *π’* is less than or equal to *v*π*(s)*, we can continue the same policy. In other
    words, choosing actions according to a policy *π*’ that has better *v*π*(s)* is
    more beneficial than another policy *π*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的策略 *π’* 应该为我们提供更好的 *v*π*(s)* 值。如果 *π’* 小于或等于 *v*π*(s)*，我们可以继续相同的策略。换句话说，根据具有更好
    *v*π*(s)* 的策略 *π’* 来选择动作比另一个策略 *π* 更有益。
- en: In this section, we have seen classic RL algorithms, but none of them use a
    neural network or other machine learning model. These algorithms work well for
    simple cases, while for more complex situations we want a more sophisticated and
    adaptable system. In the next section, we will see how we can integrate neural
    networks into RL algorithms.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了经典的强化学习算法，但它们都没有使用神经网络或其他机器学习模型。这些算法在简单情况下表现良好，而对于更复杂的情况，我们希望有一个更复杂和适应性强的系统。在下一节中，我们将看到如何将神经网络集成到强化学习算法中。
- en: Deep reinforcement learning
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: '**Deep reinforcement learning** (**deep RL**) is a subfield of RL that combines
    RL with deep learning. In other words, the idea behind it is to exploit the learning
    capabilities of a neural network to solve RL problems. In traditional RL, policies
    and value functions are represented by simple functions. These methods work well
    with low-dimensional state and action spaces (i.e., when the environment and agent
    can be easily modeled). When the environment becomes more complex or larger, traditional
    methods fail to generalize. In deep RL, instead, policies and value functions
    are represented by neural networks. A neural network can theoretically represent
    any complex function (Universal Approximation Theorem), and this allows deep RL
    methods to solve problems with high-dimensional state spaces (such as those presenting
    images, videos, or continuous tasks). Modeling complex functions thus allows the
    agent to learn a more generalized and flexible policy that is needed in complex
    situations where defining a function is impossible with traditional methods. This
    learning capability has enabled deep RL methods to solve video games, move robots,
    and more.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度强化学习**（**深度强化学习**）是强化学习的一个子领域，它将强化学习与深度学习相结合。换句话说，其背后的思想是利用神经网络的学习能力来解决强化学习问题。在传统的强化学习中，策略和价值函数由简单的函数表示。这些方法在低维状态和动作空间中表现良好（即当环境和代理可以轻松建模时）。当环境变得更加复杂或更大时，传统方法无法泛化。在深度强化学习中，策略和价值函数由神经网络表示。理论上，神经网络可以表示任何复杂的函数（通用逼近定理），这使得深度强化学习方法能够解决具有高维状态空间的问题（如呈现图像、视频或连续任务）。通过建模复杂函数，这使得代理能够学习到更通用和灵活的策略，这在复杂情况下是必要的，在这些情况下，使用传统方法定义函数是不可能的。这种学习能力使得深度强化学习方法能够解决视频游戏、移动机器人等问题。'
- en: '![Figure 8.13 – Overview of deep RL (https://arxiv.org/abs/1708.05866)](img/B21257_08_13.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – 深度强化学习概述](https://arxiv.org/abs/1708.05866)(img/B21257_08_13.jpg)'
- en: Figure 8.13 – Overview of deep RL ([https://arxiv.org/abs/1708.05866](https://arxiv.org/abs/1708.05866))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – 深度强化学习概述 ([https://arxiv.org/abs/1708.05866](https://arxiv.org/abs/1708.05866))
- en: In the upcoming subsections, we will discuss how to classify these algorithms
    and what the differences are.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将讨论如何对这些算法进行分类以及它们之间的区别。
- en: Model-free versus model-based approaches
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无模型与基于模型的比较方法
- en: 'There are so many methods of deep RL today that it is difficult to make a taxonomy
    of these models. Nevertheless, deep RL methods can be broadly divided into two
    main groups: model-free and model-based. This division is represented by the answer
    to this question: does the agent have access to (or learn) a model of the environment?'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 目前深度强化学习的方法如此之多，以至于很难对这些模型进行分类。尽管如此，深度强化学习方法可以大致分为两大类：无模型和基于模型。这种划分可以通过回答以下问题来表示：智能体是否有权访问（或学习）环境的模型？
- en: '**Model-free methods**: These methods determine the optimal policy or value
    function without building a model of the environment. These models learn directly
    from observed states, actions, and rewards. The agent learns directly from trial
    and error, receives feedback from the environment, and uses this feedback to improve
    its policy or value estimation. These approaches are usually easier to implement
    and conduct parameter tuning (they only require observing state-action-reward
    sequences or transitions). They are also more easily scalable and less computationally
    complex.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无模型方法**：这些方法在不需要构建环境模型的情况下确定最优策略或价值函数。这些模型直接从观察到的状态、动作和奖励中学习。智能体直接从试错中学习，从环境中获得反馈，并使用这些反馈来改进其策略或价值估计。这些方法通常更容易实现和进行参数调整（它们只需要观察状态-动作-奖励序列或转换）。它们也更容易扩展，计算复杂度更低。'
- en: '**Model-based methods**: These methods rely on an internal model of the environment
    to predict future states and rewards given any state-action pair. This model can
    be learned or predefined before training. Having the model allows the agent to
    have similar outcomes and plan actions for future scenarios (e.g., what the future
    actions of an opponent in a game will be and anticipating them). Model-based approaches
    have the advantage that they can reduce interaction with the real environment
    and are better at planning complex tasks. Potentially improved performance comes
    at the cost of increased complexity (building an accurate model of the environment
    can be challenging, especially for high-dimensional environments) and increased
    computational cost.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的方法**：这些方法依赖于环境内部模型来预测给定任何状态-动作对的未来状态和奖励。这个模型可以在训练前学习或预先定义。拥有模型允许智能体在类似情况下进行规划和为未来场景（例如，游戏中对手的未来动作以及预测它们）制定计划。基于模型的方法的优势在于它们可以减少与真实环境的交互，并且在规划复杂任务方面表现更好。潜在的改进性能是以增加复杂性（构建准确的环境模型可能具有挑战性，尤其是在高维环境中）和增加计算成本为代价的。'
- en: '![Figure 8.14 – Model-free versus model-based approaches](img/B21257_08_14.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14 – 无模型与基于模型的比较方法](img/B21257_08_14.jpg)'
- en: Figure 8.14 – Model-free versus model-based approaches
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – 无模型与基于模型的比较方法
- en: The primary advantage of a model-based RL method lies in its ability to plan
    and think ahead. By utilizing a model (in general, a neural network) to simulate
    the dynamics of the environment, it can predict future scenarios, making it particularly
    useful in complex environments or situations where decisions need to consider
    long-term outcomes. For instance, when rewards are sparse or delayed—such as in
    chess, where the reward is achieved only by winning the game—the model can simulate
    various paths to optimize the agent’s strategy for reaching the reward.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型强化学习方法的主要优势在于其规划和前瞻性思考的能力。通过利用模型（通常是一个神经网络）来模拟环境的动态，它可以预测未来场景，这在复杂环境或需要考虑长期结果的情况下特别有用。例如，当奖励稀疏或延迟——如在国际象棋中，只有赢得游戏才能获得奖励——模型可以模拟各种路径以优化智能体达到奖励的策略。
- en: Planning also proves advantageous in dynamic environments. The model can update
    its internal representation quickly, allowing the agent to adapt its policy without
    relearning from scratch. This minimizes the need for extensive retraining, as
    seen in applications such as autonomous driving, where the agent can adjust its
    strategy without requiring large new datasets. The insights gained from such planning
    can then be distilled into a learned policy, enhancing the agent’s performance
    over time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态环境中，规划也证明是有益的。模型可以快速更新其内部表示，使智能体能够调整其策略而无需从头开始重新学习。这最小化了大量重新训练的需求，如在自动驾驶等应用中，智能体可以调整其策略而无需大量新的数据集。从这种规划中获得的认识可以随后被提炼成学习策略，随着时间的推移提高智能体的性能。
- en: Additionally, simulating interactions with the environment reduces the need
    for extensive real-world exploration, which is critical in scenarios where interactions
    are costly, dangerous, or time-intensive, such as in robotics or autonomous vehicles.
    By leveraging its internal model, the agent can prioritize actions and refine
    its exploration process to update or improve its understanding of the environment
    more efficiently.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模拟与环境交互减少了大量现实世界探索的需求，这在交互成本高、危险或耗时的情况下至关重要，例如在机器人或自动驾驶汽车中。通过利用其内部模型，智能体可以优先考虑行动并优化其探索过程，以更有效地更新或改进其对环境的理解。
- en: This then helps the agent optimize for long-term goals because it can simulate
    the long-term consequences of its actions, monitor its progress toward a more
    distant horizon, and align its actions with a distant goal.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于智能体优化长期目标，因为它可以模拟其行动的长期后果，监控其向更远目标的进展，并使其行动与遥远的目标保持一致。
- en: Model building can be a complicated task. However, a ground-truth model of the
    environment is not always available to the agent. In this case, the agent is forced
    to learn only from experience to create its own model. This can then lead to bias
    in the agent’s model. An agent might therefore perform optimally with respect
    to a learned model but perform terribly (or suboptimally) in the real environment.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建可能是一项复杂的任务。然而，环境的确切模型并不总是对智能体可用。在这种情况下，智能体被迫仅从经验中学习以创建自己的模型。这可能导致智能体模型中的偏差。因此，智能体可能在学习的模型上表现最优，但在真实环境中表现极差（或次优）。
- en: On-policy versus off-policy methods
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在策略方法和离策略方法之间
- en: 'Another important classification in RL is how models learn from experience
    and whether they learn from the current policy or a different one (they are classified
    according to the relationship between the policy and the policy update):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，另一个重要的分类是模型如何从经验中学习，以及它们是否从当前策略或不同策略中学习（它们根据策略与策略更新的关系进行分类）：
- en: '**On-policy methods**: These methods learn from actions learned from the agent’s
    current policy (so the agent both collects data and learns from the same policy).
    On-policy methods evaluate and improve the policy used to make decisions; this
    is based on actions taken and rewards received while following the current policy
    (the agent conducts the policy update by directly evaluating and improving the
    policy). The agent therefore does not use data from other policies. The advantages
    are that the agent tends to be more stable and less prone to variance (the optimized
    policy is, in fact, the one used to interact with the environment). On-policy
    methods are inefficient because they discard data that is obtained from previous
    policies (sample inefficiency), limiting their use for complex environments since
    they would require large amounts of data. In addition, these methods are not very
    exploratory and therefore less beneficial where more exploration is required (they
    are favored for environments that are stable). An example is a chatbot that learns
    to give better answers to user questions: the chatbot uses a specific policy to
    give answers and optimizes this policy by leveraging feedback received from users.
    On-policy methods ensure that the learned policy is linked to actions taken by
    the chatbot and by real interactions with users (this ensures stability).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略学习方法**：这些方法从代理当前策略中学习到的动作中进行学习（因此代理既收集数据又从同一策略中学习）。策略学习方法评估和改进用于决策的策略；这是基于遵循当前策略时采取的动作和收到的奖励（代理通过直接评估和改进策略来执行策略更新）。因此，代理不使用来自其他策略的数据。其优点是代理往往更稳定，且不太容易受到变化的影响（优化的策略实际上是用来与环境交互的策略）。策略学习方法效率不高，因为它们丢弃了从先前策略中获得的数据（样本效率低下），这限制了它们在复杂环境中的应用，因为它们需要大量的数据。此外，这些方法不太具有探索性，因此在需要更多探索的地方不太有益（它们更适用于稳定的环境）。一个例子是聊天机器人，它学习给出更好的答案来回答用户的问题：聊天机器人使用特定的策略来给出答案，并通过利用从用户那里收到的反馈来优化这个策略。策略学习方法确保学习到的策略与聊天机器人采取的动作以及与用户的真实互动相关联（这确保了稳定性）。'
- en: '**Off-policy methods**: Off-policy methods learn the value of the optimal policy
    independently of the agent’s actions (agents learn from experiences that are generated
    by a different policy from the one used for learning). So, these methods can learn
    from past data or data that is generated by other policies. Off-policy methods
    separate the behavior policy (used to collect data) from the target policy (the
    policy being learned). In other words, the behavior policy is used to explore
    the environment while the target policy is used to improve the agent’s performance
    (to ensure more exploratory behavior while learning an optimal target policy).
    Off-policy methods have higher sample efficiency because they can reuse data and
    allow for better exploration, which can lead to faster convergence to an optimal
    policy. On the other hand, they are less stable (because they do not learn from
    actions that have been taken by the current policy, the discrepancy between behavior
    policy and target policy can lead to higher variance in updates) and can be much
    more complex. An example is a music recommendation system that suggests new titles
    to users and has to explore different genres and new releases. The behavior policy
    encourages exploration and thus generates data on user preferences, while the
    target policy seeks to optimize recommendation performance for users. Separating
    the two policies thus allows experimenting with different recommendation strategies
    without compromising the quality of the final recommendations. The advantage of
    these methods is that they allow extensive exploration, which is very useful for
    complex and dynamic environments.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离策略方法**：离策略方法独立于智能体的动作学习最优策略的价值（智能体从由不同策略生成并由学习策略使用的经验中学习）。因此，这些方法可以从过去的数据或由其他策略生成的数据中学习。离策略方法将行为策略（用于收集数据）与目标策略（正在学习策略）分开。换句话说，行为策略用于探索环境，而目标策略用于提高智能体的性能（在学习最优目标策略的同时确保更多的探索行为）。离策略方法具有更高的样本效率，因为它们可以重用数据并允许更好的探索，这可以导致更快地收敛到最优策略。另一方面，它们不太稳定（因为它们不学习当前策略已经采取的动作，行为策略和目标策略之间的差异可能导致更新中的更高方差）并且可能更加复杂。一个例子是音乐推荐系统，它向用户推荐新的标题，并必须探索不同的流派和新发行的音乐。行为策略鼓励探索，因此生成关于用户偏好的数据，而目标策略寻求优化用户的推荐性能。将这两种策略分开可以尝试不同的推荐策略，而不会影响最终推荐的品质。这些方法的优势在于它们允许广泛的探索，这对于复杂和动态环境非常有用。'
- en: '![Figure 8.15 – On-policy and off-policy methods](img/B21257_08_15.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图8.15 – 在策略和离策略方法](img/B21257_08_15.jpg)'
- en: Figure 8.15 – On-policy and off-policy methods
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – 在策略和离策略方法
- en: In the next subsection, we will begin to go into detail about how deep RL works.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个子节中，我们将开始详细讲解深度强化学习是如何工作的。
- en: Exploring deep RL in detail
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 详细探索深度强化学习
- en: We’ll begin with a definition to better understand deep RL. A state *s* in a
    system is usually a vector, matrix, or other tensor. At each time step *t*, we
    can describe the environment in the form of a tensor (e.g., the position of the
    pieces on a chessboard can be represented by a matrix). Similarly, the actions
    *a* an agent can choose can be represented in a tensor (for example, each action
    can be associated with a one-hot vector, a matrix, and so on). All of these are
    data structures that are already commonly seen in machine learning and that we
    can use as input to a deep learning model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从定义开始，以便更好地理解深度强化学习。在一个系统中的状态 *s* 通常是一个向量、矩阵或其他张量。在每个时间步 *t*，我们可以将环境描述为张量的形式（例如，棋盘上棋子的位置可以用矩阵表示）。同样，智能体可以选择的动作
    *a* 也可以用张量表示（例如，每个动作可以关联到一个独热向量、一个矩阵等）。所有这些都是机器学习中已经常见的结构，我们可以将它们作为深度学习模型的输入。
- en: So far, we have discussed policies generically, but what functions do we use
    to model them? Very often, we use neural networks. So, in this section, we will
    actively look at how a neural network can be used in RL algorithms. What we’ll
    see now is based on what we saw in this chapter, but we’ll use a neural network
    to decide what action to take (instead of just a function). As we saw in [*Chapter
    1*](B21257_01.xhtml#_idTextAnchor014), a neural network is constituted of a series
    of neurons organized in a series of layers. Neural networks take a tensor as input
    and produce a tensor as output. In this case, the output of the neural network
    is the choice of an action. Optimizing the policy, in this case, means optimizing
    the parameters of the neural network. An RL algorithm based on experience can
    change the parameters of the policy function so that it produces better results.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们泛泛地讨论了策略，但我们使用什么函数来建模它们？通常，我们使用神经网络。因此，在本节中，我们将积极探讨神经网络如何在强化学习算法中使用。我们现在看到的是基于本章的内容，但我们将使用神经网络来决定采取什么行动（而不仅仅是函数）。正如我们在[*第一章*](B21257_01.xhtml#_idTextAnchor014)中看到的，神经网络由一系列按层次组织在一起的神经元组成。神经网络以张量作为输入并产生张量作为输出。在这种情况下，神经网络的输出是选择一个动作。在这种情况下，优化策略意味着优化神经网络的参数。基于经验的强化学习算法可以改变策略函数的参数，使其产生更好的结果。
- en: '![Figure 8.16 – Neural network as an RL policy](img/B21257_08_16.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.16 – 作为强化学习策略的神经网络](img/B21257_08_16.jpg)'
- en: Figure 8.16 – Neural network as an RL policy
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 – 作为强化学习策略的神经网络
- en: Neural networks are well-known deep learning models, and we know how to optimize
    them. Using gradient-based methods allows us to understand how a change in parameters
    impacts the outcome of a function. In this case, we want to know how we should
    update the parameters of our policy *P* (the neural network model) so that we
    collect more rewards in the future. So, having a function that tells us what the
    expected rewards are for a policy, we can use the gradient to change the parameters
    of the policy and thus maximize the return.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是众所周知的深度学习模型，我们了解如何优化它们。使用基于梯度的方法使我们能够理解参数变化如何影响函数的输出。在这种情况下，我们想知道我们应该如何更新我们的策略
    *P*（神经网络模型）的参数，以便在将来收集更多的奖励。因此，有一个函数告诉我们策略的预期奖励，我们可以使用梯度来改变策略的参数，从而最大化回报。
- en: 'Using a neural network as a policy in RL has several advantages:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中使用神经网络作为策略有几个优点：
- en: Neural networks are highly expressive function approximators, so they can map
    complex nonlinear relationships between inputs (states) and outputs (actions).
    This is very useful for complex environments, such as playing video games or controlling
    robots in 3D environments. In addition, neural networks scale well for environments
    that have large and complex state and action spaces.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是高度表达的功能逼近器，因此它们可以映射输入（状态）和输出（动作）之间的复杂非线性关系。这对于复杂环境非常有用，例如玩电子游戏或控制3D环境中的机器人。此外，神经网络对于具有大而复杂的状态和动作空间的环境具有良好的扩展性。
- en: Neural networks possess the ability to generalize to situations they have not
    encountered before. This capability makes them particularly useful in handling
    unexpected state changes, thus promoting adaptability in agents. All this allows
    neural networks to be flexible and adaptable to a different range of tasks and
    environments.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络具有将情况推广到他们之前未遇到的情况的能力。这种能力使它们在处理意外状态变化时特别有用，从而促进了智能体的适应性。所有这些都使神经网络能够灵活适应不同范围的任务和环境。
- en: Neural networks can handle actions that are continuous rather than discrete,
    thus enabling their use in real-world problems (where actions are often not limited
    to a discrete set).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络可以处理连续的动作而不是离散的动作，从而使其能够在现实世界问题中使用（在现实世界中，动作通常不受离散集合的限制）。
- en: Neural networks are versatile. They can be used with different types of data
    and do not require feature engineering. This is important when the features can
    be complex or the state representation is complex (sensors, images, video, and
    so on).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是通用的。它们可以与不同类型的数据一起使用，并且不需要特征工程。当特征可能很复杂或状态表示很复杂（传感器、图像、视频等）时，这一点很重要。
- en: They can produce a probability distribution and thus can be used with a stochastic
    policy. This is important when we want to add randomness and encourage exploration.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以产生概率分布，因此可以与随机策略一起使用。当我们想要添加随机性和鼓励探索时，这一点很重要。
- en: '![Figure 8.17 – Examples of screenshots where a neural network is used to learn
    how to play Atari games (https://arxiv.org/abs/1312.5602)](img/B21257_08_17.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图8.17 – 使用神经网络学习如何玩Atari游戏的截图示例](img/B21257_08_17.jpg)'
- en: Figure 8.17 – Examples of screenshots where a neural network is used to learn
    how to play Atari games ([https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602))
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 – 使用神经网络学习如何玩Atari游戏的截图示例 ([https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602))
- en: We will now present five different algorithms in order to understand the differences
    between the different types of approaches (off- and on-policy, model-free, and
    model-based approaches).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将介绍五种不同的算法，以便了解不同类型方法之间的差异（离政策和在线策略、无模型和基于模型的方法）。
- en: '![Figure 8.18 – Summary table of RL approaches](img/B21257_08_18.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图8.18 – 强化学习方法的总结表](img/B21257_08_18.jpg)'
- en: Figure 8.18 – Summary table of RL approaches
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 – 强化学习方法的总结表
- en: Q-learning and Deep Q-Network (DQN)
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q学习与深度Q网络（DQN）
- en: '**Q-learning** is a lookup-table-based approach underlying **Deep Q-Network**
    (**DQN**), an algorithm used by DeepMind to train an agent capable of solving
    video games. In the Q-learning algorithm, we have a **Q-table of State-Action
    values**, where we have a row for each state and a column for each action, and
    each cell contains an estimated Q-value for the corresponding state-action pair.
    The Q-values are initially set to zero. When the agent receives feedback from
    interacting with the environment, we iteratively conduct the update of the values
    (until they converge to the optimal values). Note that this update is conducted
    using the Bellman equation (the Q-value in the table represents the expected future
    rewards if the agent takes that action from that state and follows the best strategy
    afterward).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q学习**是**深度Q网络**（**DQN**）的基础，这是一种由DeepMind使用的算法，用于训练一个能够解决视频游戏的智能体。在Q学习算法中，我们有一个**状态-动作值Q表**，其中每一行代表一个状态，每一列代表一个动作，每个单元格包含对应状态-动作对的估计Q值。Q值最初被设置为0。当智能体从与环境交互中接收反馈时，我们迭代地更新这些值（直到它们收敛到最优值）。请注意，此更新使用贝尔曼方程进行（表中的Q值代表智能体从该状态采取该动作并随后遵循最佳策略时的预期未来奖励）。'
- en: Q-learning finds the optimal policy by learning the optimal Q-value for each
    state-action pair. Initially, the agent chooses actions at random, but by interacting
    with the environment and receiving feedback (reward), it learns which actions
    are best. During each iteration, it conducts the table update using the Bellman
    equation. The agent generally chooses the action that has the highest Q-value
    (greedy strategy), but we can control the degree of exploration (*ε*-greedy policy).
    Over time, these estimates become more and more accurate and the model converges
    to the optimal Q-values.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习通过学习每个状态-动作对的优化Q值来找到最优策略。最初，智能体随机选择动作，但通过与环境的交互和接收反馈（奖励），它学会了哪些动作是最好的。在每次迭代中，它使用贝尔曼方程进行表格更新。智能体通常选择具有最高Q值的动作（贪婪策略），但我们可以通过控制探索程度（*ε*-贪婪策略）来控制。随着时间的推移，这些估计变得越来越准确，模型收敛到最优Q值。
- en: '![Figure 8.19 – Q learning example](img/B21257_08_19.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图8.19 – Q学习示例](img/B21257_08_19.jpg)'
- en: Figure 8.19 – Q learning example
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 – Q学习示例
- en: In complex environments, using a table to store values becomes impractical due
    to the potentially massive size and computational intractability. Instead, we
    can use a Q-function, which maps state-action pairs to Q-values. Given that neural
    networks can effectively model complex functions, they can be employed to approximate
    the Q-function efficiently. By providing as input the state *S*, the neural network
    provides as output the Q-value for the state-action pair (in other words, the
    Q-values for all the actions you can take from that state). The principle is very
    similar to the Q-learning algorithm. We start with random estimates for the Q-values,
    explore the environment with an *ε*-greedy policy, and conduct the update of the
    estimates.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂环境中，由于可能的大规模和计算不可行性，使用表格存储值变得不切实际。相反，我们可以使用Q函数，它将状态-动作对映射到Q值。鉴于神经网络可以有效地模拟复杂函数，它们可以用来有效地近似Q函数。通过提供状态*S*作为输入，神经网络提供状态-动作对的Q值作为输出（换句话说，从该状态可以采取的所有动作的Q值）。原理与Q学习算法非常相似。我们开始时对Q值进行随机估计，使用*ε*-贪婪策略探索环境，并更新估计。
- en: 'The DQN architecture consists of three main components: two neural networks
    (the Q-network and the target network) and an experience replay component. The
    Q-network (a classical neural network) is the agent that is trained to produce
    the optimal state-action value. Experience replay, on the other hand, is used
    to generate data to train the neural network.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: DQN架构由三个主要组件组成：两个神经网络（Q网络和目标网络）以及一个经验回放组件。Q网络（一个经典的神经网络）是训练用来产生最优状态-动作值的代理。另一方面，经验回放用于生成训练神经网络所需的数据。
- en: The Q-network is trained on multiple time steps and on many episodes, with the
    aim of minimizing the difference between predicted Q-values and the target Q-values.
    During the agent’s interaction with the environment, each experience (a tuple
    of state, action, reward, and next state) is stored in this experience replay
    buffer. During training, random batches of experiences (a mix of old and new experiences)
    are selected from the buffer to update the Q-network. This allows breaking the
    correlation between consecutive experiences (helps stabilize the training) and
    reusing the past experience multiple times (increases data efficiency). The target
    network is a copy of the Q-network used to generate the target Q-values for training.
    Periodically, the target network weights are updated (e.g., every few thousand
    steps) by copying the Q-network weights; this stabilizes the training. During
    training, the Q-network predicts the Q-value for actions given a state (predicted
    Q-value) and the target network predicts the target Q-value for all actions given
    the state. The predicted Q-value, target Q-value, and the observed reward are
    used to calculate the loss and update the weight of the Q-network.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Q网络在多个时间步和多个回合上进行训练，目的是最小化预测Q值和目标Q值之间的差异。在代理与环境交互的过程中，每个经验（状态、动作、奖励和下一个状态的元组）都存储在这个经验回放缓冲区中。在训练过程中，从缓冲区中随机选择经验批次（新旧经验的混合）来更新Q网络。这有助于打破连续经验之间的相关性（有助于稳定训练）并多次重用过去经验（提高数据效率）。目标网络是用于生成训练目标Q值的Q网络的副本。定期（例如每几千步）通过复制Q网络权重来更新目标网络权重；这稳定了训练。在训练过程中，Q网络预测给定状态的行动Q值（预测Q值），目标网络预测给定状态的行动目标Q值。预测Q值、目标Q值和观察到的奖励用于计算损失并更新Q网络的权重。
- en: '![Figure 8.20 – DQN training algorithm](img/B21257_08_20.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图8.20 – DQN训练算法](img/B21257_08_20.jpg)'
- en: Figure 8.20 – DQN training algorithm
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 – DQN训练算法
- en: 'DQN has a number of innovations and advantages:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: DQN有许多创新和优势：
- en: Experience replay makes training more stable and efficient. Neural networks
    usually take a batch of data as input rather than a single state, so during training,
    the gradient will have less variance and the weights converge more quickly. Experience
    replay also allows us to reduce noise during training because we can conduct a
    kind of “shuffling” of experiences and thus better generalization.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验回放使训练更加稳定和高效。神经网络通常以一批数据作为输入，而不是单个状态，因此在训练过程中，梯度将具有更小的方差，权重收敛更快。经验回放还可以在训练过程中减少噪声，因为我们可以对经验进行一种“洗牌”，从而更好地实现泛化。
- en: The introduction of a target network mitigates the issue of non-stationary targets,
    which can cause instability in training. The target network is untrained, so the
    target Q-values are stable and have few fluctuations.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标网络的概念引入减轻了非平稳目标的问题，这可能导致训练不稳定。目标网络未经训练，因此目标Q值稳定且波动很小。
- en: DQN is effective with high-dimensional spaces such as images and is able to
    extract features by itself and learn effective policies. These capabilities enabled
    DQN to master Atari games by taking raw pixels as input.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN在处理高维空间如图像时非常有效，能够自行提取特征并学习有效的策略。这些能力使得DQN能够通过输入原始像素来掌握Atari游戏。
- en: 'There are, of course, also drawbacks:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也存在一些缺点：
- en: Although it is more efficient than Q-learning, DQN still requires a large number
    of samples to learn effectively; this limits its use for tasks where there is
    little data (sample inefficiency)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然它比Q学习更高效，但DQN仍然需要大量的样本来有效地学习；这限制了它在数据很少的任务中的应用（样本效率低）
- en: It is not stable when the action spaces are continuous, while it works well
    for discrete action spaces
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当动作空间是连续的时，它不稳定，而对于离散动作空间则表现良好
- en: It is sensitive to the choice of hyperparameters (such as learning rate, replay
    buffer size, and update frequency of the target network)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对超参数的选择（如学习率、重放缓冲区大小和目标网络更新频率）敏感
- en: The REINFORCE algorithm
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: REINFORCE算法
- en: DQN focuses on learning the value of an action in different states. **REINFORCE**
    is instead a policy-based method. These methods learn policy directly, mapping
    states to actions without learning a value function. The core idea is to optimize
    policy by maximizing the expected cumulative reward the agent receives over time.
    REINFORCE is a foundational algorithm for learning how to train agents to handle
    complex, continuous action spaces.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: DQN专注于在不同状态下学习动作的价值。**REINFORCE**则是一种基于策略的方法。这些方法直接学习策略，将状态映射到动作而不学习价值函数。核心思想是通过最大化代理在一段时间内收到的期望累积奖励来优化策略。REINFORCE是学习如何训练代理处理复杂、连续动作空间的基础算法。
- en: The policy is represented by a neural network that takes the current state as
    input and produces a probability distribution over all possible actions (the probability
    that an agent will perform a certain action). This is called stochastic policy
    because we do not have an action as output directly, but probabilities. Policy
    gradient methods try to improve the policy directly (by changing parameters during
    training) so that the policy produces better results. So, again, we start with
    a random policy (the neural network weights are initialized randomly) and let
    the agent act in the environment according to its policy, which causes a trajectory
    (a series of states and actions) to be produced. If this trajectory collects high
    rewards, we conduct an update of the weights so that this trajectory is more likely
    to be produced in the future. If, on the contrary, the agent performs poorly,
    the update of the weights will be directed to make that trajectory less likely.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 策略由一个神经网络表示，它以当前状态作为输入并产生一个关于所有可能动作的概率分布（代理执行特定动作的概率）。这被称为随机策略，因为我们没有直接输出动作，而是输出概率。策略梯度方法试图直接改进策略（通过在训练期间改变参数），以便策略产生更好的结果。因此，我们再次从一个随机策略（神经网络权重随机初始化）开始，让代理根据其策略在环境中行动，这会导致产生一个轨迹（一系列状态和动作）。如果这个轨迹收集到高奖励，我们将更新权重，使得这个轨迹在未来更有可能被产生。相反，如果代理表现不佳，权重的更新将指导使该轨迹不太可能发生。
- en: '![Figure 8.21 – Example of trajectory](img/B21257_08_21.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图8.21 – 轨迹示例](img/B21257_08_21.jpg)'
- en: Figure 8.21 – Example of trajectory
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21 – 轨迹示例
- en: So, the first step in this process is to initialize a neural network (policy
    *P*) with its parameters *θ*. Since these weights are initially random, the policy
    for a state as input will lead to random actions. We then generate a trajectory
    *τ*, letting the agent interact with the environment. Starting from state *s*0,
    we let the agent move according to policy *P* with the parameters *θ*. In practice,
    state *S* is given as input to the neural network and generates a distribution
    of actions. We select an action *a*0 sampling from this distribution. This process
    is repeated for as long as possible (e.g., till the end of the game), the set
    of states and actions being our trajectory.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个过程的第一步是初始化一个神经网络（策略*P*）及其参数*θ*。由于这些权重最初是随机的，以状态作为输入的策略将导致随机动作。然后我们生成一个轨迹*τ*，让代理与环境交互。从状态*s*0开始，我们让代理根据策略*P*和参数*θ*进行移动。在实践中，状态*S*被作为输入给神经网络，并生成一个动作分布。我们从这个分布中采样一个动作*a*0。这个过程会尽可能重复（例如，直到游戏结束），状态和动作的集合就是我们的轨迹。
- en: '![Figure 8.22 – Getting a distribution from a neural network](img/B21257_08_22.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图8.22 – 从神经网络获取分布](img/B21257_08_22.jpg)'
- en: Figure 8.22 – Getting a distribution from a neural network
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22 – 从神经网络获取分布
- en: During the trajectory, we collect rewards (called reward-to-go or also return
    *G*t). The return is the total cumulative reward received from time step *t* to
    the end of the episode, discounted by a factor *γ*. The discount factor determines
    how important future rewards are in relation to immediate rewards. In this case,
    we have a function that gives us the expected return for a given policy and we
    want to maximize it. So, we calculate the gradient, and via gradient ascent, we
    modify the parameters of our neural network.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在轨迹过程中，我们收集奖励（称为奖励到去或也称为回报 *G*t）。回报是从时间步 *t* 到剧集结束所收到的总累积奖励，通过一个因子 *γ* 进行折现。折现因子决定了未来奖励相对于即时奖励的重要性。在这种情况下，我们有一个函数，它为我们提供给定策略的预期回报，我们希望最大化它。因此，我们计算梯度，并通过梯度上升修改我们神经网络的参数。
- en: REINFORCE is conceptually simple and easy to implement, suitable for continuous
    action spaces (since it directly learns a policy), and enables end-to-end learning
    (the algorithm learns directly from raw data). Some of the challenges with this
    algorithm, however, are the high variance in policy updates (it relies on full
    episodes being returned and updates can be noisy and unstable), it requires a
    lot of data (a large number of episodes are needed because it discards data after
    each update and does not reuse experiences), it is not suitable for environments
    where data is expensive, and it does not work well when there are delayed rewards.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE算法在概念上简单且易于实现，适用于连续动作空间（因为它直接学习策略），并支持端到端学习（算法直接从原始数据中学习）。然而，这个算法的一些挑战包括策略更新的高方差（它依赖于完整剧集的返回，更新可能嘈杂且不稳定），需要大量数据（因为它在每次更新后丢弃数据且不重复使用经验，所以需要大量的剧集），它不适用于数据昂贵的环境，并且在存在延迟奖励的情况下表现不佳。
- en: Note that the REINFORCE algorithm is an on-policy algorithm since the policy
    receives updates only based on experiences collected with the same policy. At
    each iteration, the agent uses the updated policy and collects experience with
    it for the update. In the case of off-policy methods, experiences collected with
    other policies are also used. This is, for example, what we saw with DQN, where
    we were using experiences in the batch that were collected with a different policy.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，REINFORCE算法是一个基于策略的算法，因为策略只根据使用相同策略收集的经验进行更新。在每次迭代中，智能体使用更新的策略并使用它来收集经验以进行更新。在离策略方法的情况下，也会使用使用其他策略收集的经验。例如，这就是我们在DQN中看到的情况，我们使用的是使用不同策略收集的批次经验。
- en: Proximal Policy Optimization (PPO)
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 近端策略优化（PPO）
- en: '**Proximal Policy Optimization** (**PPO**) is one of the most widely cited
    and used algorithms in RL. Introduced by OpenAI in 2017, PPO was designed to balance
    the simplicity of policy gradient methods, such as REINFORCE, with the stability
    of more complex algorithms, such as **Trust Region Policy Optimization** (**TRPO**).
    In essence, PPO is a practical and efficient algorithm that performs well on benchmarks
    while being relatively easy to implement and tune.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**近端策略优化**（**PPO**）是强化学习中最广泛引用和使用的算法之一。由OpenAI于2017年引入，PPO旨在平衡策略梯度方法（如REINFORCE）的简单性与更复杂算法（如**信任域策略优化**（**TRPO**））的稳定性。本质上，PPO是一个实用且高效的算法，在基准测试中表现良好，同时相对容易实现和调整。'
- en: PPO shares similarities with REINFORCE but includes important improvements that
    make training much more stable. One of the challenges in policy-based methods
    is the choice of hyperparameters (especially the learning rate) and the risk of
    unstable weight updates. The key innovation of PPO is to ensure that policy updates
    are not too large, as this could destabilize training. PPO achieves this by using
    a constraint on the objective function that limits how much the policy can change
    in a single update, thereby avoiding drastic changes in the network weights.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: PPO与REINFORCE相似，但包括重要的改进，使训练更加稳定。基于策略的方法中的一个挑战是超参数的选择（尤其是学习率）以及不稳定权重更新的风险。PPO的关键创新在于确保策略更新不要太大，因为这可能会破坏训练的稳定性。PPO通过使用一个限制目标函数的约束来实现这一点，该约束限制了策略在单个更新中可以改变的程度，从而避免了网络权重的剧烈变化。
- en: A significant problem with traditional policy gradient methods is their inability
    to recover from poor updates. If a policy performs poorly, the agent may generate
    sparse or low-quality training data in the next iteration, creating a self-reinforcing
    loop that can be difficult to escape. PPO addresses this by stabilizing policy
    updates.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 传统策略梯度方法的一个重大问题是它们无法从较差的更新中恢复。如果一个策略表现不佳，智能体可能在下一次迭代中生成稀疏或低质量的训练数据，从而形成一个难以摆脱的自我强化循环。PPO通过稳定策略更新来解决这一问题。
- en: The policy in PPO is represented by a neural network, *πθ(a*|*s)*, where *θ*
    represents the network’s weights. The network takes the current state *s* as input
    and outputs a probability distribution over possible actions *a*. Initially, the
    weights are randomly initialized. As the agent interacts with the environment,
    it generates batches of experiences (state, action, reward) under the current
    policy. The agent also calculates the advantage estimate, which measures how much
    better (or worse) a chosen action was compared to the expected value of the state.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在PPO中，策略由一个神经网络表示，*πθ(a*|*s)*，其中*θ*代表网络的权重。网络将当前状态*s*作为输入，并输出一个关于可能动作*a*的概率分布。最初，权重是随机初始化的。随着智能体与环境交互，它根据当前策略生成经验批次（状态、动作、奖励）。智能体还计算优势估计，这衡量所选动作相对于状态期望值的优劣。
- en: The main difference from simpler policy gradient methods lies in PPO’s use of
    a **clipped objective function**. This function ensures that policy updates are
    stable and prevents large, destabilizing changes. If the probability ratio between
    the new and old policies *rt(θ)* falls outside the range [1−*ϵ*,1+*ϵ*], where
    *ϵ* is a small hyperparameter (e.g., 0.2), the update is clipped. This clipping
    mechanism ensures that policy updates remain within a safe range, preventing the
    policy from diverging too much in a single update.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单的策略梯度方法的主要区别在于PPO使用**剪辑目标函数**。这个函数确保策略更新是稳定的，并防止出现大的、破坏性的变化。如果新旧策略之间的概率比*rt(θ)*超出范围[1−*ϵ*,1+*ϵ*]，其中*ϵ*是一个小的超参数（例如，0.2），则更新将被剪辑。这种剪辑机制确保策略更新保持在安全范围内，防止策略在单次更新中偏离太多。
- en: A common variant of PPO uses an **actor-critic architecture**, where the actor
    learns the policy, and the critic learns the value function. The critic provides
    feedback on the quality of the actions, helping to reduce the variance of the
    updates and improve learning efficiency (we discuss this more in detail later).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的一种常见变体使用**actor-critic架构**，其中actor学习策略，而critic学习价值函数。critic提供关于动作质量的反馈，有助于减少更新方差并提高学习效率（我们稍后会更详细地讨论这一点）。
- en: Overall, PPO is both a stable and robust algorithm, less prone to instability
    than simpler policy gradient methods and easier to use than more complex algorithms
    such as TRPO. It does not require solving complex optimization problems or calculating
    second-order gradients, making it a practical choice for many applications. However,
    PPO still requires careful tuning of hyperparameters, such as the clipping parameter
    *ϵ*, learning rate, and batch size. Additionally, it can suffer from high variance
    in environments with long episodes or delayed rewards.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，PPO是一个既稳定又鲁棒的算法，比简单的策略梯度方法更不容易不稳定，比TRPO等更复杂的算法更容易使用。它不需要解决复杂的优化问题或计算二阶梯度，使其成为许多应用的实用选择。然而，PPO仍然需要仔细调整超参数，例如剪辑参数*ϵ*、学习率和批量大小。此外，它可能在具有长周期或延迟奖励的环境中出现高方差。
- en: The actor-critic algorithm
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Actor-critic算法
- en: 'The actor-critic algorithm is another popular approach in RL, which combines
    the strengths of two different methods: value-based methods (such as Q-learning)
    and policy-based methods. The actor-critic model consists of two components:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic算法是RL中另一种流行的方法，它结合了两种不同方法的优势：基于价值的方法（如Q-learning）和基于策略的方法。actor-critic模型由两个组件组成：
- en: '**Actor**: The actor is responsible for deciding what action should be taken
    in the current state of the environment. The policy is generally a neural network
    that produces a probability distribution over actions. The actor tries to maximize
    the expected return by optimizing the policy.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Actor**：actor负责决定在环境当前状态下应该采取什么动作。策略通常是一个产生动作概率分布的神经网络。actor通过优化策略来最大化期望回报。'
- en: '**Critic**: The critic evaluates the actions taken by the actor by estimating
    the value function. This function indicates how good an action is in terms of
    expected future rewards. The value function can be the state value function *V(s)*
    or the action-value function *Q(s,a)*.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评价者**：评价者通过估计价值函数来评估动作评价者采取的动作。这个函数表示一个动作在预期未来奖励方面的好坏。价值函数可以是状态价值函数 *V(s)*
    或动作价值函数 *Q(s,a)*。'
- en: The insight behind this approach is that the actor is the decision-maker who
    learns how to improve decisions over time. The critic, on the other hand, is a
    kind of advisor who evaluates the goodness of actions and gives feedback on strategy.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法背后的洞察是，动作评价者是决策者，他学习如何随着时间的推移改进决策。另一方面，评价者是一种顾问，他评估动作的好坏，并对策略提供反馈。
- en: '![Figure 8.23 – Actor-critic approach](img/B21257_08_23.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图8.23 – 动作-评价者方法](img/B21257_08_23.jpg)'
- en: Figure 8.23 – Actor-critic approach
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23 – 动作-评价者方法
- en: 'The process can be defined by four steps:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程可以定义为四个步骤：
- en: The agent interacts with the environment, and based on its policy, selects an
    action based on the current state. It then receives feedback from the environment
    in the form of a reward and a new state.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理与环境交互，并根据其策略，基于当前状态选择一个动作。然后它从环境中以奖励和新的状态的形式接收反馈。
- en: In the second step, the critic uses the reward and the new state to calculate
    a **temporal difference** (**TD**) error. The TD error measures how far the critic’s
    current estimate of the value function is from the observed outcome. The TD error
    is then the difference between the reward at time step *t* (plus a discount factor
    *γ* for the critic’s estimates of the value of the next state *V(st+1)* to serve
    to balance the impact of immediate and future rewards) and the critic’s estimates
    of the value of the current state *V(st)*.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二步中，评价者使用奖励和新的状态来计算一个**时间差分**（**TD**）误差。TD误差衡量评价者当前对价值函数的估计与观察结果之间的差距。TD误差然后是时间步长
    *t* 的奖励（加上评价者对下一个状态价值 *V(st+1)* 的估计的折扣因子 *γ*，以平衡即时和未来奖励的影响）与评价者对当前状态价值 *V(st)*
    的估计之间的差异。
- en: The critic updates its value function parameters to minimize the TD error. This
    is done with gradient descent.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评价者通过梯度下降更新其价值函数参数以最小化TD误差。
- en: The actor is updated as well. The actor uses the TD error as a feedback signal.
    If the error is positive, it means that the action was better than expected and
    the actor should take it more often (increase the probability of taking this action
    in the future). If the error is negative, the actor should decrease the probability.
    The actor maximizes the policy using gradient ascent; we want to maximize the
    expected return.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作评价者也会更新。动作评价者使用TD误差作为反馈信号。如果误差为正，这意味着动作比预期的好，动作评价者应该更频繁地采取这个动作（增加未来采取这个动作的概率）。如果误差为负，动作评价者应该减少概率。动作评价者使用梯度上升来最大化策略；我们希望最大化期望回报。
- en: Actor-critic methods work well with continuous action spaces, where value-based
    methods have problems. It is a stable and efficient method and reduces the variance
    of policy gradient updates. On the other hand, it is sensitive to hyperparameters,
    you have to train two networks, and it is more complex than Q-learning or REINFORCE.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 动作-评价者方法在连续动作空间中表现良好，而基于价值的方法存在问题。它是一个稳定且高效的方法，并减少了策略梯度更新的方差。另一方面，它对超参数敏感，你必须训练两个网络，并且它比Q学习或REINFORCE更复杂。
- en: '**Advantage Actor-Critic** (**A2C**) is a popular variant where multiple agents
    interact with multiple instances of the environment in parallel. This allows for
    faster training.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势动作-评价者**（**A2C**）是一个流行的变体，其中多个代理并行地与环境的多个实例交互。这允许更快地训练。'
- en: AlphaZero
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AlphaZero
- en: '**AlphaZero** is a groundbreaking model-based RL algorithm developed by DeepMind
    in 2017, capable of mastering chess, shogi (Japanese chess), and Go. It has achieved
    superhuman performance, defeating human champions in these games. The success
    of AlphaZero lies in its innovative combination of deep learning and **Monte Carlo
    Tree Search** (**MCTS**), which allows it to learn and plan effectively without
    human expertise or handcrafted rules.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**AlphaZero** 是DeepMind在2017年开发的一个开创性的基于模型的强化学习算法，能够掌握国际象棋、将棋（日本象棋）和围棋。它已经达到了超人类的表现，在这些游戏中击败了人类冠军。AlphaZero的成功在于其创新性地结合了深度学习和**蒙特卡洛树搜索**（**MCTS**），这使得它能够在没有人类专业知识或手工制定的规则的情况下有效地学习和规划。'
- en: AlphaZero learns entirely through **self-play**, starting with no prior knowledge
    other than the basic rules of the game. It plays millions of games against itself,
    gradually understanding what constitutes good or bad moves through trial and error.
    This self-play approach allows AlphaZero to discover optimal strategies, often
    surpassing even those developed by expert human players. Additionally, it enables
    the model to generate a vast amount of training data, far more than could be obtained
    by simply analyzing human games. The algorithm uses a deep neural network to represent
    both the policy (which actions to take) and the value function (the expected outcome
    of the game from a given state).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaZero完全通过**自我对弈**进行学习，除了游戏的基本规则外，没有任何先验知识。它与自己进行数百万次对弈，通过试错逐渐理解什么构成好或坏的走法。这种自我对弈的方法使得AlphaZero能够发现最优策略，通常甚至超过由专家人类玩家开发出的策略。此外，它还使模型能够生成大量的训练数据，远远超过仅通过分析人类游戏所能获得的数据。该算法使用深度神经网络来表示策略（采取哪些行动）和价值函数（从给定状态出发游戏的预期结果）。
- en: Traditional chess engines used to rely on game-tree search techniques. At each
    move, they would construct a game tree that represented all possible legal moves
    from the current position and performed a **depth-first search (DFS)** to a certain
    depth. This brute-force search examined all legal moves, assigning values to the
    final nodes based on heuristic evaluations formulated by the chess community.
    These heuristics, such as king safety, pawn structure, and control of the center,
    mimic factors used by human chess players to judge the quality of a move.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的国际象棋引擎曾经依赖于游戏树搜索技术。在每一步，它们会构建一个表示从当前位置出发所有可能合法走法的游戏树，并执行到一定深度的**深度优先搜索（DFS）**。这种蛮力搜索检查了所有合法走法，根据由棋类社区制定的启发式评估为最终节点分配值。这些启发式，如国王安全、兵的结构和中心控制，模仿了人类棋手用来判断走法质量的因素。
- en: After evaluating the final nodes, traditional engines would backtrack and analyze
    the positions, pruning fewer promising branches to simplify the search. Despite
    these optimizations, this method had limitations, often leading to suboptimal
    moves and being computationally expensive. This is where MCTS comes in.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估最终节点后，传统引擎会回溯并分析位置，剪枝较少有希望的分支以简化搜索。尽管有这些优化，这种方法仍然有限制，往往导致次优走法，并且计算成本高昂。这就是MCTS（蒙特卡洛树搜索）的用武之地。
- en: MCTS is an algorithm designed for decision-making in environments where planning
    several moves ahead is essential, especially in games with large state spaces
    where an exhaustive search is infeasible. MCTS builds a search tree by simulating
    games multiple times, gradually improving its understanding of the best actions
    through experience.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: MCTS是一种算法，用于在需要提前规划数步的环境中进行决策，特别是在具有大状态空间的游戏中，其中全面搜索是不可行的。MCTS通过多次模拟游戏来构建搜索树，逐渐通过经验改进对最佳行动的理解。
- en: 'MCTS operates through four main steps, repeated to refine the search tree:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: MCTS通过以下四个主要步骤操作，重复以细化搜索树：
- en: '**Selection**: Starting from the root node (the current state), the algorithm
    selects child nodes using a strategy that balances exploration (trying less-explored
    moves) and exploitation (choosing moves that have shown promise). This is often
    done using the **Upper Confidence Bound for Trees** (**UCT**) formula, which considers
    both the average reward and the number of visits to each node.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择**：从根节点（当前状态）开始，算法使用一种平衡探索（尝试较少探索的走法）和利用（选择已显示出希望的走法）的策略选择子节点。这通常使用**树的上界置信度（UCT**）公式来完成，该公式考虑了每个节点的平均奖励和访问次数。'
- en: '**Expansion**: If the selected node is not a terminal state (the end of the
    game), the algorithm adds one or more child nodes, representing possible actions
    from this state. This expansion allows the search to cover new potential moves
    and outcomes.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扩展**：如果选定的节点不是终端状态（游戏的结束），算法会添加一个或多个子节点，代表从该状态出发的可能行动。这种扩展使得搜索能够覆盖新的潜在走法和结果。'
- en: '**Simulation (rollout**): From a newly added node, MCTS performs a simulation,
    or “rollout,” by playing the game to a terminal state using a simple or random
    policy. The outcome of this simulation (win, loss, or draw) provides a reward,
    serving as an estimate of the value of the actions taken.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模拟（滚动出**）：从新添加的节点开始，MCTS通过使用简单或随机策略将游戏玩到终端状态来进行模拟，或称为“滚动出”。这个模拟的结果（赢、输或平局）提供了一个奖励，作为对采取的行动价值的估计。'
- en: '**Backpropagation**: The reward from the simulation is then backpropagated
    up the tree, updating the values associated with each node along the path to the
    root. This includes updating the average reward and the number of visits for each
    node. Over time, these updates help the algorithm determine which moves are most
    promising.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反向传播**：然后，将模拟的奖励反向传播到树中，更新从根节点到路径上每个节点的值。这包括更新每个节点的平均奖励和访问次数。随着时间的推移，这些更新帮助算法确定哪些移动最有希望。'
- en: '![Figure 8.24 – Monte Carlo Tree Search (https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)](img/B21257_08_24.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图8.24 – 蒙特卡洛树搜索 (https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)](img/B21257_08_24.jpg)'
- en: Figure 8.24 – Monte Carlo Tree Search ([https://en.wikipedia.org/wiki/Monte_Carlo_tree_search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search))
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.24 – 蒙特卡洛树搜索 ([https://en.wikipedia.org/wiki/Monte_Carlo_tree_search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search))
- en: 'AlphaZero then uses a neural network (a convolutional neural network that takes
    as input the arrangement of pieces on the board) and produces two outputs: policy
    head (a probability distribution over all possible moves, guiding the agent on
    which moves to consider) and value head (the likelihood of winning from the current
    board position, helping the agent to evaluate the strength of various states).
    AlphaZero uses MCTS to simulate potential moves and their outcomes (the agent
    plans several moves ahead in the game). Through MCTS, the model explores the moves
    that seem most promising and gradually improves its understanding of the game.
    The tree search uses the policy and value outputs from the neural network to prioritize
    which branches of the tree to explore. AlphaZero learns to play by playing against
    itself (self-play). In each game, the agent uses MCTS to decide moves and saves
    states (positions on the board), chosen moves, and results. This data is used
    to improve the policy and value estimates (neural network weight updates).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaZero随后使用一个神经网络（一个以棋盘上棋子排列为输入的卷积神经网络）并产生两个输出：策略头（所有可能移动的概率分布，指导智能体考虑哪些移动）和价值头（从当前棋盘位置获胜的可能性，帮助智能体评估各种状态的力量）。AlphaZero使用MCTS来模拟潜在的移动及其结果（智能体在游戏中计划数步）。通过MCTS，模型探索看似最有希望的移动，并逐渐提高其对游戏的理解。树搜索使用神经网络的策略和价值输出来优先考虑探索树的哪些分支。AlphaZero通过自我对弈（自我玩耍）来学习。在每一场比赛中，智能体使用MCTS来决定移动并保存状态（棋盘上的位置）、选择的移动和结果。这些数据用于改进策略和价值估计（神经网络权重更新）。
- en: '![Figure 8.25 – AlphaZero pipeline (https://www.mdpi.com/2079-9292/10/13/1533)](img/B21257_08_25.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图8.25 – AlphaZero流程 (https://www.mdpi.com/2079-9292/10/13/1533)](img/B21257_08_25.jpg)'
- en: Figure 8.25 – AlphaZero pipeline ([https://www.mdpi.com/2079-9292/10/13/1533](https://www.mdpi.com/2079-9292/10/13/1533))
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25 – AlphaZero流程 ([https://www.mdpi.com/2079-9292/10/13/1533](https://www.mdpi.com/2079-9292/10/13/1533))
- en: 'AlphaZero therefore presents three main innovations:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，AlphaZero提出了三个主要创新：
- en: '**Generalizing across games**: The same algorithm is used for three different
    games (chess, shogi, and Go), without the need for game-specific adjustments.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨游戏泛化**：相同的算法用于三种不同的游戏（国际象棋、将棋和围棋），无需针对特定游戏进行调整。'
- en: '**It requires no human knowledge**: Unlike traditional chess engines that use
    an extensive database of human games and strategies, AlphaZero learns the game
    on its own. The model prioritizes strategies that offer long-term rewards within
    the game, rather than focusing solely on immediate benefits from individual moves.
    This approach enables the model to discover innovative strategies previously unexplored
    by humans or traditional chess engines.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无需人类知识**：与使用大量人类游戏和策略数据库的传统棋类引擎不同，AlphaZero自行学习游戏。该模型优先考虑在游戏中提供长期奖励的策略，而不是仅仅关注单个移动的即时收益。这种方法使模型能够发现人类或传统棋类引擎未曾探索的创新策略。'
- en: '**Efficient search and learning**: Using MCTS and deep learning allows more
    efficient use of computational resources. Instead of conducting an extensive search
    of all possible moves, AlphaZero focuses only on the most promising moves.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效的搜索和学习**：使用MCTS和深度学习允许更有效地使用计算资源。AlphaZero不是对所有可能的移动进行广泛的搜索，而是仅关注最有希望的移动。'
- en: Of course, AlphaZero is not without flaws either. The algorithm has a huge computational
    cost since it has to play millions of games against itself. Also, the algorithm
    works well for games (or settings where there is perfect information) but it is
    more difficult to adapt it to environments where the information is incomplete.
    Finally, there is discussion about actually understanding the game or learning
    abstract concepts, since the model fails some chess puzzles that are easy for
    humans.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，AlphaZero也不是没有缺陷。该算法的计算成本巨大，因为它必须与自己玩数百万场比赛。此外，该算法在游戏（或信息完全的环境）中表现良好，但将其适应到信息不完整的环境则更困难。最后，关于实际上理解游戏或学习抽象概念存在讨论，因为模型在某些对人类来说很容易的棋类谜题上失败了。
- en: In the next subsection, we will discuss the challenges with RL and new, exciting
    lines of research.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个子节中，我们将讨论强化学习（RL）的挑战以及新的、令人兴奋的研究方向。
- en: Challenges and future direction for deep RL
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度RL的挑战和未来方向
- en: 'Although RL has made significant progress, several challenges and several active
    lines of research remain:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RL取得了重大进展，但仍然存在一些挑战和活跃的研究方向：
- en: '**Generalization in unseen environments**: Generalization in environments that
    the agent has not seen remains a complex task. Agents are usually trained in a
    simulated environment or in specific settings, where they are able to excel after
    training. However, transferring learned skills to new environments, dynamic environments,
    or changing conditions is difficult. This limits the use of deep RL algorithms
    in the real world because real environments are rarely static or perfectly predictable.
    True generalization requires that a model not only learns solutions that are task-specific
    but also adapts to a range of situations (even if they did not occur during training).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在未见过的环境中的泛化**：在智能体未见过的环境中的泛化仍然是一个复杂任务。智能体通常在模拟环境中或特定设置中进行训练，在训练后能够表现出色。然而，将学习到的技能转移到新环境、动态环境或变化条件下是困难的。这限制了深度RL算法在现实世界中的应用，因为现实环境很少是静态或完全可预测的。真正的泛化要求模型不仅学习特定任务的解决方案，而且能够适应各种情况（即使这些情况在训练期间没有发生）。'
- en: '**Reward function design**: Reward function controls agent behavior, learning,
    and performance. Designing a reward function is difficult, especially in complex,
    scattered environments. In sparse reward settings, where there is limited feedback
    and it is often delayed, defining the reward and function is complex but critical.
    Even so, there is often a risk of creating bias, leading the policy to overfitting
    or unexpected behaviors, or making it suboptimal.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励函数设计**：奖励函数控制智能体的行为、学习和表现。设计奖励函数很困难，尤其是在复杂、分散的环境中。在稀疏奖励设置中，由于反馈有限且通常延迟，定义奖励和函数是复杂但至关重要的。即便如此，仍然存在创建偏差的风险，导致策略过度拟合或出现意外行为，或者使其次优。'
- en: '**Compound error in model-based planning**: Model-based RL is at risk of compounding
    errors. The longer the horizon of predictions, the more errors accumulate in model
    predictions, leading to significant deviations from the optimal trajectory. This
    is especially the case for complex or high-dimensional space environments, thus
    limiting their use in real environments.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的规划中的复合误差**：基于模型的RL存在复合误差的风险。预测的视野越长，模型预测中的错误积累越多，导致与最优轨迹的显著偏差。这在复杂或高维空间环境中尤其如此，因此限制了它们在现实环境中的应用。'
- en: '**Multi-task learning**: Creating an agent that can be used for multiple tasks
    remains difficult, with the risk that the agent learns only the easier ones and
    ignores the more complex (or otherwise very poorly performing) ones. Also, a multi-task
    model often exhibits performance that is far inferior to an agent optimized for
    a single task. The design of agents that can therefore be used for multi-task
    RL is difficult and still an active line of research.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多任务学习**：创建一个可用于多个任务的智能体仍然很困难，存在风险是智能体只学会了较容易的任务而忽略了更复杂（或表现非常差的）任务。此外，多任务模型通常的表现远低于针对单个任务优化的智能体。因此，设计可用于多任务RL的智能体是困难的，并且仍然是一个活跃的研究领域。'
- en: '**Multi-modal RL**: With the advancement of computer vision and NLP, there
    are deep learning models that can either handle one mode individually or multiple
    modes together. This is why there is increasing discussion of using multimodal
    RL, where an agent can move through a multimodal environment and integrate information
    from the various modalities. For example, a robot can acquire information from
    the environment in an image and receive commands or instructions in natural language.
    An agent in video games receives visual information but also information from
    dialogues with characters or from other players. Multimodal learning remains complicated
    because an agent must simultaneously learn how to process multimodal information
    and optimize policy to interact in a complex environment. Similarly, it remains
    difficult to design a reward function for these cases.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态强化学习**：随着计算机视觉和自然语言处理技术的进步，现在有一些深度学习模型可以单独处理一种模式，或者同时处理多种模式。这就是为什么越来越多的讨论使用多模态强化学习，其中智能体可以在多模态环境中移动，并整合来自各种模态的信息。例如，机器人可以从图像中获取环境信息，并以自然语言接收命令或指令。在视频游戏中，智能体接收视觉信息，但也会从与角色的对话或其他玩家那里获取信息。多模态学习仍然很复杂，因为智能体必须同时学习如何处理多模态信息并优化策略以在复杂环境中交互。同样，为这些情况设计奖励函数仍然很困难。'
- en: In the next section, we will see how a neural network can be used to learn how
    to play a video game.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用神经网络来学习如何玩视频游戏。
- en: Learning how to play a video game with reinforcement learning
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用强化学习学习如何玩视频游戏
- en: In this subsection, we will discuss how to train an agent to play a video game.
    In this case, the agent will be parameterized by a neural network. Following this
    policy, it will choose among the actions allowed by the video game, receive feedback
    from the environment, and use this feedback for parameter updating. In general,
    video games provide complex and dynamic environments that simulate real-world
    scenarios, thus making them an excellent testbed for RL algorithms. Video games
    provide a high-dimensional state space (pixel-based states, detailed universes)
    and a rich action space (discrete or continuous), are inspired by the real world,
    and can provide both immediate and delayed rewards (e.g., some actions may result
    in the direct death of the protagonist while a long-term strategy is needed to
    solve puzzles or win the game). In addition, many games require the user to explore
    the environment before they can master it. Enemies are often dynamic, and the
    model must learn how to defeat opposing agents or understand complex behaviors
    to overcome them. The game also provides clear rewards (which are often frequent
    or can otherwise be accelerated) that can then be easily defined for a reward
    function and thus make a safe playground (e.g., for algorithms for robotics).
    In addition, there are clear benchmarks and one can quickly compare the quality
    of a new algorithm.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将讨论如何训练智能体来玩视频游戏。在这种情况下，智能体将由神经网络参数化。遵循此策略，它将在视频游戏允许的动作中进行选择，从环境中获得反馈，并使用这些反馈进行参数更新。一般来说，视频游戏提供复杂和动态的环境，模拟现实世界场景，因此它们是强化学习算法的优秀测试平台。视频游戏提供高维状态空间（基于像素的状态，详细的世界）和丰富的动作空间（离散或连续），灵感来源于现实世界，可以提供即时和延迟的奖励（例如，某些动作可能导致主角直接死亡，而解决谜题或赢得游戏则需要长期策略）。此外，许多游戏要求用户在掌握环境之前先探索环境。敌人通常是动态的，模型必须学习如何击败对手或理解复杂行为以克服它们。游戏还提供清晰的奖励（通常是频繁的或可以加速的），这使得奖励函数可以轻松定义，从而成为一个安全的游乐场（例如，用于机器人算法）。此外，还有明确的基准，可以快速比较新算法的质量。
- en: 'We chose the actor-critic approach for this training because it has a number
    of features:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择actor-critic方法进行此训练，因为它具有以下特点：
- en: Actor-critic can handle complex and continuous action spaces (control a character
    in a 3D environment) and thus can be used for a wide variety of games.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor-critic可以处理复杂和连续的动作空间（例如控制3D环境中的角色），因此可以用于各种游戏。
- en: The actor in the system learns the policy directly, making it efficient for
    scenarios where finding the policy is crucial. This is necessary in video games
    where quick decision-making and strategic planning are required.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统中的actor直接学习策略，这使得在找到策略至关重要的场景中效率更高。这在需要快速决策和战略规划的视频游戏中是必要的。
- en: The critic provides feedback and speeds up learning in comparison to purely
    policy-based methods. Using a value function (critic) to evaluate actions reduces
    the variance of policy updates, so it is more stable and efficient in environments
    where rewards are scattered.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与纯粹基于策略的方法相比，评论家提供了反馈并加快了学习速度。使用价值函数（评论家）来评估动作减少了策略更新的方差，因此在奖励分散的环境中更加稳定和高效。
- en: Actor-critic allows for efficient management of the balance between exploration
    and exploitation, where the actor explores the environment and the critic guides
    it by providing feedback. For more complex environments, actor-critic may not
    be sufficient, though it is a good initial choice and often sufficient.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为者-评论家允许高效地管理探索和利用之间的平衡，其中行为者通过探索环境，而评论家通过提供反馈来引导它。对于更复杂的环境，行为者-评论家可能不足以满足需求，尽管它是一个良好的初始选择，并且通常足够用。
- en: Actor-critic can also handle long-term planning. Often, in video games, there
    can be long-term rewards; the critic’s value function helps the agent understand
    the long-term impact of its actions.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为者-评论家（Actor-critic）也可以处理长期规划。在视频游戏中，往往存在长期奖励；评论家的价值函数帮助代理理解其行动的长期影响。
- en: Some variants are efficient in parallelizing and using data. A2C is a good choice
    for parallelizing environments and thus collecting more data, thus speeding up
    training and convergence.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些变体在并行化和使用数据方面效率很高。A2C是并行化环境和收集更多数据的良好选择，从而加快训练和收敛速度。
- en: We chose Super Mario as our game because it provides a rich and complex environment.
    The environment resembles the real world, and the representation of pixel-based
    observations as input is similar to that of real-world computer vision tasks,
    making Super Mario a good testbed for RL agents who need to learn to extract meaningful
    features from visual data. This environment is also partially observable, so it
    requires the agent to explore and learn about the environment. Different levels
    may require different strategies, so the model must be able to balance exploration
    and exploitation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择超级马里奥作为我们的游戏，因为它提供了一个丰富且复杂的环境。这个环境类似于现实世界，像素观察作为输入的表示与真实世界计算机视觉任务相似，这使得超级马里奥成为需要从视觉数据中提取有意义特征的强化学习代理的良好测试平台。这个环境也是部分可观察的，因此需要代理去探索和学习环境。不同的关卡可能需要不同的策略，因此模型必须能够平衡探索和利用。
- en: 'In the game, there are different kinds of challenges, such as navigating obstacles,
    facing different kinds of enemies, and learning to jump optimally and often dynamically.
    These different challenges represent different skills that an agent should develop:
    testing the agent’s ability to make precise and timely actions (jumping over obstacles
    or gaps), assessing threats and deciding when to avoid or engage (avoiding or
    engaging enemies), and spatial awareness and strategic planning (navigating complex
    levels). The levels are progressive, so with a difficulty that progresses as the
    agent learns. In addition, there are both immediate rewards (collecting coins)
    and delayed rewards (e.g., completing a level), thus allowing for the evaluation
    of long-term strategies.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏中，存在不同种类的挑战，例如导航障碍、面对不同种类的敌人，以及学习最优和动态地跳跃。这些不同的挑战代表了代理应该发展的不同技能：测试代理做出精确和及时行动的能力（跳过障碍或缺口），评估威胁并决定何时避免或参与（避免或参与敌人），以及空间意识和战略规划（导航复杂关卡）。关卡是逐步增加的，因此难度随着代理的学习而增加。此外，既有即时奖励（收集金币），也有延迟奖励（例如，完成关卡），从而允许评估长期策略。
- en: Finally, Super Mario has been widely adopted in the RL research community as
    a benchmark. Major libraries support it, or it is found directly integrated, thus
    allowing a quick way to test algorithms or conduct comparisons. There are also
    already well-researched strategies; the game is well documented and is a good
    example for both beginners and experts in RL. There are also implementations that
    are parallelizable, thus allowing effective and fast training.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，超级马里奥在强化学习研究社区中被广泛采用作为基准。主要的库都支持它，或者它被直接集成，从而允许快速测试算法或进行比较。同时，也有已经深入研究过的策略；游戏文档详尽，是强化学习初学者和专家的良好示例。此外，还有一些可并行化的实现，从而允许有效且快速的训练。
- en: '![Figure 8.26 – Super Mario screenshots from the training](img/B21257_08_26.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图8.26 – 超级马里奥训练截图](img/B21257_08_26.jpg)'
- en: Figure 8.26 – Super Mario screenshots from the training
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.26 – 超级马里奥训练截图
- en: 'All the code can be found within the repository, at the following link: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8/RL_SuperMario](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8/RL_SuperMario)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都可以在以下链接的仓库中找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8/RL_SuperMario](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8/RL_SuperMario)
- en: '![Figure 8.27 – Screenshot of the repository](img/B21257_08_27.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.27 – 仓库截图](img/B21257_08_27.jpg)'
- en: Figure 8.27 – Screenshot of the repository
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.27 – 仓库截图
- en: Description of the scripts
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚本描述
- en: 'To perform the training, we will use some popular RL libraries (OpenAI’s Gym
    and PyTorch). In the repository, there are different scripts that are used to
    train the agent:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行训练，我们将使用一些流行的强化学习库（OpenAI 的 Gym 和 PyTorch）。在仓库中，有一些用于训练智能体的不同脚本：
- en: '`env`: This script defines the environment where our agent acts (Super Mario)
    and allows us to record a video of our agent playing, preprocess images for the
    model, define the reward function, set the world, set a virtual joystick, and
    more.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env`：此脚本定义了我们的智能体行动的环境（超级马里奥），并允许我们记录智能体玩游戏的视频，预处理模型图像，定义奖励函数，设置世界，设置虚拟摇杆等。'
- en: '`model`: This script defines a PyTorch neural network model for an actor-critic
    architecture. The model is designed to process image-like inputs, extract features,
    and then use those features to output both action probabilities (actor) and state
    value estimates (critic).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`：此脚本定义了一个用于演员-评论家架构的 PyTorch 神经网络模型。该模型旨在处理类似图像的输入，提取特征，然后使用这些特征输出动作概率（演员）和状态价值估计（评论家）。'
- en: '`optimizer`: This code defines a custom optimizer class called `GlobalAdam`,
    which extends the functionality of PyTorch’s built-in Adam optimizer.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`：此代码定义了一个名为 `GlobalAdam` 的自定义优化器类，它扩展了 PyTorch 内置的 Adam 优化器的功能。'
- en: '`train`: This script sets up and runs a distributed RL system using the **Asynchronous
    Advantage Actor-Critic** (**A3C**) method to train an agent to play Super Mario
    Bros.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train`：此脚本使用**异步优势演员-评论家**（**A3C**）方法设置和运行一个分布式强化学习系统，以训练一个能够玩超级马里奥兄弟的智能体。'
- en: '`test`: Model testing is in a separate script. This script allows you to load
    the trained model to play the game while rendering the gameplay.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test`：模型测试在单独的脚本中。此脚本允许您加载训练好的模型来玩游戏，同时渲染游戏过程。'
- en: '`process`: This script acts as the linking piece that integrates all the preceding
    components into a cohesive RL system for training and testing an agent to play
    Super Mario Bros.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process`：此脚本作为连接件，将所有前面的组件集成到一个统一的强化学习系统中，用于训练和测试玩超级马里奥兄弟的智能体。'
- en: '![Figure 8.28 – Global view of the scripts](img/B21257_08_28.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.28 – 脚本的全局视图](img/B21257_08_28.jpg)'
- en: Figure 8.28 – Global view of the scripts
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.28 – 脚本的全局视图
- en: Setting up the environment
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置环境
- en: 'The `env` script allows us to have our setup of the environment, especially
    for RL algorithms such as Deep Q-Learning or actor-critic. Inside the script,
    we import the libraries we need, after which there are some functions that are
    used to create the world and define how the agent can interact with it:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`env` 脚本允许我们设置环境，特别是对于深度 Q 学习或演员-评论家等强化学习算法。在脚本中，我们导入所需的库，之后有一些用于创建世界和定义智能体如何与之交互的函数：'
- en: '`Monitor`: The `Monitor` class allows the user to save a visual record of the
    agent’s gameplay, which is useful for debugging, analyzing agent performance,
    and sharing results. This function permits us to save a video of the game using
    `.ffmpeg`.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Monitor`：`Monitor` 类允许用户保存智能体游戏过程的视觉记录，这对于调试、分析智能体性能和分享结果非常有用。此函数允许我们使用 `.ffmpeg`
    保存游戏视频。'
- en: '`process_frame`: The `process_frame` function is used to preprocess frames
    from the game to make them more suitable for training an RL agent. This function
    checks whether the frame is in the right format, converts it to grayscale and
    reduces the size, and normalizes it (simplifies the input). This allows the agent
    to focus on the important details of the visual information.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process_frame`：`process_frame` 函数用于预处理游戏帧，使其更适合训练强化学习智能体。此函数检查帧是否为正确格式，将其转换为灰度并减小尺寸，然后进行归一化（简化输入）。这允许智能体专注于视觉信息的重要细节。'
- en: '`CustomReward`: This is a modification of the reward to encourage useful behaviors,
    track the current score, add rewards, check whether the agent finishes the level,
    and penalize it if the episode isn’t finished. In this way, it tries to incentivize
    completing the level and making progress by penalizing failures.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CustomReward`：这是对奖励的修改，以鼓励有用的行为，跟踪当前分数，添加奖励，检查代理是否完成关卡，如果没有完成则对其进行惩罚。这样，它试图通过惩罚失败来激励完成关卡并取得进步。'
- en: '`CustomSkipFrame`: This serves to speed up training by allowing skip frames,
    thus reducing computational computation (fewer environment updates).'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CustomSkipFrame`：这个功能通过允许跳过帧来加速训练，从而减少计算量（更少的环境更新）。'
- en: '`create_train_env`: This function sets up a fully customized and optimized
    Super Mario environment, making it ready for training an RL agent with efficient
    preprocessing, reward shaping, and frame skipping.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_train_env`：这个函数设置了一个完全定制和优化的超级马里奥环境，使其准备好用高效的预处理、奖励塑造和帧跳过训练RL代理。'
- en: Defining the model
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模型
- en: 'In the `model` script, we define the architecture for our algorithm. `ActorCritic`
    is the class that governs the architecture, and as a neural network, it is based
    on PyTorch (in fact, we use `nn.Module`, a classic neural network in PyTorch).
    The class has two components representing `Actor` (responsible for choosing actions)
    and `Critic`, which provides feedback. You can see that we have a shared feature
    extractor:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在`model`脚本中，我们定义了算法的架构。`ActorCritic`类控制着架构，作为一个神经网络，它基于PyTorch（实际上，我们使用`nn.Module`，这是PyTorch中的一个经典神经网络）。该类有两个组件代表`Actor`（负责选择动作）和`Critic`，它提供反馈。你可以看到我们有一个共享的特征提取器：
- en: '[PRE0]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we have a convolutional network to extract spatial features from the game;
    this output is then reshaped into a 2D tensor, which is passed for an LSTM. The
    LSTM has an update of the hidden state `hx` and the cell state `cx` (we described
    the LSTM in detail in [*Chapter 1*](B21257_01.xhtml#_idTextAnchor014)), thus managing
    episode memory.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有一个卷积网络来从游戏中提取空间特征；然后这个输出被重塑成一个2D张量，传递给LSTM。LSTM更新隐藏状态`hx`和细胞状态`cx`（我们在[*第一章*](B21257_01.xhtml#_idTextAnchor014)中详细描述了LSTM），从而管理游戏记忆。
- en: 'After that, we initialize the two components:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们初始化这两个组件：
- en: '[PRE1]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Using a single feature extractor allows us to save computation resources. The
    two components produce two different outputs: `actor_linear` produces the output
    for the actor, which is a vector of size `num_actions`. This represents the probability
    of taking each action. The `critic_linear` component produces the output for the
    critic, which is a single scalar value. This value represents the estimated value
    of the current state (the expected return from this state). This separation allows
    us to make sure that the two layers have separate goals and different learning
    signals.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个特征提取器允许我们节省计算资源。两个组件产生两个不同的输出：`actor_linear`为actor生成输出，它是一个大小为`num_actions`的向量。这代表了采取每个动作的概率。`critic_linear`组件为critic生成输出，它是一个单一的标量值。这个值代表了当前状态的估计值（从这个状态期望的回报）。这种分离确保了两个层有各自的目标和不同的学习信号。
- en: 'Next, we will define different loss functions in order to allow for different
    learning. As we can see, the two components produce different outputs:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义不同的损失函数，以便允许不同的学习。正如我们所见，两个组件产生不同的输出：
- en: '[PRE2]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since we want our process to be optimized for distributed learning, we use
    a custom version of Adam. Adam is a classical optimizer that is used for updating
    the parameters of a neural network. The `GlobalAdam` class is designed for distributed
    RL, where multiple processes or agents share the same optimizer. The key idea
    is to make certain parts of the optimizer’s state shared across processes, allowing
    agents to coordinate their updates to the model parameters efficiently. This is
    especially useful with actor-critic and especially the variant where there are
    many agents acting in the same environment. The idea is that we play the game
    several times asynchronously and then conduct global updates, reducing computation.
    The `GlobalAdam` script is simply an adaptation of Adam to RL problems, allowing
    averaging and learning from different processes:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望我们的进程针对分布式学习进行优化，我们使用 Adam 的自定义版本。Adam 是一种经典的优化器，用于更新神经网络的参数。`GlobalAdam`
    类是为分布式强化学习设计的，其中多个进程或智能体共享相同的优化器。关键思想是使优化器状态的一部分在进程间共享，允许智能体高效地协调对模型参数的更新。这特别适用于演员-评论家和特别是有多个智能体在相同环境中行动的变体。想法是我们异步玩几轮游戏，然后进行全局更新，减少计算。`GlobalAdam`
    脚本简单地将 Adam 适配到强化学习问题，允许从不同进程中进行平均和学习：
- en: '[PRE3]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Training the model
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: The `train` script then allows us to train the model asynchronously with different
    processes. The script allows us to provide several parameters (default parameters
    are already entered). For example, we can decide the level of the game (`--world`
    and `--stage`), the type of action (`--action_type`), the learning rate for the
    optimizer (`--lr`), hyperparameters specific to the algorithm and RL (`--gamma`,
    `--tau`, `--beta`), or related to the process and its parallelization (`--num_processes`,
    `--num_local_steps`, and `--num_global_steps`).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 脚本允许我们使用不同的进程异步训练模型。脚本允许我们提供几个参数（默认参数已经输入）。例如，我们可以决定游戏的难度级别（`--world`
    和 `--stage`），动作的类型（`--action_type`），优化器的学习率（`--lr`），针对算法和强化学习（RL）的特定超参数（`--gamma`，`--tau`，`--beta`），或者与进程及其并行化相关的参数（`--num_processes`，`--num_local_steps`
    和 `--num_global_steps`）。'
- en: 'The `train` function allows us to initialize the training environment, initialize
    the policy, and use the GPU. The `global_model.share_memory()` method allows the
    global model’s parameters to be accessible to all processes, enabling parallel
    updates. You can see we use `GlobalAdam` to update the global model’s parameters.
    The `torch.multiprocessing` wrapper (is a wrapper to the multiprocessing module)
    allows us to create multiple processes that operate asynchronously. This script
    then defines the training of our model using multiple parallel processes. At the
    same time, the script allows easy configuration and customization:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 函数允许我们初始化训练环境，初始化策略，并使用 GPU。`global_model.share_memory()` 方法允许全局模型的参数对所有进程可访问，从而实现并行更新。您可以看到我们使用
    `GlobalAdam` 来更新全局模型的参数。`torch.multiprocessing` 包装器（是对多进程模块的包装）允许我们创建多个异步操作的多进程。然后，此脚本定义了使用多个并行进程训练我们的模型。同时，脚本允许轻松配置和定制：'
- en: '[PRE4]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Testing the system
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试系统
- en: The `test` script allows customization such as deciding on some parameters,
    such as level of play, actions, and so on. Once we have trained our model, we
    can then load it, play the game, and register the agent playing. The model then
    plays with its policy, without optimization in this script, and thus allows us
    to observe the agent’s performance.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`test` 脚本允许定制，例如决定一些参数，如游戏难度、动作等。一旦我们训练了我们的模型，我们就可以加载它，玩游戏，并注册正在玩游戏的智能体。然后，模型根据其策略进行游戏，在此脚本中没有进行优化，因此允许我们观察智能体的表现。'
- en: Connecting all the components
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接所有组件
- en: The `process` script connects all the scripts we have seen so far into one system.
    This script uses the `create_train_env` function from the `env` module to set
    up the Super Mario Bros game environment. This is the environment where our agent
    interacts and learns. The script also initializes the `ActorCritic` model (both
    actor and critic) and uses this model to make decisions and evaluate game states.
    The `local_train` function is responsible for training and requires the `GlobalAdam`
    optimizer. This script is also used to evaluate trained model performance, so
    it uses elements we defined in the test script. This script, then, is the central
    piece that allows us to have a fully functional RL system. It orchestrates the
    environment, model, and training algorithm, making everything work together to
    train an agent to play Super Mario Bros.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`process` 脚本将我们迄今为止看到的所有脚本连接成一个系统。此脚本使用 `env` 模块中的 `create_train_env` 函数来设置超级马里奥兄弟游戏环境。这是我们代理与环境交互并学习的地方。脚本还初始化了
    `ActorCritic` 模型（包括演员和评论家），并使用此模型做出决策和评估游戏状态。`local_train` 函数负责训练，需要 `GlobalAdam`
    优化器。此脚本还用于评估训练模型性能，因此它使用了我们在测试脚本中定义的元素。因此，这个脚本是我们拥有一个完全功能性的强化学习系统的核心部分。它协调环境、模型和训练算法，使一切协同工作以训练一个能够玩超级马里奥兄弟的代理。'
- en: 'The `local_train` function enables the agent to train in parallel with other
    processes while updating a shared global model. This function establishes a seed
    for reproducibility, so we can reproduce the results. After that, we initialize
    the environment (`create_train_env`) and model (`ActorCritic`); if there is a
    GPU, we move the model to the GPU and initialize TensorBoard:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_train` 函数使代理能够在更新共享的全局模型的同时并行训练。此函数为可重复性设置了一个种子，因此我们可以重现结果。之后，我们初始化环境（`create_train_env`）和模型（`ActorCritic`）；如果有GPU，我们将模型移动到GPU并初始化TensorBoard：'
- en: '[PRE5]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'At this point, we begin the training loop where each iteration represents an
    episode of gameplay. The local parameters are synchronized with the global parameters,
    and at the end of each episode, the hidden and cell states of the LSTM are reset:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个点上，我们开始训练循环，其中每个迭代代表一次游戏回合。局部参数与全局参数同步，在每个回合结束时，LSTM的隐藏和细胞状态被重置：
- en: '[PRE6]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At this point, we begin to collect experiences for a number of steps (`opt.num_local_steps`).
    Then, for a state, the model (the local model) produces a set of probabilities,
    and from these probabilities, we sample an action. Having chosen an action, we
    interact with the environment, so we get a reward and a new state. For each of
    these steps, we record the following: whether the episode has ended, the log probability
    of the action, the value estimate, the reward, and the entropy of the policy.
    If the episode ends, the state is reset, and the hidden states are detached to
    prevent gradient backpropagation:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们开始收集多个步骤（`opt.num_local_steps`）的经验。然后，对于某个状态，模型（局部模型）生成一组概率，并从这些概率中采样一个动作。选择了一个动作后，我们与环境交互，从而获得奖励和新的状态。对于这些步骤中的每一个，我们记录以下内容：是否结束了一个回合，动作的对数概率，价值估计，奖励和政策熵。如果回合结束，状态被重置，隐藏状态被断开以防止梯度反向传播：
- en: '[PRE7]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, it is time to calculate the loss and conduct backpropagation. Here, we
    use **generalized advantage estimation** (**GAE**), to balance bias and variance
    and make the training therefore more efficient. Simply, the advantage function
    *A(s,a)* measures the goodness of an action *a* relative to the average action
    in a given state *s*. In the next script, GAE is used to compute the advantage
    values that drive the actor’s policy updates. We use GAE to update the policy
    in the actor loss in order to maximize the expected return but keep the variance
    low. In other words, we want to keep the training more stable. By adding GAE,
    the training process becomes more efficient and less susceptible to noise from
    high variance returns or inaccuracies from biased value estimates:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是计算损失和进行反向传播的时候了。在这里，我们使用**广义优势估计**（**GAE**）来平衡偏差和方差，从而使训练更加高效。简单来说，优势函数
    *A(s,a)* 衡量了一个动作 *a* 相对于给定状态 *s* 中平均动作的好坏。在下一个脚本中，GAE用于计算推动演员策略更新的优势值。我们使用GAE在演员损失中更新策略，以最大化期望回报但保持方差低。换句话说，我们希望使训练更加稳定。通过添加GAE，训练过程变得更加高效，并且对高方差回报的噪声或偏差价值估计的不准确性的敏感性降低：
- en: '[PRE8]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that we have three separate losses. The first is the actor loss, which
    encourages actions that lead to higher rewards. The critic loss penalizes errors
    in the value estimation, and the entropy loss encourages exploration by penalizing
    overly confident action distributions (in other penalizing strategies that are
    too greedy). Once we have computed the total loss, we perform the backpropagation
    as in any neural network. Right now, we have performed local training, so we use
    the gradients of the local model to conduct the global model update as well. Every
    certain time interval, we save the model and send the loss logs to TensorBoard.
    The process ends when we have reached the total number of global steps.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有三个独立的损失。第一个是演员损失，它鼓励导致更高奖励的动作。评论员损失惩罚价值估计中的错误，熵损失通过惩罚过于自信的动作分布（在其他过于贪婪的惩罚策略中）来鼓励探索。一旦我们计算了总损失，我们就像任何神经网络一样执行反向传播。目前，我们已经完成了本地训练，因此我们使用本地模型的梯度来进行全局模型更新。每隔一定时间间隔，我们保存模型并将损失日志发送到TensorBoard。当达到总的全局步数时，过程结束。
- en: 'The `local_test` function allows us to conduct the evaluation of our trained
    model. It runs as a separate process to test how well the agent performs using
    the learned policy:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_test` 函数允许我们评估训练好的模型。它作为一个独立的过程运行，以测试代理使用学习到的策略的表现：'
- en: '[PRE9]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Again, we conduct setup and initialization and load the local `ActorCritic`
    model in evaluation mode (in inference mode, practically, the model does not get
    updates during this process). At this point, we start the loop, where we load
    the last weights from the global model. For a state, we compute the probabilities
    for each action and choose the action with the highest probability. Note how,
    during training, we conducted sampling of the action; in evaluation mode, instead,
    we chose the action with a greedy policy. We interact with the environment and
    render the game, conduct action tracking, and check whether the agent gets stuck
    or repeats the same action indefinitely. If the agent exceeds the maximum number
    of steps or gets stuck, the episode ends and we reset the state. This function
    evaluates the performance of the trained agent, rendering the gameplay so that
    users can observe how well the agent has learned to play Super Mario Bros. It
    ensures the policy is effective and provides visual feedback.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们进行设置和初始化，并在评估模式（实际上，在推理模式下，模型在此过程中不会更新）下加载本地的 `ActorCritic` 模型。此时，我们开始循环，从全局模型中加载最后一步的权重。对于某个状态，我们计算每个动作的概率，并选择概率最高的动作。注意，在训练过程中，我们进行了动作的采样；而在评估模式下，我们则采用贪婪策略选择动作。我们与环境交互，渲染游戏，进行动作跟踪，并检查代理是否陷入困境或无限重复相同的动作。如果代理超过最大步数或陷入困境，则剧集结束，我们重置状态。此函数评估训练代理的性能，渲染游戏玩法，以便用户可以观察代理学习玩超级马里奥兄弟的情况。它确保策略有效，并提供视觉反馈。
- en: 'Running the scripts, we can see that the training runs in parallel:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本，我们可以看到训练是并行进行的：
- en: '![Figure 8.29 – Screenshot of the script run](img/B21257_08_29.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图8.29 – 脚本运行截图](img/B21257_08_29.jpg)'
- en: Figure 8.29 – Screenshot of the script run
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.29 – 脚本运行截图
- en: 'You can check out the video here: [https://www.youtube.com/watch?v=YWx-hnvqjr8](https://www.youtube.com/watch?v=YWx-hnvqjr8)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处查看视频：[https://www.youtube.com/watch?v=YWx-hnvqjr8](https://www.youtube.com/watch?v=YWx-hnvqjr8)
- en: To summarize, we used several scripts to implement a variant of the action-critic
    algorithm (the A3C method). This method involves training multiple agents in parallel
    to explore the environment, collect experiences, and update a shared global model
    asynchronously. In other words, we use a variant that allows us to speed up the
    training and learn a model that is more robust because it retrieves different
    experiences from different agents. For cleaner organization, we divided the process
    into several scripts that are then linked into a single script (process script).
    We defined our neural network with a common extractor for the two components,
    so we can save some computations. In addition, we used an LSTM to be able to handle
    the temporal dependencies there are between one state and another. We had to modify
    our optimizer because we needed shared memory to be able to handle several processes
    to update a global model. Asynchronous training indeed has higher complexities,
    where each agent needs to access and update the global model. After that, we defined
    how to train our model by collecting some experience. Having collected the experience,
    we conducted an update of the model weights, calculating the loss and performing
    backpropagation. Periodically, we synchronized the global and local model, conducting
    an update of the global model. After that, we defined how to evaluate our agent
    using the parameters of the global model. The agent uses the learned policy to
    play the game.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们使用了几个脚本来实现动作-评论家算法（A3C方法）的一个变体。这种方法涉及并行训练多个代理以探索环境、收集经验并异步更新共享的全局模型。换句话说，我们使用了一个允许我们加快训练速度并学习更鲁棒模型的变体，因为它从不同的代理那里检索不同的经验。为了更清晰的组织，我们将过程分解为几个脚本，然后将它们链接成一个单一的脚本（过程脚本）。我们定义了具有两个组件的公共提取器，以便我们可以节省一些计算。此外，我们使用LSTM来处理一个状态与另一个状态之间存在的时序依赖性。我们必须修改我们的优化器，因为我们需要共享内存来处理多个进程以更新全局模型。异步训练确实具有更高的复杂性，其中每个代理都需要访问和更新全局模型。之后，我们定义了如何通过收集一些经验来训练我们的模型。收集经验后，我们更新模型权重，计算损失并执行反向传播。定期地，我们同步全局和局部模型，对全局模型进行更新。之后，我们定义了如何使用全局模型的参数来评估我们的代理。代理使用学到的策略来玩游戏。
- en: These scripts allow efficient parallel training because of the A3C method. In
    fact, we can use several agents in parallel that explore the environment, gather
    experience, and then lead to a global model update. Using a parallel system causes
    agents to explore different parts of the environment, leading to more diverse
    experiences and thus a more generalized policy. In general, this is favorable
    because different strategies may be needed in video games. In the same vein, we
    added entropy loss to encourage exploration and prevent the agent from being stuck
    in a suboptimal strategy. The script is designed for efficient use of resources,
    to reduce computation, and to have fast training (we did not add an experience
    replay buffer to save memory and thus consume less RAM). The use of a global model
    ensures that knowledge learned by one agent is immediately available to all agents;
    this usually promotes rapid convergence.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这些脚本由于采用了A3C方法，因此能够实现高效的并行训练。实际上，我们可以使用多个代理并行地探索环境、收集经验，然后导致全局模型更新。使用并行系统使得代理探索环境的不同部分，导致更多样化的经验，从而产生更通用的策略。一般来说，这是有利的，因为在视频游戏中可能需要不同的策略。同样地，我们添加了熵损失来鼓励探索并防止代理陷入次优策略。该脚本旨在高效利用资源，减少计算，并实现快速训练（我们没有添加经验回放缓冲区以节省内存，从而消耗更少的RAM）。使用全局模型确保一个代理学到的知识立即对所有代理可用；这通常促进快速收敛。
- en: The choice of an on-policy learning method such as A3C can result in high variance
    in policy updates. This variance can then be amplified by the asynchronous nature,
    which may make it difficult to get consistent results across runs. In fact, the
    asynchronous approach introduces non-determinism, meaning that the results can
    vary significantly between runs. This makes the process less predictable and complicates
    the choice of hyperparameters (which is why we have provided default parameters,
    although it is possible to experiment with them). While we have tried to optimize
    the resources consumed by this script, the whole process remains resource intensive
    (like RL in general).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 选择在线策略学习方法如A3C可能会导致策略更新的高方差。这种方差随后可能被异步性放大，这可能会使得在运行之间获得一致的结果变得困难。实际上，异步方法引入了非确定性，意味着结果可能在运行之间有显著差异。这使得过程更不可预测，并复杂化了超参数的选择（这就是为什么我们提供了默认参数，尽管可以尝试调整它们）。虽然我们已经尝试优化此脚本的资源消耗，但整个过程仍然资源密集（就像RL一般）。
- en: 'A3C primarily relies on CPU-based parallelism; however, incorporating GPU-friendly
    methods could significantly enhance training efficiency. Algorithms such as PPO
    can leverage GPUs to optimize the training process. Effective use of GPUs enables
    more efficient batch processing, allowing for the accumulation of experiences
    and bulk updates to the model. For readers interested in exploring GPU-based optimization,
    here are a few potential ideas:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: A3C主要依赖于基于CPU的并行性；然而，采用对GPU友好的方法可以显著提高训练效率。例如PPO这样的算法可以利用GPU优化训练过程。有效使用GPU可以更高效地进行批量处理，允许积累经验并对模型进行批量更新。对于有兴趣探索基于GPU优化方法的读者，以下是一些潜在的想法：
- en: Test different hyperparameters and vary their values to better understand their
    impact. In the script, you can easily set and change hyperparameters. We invite
    you to test lambda (*λ*) to find a better balance between bias and variance.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试不同的超参数并改变它们的值，以更好地理解它们的影响。在脚本中，您可以轻松地设置和更改超参数。我们邀请您测试lambda (*λ*) 以找到偏差和方差之间的更好平衡。
- en: Try PPO. PPO is a popular alternative to A3C that exploits multiple epochs of
    mini-batch updates. As we have seen, it is an algorithm that promotes stability
    and works well in many cases. It also does not require many hyperparameters and
    the default ones usually work well.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试PPO。PPO是A3C的一个流行替代方案，它利用多个epoch的mini-batch更新。正如我们所见，它是一种促进稳定性且在许多情况下表现良好的算法。它也不需要很多超参数，默认参数通常效果良好。
- en: Adopt synchronous A2C as it is a simpler, synchronous version of A3C. This approach
    collects experiences in parallel and uses batches for updating. It is usually
    slower but easier to debug.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用同步A2C，因为它是一个更简单、同步的A3C版本。这种方法并行收集经验，并使用批量进行更新。它通常较慢但更容易调试。
- en: The model shown in this project can be applied to several other video games,
    showing how an RL algorithm can solve real tasks.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目中展示的模型可以应用于其他几个视频游戏，展示了RL算法如何解决实际任务。
- en: LLM interactions with RL models
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM与RL模型之间的交互
- en: RL algorithms have been instrumental for agents that can navigate complex environments,
    optimize strategies, and make decisions, with successes in areas such as robotics
    and video games. LLMs, on the other hand, have had a strong impact on **natural
    language processing** (**NLP**), enabling machines to understand human language
    and instructions. Although potential synergies can be imagined, so far these two
    technologies have evolved in parallel. In recent years, though, with the heightened
    interest in LLMs, the two fields have increasingly intersected. In this section,
    we will discuss the interaction between RL and LLMs.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: RL算法对于能够导航复杂环境、优化策略和做出决策的智能体至关重要，在机器人技术和视频游戏等领域取得了成功。另一方面，LLMs对**自然语言处理**（**NLP**）产生了重大影响，使机器能够理解人类语言和指令。尽管可以想象潜在的协同效应，但到目前为止，这两种技术是并行发展的。然而，近年来，随着对LLMs兴趣的增加，这两个领域越来越多地交叉。在本节中，我们将讨论RL和LLMs之间的交互。
- en: 'We can have three cases of interaction:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以有三种交互情况：
- en: '**RL enhancing an LLM**: Using RL to enhance the performance of an LLM in one
    or more NLP tasks'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL增强LLM**：使用RL增强一个LLM在一个或多个NLP任务中的性能'
- en: '**LLMs enhancing RL**: Using LLMs to train an RL algorithm that performs a
    task that is not necessarily NLP'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLMs增强RL**：使用LLMs训练一个执行非必要是NLP任务的RL算法'
- en: '**RL and LLMs**: Combining RL models and LLMs to plan a skill set, without
    either system being used to train or conduct fine-tuning of the other'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL和LLMs**：结合RL模型和LLMs来规划技能集，而不使用任一系统来训练或对另一个系统进行微调'
- en: Let’s discuss these in detail.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论这些内容。
- en: RL-enhanced LLMs
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RL增强的LLMs
- en: 'We have already discussed alignment and prompt engineering, in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042).
    RL is, then, used for fine-tuning, prompt engineering, and the alignment of LLMs.
    As mentioned in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042), LLMs are trained
    to predict the next word in a sequence, leading to a mismatch between the LLM’s
    training objective and human values. This can lead LLMs to produce text with bias
    or other unsafe content, and likewise to be suboptimal at following instructions.
    Alignment serves to realign the model to human values or to make an LLM more effective
    for safer deployment. One of the most widely used techniques is **reinforcement
    learning from human feedback** (**RLHF**), where the reward is inferred from human
    preferences and then used to train the LLM. This process follows three steps:
    collect human feedback data, train a reward model on this data, and conduct fine-tuning
    of the LLM with RL. Generally, the most popular choice of RL algorithm is PPO
    or derivative methods. In fact, we do not want our aligned model to be significantly
    different from the original model, which PPO guarantees.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[*第三章*](B21257_03.xhtml#_idTextAnchor042)中讨论了对齐和提示工程。然后，RL被用于微调、提示工程和LLMs的对齐。如[*第三章*](B21257_03.xhtml#_idTextAnchor042)中提到的，LLMs被训练来预测序列中的下一个单词，导致LLMs的训练目标与人类价值观之间存在不匹配。这可能导致LLMs生成带有偏见或其他不安全内容的文本，同样也可能在遵循指令方面表现不佳。对齐的作用是将模型重新对齐到人类价值观，或者使LLM在更安全的部署中更有效。最广泛使用的技术之一是**从人类反馈中进行强化学习**（**RLHF**），其中奖励是从人类偏好中推断出来的，然后用于训练LLM。这个过程遵循三个步骤：收集人类反馈数据，在此数据上训练奖励模型，以及使用RL对LLM进行微调。通常，最受欢迎的RL算法选择是PPO或其衍生方法。实际上，我们不希望我们的对齐模型与原始模型有显著差异，这是PPO保证的。
- en: Interaction with LLMs is through the prompt, and the prompt should condense
    all the instructions for the task we want LLM to accomplish. Some work has focused
    on using RL to design prompts. Prompt optimization can be represented as an RL
    problem with the goal of incorporating human knowledge and thus drawing interpretable
    and adaptable prompts. The agent is used to construct prompts that are query-dependent
    and optimized. One can also train a policy network to generate desired prompts,
    with the advantage that the prompts are generally transferable across LLMs. An
    intriguing aspect of this approach is that some of these optimized prompts are
    grammatically “gibberish,” indicating that high-quality prompts for a task need
    not follow human language patterns.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLMs的交互通过提示进行，提示应浓缩我们希望LLM完成的任务的全部指令。一些研究工作集中在使用RL来设计提示。提示优化可以表示为一个RL问题，其目标是将人类知识融入其中，从而生成可解释和可适应的提示。代理被用来构建查询依赖和优化的提示。一个人也可以训练一个策略网络来生成期望的提示，其优点是提示通常可以在LLMs之间迁移。这种方法的一个有趣方面是，其中一些优化提示在语法上是“胡言乱语”，这表明对于任务的优质提示不一定需要遵循人类语言模式。
- en: LLM-enhanced RL
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM增强的RL
- en: '**LLM-enhanced RL** refers to methods that use multi-modal information processing,
    generation, reasoning, or other high-level cognitive capabilities of pre-trained
    LLMs in assisting an RL agent. In other words, the difference from traditional
    RL is the use of an LLM and the exploitation of its knowledge and capabilities
    in some way. The addition of an LLM in some form has a twofold advantage: first,
    an LLM possesses reasoning and planning skills that allow for improved learning,
    and second, it has a greater ability to generalize. In addition, an LLM has extensive
    knowledge gained during the pre-training step and that can be transferred across
    domains and tasks, thus allowing better adaptation to environments that have not
    been seen. Models that are pre-trained generally cannot expand their knowledge
    or acquire new capabilities (continual learning is an open challenge of deep learning),
    so using a model trained with huge amounts of knowledge can help with this aspect
    (LLMs are generalists and have huge amounts of information for different domains
    in memory).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM增强的RL**指的是使用预训练的LLM的多模态信息处理、生成、推理或其他高级认知能力来辅助RL代理的方法。换句话说，与传统RL的不同之处在于使用了LLM，并以某种方式利用其知识和能力。以某种形式添加LLM具有双重优势：首先，LLM拥有推理和规划技能，这有助于提高学习效率，其次，它具有更强的泛化能力。此外，LLM在预训练阶段获得了广泛的知识，这些知识可以跨领域和任务进行迁移，从而更好地适应尚未见过的环境。通常，预训练的模型无法扩展其知识或获取新的能力（持续学习是深度学习的开放挑战），因此使用大量知识训练的模型可以帮助这一方面（LLMs是通才，在记忆中拥有不同领域的海量信息）。'
- en: An LLM can then be inserted into the classic framework of an RL system (an agent
    interacting with and receiving feedback from an environment) at more than one
    point. An LLM can then be integrated to extract information, reprocess state,
    redesign rewards, make decisions, select actions, interpret policies, analyze
    world similarity, and more.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可以插入到经典RL系统（代理与环境交互并接收反馈）的多个点。LLM可以集成以提取信息、重新处理状态、重新设计奖励、做出决策、选择动作、解释策略、分析世界相似性等。
- en: '![Figure 8.30 – Framework of LLM-enhanced RL in classical agent-environment
    interactions (https://arxiv.org/pdf/2404.00282)](img/B21257_08_30.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![图8.30 – 经典代理-环境交互中LLM增强的RL框架](https://arxiv.org/pdf/2404.00282)(img/B21257_08_30.jpg)'
- en: Figure 8.30 – Framework of LLM-enhanced RL in classical agent-environment interactions
    ([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.30 – 经典代理-环境交互中LLM增强的RL框架([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
- en: Thus, an LLM can be used inside the system as an information processor, reward
    designer, decision-maker, and generator.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLM可以作为信息处理器、奖励设计师、决策者和生成器在系统中使用。
- en: Information processor
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息处理器
- en: When a task requires textual information or visual features, it can be complex
    for an agent to understand the information and optimize the policy at the same
    time. As we saw earlier, a convolutional neural network can be used to process
    images for a model to interact with a video game or board game. In the case of
    a chatbot, we can then use a model that understands language. Alternatively, instead
    of using a model directly on the language, we can use an LLM to extract features
    that allow the agent to learn more quickly. LLMs can be good feature extractors,
    thereby reducing the dimensionality and complexity of the information. Or LLMs
    can translate natural language into a specific formal language understandable
    to an agent. For example, in the case of a robot, the natural language of different
    users will be different and not homogeneous, making it difficult for the agent
    to learn. An LLM can transform instructions into a standard, formal language that
    allows easier learning for the agent.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个任务需要文本信息或视觉特征时，代理同时理解信息和优化策略可能会很复杂。正如我们之前看到的，卷积神经网络可以用来处理图像，使模型能够与视频游戏或棋盘游戏交互。在聊天机器人的情况下，我们可以使用一个理解语言的模式。或者，我们可以在不直接使用语言模型的情况下，使用LLM提取特征，使代理能够更快地学习。LLMs可以作为好的特征提取器，从而降低信息的维度和复杂性。或者LLMs可以将自然语言翻译成代理能够理解的特定形式化语言。例如，在机器人案例中，不同用户的自然语言可能不同且不统一，这使得代理难以学习。LLM可以将指令转换成标准、形式化的语言，使代理更容易学习。
- en: A wide pre-trained model learns a representation of the data that can then be
    used for subsequent applications. A model, then, can be used to extract a data
    representation that we can use to train an agent. An LLM can be used frozen (that
    is, without the need for further training) to extract a compressed representation
    of the history of the environment. Some studies use an LLM to summarize past visual
    observations that are provided to the agent, so we can provide a memory to the
    agent. Using a frozen model is clearly the simplest alternative, but when agents
    are deployed in the real world, performance can degrade rapidly due to real-world
    variations versus the training environment. Therefore, we can conduct fine-tuning
    of both the agent and the LLM. The use of a feature extractor (an LLM or other
    large model) makes it easier for the agent to learn since these features are more
    invariant to changes in the environment (changes in brightness, color, etc...),
    but on the other hand, they have an additional computational cost.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广泛的预训练模型学习数据的表示，然后可以用于后续的应用。因此，一个模型可以用来提取我们可以用来训练智能体的数据表示。一个LLM可以被冻结使用（即，不需要进一步训练）来提取环境的压缩历史表示。一些研究使用LLM来总结提供给智能体的过去视觉观察，这样我们就可以为智能体提供一个记忆。使用冻结的模型显然是最简单的替代方案，但当智能体在现实世界中部署时，由于现实世界的变化与训练环境之间的差异，性能可能会迅速下降。因此，我们可以对智能体和LLM进行微调。使用特征提取器（一个LLM或其他大型模型）使智能体更容易学习，因为这些特征对环境变化（亮度、颜色等变化）的鲁棒性更强，但另一方面，它们有额外的计算成本。
- en: The capabilities of LLMs can be used to make the task clearer. For example,
    instructions in natural language can be adapted by an LLM into a set of instructions
    that are clearer to the agent (for example, when playing a video game, a textual
    description of the task could be transformed into a set of instructions on how
    to move the character). An LLM can also be used to translate an agent’s surroundings
    into usable information. These approaches are particularly promising but are currently
    limited in scope.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的能力可以用来使任务更清晰。例如，自然语言中的指令可以通过LLM调整为对智能体更清晰的指令集（例如，在玩视频游戏时，任务的文本描述可以转换为一套关于如何移动角色的指令）。LLM还可以用来将智能体的环境翻译成可用的信息。这些方法特别有前景，但目前范围有限。
- en: '![Figure 8.31 – LLM as an information processor (https://arxiv.org/pdf/2404.00282)](img/B21257_08_31.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图8.31 – LLM作为信息处理器](https://arxiv.org/pdf/2404.00282)(img/B21257_08_31.jpg)'
- en: Figure 8.31 – LLM as an information processor ([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.31 – LLM作为信息处理器([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
- en: Reward designer
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励设计师
- en: When knowledge of the problem is available, or when the reward can be defined
    by a clear and deterministic function (such as a game score or a win/loss condition),
    designing the reward function is straightforward. For example, in Atari games
    (or other games), it is easy to draw a reward function (e.g., victory represents
    a positive signal and defeat a negative signal). There are many applications where
    this is not possible because the tasks are long and complex, the rewards are scattered,
    and so on. In such cases, the knowledge inherent in an LLM (the knowledge gained
    during pre-training, coding abilities, and reasoning skills) could be used to
    generate the reward. It can be used indirectly (an implicit reward model) or directly
    (an explicit reward model). For example, a user can define expected behavior in
    a prompt, and an LLM can evaluate the agent’s behavior during training, providing
    a reward and a penalty. So, you can use direct feedback from the LLM, or an LLM
    can generate the code for a reward function. In the second approach, the function
    can be modified by the LLM during training (for example, after the agent has acquired
    some skills, making it harder to get a reward).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题知识可用，或者当奖励可以通过一个清晰且确定性的函数定义（例如游戏得分或胜负条件）时，设计奖励函数是直接的。例如，在Atari游戏（或其他游戏）中，很容易绘制出奖励函数（例如，胜利代表一个积极信号，失败代表一个消极信号）。在许多应用中，这是不可能的，因为任务既长又复杂，奖励分散，等等。在这种情况下，LLM固有的知识（在预训练期间获得的知识、编码能力和推理技能）可以用来生成奖励。它可以间接使用（隐式奖励模型）或直接使用（显式奖励模型）。例如，用户可以在提示中定义期望的行为，LLM可以在训练期间评估智能体的行为，提供奖励和惩罚。因此，你可以使用LLM的直接反馈，或者LLM可以生成奖励函数的代码。在第二种方法中，函数可以在训练期间由LLM修改（例如，在智能体获得一些技能后，使其更难获得奖励）。
- en: An LLM can be an implicit reward model that provides a reward (or auxiliary
    reward) based on the task description. One technique for this is direct prompting,
    in which instructions are given to the LLM to evaluate the agent’s behavior or
    decide on a reward. These approaches can mimic human feedback to evaluate an agent’s
    behavior in real time. Alternatively, an alignment score can be used, for example,
    between the outcome of an action and the goal (in other words, evaluating the
    similarity between the expected outcome and reality). In some approaches, one
    uses the contrastive alignment between language instructions and the image observations
    of the agent, thus exploiting models that are multimodal. Obviously, the process
    of aligning human intentions and LLM-reward generation is not easy. There can
    be ambiguities, and the system does not always work with low-quality instruction,
    but it seems a promising avenue.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LLM可以是一个隐式奖励模型，它根据任务描述提供奖励（或辅助奖励）。实现这一点的技术之一是直接提示，其中向LLM提供指令以评估代理的行为或决定奖励。这些方法可以模拟人类反馈以实时评估代理的行为。或者，可以使用对齐分数，例如，在动作结果和目标之间（换句话说，评估预期结果与现实之间的相似性）。在某些方法中，使用语言指令和代理的图像观察之间的对比对齐，从而利用多模态模型。显然，对齐人类意图和LLM奖励生成的过程并不容易。可能会有歧义，系统并不总是能够处理低质量的指令，但这似乎是一条有希望的途径。
- en: An explicit reward model exploits the ability of an LLM to generate code, thus
    generating a function (making the decision-making and reward-generation process
    by the LLM more transparent). This allows functions for subgoals to be generated
    automatically (e.g., having a robot learn low-level tasks using high-level instructions
    that are translated into a reward function by the LLM). The main limitation of
    this approach is the common-sense reasoning limitation of LLMs. LLMs are not capable
    of real reasoning or true generalization, so they are limited by what they have
    seen during pre-training. Highly specialized tasks are not seen by LLMs during
    pre-training, thus limiting the applicability of these approaches to a selected
    set of tasks. Adding context and additional information could mitigate this problem.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 显式奖励模型利用了LLM生成代码的能力，从而生成一个函数（使LLM的决策和奖励生成过程更加透明）。这允许自动生成子目标的函数（例如，让机器人通过使用LLM翻译成奖励函数的高级指令来学习低级任务）。这种方法的主要局限性是LLMs的常识推理限制。LLMs无法进行真正的推理或真正的泛化，因此它们受限于预训练期间所见的内容。在预训练期间，LLMs没有看到高度专业化的任务，因此限制了这些方法在选定任务集上的适用性。添加上下文和附加信息可以缓解这个问题。
- en: '![Figure 8.32 – LLM as a reward designer (https://arxiv.org/pdf/2404.00282)](img/B21257_08_32.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![图8.32 – LLM作为奖励设计师 (https://arxiv.org/pdf/2404.00282)](img/B21257_08_32.jpg)'
- en: Figure 8.32 – LLM as a reward designer ([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.32 – LLM作为奖励设计师 ([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
- en: Decision-maker
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策者
- en: Since RL has problems in many cases with sample and exploration inefficiency,
    LLMs can be used in decision-making and thus help in choosing actions. LLMs can
    be used to reduce the set of actions in a certain state (for example, when many
    actions are possible). Reducing the set of actions reduces the exploration space,
    thus increasing exploration efficiency. For example, an LLM can be used to train
    robots on what actions to take in a world, reducing exploration time.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在许多情况下RL存在样本和探索效率低下的问题，LLMs可以在决策中使用，从而帮助选择行动。LLMs可以用来减少某个状态下的动作集合（例如，当存在许多可能的动作时）。减少动作集合减少了探索空间，从而提高了探索效率。例如，LLM可以用来训练机器人在世界中的行动，减少探索时间。
- en: The transformer (or derivative models) has shown great potential in RL. The
    idea behind it is to treat these problems as sequence modeling problems (instead
    of trial and error). LLMs can then be seen as a decision-making model, which has
    to decide on a sequence of problems (as we mentioned in [*Chapter 2*](B21257_02.xhtml#_idTextAnchor032),
    the transformer is trained on a sequence of problems, so making a decision on
    a sequence of states is congenial to its training). An LLM can be fine-tuned to
    leverage the internal representation of the model. In fact, in this way, we leverage
    the representation learned from an LLM (being trained with a huge quantity of
    text, an LLM has a vast amount of knowledge already acquired that can be applied
    to a task) to decide an action. Using prior knowledge reduces the need for data
    collection and exploration (hence, sample efficiency) and makes the system more
    efficient toward long-term rewards or sparse reward environments. Several studies
    have shown not only the transferability of knowledge learned from an LLM to other
    models but also the improved performance of the whole system on different benchmarks.
    In addition, vision-language models can be used to be able to adapt the system
    to multimodal environments. Using an LLM as a decision-maker is still computationally
    expensive (even if only used in inference and without the need for fine-tuning).
    As a result, current studies are focusing on trying to reduce the computational
    cost of these approaches.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器（或其衍生模型）在强化学习（RL）中显示出巨大的潜力。其背后的想法是将这些问题视为序列建模问题（而不是试错）。因此，LLM可以被看作是一个决策模型，它必须决定一系列问题（正如我们在[*第2章*](B21257_02.xhtml#_idTextAnchor032)中提到的，变换器是在一系列问题上进行训练的，因此对一系列状态做出决策符合其训练）。一个LLM可以被微调以利用模型的内部表示。实际上，通过这种方式，我们利用从LLM（使用大量文本进行训练，LLM已经积累了大量可以应用于任务的已知知识）学习到的表示来决定一个动作。使用先验知识减少了数据收集和探索的需求（因此，提高了样本效率），并使系统在长期奖励或稀疏奖励环境中更加高效。一些研究表明，不仅从LLM学习到的知识可以转移到其他模型，而且整个系统在不同基准上的性能也得到了提高。此外，视觉-语言模型可以用来使系统能够适应多模态环境。将LLM用作决策者仍然计算成本高昂（即使仅在推理中使用且不需要微调）。因此，当前的研究正致力于尝试降低这些方法的计算成本。
- en: Alternatively, an LLM can guide the agent in choosing actions by generating
    reasonable action candidates or expert actions. For example, in environments such
    as text-based games, the action space is very large, and only a fraction of the
    actions is currently available, so an agent can learn with extensive trial-and-error;
    however, this exploration is very inefficient. An LLM can reduce this action space
    by generating an action set by understanding the task. This makes it possible
    to reduce exploration and make it more efficient, collect more rewards, and speed
    up training. Typically, in these approaches, we have an LLM that generates a set
    of actions and another neural network that generates the Q-values of these candidates.
    The same approach has been extended to robots that have to follow human instructions,
    where an LLM generates possible actions. This approach is limited owing to the
    inheritance of the biases and limitations of an LLM (since an LLM decides the
    action space and generates it according to its knowledge and biases).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，LLM可以通过生成合理的动作候选或专家动作来引导智能体选择动作。例如，在基于文本的游戏等环境中，动作空间非常大，而目前只有一小部分动作可用，因此智能体可以通过广泛的试错来学习；然而，这种探索效率非常低。LLM可以通过理解任务来生成动作集，从而减少动作空间。这使得减少探索并使其更有效率、收集更多奖励和加快训练成为可能。通常，在这些方法中，我们有一个生成动作集的LLM和另一个生成候选动作Q值的神经网络。相同的方法已经扩展到需要遵循人类指令的机器人，其中LLM生成可能的动作。这种方法由于LLM的偏差和局限性的继承而受到限制（因为LLM决定动作空间并根据其知识和偏差生成它）。
- en: '![Figure 8.33 – LLM as a decision-maker (https://arxiv.org/pdf/2404.00282)](img/B21257_08_33.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![图8.33 – 作为决策者的LLM (https://arxiv.org/pdf/2404.00282)](img/B21257_08_33.jpg)'
- en: Figure 8.33 – LLM as a decision-maker ([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.33 – 作为决策者的LLM ([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
- en: Generator
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器
- en: Model-based RL relies on world models to learn the dynamics of the environment
    and simulate trajectories. The capabilities of an LLM can be to generate accurate
    trajectories or to explain policy choices.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的RL依赖于世界模型来学习环境的动态并模拟轨迹。LLM的能力可以是生成准确的轨迹或解释策略选择。
- en: An LLM has an inherent generative capacity that allows it to be used as a generator.
    An LLM can then be used as a world model simulator, where the system generates
    accurate trajectories that the agent uses to learn and plan. This has been used
    with video games, where an LLM can generate the trajectories and thus reduce the
    time it takes an agent to learn the game (better sample efficiency). The LLM’s
    generative capabilities can then be used to predict the future. Although promising,
    there is still difficulty in aligning the abstract knowledge of an LLM with the
    reality of an environment, limiting the impact of its generative capability.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: LLM具有固有的生成能力，使其可以用作生成器。然后，LLM可以用作世界模型模拟器，系统生成智能体用于学习和规划的准确轨迹。这已经在视频游戏中得到应用，其中LLM可以生成轨迹，从而减少智能体学习游戏所需的时间（提高样本效率）。LLM的生成能力还可以用于预测未来。尽管前景看好，但仍然难以将LLM的抽象知识与环境现实对齐，限制了其生成能力的影响。
- en: Another interesting approach is where an LLM is used to explain the policy of
    an RL system. **Explainable RL** (**XRL**) is a subfield at the intersection of
    explainable machine learning and RL that is growing. XRL seeks to explain an agent’s
    behavior clearly to a human being. An LLM could then be used to explain in natural
    language why an agent makes a certain decision or responds in a certain way to
    a change in environment. As a policy interpreter, an LLM given a state and an
    action should explain an agent’s behavior. These explanations should then be understandable
    to a human, thus allowing an agent’s safety to be checked. Of course, the quality
    of the explanations depends on the LLM’s ability to understand the representation
    of the features of the environment and the implicit logic of the policy. It is
    difficult to use domain knowledge or examples to improve understanding of a complex
    policy (especially for complex environments).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有趣的方法是使用一个大型语言模型（LLM）来解释强化学习（RL）系统的策略。**可解释的强化学习**（**XRL**）是可解释机器学习和强化学习交叉领域的一个新兴子领域。XRL旨在向人类清晰地解释智能体的行为。然后，LLM可以用来用自然语言解释智能体为何做出某种决策或对环境变化做出某种反应。作为一个策略解释器，给定一个状态和一个动作，LLM应该解释智能体的行为。这些解释应该对人类来说是可理解的，从而允许检查智能体的安全性。当然，解释的质量取决于LLM理解环境特征表示和政策隐含逻辑的能力。使用领域知识或例子来提高对复杂策略（尤其是对复杂环境）的理解是困难的。
- en: '![Figure 8.34 – LLM as a generator (https://arxiv.org/pdf/2404.00282)](img/B21257_08_34.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![图8.34 – LLM作为生成器 (https://arxiv.org/pdf/2404.00282)](img/B21257_08_34.jpg)'
- en: Figure 8.34 – LLM as a generator ([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.34 – LLM作为生成器 ([https://arxiv.org/pdf/2404.00282](https://arxiv.org/pdf/2404.00282))
- en: 'LLM-enhanced RL can be useful in a variety of applications:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: LLM增强的RL可以在各种应用中发挥作用：
- en: '**Robotics**: Using LLMs can improve the interaction between humans and robots,
    help robots better understand human needs or human logic, and improve their decision-making
    and planning capabilities.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人技术**：使用LLM可以提高人类与机器人之间的交互，帮助机器人更好地理解人类需求或人类逻辑，并提高它们的决策和规划能力。'
- en: '**Autonomous driving**: RL is used in autonomous driving to make decisions
    in changing environments that are complex and where input from different sensors
    (visual, lidar, radar) must be analyzed along with contextual information (traffic
    laws, human behavior, unexpected problems). LLMs can improve the ability to process
    and integrate this multimodal information, better understand instructions, and
    improve the goal and rewards (e.g., design reward functions that take into account
    not only safety but also passenger comfort and engine efficiency).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动驾驶**：在自动驾驶中，强化学习用于在复杂环境中做出决策，这些环境复杂，需要分析来自不同传感器（视觉、激光雷达、雷达）的输入，并考虑上下文信息（交通法规、人类行为、意外问题）。LLM可以提高处理和整合这种多模态信息的能力，更好地理解指令，并改善目标和奖励（例如，设计考虑安全、乘客舒适度和发动机效率的奖励函数）。'
- en: '**Healthcare recommendations**: RL is used in healthcare to learn recommendations
    and suggestions. LLMs can be used for their vast knowledge and ability to analyze
    huge amounts of patient data and medical data, accelerating the agent’s learning
    process, or providing information for better learning.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗建议**：在医疗保健中，强化学习用于学习建议和推荐。LLM可以用于其广泛的知识和能够分析大量患者数据和医疗数据的能力，加速智能体的学习过程，或为更好的学习提供信息。'
- en: '**Energy management**: RL is used to improve the use, transportation, conversion,
    and storage of energy. In addition, it is expected to play an important role in
    future technologies such as nuclear fusion. LLMs can be used to improve sample
    efficiency, multitask optimization, and much more.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**能源管理**：强化学习用于提高能源的使用、运输、转换和储存。此外，它预计将在未来的技术（如核聚变）中发挥重要作用。LLM可用于提高样本效率、多任务优化等。'
- en: Despite all these opportunities, there are also a number of limitations to the
    use of LLMs in RL. The first challenge is that the LLM-enhanced RL paradigm is
    highly dependent on the capabilities of the LLM. LLMs suffer from bias and can
    hallucinate; an agent then inherits these problems from the LLM. In addition,
    LLMs can also misinterpret the task and data, especially when they are complex
    or noisy. In addition, if the task or environment is not represented in their
    pre-training, LLMs have problems adapting to new environments and tasks. To limit
    these effects, the use of synthetic data, fine-tuning the model, or use of continual
    learning has been proposed. Continual learning could allow a model to adapt to
    new tasks and new environments, without forgetting what the model has learned
    previously. To date, though, continual learning and catastrophic forgetting are
    open problems in deep learning.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些机会，但在强化学习中使用LLM也存在一些局限性。第一个挑战是LLM增强的RL范式高度依赖于LLM的能力。LLM存在偏差并且可能产生幻觉；智能体随后从LLM继承了这些问题。此外，LLM还可能误解任务和数据，尤其是在它们复杂或嘈杂时。此外，如果任务或环境没有在它们的预训练中表示，LLM在适应新环境和任务时会有问题。为了限制这些影响，已经提出了使用合成数据、微调模型或使用持续学习的方法。持续学习可能允许模型适应新任务和新环境，而不会忘记模型之前学到的内容。然而，到目前为止，持续学习和灾难性遗忘仍然是深度学习中的开放性问题。
- en: In addition, the addition of an LLM brings a higher computational cost (both
    in training and in inference), and an increase in latency time. Several techniques
    can be used to reduce this computational cost, such as quantization, pruning,
    or using small models. Some approaches use *mixture of experts*, allowing conditional
    computation, transformer variants (state space models), caching strategies, and
    so on.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，添加一个大型语言模型（LLM）会带来更高的计算成本（在训练和推理过程中），以及延迟时间的增加。可以使用几种技术来降低这种计算成本，例如量化、剪枝或使用小型模型。一些方法使用*专家混合*，允许条件计算、变换器变体（状态空间模型）、缓存策略等。
- en: Finally, one should not forget that the use of LLMs also opens up ethical, legal,
    and safety issues. The same problems we saw in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042)
    are also applicable to these systems. For example, data privacy and intellectual
    property remain open topics for applications in sensitive fields such as healthcare
    or finance.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，不应忘记使用LLM也引发了道德、法律和安全问题。我们在[*第3章*](B21257_03.xhtml#_idTextAnchor042)中看到的问题也适用于这些系统。例如，数据隐私和知识产权仍然是敏感领域（如医疗保健或金融）应用中的开放性问题。
- en: Key takeaways
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: Because this chapter was dense in terms of theory, we decided to add a small
    recap section. This chapter introduced RL as a core approach to enabling intelligent
    agents to learn from interaction with dynamic environments through trial and error,
    similar to how humans learn by acting, observing outcomes, and adjusting behavior.
    RL differs from supervised learning by focusing on learning from rewards rather
    than labeled data, and it is especially suited to tasks with delayed feedback
    and evolving decision sequences.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章在理论方面内容密集，我们决定添加一个小结部分。本章介绍了强化学习（RL）作为使智能体通过与环境交互并通过试错来学习的一种核心方法，类似于人类通过行动、观察结果和调整行为来学习的方式。RL与监督学习不同，它侧重于从奖励而非标记数据中学习，并且特别适合具有延迟反馈和演变决策序列的任务。
- en: RL is a machine learning paradigm where an agent learns to make decisions by
    interacting with an environment to maximize cumulative rewards. It learns through
    trial and error, balancing exploration (trying new actions) and exploitation (using
    known strategies).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种机器学习范式，其中智能体通过与环境的交互来学习做出决策，以最大化累积奖励。它通过试错来学习，平衡探索（尝试新动作）和利用（使用已知策略）。
- en: 'In summary, we have these classes of methods:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们有以下几类方法：
- en: '**Model-free versus** **model-based RL**:'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无模型与** **基于模型** **的强化学习**：'
- en: '**Model-free methods** (e.g., DQN, REINFORCE) learn directly from interaction
    without modeling the environment. They are simpler and more scalable.'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无模型方法**（例如，DQN、REINFORCE）直接从交互中学习，而不对环境进行建模。它们更简单且更具可扩展性。'
- en: '**Model-based methods** use an internal model to simulate outcomes and plan
    ahead. They are more sample-efficient and suitable for environments where planning
    is crucial but are harder to design and compute.'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的方法**使用内部模型来模拟结果并提前规划。它们更有效率，适用于规划至关重要的环境，但设计和计算更困难。'
- en: '**On-policy versus** **off-policy methods**:'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略方法与离策略方法的比较**：'
- en: '**On-policy methods** learn from the data generated by the current policy (e.g.,
    REINFORCE, PPO), making them more stable but sample inefficient.'
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离策略方法**从当前策略生成的数据中学习（例如REINFORCE、PPO），这使得它们更稳定但样本效率较低。'
- en: '**Off-policy methods** (e.g., DQN) can learn from past or alternative policies,
    improving sample efficiency and exploration flexibility.'
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离策略方法**（例如DQN）可以从过去或替代策略中学习，从而提高样本效率和探索灵活性。'
- en: '**Main** **algorithms discussed**:'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**讨论的主要算法**：'
- en: '**Q-Learning and DQN**: Learn value functions using lookup tables or neural
    networks.'
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q-Learning和DQN**：使用查找表或神经网络学习价值函数。'
- en: '**REINFORCE**: A basic policy gradient method using stochastic policies.'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**REINFORCE**：使用随机策略的基本策略梯度方法。'
- en: '**PPO**: Balances stability and performance by clipping policy updates.'
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PPO**：通过剪辑策略更新来平衡稳定性和性能。'
- en: '**Actor-Critic**: Combines value estimation and policy learning for more robust
    updates.'
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Actor-Critic**：结合价值估计和政策学习，以实现更稳健的更新。'
- en: '**AlphaZero**: Combines deep learning with Monte Carlo Tree Search for self-play-based
    strategy optimization in complex games.'
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlphaZero**：将深度学习与蒙特卡洛树搜索相结合，用于复杂游戏中的基于自我对弈的策略优化。'
- en: '**Practical** **use cases**:'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际应用案例**：'
- en: '**Gaming**: RL agents such as AlphaZero and DQN have mastered games such as
    Go, Chess, and Atari titles.'
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**游戏**：例如AlphaZero和DQN这样的强化学习代理已经掌握了围棋、象棋和Atari游戏等。'
- en: '**Robotics**: RL allows robots to learn complex movement and interaction policies
    through simulation and real-world feedback.'
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人技术**：强化学习允许机器人通过模拟和现实世界的反馈学习复杂的运动和交互策略。'
- en: '**Autonomous vehicles**: RL enables the learning of driving strategies in dynamic
    and uncertain environments.'
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动驾驶汽车**：强化学习使代理能够在动态和不确定的环境中学习驾驶策略。'
- en: '**Optimization and control**: Applied in finance, healthcare, logistics, and
    industrial automation for sequential decision-making.'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化和控制**：应用于金融、医疗保健、物流和工业自动化中的顺序决策。'
- en: Summary
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In the previous chapters, the main question was how to find information and
    how to deliver it effectively to an LLM. In such cases, the model is a passive
    agent that receives information and responds. With this chapter, we are trying
    to move away from this paradigm, toward an idea where an agent explores an environment,
    learns through this exploration, performs actions, and learns from the feedback
    that the environment provides to it. In this view, the model is an active component
    that interacts with the environment and can modify it. This view is also much
    closer to how we humans learn. In our exploration of the external world, we receive
    feedback that guides us in our learning. Although much of the world has been noted
    in texts, the real world cannot be reduced to a textual description. Therefore,
    an agent cannot learn certain knowledge and skills without interacting with the
    world. RL is a field of artificial intelligence that focuses on an agent’s interactions
    with the environment and how it can learn from it.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，主要问题是如何找到信息以及如何有效地将信息传递给一个语言模型（LLM）。在这种情况下，模型是一个被动代理，接收信息并做出响应。在本章中，我们试图摆脱这种范式，转向一个理念，即代理探索环境，通过这种探索学习，执行动作，并从环境提供的反馈中学习。在这种观点下，模型是一个主动组件，与环境互动并可以对其进行修改。这种观点也与人类学习的方式更为接近。在我们对外部世界的探索中，我们接收到的反馈引导我们的学习。尽管世界的大部分内容已经在文本中被记录下来，但现实世界不能简化为文本描述。因此，代理在与世界互动之前无法学习某些知识和技能。强化学习是人工智能的一个领域，它关注代理与环境之间的互动以及它如何从中学习。
- en: In this chapter, therefore, we introduced the fundamentals of RL. In the first
    section, we discussed the basic components of an RL system (agent, environment,
    reward, and action). We then discussed the main question of RL, how to balance
    exploration and exploitation. Indeed, an agent has a goal (accomplish a task)
    but learns how to accomplish this task through exploration. For example, we saw
    in the multi-armed bandit example how a greedy model performs worse than a model
    that explores the possibilities. This principle remains fundamental when we define
    an agent to solve complex problems such as solving a video game. To solve complex
    tasks, we introduced the use of neural networks (deep RL). We saw that there are
    different types of algorithms with different advantages and disadvantages, and
    we saw how we can set one of them to win in a classic video game. Once we trained
    our model, we discussed how LLM and RL fields are increasingly intersecting. In
    this way, we saw how the strengths of the two fields can be synergistic.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们介绍了强化学习（RL）的基础知识。在第一部分，我们讨论了RL系统的基本组成部分（代理、环境、奖励和动作）。然后，我们讨论了RL的主要问题，即如何平衡探索和利用。实际上，代理有一个目标（完成任务），但通过探索来学习如何完成这个任务。例如，我们在多臂老虎机示例中看到，贪婪模型的表现不如探索所有可能性的模型。当我们定义代理来解决复杂问题，如解决视频游戏时，这一原则仍然是基本的。为了解决复杂任务，我们引入了神经网络（深度强化学习）的使用。我们看到了不同类型的算法具有不同的优缺点，并看到了我们如何将其中之一设置为在经典视频游戏中获胜。一旦我们训练了我们的模型，我们就讨论了LLM和RL领域如何日益交叉。这样，我们就看到了两个领域的优势如何协同作用。
- en: From this chapter on, the focus will be more applicative. We will see how an
    agent can generally accomplish a task. In the upcoming chapters, the agent will
    mainly be an LLM who will use tools to perform actions and accomplish tasks. The
    choice, then, for the agent will not be which action to take but which tool to
    choose in order to accomplish a task. Despite the fact that an LLM agent interacts
    with the environment, one main difference is that there will be no training. Training
    an LLM is a complex task, so in these systems, we try to train them as little
    as possible. If, in the previous chapters (*5–7*), we tried to leverage the comprehension
    skills of an LLM, in the next chapters, we will try to leverage the skills of
    LLMs to interact with the environment or with other agents – skills that are possible
    anyway because an LLM can understand a task and instructions.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，重点将更加应用性。我们将看到代理通常如何完成任务。在接下来的章节中，代理将主要是一个LLM，它将使用工具执行动作和完成任务。因此，代理的选择将不是采取哪个动作，而是选择哪个工具来完成一个任务。尽管LLM代理与环境交互，但一个主要区别是不会有训练。训练LLM是一个复杂任务，因此在这些系统中，我们尽量减少训练。如果在之前的章节（*5–7*）中，我们试图利用LLM的理解能力，那么在接下来的章节中，我们将尝试利用LLM与环境或与其他代理交互的技能——这些技能无论如何都是可能的，因为LLM可以理解任务和指令。
- en: Further reading
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Ghasemi, *An Introduction to Reinforcement Learning: Fundamental Concepts and
    Practical Applications*, 2024, [https://arxiv.org/abs/2408.07712](https://arxiv.org/abs/2408.07712)'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghasemi, *《强化学习导论：基本概念与实际应用*》，2024年，[https://arxiv.org/abs/2408.07712](https://arxiv.org/abs/2408.07712)
- en: Mnih, *Playing Atari with Deep Reinforcement Learning*, 2013, [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih, *《使用深度强化学习玩Atari游戏*》，2013年，[https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)
- en: Hugging Face, *Proximal Policy Optimization (**PPO)*, [https://huggingface.co/blog/deep-rl-ppo](https://huggingface.co/blog/deep-rl-ppo)
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face, *《近端策略优化（**PPO**）*》，[https://huggingface.co/blog/deep-rl-ppo](https://huggingface.co/blog/deep-rl-ppo)
- en: Wang, *Learning Reinforcement Learning by* *LearningREINFORCE*, [https://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf](https://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf)
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, *通过* *LearningREINFORCE* *学习强化学习*，[https://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf](https://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf)
- en: Kaufmann, *A Survey of Reinforcement Learning from Human Feedback*, 2024, [https://arxiv.org/pdf/2312.14925](https://arxiv.org/pdf/2312.14925)
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaufmann, *《基于人类反馈的强化学习概述*》，2024年，[https://arxiv.org/pdf/2312.14925](https://arxiv.org/pdf/2312.14925)
- en: Bongratz, *How to Choose a Reinforcement-Learning Algorithm*, 2024, [https://arxiv.org/abs/2407.20917v1](https://arxiv.org/abs/2407.20917v1)
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bongratz, *《如何选择强化学习算法*》，2024年，[https://arxiv.org/abs/2407.20917v1](https://arxiv.org/abs/2407.20917v1)
- en: Schulman, *Proximal Policy Optimization Algorithms*, 2017, [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman, *《近端策略优化算法*》，2017年，[https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
- en: OpenAI, *Proximal Policy* *Optimization*, [https://openai.com/index/openai-baselines-ppo/](https://openai.com/index/openai-baselines-ppo/)
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI, *《近端策略优化》*, [https://openai.com/index/openai-baselines-ppo/](https://openai.com/index/openai-baselines-ppo/)
- en: OpenAI Spinning UP, *Proximal Policy* *Optimization*, [https://spinningup.openai.com/en/latest/algorithms/ppo.html](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Spinning UP, *《近端策略优化》*, [https://spinningup.openai.com/en/latest/algorithms/ppo.html](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
- en: Bick, *Towards Delivering a Coherent Self-Contained Explanation of Proximal
    Policy Optimization*, 2021, [https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bick, *《迈向对近端策略优化的连贯自包含解释》*, 2021, [https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)
- en: Silver, *Mastering Chess and Shogi by Self-Play with a General Reinforcement
    Learning Algorithm*, 2017, [https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815)
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver, *《通过通用强化学习算法自我博弈掌握国际象棋和将棋》*, 2017, [https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815)
- en: McGrath, *Acquisition of Chess Knowledge in AlphaZero*, 2021, [https://arxiv.org/abs/2111.09259](https://arxiv.org/abs/2111.09259)
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McGrath, *《AlphaZero中棋类知识的获取》*, 2021, [https://arxiv.org/abs/2111.09259](https://arxiv.org/abs/2111.09259)
- en: 'DeepMind, *AlphaZero: Shedding* *New Light on Chess, Shogi, and* *Go*, 2018,
    [https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepMind, *《AlphaZero：对国际象棋、将棋和围棋的新见解》*, 2018, [https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/)
- en: Gao, *Efficiently Mastering the Game of NoGo with Deep Reinforcement Learning
    Supported by Domain Knowledge*, 2021, [https://www.mdpi.com/2079-9292/10/13/1533](https://www.mdpi.com/2079-9292/10/13/1533)
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao, *《利用领域知识高效掌握NoGo游戏与深度强化学习》*, 2021, [https://www.mdpi.com/2079-9292/10/13/1533](https://www.mdpi.com/2079-9292/10/13/1533)
- en: Francois-Lavet, *An Introduction to Deep Reinforcement Learning*, 2018, [https://arxiv.org/abs/1811.12560](https://arxiv.org/abs/1811.12560)
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Francois-Lavet, *《深度强化学习导论》*, 2018, [https://arxiv.org/abs/1811.12560](https://arxiv.org/abs/1811.12560)
- en: 'Tang, *Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes*,
    2024, [https://arxiv.org/abs/2408.03539](https://arxiv.org/abs/2408.03539)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang, *《机器人深度强化学习：现实世界成功案例综述》*, 2024, [https://arxiv.org/abs/2408.03539](https://arxiv.org/abs/2408.03539)
- en: 'Mohan, *Structure in Deep Reinforcement Learning: A Survey and Open Problems*,
    2023, [https://arxiv.org/abs/2306.16021](https://arxiv.org/abs/2306.16021)'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohan, *《深度强化学习中的结构：综述与开放性问题》*, 2023, [https://arxiv.org/abs/2306.16021](https://arxiv.org/abs/2306.16021)
- en: 'Cao, *Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,
    Taxonomy, and Methods*, 2024, [https://arxiv.org/abs/2404.00282](https://arxiv.org/abs/2404.00282)'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao, *《关于大型语言模型增强的强化学习综述：概念、分类法和方法》*, 2024, [https://arxiv.org/abs/2404.00282](https://arxiv.org/abs/2404.00282)
- en: 'Part 3: Creating Sophisticated AI to Solve Complex Scenarios'
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：创建解决复杂场景的复杂AI
- en: This final part focuses on assembling the components introduced in the previous
    chapters to build fully-fledged, production-ready AI systems. It begins with the
    design and orchestration of single- and multi-agent systems, where LLMs collaborate
    with tools, APIs, and other models to tackle complex, multi-step tasks. The section
    then guides you through the practical aspects of building and deploying AI agent
    applications using modern tools such as Streamlit, asynchronous programming, and
    containerization technologies such as Docker. Finally, the book closes with a
    forward-looking discussion on the future of AI agents, their impact across industries
    such as healthcare and law, and the ethical and technical challenges that lie
    ahead. This part empowers you to move from experimentation to real-world deployment,
    preparing them to contribute to the next wave of intelligent systems.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 本最后一部分专注于将前几章中介绍的部分组件组装起来，以构建完全成熟、适用于生产的AI系统。它从单代理和多代理系统的设计和编排开始，其中LLMs与工具、API和其他模型协作，以解决复杂的多步骤任务。该部分随后引导您了解使用现代工具（如Streamlit、异步编程和Docker等容器化技术）构建和部署AI代理应用程序的实际方面。最后，本书以对AI代理未来的展望结束，讨论其在医疗保健和法律等行业的跨行业影响，以及未来面临的伦理和技术挑战。本部分使您能够从实验过渡到现实世界的部署，为参与下一波智能系统做好准备。
- en: 'This part has the following chapters:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 9*](B21257_09.xhtml#_idTextAnchor156)*, Creating Single- and Multi-Agent
    Systems*'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B21257_09.xhtml#_idTextAnchor156)*，创建单代理和多代理系统*'
- en: '[*Chapter 10*](B21257_10.xhtml#_idTextAnchor179)*, Building an AI Agent Application*'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B21257_10.xhtml#_idTextAnchor179)*，构建人工智能代理应用*'
- en: '[*Chapter 11*](B21257_11.xhtml#_idTextAnchor215)*, The Future Ahead*'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B21257_11.xhtml#_idTextAnchor215)*，未来的展望*'
