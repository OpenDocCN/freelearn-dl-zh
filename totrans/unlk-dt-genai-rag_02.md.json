["```py\n %pip install langchain_community\n%pip install langchain_experimental\n%pip install langchain-openai\n%pip install langchainhub\n%pip install chromadb\n%pip install langchain\n%pip install beautifulsoup4\n```", "```py\n import IPython\napp = IPython.Application.instance(;\napp.kernel.do_shutdown(True)\n```", "```py\n import os\nfrom langchain_community.document_loaders import WebBaseLoader\nimport bs4\nimport openai\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nimport chromadb\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_experimental.text_splitter import SemanticChunker\n```", "```py\n os.environ['OPENAI_API_KEY'] = 'sk-###################'\nopenai.api_key = os.environ['OPENAI_API_KEY']\n```", "```py\n loader = WebBaseLoader(\n    web_paths=(\"https://kbourne.github.io/chapter1.html\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n           class_=(\"post-content\", \"post-title\",\n                   \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n```", "```py\n text_splitter = SemanticChunker(OpenAIEmbeddings())\nsplits = text_splitter.split_documents(docs)\n```", "```py\n vectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n```", "```py\n query = \"How does RAG compare with fine-tuning?\" relevant_docs = retriever.get_relevant_documents(query)\nrelevant_docs\n```", "```py\n prompt = hub.pull(\"jclemens24/rag-prompt\")\nprint(prompt)\n```", "```py\n input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved-context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n```", "```py\n \"You are an assistant for question-answering tasks. Use the following pieces of retrieved-context to answer the question. If you don't know the answer, just say that you don't know. Question: {question}\nContext: {context}\nAnswer:\"\n```", "```py\n def format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n```", "```py\n llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n```", "```py\n rag_chain = (\n    {\"context\": retriever | format_docs,\n     \"question\": RunnablePassthrough()}\n         | prompt\n         | llm\n         | StrOutputParser()\n)\n```", "```py\n rag_chain.invoke(\"What are the advantages of using RAG?\")\n```", "```py\n \"The advantages of using Retrieval Augmented Generation (RAG) include:\\n\\n1\\. **Improved Accuracy and Relevance:** RAG enhances the accuracy and relevance of responses generated by large language models (LLMs) by fetching and incorporating specific information from databases or datasets in real time. This ensures outputs are based on both the model's pre-existing knowledge and the most current and relevant data provided.\\n\\n2\\. **Customization and Flexibility:** RAG allows for the customization of responses based on domain-specific needs by integrating a company's internal databases into the model's response generation process. This level of customization is invaluable for creating personalized experiences and for applications requiring high specificity and detail.\\n\\n3\\. **Expanding Model Knowledge Beyond Training Data:** RAG overcomes the limitations of LLMs, which are bound by the scope of their training data. By enabling models to access and utilize information not included in their initial training sets, RAG effectively expands the knowledge base of the model without the need for retraining. This makes LLMs more versatile and adaptable to new domains or rapidly evolving topics.\"\n```", "```py\n \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Question:    What are the Advantages of using RAG? Context:    Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer's needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that are not readily accessible or are not being fully utilized. Before RAG, most of the services you saw that connected customers or employees with the data resources of the company were just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\\nEstablished Large Language Models (LLM), what we call the foundation models, can be learned in two ways:\\n Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model\\'s intelligence based\n[TRUNCATED FOR BREVITY!]\nAnswer:\"\n```", "```py\n %pip install langchain_community\n%pip install langchain_experimental\n%pip install langchain-openai\n%pip install langchainhub\n%pip install chromadb\n%pip install langchain\n%pip install beautifulsoup4\n```", "```py\n import os\nfrom langchain_community.document_loaders import WebBaseLoader\nimport bs4\nimport openai\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nimport chromadb\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_experimental.text_splitter import SemanticChunker\nos.environ['OPENAI_API_KEY'] = 'sk-###################'\nopenai.api_key = os.environ['OPENAI_API_KEY']\n#### INDEXING ####\nloader = WebBaseLoader(\n    web_paths=(\"https://kbourne.github.io/chapter1.html\",),\n    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n                       class_=(\"post-content\",\n                               \"post-title\",\n                               \"post-header\")\n                   )\n         ),\n)\ndocs = loader.load()\ntext_splitter = SemanticChunker(OpenAIEmbeddings())\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n#### RETRIEVAL and GENERATION ####\nprompt = hub.pull(\"jclemens24/rag-prompt\")\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\")\nrag_chain = (\n    {\"context\": retriever | format_docs,\n     \"question\": RunnablePassthrough()}\n         | prompt\n         | llm\n         | StrOutputParser()\n)\nrag_chain.invoke(\"What are the Advantages of using RAG?\")\n```"]