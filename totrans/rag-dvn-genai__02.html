<html><head></head><body>
  <div id="_idContainer031" class="Basic-Text-Frame">
    <h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">2</span></h1>
    <h1 id="_idParaDest-50" class="chapterTitle"><span class="koboSpan" id="kobo.2.1">RAG Embedding Vector Stores with Deep Lake and OpenAI</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.3.1">There will come a point in the execution of your project where complexity is unavoidable when implementing RAG-driven generative AI. </span><span class="koboSpan" id="kobo.3.2">Embeddings transform bulky structured or unstructured texts into compact, high-dimensional vectors that capture their semantic essence, enabling faster and more efficient information retrieval. </span><span class="koboSpan" id="kobo.3.3">However, we will inevitably be faced with a storage issue as the creation and storage of document embeddings become necessary when managing increasingly large datasets. </span><span class="koboSpan" id="kobo.3.4">You could ask the question at this point, why not use keywords instead of embeddings? </span><span class="koboSpan" id="kobo.3.5">And the answer is simple: although embeddings require more storage space, they capture the deeper semantic meanings of texts, with more nuanced and context-aware retrieval compared to the rigid and often-matched keywords. </span><span class="koboSpan" id="kobo.3.6">This results in better, more pertinent retrievals. </span><span class="koboSpan" id="kobo.3.7">Hence, our option is to turn to vector stores in which embeddings are organized and rapidly accessible.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.4.1">We will begin this chapter by exploring how to go from raw data to an Activeloop Deep Lake vector store via loading OpenAI embedding models. </span><span class="koboSpan" id="kobo.4.2">This requires installing and implementing several cross-platform packages, which leads us to the architecture of such systems. </span><span class="koboSpan" id="kobo.4.3">We will organize our RAG pipeline into separate components because breaking down the RAG pipeline into independent parts will enable several teams to work on a project simultaneously. </span><span class="koboSpan" id="kobo.4.4">We will then set the blueprint for a RAG-driven generative AI pipeline. </span><span class="koboSpan" id="kobo.4.5">Finally, we will build a three-component RAG pipeline from scratch in Python with Activeloop Deep Lake, OpenAI, and custom-built functions.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.5.1">This coding journey will take us into the depths of cross-platform environment issues with packages and dependencies. </span><span class="koboSpan" id="kobo.5.2">We will also face the challenges of chunking data, embedding vectors, and loading them on vector stores. </span><span class="koboSpan" id="kobo.5.3">We will augment the input of a GPT-4o model with retrieval queries and produce solid outputs. </span><span class="koboSpan" id="kobo.5.4">By the end of this chapter, you will fully understand how to leverage the power of embedded documents in vector stores for generative AI.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.6.1">To sum up, this chapter covers the following topics:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.7.1">Introducing document embeddings and vector stores</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.8.1">How to break a RAG pipeline into independent components</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.9.1">Building a RAG pipeline from raw data to Activeloop Deep Lake</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.10.1">Facing the environmental challenge of cross-platform packages and libraries</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.11.1">Leveraging the power of LLMs to embed data with an OpenAI embedding model</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.12.1">Querying an Activeloop Deep Lake vector store to augment user inputs</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.13.1">Generative solid augmented outputs with OpenAI GPT-4o</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.14.1">Let’s begin by learning how to go from raw data to a vector store.</span></p>
    <h1 id="_idParaDest-51" class="heading-1"><span class="koboSpan" id="kobo.15.1">From raw data to embeddings in vector stores</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.16.1">Embeddings convert</span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.17.1"> any form of data (text, images, or audio) into real numbers. </span><span class="koboSpan" id="kobo.17.2">Thus, a document is converted into a vector. </span><span class="koboSpan" id="kobo.17.3">These mathematical representations of documents allow us to calculate the distances between documents and retrieve similar data.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.18.1">The raw data (books, articles, blogs, pictures, or songs) is first collected and cleaned to remove noise. </span><span class="koboSpan" id="kobo.18.2">The prepared data is then fed into a model such as OpenAI </span><code class="inlineCode"><span class="koboSpan" id="kobo.19.1">text-embedding-3-small</span></code><span class="koboSpan" id="kobo.20.1">, which will embed the data. </span><span class="koboSpan" id="kobo.20.2">Activeloop Deep Lake, for example, which we will implement in this </span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.21.1">chapter, will break a text down into pre-defined chunks defined by a certain number of characters. </span><span class="koboSpan" id="kobo.21.2">The size of a chunk could be 1,000 characters, for instance. </span><span class="koboSpan" id="kobo.21.3">We can let the system optimize these chunks, as we will implement them in the </span><em class="italic"><span class="koboSpan" id="kobo.22.1">Optimizing chunking</span></em><span class="koboSpan" id="kobo.23.1"> section of the next chapter. </span><span class="koboSpan" id="kobo.23.2">These chunks of text make it easier to process large amounts of data and provide more detailed embeddings of a document, as shown here:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.24.1"><img src="../Images/B31169_02_01.png" alt="A diagram of a number and embedding  Description automatically generated with medium confidence"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.25.1">Figure 2.1: Excerpt of an Activeloop vector store dataset record</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.26.1">Transparency has</span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.27.1"> been the holy grail in AI since the beginning of parametric models, in which the information is buried in learned parameters that produce black box systems. </span><span class="koboSpan" id="kobo.27.2">RAG is a game changer, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.28.1">Figure 2.1</span></em><span class="koboSpan" id="kobo.29.1">, because the content is fully traceable:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.30.1">Left side (Text): In RAG frameworks, every piece of generated content is traceable back to its source data, ensuring the output’s transparency. </span><span class="koboSpan" id="kobo.30.2">The OpenAI generative model will respond, taking the augmented input into account.</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.31.1">Right side (Embeddings): Data embeddings</span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.32.1"> are directly visible and linked to the text, contrasting with parametric models where data origins are encoded within model parameters.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.33.1">Once we have our text and embeddings, the next step is to store them efficiently for quick retrieval. </span><span class="koboSpan" id="kobo.33.2">This is where </span><em class="italic"><span class="koboSpan" id="kobo.34.1">vector stores</span></em><span class="koboSpan" id="kobo.35.1"> come into</span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.36.1"> play. </span><span class="koboSpan" id="kobo.36.2">A vector store is a specialized database designed to handle high-dimensional data like embeddings. </span><span class="koboSpan" id="kobo.36.3">We can create datasets on serverless platforms such as Activeloop, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.37.1">Figure 2.2</span></em><span class="koboSpan" id="kobo.38.1">. </span><span class="koboSpan" id="kobo.38.2">We can create and access them in code through an API, as we will do in the </span><em class="italic"><span class="koboSpan" id="kobo.39.1">Building a RAG pipeline</span></em><span class="koboSpan" id="kobo.40.1"> section of this chapter.</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.41.1"><img src="../Images/B31169_02_02.png" alt="A screenshot of a computer  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.42.1">Figure 2.2: Managing datasets with vector stores</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.43.1">Another feature of </span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.44.1">vector stores is their ability to retrieve data with optimized methods. </span><span class="koboSpan" id="kobo.44.2">Vector stores are built with powerful indexing methods, which we will discuss in the next chapter. </span><span class="koboSpan" id="kobo.44.3">This retrieving capacity allows a RAG model to quickly find and retrieve the most relevant embeddings during the generation phase, augment user inputs, and increase the model’s ability to produce high-quality output.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.45.1">We will now see how to organize a RAG pipeline that goes from data collection, processing, and retrieval to augmented-input generation.</span></p>
    <h1 id="_idParaDest-52" class="heading-1"><span class="koboSpan" id="kobo.46.1">Organizing RAG in a pipeline</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.47.1">A RAG pipeline</span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.48.1"> will typically collect data and prepare it by cleaning it, for example, chunking the documents, embedding them, and storing them in a vector store dataset. </span><span class="koboSpan" id="kobo.48.2">The vector dataset is then queried to augment the user input of a generative AI model to produce an output. </span><span class="koboSpan" id="kobo.48.3">However, it is highly recommended not to run this sequence of RAG in one single program when it comes to using a vector store. </span><span class="koboSpan" id="kobo.48.4">We should at least separate the process into three components:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.49.1">Data collection and preparation</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.50.1">Data embedding and loading into the dataset of a vector store</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.51.1">Querying the vectorized dataset to augment the input of a generative AI model to produce a response</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.52.1">Let’s go through the main</span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.53.1"> reasons for this component approach:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.54.1">Specialization</span></strong><span class="koboSpan" id="kobo.55.1">, which will allow each member of a team to do what they are best at, either collecting and cleaning data, running embedding models, managing vector stores, or tweaking generative AI models.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.56.1">Scalability</span></strong><span class="koboSpan" id="kobo.57.1">, making it easier to upgrade separate components as the technology evolves and scale the different components with specialized methods. </span><span class="koboSpan" id="kobo.57.2">Storing raw data, for example, can be scaled on a different server than the cloud platform, where the embedded vectors are stored in a vectorized dataset.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.58.1">Parallel development</span></strong><span class="koboSpan" id="kobo.59.1">, which allows each team to advance at their pace without waiting for others. </span><span class="koboSpan" id="kobo.59.2">Improvements can be made continually on one component without disrupting the processes of the other components.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.60.1">Maintenance</span></strong><span class="koboSpan" id="kobo.61.1"> is component-independent. </span><span class="koboSpan" id="kobo.61.2">One team can work on one component without affecting the other parts of the system. </span><span class="koboSpan" id="kobo.61.3">For example, if the RAG pipeline is in production, users can continue querying and running generative AI through the vector store while a team fixes the data collection component.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.62.1">Security</span></strong><span class="koboSpan" id="kobo.63.1"> concerns and privacy are minimized because each team can work separately with specific authorization, access, and roles for each component.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.64.1">As we can see, in real-life production environments or large-scale projects, it is rare for a single program or team to manage end-to-end processes. </span><span class="koboSpan" id="kobo.64.2">We are now ready to draw the blueprint of the RAG pipeline that we will build in Python in this chapter.</span></p>
    <h1 id="_idParaDest-53" class="heading-1"><span class="koboSpan" id="kobo.65.1">A RAG-driven generative AI pipeline</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.66.1">Let’s dive into what a real-life</span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.67.1"> RAG pipeline looks like. </span><span class="koboSpan" id="kobo.67.2">Imagine we’re a team that has to deliver a whole system in just a few weeks. </span><span class="koboSpan" id="kobo.67.3">Right off the bat, we’re bombarded with questions like:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.68.1">Who’s going to gather and clean up all the data?</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.69.1">Who’s going to handle setting up OpenAI’s embedding model?</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.70.1">Who’s writing the code to get those embeddings up and running and managing the vector store?</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.71.1">Who’s going to take care of implementing GPT-4 and managing what it spits out?</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.72.1">Within a few minutes, everyone starts looking pretty worried. </span><span class="koboSpan" id="kobo.72.2">The whole thing feels overwhelming—like, seriously, who would even think about tackling all that alone?</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.73.1">So here’s what we do. </span><span class="koboSpan" id="kobo.73.2">We </span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.74.1">split into three groups, each of us taking on different parts of the pipeline, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.75.1">Figure 2.3</span></em><span class="koboSpan" id="kobo.76.1">:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.77.1"><img src="../Images/B31169_02_03.png" alt="A diagram of a pipeline  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.78.1">Figure 2.3: RAG pipeline components</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.79.1">Each of the three groups has one component to implement:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.80.1">Data Collection and Prep (D1 and D2)</span></strong><span class="koboSpan" id="kobo.81.1">: One team </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.82.1">takes on </span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.83.1">collecting the data and cleaning it.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.84.1">Data Embedding and Storage (D2 and D3)</span></strong><span class="koboSpan" id="kobo.85.1">: Another team works on getting the data through</span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.86.1"> OpenAI’s embedding model and stores these vectors in an Activeloop Deep Lake dataset.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.87.1">Augmented Generation (D4, G1-G4, and E1)</span></strong><span class="koboSpan" id="kobo.88.1">: The last team handles the big job of generating</span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.89.1"> content based on user input and retrieval queries. </span><span class="koboSpan" id="kobo.89.2">They use GPT-4 for this, and even though it sounds like a lot, it’s actually a bit easier because they aren’t waiting on anyone else—they just need the computer to do its calculations and evaluate the </span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.90.1">output.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.91.1">Suddenly, the project doesn’t seem so scary. </span><span class="koboSpan" id="kobo.91.2">Everyone has their part to focus on, and we can all work without being distracted by the other teams. </span><span class="koboSpan" id="kobo.91.3">This way, we can all move faster and get the job done without the hold-ups that usually slow things down.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.92.1">The organization of the project, represented in </span><em class="italic"><span class="koboSpan" id="kobo.93.1">Figure 2.3</span></em><span class="koboSpan" id="kobo.94.1">, is a variant of the RAG ecosystem’s framework represented in </span><em class="italic"><span class="koboSpan" id="kobo.95.1">Figure 1.3</span></em><span class="koboSpan" id="kobo.96.1"> of </span><em class="italic"><span class="koboSpan" id="kobo.97.1">Chapter 1</span></em><span class="koboSpan" id="kobo.98.1">, </span><em class="italic"><span class="koboSpan" id="kobo.99.1">Why Retrieval Augmented Generation?</span></em></p>
    <p class="normal"><span class="koboSpan" id="kobo.100.1">We can now begin building a RAG pipeline.</span></p>
    <h1 id="_idParaDest-54" class="heading-1"><span class="koboSpan" id="kobo.101.1">Building a RAG pipeline</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.102.1">We will now build a RAG pipeline by</span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.103.1"> implementing the pipeline described in the previous section and illustrated in </span><em class="italic"><span class="koboSpan" id="kobo.104.1">Figure 2.3</span></em><span class="koboSpan" id="kobo.105.1">. </span><span class="koboSpan" id="kobo.105.2">We will implement three components assuming that three teams (</span><code class="inlineCode"><span class="koboSpan" id="kobo.106.1">Team #1</span></code><span class="koboSpan" id="kobo.107.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.108.1">Team #2</span></code><span class="koboSpan" id="kobo.109.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.110.1">Team #3</span></code><span class="koboSpan" id="kobo.111.1">) work in parallel to implement the pipeline:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.112.1">Data collection and preparation by </span><code class="inlineCode"><span class="koboSpan" id="kobo.113.1">Team #1</span></code></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.114.1">Data embedding and storage by </span><code class="inlineCode"><span class="koboSpan" id="kobo.115.1">Team #2</span></code></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.116.1">Augmented generation by </span><code class="inlineCode"><span class="koboSpan" id="kobo.117.1">Team #3</span></code></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.118.1">The first step is to set up the environment for these components.</span></p>
    <h2 id="_idParaDest-55" class="heading-2"><span class="koboSpan" id="kobo.119.1">Setting up the environment</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.120.1">Let’s face it here and now. </span><span class="koboSpan" id="kobo.120.2">Installing </span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.121.1">cross-platform, cross-library packages with their dependencies can be quite challenging! </span><span class="koboSpan" id="kobo.121.2">It is important to take this </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.122.1">complexity into account and be prepared to get the environment running correctly. </span><span class="koboSpan" id="kobo.122.2">Each package has dependencies that may have conflicting versions. </span><span class="koboSpan" id="kobo.122.3">Even if we adapt the versions, an application may not run as expected anymore. </span><span class="koboSpan" id="kobo.122.4">So, take your time to install the right versions of the packages and dependencies.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.123.1">We will only describe the environment once in this section for all three components and refer to this section when necessary.</span></p>
    <h3 id="_idParaDest-56" class="heading-3"><span class="koboSpan" id="kobo.124.1">The installation packages and libraries</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.125.1">To </span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.126.1">build the RAG pipeline in this section, we will need packages and need to freeze the package versions to prevent dependency conflicts and issues</span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.127.1"> with the functions of the libraries, such as:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.128.1">Possible conflicts between the versions of the dependencies.</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.129.1">Possible conflicts when one of the libraries needs to be updated for an application to run. </span><span class="koboSpan" id="kobo.129.2">For example, in August 2024, installing </span><code class="inlineCode"><span class="koboSpan" id="kobo.130.1">Deep Lake</span></code><span class="koboSpan" id="kobo.131.1"> required </span><code class="inlineCode"><span class="koboSpan" id="kobo.132.1">Pillow</span></code><span class="koboSpan" id="kobo.133.1"> version 10.x.x and Google Colab’s version was 9.x.x. </span><span class="koboSpan" id="kobo.133.2">Thus, it was necessary to uninstall </span><code class="inlineCode"><span class="koboSpan" id="kobo.134.1">Pillow</span></code><span class="koboSpan" id="kobo.135.1"> and reinstall it with a recent version before installing </span><code class="inlineCode"><span class="koboSpan" id="kobo.136.1">Deep Lake</span></code><span class="koboSpan" id="kobo.137.1">. </span><span class="koboSpan" id="kobo.137.2">Google Colab will no doubt update Pillow. </span><span class="koboSpan" id="kobo.137.3">Many cases such as this occur in a fast-moving market.</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.138.1">Possible deprecations if the versions remain frozen for too long.</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.139.1">Possible issues if the versions are frozen for too long and bugs are not corrected by upgrades.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.140.1">Thus, if we freeze the versions, an application may remain stable for some time but encounter issues. </span><span class="koboSpan" id="kobo.140.2">But if we upgrade the versions too quickly, some of the other libraries may not work anymore. </span><span class="koboSpan" id="kobo.140.3">There is no silver bullet! </span><span class="koboSpan" id="kobo.140.4">It’s a continual quality control process.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.141.1">For our program, in this section, we will freeze the versions. </span><span class="koboSpan" id="kobo.141.2">Let’s now go through the installation steps to create the environment for our pipeline.</span></p>
    <h3 id="_idParaDest-57" class="heading-3"><span class="koboSpan" id="kobo.142.1">The components involved in the installation process</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.143.1">Let’s </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.144.1">begin by describing the components that are installed in the </span><em class="italic"><span class="koboSpan" id="kobo.145.1">Installing the environment</span></em><span class="koboSpan" id="kobo.146.1"> section of each notebook. </span><span class="koboSpan" id="kobo.146.2">The components are not necessarily installed in all notebooks; this section serves as an inventory of the packages.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.147.1">In the first pipeline section, </span><em class="italic"><span class="koboSpan" id="kobo.148.1">1. </span><span class="koboSpan" id="kobo.148.2">Data collection and preparation</span></em><span class="koboSpan" id="kobo.149.1">, we will only need to install Beautiful Soup and Requests:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.150.1">!pip install beautifulsoup4==</span><span class="hljs-number"><span class="koboSpan" id="kobo.151.1">4.12.3</span></span><span class="koboSpan" id="kobo.152.1">
!pip install requests==</span><span class="hljs-number"><span class="koboSpan" id="kobo.153.1">2.31.0</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.154.1">This explains why this component of the pipeline should remain separate. </span><span class="koboSpan" id="kobo.154.2">It’s a straightforward job for a developer who enjoys creating interfaces to interact with the web. </span><span class="koboSpan" id="kobo.154.3">It’s also a perfect fit for a junior developer who wants to get involved in data collection and analysis.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.155.1">The </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.156.1">two other pipeline components we will build in this section, </span><em class="italic"><span class="koboSpan" id="kobo.157.1">2. </span><span class="koboSpan" id="kobo.157.2">Data embedding and storage</span></em><span class="koboSpan" id="kobo.158.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.159.1">3. </span><span class="koboSpan" id="kobo.159.2">Augmented generation</span></em><span class="koboSpan" id="kobo.160.1">, will require more attention as well as the installation of </span><code class="inlineCode"><span class="koboSpan" id="kobo.161.1">requirements01.txt</span></code><span class="koboSpan" id="kobo.162.1">, as explained in the previous section. </span><span class="koboSpan" id="kobo.162.2">For now, let’s continue with the installation step by step.</span></p>
    <h4 class="heading-4"><span class="koboSpan" id="kobo.163.1">Mounting a drive</span></h4>
    <p class="normal"><span class="koboSpan" id="kobo.164.1">In</span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.165.1"> this scenario, the program mounts Google Drive in Google Colab to safely read the OpenAI API key to access OpenAI models and the Activeloop API token for authentication to access Activeloop Deep Lake datasets:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.166.1">#Google Drive option to store API Keys</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.167.1">#Store your key in a file and read it(you can type it directly in the # #notebook but it will be visible for somebody next to you)</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.168.1">from</span></span><span class="koboSpan" id="kobo.169.1"> google.colab </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.170.1">import</span></span><span class="koboSpan" id="kobo.171.1"> drive
drive.mount(</span><span class="hljs-string"><span class="koboSpan" id="kobo.172.1">'/content/drive'</span></span><span class="koboSpan" id="kobo.173.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.174.1">You can choose to store your keys and tokens elsewhere. </span><span class="koboSpan" id="kobo.174.2">Just make sure they are in a safe location.</span></p>
    <h4 class="heading-4"><span class="koboSpan" id="kobo.175.1">Creating a subprocess to download files from GitHub</span></h4>
    <p class="normal"><span class="koboSpan" id="kobo.176.1">The </span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.177.1">goal here is to write a function to download the </span><code class="inlineCode"><span class="koboSpan" id="kobo.178.1">grequests.py</span></code><span class="koboSpan" id="kobo.179.1"> file from GitHub. </span><span class="koboSpan" id="kobo.179.2">This program contains a function to download files using </span><code class="inlineCode"><span class="koboSpan" id="kobo.180.1">curl</span></code><span class="koboSpan" id="kobo.181.1">, with the option to add a private token if necessary:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.182.1">import</span></span><span class="koboSpan" id="kobo.183.1"> subprocess
url = </span><span class="hljs-string"><span class="koboSpan" id="kobo.184.1">"https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py"</span></span><span class="koboSpan" id="kobo.185.1">
output_file = </span><span class="hljs-string"><span class="koboSpan" id="kobo.186.1">"grequests.py"</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.187.1"># Prepare the curl command using the private token</span></span><span class="koboSpan" id="kobo.188.1">
curl_command = [
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.189.1">"curl"</span></span><span class="koboSpan" id="kobo.190.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.191.1">"-o"</span></span><span class="koboSpan" id="kobo.192.1">, output_file,
    url
]
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.193.1"># Execute the curl command</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.194.1">try</span></span><span class="koboSpan" id="kobo.195.1">:
    subprocess.run(curl_command, check=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.196.1">True</span></span><span class="koboSpan" id="kobo.197.1">)
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.198.1">print</span></span><span class="koboSpan" id="kobo.199.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.200.1">"Download successful."</span></span><span class="koboSpan" id="kobo.201.1">)
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.202.1">except</span></span><span class="koboSpan" id="kobo.203.1"> subprocess.CalledProcessError:
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.204.1">print</span></span><span class="koboSpan" id="kobo.205.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.206.1">"Failed to download the file."</span></span><span class="koboSpan" id="kobo.207.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.208.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.209.1">grequests.py</span></code><span class="koboSpan" id="kobo.210.1"> file contains a function that can, if necessary, accept a private token or any</span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.211.1"> other security system that requires credentials when retrieving data with </span><code class="inlineCode"><span class="koboSpan" id="kobo.212.1">curl</span></code><span class="koboSpan" id="kobo.213.1"> commands:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.214.1">import</span></span><span class="koboSpan" id="kobo.215.1"> subprocess
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.216.1">import</span></span><span class="koboSpan" id="kobo.217.1"> os
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.218.1"># add a private token after the filename if necessary</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.219.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.220.1">download</span></span><span class="koboSpan" id="kobo.221.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.222.1">directory, filename</span></span><span class="koboSpan" id="kobo.223.1">):
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.224.1"># The base URL of the image files in the GitHub repository</span></span><span class="koboSpan" id="kobo.225.1">
    base_url = </span><span class="hljs-string"><span class="koboSpan" id="kobo.226.1">'https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/'</span></span>
    <span class="hljs-comment"><span class="koboSpan" id="kobo.227.1"># Complete URL for the file</span></span><span class="koboSpan" id="kobo.228.1">
    file_url = </span><span class="hljs-string"><span class="koboSpan" id="kobo.229.1">f"</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.230.1">{base_url}{directory}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.231.1">/</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.232.1">{filename}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.233.1">"</span></span>
    <span class="hljs-comment"><span class="koboSpan" id="kobo.234.1"># Use curl to download the file, including an Authorization header for the private token</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.235.1">try</span></span><span class="koboSpan" id="kobo.236.1">:
        </span><span class="hljs-comment"><span class="koboSpan" id="kobo.237.1"># Prepare the curl command with the Authorization header</span></span>
        <span class="hljs-comment"><span class="koboSpan" id="kobo.238.1">#curl_command = f'curl -H "Authorization: token {private_token}" -o {filename} {file_url}'</span></span><span class="koboSpan" id="kobo.239.1">
        curl_command = </span><span class="hljs-string"><span class="koboSpan" id="kobo.240.1">f'curl -H -o </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.241.1">{filename}</span></span><span class="hljs-string"> </span><span class="hljs-subst"><span class="koboSpan" id="kobo.242.1">{file_url}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.243.1">'</span></span>
        <span class="hljs-comment"><span class="koboSpan" id="kobo.244.1"># Execute the curl command</span></span><span class="koboSpan" id="kobo.245.1">
        subprocess.run(curl_command, check=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.246.1">True</span></span><span class="koboSpan" id="kobo.247.1">, shell=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.248.1">True</span></span><span class="koboSpan" id="kobo.249.1">)
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.250.1">print</span></span><span class="koboSpan" id="kobo.251.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.252.1">f"Downloaded '</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.253.1">{filename}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.254.1">' successfully."</span></span><span class="koboSpan" id="kobo.255.1">)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.256.1">except</span></span><span class="koboSpan" id="kobo.257.1"> subprocess.CalledProcessError:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.258.1">print</span></span><span class="koboSpan" id="kobo.259.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.260.1">f"Failed to download '</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.261.1">{filename}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.262.1">'. </span><span class="koboSpan" id="kobo.262.2">Check the URL, your internet connection, and if the token is correct and has appropriate permissions."</span></span><span class="koboSpan" id="kobo.263.1">)
</span></code></pre>
    <h4 class="heading-4"><span class="koboSpan" id="kobo.264.1">Installing requirements</span></h4>
    <p class="normal"><span class="koboSpan" id="kobo.265.1">Now, we </span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.266.1">will install the requirements for this section when working with Activeloop Deep Lake and OpenAI. </span><span class="koboSpan" id="kobo.266.2">We will only need:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.267.1">!pip install deeplake==</span><span class="hljs-number"><span class="koboSpan" id="kobo.268.1">3.9.18</span></span><span class="koboSpan" id="kobo.269.1">
!pip install openai==</span><span class="hljs-number"><span class="koboSpan" id="kobo.270.1">1.40.3</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.271.1">As of August 2024, Google Colab’s version of Pillow conflicts with </span><code class="inlineCode"><span class="koboSpan" id="kobo.272.1">deeplake</span></code><span class="koboSpan" id="kobo.273.1">'s package. </span><span class="koboSpan" id="kobo.273.2">However, the </span><code class="inlineCode"><span class="koboSpan" id="kobo.274.1">deeplake</span></code><span class="koboSpan" id="kobo.275.1"> installation package deals with this automatically. </span><span class="koboSpan" id="kobo.275.2">All you have to do is restart the session and run it again, which is why </span><code class="inlineCode"><span class="koboSpan" id="kobo.276.1">pip install deeplake==3.9.18</span></code><span class="koboSpan" id="kobo.277.1"> is the first line of each notebook it is installed in.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.278.1">After installing the requirements, we must run a line of code for Activeloop to activate a public DNS server:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.279.1"># For Google Colab and Activeloop(Deeplake library)</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.280.1">#This line writes the string "nameserver 8.8.8.8" to the file. </span><span class="koboSpan" id="kobo.280.2">This is specifying that the DNS server the system</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.281.1">#should use is at the IP address 8.8.8.8, which is one of Google's Public DNS servers.</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.282.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.283.1">open</span></span><span class="koboSpan" id="kobo.284.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.285.1">'/etc/resolv.conf'</span></span><span class="koboSpan" id="kobo.286.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.287.1">'w'</span></span><span class="koboSpan" id="kobo.288.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.289.1">as</span></span><span class="koboSpan" id="kobo.290.1"> file:
   file.write(</span><span class="hljs-string"><span class="koboSpan" id="kobo.291.1">"nameserver 8.8.8.8"</span></span><span class="koboSpan" id="kobo.292.1">)
</span></code></pre>
    <h4 class="heading-4"><span class="koboSpan" id="kobo.293.1">Authentication process</span></h4>
    <p class="normal"><span class="koboSpan" id="kobo.294.1">You </span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.295.1">will need to sign up to OpenAI</span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.296.1"> to obtain an API key: </span><a href="https://openai.com/"><span class="url"><span class="koboSpan" id="kobo.297.1">https://openai.com/</span></span></a><span class="koboSpan" id="kobo.298.1">. </span><span class="koboSpan" id="kobo.298.2">Make sure to check the pricing policy before using the key. </span><span class="koboSpan" id="kobo.298.3">First, let’s activate OpenAI’s API key:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.299.1">#Retrieving and setting OpenAI API key</span></span><span class="koboSpan" id="kobo.300.1">
f = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.301.1">open</span></span><span class="koboSpan" id="kobo.302.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.303.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.304.1">drive/MyDrive/files/api_key.txt"</span></span><span class="koboSpan" id="kobo.305.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.306.1">"r"</span></span><span class="koboSpan" id="kobo.307.1">)
API_KEY=f.readline().strip()
f.close()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.308.1">#The OpenAI API key</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.309.1">import</span></span><span class="koboSpan" id="kobo.310.1"> os
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.311.1">import</span></span><span class="koboSpan" id="kobo.312.1"> openai
os.environ[</span><span class="hljs-string"><span class="koboSpan" id="kobo.313.1">'OPENAI_API_KEY'</span></span><span class="koboSpan" id="kobo.314.1">] =API_KEY
openai.api_key = os.getenv(</span><span class="hljs-string"><span class="koboSpan" id="kobo.315.1">"OPENAI_API_KEY"</span></span><span class="koboSpan" id="kobo.316.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.317.1">Then, we activate Activeloop’s API token for Deep Lake:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.318.1">#Retrieving and setting Activeloop API token</span></span><span class="koboSpan" id="kobo.319.1">
f = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.320.1">open</span></span><span class="koboSpan" id="kobo.321.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.322.1">"drive/MyDrive/files/activeloop.txt"</span></span><span class="koboSpan" id="kobo.323.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.324.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.325.1">r"</span></span><span class="koboSpan" id="kobo.326.1">)
API_token=f.readline().strip()
f.close()
ACTIVELOOP_TOKEN=API_token
os.environ[</span><span class="hljs-string"><span class="koboSpan" id="kobo.327.1">'ACTIVELOOP_TOKEN'</span></span><span class="koboSpan" id="kobo.328.1">] =ACTIVELOOP_TOKEN
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.329.1">You will</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.330.1"> need to sign up on Activeloop</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.331.1"> to obtain an API token: </span><a href="https://www.activeloop.ai/"><span class="url"><span class="koboSpan" id="kobo.332.1">https://www.activeloop.ai/</span></span></a><span class="koboSpan" id="kobo.333.1">. </span><span class="koboSpan" id="kobo.333.2">Again, make sure to check the pricing policy before using the Activeloop token.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.334.1">Once the environment is installed, you can hide the </span><em class="italic"><span class="koboSpan" id="kobo.335.1">Installing the environment</span></em><span class="koboSpan" id="kobo.336.1"> cells we just ran to focus on the content of the pipeline components, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.337.1">Figure 2.4</span></em><span class="koboSpan" id="kobo.338.1">:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.339.1"><img src="../Images/B31169_02_04.png" alt="A white background with black text  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.340.1">Figure 2:4: Hiding the installation cells</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.341.1">The installation cells will then be hidden but can still be run, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.342.1">Figure 2.5</span></em><span class="koboSpan" id="kobo.343.1">:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.344.1"><img src="../Images/B31169_02_05.png" alt="A screenshot of a computer  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.345.1">Figure 2.5: Running hidden cells</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.346.1">We can now focus on the pipeline components for each pipeline component. </span><span class="koboSpan" id="kobo.346.2">Let’s begin with data collection and preparation.</span></p>
    <h2 id="_idParaDest-58" class="heading-2"><span class="koboSpan" id="kobo.347.1">1. </span><span class="koboSpan" id="kobo.347.2">Data collection and preparation</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.348.1">Data collection and </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.349.1">preparation is the first pipeline component, as described earlier</span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.350.1"> in this chapter. </span><code class="inlineCode"><span class="koboSpan" id="kobo.351.1">Team #1</span></code><span class="koboSpan" id="kobo.352.1"> will only focus on their component, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.353.1">Figure 2.6</span></em><span class="koboSpan" id="kobo.354.1">:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.355.1"><img src="../Images/B31169_02_06.png" alt="A diagram of a pipeline component  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.356.1">Figure 2.6: Pipeline component #1: Data collection and preparation</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.357.1">Let’s jump in and lend a hand to </span><code class="inlineCode"><span class="koboSpan" id="kobo.358.1">Team #1</span></code><span class="koboSpan" id="kobo.359.1">. </span><span class="koboSpan" id="kobo.359.2">Our work is clearly defined, so we can enjoy the time taken to implement the component. </span><span class="koboSpan" id="kobo.359.3">We will retrieve and process 10 Wikipedia articles that provide a comprehensive view of various aspects of space exploration:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.360.1">Space exploration</span></strong><span class="koboSpan" id="kobo.361.1">: Overview</span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.362.1"> of the history, technologies, missions, and plans involved in the exploration of space (</span><a href="https://en.wikipedia.org/wiki/Space_exploration"><span class="url"><span class="koboSpan" id="kobo.363.1">https://en.wikipedia.org/wiki/Space_exploration</span></span></a><span class="koboSpan" id="kobo.364.1">)</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.365.1">Apollo program</span></strong><span class="koboSpan" id="kobo.366.1">: Details about the NASA program that landed the first humans on the Moon and its </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.367.1">significant missions (</span><a href="https://en.wikipedia.org/wiki/Apollo_program"><span class="url"><span class="koboSpan" id="kobo.368.1">https://en.wikipedia.org/wiki/Apollo_program</span></span></a><span class="koboSpan" id="kobo.369.1">)</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.370.1">Hubble Space Telescope</span></strong><span class="koboSpan" id="kobo.371.1">: Information on one of the most significant telescopes ever built, which</span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.372.1"> has been crucial in many astronomical discoveries (</span><a href="https://en.wikipedia.org/wiki/Hubble_Space_Telescope"><span class="url"><span class="koboSpan" id="kobo.373.1">https://en.wikipedia.org/wiki/Hubble_Space_Telescope</span></span></a><span class="koboSpan" id="kobo.374.1">)</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.375.1">Mars rover</span></strong><span class="koboSpan" id="kobo.376.1">: Insight</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.377.1"> into the rovers that have been sent to Mars to study its surface and environment (</span><a href="https://en.wikipedia.org/wiki/Mars_rover"><span class="url"><span class="koboSpan" id="kobo.378.1">https://en.wikipedia.org/wiki/Mars_rover</span></span></a><span class="koboSpan" id="kobo.379.1">)</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.380.1">International Space Station (ISS)</span></strong><span class="koboSpan" id="kobo.381.1">: Details about the ISS, its construction, international collaboration, and</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.382.1"> its role in space research (</span><a href="https://en.wikipedia.org/wiki/International_Space_Station"><span class="url"><span class="koboSpan" id="kobo.383.1">https://en.wikipedia.org/wiki/International_Space_Station</span></span></a><span class="koboSpan" id="kobo.384.1">)</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.385.1">SpaceX</span></strong><span class="koboSpan" id="kobo.386.1">: Covers the history, achievements, and goals of SpaceX, one of the most influential </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.387.1">private spaceflight companies (</span><a href="https://en.wikipedia.org/wiki/SpaceX"><span class="url"><span class="koboSpan" id="kobo.388.1">https://en.wikipedia.org/wiki/SpaceX</span></span></a><span class="koboSpan" id="kobo.389.1">)</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.390.1">Juno (spacecraft)</span></strong><span class="koboSpan" id="kobo.391.1">: Information about the NASA space probe that orbits and studies Jupiter, its </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.392.1">structure, and moons (</span><a href="https://en.wikipedia.org/wiki/Juno_(spacecraft)"><span class="url"><span class="koboSpan" id="kobo.393.1">https://en.wikipedia.org/wiki/Juno_(spacecraft))</span></span></a></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.394.1">Voyager program</span></strong><span class="koboSpan" id="kobo.395.1">: Details on the Voyager missions, including their contributions to our </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.396.1">understanding of the outer solar system and interstellar space (</span><a href="https://en.wikipedia.org/wiki/Voyager_program"><span class="url"><span class="koboSpan" id="kobo.397.1">https://en.wikipedia.org/wiki/Voyager_program</span></span></a><span class="koboSpan" id="kobo.398.1">)</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.399.1">Galileo (spacecraft)</span></strong><span class="koboSpan" id="kobo.400.1">: Overview of the mission that studied Jupiter and its moons, providing valuable</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.401.1"> data on the gas giant and its system (</span><a href="https://en.wikipedia.org/wiki/Galileo_(spacecraft)"><span class="url"><span class="koboSpan" id="kobo.402.1">https://en.wikipedia.org/wiki/Galileo_(spacecraft)</span></span></a><span class="koboSpan" id="kobo.403.1">)</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.404.1">Kepler space telescope</span></strong><span class="koboSpan" id="kobo.405.1">: Information </span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.406.1">about the space telescope designed to discover Earth-size planets orbiting other stars (</span><a href="https://en.wikipedia.org/wiki/Kepler_Space_Telescope"><span class="url"><span class="koboSpan" id="kobo.407.1">https://en.wikipedia.org/wiki/Kepler_Space_Telescope</span></span></a><span class="koboSpan" id="kobo.408.1">)</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.409.1">These articles </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.410.1">cover a wide range of topics in space exploration, from </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.411.1">historical programs to modern technological advances and missions.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.412.1">Now, open </span><code class="inlineCode"><span class="koboSpan" id="kobo.413.1">1-Data_collection_preparation.ipynb</span></code><span class="koboSpan" id="kobo.414.1"> in the GitHub repository. </span><span class="koboSpan" id="kobo.414.2">We will first collect the data.</span></p>
    <h3 id="_idParaDest-59" class="heading-3"><span class="koboSpan" id="kobo.415.1">Collecting the data</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.416.1">We just</span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.417.1"> need </span><code class="inlineCode"><span class="koboSpan" id="kobo.418.1">import requests</span></code><span class="koboSpan" id="kobo.419.1"> for the HTTP requests, </span><code class="inlineCode"><span class="koboSpan" id="kobo.420.1">from bs4 import BeautifulSoup</span></code><span class="koboSpan" id="kobo.421.1"> for HTML parsing, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.422.1">import re</span></code><span class="koboSpan" id="kobo.423.1">, the regular expressions module:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.424.1">import</span></span><span class="koboSpan" id="kobo.425.1"> requests
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.426.1">from</span></span><span class="koboSpan" id="kobo.427.1"> bs4 </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.428.1">import</span></span><span class="koboSpan" id="kobo.429.1"> BeautifulSoup
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.430.1">import</span></span><span class="koboSpan" id="kobo.431.1"> re
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.432.1">We then select the URLs we need:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.433.1"># URLs of the Wikipedia articles</span></span><span class="koboSpan" id="kobo.434.1">
urls = [
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.435.1">"https://en.wikipedia.org/wiki/Space_exploration"</span></span><span class="koboSpan" id="kobo.436.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.437.1">"https://en.wikipedia.org/wiki/Apollo_program"</span></span><span class="koboSpan" id="kobo.438.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.439.1">"https://en.wikipedia.org/wiki/Hubble_Space_Telescope"</span></span><span class="koboSpan" id="kobo.440.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.441.1">"https://en.wikipedia.org/wiki/Mars_over"</span></span><span class="koboSpan" id="kobo.442.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.443.1">"https://en.wikipedia.org/wiki/International_Space_Station"</span></span><span class="koboSpan" id="kobo.444.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.445.1">"https://en.wikipedia.org/wiki/SpaceX"</span></span><span class="koboSpan" id="kobo.446.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.447.1">"https://en.wikipedia.org/wiki/Juno_(spacecraft)"</span></span><span class="koboSpan" id="kobo.448.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.449.1">"https://en.wikipedia.org/wiki/Voyager_program"</span></span><span class="koboSpan" id="kobo.450.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.451.1">"https://en.wikipedia.org/wiki/Galileo_(spacecraft)"</span></span><span class="koboSpan" id="kobo.452.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.453.1">"https://en.wikipedia.org/wiki/Kepler_Space_Telescope"</span></span><span class="koboSpan" id="kobo.454.1">
]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.455.1">This list is in code. </span><span class="koboSpan" id="kobo.455.2">However, it could be stored in a database, a file, or any other format, such as JSON. </span><span class="koboSpan" id="kobo.455.3">We can now prepare the data.</span></p>
    <h3 id="_idParaDest-60" class="heading-3"><span class="koboSpan" id="kobo.456.1">Preparing the data</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.457.1">First, we</span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.458.1"> write a cleaning function. </span><span class="koboSpan" id="kobo.458.2">This function removes numerical references such as [1] [2] from a given text string, using regular expressions, and returns the cleaned text:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.459.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.460.1">clean_text</span></span><span class="koboSpan" id="kobo.461.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.462.1">content</span></span><span class="koboSpan" id="kobo.463.1">):
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.464.1"># Remove references that usually appear as [1], [2], etc.</span></span><span class="koboSpan" id="kobo.465.1">
    content = re.sub(</span><span class="hljs-string"><span class="koboSpan" id="kobo.466.1">r'\[\d+\]'</span></span><span class="koboSpan" id="kobo.467.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.468.1">''</span></span><span class="koboSpan" id="kobo.469.1">, content)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.470.1">return</span></span><span class="koboSpan" id="kobo.471.1"> content
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.472.1">Then, we</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.473.1"> write a classical fetch and clean function, which will return a nice and clean text by extracting the content we need from the documents:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.474.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.475.1">fetch_and_clean</span></span><span class="koboSpan" id="kobo.476.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.477.1">url</span></span><span class="koboSpan" id="kobo.478.1">):
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.479.1"># Fetch the content of the URL</span></span><span class="koboSpan" id="kobo.480.1">
    response = requests.get(url)
    soup = BeautifulSoup(response.content, </span><span class="hljs-string"><span class="koboSpan" id="kobo.481.1">'html.parser'</span></span><span class="koboSpan" id="kobo.482.1">)
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.483.1"># Find the main content of the article, ignoring side boxes and headers</span></span><span class="koboSpan" id="kobo.484.1">
    content = soup.find(</span><span class="hljs-string"><span class="koboSpan" id="kobo.485.1">'div'</span></span><span class="koboSpan" id="kobo.486.1">, {</span><span class="hljs-string"><span class="koboSpan" id="kobo.487.1">'class'</span></span><span class="koboSpan" id="kobo.488.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.489.1">'mw-parser-output'</span></span><span class="koboSpan" id="kobo.490.1">})
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.491.1"># Remove the bibliography section, which generally follows a header like "References", "Bibliography"</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.492.1">for</span></span><span class="koboSpan" id="kobo.493.1"> section_title </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.494.1">in</span></span><span class="koboSpan" id="kobo.495.1"> [</span><span class="hljs-string"><span class="koboSpan" id="kobo.496.1">'References'</span></span><span class="koboSpan" id="kobo.497.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.498.1">'Bibliography'</span></span><span class="koboSpan" id="kobo.499.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.500.1">'External links'</span></span><span class="koboSpan" id="kobo.501.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.502.1">'See also'</span></span><span class="koboSpan" id="kobo.503.1">]:
        section = content.find(</span><span class="hljs-string"><span class="koboSpan" id="kobo.504.1">'span'</span></span><span class="koboSpan" id="kobo.505.1">, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.506.1">id</span></span><span class="koboSpan" id="kobo.507.1">=section_title)
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.508.1">if</span></span><span class="koboSpan" id="kobo.509.1"> section:
            </span><span class="hljs-comment"><span class="koboSpan" id="kobo.510.1"># Remove all content from this section to the end of the document</span></span>
            <span class="hljs-keyword"><span class="koboSpan" id="kobo.511.1">for</span></span><span class="koboSpan" id="kobo.512.1"> sib </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.513.1">in</span></span><span class="koboSpan" id="kobo.514.1"> section.parent.find_next_siblings():
                sib.decompose()
            section.parent.decompose()
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.515.1"># Extract and clean the text</span></span><span class="koboSpan" id="kobo.516.1">
    text = content.get_text(separator=</span><span class="hljs-string"><span class="koboSpan" id="kobo.517.1">' '</span></span><span class="koboSpan" id="kobo.518.1">, strip=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.519.1">True</span></span><span class="koboSpan" id="kobo.520.1">)
    text = clean_text(text)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.521.1">return</span></span><span class="koboSpan" id="kobo.522.1"> text
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.523.1">Finally, we write the content in </span><code class="inlineCode"><span class="koboSpan" id="kobo.524.1">llm.txt</span></code><span class="koboSpan" id="kobo.525.1"> file for the team working on the data embedding and storage functions:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.526.1"># File to write the clean text</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.527.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.528.1">open</span></span><span class="koboSpan" id="kobo.529.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.530.1">'llm.txt'</span></span><span class="koboSpan" id="kobo.531.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.532.1">'w'</span></span><span class="koboSpan" id="kobo.533.1">, encoding=</span><span class="hljs-string"><span class="koboSpan" id="kobo.534.1">'utf-8'</span></span><span class="koboSpan" id="kobo.535.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.536.1">as</span></span><span class="koboSpan" id="kobo.537.1"> file:
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.538.1">for</span></span><span class="koboSpan" id="kobo.539.1"> url </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.540.1">in</span></span><span class="koboSpan" id="kobo.541.1"> urls:
        clean_article_text = fetch_and_clean(url)
        file.write(clean_article_text + </span><span class="hljs-string"><span class="koboSpan" id="kobo.542.1">'\n'</span></span><span class="koboSpan" id="kobo.543.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.544.1">print</span></span><span class="koboSpan" id="kobo.545.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.546.1">"Content written to llm.txt"</span></span><span class="koboSpan" id="kobo.547.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.548.1">The output confirms that the text has been written:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.549.1">Content written to llm.txt
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.550.1">The program</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.551.1"> can be modified to save the data in other formats and locations, as required for a project’s specific needs. </span><span class="koboSpan" id="kobo.551.2">The file can then be verified before we move on to the next batch of data to retrieve and process:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.552.1"># Open the file and read the first 20 lines</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.553.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.554.1">open</span></span><span class="koboSpan" id="kobo.555.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.556.1">'llm.txt'</span></span><span class="koboSpan" id="kobo.557.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.558.1">'r'</span></span><span class="koboSpan" id="kobo.559.1">, encoding=</span><span class="hljs-string"><span class="koboSpan" id="kobo.560.1">'utf-8'</span></span><span class="koboSpan" id="kobo.561.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.562.1">as</span></span><span class="koboSpan" id="kobo.563.1"> file:
    lines = file.readlines()
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.564.1"># Print the first 20 lines</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.565.1">for</span></span><span class="koboSpan" id="kobo.566.1"> line </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.567.1">in</span></span><span class="koboSpan" id="kobo.568.1"> lines[:</span><span class="hljs-number"><span class="koboSpan" id="kobo.569.1">20</span></span><span class="koboSpan" id="kobo.570.1">]:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.571.1">print</span></span><span class="koboSpan" id="kobo.572.1">(line.strip())
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.573.1">The output shows the first lines of the document that will be processed:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.574.1">Exploration of space, planets, and moons "Space Exploration" redirects here. </span><span class="koboSpan" id="kobo.574.2">For the company, see SpaceX . </span><span class="koboSpan" id="kobo.574.3">For broader coverage of this topic, see Exploration . </span><span class="koboSpan" id="kobo.574.4">Buzz Aldrin taking a core sample of the Moon during the Apollo 11 mission…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.575.1">This </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.576.1">component can be managed by a team that enjoys searching for documents on the web or within a company’s data environment. </span><span class="koboSpan" id="kobo.576.2">The team will gain experience in identifying the best documents for a project, which is the foundation of any RAG framework.</span></p>
    <p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.577.1">Team #2</span></code><span class="koboSpan" id="kobo.578.1"> can now work on the data to embed the documents and store them.</span></p>
    <h2 id="_idParaDest-61" class="heading-2"><span class="koboSpan" id="kobo.579.1">2. </span><span class="koboSpan" id="kobo.579.2">Data embedding and storage</span></h2>
    <p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.580.1">Team #2</span></code><span class="koboSpan" id="kobo.581.1">'s job is to </span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.582.1">focus on the second component of</span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.583.1"> the pipeline. </span><span class="koboSpan" id="kobo.583.2">They will receive batches of prepared data to work on. </span><span class="koboSpan" id="kobo.583.3">They don’t have to worry about retrieving data. </span><code class="inlineCode"><span class="koboSpan" id="kobo.584.1">Team #1</span></code><span class="koboSpan" id="kobo.585.1"> has their back with their data collection and preparation component.</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.586.1"><img src="../Images/B31169_02_07.png" alt="A diagram of a pipeline component  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.587.1">Figure 2.7: Pipeline component #2: Data embedding and storage</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.588.1">Let’s now jump </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.589.1">in and help </span><code class="inlineCode"><span class="koboSpan" id="kobo.590.1">Team #2</span></code><span class="koboSpan" id="kobo.591.1"> to get the job done. </span><span class="koboSpan" id="kobo.591.2">Open </span><code class="inlineCode"><span class="koboSpan" id="kobo.592.1">2-Embeddings_vector_store.ipynb</span></code><span class="koboSpan" id="kobo.593.1"> in the GitHub Repository. </span><span class="koboSpan" id="kobo.593.2">We will embed and </span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.594.1">store the data provided by </span><code class="inlineCode"><span class="koboSpan" id="kobo.595.1">Team #1</span></code><span class="koboSpan" id="kobo.596.1"> and retrieve a batch of documents to work on.</span></p>
    <h3 id="_idParaDest-62" class="heading-3"><span class="koboSpan" id="kobo.597.1">Retrieving a batch of prepared documents</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.598.1">First, we download a batch of</span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.599.1"> documents available on a server and provided by </span><code class="inlineCode"><span class="koboSpan" id="kobo.600.1">Team #1</span></code><span class="koboSpan" id="kobo.601.1">, which is the first of a continual stream of incoming documents. </span><span class="koboSpan" id="kobo.601.2">In this case, we assume it’s the space exploration file:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.602.1">from</span></span><span class="koboSpan" id="kobo.603.1"> grequests </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.604.1">import</span></span><span class="koboSpan" id="kobo.605.1"> download
source_text = </span><span class="hljs-string"><span class="koboSpan" id="kobo.606.1">"llm.txt"</span></span><span class="koboSpan" id="kobo.607.1">
directory = </span><span class="hljs-string"><span class="koboSpan" id="kobo.608.1">"Chapter02"</span></span><span class="koboSpan" id="kobo.609.1">
filename = </span><span class="hljs-string"><span class="koboSpan" id="kobo.610.1">"llm.txt"</span></span><span class="koboSpan" id="kobo.611.1">
download(directory, filename)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.612.1">Note that </span><code class="inlineCode"><span class="koboSpan" id="kobo.613.1">source_text = "llm.txt"</span></code><span class="koboSpan" id="kobo.614.1"> will be used by the function that will add the data to our vector store. </span><span class="koboSpan" id="kobo.614.2">We then briefly check the document just to be sure, knowing that </span><code class="inlineCode"><span class="koboSpan" id="kobo.615.1">Team #1</span></code><span class="koboSpan" id="kobo.616.1"> has already verified the information:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.617.1"># Open the file and read the first 20 lines</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.618.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.619.1">open</span></span><span class="koboSpan" id="kobo.620.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.621.1">'llm.txt'</span></span><span class="koboSpan" id="kobo.622.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.623.1">'r'</span></span><span class="koboSpan" id="kobo.624.1">, encoding=</span><span class="hljs-string"><span class="koboSpan" id="kobo.625.1">'utf-8'</span></span><span class="koboSpan" id="kobo.626.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.627.1">as</span></span><span class="koboSpan" id="kobo.628.1"> file:
    lines = file.readlines()
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.629.1"># Print the first 20 lines</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.630.1">for</span></span><span class="koboSpan" id="kobo.631.1"> line </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.632.1">in</span></span><span class="koboSpan" id="kobo.633.1"> lines[:</span><span class="hljs-number"><span class="koboSpan" id="kobo.634.1">20</span></span><span class="koboSpan" id="kobo.635.1">]:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.636.1">print</span></span><span class="koboSpan" id="kobo.637.1">(line.strip())
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.638.1">The output is satisfactory, as shown in the following excerpt:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.639.1">Exploration of space, planets, and moons "Space Exploration" redirects here.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.640.1">We will now chunk the data. </span><span class="koboSpan" id="kobo.640.2">We will determine a chunk size defined by the number of characters. </span><span class="koboSpan" id="kobo.640.3">In this case, it is </span><code class="inlineCode"><span class="koboSpan" id="kobo.641.1">CHUNK_SIZE = 1000</span></code><span class="koboSpan" id="kobo.642.1">, but we can select chunk sizes using different strategies. </span><em class="chapterRef"><span class="koboSpan" id="kobo.643.1">Chapter 7</span></em><span class="koboSpan" id="kobo.644.1">, </span><em class="italic"><span class="koboSpan" id="kobo.645.1">Building Scalable Knowledge-Graph-based RAG with Wikipedia API and LlamaIndex</span></em><span class="koboSpan" id="kobo.646.1">, will take chunk size optimization further with automated seamless chunking.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.647.1">Chunking is necessary to optimize </span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.648.1">data processing: selecting portions of text, embedding, and loading the data. </span><span class="koboSpan" id="kobo.648.2">It also makes the embedded dataset easier to query. </span><span class="koboSpan" id="kobo.648.3">The following code chunks a document to complete the preparation process:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.649.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.650.1">open</span></span><span class="koboSpan" id="kobo.651.1">(source_text, </span><span class="hljs-string"><span class="koboSpan" id="kobo.652.1">'r'</span></span><span class="koboSpan" id="kobo.653.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.654.1">as</span></span><span class="koboSpan" id="kobo.655.1"> f:
    text = f.read()
CHUNK_SIZE = </span><span class="hljs-number"><span class="koboSpan" id="kobo.656.1">1000</span></span><span class="koboSpan" id="kobo.657.1">
chunked_text = [text[i:i+CHUNK_SIZE] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.658.1">for</span></span><span class="koboSpan" id="kobo.659.1"> i </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.660.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.661.1">range</span></span><span class="koboSpan" id="kobo.662.1">(</span><span class="hljs-number"><span class="koboSpan" id="kobo.663.1">0</span></span><span class="koboSpan" id="kobo.664.1">,</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.665.1">len</span></span><span class="koboSpan" id="kobo.666.1">(text), CHUNK_SIZE)]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.667.1">We are now ready to create a vector store to vectorize data or add data to an existing one.</span></p>
    <h3 id="_idParaDest-63" class="heading-3"><span class="koboSpan" id="kobo.668.1">Verifying if the vector store exists and creating it if not</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.669.1">First, we need to</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.670.1"> define the path of our Activeloop vector store path, whether our dataset exists or not:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.671.1">vector_store_path = </span><span class="hljs-string"><span class="koboSpan" id="kobo.672.1">"hub://denis76/space_exploration_v1"</span></span>
</code></pre>
    <div class="note">
      <p class="normal"><span class="koboSpan" id="kobo.673.1">Make sure to replace </span><code class="inlineCode"><span class="koboSpan" id="kobo.674.1">`hub://denis76/space_exploration_v1`</span></code><span class="koboSpan" id="kobo.675.1"> with your organization and dataset name.</span></p>
    </div>
    <p class="normal"><span class="koboSpan" id="kobo.676.1">Then, we write a function to attempt to load the vector store</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.677.1"> or automatically create one if it doesn’t exist:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.678.1">from</span></span><span class="koboSpan" id="kobo.679.1"> deeplake.core.vectorstore.deeplake_vectorstore </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.680.1">import</span></span><span class="koboSpan" id="kobo.681.1"> VectorStore
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.682.1">import</span></span><span class="koboSpan" id="kobo.683.1"> deeplake.util
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.684.1">try</span></span><span class="koboSpan" id="kobo.685.1">:
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.686.1"># Attempt to load the vector store</span></span><span class="koboSpan" id="kobo.687.1">
    vector_store = VectorStore(path=vector_store_path)
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.688.1">print</span></span><span class="koboSpan" id="kobo.689.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.690.1">"Vector store exists"</span></span><span class="koboSpan" id="kobo.691.1">)
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.692.1">except</span></span><span class="koboSpan" id="kobo.693.1"> FileNotFoundError:
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.694.1">print</span></span><span class="koboSpan" id="kobo.695.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.696.1">"Vector store does not exist. </span><span class="koboSpan" id="kobo.696.2">You can create it."</span></span><span class="koboSpan" id="kobo.697.1">)
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.698.1"># Code to create the vector store goes here</span></span><span class="koboSpan" id="kobo.699.1">
    create_vector_store=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.700.1">True</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.701.1">The output confirms that the vector store has been created:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.702.1">Your Deep Lake dataset has been successfully created!
</span><span class="koboSpan" id="kobo.702.2">Vector store exists
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.703.1">We now need to create an embedding function.</span></p>
    <h3 id="_idParaDest-64" class="heading-3"><span class="koboSpan" id="kobo.704.1">The embedding function</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.705.1">The embedding function</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.706.1"> will transform the chunks of data we created into vectors to enable vector-based search. </span><span class="koboSpan" id="kobo.706.2">In this program, we will use </span><code class="inlineCode"><span class="koboSpan" id="kobo.707.1">"text-embedding-3-small"</span></code><span class="koboSpan" id="kobo.708.1"> to embed the documents.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.709.1">OpenAI has other embedding models</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.710.1"> that you can use: </span><a href="https://platform.openai.com/docs/models/embeddings"><span class="url"><span class="koboSpan" id="kobo.711.1">https://platform.openai.com/docs/models/embeddings</span></span></a><span class="koboSpan" id="kobo.712.1">. </span><em class="chapterRef"><span class="koboSpan" id="kobo.713.1">Chapter 6</span></em><span class="koboSpan" id="kobo.714.1">, </span><em class="italic"><span class="koboSpan" id="kobo.715.1">Scaling RAG Bank Customer Data with Pinecone</span></em><span class="koboSpan" id="kobo.716.1">, provides alternative code for embedding models in the </span><em class="italic"><span class="koboSpan" id="kobo.717.1">Embedding</span></em><span class="koboSpan" id="kobo.718.1"> section. </span><span class="koboSpan" id="kobo.718.2">In any case, it is recommended to evaluate embedding models before choosing one in production. </span><span class="koboSpan" id="kobo.718.3">Examine the characteristics of each embedding model, as described by OpenAI, focusing on their length and capacities. </span><code class="inlineCode"><span class="koboSpan" id="kobo.719.1">text-embedding-3-small</span></code><span class="koboSpan" id="kobo.720.1"> was chosen in this case because it stands out as a robust choice for efficiency and speed:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.721.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.722.1">embedding_function</span></span><span class="koboSpan" id="kobo.723.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.724.1">texts, model=</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.725.1">"text-embedding-3-small"</span></span><span class="koboSpan" id="kobo.726.1">):
   </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.727.1">if</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.728.1">isinstance</span></span><span class="koboSpan" id="kobo.729.1">(texts, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.730.1">str</span></span><span class="koboSpan" id="kobo.731.1">):
       texts = [texts]
   texts = [t.replace(</span><span class="hljs-string"><span class="koboSpan" id="kobo.732.1">"\n"</span></span><span class="koboSpan" id="kobo.733.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.734.1">" "</span></span><span class="koboSpan" id="kobo.735.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.736.1">for</span></span><span class="koboSpan" id="kobo.737.1"> t </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.738.1">in</span></span><span class="koboSpan" id="kobo.739.1"> texts]
   </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.740.1">return</span></span><span class="koboSpan" id="kobo.741.1"> [data.embedding </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.742.1">for</span></span><span class="koboSpan" id="kobo.743.1"> data </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.744.1">in</span></span><span class="koboSpan" id="kobo.745.1"> openai.embeddings.create(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.746.1">input</span></span><span class="koboSpan" id="kobo.747.1"> = texts, model=model).data]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.748.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.749.1">text-embedding-3-small</span></code><span class="koboSpan" id="kobo.750.1"> text embedding model from OpenAI typically uses embeddings</span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.751.1"> with a restricted number of dimensions, to balance obtaining enough detail in the embeddings with large computational workloads and storage space. </span><span class="koboSpan" id="kobo.751.2">Make sure to check the model page and pricing information before running the code: </span><a href="https://platform.openai.com/docs/guides/embeddings/embedding-models"><span class="url"><span class="koboSpan" id="kobo.752.1">https://platform.openai.com/docs/guides/embeddings/embedding-models</span></span></a><span class="koboSpan" id="kobo.753.1">.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.754.1">We are now all set to begin populating the vector store.</span></p>
    <h3 id="_idParaDest-65" class="heading-3"><span class="koboSpan" id="kobo.755.1">Adding data to the vector store</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.756.1">We</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.757.1"> set the adding data flag to </span><code class="inlineCode"><span class="koboSpan" id="kobo.758.1">True</span></code><span class="koboSpan" id="kobo.759.1">:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.760.1">add_to_vector_store=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.761.1">True</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.762.1">if</span></span><span class="koboSpan" id="kobo.763.1"> add_to_vector_store == </span><span class="hljs-literal"><span class="koboSpan" id="kobo.764.1">True</span></span><span class="koboSpan" id="kobo.765.1">:
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.766.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.767.1">open</span></span><span class="koboSpan" id="kobo.768.1">(source_text, </span><span class="hljs-string"><span class="koboSpan" id="kobo.769.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.770.1">r'</span></span><span class="koboSpan" id="kobo.771.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.772.1">as</span></span><span class="koboSpan" id="kobo.773.1"> f:
        text = f.read()
        CHUNK_SIZE = </span><span class="hljs-number"><span class="koboSpan" id="kobo.774.1">1000</span></span><span class="koboSpan" id="kobo.775.1">
        chunked_text = [text[i:i+</span><span class="hljs-number"><span class="koboSpan" id="kobo.776.1">1000</span></span><span class="koboSpan" id="kobo.777.1">] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.778.1">for</span></span><span class="koboSpan" id="kobo.779.1"> i </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.780.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.781.1">range</span></span><span class="koboSpan" id="kobo.782.1">(</span><span class="hljs-number"><span class="koboSpan" id="kobo.783.1">0</span></span><span class="koboSpan" id="kobo.784.1">, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.785.1">len</span></span><span class="koboSpan" id="kobo.786.1">(text), CHUNK_SIZE)]
vector_store.add(text = chunked_text,
              embedding_function = embedding_function,
              embedding_data = chunked_text,
              metadata = [{</span><span class="hljs-string"><span class="koboSpan" id="kobo.787.1">"source"</span></span><span class="koboSpan" id="kobo.788.1">: source_text}]*</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.789.1">len</span></span><span class="koboSpan" id="kobo.790.1">(chunked_text))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.791.1">The source text, </span><code class="inlineCode"><span class="koboSpan" id="kobo.792.1">source_text = "llm.txt"</span></code><span class="koboSpan" id="kobo.793.1">, has been embedded and stored. </span><span class="koboSpan" id="kobo.793.2">A summary of the dataset’s structure is displayed, showing that the dataset was loaded:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.794.1">Creating 839 embeddings in 2 batches of size 500:: 100%|██████████| 2/2 [01:44&lt;00:00, 52.04s/it]
Dataset(path='hub://denis76/space_exploration_v1', tensors=['text', 'metadata', 'embedding', 'id'])
  tensor      htype       shape      dtype  compression
  -------    -------     -------    -------  -------
   text       text      (839, 1)      str     None  
 metadata     json      (839, 1)      str     None  
 embedding  embedding  (839, 1536)  float32   None  
    id        text      (839, 1)      str     None   
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.795.1">Observe that the dataset contains four tensors:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.796.1">embedding</span></code><span class="koboSpan" id="kobo.797.1">: Each chunk of data is embedded in a vector</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.798.1">id</span></code><span class="koboSpan" id="kobo.799.1">: The ID is a string of characters and is unique</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.800.1">metadata</span></code><span class="koboSpan" id="kobo.801.1">: The metadata contains the source of the data—in this case, the </span><code class="inlineCode"><span class="koboSpan" id="kobo.802.1">llm.txt</span></code><span class="koboSpan" id="kobo.803.1"> file.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.804.1">text</span></code><span class="koboSpan" id="kobo.805.1">: The content of a chunk of text in the dataset</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.806.1">This dataset structure can vary from one project to another, as we will see in </span><em class="chapterRef"><span class="koboSpan" id="kobo.807.1">Chapter 4</span></em><span class="koboSpan" id="kobo.808.1">, </span><em class="italic"><span class="koboSpan" id="kobo.809.1">Multimodal Modular RAG for Drone Technology</span></em><span class="koboSpan" id="kobo.810.1">. </span><span class="koboSpan" id="kobo.810.2">We can also visualize how the dataset is organized at any time to verify the structure. </span><span class="koboSpan" id="kobo.810.3">The following code will display the summary </span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.811.1">that was just displayed:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.812.1"># Print the summary of the Vector Store</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.813.1">print</span></span><span class="koboSpan" id="kobo.814.1">(vector_store.summary())
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.815.1">We can also visualize vector store information if we wish.</span></p>
    <h2 id="_idParaDest-66" class="heading-2"><span class="koboSpan" id="kobo.816.1">Vector store information</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.817.1">Activeloop’s API reference </span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.818.1">provides us with all the information we</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.819.1"> need to manage our datasets: </span><a href="https://docs.deeplake.ai/en/latest/"><span class="url"><span class="koboSpan" id="kobo.820.1">https://docs.deeplake.ai/en/latest/</span></span></a><span class="koboSpan" id="kobo.821.1">. </span><span class="koboSpan" id="kobo.821.2">We can visualize our datasets once we sign in at </span><a href="https://app.activeloop.ai/datasets/mydatasets/"><span class="url"><span class="koboSpan" id="kobo.822.1">https://app.activeloop.ai/datasets/mydatasets/</span></span></a><span class="koboSpan" id="kobo.823.1">.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.824.1">We can also load our dataset in one line of code:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.825.1">ds = deeplake.load(vector_store_path)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.826.1">The output provides a path to visualize our datasets and query and explore them online:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.827.1">This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/space_exploration_v1
hub://denis76/space_exploration_v1 loaded successfully.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.828.1">You can also access your dataset directly on Activeloop by signing in and going to your datasets. </span><span class="koboSpan" id="kobo.828.2">You will find online dataset exploration tools to query your dataset and more, as shown here:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.829.1"><img src="../Images/B31169_02_08.png" alt="A screenshot of a web page  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.830.1">Figure 2.8: Querying and exploring a Deep Lake dataset online.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.831.1">Among the </span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.832.1">many functions available, we can display the estimated size of a dataset:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.833.1">#Estimates the size in bytes of the dataset.</span></span><span class="koboSpan" id="kobo.834.1">
ds_size=ds.size_approx()
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.835.1">Once we have obtained the size, we can convert it into megabytes and gigabytes:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.836.1"># Convert bytes to megabytes and limit to 5 decimal places</span></span><span class="koboSpan" id="kobo.837.1">
ds_size_mb = ds_size / </span><span class="hljs-number"><span class="koboSpan" id="kobo.838.1">1048576</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.839.1">print</span></span><span class="koboSpan" id="kobo.840.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.841.1">f"Dataset size in megabytes: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.842.1">{ds_size_mb:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.843.1">.5</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.844.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.845.1"> MB"</span></span><span class="koboSpan" id="kobo.846.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.847.1"># Convert bytes to gigabytes and limit to 5 decimal places</span></span><span class="koboSpan" id="kobo.848.1">
ds_size_gb = ds_size / </span><span class="hljs-number"><span class="koboSpan" id="kobo.849.1">1073741824</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.850.1">print</span></span><span class="koboSpan" id="kobo.851.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.852.1">f"Dataset size in gigabytes: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.853.1">{ds_size_gb:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.854.1">.5</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.855.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.856.1"> GB"</span></span><span class="koboSpan" id="kobo.857.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.858.1">The</span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.859.1"> output shows the size of the dataset in megabytes and gigabytes:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.860.1">Dataset size in megabytes: 55.31311 MB
Dataset size in gigabytes: 0.05402 GB
</span></code></pre>
    <p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.861.1">Team #2</span></code><span class="koboSpan" id="kobo.862.1">'s pipeline component for data embedding and storage seems to be working. </span><span class="koboSpan" id="kobo.862.2">Let’s now explore augmented generation.</span></p>
    <h2 id="_idParaDest-67" class="heading-2"><span class="koboSpan" id="kobo.863.1">3. </span><span class="koboSpan" id="kobo.863.2">Augmented input generation</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.864.1">Augmented </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.865.1">generation is the third pipeline component. </span><span class="koboSpan" id="kobo.865.2">We will </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.866.1">use the data we retrieved to augment the user input. </span><span class="koboSpan" id="kobo.866.2">This component processes the user input, queries the vector store, augments the input, and calls </span><code class="inlineCode"><span class="koboSpan" id="kobo.867.1">gpt-4-turbo</span></code><span class="koboSpan" id="kobo.868.1">, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.869.1">Figure 2.9</span></em><span class="koboSpan" id="kobo.870.1">:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.871.1"><img src="../Images/B31169_02_09.png" alt="A diagram of a pipeline  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.872.1">Figure 2.9: Pipeline component #3: Augmented input generation</span></p>
    <p class="normal"><em class="italic"><span class="koboSpan" id="kobo.873.1">Figure 2.9</span></em><span class="koboSpan" id="kobo.874.1"> shows</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.875.1"> that pipeline component #3 fully deserves its </span><strong class="keyWord"><span class="koboSpan" id="kobo.876.1">Retrieval Augmented Generation</span></strong><span class="koboSpan" id="kobo.877.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.878.1">RAG</span></strong><span class="koboSpan" id="kobo.879.1">) name. </span><span class="koboSpan" id="kobo.879.2">However, it would be impossible to run this component without the </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.880.1">work put in </span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.881.1">by </span><code class="inlineCode"><span class="koboSpan" id="kobo.882.1">Team #1</span></code><span class="koboSpan" id="kobo.883.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.884.1">Team #2</span></code><span class="koboSpan" id="kobo.885.1"> to provide the necessary information to generate augmented input content.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.886.1">Let’s jump in and see how </span><code class="inlineCode"><span class="koboSpan" id="kobo.887.1">Team #3</span></code><span class="koboSpan" id="kobo.888.1"> does the job. </span><span class="koboSpan" id="kobo.888.2">Open </span><code class="inlineCode"><span class="koboSpan" id="kobo.889.1">3-Augmented_Generation.ipynb</span></code><span class="koboSpan" id="kobo.890.1"> in the GitHub repository. </span><span class="koboSpan" id="kobo.890.2">The </span><em class="italic"><span class="koboSpan" id="kobo.891.1">Installing the environment</span></em><span class="koboSpan" id="kobo.892.1"> section of the notebook is described in the </span><em class="italic"><span class="koboSpan" id="kobo.893.1">Setting up the environment</span></em><span class="koboSpan" id="kobo.894.1"> section of this chapter. </span><span class="koboSpan" id="kobo.894.2">We select the vector store (replace the vector store path with your vector store):</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.895.1">vector_store_path = </span><span class="hljs-string"><span class="koboSpan" id="kobo.896.1">"hub://denis76/space_exploration_v1"</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.897.1">Then, we load the dataset:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.898.1">from</span></span><span class="koboSpan" id="kobo.899.1"> deeplake.core.vectorstore.deeplake_vectorstore </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.900.1">import</span></span><span class="koboSpan" id="kobo.901.1"> VectorStore
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.902.1">import</span></span><span class="koboSpan" id="kobo.903.1"> deeplake.util
ds = deeplake.load(vector_store_path)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.904.1">We print a confirmation message that the vector store exists. </span><span class="koboSpan" id="kobo.904.2">At this point stage, </span><code class="inlineCode"><span class="koboSpan" id="kobo.905.1">Team #2</span></code><span class="koboSpan" id="kobo.906.1"> previously ensured that everything was working well, so we can just move ahead rapidly:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.907.1">vector_store = VectorStore(path=vector_store_path)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.908.1">The output confirms that the dataset exists and is loaded:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.909.1">Deep Lake Dataset in hub://denis76/space_exploration_v1 already exists, loading from the storage
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.910.1">We assume that </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.911.1">pipeline </span><code class="inlineCode"><span class="koboSpan" id="kobo.912.1">component #2</span></code><span class="koboSpan" id="kobo.913.1">, as built in the </span><em class="italic"><span class="koboSpan" id="kobo.914.1">Data embedding and storage</span></em><span class="koboSpan" id="kobo.915.1"> section, has created and populated the </span><code class="inlineCode"><span class="koboSpan" id="kobo.916.1">vector_store</span></code><span class="koboSpan" id="kobo.917.1"> and </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.918.1">has verified that it can be queried. </span><span class="koboSpan" id="kobo.918.2">Let’s now process the user input.</span></p>
    <h3 id="_idParaDest-68" class="heading-3"><span class="koboSpan" id="kobo.919.1">Input and query retrieval</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.920.1">We will need the embedding function to </span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.921.1">embed the user input:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.922.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.923.1">embedding_function</span></span><span class="koboSpan" id="kobo.924.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.925.1">texts, model=</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.926.1">"text-embedding-3-small"</span></span><span class="koboSpan" id="kobo.927.1">):
   </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.928.1">if</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.929.1">isinstance</span></span><span class="koboSpan" id="kobo.930.1">(texts, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.931.1">str</span></span><span class="koboSpan" id="kobo.932.1">):
       texts = [texts]
   texts = [t.replace(</span><span class="hljs-string"><span class="koboSpan" id="kobo.933.1">"\n"</span></span><span class="koboSpan" id="kobo.934.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.935.1">" "</span></span><span class="koboSpan" id="kobo.936.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.937.1">for</span></span><span class="koboSpan" id="kobo.938.1"> t </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.939.1">in</span></span><span class="koboSpan" id="kobo.940.1"> texts]
   </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.941.1">return</span></span><span class="koboSpan" id="kobo.942.1"> [data.embedding </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.943.1">for</span></span><span class="koboSpan" id="kobo.944.1"> data </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.945.1">in</span></span><span class="koboSpan" id="kobo.946.1"> openai.embeddings.create(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.947.1">input</span></span><span class="koboSpan" id="kobo.948.1"> = texts, model=model).data]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.949.1">Note that we are using the same embedding model as the data embedding and storage component to ensure full compatibility between the input and the vector dataset: </span><code class="inlineCode"><span class="koboSpan" id="kobo.950.1">text-embedding-ada-002</span></code><span class="koboSpan" id="kobo.951.1">.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.952.1">We can now either use an interactive prompt for an input or process user inputs in batches. </span><span class="koboSpan" id="kobo.952.2">In this case, we process a user input that has already been entered that could be fetched from a user interface, for example.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.953.1">We first ask the user for an input or define one:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.954.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.955.1">get_user_prompt</span></span><span class="koboSpan" id="kobo.956.1">():
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.957.1"># Request user input for the search prompt</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.958.1">return</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.959.1">input</span></span><span class="koboSpan" id="kobo.960.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.961.1">"Enter your search query: "</span></span><span class="koboSpan" id="kobo.962.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.963.1"># Get the user's search query</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.964.1">#user_prompt = get_user_prompt()</span></span><span class="koboSpan" id="kobo.965.1">
user_prompt=</span><span class="hljs-string"><span class="koboSpan" id="kobo.966.1">"Tell me about space exploration on the Moon and Mars."</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.967.1">We then plug the prompt into the search query and store the output in </span><code class="inlineCode"><span class="koboSpan" id="kobo.968.1">search_results</span></code><span class="koboSpan" id="kobo.969.1">:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.970.1">search_results = vector_store.search(embedding_data=user_prompt, embedding_function=embedding_function)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.971.1">The user prompt and search results stored in </span><code class="inlineCode"><span class="koboSpan" id="kobo.972.1">search_results</span></code><span class="koboSpan" id="kobo.973.1"> are formatted to be displayed. </span><span class="koboSpan" id="kobo.973.2">First, let’s print the user prompt:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.974.1">print</span></span><span class="koboSpan" id="kobo.975.1">(user_prompt)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.976.1">We can also wrap the retrieved text to obtain a formatted output:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.977.1"># Function to wrap text to a specified width</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.978.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.979.1">wrap_text</span></span><span class="koboSpan" id="kobo.980.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.981.1">text, width=</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.982.1">80</span></span><span class="koboSpan" id="kobo.983.1">):
    lines = []
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.984.1">while</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.985.1">len</span></span><span class="koboSpan" id="kobo.986.1">(text) &gt; width:
        split_index = text.rfind(</span><span class="hljs-string"><span class="koboSpan" id="kobo.987.1">' '</span></span><span class="koboSpan" id="kobo.988.1">, </span><span class="hljs-number"><span class="koboSpan" id="kobo.989.1">0</span></span><span class="koboSpan" id="kobo.990.1">, width)
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.991.1">if</span></span><span class="koboSpan" id="kobo.992.1"> split_index == -</span><span class="hljs-number"><span class="koboSpan" id="kobo.993.1">1</span></span><span class="koboSpan" id="kobo.994.1">:
            split_index = width
        lines.append(text[:split_index])
        text = text[split_index:].strip()
    lines.append(text)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.995.1">return</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.996.1">'\n'</span></span><span class="koboSpan" id="kobo.997.1">.join(lines)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.998.1">However, let’s</span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.999.1"> only select one of the top results and print it:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1000.1">import</span></span><span class="koboSpan" id="kobo.1001.1"> textwrap
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1002.1"># Assuming the search results are ordered with the top result first</span></span><span class="koboSpan" id="kobo.1003.1">
top_score = search_results[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1004.1">'score'</span></span><span class="koboSpan" id="kobo.1005.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.1006.1">0</span></span><span class="koboSpan" id="kobo.1007.1">]
top_text = search_results[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1008.1">'text'</span></span><span class="koboSpan" id="kobo.1009.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.1010.1">0</span></span><span class="koboSpan" id="kobo.1011.1">].strip()
top_metadata = search_results[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1012.1">'metadata'</span></span><span class="koboSpan" id="kobo.1013.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.1014.1">0</span></span><span class="koboSpan" id="kobo.1015.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.1016.1">'source'</span></span><span class="koboSpan" id="kobo.1017.1">]
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1018.1"># Print the top search result</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.1019.1">print</span></span><span class="koboSpan" id="kobo.1020.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1021.1">"Top Search Result:"</span></span><span class="koboSpan" id="kobo.1022.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1023.1">print</span></span><span class="koboSpan" id="kobo.1024.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1025.1">f"Score: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1026.1">{top_score}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1027.1">"</span></span><span class="koboSpan" id="kobo.1028.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1029.1">print</span></span><span class="koboSpan" id="kobo.1030.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1031.1">f"Source: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1032.1">{top_metadata}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1033.1">"</span></span><span class="koboSpan" id="kobo.1034.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1035.1">print</span></span><span class="koboSpan" id="kobo.1036.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1037.1">"Text:"</span></span><span class="koboSpan" id="kobo.1038.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1039.1">print</span></span><span class="koboSpan" id="kobo.1040.1">(wrap_text(top_text))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1041.1">The </span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.1042.1">following output shows that we have a reasonably good match:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1043.1">Top Search Result:
Score: 0.6016581654548645
Source: llm.txt
Text:
Exploration of space, planets, and moons "Space Exploration" redirects here.
</span><span class="koboSpan" id="kobo.1043.2">For the company, see SpaceX . </span><span class="koboSpan" id="kobo.1043.3">For broader coverage of this topic, see
Exploration . </span><span class="koboSpan" id="kobo.1043.4">Buzz Aldrin taking a core sample of the Moon during the Apollo 11 mission Self-portrait of Curiosity rover on Mars 's surface Part of a series on…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1044.1">We are ready to augment the input with the additional information we have retrieved.</span></p>
    <h3 id="_idParaDest-69" class="heading-3"><span class="koboSpan" id="kobo.1045.1">Augmented input</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.1046.1">The program adds the top</span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.1047.1"> retrieved text to the user input:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1048.1">augmented_input=user_prompt+</span><span class="hljs-string"><span class="koboSpan" id="kobo.1049.1">" "</span></span><span class="koboSpan" id="kobo.1050.1">+top_text
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1051.1">print</span></span><span class="koboSpan" id="kobo.1052.1">(augmented_input)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1053.1">The output displays the augmented input:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1054.1">Tell me about space exploration on the Moon and Mars. </span><span class="koboSpan" id="kobo.1054.2">Exploration of space, planets …
</span></code></pre>
    <p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.1055.1">gpt-4o</span></code><span class="koboSpan" id="kobo.1056.1"> can now process the augmented input and generate content:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1057.1">from</span></span><span class="koboSpan" id="kobo.1058.1"> openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1059.1">import</span></span><span class="koboSpan" id="kobo.1060.1"> OpenAI
client = OpenAI()
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1061.1">import</span></span><span class="koboSpan" id="kobo.1062.1"> time
gpt_model = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1063.1">"gpt-4o"</span></span><span class="koboSpan" id="kobo.1064.1"> 
start_time = time.time()  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1065.1"># Start timing before the request</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1066.1">Note that we are timing the process. </span><span class="koboSpan" id="kobo.1066.2">We now write the generative AI call, adding roles to the message we create for the model:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1067.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1068.1">call_gpt4_with_full_text</span></span><span class="koboSpan" id="kobo.1069.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1070.1">itext</span></span><span class="koboSpan" id="kobo.1071.1">):
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1072.1"># Join all lines to form a single string</span></span><span class="koboSpan" id="kobo.1073.1">
    text_input = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1074.1">'\n'</span></span><span class="koboSpan" id="kobo.1075.1">.join(itext)
    prompt = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1076.1">f"Please summarize or elaborate on the following content:\n</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1077.1">{text_input}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1078.1">"</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.1079.1">try</span></span><span class="koboSpan" id="kobo.1080.1">:
        response = client.chat.completions.create(
            model=gpt_model,
            messages=[
                {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1081.1">"role"</span></span><span class="koboSpan" id="kobo.1082.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1083.1">"system"</span></span><span class="koboSpan" id="kobo.1084.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1085.1">"content"</span></span><span class="koboSpan" id="kobo.1086.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1087.1">"You are a space exploration expert."</span></span><span class="koboSpan" id="kobo.1088.1">},
                {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1089.1">"role"</span></span><span class="koboSpan" id="kobo.1090.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1091.1">"assistant"</span></span><span class="koboSpan" id="kobo.1092.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1093.1">"content"</span></span><span class="koboSpan" id="kobo.1094.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1095.1">"You can read the input and answer in detail."</span></span><span class="koboSpan" id="kobo.1096.1">},
                {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1097.1">"role"</span></span><span class="koboSpan" id="kobo.1098.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1099.1">"user"</span></span><span class="koboSpan" id="kobo.1100.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1101.1">"content"</span></span><span class="koboSpan" id="kobo.1102.1">: prompt}
            ],
            temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1103.1">0.1</span></span>  <span class="hljs-comment"><span class="koboSpan" id="kobo.1104.1"># Fine-tune parameters as needed</span></span><span class="koboSpan" id="kobo.1105.1">
        )
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1106.1">return</span></span><span class="koboSpan" id="kobo.1107.1"> response.choices[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1108.1">0</span></span><span class="koboSpan" id="kobo.1109.1">].message.content.strip()
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1110.1">except</span></span><span class="koboSpan" id="kobo.1111.1"> Exception </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1112.1">as</span></span><span class="koboSpan" id="kobo.1113.1"> e:
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1114.1">return</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1115.1">str</span></span><span class="koboSpan" id="kobo.1116.1">(e)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1117.1">The generative</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.1118.1"> model is called with the augmented input; the response time is calculated and displayed along with the output:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1119.1">gpt4_response = call_gpt4_with_full_text(augmented_input)
response_time = time.time() - start_time  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1120.1"># Measure response time</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.1121.1">print</span></span><span class="koboSpan" id="kobo.1122.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1123.1">f"Response Time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1124.1">{response_time:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1125.1">.2</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1126.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1127.1"> seconds"</span></span><span class="koboSpan" id="kobo.1128.1">)  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1129.1"># Print response time</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.1130.1">print</span></span><span class="koboSpan" id="kobo.1131.1">(gpt_model, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1132.1">"Response:"</span></span><span class="koboSpan" id="kobo.1133.1">, gpt4_response)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1134.1">Note that the raw output is displayed with the response time:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1135.1">Response Time: 8.44 seconds
gpt-4o Response: Space exploration on the Moon and Mars has been a significant focus of human spaceflight and robotic missions. </span><span class="koboSpan" id="kobo.1135.2">Here's a detailed summary…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1136.1">Let’s format the output with </span><code class="inlineCode"><span class="koboSpan" id="kobo.1137.1">textwrap</span></code><span class="koboSpan" id="kobo.1138.1"> and print the result. </span><code class="inlineCode"><span class="koboSpan" id="kobo.1139.1">print_formatted_response(response)</span></code><span class="koboSpan" id="kobo.1140.1"> first checks if the response returned contains Markdown</span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.1141.1"> features. </span><span class="koboSpan" id="kobo.1141.2">If so, it will format the response; if not, it will perform a standard output text wrap:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1142.1">import</span></span><span class="koboSpan" id="kobo.1143.1"> textwrap
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1144.1">import</span></span><span class="koboSpan" id="kobo.1145.1"> re
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1146.1">from</span></span><span class="koboSpan" id="kobo.1147.1"> IPython.display </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1148.1">import</span></span><span class="koboSpan" id="kobo.1149.1"> display, Markdown, HTML
import markdown
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1150.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1151.1">print_formatted_response</span></span><span class="koboSpan" id="kobo.1152.1">(response):
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1153.1">    # Check for markdown by looking for patterns like headers, bold, lists, etc.</span></span><span class="koboSpan" id="kobo.1154.1">
    markdown_patterns = [
        r"^#+\s",           </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1155.1"># Headers</span></span><span class="koboSpan" id="kobo.1156.1">
        r"^\*+",            </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1157.1"># Bullet points</span></span><span class="koboSpan" id="kobo.1158.1">
        r"\*\*",            </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1159.1"># Bold</span></span><span class="koboSpan" id="kobo.1160.1">
        r"_",               </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1161.1"># Italics</span></span><span class="koboSpan" id="kobo.1162.1">
        r"\[.+\]\(.+\)",    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1163.1"># Links</span></span><span class="koboSpan" id="kobo.1164.1">
        r"-\s",             </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1165.1"># Dashes used for lists</span></span><span class="koboSpan" id="kobo.1166.1">
        r"\`\`\`"           </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1167.1"># Code blocks</span></span><span class="koboSpan" id="kobo.1168.1">
    ]
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1169.1">    # If any pattern matches, assume the response is in markdown</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.1170.1">if</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1171.1">any</span></span><span class="koboSpan" id="kobo.1172.1">(re.search(pattern, response, re.MULTILINE) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1173.1">for</span></span><span class="koboSpan" id="kobo.1174.1"> pattern </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1175.1">in</span></span><span class="koboSpan" id="kobo.1176.1"> markdown_patterns):
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1177.1">        # Markdown detected, convert to HTML for nicer display</span></span><span class="koboSpan" id="kobo.1178.1">
        html_output = markdown.markdown(response)
        display(HTML(html_output))  # Use display(HTML()) to render HTML in Colab
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1179.1">else</span></span><span class="koboSpan" id="kobo.1180.1">:
        # No markdown detected, wrap and print as plain text
        wrapper = textwrap.TextWrapper(width=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1181.1">80</span></span><span class="koboSpan" id="kobo.1182.1">)
        wrapped_text = wrapper.fill(text=response)
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1183.1">print</span></span><span class="koboSpan" id="kobo.1184.1">("Text Response:")
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1185.1">print</span></span><span class="koboSpan" id="kobo.1186.1">("--------------------")
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1187.1">print</span></span><span class="koboSpan" id="kobo.1188.1">(wrapped_text)
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1189.1">print</span></span><span class="koboSpan" id="kobo.1190.1">("--------------------\n")
print_formatted_response(gpt4_response)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1191.1">The output is satisfactory:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1192.1">Moon Exploration
    Historical Missions:
    1. </span><span class="koboSpan" id="kobo.1192.2">Apollo Missions: NASA's Apollo program, particularly Apollo 11, marked the first manned Moon landing in 1969. </span><span class="koboSpan" id="kobo.1192.3">Astronauts like Buzz Aldrin collected core samples and conducted experiments.
    </span><span class="koboSpan" id="kobo.1192.4">2. </span><span class="koboSpan" id="kobo.1192.5">Lunar Missions: Various missions have been conducted to explore the Moon, including robotic landers and orbiters from different countries.
</span><span class="koboSpan" id="kobo.1192.6">Scientific Goals:
    3. </span><span class="koboSpan" id="kobo.1192.7">Geological Studies: Understanding the Moon's composition, structure, and history.
    </span><span class="koboSpan" id="kobo.1192.8">4. </span><span class="koboSpan" id="kobo.1192.9">Resource Utilization: Investigating the potential for mining resources like Helium-3 and water ice.
    </span><span class="koboSpan" id="kobo.1192.10">Future Plans:
    1. </span><span class="koboSpan" id="kobo.1192.11">Artemis Program: NASA's initiative to return humans to the Moon and establish a sustainable presence by the late 2020s.
    </span><span class="koboSpan" id="kobo.1192.12">2. </span><span class="koboSpan" id="kobo.1192.13">International Collaboration: Partnerships with other space agencies and private companies to build lunar bases and conduct scientific research.
</span><span class="koboSpan" id="kobo.1192.14">Mars Exploration
    Robotic Missions:
    1. </span><span class="koboSpan" id="kobo.1192.15">Rovers: NASA's rovers like Curiosity and Perseverance have been exploring Mars' surface, analyzing soil and rock samples, and searching for signs of past life.
    </span><span class="koboSpan" id="kobo.1192.16">2. </span><span class="koboSpan" id="kobo.1192.17">Orbiters: Various orbiters have been mapping Mars' surface and studying its atmosphere…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1193.1">Let’s introduce an evaluation metric to measure the quality of the output.</span></p>
    <h1 id="_idParaDest-70" class="heading-1"><span class="koboSpan" id="kobo.1194.1">Evaluating the output with cosine similarity</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.1195.1">In this section, we will implement cosine similarity to measure the similarity between user input and the generative AI model’s output</span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.1196.1">. </span><span class="koboSpan" id="kobo.1196.2">We will also measure the augmented user input with the generative AI model’s output. </span><span class="koboSpan" id="kobo.1196.3">Let’s first define a cosine similarity function:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1197.1">from</span></span><span class="koboSpan" id="kobo.1198.1"> sklearn.feature_extraction.text </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1199.1">import</span></span><span class="koboSpan" id="kobo.1200.1"> TfidfVectorizer
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1201.1">from</span></span><span class="koboSpan" id="kobo.1202.1"> sklearn.metrics.pairwise </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1203.1">import</span></span><span class="koboSpan" id="kobo.1204.1"> cosine_similarity
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1205.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1206.1">calculate_cosine_similarity</span></span><span class="koboSpan" id="kobo.1207.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1208.1">text1, text2</span></span><span class="koboSpan" id="kobo.1209.1">):
    vectorizer = TfidfVectorizer()
    tfidf = vectorizer.fit_transform([text1, text2])
    similarity = cosine_similarity(tfidf[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1210.1">0</span></span><span class="koboSpan" id="kobo.1211.1">:</span><span class="hljs-number"><span class="koboSpan" id="kobo.1212.1">1</span></span><span class="koboSpan" id="kobo.1213.1">], tfidf[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1214.1">1</span></span><span class="koboSpan" id="kobo.1215.1">:</span><span class="hljs-number"><span class="koboSpan" id="kobo.1216.1">2</span></span><span class="koboSpan" id="kobo.1217.1">])
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1218.1">return</span></span><span class="koboSpan" id="kobo.1219.1"> similarity[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1220.1">0</span></span><span class="koboSpan" id="kobo.1221.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.1222.1">0</span></span><span class="koboSpan" id="kobo.1223.1">]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1224.1">Then, let’s calculate a score that measures the similarity between the user prompt and GPT-4’s response:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1225.1">similarity_score = calculate_cosine_similarity(user_prompt, gpt4_response)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1226.1">print</span></span><span class="koboSpan" id="kobo.1227.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1228.1">f"Cosine Similarity Score: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1229.1">{similarity_score:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1230.1">.3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1231.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1232.1">"</span></span><span class="koboSpan" id="kobo.1233.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1234.1">The score is low, although the output seemed acceptable for a human:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1235.1">Cosine Similarity Score: 0.396
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1236.1">It seems that either we missed something or need to use another metric.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1237.1">Let’s try to calculate the similarity between the augmented input and GPT-4’s response:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1238.1"># Example usage with your existing functions</span></span><span class="koboSpan" id="kobo.1239.1">
similarity_score = calculate_cosine_similarity(augmented_input, gpt4_response)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1240.1">print</span></span><span class="koboSpan" id="kobo.1241.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1242.1">f"Cosine Similarity Score: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1243.1">{similarity_score:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1244.1">.3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1245.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1246.1">"</span></span><span class="koboSpan" id="kobo.1247.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1248.1">The score seems better:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1249.1">Cosine Similarity Score: 0.857
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1250.1">Can we use another method? </span><span class="koboSpan" id="kobo.1250.2">Cosine similarity, when using </span><strong class="keyWord"><span class="koboSpan" id="kobo.1251.1">Term Frequency-Inverse Document Frequency</span></strong><span class="koboSpan" id="kobo.1252.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.1253.1">TF-IDF</span></strong><span class="koboSpan" id="kobo.1254.1">), relies heavily on </span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.1255.1">exact vocabulary overlap and takes into account important language features, such as semantic meanings, synonyms, or contextual usage. </span><span class="koboSpan" id="kobo.1255.2">As such, this method may produce lower similarity scores for texts that are conceptually similar but differ in word choice.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1256.1">In contrast, using </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.1257.1">Sentence Transformers to calculate similarity involves embeddings that capture deeper semantic relationships between words and phrases. </span><span class="koboSpan" id="kobo.1257.2">This approach is more effective in recognizing the contextual and conceptual similarity between texts. </span><span class="koboSpan" id="kobo.1257.3">Let’s try this approach.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1258.1">First, let’s install </span><code class="inlineCode"><span class="koboSpan" id="kobo.1259.1">sentence-transformers</span></code><span class="koboSpan" id="kobo.1260.1">:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1261.1">!pip install sentence-transformers
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1262.1">Be careful installing this library at the end of the session, since it may induce potential conflicts with the RAG pipeline’s requirements. </span><span class="koboSpan" id="kobo.1262.2">Depending on a project’s needs, this code could be yet another separate pipeline component.</span></p>
    <div class="note">
      <p class="normal"><span class="koboSpan" id="kobo.1263.1">As of August 2024, using a Hugging Face token is optional. </span><span class="koboSpan" id="kobo.1263.2">If Hugging Face requires a token, sign up to Hugging Face to obtain an API token, check the conditions, and set up the key as instructed.</span></p>
    </div>
    <p class="normal"><span class="koboSpan" id="kobo.1264.1">We will now use a MiniLM architecture to perform the task with </span><code class="inlineCode"><span class="koboSpan" id="kobo.1265.1">all-MiniLM-L6-v2</span></code><span class="koboSpan" id="kobo.1266.1">. </span><span class="koboSpan" id="kobo.1266.2">This model is available through the Hugging Face Model Hub we are using. </span><span class="koboSpan" id="kobo.1266.3">It’s part of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1267.1">sentence-transformers</span></code><span class="koboSpan" id="kobo.1268.1"> library, which is an extension of the Hugging Face Transformers library. </span><span class="koboSpan" id="kobo.1268.2">We are using this architecture because it offers a compact and efficient model, with a strong performance in generating meaningful sentence embeddings quickly. </span><span class="koboSpan" id="kobo.1268.3">Let’s now implement it</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.1269.1"> with the following function:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1270.1">from</span></span><span class="koboSpan" id="kobo.1271.1"> sentence_transformers </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1272.1">import</span></span><span class="koboSpan" id="kobo.1273.1"> SentenceTransformer
model = SentenceTransformer(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1274.1">'all-MiniLM-L6-v2'</span></span><span class="koboSpan" id="kobo.1275.1">)
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1276.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1277.1">calculate_cosine_similarity_with_embeddings</span></span><span class="koboSpan" id="kobo.1278.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1279.1">text1, text2</span></span><span class="koboSpan" id="kobo.1280.1">):
    embeddings1 = model.encode(text1)
    embeddings2 = model.encode(text2)
    similarity = cosine_similarity([embeddings1], [embeddings2])
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1281.1">return</span></span><span class="koboSpan" id="kobo.1282.1"> similarity[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1283.1">0</span></span><span class="koboSpan" id="kobo.1284.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.1285.1">0</span></span><span class="koboSpan" id="kobo.1286.1">]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1287.1">We can now call the function to calculate the similarity between the augmented user input and GPT-4’s response:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1288.1">similarity_score = calculate_cosine_similarity_with_embeddings(augmented_input, gpt4_response)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1289.1">print</span></span><span class="koboSpan" id="kobo.1290.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1291.1">f"Cosine Similarity Score: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1292.1">{similarity_score:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1293.1">.3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1294.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1295.1">"</span></span><span class="koboSpan" id="kobo.1296.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1297.1">The output shows that the Sentence Transformer captures semantic similarities between the texts more effectively, resulting in a high cosine similarity score:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1298.1">Cosine Similarity Score: 0.739
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1299.1">The </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.1300.1">choice of metrics depends on the specific requirements of each project phase. </span><em class="chapterRef"><span class="koboSpan" id="kobo.1301.1">Chapter 3</span></em><span class="koboSpan" id="kobo.1302.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1303.1">Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI</span></em><span class="koboSpan" id="kobo.1304.1">, will provide advanced metrics when we implement index-based RAG. </span><span class="koboSpan" id="kobo.1304.2">At this stage, however, the RAG pipeline’s three components have been successfully built. </span><span class="koboSpan" id="kobo.1304.3">Let’s summarize our journey and move to the next level!</span></p>
    <h1 id="_idParaDest-71" class="heading-1"><span class="koboSpan" id="kobo.1305.1">Summary</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.1306.1">In this chapter, we tackled the complexities of using RAG-driven generative AI, focusing on the essential role of document embeddings when handling large datasets. </span><span class="koboSpan" id="kobo.1306.2">We saw how to go from raw texts to embeddings and store them in vector stores. </span><span class="koboSpan" id="kobo.1306.3">Vector stores such as Activeloop, unlike parametric generative AI models, provide API tools and visual interfaces that allow us to see embedded text at any moment.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1307.1">A RAG pipeline detailed the organizational process of integrating OpenAI embeddings into Activeloop Deep Lake vector stores. </span><span class="koboSpan" id="kobo.1307.2">The RAG pipeline was broken down into distinct components that can vary from one project to another. </span><span class="koboSpan" id="kobo.1307.3">This separation allows multiple teams to work simultaneously without dependency, accelerating development and facilitating specialized focus on individual aspects, such as data collection, embedding processing, and query generation for the augmented generation AI process.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1308.1">We then built a three-component RAG pipeline, beginning by highlighting the necessity of specific cross-platform packages and careful system architecture planning. </span><span class="koboSpan" id="kobo.1308.2">The resources involved were Python functions built from scratch, Activeloop Deep Lake to organize and store the embeddings in a dataset in a vector store, an OpenAI embedding model, and OpenAI’s GPT-4o generative AI model. </span><span class="koboSpan" id="kobo.1308.3">The program guided us through building a three-part RAG pipeline using Python, with practical steps that involved setting up the environment, handling dependencies, and addressing implementation challenges like data chunking and vector store integration.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1309.1">This journey provided a robust understanding of embedding documents in vector stores and leveraging them for enhanced generative AI outputs, preparing us to apply these insights to real-world AI applications in well-organized processes and teams within an organization. </span><span class="koboSpan" id="kobo.1309.2">Vector stores enhance the retrieval of documents that require precision in information retrieval. </span><span class="koboSpan" id="kobo.1309.3">Indexing takes RAG further and increases the speed and relevance of retrievals. </span><span class="koboSpan" id="kobo.1309.4">The next chapter will take us a step further by introducing advanced indexing methods to retrieve and augment inputs.</span></p>
    <h1 id="_idParaDest-72" class="heading-1"><span class="koboSpan" id="kobo.1310.1">Questions</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.1311.1">Answer the following questions with </span><em class="italic"><span class="koboSpan" id="kobo.1312.1">Yes</span></em><span class="koboSpan" id="kobo.1313.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.1314.1">No</span></em><span class="koboSpan" id="kobo.1315.1">:</span></p>
    <ol>
      <li class="numberedList" value="1"><span class="koboSpan" id="kobo.1316.1">Do embeddings convert text into high-dimensional vectors for faster retrieval in RAG? </span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1317.1">Are keyword searches more effective than embeddings in retrieving detailed semantic content? </span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1318.1">Is it recommended to separate RAG pipelines into independent components?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1319.1">Does the RAG pipeline consist of only two main components? </span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1320.1">Can Activeloop Deep Lake handle both embedding and vector storage? </span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1321.1">Is the text-embedding-3-small model from OpenAI used to generate embeddings in this chapter? </span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1322.1">Are data embeddings visible and directly traceable in an RAG-driven system? </span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1323.1">Can a RAG pipeline run smoothly without splitting into separate components?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1324.1">Is chunking large texts into smaller parts necessary for embedding and storage? </span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.1325.1">Are cosine similarity metrics used to evaluate the relevance of retrieved information?</span></li>
    </ol>
    <h1 id="_idParaDest-73" class="heading-1"><span class="koboSpan" id="kobo.1326.1">References</span></h1>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.1327.1">OpenAI Ada documentation for embeddings: </span><a href="https://platform.openai.com/docs/guides/embeddings/embedding-models"><span class="url"><span class="koboSpan" id="kobo.1328.1">https://platform.openai.com/docs/guides/embeddings/embedding-models</span></span></a></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1329.1">OpenAI GPT documentation for content generation: </span><a href="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4"><span class="url"><span class="koboSpan" id="kobo.1330.1">https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4</span></span></a></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1331.1">Activeloop API documentation: </span><a href="https://docs.deeplake.ai/en/latest/"><span class="url"><span class="koboSpan" id="kobo.1332.1">https://docs.deeplake.ai/en/latest/</span></span></a></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1333.1">MiniLM model reference: </span><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"><span class="url"><span class="koboSpan" id="kobo.1334.1">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</span></span></a></li>
    </ul>
    <h1 id="_idParaDest-74" class="heading-1"><span class="koboSpan" id="kobo.1335.1">Further reading</span></h1>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.1336.1">OpenAI’s documentation on embeddings: </span><a href="https://platform.openai.com/docs/guides/embeddings"><span class="url"><span class="koboSpan" id="kobo.1337.1">https://platform.openai.com/docs/guides/embeddings</span></span></a></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1338.1">Activeloop documentation: </span><a href="https://docs.activeloop.ai/"><span class="url"><span class="koboSpan" id="kobo.1339.1">https://docs.activeloop.ai/</span></span></a></li>
    </ul>
    <h1 id="_idParaDest-75" class="heading-1"><span class="koboSpan" id="kobo.1340.1">Join our community on Discord</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.1341.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
    <p class="normal"><a href="https://www.packt.link/rag"><span class="url"><span class="koboSpan" id="kobo.1342.1">https://www.packt.link/rag</span></span></a></p>
    <p class="normal"><span class="koboSpan" id="kobo.1343.1"><img src="../Images/QR_Code50409000288080484.png" alt=""/></span></p>
  </div>
</body></html>