<html><head></head><body>
  <div><h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-50" class="chapterTitle">RAG Embedding Vector Stores with Deep Lake and OpenAI</h1>
    <p class="normal">There will come a point in the execution of your project where complexity is unavoidable when implementing RAG-driven generative AI. Embeddings transform bulky structured or unstructured texts into compact, high-dimensional vectors that capture their semantic essence, enabling faster and more efficient information retrieval. However, we will inevitably be faced with a storage issue as the creation and storage of document embeddings become necessary when managing increasingly large datasets. You could ask the question at this point, why not use keywords instead of embeddings? And the answer is simple: although embeddings require more storage space, they capture the deeper semantic meanings of texts, with more nuanced and context-aware retrieval compared to the rigid and often-matched keywords. This results in better, more pertinent retrievals. Hence, our option is to turn to vector stores in which embeddings are organized and rapidly accessible.</p>
    <p class="normal">We will begin this chapter by exploring how to go from raw data to an Activeloop Deep Lake vector store via loading OpenAI embedding models. This requires installing and implementing several cross-platform packages, which leads us to the architecture of such systems. We will organize our RAG pipeline into separate components because breaking down the RAG pipeline into independent parts will enable several teams to work on a project simultaneously. We will then set the blueprint for a RAG-driven generative AI pipeline. Finally, we will build a three-component RAG pipeline from scratch in Python with Activeloop Deep Lake, OpenAI, and custom-built functions.</p>
    <p class="normal">This coding journey will take us into the depths of cross-platform environment issues with packages and dependencies. We will also face the challenges of chunking data, embedding vectors, and loading them on vector stores. We will augment the input of a GPT-4o model with retrieval queries and produce solid outputs. By the end of this chapter, you will fully understand how to leverage the power of embedded documents in vector stores for generative AI.</p>
    <p class="normal">To sum up, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">Introducing document embeddings and vector stores</li>
      <li class="bulletList">How to break a RAG pipeline into independent components</li>
      <li class="bulletList">Building a RAG pipeline from raw data to Activeloop Deep Lake</li>
      <li class="bulletList">Facing the environmental challenge of cross-platform packages and libraries</li>
      <li class="bulletList">Leveraging the power of LLMs to embed data with an OpenAI embedding model</li>
      <li class="bulletList">Querying an Activeloop Deep Lake vector store to augment user inputs</li>
      <li class="bulletList">Generative solid augmented outputs with OpenAI GPT-4o</li>
    </ul>
    <p class="normal">Let’s begin by learning how to go from raw data to a vector store.</p>
    <h1 id="_idParaDest-51" class="heading-1">From raw data to embeddings in vector stores</h1>
    <p class="normal">Embeddings convert<a id="_idIndexMarker089"/> any form of data (text, images, or audio) into real numbers. Thus, a document is converted into a vector. These mathematical representations of documents allow us to calculate the distances between documents and retrieve similar data.</p>
    <p class="normal">The raw data (books, articles, blogs, pictures, or songs) is first collected and cleaned to remove noise. The prepared data is then fed into a model such as OpenAI <code class="inlineCode">text-embedding-3-small</code>, which will embed the data. Activeloop Deep Lake, for example, which we will implement in this <a id="_idIndexMarker090"/>chapter, will break a text down into pre-defined chunks defined by a certain number of characters. The size of a chunk could be 1,000 characters, for instance. We can let the system optimize these chunks, as we will implement them in the <em class="italic">Optimizing chunking</em> section of the next chapter. These chunks of text make it easier to process large amounts of data and provide more detailed embeddings of a document, as shown here:</p>
    <figure class="mediaobject"><img src="img/B31169_02_01.png" alt="A diagram of a number and embedding  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 2.1: Excerpt of an Activeloop vector store dataset record</p>
    <p class="normal">Transparency has<a id="_idIndexMarker091"/> been the holy grail in AI since the beginning of parametric models, in which the information is buried in learned parameters that produce black box systems. RAG is a game changer, as shown in <em class="italic">Figure 2.1</em>, because the content is fully traceable:</p>
    <ul>
      <li class="bulletList">Left side (Text): In RAG frameworks, every piece of generated content is traceable back to its source data, ensuring the output’s transparency. The OpenAI generative model will respond, taking the augmented input into account.</li>
      <li class="bulletList">Right side (Embeddings): Data embeddings<a id="_idIndexMarker092"/> are directly visible and linked to the text, contrasting with parametric models where data origins are encoded within model parameters.</li>
    </ul>
    <p class="normal">Once we have our text and embeddings, the next step is to store them efficiently for quick retrieval. This is where <em class="italic">vector stores</em> come into<a id="_idIndexMarker093"/> play. A vector store is a specialized database designed to handle high-dimensional data like embeddings. We can create datasets on serverless platforms such as Activeloop, as shown in <em class="italic">Figure 2.2</em>. We can create and access them in code through an API, as we will do in the <em class="italic">Building a RAG pipeline</em> section of this chapter.</p>
    <figure class="mediaobject"><img src="img/B31169_02_02.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.2: Managing datasets with vector stores</p>
    <p class="normal">Another feature of <a id="_idIndexMarker094"/>vector stores is their ability to retrieve data with optimized methods. Vector stores are built with powerful indexing methods, which we will discuss in the next chapter. This retrieving capacity allows a RAG model to quickly find and retrieve the most relevant embeddings during the generation phase, augment user inputs, and increase the model’s ability to produce high-quality output.</p>
    <p class="normal">We will now see how to organize a RAG pipeline that goes from data collection, processing, and retrieval to augmented-input generation.</p>
    <h1 id="_idParaDest-52" class="heading-1">Organizing RAG in a pipeline</h1>
    <p class="normal">A RAG pipeline<a id="_idIndexMarker095"/> will typically collect data and prepare it by cleaning it, for example, chunking the documents, embedding them, and storing them in a vector store dataset. The vector dataset is then queried to augment the user input of a generative AI model to produce an output. However, it is highly recommended not to run this sequence of RAG in one single program when it comes to using a vector store. We should at least separate the process into three components:</p>
    <ul>
      <li class="bulletList">Data collection and preparation</li>
      <li class="bulletList">Data embedding and loading into the dataset of a vector store</li>
      <li class="bulletList">Querying the vectorized dataset to augment the input of a generative AI model to produce a response</li>
    </ul>
    <p class="normal">Let’s go through the main<a id="_idIndexMarker096"/> reasons for this component approach:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Specialization</strong>, which will allow each member of a team to do what they are best at, either collecting and cleaning data, running embedding models, managing vector stores, or tweaking generative AI models.</li>
      <li class="bulletList"><strong class="keyWord">Scalability</strong>, making it easier to upgrade separate components as the technology evolves and scale the different components with specialized methods. Storing raw data, for example, can be scaled on a different server than the cloud platform, where the embedded vectors are stored in a vectorized dataset.</li>
      <li class="bulletList"><strong class="keyWord">Parallel development</strong>, which allows each team to advance at their pace without waiting for others. Improvements can be made continually on one component without disrupting the processes of the other components.</li>
      <li class="bulletList"><strong class="keyWord">Maintenance</strong> is component-independent. One team can work on one component without affecting the other parts of the system. For example, if the RAG pipeline is in production, users can continue querying and running generative AI through the vector store while a team fixes the data collection component.</li>
      <li class="bulletList"><strong class="keyWord">Security</strong> concerns and privacy are minimized because each team can work separately with specific authorization, access, and roles for each component.</li>
    </ul>
    <p class="normal">As we can see, in real-life production environments or large-scale projects, it is rare for a single program or team to manage end-to-end processes. We are now ready to draw the blueprint of the RAG pipeline that we will build in Python in this chapter.</p>
    <h1 id="_idParaDest-53" class="heading-1">A RAG-driven generative AI pipeline</h1>
    <p class="normal">Let’s dive into what a real-life<a id="_idIndexMarker097"/> RAG pipeline looks like. Imagine we’re a team that has to deliver a whole system in just a few weeks. Right off the bat, we’re bombarded with questions like:</p>
    <ul>
      <li class="bulletList">Who’s going to gather and clean up all the data?</li>
      <li class="bulletList">Who’s going to handle setting up OpenAI’s embedding model?</li>
      <li class="bulletList">Who’s writing the code to get those embeddings up and running and managing the vector store?</li>
      <li class="bulletList">Who’s going to take care of implementing GPT-4 and managing what it spits out?</li>
    </ul>
    <p class="normal">Within a few minutes, everyone starts looking pretty worried. The whole thing feels overwhelming—like, seriously, who would even think about tackling all that alone?</p>
    <p class="normal">So here’s what we do. We <a id="_idIndexMarker098"/>split into three groups, each of us taking on different parts of the pipeline, as shown in <em class="italic">Figure 2.3</em>:</p>
    <figure class="mediaobject"><img src="img/B31169_02_03.png" alt="A diagram of a pipeline  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.3: RAG pipeline components</p>
    <p class="normal">Each of the three groups has one component to implement:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Data Collection and Prep (D1 and D2)</strong>: One team <a id="_idIndexMarker099"/>takes on <a id="_idIndexMarker100"/>collecting the data and cleaning it.</li>
      <li class="bulletList"><strong class="keyWord">Data Embedding and Storage (D2 and D3)</strong>: Another team works on getting the data through<a id="_idIndexMarker101"/> OpenAI’s embedding model and stores these vectors in an Activeloop Deep Lake dataset.</li>
      <li class="bulletList"><strong class="keyWord">Augmented Generation (D4, G1-G4, and E1)</strong>: The last team handles the big job of generating<a id="_idIndexMarker102"/> content based on user input and retrieval queries. They use GPT-4 for this, and even though it sounds like a lot, it’s actually a bit easier because they aren’t waiting on anyone else—they just need the computer to do its calculations and evaluate the <a id="_idIndexMarker103"/>output.</li>
    </ul>
    <p class="normal">Suddenly, the project doesn’t seem so scary. Everyone has their part to focus on, and we can all work without being distracted by the other teams. This way, we can all move faster and get the job done without the hold-ups that usually slow things down.</p>
    <p class="normal">The organization of the project, represented in <em class="italic">Figure 2.3</em>, is a variant of the RAG ecosystem’s framework represented in <em class="italic">Figure 1.3</em> of <em class="italic">Chapter 1</em>, <em class="italic">Why Retrieval Augmented Generation?</em></p>
    <p class="normal">We can now begin building a RAG pipeline.</p>
    <h1 id="_idParaDest-54" class="heading-1">Building a RAG pipeline</h1>
    <p class="normal">We will now build a RAG pipeline by<a id="_idIndexMarker104"/> implementing the pipeline described in the previous section and illustrated in <em class="italic">Figure 2.3</em>. We will implement three components assuming that three teams (<code class="inlineCode">Team #1</code>, <code class="inlineCode">Team #2</code>, and <code class="inlineCode">Team #3</code>) work in parallel to implement the pipeline:</p>
    <ul>
      <li class="bulletList">Data collection and preparation by <code class="inlineCode">Team #1</code></li>
      <li class="bulletList">Data embedding and storage by <code class="inlineCode">Team #2</code></li>
      <li class="bulletList">Augmented generation by <code class="inlineCode">Team #3</code></li>
    </ul>
    <p class="normal">The first step is to set up the environment for these components.</p>
    <h2 id="_idParaDest-55" class="heading-2">Setting up the environment</h2>
    <p class="normal">Let’s face it here and now. Installing <a id="_idIndexMarker105"/>cross-platform, cross-library packages with their dependencies can be quite challenging! It is important to take this <a id="_idIndexMarker106"/>complexity into account and be prepared to get the environment running correctly. Each package has dependencies that may have conflicting versions. Even if we adapt the versions, an application may not run as expected anymore. So, take your time to install the right versions of the packages and dependencies.</p>
    <p class="normal">We will only describe the environment once in this section for all three components and refer to this section when necessary.</p>
    <h3 id="_idParaDest-56" class="heading-3">The installation packages and libraries</h3>
    <p class="normal">To <a id="_idIndexMarker107"/>build the RAG pipeline in this section, we will need packages and need to freeze the package versions to prevent dependency conflicts and issues<a id="_idIndexMarker108"/> with the functions of the libraries, such as:</p>
    <ul>
      <li class="bulletList">Possible conflicts between the versions of the dependencies.</li>
      <li class="bulletList">Possible conflicts when one of the libraries needs to be updated for an application to run. For example, in August 2024, installing <code class="inlineCode">Deep Lake</code> required <code class="inlineCode">Pillow</code> version 10.x.x and Google Colab’s version was 9.x.x. Thus, it was necessary to uninstall <code class="inlineCode">Pillow</code> and reinstall it with a recent version before installing <code class="inlineCode">Deep Lake</code>. Google Colab will no doubt update Pillow. Many cases such as this occur in a fast-moving market.</li>
      <li class="bulletList">Possible deprecations if the versions remain frozen for too long.</li>
      <li class="bulletList">Possible issues if the versions are frozen for too long and bugs are not corrected by upgrades.</li>
    </ul>
    <p class="normal">Thus, if we freeze the versions, an application may remain stable for some time but encounter issues. But if we upgrade the versions too quickly, some of the other libraries may not work anymore. There is no silver bullet! It’s a continual quality control process.</p>
    <p class="normal">For our program, in this section, we will freeze the versions. Let’s now go through the installation steps to create the environment for our pipeline.</p>
    <h3 id="_idParaDest-57" class="heading-3">The components involved in the installation process</h3>
    <p class="normal">Let’s <a id="_idIndexMarker109"/>begin by describing the components that are installed in the <em class="italic">Installing the environment</em> section of each notebook. The components are not necessarily installed in all notebooks; this section serves as an inventory of the packages.</p>
    <p class="normal">In the first pipeline section, <em class="italic">1. Data collection and preparation</em>, we will only need to install Beautiful Soup and Requests:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install beautifulsoup4==4.12.3
!pip install requests==2.31.0
</code></pre>
    <p class="normal">This explains why this component of the pipeline should remain separate. It’s a straightforward job for a developer who enjoys creating interfaces to interact with the web. It’s also a perfect fit for a junior developer who wants to get involved in data collection and analysis.</p>
    <p class="normal">The <a id="_idIndexMarker110"/>two other pipeline components we will build in this section, <em class="italic">2. Data embedding and storage</em> and <em class="italic">3. Augmented generation</em>, will require more attention as well as the installation of <code class="inlineCode">requirements01.txt</code>, as explained in the previous section. For now, let’s continue with the installation step by step.</p>
    <h4 class="heading-4">Mounting a drive</h4>
    <p class="normal">In<a id="_idIndexMarker111"/> this scenario, the program mounts Google Drive in Google Colab to safely read the OpenAI API key to access OpenAI models and the Activeloop API token for authentication to access Activeloop Deep Lake datasets:</p>
    <pre class="programlisting code"><code class="hljs-code">#Google Drive option to store API Keys
#Store your key in a file and read it(you can type it directly in the # #notebook but it will be visible for somebody next to you)
from google.colab import drive
drive.mount('/content/drive')
</code></pre>
    <p class="normal">You can choose to store your keys and tokens elsewhere. Just make sure they are in a safe location.</p>
    <h4 class="heading-4">Creating a subprocess to download files from GitHub</h4>
    <p class="normal">The <a id="_idIndexMarker112"/>goal here is to write a function to download the <code class="inlineCode">grequests.py</code> file from GitHub. This program contains a function to download files using <code class="inlineCode">curl</code>, with the option to add a private token if necessary:</p>
    <pre class="programlisting code"><code class="hljs-code">import subprocess
url = "https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py"
output_file = "grequests.py"
# Prepare the curl command using the private token
curl_command = [
    "curl",
    "-o", output_file,
    url
]
# Execute the curl command
try:
    subprocess.run(curl_command, check=True)
    print("Download successful.")
except subprocess.CalledProcessError:
    print("Failed to download the file.")
</code></pre>
    <p class="normal">The <code class="inlineCode">grequests.py</code> file contains a function that can, if necessary, accept a private token or any<a id="_idIndexMarker113"/> other security system that requires credentials when retrieving data with <code class="inlineCode">curl</code> commands:</p>
    <pre class="programlisting code"><code class="hljs-code">import subprocess
import os
# add a private token after the filename if necessary
def download(directory, filename):
    # The base URL of the image files in the GitHub repository
    base_url = 'https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/'
    # Complete URL for the file
    file_url = f"{base_url}{directory}/{filename}"
    # Use curl to download the file, including an Authorization header for the private token
    try:
        # Prepare the curl command with the Authorization header
        #curl_command = f'curl -H "Authorization: token {private_token}" -o {filename} {file_url}'
        curl_command = f'curl -H -o {filename} {file_url}'
        # Execute the curl command
        subprocess.run(curl_command, check=True, shell=True)
        print(f"Downloaded '{filename}' successfully.")
    except subprocess.CalledProcessError:
        print(f"Failed to download '{filename}'. Check the URL, your internet connection, and if the token is correct and has appropriate permissions.")
</code></pre>
    <h4 class="heading-4">Installing requirements</h4>
    <p class="normal">Now, we <a id="_idIndexMarker114"/>will install the requirements for this section when working with Activeloop Deep Lake and OpenAI. We will only need:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install deeplake==3.9.18
!pip install openai==1.40.3
</code></pre>
    <p class="normal">As of August 2024, Google Colab’s version of Pillow conflicts with <code class="inlineCode">deeplake</code>'s package. However, the <code class="inlineCode">deeplake</code> installation package deals with this automatically. All you have to do is restart the session and run it again, which is why <code class="inlineCode">pip install deeplake==3.9.18</code> is the first line of each notebook it is installed in.</p>
    <p class="normal">After installing the requirements, we must run a line of code for Activeloop to activate a public DNS server:</p>
    <pre class="programlisting code"><code class="hljs-code"># For Google Colab and Activeloop(Deeplake library)
#This line writes the string "nameserver 8.8.8.8" to the file. This is specifying that the DNS server the system
#should use is at the IP address 8.8.8.8, which is one of Google's Public DNS servers.
with open('/etc/resolv.conf', 'w') as file:
   file.write("nameserver 8.8.8.8")
</code></pre>
    <h4 class="heading-4">Authentication process</h4>
    <p class="normal">You <a id="_idIndexMarker115"/>will need to sign up to OpenAI<a id="_idIndexMarker116"/> to obtain an API key: <a href="https://openai.com/">https://openai.com/</a>. Make sure to check the pricing policy before using the key. First, let’s activate OpenAI’s API key:</p>
    <pre class="programlisting code"><code class="hljs-code">#Retrieving and setting OpenAI API key
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline().strip()
f.close()
#The OpenAI API key
import os
import openai
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
</code></pre>
    <p class="normal">Then, we activate Activeloop’s API token for Deep Lake:</p>
    <pre class="programlisting code"><code class="hljs-code">#Retrieving and setting Activeloop API token
f = open("drive/MyDrive/files/activeloop.txt", "r")
API_token=f.readline().strip()
f.close()
ACTIVELOOP_TOKEN=API_token
os.environ['ACTIVELOOP_TOKEN'] =ACTIVELOOP_TOKEN
</code></pre>
    <p class="normal">You will<a id="_idIndexMarker117"/> need to sign up on Activeloop<a id="_idIndexMarker118"/> to obtain an API token: <a href="https://www.activeloop.ai/">https://www.activeloop.ai/</a>. Again, make sure to check the pricing policy before using the Activeloop token.</p>
    <p class="normal">Once the environment is installed, you can hide the <em class="italic">Installing the environment</em> cells we just ran to focus on the content of the pipeline components, as shown in <em class="italic">Figure 2.4</em>:</p>
    <figure class="mediaobject"><img src="img/B31169_02_04.png" alt="A white background with black text  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2:4: Hiding the installation cells</p>
    <p class="normal">The installation cells will then be hidden but can still be run, as shown in <em class="italic">Figure 2.5</em>:</p>
    <figure class="mediaobject"><img src="img/B31169_02_05.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.5: Running hidden cells</p>
    <p class="normal">We can now focus on the pipeline components for each pipeline component. Let’s begin with data collection and preparation.</p>
    <h2 id="_idParaDest-58" class="heading-2">1. Data collection and preparation</h2>
    <p class="normal">Data collection and <a id="_idIndexMarker119"/>preparation is the first pipeline component, as described earlier<a id="_idIndexMarker120"/> in this chapter. <code class="inlineCode">Team #1</code> will only focus on their component, as shown in <em class="italic">Figure 2.6</em>:</p>
    <figure class="mediaobject"><img src="img/B31169_02_06.png" alt="A diagram of a pipeline component  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.6: Pipeline component #1: Data collection and preparation</p>
    <p class="normal">Let’s jump in and lend a hand to <code class="inlineCode">Team #1</code>. Our work is clearly defined, so we can enjoy the time taken to implement the component. We will retrieve and process 10 Wikipedia articles that provide a comprehensive view of various aspects of space exploration:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Space exploration</strong>: Overview<a id="_idIndexMarker121"/> of the history, technologies, missions, and plans involved in the exploration of space (<a href="https://en.wikipedia.org/wiki/Space_exploration">https://en.wikipedia.org/wiki/Space_exploration</a>)</li>
      <li class="bulletList"><strong class="keyWord">Apollo program</strong>: Details about the NASA program that landed the first humans on the Moon and its <a id="_idIndexMarker122"/>significant missions (<a href="https://en.wikipedia.org/wiki/Apollo_program">https://en.wikipedia.org/wiki/Apollo_program</a>)</li>
      <li class="bulletList"><strong class="keyWord">Hubble Space Telescope</strong>: Information on one of the most significant telescopes ever built, which<a id="_idIndexMarker123"/> has been crucial in many astronomical discoveries (<a href="https://en.wikipedia.org/wiki/Hubble_Space_Telescope">https://en.wikipedia.org/wiki/Hubble_Space_Telescope</a>)</li>
      <li class="bulletList"><strong class="keyWord">Mars rover</strong>: Insight<a id="_idIndexMarker124"/> into the rovers that have been sent to Mars to study its surface and environment (<a href="https://en.wikipedia.org/wiki/Mars_rover">https://en.wikipedia.org/wiki/Mars_rover</a>)</li>
      <li class="bulletList"><strong class="keyWord">International Space Station (ISS)</strong>: Details about the ISS, its construction, international collaboration, and<a id="_idIndexMarker125"/> its role in space research (<a href="https://en.wikipedia.org/wiki/International_Space_Station">https://en.wikipedia.org/wiki/International_Space_Station</a>)</li>
      <li class="bulletList"><strong class="keyWord">SpaceX</strong>: Covers the history, achievements, and goals of SpaceX, one of the most influential <a id="_idIndexMarker126"/>private spaceflight companies (<a href="https://en.wikipedia.org/wiki/SpaceX">https://en.wikipedia.org/wiki/SpaceX</a>)</li>
      <li class="bulletList"><strong class="keyWord">Juno (spacecraft)</strong>: Information about the NASA space probe that orbits and studies Jupiter, its <a id="_idIndexMarker127"/>structure, and moons (<a href="https://en.wikipedia.org/wiki/Juno_(spacecraft)">https://en.wikipedia.org/wiki/Juno_(spacecraft))</a></li>
      <li class="bulletList"><strong class="keyWord">Voyager program</strong>: Details on the Voyager missions, including their contributions to our <a id="_idIndexMarker128"/>understanding of the outer solar system and interstellar space (<a href="https://en.wikipedia.org/wiki/Voyager_program">https://en.wikipedia.org/wiki/Voyager_program</a>)</li>
      <li class="bulletList"><strong class="keyWord">Galileo (spacecraft)</strong>: Overview of the mission that studied Jupiter and its moons, providing valuable<a id="_idIndexMarker129"/> data on the gas giant and its system (<a href="https://en.wikipedia.org/wiki/Galileo_(spacecraft)">https://en.wikipedia.org/wiki/Galileo_(spacecraft)</a>)</li>
      <li class="bulletList"><strong class="keyWord">Kepler space telescope</strong>: Information <a id="_idIndexMarker130"/>about the space telescope designed to discover Earth-size planets orbiting other stars (<a href="https://en.wikipedia.org/wiki/Kepler_Space_Telescope">https://en.wikipedia.org/wiki/Kepler_Space_Telescope</a>)</li>
    </ul>
    <p class="normal">These articles <a id="_idIndexMarker131"/>cover a wide range of topics in space exploration, from <a id="_idIndexMarker132"/>historical programs to modern technological advances and missions.</p>
    <p class="normal">Now, open <code class="inlineCode">1-Data_collection_preparation.ipynb</code> in the GitHub repository. We will first collect the data.</p>
    <h3 id="_idParaDest-59" class="heading-3">Collecting the data</h3>
    <p class="normal">We just<a id="_idIndexMarker133"/> need <code class="inlineCode">import requests</code> for the HTTP requests, <code class="inlineCode">from bs4 import BeautifulSoup</code> for HTML parsing, and <code class="inlineCode">import re</code>, the regular expressions module:</p>
    <pre class="programlisting code"><code class="hljs-code">import requests
from bs4 import BeautifulSoup
import re
</code></pre>
    <p class="normal">We then select the URLs we need:</p>
    <pre class="programlisting code"><code class="hljs-code"># URLs of the Wikipedia articles
urls = [
    "https://en.wikipedia.org/wiki/Space_exploration",
    "https://en.wikipedia.org/wiki/Apollo_program",
    "https://en.wikipedia.org/wiki/Hubble_Space_Telescope",
    "https://en.wikipedia.org/wiki/Mars_over",
    "https://en.wikipedia.org/wiki/International_Space_Station",
    "https://en.wikipedia.org/wiki/SpaceX",
    "https://en.wikipedia.org/wiki/Juno_(spacecraft)",
    "https://en.wikipedia.org/wiki/Voyager_program",
    "https://en.wikipedia.org/wiki/Galileo_(spacecraft)",
    "https://en.wikipedia.org/wiki/Kepler_Space_Telescope"
]
</code></pre>
    <p class="normal">This list is in code. However, it could be stored in a database, a file, or any other format, such as JSON. We can now prepare the data.</p>
    <h3 id="_idParaDest-60" class="heading-3">Preparing the data</h3>
    <p class="normal">First, we<a id="_idIndexMarker134"/> write a cleaning function. This function removes numerical references such as [1] [2] from a given text string, using regular expressions, and returns the cleaned text:</p>
    <pre class="programlisting code"><code class="hljs-code">def clean_text(content):
    # Remove references that usually appear as [1], [2], etc.
    content = re.sub(r'\[\d+\]', '', content)
    return content
</code></pre>
    <p class="normal">Then, we<a id="_idIndexMarker135"/> write a classical fetch and clean function, which will return a nice and clean text by extracting the content we need from the documents:</p>
    <pre class="programlisting code"><code class="hljs-code">def fetch_and_clean(url):
    # Fetch the content of the URL
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    # Find the main content of the article, ignoring side boxes and headers
    content = soup.find('div', {'class': 'mw-parser-output'})
    # Remove the bibliography section, which generally follows a header like "References", "Bibliography"
    for section_title in ['References', 'Bibliography', 'External links', 'See also']:
        section = content.find('span', id=section_title)
        if section:
            # Remove all content from this section to the end of the document
            for sib in section.parent.find_next_siblings():
                sib.decompose()
            section.parent.decompose()
    # Extract and clean the text
    text = content.get_text(separator=' ', strip=True)
    text = clean_text(text)
    return text
</code></pre>
    <p class="normal">Finally, we write the content in <code class="inlineCode">llm.txt</code> file for the team working on the data embedding and storage functions:</p>
    <pre class="programlisting code"><code class="hljs-code"># File to write the clean text
with open('llm.txt', 'w', encoding='utf-8') as file:
    for url in urls:
        clean_article_text = fetch_and_clean(url)
        file.write(clean_article_text + '\n')
print("Content written to llm.txt")
</code></pre>
    <p class="normal">The output confirms that the text has been written:</p>
    <pre class="programlisting con"><code class="hljs-con">Content written to llm.txt
</code></pre>
    <p class="normal">The program<a id="_idIndexMarker136"/> can be modified to save the data in other formats and locations, as required for a project’s specific needs. The file can then be verified before we move on to the next batch of data to retrieve and process:</p>
    <pre class="programlisting code"><code class="hljs-code"># Open the file and read the first 20 lines
with open('llm.txt', 'r', encoding='utf-8') as file:
    lines = file.readlines()
    # Print the first 20 lines
    for line in lines[:20]:
        print(line.strip())
</code></pre>
    <p class="normal">The output shows the first lines of the document that will be processed:</p>
    <pre class="programlisting con"><code class="hljs-con">Exploration of space, planets, and moons "Space Exploration" redirects here. For the company, see SpaceX . For broader coverage of this topic, see Exploration . Buzz Aldrin taking a core sample of the Moon during the Apollo 11 mission…
</code></pre>
    <p class="normal">This <a id="_idIndexMarker137"/>component can be managed by a team that enjoys searching for documents on the web or within a company’s data environment. The team will gain experience in identifying the best documents for a project, which is the foundation of any RAG framework.</p>
    <p class="normal"><code class="inlineCode">Team #2</code> can now work on the data to embed the documents and store them.</p>
    <h2 id="_idParaDest-61" class="heading-2">2. Data embedding and storage</h2>
    <p class="normal"><code class="inlineCode">Team #2</code>'s job is to <a id="_idIndexMarker138"/>focus on the second component of<a id="_idIndexMarker139"/> the pipeline. They will receive batches of prepared data to work on. They don’t have to worry about retrieving data. <code class="inlineCode">Team #1</code> has their back with their data collection and preparation component.</p>
    <figure class="mediaobject"><img src="img/B31169_02_07.png" alt="A diagram of a pipeline component  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.7: Pipeline component #2: Data embedding and storage</p>
    <p class="normal">Let’s now jump <a id="_idIndexMarker140"/>in and help <code class="inlineCode">Team #2</code> to get the job done. Open <code class="inlineCode">2-Embeddings_vector_store.ipynb</code> in the GitHub Repository. We will embed and <a id="_idIndexMarker141"/>store the data provided by <code class="inlineCode">Team #1</code> and retrieve a batch of documents to work on.</p>
    <h3 id="_idParaDest-62" class="heading-3">Retrieving a batch of prepared documents</h3>
    <p class="normal">First, we download a batch of<a id="_idIndexMarker142"/> documents available on a server and provided by <code class="inlineCode">Team #1</code>, which is the first of a continual stream of incoming documents. In this case, we assume it’s the space exploration file:</p>
    <pre class="programlisting code"><code class="hljs-code">from grequests import download
source_text = "llm.txt"
directory = "Chapter02"
filename = "llm.txt"
download(directory, filename)
</code></pre>
    <p class="normal">Note that <code class="inlineCode">source_text = "llm.txt"</code> will be used by the function that will add the data to our vector store. We then briefly check the document just to be sure, knowing that <code class="inlineCode">Team #1</code> has already verified the information:</p>
    <pre class="programlisting code"><code class="hljs-code"># Open the file and read the first 20 lines
with open('llm.txt', 'r', encoding='utf-8') as file:
    lines = file.readlines()
    # Print the first 20 lines
    for line in lines[:20]:
        print(line.strip())
</code></pre>
    <p class="normal">The output is satisfactory, as shown in the following excerpt:</p>
    <pre class="programlisting con"><code class="hljs-con">Exploration of space, planets, and moons "Space Exploration" redirects here.
</code></pre>
    <p class="normal">We will now chunk the data. We will determine a chunk size defined by the number of characters. In this case, it is <code class="inlineCode">CHUNK_SIZE = 1000</code>, but we can select chunk sizes using different strategies. <em class="chapterRef">Chapter 7</em>, <em class="italic">Building Scalable Knowledge-Graph-based RAG with Wikipedia API and LlamaIndex</em>, will take chunk size optimization further with automated seamless chunking.</p>
    <p class="normal">Chunking is necessary to optimize <a id="_idIndexMarker143"/>data processing: selecting portions of text, embedding, and loading the data. It also makes the embedded dataset easier to query. The following code chunks a document to complete the preparation process:</p>
    <pre class="programlisting code"><code class="hljs-code">with open(source_text, 'r') as f:
    text = f.read()
CHUNK_SIZE = 1000
chunked_text = [text[i:i+CHUNK_SIZE] for i in range(0,len(text), CHUNK_SIZE)]
</code></pre>
    <p class="normal">We are now ready to create a vector store to vectorize data or add data to an existing one.</p>
    <h3 id="_idParaDest-63" class="heading-3">Verifying if the vector store exists and creating it if not</h3>
    <p class="normal">First, we need to<a id="_idIndexMarker144"/> define the path of our Activeloop vector store path, whether our dataset exists or not:</p>
    <pre class="programlisting code"><code class="hljs-code">vector_store_path = "hub://denis76/space_exploration_v1"
</code></pre>
    <div><p class="normal">Make sure to replace <code class="inlineCode">`hub://denis76/space_exploration_v1`</code> with your organization and dataset name.</p>
    </div>
    <p class="normal">Then, we write a function to attempt to load the vector store<a id="_idIndexMarker145"/> or automatically create one if it doesn’t exist:</p>
    <pre class="programlisting code"><code class="hljs-code">from deeplake.core.vectorstore.deeplake_vectorstore import VectorStore
import deeplake.util
try:
    # Attempt to load the vector store
    vector_store = VectorStore(path=vector_store_path)
    print("Vector store exists")
except FileNotFoundError:
    print("Vector store does not exist. You can create it.")
    # Code to create the vector store goes here
    create_vector_store=True
</code></pre>
    <p class="normal">The output confirms that the vector store has been created:</p>
    <pre class="programlisting con"><code class="hljs-con">Your Deep Lake dataset has been successfully created!
Vector store exists
</code></pre>
    <p class="normal">We now need to create an embedding function.</p>
    <h3 id="_idParaDest-64" class="heading-3">The embedding function</h3>
    <p class="normal">The embedding function<a id="_idIndexMarker146"/> will transform the chunks of data we created into vectors to enable vector-based search. In this program, we will use <code class="inlineCode">"text-embedding-3-small"</code> to embed the documents.</p>
    <p class="normal">OpenAI has other embedding models<a id="_idIndexMarker147"/> that you can use: <a href="https://platform.openai.com/docs/models/embeddings">https://platform.openai.com/docs/models/embeddings</a>. <em class="chapterRef">Chapter 6</em>, <em class="italic">Scaling RAG Bank Customer Data with Pinecone</em>, provides alternative code for embedding models in the <em class="italic">Embedding</em> section. In any case, it is recommended to evaluate embedding models before choosing one in production. Examine the characteristics of each embedding model, as described by OpenAI, focusing on their length and capacities. <code class="inlineCode">text-embedding-3-small</code> was chosen in this case because it stands out as a robust choice for efficiency and speed:</p>
    <pre class="programlisting code"><code class="hljs-code">def embedding_function(texts, model="text-embedding-3-small"):
   if isinstance(texts, str):
       texts = [texts]
   texts = [t.replace("\n", " ") for t in texts]
   return [data.embedding for data in openai.embeddings.create(input = texts, model=model).data]
</code></pre>
    <p class="normal">The <code class="inlineCode">text-embedding-3-small</code> text embedding model from OpenAI typically uses embeddings<a id="_idIndexMarker148"/> with a restricted number of dimensions, to balance obtaining enough detail in the embeddings with large computational workloads and storage space. Make sure to check the model page and pricing information before running the code: <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models">https://platform.openai.com/docs/guides/embeddings/embedding-models</a>.</p>
    <p class="normal">We are now all set to begin populating the vector store.</p>
    <h3 id="_idParaDest-65" class="heading-3">Adding data to the vector store</h3>
    <p class="normal">We<a id="_idIndexMarker149"/> set the adding data flag to <code class="inlineCode">True</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">add_to_vector_store=True
if add_to_vector_store == True:
    with open(source_text, 'r') as f:
        text = f.read()
        CHUNK_SIZE = 1000
        chunked_text = [text[i:i+1000] for i in range(0, len(text), CHUNK_SIZE)]
vector_store.add(text = chunked_text,
              embedding_function = embedding_function,
              embedding_data = chunked_text,
              metadata = [{"source": source_text}]*len(chunked_text))
</code></pre>
    <p class="normal">The source text, <code class="inlineCode">source_text = "llm.txt"</code>, has been embedded and stored. A summary of the dataset’s structure is displayed, showing that the dataset was loaded:</p>
    <pre class="programlisting con"><code class="hljs-con">Creating 839 embeddings in 2 batches of size 500:: 100%|██████████| 2/2 [01:44&lt;00:00, 52.04s/it]
Dataset(path='hub://denis76/space_exploration_v1', tensors=['text', 'metadata', 'embedding', 'id'])
  tensor      htype       shape      dtype  compression
  -------    -------     -------    -------  -------
   text       text      (839, 1)      str     None  
 metadata     json      (839, 1)      str     None  
 embedding  embedding  (839, 1536)  float32   None  
    id        text      (839, 1)      str     None   
</code></pre>
    <p class="normal">Observe that the dataset contains four tensors:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">embedding</code>: Each chunk of data is embedded in a vector</li>
      <li class="bulletList"><code class="inlineCode">id</code>: The ID is a string of characters and is unique</li>
      <li class="bulletList"><code class="inlineCode">metadata</code>: The metadata contains the source of the data—in this case, the <code class="inlineCode">llm.txt</code> file.</li>
      <li class="bulletList"><code class="inlineCode">text</code>: The content of a chunk of text in the dataset</li>
    </ul>
    <p class="normal">This dataset structure can vary from one project to another, as we will see in <em class="chapterRef">Chapter 4</em>, <em class="italic">Multimodal Modular RAG for Drone Technology</em>. We can also visualize how the dataset is organized at any time to verify the structure. The following code will display the summary <a id="_idIndexMarker150"/>that was just displayed:</p>
    <pre class="programlisting code"><code class="hljs-code"># Print the summary of the Vector Store
print(vector_store.summary())
</code></pre>
    <p class="normal">We can also visualize vector store information if we wish.</p>
    <h2 id="_idParaDest-66" class="heading-2">Vector store information</h2>
    <p class="normal">Activeloop’s API reference <a id="_idIndexMarker151"/>provides us with all the information we<a id="_idIndexMarker152"/> need to manage our datasets: <a href="https://docs.deeplake.ai/en/latest/">https://docs.deeplake.ai/en/latest/</a>. We can visualize our datasets once we sign in at <a href="https://app.activeloop.ai/datasets/mydatasets/">https://app.activeloop.ai/datasets/mydatasets/</a>.</p>
    <p class="normal">We can also load our dataset in one line of code:</p>
    <pre class="programlisting code"><code class="hljs-code">ds = deeplake.load(vector_store_path)
</code></pre>
    <p class="normal">The output provides a path to visualize our datasets and query and explore them online:</p>
    <pre class="programlisting con"><code class="hljs-con">This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/space_exploration_v1
hub://denis76/space_exploration_v1 loaded successfully.
</code></pre>
    <p class="normal">You can also access your dataset directly on Activeloop by signing in and going to your datasets. You will find online dataset exploration tools to query your dataset and more, as shown here:</p>
    <figure class="mediaobject"><img src="img/B31169_02_08.png" alt="A screenshot of a web page  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.8: Querying and exploring a Deep Lake dataset online.</p>
    <p class="normal">Among the <a id="_idIndexMarker153"/>many functions available, we can display the estimated size of a dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">#Estimates the size in bytes of the dataset.
ds_size=ds.size_approx()
</code></pre>
    <p class="normal">Once we have obtained the size, we can convert it into megabytes and gigabytes:</p>
    <pre class="programlisting code"><code class="hljs-code"># Convert bytes to megabytes and limit to 5 decimal places
ds_size_mb = ds_size / 1048576
print(f"Dataset size in megabytes: {ds_size_mb:.5f} MB")
# Convert bytes to gigabytes and limit to 5 decimal places
ds_size_gb = ds_size / 1073741824
print(f"Dataset size in gigabytes: {ds_size_gb:.5f} GB")
</code></pre>
    <p class="normal">The<a id="_idIndexMarker154"/> output shows the size of the dataset in megabytes and gigabytes:</p>
    <pre class="programlisting con"><code class="hljs-con">Dataset size in megabytes: 55.31311 MB
Dataset size in gigabytes: 0.05402 GB
</code></pre>
    <p class="normal"><code class="inlineCode">Team #2</code>'s pipeline component for data embedding and storage seems to be working. Let’s now explore augmented generation.</p>
    <h2 id="_idParaDest-67" class="heading-2">3. Augmented input generation</h2>
    <p class="normal">Augmented <a id="_idIndexMarker155"/>generation is the third pipeline component. We will <a id="_idIndexMarker156"/>use the data we retrieved to augment the user input. This component processes the user input, queries the vector store, augments the input, and calls <code class="inlineCode">gpt-4-turbo</code>, as shown in <em class="italic">Figure 2.9</em>:</p>
    <figure class="mediaobject"><img src="img/B31169_02_09.png" alt="A diagram of a pipeline  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.9: Pipeline component #3: Augmented input generation</p>
    <p class="normal"><em class="italic">Figure 2.9</em> shows<a id="_idIndexMarker157"/> that pipeline component #3 fully deserves its <strong class="keyWord">Retrieval Augmented Generation</strong> (<strong class="keyWord">RAG</strong>) name. However, it would be impossible to run this component without the <a id="_idIndexMarker158"/>work put in <a id="_idIndexMarker159"/>by <code class="inlineCode">Team #1</code> and <code class="inlineCode">Team #2</code> to provide the necessary information to generate augmented input content.</p>
    <p class="normal">Let’s jump in and see how <code class="inlineCode">Team #3</code> does the job. Open <code class="inlineCode">3-Augmented_Generation.ipynb</code> in the GitHub repository. The <em class="italic">Installing the environment</em> section of the notebook is described in the <em class="italic">Setting up the environment</em> section of this chapter. We select the vector store (replace the vector store path with your vector store):</p>
    <pre class="programlisting code"><code class="hljs-code">vector_store_path = "hub://denis76/space_exploration_v1"
</code></pre>
    <p class="normal">Then, we load the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">from deeplake.core.vectorstore.deeplake_vectorstore import VectorStore
import deeplake.util
ds = deeplake.load(vector_store_path)
</code></pre>
    <p class="normal">We print a confirmation message that the vector store exists. At this point stage, <code class="inlineCode">Team #2</code> previously ensured that everything was working well, so we can just move ahead rapidly:</p>
    <pre class="programlisting code"><code class="hljs-code">vector_store = VectorStore(path=vector_store_path)
</code></pre>
    <p class="normal">The output confirms that the dataset exists and is loaded:</p>
    <pre class="programlisting con"><code class="hljs-con">Deep Lake Dataset in hub://denis76/space_exploration_v1 already exists, loading from the storage
</code></pre>
    <p class="normal">We assume that <a id="_idIndexMarker160"/>pipeline <code class="inlineCode">component #2</code>, as built in the <em class="italic">Data embedding and storage</em> section, has created and populated the <code class="inlineCode">vector_store</code> and <a id="_idIndexMarker161"/>has verified that it can be queried. Let’s now process the user input.</p>
    <h3 id="_idParaDest-68" class="heading-3">Input and query retrieval</h3>
    <p class="normal">We will need the embedding function to <a id="_idIndexMarker162"/>embed the user input:</p>
    <pre class="programlisting code"><code class="hljs-code">def embedding_function(texts, model="text-embedding-3-small"):
   if isinstance(texts, str):
       texts = [texts]
   texts = [t.replace("\n", " ") for t in texts]
   return [data.embedding for data in openai.embeddings.create(input = texts, model=model).data]
</code></pre>
    <p class="normal">Note that we are using the same embedding model as the data embedding and storage component to ensure full compatibility between the input and the vector dataset: <code class="inlineCode">text-embedding-ada-002</code>.</p>
    <p class="normal">We can now either use an interactive prompt for an input or process user inputs in batches. In this case, we process a user input that has already been entered that could be fetched from a user interface, for example.</p>
    <p class="normal">We first ask the user for an input or define one:</p>
    <pre class="programlisting code"><code class="hljs-code">def get_user_prompt():
    # Request user input for the search prompt
    return input("Enter your search query: ")
# Get the user's search query
#user_prompt = get_user_prompt()
user_prompt="Tell me about space exploration on the Moon and Mars."
</code></pre>
    <p class="normal">We then plug the prompt into the search query and store the output in <code class="inlineCode">search_results</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">search_results = vector_store.search(embedding_data=user_prompt, embedding_function=embedding_function)
</code></pre>
    <p class="normal">The user prompt and search results stored in <code class="inlineCode">search_results</code> are formatted to be displayed. First, let’s print the user prompt:</p>
    <pre class="programlisting code"><code class="hljs-code">print(user_prompt)
</code></pre>
    <p class="normal">We can also wrap the retrieved text to obtain a formatted output:</p>
    <pre class="programlisting code"><code class="hljs-code"># Function to wrap text to a specified width
def wrap_text(text, width=80):
    lines = []
    while len(text) &gt; width:
        split_index = text.rfind(' ', 0, width)
        if split_index == -1:
            split_index = width
        lines.append(text[:split_index])
        text = text[split_index:].strip()
    lines.append(text)
    return '\n'.join(lines)
</code></pre>
    <p class="normal">However, let’s<a id="_idIndexMarker163"/> only select one of the top results and print it:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap
# Assuming the search results are ordered with the top result first
top_score = search_results['score'][0]
top_text = search_results['text'][0].strip()
top_metadata = search_results['metadata'][0]['source']
# Print the top search result
print("Top Search Result:")
print(f"Score: {top_score}")
print(f"Source: {top_metadata}")
print("Text:")
print(wrap_text(top_text))
</code></pre>
    <p class="normal">The <a id="_idIndexMarker164"/>following output shows that we have a reasonably good match:</p>
    <pre class="programlisting con"><code class="hljs-con">Top Search Result:
Score: 0.6016581654548645
Source: llm.txt
Text:
Exploration of space, planets, and moons "Space Exploration" redirects here.
For the company, see SpaceX . For broader coverage of this topic, see
Exploration . Buzz Aldrin taking a core sample of the Moon during the Apollo 11 mission Self-portrait of Curiosity rover on Mars 's surface Part of a series on…
</code></pre>
    <p class="normal">We are ready to augment the input with the additional information we have retrieved.</p>
    <h3 id="_idParaDest-69" class="heading-3">Augmented input</h3>
    <p class="normal">The program adds the top<a id="_idIndexMarker165"/> retrieved text to the user input:</p>
    <pre class="programlisting code"><code class="hljs-code">augmented_input=user_prompt+" "+top_text
print(augmented_input)
</code></pre>
    <p class="normal">The output displays the augmented input:</p>
    <pre class="programlisting con"><code class="hljs-con">Tell me about space exploration on the Moon and Mars. Exploration of space, planets …
</code></pre>
    <p class="normal"><code class="inlineCode">gpt-4o</code> can now process the augmented input and generate content:</p>
    <pre class="programlisting code"><code class="hljs-code">from openai import OpenAI
client = OpenAI()
import time
gpt_model = "gpt-4o" 
start_time = time.time()  # Start timing before the request
</code></pre>
    <p class="normal">Note that we are timing the process. We now write the generative AI call, adding roles to the message we create for the model:</p>
    <pre class="programlisting code"><code class="hljs-code">def call_gpt4_with_full_text(itext):
    # Join all lines to form a single string
    text_input = '\n'.join(itext)
    prompt = f"Please summarize or elaborate on the following content:\n{text_input}"
    try:
        response = client.chat.completions.create(
            model=gpt_model,
            messages=[
                {"role": "system", "content": "You are a space exploration expert."},
                {"role": "assistant", "content": "You can read the input and answer in detail."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1  # Fine-tune parameters as needed
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return str(e)
</code></pre>
    <p class="normal">The generative<a id="_idIndexMarker166"/> model is called with the augmented input; the response time is calculated and displayed along with the output:</p>
    <pre class="programlisting code"><code class="hljs-code">gpt4_response = call_gpt4_with_full_text(augmented_input)
response_time = time.time() - start_time  # Measure response time
print(f"Response Time: {response_time:.2f} seconds")  # Print response time
print(gpt_model, "Response:", gpt4_response)
</code></pre>
    <p class="normal">Note that the raw output is displayed with the response time:</p>
    <pre class="programlisting con"><code class="hljs-con">Response Time: 8.44 seconds
gpt-4o Response: Space exploration on the Moon and Mars has been a significant focus of human spaceflight and robotic missions. Here's a detailed summary…
</code></pre>
    <p class="normal">Let’s format the output with <code class="inlineCode">textwrap</code> and print the result. <code class="inlineCode">print_formatted_response(response)</code> first checks if the response returned contains Markdown<a id="_idIndexMarker167"/> features. If so, it will format the response; if not, it will perform a standard output text wrap:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap
import re
from IPython.display import display, Markdown, HTML
import markdown
def print_formatted_response(response):
    # Check for markdown by looking for patterns like headers, bold, lists, etc.
    markdown_patterns = [
        r"^#+\s",           # Headers
        r"^\*+",            # Bullet points
        r"\*\*",            # Bold
        r"_",               # Italics
        r"\[.+\]\(.+\)",    # Links
        r"-\s",             # Dashes used for lists
        r"\`\`\`"           # Code blocks
    ]
    # If any pattern matches, assume the response is in markdown
    if any(re.search(pattern, response, re.MULTILINE) for pattern in markdown_patterns):
        # Markdown detected, convert to HTML for nicer display
        html_output = markdown.markdown(response)
        display(HTML(html_output))  # Use display(HTML()) to render HTML in Colab
    else:
        # No markdown detected, wrap and print as plain text
        wrapper = textwrap.TextWrapper(width=80)
        wrapped_text = wrapper.fill(text=response)
        print("Text Response:")
        print("--------------------")
        print(wrapped_text)
        print("--------------------\n")
print_formatted_response(gpt4_response)
</code></pre>
    <p class="normal">The output is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">Moon Exploration
    Historical Missions:
    1. Apollo Missions: NASA's Apollo program, particularly Apollo 11, marked the first manned Moon landing in 1969. Astronauts like Buzz Aldrin collected core samples and conducted experiments.
    2. Lunar Missions: Various missions have been conducted to explore the Moon, including robotic landers and orbiters from different countries.
Scientific Goals:
    3. Geological Studies: Understanding the Moon's composition, structure, and history.
    4. Resource Utilization: Investigating the potential for mining resources like Helium-3 and water ice.
    Future Plans:
    1. Artemis Program: NASA's initiative to return humans to the Moon and establish a sustainable presence by the late 2020s.
    2. International Collaboration: Partnerships with other space agencies and private companies to build lunar bases and conduct scientific research.
Mars Exploration
    Robotic Missions:
    1. Rovers: NASA's rovers like Curiosity and Perseverance have been exploring Mars' surface, analyzing soil and rock samples, and searching for signs of past life.
    2. Orbiters: Various orbiters have been mapping Mars' surface and studying its atmosphere…
</code></pre>
    <p class="normal">Let’s introduce an evaluation metric to measure the quality of the output.</p>
    <h1 id="_idParaDest-70" class="heading-1">Evaluating the output with cosine similarity</h1>
    <p class="normal">In this section, we will implement cosine similarity to measure the similarity between user input and the generative AI model’s output<a id="_idIndexMarker168"/>. We will also measure the augmented user input with the generative AI model’s output. Let’s first define a cosine similarity function:</p>
    <pre class="programlisting code"><code class="hljs-code">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
def calculate_cosine_similarity(text1, text2):
    vectorizer = TfidfVectorizer()
    tfidf = vectorizer.fit_transform([text1, text2])
    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])
    return similarity[0][0]
</code></pre>
    <p class="normal">Then, let’s calculate a score that measures the similarity between the user prompt and GPT-4’s response:</p>
    <pre class="programlisting code"><code class="hljs-code">similarity_score = calculate_cosine_similarity(user_prompt, gpt4_response)
print(f"Cosine Similarity Score: {similarity_score:.3f}")
</code></pre>
    <p class="normal">The score is low, although the output seemed acceptable for a human:</p>
    <pre class="programlisting con"><code class="hljs-con">Cosine Similarity Score: 0.396
</code></pre>
    <p class="normal">It seems that either we missed something or need to use another metric.</p>
    <p class="normal">Let’s try to calculate the similarity between the augmented input and GPT-4’s response:</p>
    <pre class="programlisting code"><code class="hljs-code"># Example usage with your existing functions
similarity_score = calculate_cosine_similarity(augmented_input, gpt4_response)
print(f"Cosine Similarity Score: {similarity_score:.3f}")
</code></pre>
    <p class="normal">The score seems better:</p>
    <pre class="programlisting con"><code class="hljs-con">Cosine Similarity Score: 0.857
</code></pre>
    <p class="normal">Can we use another method? Cosine similarity, when using <strong class="keyWord">Term Frequency-Inverse Document Frequency</strong> (<strong class="keyWord">TF-IDF</strong>), relies heavily on <a id="_idIndexMarker169"/>exact vocabulary overlap and takes into account important language features, such as semantic meanings, synonyms, or contextual usage. As such, this method may produce lower similarity scores for texts that are conceptually similar but differ in word choice.</p>
    <p class="normal">In contrast, using <a id="_idIndexMarker170"/>Sentence Transformers to calculate similarity involves embeddings that capture deeper semantic relationships between words and phrases. This approach is more effective in recognizing the contextual and conceptual similarity between texts. Let’s try this approach.</p>
    <p class="normal">First, let’s install <code class="inlineCode">sentence-transformers</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install sentence-transformers
</code></pre>
    <p class="normal">Be careful installing this library at the end of the session, since it may induce potential conflicts with the RAG pipeline’s requirements. Depending on a project’s needs, this code could be yet another separate pipeline component.</p>
    <div><p class="normal">As of August 2024, using a Hugging Face token is optional. If Hugging Face requires a token, sign up to Hugging Face to obtain an API token, check the conditions, and set up the key as instructed.</p>
    </div>
    <p class="normal">We will now use a MiniLM architecture to perform the task with <code class="inlineCode">all-MiniLM-L6-v2</code>. This model is available through the Hugging Face Model Hub we are using. It’s part of the <code class="inlineCode">sentence-transformers</code> library, which is an extension of the Hugging Face Transformers library. We are using this architecture because it offers a compact and efficient model, with a strong performance in generating meaningful sentence embeddings quickly. Let’s now implement it<a id="_idIndexMarker171"/> with the following function:</p>
    <pre class="programlisting code"><code class="hljs-code">from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
def calculate_cosine_similarity_with_embeddings(text1, text2):
    embeddings1 = model.encode(text1)
    embeddings2 = model.encode(text2)
    similarity = cosine_similarity([embeddings1], [embeddings2])
    return similarity[0][0]
</code></pre>
    <p class="normal">We can now call the function to calculate the similarity between the augmented user input and GPT-4’s response:</p>
    <pre class="programlisting code"><code class="hljs-code">similarity_score = calculate_cosine_similarity_with_embeddings(augmented_input, gpt4_response)
print(f"Cosine Similarity Score: {similarity_score:.3f}")
</code></pre>
    <p class="normal">The output shows that the Sentence Transformer captures semantic similarities between the texts more effectively, resulting in a high cosine similarity score:</p>
    <pre class="programlisting con"><code class="hljs-con">Cosine Similarity Score: 0.739
</code></pre>
    <p class="normal">The <a id="_idIndexMarker172"/>choice of metrics depends on the specific requirements of each project phase. <em class="chapterRef">Chapter 3</em>, <em class="italic">Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI</em>, will provide advanced metrics when we implement index-based RAG. At this stage, however, the RAG pipeline’s three components have been successfully built. Let’s summarize our journey and move to the next level!</p>
    <h1 id="_idParaDest-71" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we tackled the complexities of using RAG-driven generative AI, focusing on the essential role of document embeddings when handling large datasets. We saw how to go from raw texts to embeddings and store them in vector stores. Vector stores such as Activeloop, unlike parametric generative AI models, provide API tools and visual interfaces that allow us to see embedded text at any moment.</p>
    <p class="normal">A RAG pipeline detailed the organizational process of integrating OpenAI embeddings into Activeloop Deep Lake vector stores. The RAG pipeline was broken down into distinct components that can vary from one project to another. This separation allows multiple teams to work simultaneously without dependency, accelerating development and facilitating specialized focus on individual aspects, such as data collection, embedding processing, and query generation for the augmented generation AI process.</p>
    <p class="normal">We then built a three-component RAG pipeline, beginning by highlighting the necessity of specific cross-platform packages and careful system architecture planning. The resources involved were Python functions built from scratch, Activeloop Deep Lake to organize and store the embeddings in a dataset in a vector store, an OpenAI embedding model, and OpenAI’s GPT-4o generative AI model. The program guided us through building a three-part RAG pipeline using Python, with practical steps that involved setting up the environment, handling dependencies, and addressing implementation challenges like data chunking and vector store integration.</p>
    <p class="normal">This journey provided a robust understanding of embedding documents in vector stores and leveraging them for enhanced generative AI outputs, preparing us to apply these insights to real-world AI applications in well-organized processes and teams within an organization. Vector stores enhance the retrieval of documents that require precision in information retrieval. Indexing takes RAG further and increases the speed and relevance of retrievals. The next chapter will take us a step further by introducing advanced indexing methods to retrieve and augment inputs.</p>
    <h1 id="_idParaDest-72" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with <em class="italic">Yes</em> or <em class="italic">No</em>:</p>
    <ol>
      <li class="numberedList" value="1">Do embeddings convert text into high-dimensional vectors for faster retrieval in RAG? </li>
      <li class="numberedList">Are keyword searches more effective than embeddings in retrieving detailed semantic content? </li>
      <li class="numberedList">Is it recommended to separate RAG pipelines into independent components?</li>
      <li class="numberedList">Does the RAG pipeline consist of only two main components? </li>
      <li class="numberedList">Can Activeloop Deep Lake handle both embedding and vector storage? </li>
      <li class="numberedList">Is the text-embedding-3-small model from OpenAI used to generate embeddings in this chapter? </li>
      <li class="numberedList">Are data embeddings visible and directly traceable in an RAG-driven system? </li>
      <li class="numberedList">Can a RAG pipeline run smoothly without splitting into separate components?</li>
      <li class="numberedList">Is chunking large texts into smaller parts necessary for embedding and storage? </li>
      <li class="numberedList">Are cosine similarity metrics used to evaluate the relevance of retrieved information?</li>
    </ol>
    <h1 id="_idParaDest-73" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">OpenAI Ada documentation for embeddings: <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models">https://platform.openai.com/docs/guides/embeddings/embedding-models</a></li>
      <li class="bulletList">OpenAI GPT documentation for content generation: <a href="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4">https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4</a></li>
      <li class="bulletList">Activeloop API documentation: <a href="https://docs.deeplake.ai/en/latest/">https://docs.deeplake.ai/en/latest/</a></li>
      <li class="bulletList">MiniLM model reference: <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></li>
    </ul>
    <h1 id="_idParaDest-74" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">OpenAI’s documentation on embeddings: <a href="https://platform.openai.com/docs/guides/embeddings">https://platform.openai.com/docs/guides/embeddings</a></li>
      <li class="bulletList">Activeloop documentation: <a href="https://docs.activeloop.ai/">https://docs.activeloop.ai/</a></li>
    </ul>
    <h1 id="_idParaDest-75" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>