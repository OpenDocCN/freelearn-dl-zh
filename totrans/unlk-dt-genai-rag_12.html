<html><head></head><body>
		<div id="_idContainer073">
			<h1 id="_idParaDest-243" class="chapter-number"><a id="_idTextAnchor242"/><st c="0">12</st></h1>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor243"/><st c="3">Combining RAG with the Power of AI Agents and LangGraph</st></h1>
			<p><st c="59">One call to an </st><strong class="bold"><st c="75">large language model</st></strong><st c="95"> (</st><strong class="bold"><st c="97">LLM</st></strong><st c="100">) can be powerful, but put your logic in a loop with a goal toward achieving a more sophisticated task and you can take your </st><strong class="bold"><st c="226">retrieval-augmented generation</st></strong><st c="256"> (</st><strong class="bold"><st c="258">RAG</st></strong><st c="261">) development to a whole new level. </st><st c="298">That is the concept behind </st><strong class="bold"><st c="325">agents</st></strong><st c="331">. The past year of development for LangChain has focused significant energy on improving support for </st><em class="italic"><st c="432">agentic</st></em><st c="439"> workflows, adding functionality that enables more precise control over agent behavior and capabilities. </st><st c="544">Part of this progress has been in the emergence of </st><strong class="bold"><st c="595">LangGraph</st></strong><st c="604">, another relatively new part of LangChain. </st><st c="648">Together, agents and LangGraph pair well as a powerful approach to improving </st><span class="No-Break"><st c="725">RAG applications.</st></span></p>
			<p><st c="742">In this chapter, we will focus on gaining a deeper understanding of the elements of agents that can be utilized in RAG and then tie them back to your RAG efforts, covering topics such as </st><span class="No-Break"><st c="930">the following:</st></span></p>
			<ul>
				<li><st c="944">Fundamentals of AI agents and </st><span class="No-Break"><st c="975">RAG integration</st></span></li>
				<li><st c="990">Graphs, AI agents, </st><span class="No-Break"><st c="1010">and LangGraph</st></span></li>
				<li><st c="1023">Adding a LangGraph retrieval agent to your </st><span class="No-Break"><st c="1067">RAG application</st></span></li>
				<li><st c="1082">Tools </st><span class="No-Break"><st c="1089">and toolkits</st></span></li>
				<li><span class="No-Break"><st c="1101">Agent state</st></span></li>
				<li><st c="1113">Core concepts of </st><span class="No-Break"><st c="1131">graph theory</st></span></li>
			</ul>
			<p><st c="1143">By the end of this chapter, you will have a solid grasp of how AI agents and LangGraph can enhance your RAG applications. </st><st c="1266">In the next section, we will dive into the fundamentals of AI agents and RAG integration, setting the stage for the concepts and code lab </st><span class="No-Break"><st c="1404">that follow.</st></span></p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor244"/><st c="1416">Technical requirements</st></h1>
			<p><st c="1439">The code for this chapter is placed in the following GitHub </st><span class="No-Break"><st c="1500">repository: </st></span><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_12"><span class="No-Break"><st c="1512">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_12</st></span></a></p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor245"/><st c="1609">Fundamentals of AI agents and RAG integration</st></h1>
			<p><st c="1655">When talking with new developers in generative AI, we have been told that the concept of an AI agent often tends to be one of the more challenging concepts to grasp. </st><st c="1822">When experts talk about agents, they often talk about them in very abstract terms, focusing on all the things AI agents can be responsible for in a RAG application, but failing to really explain thoroughly what an AI agent is and how </st><span class="No-Break"><st c="2056">it works.</st></span></p>
			<p><st c="2065">I find that it is easiest to dispel the mystery of the AI</st><a id="_idIndexMarker777"/><st c="2123"> agent by explaining what it really is, which is actually a very simple concept. </st><st c="2204">To build an AI agent in its most basic form, you are simply taking the same LLM concept you have already been working with throughout these chapters and adding a loop that terminates when the intended task is done. </st><st c="2419">That’s it! </st><st c="2430">It’s just a </st><span class="No-Break"><st c="2442">loop folks!</st></span></p>
			<p><span class="No-Break"><em class="italic"><st c="2453">Figure 12</st></em></span><em class="italic"><st c="2463">.1</st></em><st c="2465"> represents the </st><strong class="bold"><st c="2481">RAG agent loop</st></strong><st c="2495"> you will be working</st><a id="_idIndexMarker778"/><st c="2515"> with in the code lab that you are about to </st><span class="No-Break"><st c="2559">dive into:</st></span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B22475_12_01.jpg" alt="Figure 12.1 – Graph of the agent’s control flow"/><st c="2569"/>
				</div>
			</div>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="2598">Figure 12.1 – Graph of the agent’s control flow</st></p>
			<p><st c="2645">This represents a relatively simple set of logic steps that loop through until the agent decides it has successfully completed the task you have given it. </st><st c="2801">The oval</st><a id="_idIndexMarker779"/><st c="2809"> boxes, such as </st><em class="italic"><st c="2825">agent</st></em><st c="2830"> and </st><em class="italic"><st c="2835">retrieve</st></em><st c="2843">, are called </st><strong class="bold"><st c="2856">nodes</st></strong><st c="2861"> and the lines are</st><a id="_idIndexMarker780"/><st c="2879"> called </st><strong class="bold"><st c="2887">edges</st></strong><st c="2892">. The dotted lines are also edges, but they are a specific type called </st><strong class="bold"><st c="2963">conditional edges</st></strong><st c="2980">, which are edges</st><a id="_idIndexMarker781"/><st c="2997"> that are also </st><span class="No-Break"><st c="3012">decision points.</st></span></p>
			<p><st c="3028">Despite the simplicity, the concept of adding a loop to your LLM calls does make it much more powerful than just using LLMs directly, because it takes more advantage of the LLM’s ability to reason and break tasks down into simpler tasks. </st><st c="3267">This improves the chances of success in whatever task you are pursuing and will come in especially handy with more complex multi-step </st><span class="No-Break"><st c="3401">RAG tasks.</st></span></p>
			<p><st c="3411">While your LLM is looping through agent tasks, you also provide functions called </st><em class="italic"><st c="3493">tools</st></em><st c="3498"> to the agent, and the LLM will use its reasoning capabilities to determine which tool to use, how to use that tool, and what data to feed it. </st><st c="3641">This is where it can get really complex very quickly. </st><st c="3695">You can have multiple agents, numerous tools, integrated knowledge graphs that help guide your agents down a specific path, numerous frameworks that offer different </st><em class="italic"><st c="3860">flavors</st></em><st c="3867"> of agents, numerous approaches to agent architecture, and much more. </st><st c="3937">But in this chapter, we are going to focus specifically on how an AI agent can help improve RAG applications. </st><st c="4047">Once you see the power of using an AI agent though, I have no doubt you will want to use it in other generative AI applications, and </st><span class="No-Break"><st c="4180">you should!</st></span></p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor246"/><st c="4191">Living in an AI agent world</st></h2>
			<p><st c="4219">With all the excitement</st><a id="_idIndexMarker782"/><st c="4243"> around agents, you might think LLMs are already going obsolete. </st><st c="4308">But that couldn’t be further from the truth. </st><st c="4353">With AI agents, you are really tapping into an even more powerful version of an LLM, a version where the LLM serves as the “brain” of the agent, letting it reason and come up with multi-step solutions well beyond the one-off chat questions most people are using them for. </st><st c="4625">The agent just provides a layer between the user and the LLM and pushes the LLM to accomplish a task that may take multiple queries of the LLM but will eventually, in theory, end up with a much </st><span class="No-Break"><st c="4819">better result.</st></span></p>
			<p><st c="4833">If you think about it, this matches up more with how problems are solved in the real world, where even simple decisions can be complex. </st><st c="4970">Most tasks we do are based on a long chain of observations, reasoning, and adjustments to new experiences. </st><st c="5077">Very rarely do we interact with people, tasks, and things in the real world in the same way we interact with LLMs online. </st><st c="5199">There is often this building of understanding, knowledge, and context that takes place and helps us find the best solutions. </st><st c="5324">AI agents are better able to handle this type of approach </st><span class="No-Break"><st c="5382">to problem-solving.</st></span></p>
			<p><st c="5401">Agents can make a big difference to your RAG efforts, but what about this concept of the LLMs being their brains? </st><st c="5516">Let’s dive into the </st><span class="No-Break"><st c="5536">concept further.</st></span></p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor247"/><st c="5552">LLMs as the agents’ brains</st></h2>
			<p><st c="5579">If you consider the LLM</st><a id="_idIndexMarker783"/><st c="5603"> as the brain of your AI agent, the next logical step is that you likely want the </st><em class="italic"><st c="5685">smartest</st></em><st c="5693"> LLM you can find to be that brain. </st><st c="5729">The capabilities of the LLM are going to affect your AI agent’s ability to reason and make decisions, which will certainly impact the results of the queries to your </st><span class="No-Break"><st c="5894">RAG application.</st></span></p>
			<p><st c="5910">There is one major</st><a id="_idIndexMarker784"/><st c="5929"> way this metaphor of an LLM brain breaks down though, but in a very good way. </st><st c="6008">Unlike agents in the real world, the AI agent can always swap out their LLM brain for another LLM brain. </st><st c="6113">We could even give it multiple LLM brains that can serve to check each other and make sure things are proceeding as planned. </st><st c="6238">This gives us greater flexibility that will help us continually improve the capabilities of </st><span class="No-Break"><st c="6330">our agents.</st></span></p>
			<p><st c="6341">So, how does LangGraph, or graphs in general, relate to AI agents? </st><st c="6409">We will discuss </st><span class="No-Break"><st c="6425">that next.</st></span></p>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor248"/><st c="6435">Graphs, AI agents, and LangGraph</st></h1>
			<p><st c="6468">LangChain introduced</st><a id="_idIndexMarker785"/><st c="6489"> LangGraph in</st><a id="_idIndexMarker786"/><st c="6502"> 2024, so it is still relatively new. </st><st c="6540">It is an extension built on top of </st><strong class="bold"><st c="6575">LangChain Expression Language</st></strong><st c="6604"> (</st><strong class="bold"><st c="6606">LCEL</st></strong><st c="6610">) to create</st><a id="_idIndexMarker787"/><st c="6622"> composable and customizable agentic workloads. </st><st c="6670">LangGraph leans heavily on graph theory concepts, such as nodes and edges (described earlier), but with a focus on using them to manage your AI agents. </st><st c="6822">While an older way to manage agents, the </st><strong class="source-inline"><st c="6863">AgentExecutor</st></strong><st c="6876"> class, still exists, LangGraph is now the </st><em class="italic"><st c="6919">recommended</st></em><st c="6930"> way to build agents </st><span class="No-Break"><st c="6951">in LangChain.</st></span></p>
			<p><st c="6964">LangGraph adds two important components</st><a id="_idIndexMarker788"/><st c="7004"> for </st><span class="No-Break"><st c="7009">supporting agents:</st></span></p>
			<ul>
				<li><st c="7027">The ability to easily define cycles (</st><span class="No-Break"><st c="7065">cyclical graphs)</st></span></li>
				<li><span class="No-Break"><st c="7082">Built-in memory</st></span></li>
			</ul>
			<p><st c="7098">It provides a pre-built object equivalent to </st><strong class="source-inline"><st c="7144">AgentExecutor</st></strong><st c="7157">, allowing developers to orchestrate agents using a </st><span class="No-Break"><st c="7209">graph-based approach.</st></span></p>
			<p><st c="7230">Over the past couple of years, numerous papers, concepts, and approaches have emerged for building agents into RAG applications, such as orchestration agents, ReAct agents, self-refine agents, and multi-agent frameworks. </st><st c="7452">A common theme among these approaches is the concept of a cyclical graph that represents the agent’s control flow. </st><st c="7567">While many of these approaches, from an implementation standpoint, are going obsolete, their concepts are still highly useful and are captured in the graph-based environment </st><span class="No-Break"><st c="7741">of LangGraph.</st></span></p>
			<p><strong class="bold"><st c="7754">LangGraph</st></strong><st c="7764"> has become</st><a id="_idIndexMarker789"/><st c="7775"> a powerful tool for supporting agents and managing their flow and process in RAG applications. </st><st c="7871">It enables developers to describe and represent single and multi-agent flows as graphs, providing extremely controlled </st><em class="italic"><st c="7990">flows</st></em><st c="7995">. This controllability is crucial for avoiding the pitfalls encountered by developers when creating agents </st><span class="No-Break"><st c="8102">early on.</st></span></p>
			<p><st c="8111">As an example, the popular ReAct approach was an early paradigm for building agents. </st><strong class="bold"><st c="8197">ReAct</st></strong><st c="8202"> stands for </st><strong class="bold"><st c="8214">reason + act</st></strong><st c="8226">. In this pattern, an LLM</st><a id="_idIndexMarker790"/><st c="8251"> first thinks about what to do and then decides an action to take. </st><st c="8318">That action is then executed in an environment and an observation is returned. </st><st c="8397">With that observation, the LLM then repeats this process. </st><st c="8455">It uses reasoning to think about what to do next, decides another action to take, and continues until it has been determined that the goal has been met. </st><st c="8608">If you map this process out, it may look something like what you see here in </st><span class="No-Break"><em class="italic"><st c="8685">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="8694">.2</st></em></span><span class="No-Break"><st c="8696">:</st></span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B22475_12_02.jpg" alt="Figure 12.2 – ReAct cyclical graph representation"/><st c="8698"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="8721">Figure 12.2 – ReAct cyclical graph representation</st></p>
			<p><st c="8770">The set of loops in </st><span class="No-Break"><em class="italic"><st c="8791">Figure 12</st></em></span><em class="italic"><st c="8800">.2</st></em><st c="8802"> can be represented</st><a id="_idIndexMarker791"/><st c="8821"> by cyclical graphs in LangGraph, with each step represented by nodes and edges. </st><st c="8902">Using this graphing paradigm, you can see how a tool such as LangGraph, a tool for building graphs in LangChain, can form the backbone of your agent framework. </st><st c="9062">As we build our agent framework, we can represent these agent loops using LangGraph, which helps you describe and orchestrate the control flow. </st><st c="9206">This focus on the control flow is critical to addressing some of the early challenges with agents, where a lack of control leads to rogue agents that can’t complete their loops or focus on the </st><span class="No-Break"><st c="9399">wrong task.</st></span></p>
			<p><st c="9410">Another key element that LangGraph has built into it is persistence. </st><st c="9480">Persistence can be used to maintain the memory of the agent, giving it the information it needs to reflect on all of its actions so far, and representing the </st><em class="italic"><st c="9638">OBSERVE</st></em><st c="9645"> component presented in </st><span class="No-Break"><em class="italic"><st c="9669">Figure 12</st></em></span><em class="italic"><st c="9678">.2</st></em><st c="9680">. This is really helpful for having multiple conversations at the same time or remembering previous iterations and actions. </st><st c="9804">This persistence also enables human-in-the-loop features that give you better control over what the agent is doing at key intervals during </st><span class="No-Break"><st c="9943">its actions.</st></span></p>
			<p><st c="9955">The paper that introduced the ReAct approach to agent building can be found </st><span class="No-Break"><st c="10032">here: </st></span><a href="https://arxiv.org/abs/2210.03629"><span class="No-Break"><st c="10038">https://arxiv.org/abs/2210.03629</st></span></a></p>
			<p><st c="10070">Let’s dive right into the code lab for building our agent and walk through more key individual concepts as we encounter them in </st><span class="No-Break"><st c="10199">the code.</st></span></p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor249"/><st c="10208">Code lab 12.1 – adding a LangGraph agent to RAG</st></h1>
			<p><st c="10256">In this code lab, we will add an agent</st><a id="_idIndexMarker792"/><st c="10295"> to our existing RAG pipeline that can make decisions about whether to retrieve from an index or use a web search. </st><st c="10410">We will show the inner thoughts of the agent as it processes data that it retrieves toward the goal of providing you with a more thorough response to your question. </st><st c="10575">As we add the code for our agent, we will see new components, such as tools, toolkits, graphs, nodes, edges, and, of course, the agent itself. </st><st c="10718">For each component, we will go more in-depth into how that component interacts and supports your RAG application. </st><st c="10832">We will also add code so that this functions more like a chat session, rather than a </st><span class="No-Break"><st c="10917">Q&amp;A session:</st></span></p>
			<ol>
				<li><st c="10929">First, we will install some new packages to support our </st><span class="No-Break"><st c="10986">agent development:</st></span><pre class="source-code">
<strong class="bold"><st c="11004">%pip install tiktoken</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="11026">%pip install langgraph</st></strong></pre><p class="list-inset"><st c="11049">In the first line, we install the </st><strong class="source-inline"><st c="11084">tiktoken</st></strong><st c="11092"> package, which is an OpenAI package used for tokenizing text data before feeding it into language models. </st><st c="11199">Last, we pull in the </st><strong class="source-inline"><st c="11220">langgraph</st></strong><st c="11229"> package we have </st><span class="No-Break"><st c="11246">been discussing.</st></span></p></li>
				<li><st c="11262">Next, we add a new LLM definition and update our </st><span class="No-Break"><st c="11312">existing one:</st></span><pre class="source-code">
<strong class="bold"><st c="11325">llm = ChatOpenAI(model_name="gpt-4o-mini",</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="11368">    temperature=0, streaming=True)</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="11399">agent_llm = ChatOpenAI(model_name="gpt-4o-mini",</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="11448">    temperature=0, streaming=True)</st></strong></pre></li>
			</ol>
			<p><st c="11479">The new </st><strong class="source-inline"><st c="11488">agent_llm</st></strong><st c="11497"> LLM instance will serve as our agent’s brain, handling reasoning and execution of the agent tasks, whereas the original </st><strong class="source-inline"><st c="11618">llm</st></strong><st c="11621"> instance will still be present in our general LLM to do the same LLM tasks we have used it for in the past. </st><st c="11730">While the two LLMs are defined with the same model and parameters in our example, you could and should experiment with using different LLMs for these different tasks, to see if there is a combination that works better for your RAG applications. </st><st c="11975">You could even add additional LLMs to handle specific tasks, such as the </st><strong class="source-inline"><st c="12048">improve</st></strong><st c="12055"> or </st><strong class="source-inline"><st c="12059">score_documents</st></strong><st c="12074"> functions in this code, if you find an LLM better at those tasks or have trained or fine-tuned your own for these particular actions. </st><st c="12209">For example, It is common for simple tasks to be handled by faster, lower-cost LLMs as long as they can perform the task successfully. </st><st c="12344">There is a lot of flexibility built into this code that you can take advantage of! </st><st c="12427">Also, note that we add </st><strong class="source-inline"><st c="12450">streaming=True</st></strong><st c="12464"> to the LLM definition. </st><st c="12488">This turns on streaming data from the LLM, which is more conducive to an agent that may make several calls, sometimes in parallel, constantly interacting with </st><span class="No-Break"><st c="12647">the LLM.</st></span></p>
			<p><st c="12655">Now, we are going to skip down to after the retriever definitions (</st><strong class="source-inline"><st c="12723">dense_retriever</st></strong><st c="12739">, </st><strong class="source-inline"><st c="12741">sparse_retriever</st></strong><st c="12757">, and </st><strong class="source-inline"><st c="12763">ensemble_retriever</st></strong><st c="12781">) and add our first tool. </st><st c="12808">A </st><strong class="bold"><st c="12810">tool</st></strong><st c="12814"> has a very specific and important meaning when it comes to agents; so, let’s talk about </st><span class="No-Break"><st c="12903">that now.</st></span></p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor250"/><st c="12912">Tools and toolkits</st></h2>
			<p><st c="12931">In the following code, we are going to add</st><a id="_idIndexMarker793"/><st c="12974"> a </st><strong class="bold"><st c="12977">web </st></strong><span class="No-Break"><strong class="bold"><st c="12981">search</st></strong></span><span class="No-Break"><st c="12987"> tool:</st></span></p>
			<pre class="source-code"><st c="12993">
from langchain_community.tools.tavily_search import TavilySearchResults
_ = load_dotenv(dotenv_path='env.txt')
os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')
!export TAVILY_API_KEY=os.environ['TAVILY_API_KEY']
web_search = TavilySearchResults(max_results=4)
web_search_name = web_search.name</st></pre>
			<p><st c="13297">You will need to get another API key and add it to the </st><strong class="source-inline"><st c="13353">env.txt</st></strong><st c="13360"> file we have used in the past for the OpenAI and Together APIs. </st><st c="13425">Just like with those APIs, you will need to go to that website, set up your API key, and then copy that into your </st><strong class="source-inline"><st c="13539">env.txt</st></strong><st c="13546"> file. </st><st c="13553">The Tavily website can be found at this </st><span class="No-Break"><st c="13593">URL: </st></span><a href="https://tavily.com/"><span class="No-Break"><st c="13598">https://tavily.com/</st></span></a></p>
			<p><st c="13617">We run</st><a id="_idIndexMarker794"/><st c="13624"> the code again that loads the data from the </st><strong class="source-inline"><st c="13669">env.txt</st></strong><st c="13676"> file and then we set up the </st><strong class="source-inline"><st c="13705">TavilySearchResults</st></strong><st c="13724"> object with </st><strong class="source-inline"><st c="13737">max_results</st></strong><st c="13748"> of </st><strong class="source-inline"><st c="13752">4</st></strong><st c="13753">, meaning when we run it for search, we only want four search results maximum. </st><st c="13832">We then assign the </st><strong class="source-inline"><st c="13851">web_search.name</st></strong><st c="13866"> variable to a variable called </st><strong class="source-inline"><st c="13897">web_search_name</st></strong><st c="13912"> so that we have that available later when we want to tell the agent about it. </st><st c="13991">You can run this tool directly using </st><span class="No-Break"><st c="14028">this code:</st></span></p>
			<pre class="source-code"><st c="14038">
web_search.invoke(user_query)</st></pre>
			<p><st c="14068">Running this tool code with </st><strong class="source-inline"><st c="14097">user_query</st></strong><st c="14107"> will give you a result like this (truncated </st><span class="No-Break"><st c="14152">for brevity):</st></span></p>
			<pre class="source-code"><st c="14165">
[{'url': 'http://sustainability.google/',
  'content': "Google Maps\nChoose the most fuel-efficient route\nGoogle Shopping\nShop for more efficient appliances for your home\nGoogle Flights\nFind a flight with lower per-traveler carbon emissions\nGoogle Nest\...[TRUNCATED HERE]"},
…
  'content': "2023 Environmental Report. </st><st c="14486">Google's 2023 Environmental Report outlines how we're driving positive environmental outcomes throughout our business in three key ways: developing products and technology that empower individuals on their journey to a more sustainable life, working together with partners and organizations everywhere to transition to resilient, low-carbon systems, and operating ..."}]</st></pre>
			<p><st c="14856">We truncated this so we take up less space in the book, but try this in the code and you will see four results, as we asked for, and they all seem to be highly related to the topic </st><strong class="source-inline"><st c="15038">user_query</st></strong><st c="15048"> is asking about. </st><st c="15066">Note that you will not need to run this tool directly in your code like we </st><span class="No-Break"><st c="15141">just did.</st></span></p>
			<p><st c="15150">At this point, you have just established your first agent tool! </st><st c="15215">This is a search engine tool that your agent can use to retrieve more information from the internet to help it achieve its goal of answering the question your user poses </st><span class="No-Break"><st c="15385">to it.</st></span></p>
			<p><st c="15391">The </st><em class="italic"><st c="15396">tool</st></em><st c="15400"> concept in LangChain</st><a id="_idIndexMarker795"/><st c="15421"> and when building agents comes from the idea that you want to make actions available to your agent so that it can carry out its tasks. </st><st c="15557">Tools are the mechanism that allows this to happen. </st><st c="15609">You define a tool like we just did for the web search, and then you later add it to a list of tools that the agent can use to accomplish its tasks. </st><st c="15757">Before we set up that list though, we want to create another tool that is central for a RAG application: a </st><span class="No-Break"><st c="15864">retriever tool:</st></span></p>
			<pre class="source-code"><st c="15879">
from langchain.tools.retriever import create_retriever_tool
retriever_tool = create_retriever_tool(
    ensemble_retriever,
    "retrieve_google_environmental_question_answers",
    "Extensive information about Google environmental
     efforts from 2023.",
)
retriever_tool_name = retriever_tool.name</st></pre>
			<p><st c="16164">Note that with the web search</st><a id="_idIndexMarker796"/><st c="16194"> tool, we imported it from </st><strong class="source-inline"><st c="16221">langchain_community.tools.tavily_search</st></strong><st c="16260">, whereas with this tool, we use </st><strong class="source-inline"><st c="16293">langchain.tools.retriever</st></strong><st c="16318">. This reflects the fact that Tavily is a third-party tool, whereas the retriever tool we create here is part of the core LangChain functionality. </st><st c="16465">After importing the </st><strong class="source-inline"><st c="16485">create_retriever_tool</st></strong><st c="16506"> function, we use it to create the </st><strong class="source-inline"><st c="16541">retriever_tool</st></strong><st c="16555"> tool for our agent. </st><st c="16576">Again, like with </st><strong class="source-inline"><st c="16593">web_search_name</st></strong><st c="16608">, we pull out the </st><strong class="source-inline"><st c="16626">retriever_tool.name</st></strong><st c="16645"> variable we can reference later when we want to refer to it for the agent. </st><st c="16721">You may notice the name of the actual retriever this tool will use, the </st><strong class="source-inline"><st c="16793">ensemble_retriever</st></strong><st c="16811"> retriever, which we created in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="16843">Chapter 8</st></em></span></a><st c="16852">’s </st><em class="italic"><st c="16856">8.3 </st></em><span class="No-Break"><em class="italic"><st c="16860">code lab</st></em></span><span class="No-Break"><st c="16868">!</st></span></p>
			<p><st c="16869">You should also note that the name that we are giving this tool, as far as the agent is concerned, is found in the second field, and we are calling it </st><strong class="source-inline"><st c="17020">retrieve_google_environmental_question_answers</st></strong><st c="17066">. When we name variables in code, we normally try to keep them smaller, but for tools that agents will use, it is helpful to provide more verbose names that will help the agent understand what can be </st><span class="No-Break"><st c="17266">used fully.</st></span></p>
			<p><st c="17277">We now have two tools for our agent! </st><st c="17315">However, we still need to tell the agent about them eventually; so, we package them up into a list that we can later share with </st><span class="No-Break"><st c="17443">the agent:</st></span></p>
			<pre class="source-code"><st c="17453">
tools = [web_search, retriever_tool]</st></pre>
			<p><st c="17490">You see here the two tools we created previously, the </st><strong class="source-inline"><st c="17545">web_search</st></strong><st c="17555"> tool and the </st><strong class="source-inline"><st c="17569">retriever_tool</st></strong><st c="17583"> tool, getting added to the tools list. </st><st c="17623">If we had other tools we wanted to make available to the agent, we could add those to the list as well. </st><st c="17727">In the LangChain</st><a id="_idIndexMarker797"/><st c="17743"> ecosystem, there are hundreds of tools </st><span class="No-Break"><st c="17783">available: </st></span><a href="https://python.langchain.com/v0.2/docs/integrations/tools/"><span class="No-Break"><st c="17794">https://python.langchain.com/v0.2/docs/integrations/tools/</st></span></a></p>
			<p><st c="17852">You will want to make sure the LLM you are using is “good” at reasoning and using tools. </st><st c="17942">In general, chat models tend to have been fine-tuned for tool calling and will be better at using tools. </st><st c="18047">Non-chat-fine-tuned models may not be able to use tools, especially if the tools are complex or require multiple calls. </st><st c="18167">Using well-written names and descriptions can play an important role in setting your agent LLM up for success </st><span class="No-Break"><st c="18277">as well.</st></span></p>
			<p><st c="18285">In the agent we are building, we have all the tools we need, but you will also want to look at the toolkits, which are convenient groups of tools. </st><st c="18433">LangChain provides a list of the current toolkits</st><a id="_idIndexMarker798"/><st c="18482"> available on their </st><span class="No-Break"><st c="18502">website: </st></span><a href="https://python.langchain.com/v0.2/docs/integrations/toolkits/"><span class="No-Break"><st c="18511">https://python.langchain.com/v0.2/docs/integrations/toolkits/</st></span></a></p>
			<p><st c="18572">For example, if you have a data infrastructure that uses pandas DataFrames, you could use the pandas DataFrame toolkit to offer your agent various tools to access those DataFrames in different ways. </st><st c="18772">Drawing straight from the LangChain website, toolkits are described as </st><span class="No-Break"><st c="18843">follows: (</st></span><a href="https://python.langchain.com/v0.1/docs/modules/agents/concepts/#toolkits"><span class="No-Break"><st c="18853">https://python.langchain.com/v0.1/docs/modules/agents/concepts/#toolkits</st></span></a><span class="No-Break"><st c="18926">)</st></span></p>
			<p class="author-quote"><st c="18928">For many common tasks, an agent will need a set of related tools. </st><st c="18994">For this LangChain provides the concept of toolkits - groups of around 3-5 tools needed to accomplish specific objectives. </st><st c="19117">For example, the GitHub toolkit has a tool for searching through GitHub issues, a tool for reading a file, a tool for commenting, etc.</st></p>
			<p><st c="19251">So, basically, if you are focusing on a set of common tasks for your agent or a popular integration partner with LangChain (such as a Salesforce integration), there is likely a toolkit that will give you access to all the tools you need all </st><span class="No-Break"><st c="19493">at once.</st></span></p>
			<p><st c="19501">Now that we have the tools established, let’s start building the components of our agent, starting with the </st><span class="No-Break"><st c="19610">agent state.</st></span></p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor251"/><st c="19622">Agent state</st></h2>
			<p><st c="19634">The </st><strong class="bold"><st c="19639">agent state</st></strong><st c="19650"> is a key component</st><a id="_idIndexMarker799"/><st c="19669"> of any agent you build with LangGraph. </st><st c="19709">Using LangGraph, you create an </st><strong class="source-inline"><st c="19740">AgentState</st></strong><st c="19750"> class that establishes the “state” for your agent and tracks it over time. </st><st c="19826">This state is a local mechanism to the agent that you make available to all parts of the graph and can be stored in a </st><span class="No-Break"><st c="19944">persistence layer.</st></span></p>
			<p><st c="19962">Here, we set up this state for our </st><span class="No-Break"><st c="19998">RAG agent:</st></span></p>
			<pre class="source-code"><st c="20008">
from typing import Annotated, Literal, Sequence, TypedDict
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage],
                        add_messages]</st></pre>
			<p><st c="20250">This imports relevant packages for setting up </st><strong class="source-inline"><st c="20297">AgentState</st></strong><st c="20307">. For example, </st><strong class="source-inline"><st c="20322">BaseMessage</st></strong><st c="20333"> is a base class for representing messages in the conversation between the user and the AI agent. </st><st c="20431">It will be used to define the structure and properties of messages in the state of the conversation. </st><st c="20532">It then defines a graph and a </st><strong class="source-inline"><st c="20562">"state"</st></strong><st c="20569"> object that it passes around to each node. </st><st c="20613">You can set the state to be a variety of types of objects that you can store different types of data, but for our RAG agent, we set up our state to be a list </st><span class="No-Break"><st c="20771">of </st></span><span class="No-Break"><strong class="source-inline"><st c="20774">"messages"</st></strong></span><span class="No-Break"><st c="20784">.</st></span></p>
			<p><st c="20785">We then need to import another round of packages to set up other parts of </st><span class="No-Break"><st c="20860">our agent:</st></span></p>
			<pre class="source-code"><st c="20870">
from langchain_core.messages import HumanMessage
from langchain_core.pydantic_v1 import BaseModel, Field
from langgraph.prebuilt import tools_condition</st></pre>
			<p><st c="21022">In this code, we start with importing </st><strong class="source-inline"><st c="21061">HumanMessage</st></strong><st c="21073">. </st><strong class="source-inline"><st c="21075">HumanMessage</st></strong><st c="21087"> is a specific type of message that represents a message sent by the human user. </st><st c="21168">It will used when constructing the prompt for the agent to generate a response. </st><st c="21248">We also import </st><strong class="source-inline"><st c="21263">BaseModel</st></strong><st c="21272"> and </st><strong class="source-inline"><st c="21277">Field</st></strong><st c="21282">. </st><strong class="source-inline"><st c="21284">BaseModel</st></strong><st c="21293"> is a class from the </st><strong class="source-inline"><st c="21314">Pydantic</st></strong><st c="21322"> library that is used to define data models and validate data. </st><strong class="source-inline"><st c="21385">Field</st></strong><st c="21390"> is a class from </st><strong class="source-inline"><st c="21407">Pydantic</st></strong><st c="21415"> that is used to define the properties and validation rules for fields in a data model. </st><st c="21503">Last, we import </st><strong class="source-inline"><st c="21519">tools_condition</st></strong><st c="21534">. The </st><strong class="source-inline"><st c="21540">tools_condition</st></strong><st c="21555"> function is a pre-built function provided by the </st><strong class="source-inline"><st c="21605">LangGraph</st></strong><st c="21614"> library. </st><st c="21624">It is used to assess the agent’s decision on whether to use specific tools based on the current state of </st><span class="No-Break"><st c="21729">the conversation.</st></span></p>
			<p><st c="21746">These imported classes and </st><a id="_idIndexMarker800"/><st c="21774">functions are used throughout the code to define the structure of messages, validate data, and control the flow of the conversation based on the agent’s decisions. </st><st c="21938">They provide the necessary building blocks and utilities for constructing the language model application using the </st><span class="No-Break"><strong class="source-inline"><st c="22053">LangGraph</st></strong></span><span class="No-Break"><st c="22062"> library.</st></span></p>
			<p><st c="22071">We then define our primary prompt (representing what the user would input) </st><span class="No-Break"><st c="22147">like this:</st></span></p>
			<pre class="source-code"><st c="22157">
generation_prompt = PromptTemplate.from_template(
    """You are an assistant for question-answering tasks.
    </st><st c="22262">Use the following pieces of retrieved context to answer
    the question. </st><st c="22332">If you don't know the answer, just say
    that you don't know. </st><st c="22392">Provide a thorough description to
    fully answer the question, utilizing any relevant
    information you find.
    </st><st c="22498">Question: {question}
    Context: {context}
    Answer:"""
)</st></pre>
			<p><st c="22550">This is a replacement for the code that we were using in the past </st><span class="No-Break"><st c="22617">code labs:</st></span></p>
			<pre class="console"><st c="22627">
prompt = hub.pull("jclemens24/rag-prompt")</st></pre>
			<p><st c="22670">We alter the name to </st><strong class="source-inline"><st c="22692">generation_prompt</st></strong><st c="22709"> to make this prompt’s use </st><span class="No-Break"><st c="22736">more clear.</st></span></p>
			<p><st c="22747">Our graph usage is about to pick up in our code, but first, we need to cover some basic graph </st><span class="No-Break"><st c="22842">theory concepts.</st></span></p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor252"/><st c="22858">Core concepts of graph theory</st></h1>
			<p><st c="22888">To better understand how we are going to use LangGraph in the next few blocks of code, it is helpful to review some key concepts</st><a id="_idIndexMarker801"/><st c="23017"> in </st><strong class="bold"><st c="23021">graph theory</st></strong><st c="23033">. </st><strong class="bold"><st c="23035">Graphs</st></strong><st c="23041"> are mathematical</st><a id="_idIndexMarker802"/><st c="23058"> structures that can be used to represent relationships between different objects. </st><st c="23141">The objects are</st><a id="_idIndexMarker803"/><st c="23156"> called </st><strong class="bold"><st c="23164">nodes</st></strong><st c="23169"> and the relationships between them, typically drawn</st><a id="_idIndexMarker804"/><st c="23221"> with a line, are called </st><strong class="bold"><st c="23246">edges</st></strong><st c="23251">. You have already seen these concepts in </st><span class="No-Break"><em class="italic"><st c="23293">Figure 12</st></em></span><em class="italic"><st c="23302">.1</st></em><st c="23304">, but it is important to understand how they relate to any graph and how that is used </st><span class="No-Break"><st c="23390">in LangGraph.</st></span></p>
			<p><st c="23403">With LangGraph, there are also specific types of edges representing different types of these relationships. </st><st c="23512">The “conditional edge” that we mentioned along with </st><span class="No-Break"><em class="italic"><st c="23564">Figure 12</st></em></span><em class="italic"><st c="23573">.1</st></em><st c="23575">, for example, represents when you need to make a decision about which node you should go to next; so, they represent the decisions. </st><st c="23708">When talking about the ReAct paradigm, this has also been called</st><a id="_idIndexMarker805"/><st c="23772"> the </st><strong class="bold"><st c="23777">action edge</st></strong><st c="23788">, as it is where the action takes place, relating to the </st><em class="italic"><st c="23845">reason + action</st></em><st c="23860"> approach of ReAct. </st><span class="No-Break"><em class="italic"><st c="23880">Figure 12</st></em></span><em class="italic"><st c="23889">.3</st></em><st c="23891"> shows a basic graph consisting of nodes </st><span class="No-Break"><st c="23932">and edges:</st></span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B22475_12_03.jpg" alt="Figure 12.3 – Basic graph representing our RAG application"/><st c="23942"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="23955">Figure 12.3 – Basic graph representing our RAG application</st></p>
			<p><st c="24013">In this cyclical graph shown in </st><span class="No-Break"><em class="italic"><st c="24046">Figure 12</st></em></span><em class="italic"><st c="24055">.3</st></em><st c="24057">, you see nodes representing the start, agent, retrieve tool, generation, observation, and end. </st><st c="24153">The key edges are where the LLM makes the decision of what tool to use (retrieve is the only one available here), observes if what is retrieved is sufficient, and then pushes to generation. </st><st c="24343">If it is decided that the retrieved data is not sufficient, there is an edge that sends the observation back to the agent to decide if it wants to try again. </st><st c="24501">These decision points are the </st><em class="italic"><st c="24531">conditional edges</st></em> <span class="No-Break"><st c="24548">we discussed.</st></span></p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor253"/><st c="24562">Nodes and edges in our agent</st></h1>
			<p><st c="24591">OK, so let’s</st><a id="_idIndexMarker806"/><st c="24604"> review. </st><st c="24613">We’ve mentioned that an agentic RAG graph has three key components: the </st><em class="italic"><st c="24685">state</st></em><st c="24690"> that we already talked about, the </st><em class="italic"><st c="24725">nodes</st></em><st c="24730"> that append to or update the </st><a id="_idIndexMarker807"/><st c="24760">state, and the </st><em class="italic"><st c="24775">conditional edges</st></em><st c="24792"> that decide which node to visit next. </st><st c="24831">We are now to the point where we can step through each of these in code blocks, seeing how the three components interact with </st><span class="No-Break"><st c="24957">each other.</st></span></p>
			<p><st c="24968">Given this background, the first thing we will add to the code is the conditional edge, where the decisions are made. </st><st c="25087">In this case, we are going to define an edge that determines if the retrieved documents are relevant to the question. </st><st c="25205">This is the function that will decide whether to move on to the generation stage or to go back and </st><span class="No-Break"><st c="25304">try again:</st></span></p>
			<ol>
				<li><st c="25314">We will step through this code in multiple steps, but keep in mind that this is one large function, starting with </st><span class="No-Break"><st c="25429">the definition:</st></span><pre class="source-code"><st c="25444">
def score_documents(state) -&gt; Literal[</st></pre><pre class="source-code"><st c="25483">
    "generate", "improve"]:</st></pre><p class="list-inset"><st c="25507">This code starts by defining a function called </st><strong class="source-inline"><st c="25555">score_documents</st></strong><st c="25570"> that determines whether the retrieved documents are relevant to the given question. </st><st c="25655">The function takes the state we’ve been discussing as a parameter, which is a set of messages that have been collected. </st><st c="25775">This is how we make the state </st><strong class="source-inline"><st c="25805">available</st></strong><st c="25814"> to this conditional </st><span class="No-Break"><st c="25835">edge function.</st></span></p></li>
				<li><st c="25849">Now, we build the </st><span class="No-Break"><st c="25868">data model:</st></span><pre class="source-code"><st c="25879">
    class scoring(BaseModel):</st></pre><pre class="source-code"><st c="25905">
        binary_score: str = Field(</st></pre><pre class="source-code"><st c="25932">
          description="Relevance score 'yes' or 'no'")</st></pre><p class="list-inset"><st c="25977">This defines a data model class called </st><strong class="source-inline"><st c="26017">scoring</st></strong><st c="26024"> using </st><strong class="source-inline"><st c="26031">Pydantic</st></strong><st c="26039">’s </st><strong class="source-inline"><st c="26043">BaseModel</st></strong><st c="26052">. The </st><strong class="source-inline"><st c="26058">scoring</st></strong><st c="26065"> class has a single field called </st><strong class="source-inline"><st c="26098">binary_score</st></strong><st c="26110">, which is a string representing the relevance score as either </st><strong class="source-inline"><st c="26173">yes</st></strong> <span class="No-Break"><st c="26176">or </st></span><span class="No-Break"><strong class="source-inline"><st c="26180">no</st></strong></span><span class="No-Break"><st c="26182">.</st></span></p></li>
				<li><st c="26183">Next, we add the LLM that will make </st><span class="No-Break"><st c="26220">this decision:</st></span><pre class="source-code"><st c="26234">
    llm_with_tool = llm.with_structured_output(</st></pre><pre class="source-code"><st c="26278">
        scoring)</st></pre><p class="list-inset"><st c="26287">This creates an instance of </st><strong class="source-inline"><st c="26316">llm_with_tool</st></strong><st c="26329"> by calling </st><strong class="source-inline"><st c="26341">llm.with_structured_output(scoring)</st></strong><st c="26376">, combining the LLM with the scoring data model for structured </st><span class="No-Break"><st c="26439">output validation.</st></span></p></li>
				<li><st c="26457">As we have</st><a id="_idIndexMarker808"/><st c="26468"> seen in the past, we need to set up a </st><strong class="source-inline"><st c="26507">PromptTemplate</st></strong><st c="26521"> class to pass to the </st><a id="_idIndexMarker809"/><st c="26543">LLM. </st><st c="26548">Here is </st><span class="No-Break"><st c="26556">that prompt:</st></span><pre class="source-code"><st c="26568">
    prompt = PromptTemplate(</st></pre><pre class="source-code"><st c="26593">
        template="""You are assessing relevance of a retrieved document to a user question with a binary grade. </st><st c="26698">Here is the retrieved document:</st></pre><pre class="source-code"><st c="26729">
{context}</st></pre><pre class="source-code"><st c="26739">
Here is the user question: {question}</st></pre><pre class="source-code"><st c="26777">
If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. </st><st c="26886">Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.""",</st></pre><pre class="source-code"><st c="26991">
        input_variables=["context", "question"],</st></pre><pre class="source-code"><st c="27032">
    )</st></pre><p class="list-inset"><st c="27034">This defines a prompt using the </st><strong class="source-inline"><st c="27066">PromptTemplate</st></strong><st c="27080"> class, providing instructions to the LLM for applying a binary score for the relevance of the retrieved document based on the </st><span class="No-Break"><st c="27207">given question.</st></span></p></li>
				<li><st c="27222">We can then use LCEL to build the chain that combines the prompt with the </st><strong class="source-inline"><st c="27297">llm_with_tool</st></strong><st c="27310"> tool we just </st><span class="No-Break"><st c="27324">set up:</st></span><pre class="source-code"><st c="27331">
    chain = prompt | llm_with_tool</st></pre><p class="list-inset"><st c="27362">This chain represents the pipeline for scoring the documents. </st><st c="27425">This defines the chain, but we haven’t invoked </st><span class="No-Break"><st c="27472">it yet.</st></span></p></li>
				<li><st c="27479">First, we want to </st><a id="_idIndexMarker810"/><st c="27498">pull in the state. </st><st c="27517">Next, we pull the state (</st><strong class="source-inline"><st c="27542">"messages"</st></strong><st c="27553">) into the</st><a id="_idIndexMarker811"/><st c="27564"> function so that we can use it, and we take the </st><span class="No-Break"><st c="27613">last message:</st></span><pre class="source-code"><st c="27626">
    messages = state["messages"]</st></pre><pre class="source-code"><st c="27655">
    last_message = messages[-1]</st></pre><pre class="source-code"><st c="27683">
    question = messages[0].content</st></pre><pre class="source-code"><st c="27714">
    docs = last_message.content</st></pre><p class="list-inset"><st c="27742">This extracts the necessary information from the </st><strong class="source-inline"><st c="27792">"state"</st></strong><st c="27799"> parameter and then preps the state/message as the context we are going to pass to our agent brain (LLM). </st><st c="27905">The specific components extracted here include </st><span class="No-Break"><st c="27952">the following:</st></span></p><ul><li><strong class="source-inline"><st c="27966">messages</st></strong><st c="27975">: The list of messages in </st><span class="No-Break"><st c="28002">the conversation</st></span></li><li><strong class="source-inline"><st c="28018">last_message</st></strong><st c="28031">: The last message in </st><span class="No-Break"><st c="28054">the conversation</st></span></li><li><strong class="source-inline"><st c="28070">question</st></strong><st c="28079">: The content of the first message, which is assumed to be the </st><span class="No-Break"><st c="28143">user’s question</st></span></li><li><strong class="source-inline"><st c="28158">docs</st></strong><st c="28163">: The content of the last message, which is assumed to be the </st><span class="No-Break"><st c="28226">retrieved documents</st></span></li></ul><p class="list-inset"><st c="28245">Then, finally, we invoke the chain with the prompt filled (if you remember, we call this </st><strong class="bold"><st c="28335">hydrating</st></strong><st c="28344"> the</st><a id="_idIndexMarker812"/><st c="28348"> prompt) with </st><strong class="source-inline"><st c="28362">question</st></strong><st c="28370"> and context </st><strong class="source-inline"><st c="28383">docs</st></strong><st c="28387"> to get the </st><span class="No-Break"><st c="28399">scored result:</st></span></p><pre class="source-code"><st c="28413">
    scored_result = chain.invoke({"question":</st></pre><pre class="source-code"><st c="28455">
        question, "context": docs})</st></pre><pre class="source-code"><st c="28483">
    score = scored_result.binary_score</st></pre><p class="list-inset"><st c="28518">This extracts</st><a id="_idIndexMarker813"/><st c="28532"> the </st><strong class="source-inline"><st c="28537">binary_score</st></strong><st c="28549"> variable from the </st><strong class="source-inline"><st c="28568">scored_result</st></strong><st c="28581"> object and assigns it to the </st><strong class="source-inline"><st c="28611">score</st></strong><st c="28616"> variable. </st><st c="28627">The </st><strong class="source-inline"><st c="28631">llm_with_tool</st></strong><st c="28644"> step, which</st><a id="_idIndexMarker814"/><st c="28656"> is the last step in the LangChain chain, aptly called </st><strong class="source-inline"><st c="28711">chain</st></strong><st c="28716">, is going to return a string-based binary result based on the response from the </st><span class="No-Break"><st c="28797">scoring function:</st></span></p><pre class="source-code"><st c="28814">
    if score == "yes":</st></pre><pre class="source-code"><st c="28833">
        print("---DECISION: DOCS RELEVANT---")</st></pre><pre class="source-code"><st c="28872">
        return "generate"</st></pre><pre class="source-code"><st c="28890">
    else:</st></pre><pre class="source-code"><st c="28896">
        print("---DECISION: DOCS NOT RELEVANT---")</st></pre><pre class="source-code"><st c="28939">
        print(score)</st></pre><pre class="source-code"><st c="28952">
        return "improve"</st></pre><p class="list-inset"><st c="28969">This checks the value of the score. </st><st c="29006">If the </st><strong class="source-inline"><st c="29013">score</st></strong><st c="29018"> value is </st><strong class="source-inline"><st c="29028">yes</st></strong><st c="29031">, it prints a message indicating that the documents are relevant and returns </st><strong class="source-inline"><st c="29108">generate</st></strong><st c="29116"> as the final output from the </st><strong class="source-inline"><st c="29146">score_documents</st></strong><st c="29161"> function, suggesting that the next step is to generate a response. </st><st c="29229">If the </st><strong class="source-inline"><st c="29236">score</st></strong><st c="29241"> value is </st><strong class="source-inline"><st c="29251">no</st></strong><st c="29253">, or, technically, anything other than </st><strong class="source-inline"><st c="29292">yes</st></strong><st c="29295">, it prints messages indicating that the documents are not relevant and returns </st><strong class="source-inline"><st c="29375">improve</st></strong><st c="29382">, suggesting that the next step is to improve the query from </st><span class="No-Break"><st c="29443">the user.</st></span></p><p class="list-inset"><st c="29452">Overall, this function acts as a decision point in the workflow, determining whether the retrieved documents are relevant to the question and directing the flow to either generate a response or rewrite the question based on the </st><span class="No-Break"><st c="29681">relevance score.</st></span></p></li>
				<li><st c="29697">Now that we</st><a id="_idIndexMarker815"/><st c="29709"> have our conditional edge defined, we are going to move on to </st><a id="_idIndexMarker816"/><st c="29772">defining our nodes, starting with </st><span class="No-Break"><st c="29806">the agent:</st></span><pre class="source-code"><st c="29816">
def agent(state):</st></pre><pre class="source-code"><st c="29834">
    print("---CALL AGENT---")</st></pre><pre class="source-code"><st c="29860">
    messages = state["messages"]</st></pre><pre class="source-code"><st c="29889">
    llm = llm.bind_tools(tools)</st></pre><pre class="source-code"><st c="29917">
    response = llm.invoke(messages)</st></pre><pre class="source-code"><st c="29949">
    return {"messages": [response]}</st></pre><p class="list-inset"><st c="29981">This function represents the agent node on our graph and invokes the agent model to generate a response based on the current state. </st><st c="30114">The </st><strong class="source-inline"><st c="30118">agent</st></strong><st c="30123"> function takes the current state (</st><strong class="source-inline"><st c="30158">"state"</st></strong><st c="30166">) as input, which contains the messages in the conversation, prints a message indicating that it is calling the agent, extracts the messages from the state dictionary, uses the </st><strong class="source-inline"><st c="30344">agent_llm</st></strong><st c="30353"> instance of the </st><strong class="source-inline"><st c="30370">ChatOpenAI</st></strong><st c="30380"> class we defined earlier, representing the agent </st><em class="italic"><st c="30430">brain</st></em><st c="30435">, and then binds the tools to the model using the </st><strong class="source-inline"><st c="30485">bind_tools</st></strong><st c="30495"> method. </st><st c="30504">We then invoke the agent’s </st><strong class="source-inline"><st c="30531">llm</st></strong><st c="30534"> instance with the messages and assign the result to the </st><span class="No-Break"><strong class="source-inline"><st c="30591">response</st></strong></span><span class="No-Break"><st c="30599"> variable.</st></span></p></li>
				<li><st c="30609">Our next node, </st><strong class="source-inline"><st c="30625">improve</st></strong><st c="30632">, is responsible for transforming </st><strong class="source-inline"><st c="30666">user_query</st></strong><st c="30676"> to produce a better question if the agent determines this </st><span class="No-Break"><st c="30735">is needed:</st></span><pre class="source-code"><st c="30745">
def improve(state):</st></pre><pre class="source-code"><st c="30765">
    print("---TRANSFORM QUERY---")</st></pre><pre class="source-code"><st c="30796">
    messages = state["messages"]</st></pre><pre class="source-code"><st c="30825">
    question = messages[0].content</st></pre><pre class="source-code"><st c="30856">
    msg = [</st></pre><pre class="source-code"><st c="30864">
        HumanMessage(content=f"""\n</st></pre><pre class="source-code"><st c="30892">
            Look at the input and try to reason about</st></pre><pre class="source-code"><st c="30934">
            the underlying semantic intent / meaning.</st></pre><pre class="source-code"><st c="30976">
            \n</st></pre><pre class="source-code"><st c="30979">
            Here is the initial question:</st></pre><pre class="source-code"><st c="31009">
            \n ------- \n</st></pre><pre class="source-code"><st c="31023">
            {question}</st></pre><pre class="source-code"><st c="31034">
            \n ------- \n</st></pre><pre class="source-code"><st c="31048">
            Formulate an improved question:</st></pre><pre class="source-code"><st c="31080">
            """,</st></pre><pre class="source-code"><st c="31085">
        )</st></pre><pre class="source-code"><st c="31087">
    ]</st></pre><pre class="source-code"><st c="31089">
    response = llm.invoke(msg)</st></pre><pre class="source-code"><st c="31116">
    return {"messages": [response]}</st></pre><p class="list-inset"><st c="31148">This function, like all </st><a id="_idIndexMarker817"/><st c="31173">of our node and edge-related functions, takes the</st><a id="_idIndexMarker818"/><st c="31222"> current state (</st><strong class="source-inline"><st c="31238">"state"</st></strong><st c="31246">) as input. </st><st c="31259">The function returns a dictionary with the response appended to the messages list. </st><st c="31342">The function prints a message indicating that it is transforming the query, extracts the messages from the state dictionary, retrieves the content of the first message (</st><strong class="source-inline"><st c="31511">messages[0].content</st></strong><st c="31531">), which is assumed to be the initial question, and assigns it to the </st><strong class="source-inline"><st c="31602">question</st></strong><st c="31610"> variable. </st><st c="31621">We then set up a message using the </st><strong class="source-inline"><st c="31656">HumanMessage</st></strong><st c="31668"> class, indicating that we want the </st><strong class="source-inline"><st c="31704">llm</st></strong><st c="31707"> instance to reason about the underlying semantic intent of the question and formulate an improved question. </st><st c="31816">The result from the </st><strong class="source-inline"><st c="31836">llm</st></strong><st c="31839"> instance is assigned to the </st><strong class="source-inline"><st c="31868">response</st></strong><st c="31876"> variable. </st><st c="31887">Finally, it returns a dictionary with the response appended to the </st><span class="No-Break"><st c="31954">messages list.</st></span></p></li>
				<li><st c="31968">Our next node function is the </st><span class="No-Break"><strong class="source-inline"><st c="31999">generate</st></strong></span><span class="No-Break"><st c="32007"> function:</st></span><pre class="source-code"><st c="32017">
def generate(state):</st></pre><pre class="source-code"><st c="32038">
    print("---GENERATE---")</st></pre><pre class="source-code"><st c="32062">
    messages = state["messages"]</st></pre><pre class="source-code"><st c="32091">
    question = messages[0].content</st></pre><pre class="source-code"><st c="32122">
    last_message = messages[-1]</st></pre><pre class="source-code"><st c="32150">
    question = messages[0].content</st></pre><pre class="source-code"><st c="32181">
    docs = last_message.content</st></pre><pre class="source-code"><st c="32209">
    rag_chain = generation_prompt | llm |</st></pre><pre class="source-code"><st c="32247">
        str_output_parser</st></pre><pre class="source-code"><st c="32265">
    response = rag_chain.invoke({"context": docs,</st></pre><pre class="source-code"><st c="32311">
        "question": question})</st></pre><pre class="source-code"><st c="32334">
    return {"messages": [response]}</st></pre><p class="list-inset"><st c="32366">This function is </st><a id="_idIndexMarker819"/><st c="32384">similar to our generation step in the previous chapter’s code </st><a id="_idIndexMarker820"/><st c="32446">labs but simplified to provide just the response. </st><st c="32496">It generates an answer based on the retrieved documents and the question. </st><st c="32570">The function takes the current state (</st><strong class="source-inline"><st c="32608">"state"</st></strong><st c="32616">) as input, which contains the messages in the conversation, prints a message indicating that it is generating an answer, extracts the messages from the state dictionary, retrieves the content of the first message (</st><strong class="source-inline"><st c="32832">messages[0].content</st></strong><st c="32852">), which is assumed to be the question, and assigns it to the </st><span class="No-Break"><strong class="source-inline"><st c="32915">question</st></strong></span><span class="No-Break"><st c="32923"> variable.</st></span></p><p class="list-inset"><st c="32933">The function then retrieves the last message (</st><strong class="source-inline"><st c="32980">messages[-1]</st></strong><st c="32993">) and assigns it to the </st><strong class="source-inline"><st c="33018">last_message</st></strong><st c="33030"> variable. </st><st c="33041">The </st><strong class="source-inline"><st c="33045">docs</st></strong><st c="33049"> variable is assigned the content of </st><strong class="source-inline"><st c="33086">last_message</st></strong><st c="33098">, which is assumed to be the retrieved documents. </st><st c="33148">At this point, we create a chain called </st><strong class="source-inline"><st c="33188">rag_chain</st></strong><st c="33197"> by combining the </st><strong class="source-inline"><st c="33215">generation_prompt</st></strong><st c="33232">, </st><strong class="source-inline"><st c="33234">llm</st></strong><st c="33237">, and </st><strong class="source-inline"><st c="33243">str_output_parser</st></strong><st c="33260"> variables using the </st><strong class="source-inline"><st c="33281">|</st></strong><st c="33282"> operator. </st><st c="33293">As with other LLM prompting, we hydrate the predefined </st><strong class="source-inline"><st c="33348">generation_prompt</st></strong><st c="33365"> as the prompt for generating the answer, which returns a</st><a id="_idIndexMarker821"/><st c="33422"> dictionary </st><a id="_idIndexMarker822"/><st c="33434">with the </st><strong class="source-inline"><st c="33443">response</st></strong><st c="33451"> variable appended to the </st><span class="No-Break"><strong class="source-inline"><st c="33477">messages</st></strong></span><span class="No-Break"><st c="33485"> list.</st></span></p></li>
			</ol>
			<p><st c="33491">Next, we want to set up our cyclical graphs using LangGraph and assign our nodes and edges </st><span class="No-Break"><st c="33583">to them.</st></span></p>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor254"/><st c="33591">Cyclical graph setup</st></h1>
			<p><st c="33612">The next </st><a id="_idIndexMarker823"/><st c="33622">big step in our code </st><a id="_idIndexMarker824"/><st c="33643">is setting up our graphs </st><span class="No-Break"><st c="33668">using LangGraph:</st></span></p>
			<ol>
				<li><st c="33684">First, we import some important packages to get </st><span class="No-Break"><st c="33733">us started:</st></span><pre class="source-code"><st c="33744">
from langgraph.graph import END, StateGraph</st></pre><pre class="source-code"><st c="33788">
from langgraph.prebuilt import ToolNode</st></pre><p class="list-inset"><st c="33828">This code imports the following necessary classes and functions from the </st><span class="No-Break"><strong class="source-inline"><st c="33902">langgraph</st></strong></span><span class="No-Break"><st c="33911"> library:</st></span></p><ul><li><strong class="source-inline"><st c="33920">END</st></strong><st c="33924">: A special node representing the end of </st><span class="No-Break"><st c="33966">the workflow</st></span></li><li><strong class="source-inline"><st c="33978">StateGraph</st></strong><st c="33989">: A class for defining the state graph of </st><span class="No-Break"><st c="34032">the workflow</st></span></li><li><strong class="source-inline"><st c="34044">ToolNode</st></strong><st c="34053">: A class for defining a node that represents a tool </st><span class="No-Break"><st c="34107">or action</st></span></li></ul></li>
				<li><st c="34116">We then pass </st><strong class="source-inline"><st c="34130">AgentState</st></strong><st c="34140"> as an argument to the </st><strong class="source-inline"><st c="34163">StateGraph</st></strong><st c="34173"> class we just imported for defining the state graph of </st><span class="No-Break"><st c="34229">the workflow:</st></span><pre class="source-code"><st c="34242">
workflow = StateGraph(AgentState)</st></pre><p class="list-inset"><st c="34276">This creates a new instance of </st><strong class="source-inline"><st c="34308">StateGraph</st></strong><st c="34318"> called </st><strong class="source-inline"><st c="34326">workflow</st></strong><st c="34334"> and defines a new graph for that </st><strong class="source-inline"><st c="34368">workflow</st></strong> <span class="No-Break"><strong class="source-inline"><st c="34376">StateGraph</st></strong></span><span class="No-Break"><st c="34387"> instance.</st></span></p></li>
				<li><st c="34397">Next, we define the nodes we will cycle between and assign our node functions </st><span class="No-Break"><st c="34476">to them:</st></span><pre class="source-code"><st c="34484">
workflow.add_node("agent", agent)  # agent</st></pre><pre class="source-code"><st c="34526">
retrieve = ToolNode(tools)</st></pre><pre class="source-code"><st c="34553">
workflow.add_node("retrieve", retrieve)</st></pre><pre class="source-code"><st c="34593">
# retrieval from web and or retriever</st></pre><pre class="source-code"><st c="34631">
workflow.add_node("improve", improve)</st></pre><pre class="source-code"><st c="34669">
 # Improving the question for better retrieval</st></pre><pre class="source-code"><st c="34715">
workflow.add_node("generate", generate)  # Generating a response after we know the documents are relevant</st></pre><p class="list-inset"><st c="34820">This code adds </st><a id="_idIndexMarker825"/><st c="34836">multiple nodes to the </st><strong class="source-inline"><st c="34858">workflow</st></strong><st c="34866"> instance </st><a id="_idIndexMarker826"/><st c="34876">using the </st><span class="No-Break"><strong class="source-inline"><st c="34886">add_node</st></strong></span><span class="No-Break"><st c="34894"> method:</st></span></p><ul><li><strong class="source-inline"><st c="34902">"agent"</st></strong><st c="34910">: This node represents the agent node, which invokes the </st><span class="No-Break"><st c="34968">agent function.</st></span></li><li><strong class="source-inline"><st c="34983">"retrieve"</st></strong><st c="34994">: This node represents the retrieval node, which is a special </st><strong class="source-inline"><st c="35057">ToolNode</st></strong><st c="35065"> containing the tools list we defined early with the </st><strong class="source-inline"><st c="35118">web_search</st></strong><st c="35128"> and </st><strong class="source-inline"><st c="35133">retriever_tool</st></strong><st c="35147"> tools. </st><st c="35155">In this code, to aid in readability, we explicitly break out the </st><strong class="source-inline"><st c="35220">ToolNode</st></strong><st c="35228"> class instance and define the </st><strong class="source-inline"><st c="35259">retrieve</st></strong><st c="35267"> variable with it, which indicates the “retrieve” focus of this node more explicitly. </st><st c="35353">We then pass that </st><strong class="source-inline"><st c="35371">retrieve</st></strong><st c="35379"> variable into the </st><span class="No-Break"><strong class="source-inline"><st c="35398">add_node</st></strong></span><span class="No-Break"><st c="35406"> function.</st></span></li><li><strong class="source-inline"><st c="35416">"improve"</st></strong><st c="35426">: This node represents the node for improving the question, which invokes the </st><span class="No-Break"><strong class="source-inline"><st c="35505">improve</st></strong></span><span class="No-Break"><st c="35512"> function.</st></span></li><li><strong class="source-inline"><st c="35522">"generate"</st></strong><st c="35533">: This node represents the node for generating a response, which invokes the </st><span class="No-Break"><strong class="source-inline"><st c="35611">generate</st></strong></span><span class="No-Break"><st c="35619"> function.</st></span></li></ul></li>
				<li><st c="35629">Next, we need to define our starting point for </st><span class="No-Break"><st c="35677">our workflow:</st></span><pre class="source-code"><st c="35690">
workflow.set_entry_point("agent")</st></pre><p class="list-inset"><st c="35724">This sets the entry point of the </st><strong class="source-inline"><st c="35758">workflow</st></strong><st c="35766"> instance to the </st><strong class="source-inline"><st c="35783">"agent"</st></strong><st c="35790"> node </st><span class="No-Break"><st c="35796">using </st></span><span class="No-Break"><strong class="source-inline"><st c="35802">workflow.set_entry_point("agent")</st></strong></span><span class="No-Break"><st c="35835">.</st></span></p></li>
				<li><st c="35836">Next, we call the </st><strong class="source-inline"><st c="35855">"agent"</st></strong><st c="35862"> node to decide whether to retrieve </st><span class="No-Break"><st c="35898">or not:</st></span><pre class="source-code"><st c="35905">
workflow.add_conditional_edges("agent", tools_condition,</st></pre><pre class="source-code"><st c="35962">
    {</st></pre><pre class="source-code"><st c="35964">
        "tools": "retrieve",</st></pre><pre class="source-code"><st c="35985">
        END: END,</st></pre><pre class="source-code"><st c="35995">
    },</st></pre><pre class="source-code"><st c="35998">
)</st></pre><p class="list-inset"><st c="36000">In this code, </st><strong class="source-inline"><st c="36014">tools_condition</st></strong><st c="36029"> is used as a conditional edge in the workflow graph. </st><st c="36083">It determines whether the agent should proceed to the retrieval step (</st><strong class="source-inline"><st c="36153">"tools": "retrieve"</st></strong><st c="36173">) or end the conversation (</st><strong class="source-inline"><st c="36201">END: END</st></strong><st c="36210">) based on the agent’s decision. </st><st c="36244">The retrieval step represents both of the tools that we made available for the agent to use where needed, and the other option, to end the conversation simply ends </st><span class="No-Break"><st c="36408">the workflow.</st></span></p></li>
				<li><st c="36421">Here, we add more </st><a id="_idIndexMarker827"/><st c="36440">edges, which are used </st><a id="_idIndexMarker828"/><st c="36462">after the </st><strong class="source-inline"><st c="36472">"action"</st></strong><st c="36480"> node </st><span class="No-Break"><st c="36486">is called:</st></span><pre class="source-code"><st c="36496">
workflow.add_conditional_edges("retrieve",</st></pre><pre class="source-code"><st c="36539">
    score_documents)</st></pre><pre class="source-code"><st c="36556">
workflow.add_edge("generate", END)</st></pre><pre class="source-code"><st c="36591">
workflow.add_edge("improve", "agent")</st></pre><p class="list-inset"><st c="36629">After the </st><strong class="source-inline"><st c="36640">"retrieve"</st></strong><st c="36650"> node is called, it adds conditional edges using </st><strong class="source-inline"><st c="36699">workflow.add_conditional_edges("retrieve", score_documents)</st></strong><st c="36758">. This assesses the retrieved documents using the </st><strong class="source-inline"><st c="36808">score_documents</st></strong><st c="36823"> function and determines the next node based on the score. </st><st c="36882">This also adds an edge from the </st><strong class="source-inline"><st c="36914">"generate"</st></strong><st c="36924"> node to the </st><strong class="source-inline"><st c="36937">END</st></strong><st c="36940"> node using </st><strong class="source-inline"><st c="36952">workflow.add_edge("generate", END)</st></strong><st c="36986">. This indicates that, after generating a response, the workflow ends. </st><st c="37057">Last, this adds an edge from the </st><strong class="source-inline"><st c="37090">"improve"</st></strong><st c="37099"> node back to the </st><strong class="source-inline"><st c="37117">"agent"</st></strong><st c="37124"> node using </st><strong class="source-inline"><st c="37136">workflow.add_edge("improve", "agent")</st></strong><st c="37173">. This creates a loop where the improved question is sent back to the agent for </st><span class="No-Break"><st c="37253">further processing.</st></span></p></li>
				<li><st c="37272">We are now ready to compile </st><span class="No-Break"><st c="37301">the graph:</st></span><pre class="source-code"><st c="37311">
graph = workflow.compile()</st></pre><p class="list-inset"><st c="37338">This line compiles the workflow graph using </st><strong class="source-inline"><st c="37383">workflow.compile</st></strong><st c="37399"> and assigns the compiled graph to the </st><strong class="source-inline"><st c="37438">graph</st></strong><st c="37443"> variable, which now represents a compiled version of the </st><strong class="source-inline"><st c="37501">StateGraph</st></strong><st c="37511"> graph instance we </st><span class="No-Break"><st c="37530">started with.</st></span></p></li>
				<li><st c="37543">We have already shown you the visualization of what this graph looks like earlier in this chapter in </st><span class="No-Break"><em class="italic"><st c="37645">Figure 12</st></em></span><em class="italic"><st c="37654">.1</st></em><st c="37656">, but if you want to run the visualization yourself, you </st><a id="_idIndexMarker829"/><st c="37713">can use </st><a id="_idIndexMarker830"/><span class="No-Break"><st c="37721">this code:</st></span><pre class="source-code"><st c="37731">
from IPython.display import Image, display</st></pre><pre class="source-code"><st c="37774">
try:</st></pre><pre class="source-code"><st c="37779">
    display(Image(graph.get_graph(</st></pre><pre class="source-code"><st c="37810">
        xray=True).draw_mermaid_png()))</st></pre><pre class="source-code"><st c="37842">
except:</st></pre><pre class="source-code"><st c="37850">
    pass</st></pre><p class="list-inset"><st c="37855">We can use </st><strong class="source-inline"><st c="37867">IPython</st></strong><st c="37874"> to generate </st><span class="No-Break"><st c="37887">this visualization.</st></span></p></li>
				<li><st c="37906">Last, we are going to finally put our agent </st><span class="No-Break"><st c="37951">to work:</st></span><pre class="source-code"><st c="37959">
import pprint</st></pre><pre class="source-code"><st c="37973">
inputs = {</st></pre><pre class="source-code"><st c="37984">
    "messages": [</st></pre><pre class="source-code"><st c="37998">
        ("user", user_query),</st></pre><pre class="source-code"><st c="38020">
    ]</st></pre><pre class="source-code"><st c="38022">
}</st></pre><p class="list-inset"><st c="38024">This imports the </st><strong class="source-inline"><st c="38041">pprint</st></strong><st c="38047"> module, which provides a pretty-print function for formatting and printing data structures, allowing us to see a more human-readable version of our agent output. </st><st c="38210">We then define a dictionary called </st><strong class="source-inline"><st c="38245">inputs</st></strong><st c="38251"> that represents the initial input to the workflow graph. </st><st c="38309">The inputs dictionary contains a </st><strong class="source-inline"><st c="38342">"messages"</st></strong><st c="38352"> key with a list of tuples. </st><st c="38380">In this case, it has a single tuple, </st><strong class="source-inline"><st c="38417">("user", user_query)</st></strong><st c="38437">, where the </st><strong class="source-inline"><st c="38449">"user"</st></strong><st c="38455"> string represents the role of the message sender (</st><strong class="source-inline"><st c="38506">user</st></strong><st c="38511">) and </st><strong class="source-inline"><st c="38518">user_query</st></strong><st c="38528"> is the user’s query </st><span class="No-Break"><st c="38549">or question.</st></span></p></li>
				<li><st c="38561">We then initialize an empty string variable called </st><strong class="source-inline"><st c="38613">final_answer</st></strong><st c="38625"> to store the final answer generated by </st><span class="No-Break"><st c="38665">the workflow:</st></span><pre class="source-code"><st c="38678">
final_answer = ''</st></pre></li>
				<li><st c="38696">We then start our agent </st><a id="_idIndexMarker831"/><st c="38721">loop using the graph instance </st><a id="_idIndexMarker832"/><st c="38751">as </st><span class="No-Break"><st c="38754">the basis:</st></span><pre class="source-code"><st c="38764">
for output in graph.stream(inputs):</st></pre><pre class="source-code"><st c="38800">
    for key, value in output.items():</st></pre><pre class="source-code"><st c="38834">
        pprint.pprint(f"Output from node '{key}':")</st></pre><pre class="source-code"><st c="38878">
        pprint.pprint("---")</st></pre><pre class="source-code"><st c="38899">
        pprint.pprint(value, indent=2, width=80,</st></pre><pre class="source-code"><st c="38940">
            depth=None)</st></pre><pre class="source-code"><st c="38952">
        final_answer = value</st></pre><p class="list-inset"><st c="38973">This starts a double loop using the output in </st><strong class="source-inline"><st c="39020">graph.stream(inputs)</st></strong><st c="39040">. This iterates over the outputs generated by the </st><strong class="source-inline"><st c="39090">graph</st></strong><st c="39095"> instance as it processes the inputs. </st><st c="39133">The </st><strong class="source-inline"><st c="39137">graph.stream(inputs)</st></strong><st c="39157"> method streams the outputs from the </st><strong class="source-inline"><st c="39194">graph</st></strong> <span class="No-Break"><st c="39199">instance execution.</st></span></p><p class="list-inset"><st c="39219">Inside the outer loop, it starts another loop for two variables, </st><strong class="source-inline"><st c="39285">key</st></strong><st c="39288"> and </st><strong class="source-inline"><st c="39293">value</st></strong><st c="39298">, representing the key-value pairs in the </st><strong class="source-inline"><st c="39340">output.items</st></strong><st c="39352"> variable. </st><st c="39363">This iterates over each of those key-value pairs, where the </st><strong class="source-inline"><st c="39423">key</st></strong><st c="39426"> variable represents the node name and the </st><strong class="source-inline"><st c="39469">value</st></strong><st c="39474"> variable represents the output generated by that node. </st><st c="39530">This will print the node name using </st><strong class="source-inline"><st c="39566">pprint.pprint(f"Output from node '{key}':")</st></strong><st c="39609"> to indicate which node generated </st><span class="No-Break"><st c="39643">the output.</st></span></p><p class="list-inset"><st c="39654">The code pretty-prints the value (output) using </st><strong class="source-inline"><st c="39703">pprint.pprint(value, indent=2, width=80, depth=None)</st></strong><st c="39755">. The </st><strong class="source-inline"><st c="39761">indent</st></strong><st c="39767"> parameter specifies the indentation level, </st><strong class="source-inline"><st c="39811">width</st></strong><st c="39816"> specifies the maximum width of the output, and </st><strong class="source-inline"><st c="39864">depth</st></strong><st c="39869"> specifies the maximum depth of nested data structures to print (</st><strong class="source-inline"><st c="39934">None</st></strong><st c="39939"> means </st><span class="No-Break"><st c="39946">no limit).</st></span></p><p class="list-inset"><st c="39956">It assigns the value (output) to the </st><strong class="source-inline"><st c="39994">final_answer</st></strong><st c="40006"> variable, overwriting it in each iteration. </st><st c="40051">After the loop ends, </st><strong class="source-inline"><st c="40072">final_answer</st></strong><st c="40084"> will contain the output generated by the last node in </st><span class="No-Break"><st c="40139">the workflow.</st></span></p><p class="list-inset"><st c="40152">A nice </st><a id="_idIndexMarker833"/><st c="40160">feature of this code is that it allows you to see the</st><a id="_idIndexMarker834"/><st c="40213"> intermediate outputs generated by each node in the graph and track the progress of the query processing. </st><st c="40319">These print outputs represent the agent’s “thoughts” as it makes decisions within the loop. </st><st c="40411">The pretty-printing helps in formatting the outputs for </st><span class="No-Break"><st c="40467">better readability.</st></span></p><p class="list-inset"><st c="40486">When we start the agent and start seeing the output, we can see that a lot is </st><span class="No-Break"><st c="40565">going on!</st></span></p><p class="list-inset"><st c="40574">I will truncate a lot of the printout, but this will give you an idea of what </st><span class="No-Break"><st c="40653">is provided:</st></span></p><pre class="source-code"><st c="40665">
---CALL AGENT---</st></pre><pre class="source-code"><st c="40682">
"Output from node 'agent':"</st></pre><pre class="source-code"><st c="40710">
'---'</st></pre><pre class="source-code"><st c="40716">
{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_46NqZuz3gN2F9IR5jq0MRdVm', 'function': {'arguments': '{"query":"Google\'s environmental initiatives"}', 'name': 'retrieve_google_environmental_question_answers'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-eba27f1e-1c32-4ffc-a161-55a32d645498-0', tool_calls=[{'name': 'retrieve_google_environmental_question_answers', 'args': {'query': "Google's environmental initiatives"}, 'id': 'call_46NqZuz3gN2F9IR5jq0MRdVm'}])]}</st></pre><pre class="source-code"><st c="41270">
'\n---\n'</st></pre><p class="list-inset"><st c="41280">This is the first part of our printout. </st><st c="41321">Here, we see the agent is deciding to use the </st><strong class="source-inline"><st c="41367">retrieve_google_environmental_question_answers</st></strong><st c="41413"> tool. </st><st c="41420">If you will recall, that is the text-based name we gave to the retriever tool when defining it. </st><span class="No-Break"><st c="41516">Good choice!</st></span></p></li>
				<li><st c="41528">Next, the agent is going to determine if it thinks the documents retrieved </st><span class="No-Break"><st c="41604">are relevant:</st></span><pre class="source-code"><st c="41617">
---CHECK RELEVANCE---</st></pre><pre class="source-code"><st c="41639">
---DECISION: DOCS RELEVANT---</st></pre><p class="list-inset"><st c="41669">The decision is that they are. </st><st c="41701">Again, smart thinking, </st><span class="No-Break"><st c="41724">Mr. </st><st c="41728">Agent.</st></span></p></li>
				<li><st c="41734">Last, we see the</st><a id="_idIndexMarker835"/><st c="41751"> output of what the agent is looking at, retrieved from</st><a id="_idIndexMarker836"/><st c="41806"> the PDF document and the ensemble retriever we have been using (there was a lot of retrieved data here, so I truncated most of the </st><span class="No-Break"><st c="41938">actual content):</st></span><pre class="source-code"><st c="41954">
"Output from node 'retrieve':"</st></pre><pre class="source-code"><st c="41985">
'---'</st></pre><pre class="source-code"><st c="41991">
{ 'messages': [ ToolMessage(content='iMasons Climate AccordGoogle is a founding member and part of the governing body of the iMasons Climate Accord, a coalition united on carbon reduction in digital infrastructure.\nReFEDIn 2022, to activate industry-wide change…[TRUNCATED]', tool_call_id='call_46NqZuz3gN2F9IR5jq0MRdVm')]}</st></pre><pre class="source-code"><st c="42316">
'\n---\n'</st></pre><p class="list-inset"><st c="42326">When you look at the actual printout for this portion, you see that the retrieved data is concatenated together and provides substantial and in-depth data for our agent to </st><span class="No-Break"><st c="42499">work with.</st></span></p></li>
				<li><st c="42509">At this point, just like our original RAG application was doing, the agent takes the question, retrieved data, and formulates a response based on the generation </st><a id="_idIndexMarker837"/><st c="42671">prompt we </st><a id="_idIndexMarker838"/><span class="No-Break"><st c="42681">gave it:</st></span><pre class="source-code"><st c="42689">
---GENERATE---</st></pre><pre class="source-code"><st c="42704">
"Output from node 'generate':"</st></pre><pre class="source-code"><st c="42735">
'---'</st></pre><pre class="source-code"><st c="42741">
{ 'messages': [ 'Google has a comprehensive and multifaceted approach to '</st></pre><pre class="source-code"><st c="42816">
'environmental sustainability, encompassing various '</st></pre><pre class="source-code"><st c="42870">
'initiatives aimed at reducing carbon emissions, promoting'</st></pre><pre class="source-code"><st c="42930">
'sustainable practices, and leveraging technology for '</st></pre><pre class="source-code"><st c="42986">
"environmental benefits. </st><st c="43012">Here are some key aspects of Google's "</st></pre><pre class="source-code"><st c="43051">
'environmental initiatives:\n''\n'</st></pre><pre class="source-code"><st c="43086">
'1. </st><st c="43091">**Carbon Reduction and Renewable Energy**…']}</st></pre><pre class="source-code"><st c="43136">
'\n---\n'</st></pre><p class="list-inset"><st c="43146">We included a mechanism here to print out the final message separately </st><span class="No-Break"><st c="43218">for readability:</st></span></p><pre class="source-code"><st c="43234">
final_answer['messages'][0]</st></pre><p class="list-inset"><st c="43262">This will print </st><span class="No-Break"><st c="43279">this out:</st></span></p><pre class="source-code">
<strong class="bold"><st c="43288">"Google has a comprehensive and multifaceted approach to environmental sustainability, encompassing various initiatives aimed at reducing carbon emissions, promoting sustainable practices, and leveraging technology for environmental benefits. </st><st c="43532">Here are some key aspects of Google's environmental initiatives:\n\n1. </st><st c="43603">**Carbon Reduction and Renewable Energy**:\n   - **iMasons Climate Accord**: Google is a founding member and part of the governing body of this coalition focused on reducing carbon emissions in digital infrastructure.\n   - **Net-Zero Carbon**: Google is committed to operating sustainably with a focus on achieving net-zero carbon emissions. </st><st c="43942">This includes investments in carbon-free energy and energy-efficient facilities, such as their all-electric, net water-positive Bay View campus..."</st></strong></pre></li>
			</ol>
			<p><st c="44089">That is the full output of </st><span class="No-Break"><st c="44117">our agent!</st></span></p>
			<h1 id="_idParaDest-256"><a id="_idTextAnchor255"/><st c="44127">Summary</st></h1>
			<p><st c="44135">In this chapter, we explored how AI agents and LangGraph can be combined to create more powerful and sophisticated RAG applications. </st><st c="44269">We learned that an AI agent is essentially an LLM with a loop that allows it to reason and break tasks down into simpler steps, improving the chances of success in complex RAG tasks. </st><st c="44452">LangGraph, an extension built on top of LCEL, provides support for building composable and customizable agentic workloads, enabling developers to orchestrate agents using a </st><span class="No-Break"><st c="44625">graph-based approach.</st></span></p>
			<p><st c="44646">We dove into the fundamentals of AI agents and RAG integration, discussing the concept of tools that agents can use to carry out tasks, and how LangGraph’s </st><strong class="source-inline"><st c="44803">AgentState</st></strong><st c="44813"> class tracks the state of the agent over time. </st><st c="44861">We also covered the core concepts of graph theory, including nodes, edges, and conditional edges, which are crucial for understanding how </st><span class="No-Break"><st c="44999">LangGraph works.</st></span></p>
			<p><st c="45015">In the code lab, we built a LangGraph retrieval agent for our RAG application, demonstrating how to create tools, define the agent state, set up prompts, and establish the cyclical graphs using LangGraph. </st><st c="45221">We saw how the agent uses its reasoning capabilities to determine which tools to use, how to use them, and what data to feed them, ultimately providing a more thorough response to the </st><span class="No-Break"><st c="45405">user’s question.</st></span></p>
			<p><st c="45421">Looking ahead, the next chapter will focus on how prompt engineering can be used to improve </st><span class="No-Break"><st c="45514">RAG applications.</st></span></p>
		</div>
	<div id="charCountTotal" value="45531"/></body></html>