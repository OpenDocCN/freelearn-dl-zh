<html><head></head><body>
		<div><h1 id="_idParaDest-243" class="chapter-number"><a id="_idTextAnchor242"/><st c="0">12</st></h1>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor243"/><st c="3">Combining RAG with the Power of AI Agents and LangGraph</st></h1>
			<p><st c="59">One call to an </st><strong class="bold"><st c="75">large language model</st></strong><st c="95"> (</st><strong class="bold"><st c="97">LLM</st></strong><st c="100">) can be powerful, but put your logic in a loop with a goal toward achieving a more sophisticated task and you can take your </st><strong class="bold"><st c="226">retrieval-augmented generation</st></strong><st c="256"> (</st><strong class="bold"><st c="258">RAG</st></strong><st c="261">) development to a whole new level. </st><st c="298">That is the concept behind </st><strong class="bold"><st c="325">agents</st></strong><st c="331">. The past year of development for LangChain has focused significant energy on improving support for </st><em class="italic"><st c="432">agentic</st></em><st c="439"> workflows, adding functionality that enables more precise control over agent behavior and capabilities. </st><st c="544">Part of this progress has been in the emergence of </st><strong class="bold"><st c="595">LangGraph</st></strong><st c="604">, another relatively new part of LangChain. </st><st c="648">Together, agents and LangGraph pair well as a powerful approach to improving </st><st c="725">RAG applications.</st></p>
			<p><st c="742">In this chapter, we will focus on gaining a deeper understanding of the elements of agents that can be utilized in RAG and then tie them back to your RAG efforts, covering topics such as </st><st c="930">the following:</st></p>
			<ul>
				<li><st c="944">Fundamentals of AI agents and </st><st c="975">RAG integration</st></li>
				<li><st c="990">Graphs, AI agents, </st><st c="1010">and LangGraph</st></li>
				<li><st c="1023">Adding a LangGraph retrieval agent to your </st><st c="1067">RAG application</st></li>
				<li><st c="1082">Tools </st><st c="1089">and toolkits</st></li>
				<li><st c="1101">Agent state</st></li>
				<li><st c="1113">Core concepts of </st><st c="1131">graph theory</st></li>
			</ul>
			<p><st c="1143">By the end of this chapter, you will have a solid grasp of how AI agents and LangGraph can enhance your RAG applications. </st><st c="1266">In the next section, we will dive into the fundamentals of AI agents and RAG integration, setting the stage for the concepts and code lab </st><st c="1404">that follow.</st></p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor244"/><st c="1416">Technical requirements</st></h1>
			<p><st c="1439">The code for this chapter is placed in the following GitHub </st><st c="1500">repository: </st><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_12"><st c="1512">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_12</st></a></p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor245"/><st c="1609">Fundamentals of AI agents and RAG integration</st></h1>
			<p><st c="1655">When talking with new developers in generative AI, we have been told that the concept of an AI agent often tends to be one of the more challenging concepts to grasp. </st><st c="1822">When experts talk about agents, they often talk about them in very abstract terms, focusing on all the things AI agents can be responsible for in a RAG application, but failing to really explain thoroughly what an AI agent is and how </st><st c="2056">it works.</st></p>
			<p><st c="2065">I find that it is easiest to dispel the mystery of the AI</st><a id="_idIndexMarker777"/><st c="2123"> agent by explaining what it really is, which is actually a very simple concept. </st><st c="2204">To build an AI agent in its most basic form, you are simply taking the same LLM concept you have already been working with throughout these chapters and adding a loop that terminates when the intended task is done. </st><st c="2419">That’s it! </st><st c="2430">It’s just a </st><st c="2442">loop folks!</st></p>
			<p><em class="italic"><st c="2453">Figure 12</st></em><em class="italic"><st c="2463">.1</st></em><st c="2465"> represents the </st><strong class="bold"><st c="2481">RAG agent loop</st></strong><st c="2495"> you will be working</st><a id="_idIndexMarker778"/><st c="2515"> with in the code lab that you are about to </st><st c="2559">dive into:</st></p>
			<div><div><img src="img/B22475_12_01.jpg" alt="Figure 12.1 – Graph of the agent’s control flow"/><st c="2569"/>
				</div>
			</div>
			<div><div></div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="2598">Figure 12.1 – Graph of the agent’s control flow</st></p>
			<p><st c="2645">This represents a relatively simple set of logic steps that loop through until the agent decides it has successfully completed the task you have given it. </st><st c="2801">The oval</st><a id="_idIndexMarker779"/><st c="2809"> boxes, such as </st><em class="italic"><st c="2825">agent</st></em><st c="2830"> and </st><em class="italic"><st c="2835">retrieve</st></em><st c="2843">, are called </st><strong class="bold"><st c="2856">nodes</st></strong><st c="2861"> and the lines are</st><a id="_idIndexMarker780"/><st c="2879"> called </st><strong class="bold"><st c="2887">edges</st></strong><st c="2892">. The dotted lines are also edges, but they are a specific type called </st><strong class="bold"><st c="2963">conditional edges</st></strong><st c="2980">, which are edges</st><a id="_idIndexMarker781"/><st c="2997"> that are also </st><st c="3012">decision points.</st></p>
			<p><st c="3028">Despite the simplicity, the concept of adding a loop to your LLM calls does make it much more powerful than just using LLMs directly, because it takes more advantage of the LLM’s ability to reason and break tasks down into simpler tasks. </st><st c="3267">This improves the chances of success in whatever task you are pursuing and will come in especially handy with more complex multi-step </st><st c="3401">RAG tasks.</st></p>
			<p><st c="3411">While your LLM is looping through agent tasks, you also provide functions called </st><em class="italic"><st c="3493">tools</st></em><st c="3498"> to the agent, and the LLM will use its reasoning capabilities to determine which tool to use, how to use that tool, and what data to feed it. </st><st c="3641">This is where it can get really complex very quickly. </st><st c="3695">You can have multiple agents, numerous tools, integrated knowledge graphs that help guide your agents down a specific path, numerous frameworks that offer different </st><em class="italic"><st c="3860">flavors</st></em><st c="3867"> of agents, numerous approaches to agent architecture, and much more. </st><st c="3937">But in this chapter, we are going to focus specifically on how an AI agent can help improve RAG applications. </st><st c="4047">Once you see the power of using an AI agent though, I have no doubt you will want to use it in other generative AI applications, and </st><st c="4180">you should!</st></p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor246"/><st c="4191">Living in an AI agent world</st></h2>
			<p><st c="4219">With all the excitement</st><a id="_idIndexMarker782"/><st c="4243"> around agents, you might think LLMs are already going obsolete. </st><st c="4308">But that couldn’t be further from the truth. </st><st c="4353">With AI agents, you are really tapping into an even more powerful version of an LLM, a version where the LLM serves as the “brain” of the agent, letting it reason and come up with multi-step solutions well beyond the one-off chat questions most people are using them for. </st><st c="4625">The agent just provides a layer between the user and the LLM and pushes the LLM to accomplish a task that may take multiple queries of the LLM but will eventually, in theory, end up with a much </st><st c="4819">better result.</st></p>
			<p><st c="4833">If you think about it, this matches up more with how problems are solved in the real world, where even simple decisions can be complex. </st><st c="4970">Most tasks we do are based on a long chain of observations, reasoning, and adjustments to new experiences. </st><st c="5077">Very rarely do we interact with people, tasks, and things in the real world in the same way we interact with LLMs online. </st><st c="5199">There is often this building of understanding, knowledge, and context that takes place and helps us find the best solutions. </st><st c="5324">AI agents are better able to handle this type of approach </st><st c="5382">to problem-solving.</st></p>
			<p><st c="5401">Agents can make a big difference to your RAG efforts, but what about this concept of the LLMs being their brains? </st><st c="5516">Let’s dive into the </st><st c="5536">concept further.</st></p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor247"/><st c="5552">LLMs as the agents’ brains</st></h2>
			<p><st c="5579">If you consider the LLM</st><a id="_idIndexMarker783"/><st c="5603"> as the brain of your AI agent, the next logical step is that you likely want the </st><em class="italic"><st c="5685">smartest</st></em><st c="5693"> LLM you can find to be that brain. </st><st c="5729">The capabilities of the LLM are going to affect your AI agent’s ability to reason and make decisions, which will certainly impact the results of the queries to your </st><st c="5894">RAG application.</st></p>
			<p><st c="5910">There is one major</st><a id="_idIndexMarker784"/><st c="5929"> way this metaphor of an LLM brain breaks down though, but in a very good way. </st><st c="6008">Unlike agents in the real world, the AI agent can always swap out their LLM brain for another LLM brain. </st><st c="6113">We could even give it multiple LLM brains that can serve to check each other and make sure things are proceeding as planned. </st><st c="6238">This gives us greater flexibility that will help us continually improve the capabilities of </st><st c="6330">our agents.</st></p>
			<p><st c="6341">So, how does LangGraph, or graphs in general, relate to AI agents? </st><st c="6409">We will discuss </st><st c="6425">that next.</st></p>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor248"/><st c="6435">Graphs, AI agents, and LangGraph</st></h1>
			<p><st c="6468">LangChain introduced</st><a id="_idIndexMarker785"/><st c="6489"> LangGraph in</st><a id="_idIndexMarker786"/><st c="6502"> 2024, so it is still relatively new. </st><st c="6540">It is an extension built on top of </st><code><st c="6863">AgentExecutor</st></code><st c="6876"> class, still exists, LangGraph is now the </st><em class="italic"><st c="6919">recommended</st></em><st c="6930"> way to build agents </st><st c="6951">in LangChain.</st></p>
			<p><st c="6964">LangGraph adds two important components</st><a id="_idIndexMarker788"/><st c="7004"> for </st><st c="7009">supporting agents:</st></p>
			<ul>
				<li><st c="7027">The ability to easily define cycles (</st><st c="7065">cyclical graphs)</st></li>
				<li><st c="7082">Built-in memory</st></li>
			</ul>
			<p><st c="7098">It provides a pre-built object equivalent to </st><code><st c="7144">AgentExecutor</st></code><st c="7157">, allowing developers to orchestrate agents using a </st><st c="7209">graph-based approach.</st></p>
			<p><st c="7230">Over the past couple of years, numerous papers, concepts, and approaches have emerged for building agents into RAG applications, such as orchestration agents, ReAct agents, self-refine agents, and multi-agent frameworks. </st><st c="7452">A common theme among these approaches is the concept of a cyclical graph that represents the agent’s control flow. </st><st c="7567">While many of these approaches, from an implementation standpoint, are going obsolete, their concepts are still highly useful and are captured in the graph-based environment </st><st c="7741">of LangGraph.</st></p>
			<p><strong class="bold"><st c="7754">LangGraph</st></strong><st c="7764"> has become</st><a id="_idIndexMarker789"/><st c="7775"> a powerful tool for supporting agents and managing their flow and process in RAG applications. </st><st c="7871">It enables developers to describe and represent single and multi-agent flows as graphs, providing extremely controlled </st><em class="italic"><st c="7990">flows</st></em><st c="7995">. This controllability is crucial for avoiding the pitfalls encountered by developers when creating agents </st><st c="8102">early on.</st></p>
			<p><st c="8111">As an example, the popular ReAct approach was an early paradigm for building agents. </st><strong class="bold"><st c="8197">ReAct</st></strong><st c="8202"> stands for </st><strong class="bold"><st c="8214">reason + act</st></strong><st c="8226">. In this pattern, an LLM</st><a id="_idIndexMarker790"/><st c="8251"> first thinks about what to do and then decides an action to take. </st><st c="8318">That action is then executed in an environment and an observation is returned. </st><st c="8397">With that observation, the LLM then repeats this process. </st><st c="8455">It uses reasoning to think about what to do next, decides another action to take, and continues until it has been determined that the goal has been met. </st><st c="8608">If you map this process out, it may look something like what you see here in </st><em class="italic"><st c="8685">Figure 12</st></em><em class="italic"><st c="8694">.2</st></em><st c="8696">:</st></p>
			<div><div><img src="img/B22475_12_02.jpg" alt="Figure 12.2 – ReAct cyclical graph representation"/><st c="8698"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="8721">Figure 12.2 – ReAct cyclical graph representation</st></p>
			<p><st c="8770">The set of loops in </st><em class="italic"><st c="8791">Figure 12</st></em><em class="italic"><st c="8800">.2</st></em><st c="8802"> can be represented</st><a id="_idIndexMarker791"/><st c="8821"> by cyclical graphs in LangGraph, with each step represented by nodes and edges. </st><st c="8902">Using this graphing paradigm, you can see how a tool such as LangGraph, a tool for building graphs in LangChain, can form the backbone of your agent framework. </st><st c="9062">As we build our agent framework, we can represent these agent loops using LangGraph, which helps you describe and orchestrate the control flow. </st><st c="9206">This focus on the control flow is critical to addressing some of the early challenges with agents, where a lack of control leads to rogue agents that can’t complete their loops or focus on the </st><st c="9399">wrong task.</st></p>
			<p><st c="9410">Another key element that LangGraph has built into it is persistence. </st><st c="9480">Persistence can be used to maintain the memory of the agent, giving it the information it needs to reflect on all of its actions so far, and representing the </st><em class="italic"><st c="9638">OBSERVE</st></em><st c="9645"> component presented in </st><em class="italic"><st c="9669">Figure 12</st></em><em class="italic"><st c="9678">.2</st></em><st c="9680">. This is really helpful for having multiple conversations at the same time or remembering previous iterations and actions. </st><st c="9804">This persistence also enables human-in-the-loop features that give you better control over what the agent is doing at key intervals during </st><st c="9943">its actions.</st></p>
			<p><st c="9955">The paper that introduced the ReAct approach to agent building can be found </st><st c="10032">here: </st><a href="https://arxiv.org/abs/2210.03629"><st c="10038">https://arxiv.org/abs/2210.03629</st></a></p>
			<p><st c="10070">Let’s dive right into the code lab for building our agent and walk through more key individual concepts as we encounter them in </st><st c="10199">the code.</st></p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor249"/><st c="10208">Code lab 12.1 – adding a LangGraph agent to RAG</st></h1>
			<p><st c="10256">In this code lab, we will add an agent</st><a id="_idIndexMarker792"/><st c="10295"> to our existing RAG pipeline that can make decisions about whether to retrieve from an index or use a web search. </st><st c="10410">We will show the inner thoughts of the agent as it processes data that it retrieves toward the goal of providing you with a more thorough response to your question. </st><st c="10575">As we add the code for our agent, we will see new components, such as tools, toolkits, graphs, nodes, edges, and, of course, the agent itself. </st><st c="10718">For each component, we will go more in-depth into how that component interacts and supports your RAG application. </st><st c="10832">We will also add code so that this functions more like a chat session, rather than a </st><st c="10917">Q&amp;A session:</st></p>
			<ol>
				<li><st c="10929">First, we will install some new packages to support our </st><st c="10986">agent development:</st><pre class="source-code">
<strong class="bold"><st c="11004">%pip install tiktoken</st></strong></pre><pre class="source-code">
<code><st c="11084">tiktoken</st></code><st c="11092"> package, which is an OpenAI package used for tokenizing text data before feeding it into language models. </st><st c="11199">Last, we pull in the </st><code><st c="11220">langgraph</st></code><st c="11229"> package we have </st><st c="11246">been discussing.</st></p></li>
				<li><st c="11262">Next, we add a new LLM definition and update our </st><st c="11312">existing one:</st><pre class="source-code">
<strong class="bold"><st c="11325">llm = ChatOpenAI(model_name="gpt-4o-mini",</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="11368">    temperature=0, streaming=True)</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="11399">agent_llm = ChatOpenAI(model_name="gpt-4o-mini",</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="11448">    temperature=0, streaming=True)</st></strong></pre></li>
			</ol>
			<p><st c="11479">The new </st><code><st c="11488">agent_llm</st></code><st c="11497"> LLM instance will serve as our agent’s brain, handling reasoning and execution of the agent tasks, whereas the original </st><code><st c="11618">llm</st></code><st c="11621"> instance will still be present in our general LLM to do the same LLM tasks we have used it for in the past. </st><st c="11730">While the two LLMs are defined with the same model and parameters in our example, you could and should experiment with using different LLMs for these different tasks, to see if there is a combination that works better for your RAG applications. </st><st c="11975">You could even add additional LLMs to handle specific tasks, such as the </st><code><st c="12048">improve</st></code><st c="12055"> or </st><code><st c="12059">score_documents</st></code><st c="12074"> functions in this code, if you find an LLM better at those tasks or have trained or fine-tuned your own for these particular actions. </st><st c="12209">For example, It is common for simple tasks to be handled by faster, lower-cost LLMs as long as they can perform the task successfully. </st><st c="12344">There is a lot of flexibility built into this code that you can take advantage of! </st><st c="12427">Also, note that we add </st><code><st c="12450">streaming=True</st></code><st c="12464"> to the LLM definition. </st><st c="12488">This turns on streaming data from the LLM, which is more conducive to an agent that may make several calls, sometimes in parallel, constantly interacting with </st><st c="12647">the LLM.</st></p>
			<p><st c="12655">Now, we are going to skip down to after the retriever definitions (</st><code><st c="12723">dense_retriever</st></code><st c="12739">, </st><code><st c="12741">sparse_retriever</st></code><st c="12757">, and </st><code><st c="12763">ensemble_retriever</st></code><st c="12781">) and add our first tool. </st><st c="12808">A </st><strong class="bold"><st c="12810">tool</st></strong><st c="12814"> has a very specific and important meaning when it comes to agents; so, let’s talk about </st><st c="12903">that now.</st></p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor250"/><st c="12912">Tools and toolkits</st></h2>
			<p><st c="12931">In the following code, we are going to add</st><a id="_idIndexMarker793"/><st c="12974"> a </st><strong class="bold"><st c="12977">web </st></strong><strong class="bold"><st c="12981">search</st></strong><st c="12987"> tool:</st></p>
			<pre class="source-code"><st c="12993">
from langchain_community.tools.tavily_search import TavilySearchResults
_ = load_dotenv(dotenv_path='env.txt')
os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')
!export TAVILY_API_KEY=os.environ['TAVILY_API_KEY']
web_search = TavilySearchResults(max_results=4)
web_search_name = web_search.name</st></pre>
			<p><st c="13297">You will need to get another API key and add it to the </st><code><st c="13353">env.txt</st></code><st c="13360"> file we have used in the past for the OpenAI and Together APIs. </st><st c="13425">Just like with those APIs, you will need to go to that website, set up your API key, and then copy that into your </st><code><st c="13539">env.txt</st></code><st c="13546"> file. </st><st c="13553">The Tavily website can be found at this </st><st c="13593">URL: </st><a href="https://tavily.com/"><st c="13598">https://tavily.com/</st></a></p>
			<p><st c="13617">We run</st><a id="_idIndexMarker794"/><st c="13624"> the code again that loads the data from the </st><code><st c="13669">env.txt</st></code><st c="13676"> file and then we set up the </st><code><st c="13705">TavilySearchResults</st></code><st c="13724"> object with </st><code><st c="13737">max_results</st></code><st c="13748"> of </st><code><st c="13752">4</st></code><st c="13753">, meaning when we run it for search, we only want four search results maximum. </st><st c="13832">We then assign the </st><code><st c="13851">web_search.name</st></code><st c="13866"> variable to a variable called </st><code><st c="13897">web_search_name</st></code><st c="13912"> so that we have that available later when we want to tell the agent about it. </st><st c="13991">You can run this tool directly using </st><st c="14028">this code:</st></p>
			<pre class="source-code"><st c="14038">
web_search.invoke(user_query)</st></pre>
			<p><st c="14068">Running this tool code with </st><code><st c="14097">user_query</st></code><st c="14107"> will give you a result like this (truncated </st><st c="14152">for brevity):</st></p>
			<pre class="source-code"><st c="14165">
[{'url': 'http://sustainability.google/',
  'content': "Google Maps\nChoose the most fuel-efficient route\nGoogle Shopping\nShop for more efficient appliances for your home\nGoogle Flights\nFind a flight with lower per-traveler carbon emissions\nGoogle Nest\...[TRUNCATED HERE]"},
…
  'content': "2023 Environmental Report. </st><st c="14486">Google's 2023 Environmental Report outlines how we're driving positive environmental outcomes throughout our business in three key ways: developing products and technology that empower individuals on their journey to a more sustainable life, working together with partners and organizations everywhere to transition to resilient, low-carbon systems, and operating ..."}]</st></pre>
			<p><st c="14856">We truncated this so we take up less space in the book, but try this in the code and you will see four results, as we asked for, and they all seem to be highly related to the topic </st><code><st c="15038">user_query</st></code><st c="15048"> is asking about. </st><st c="15066">Note that you will not need to run this tool directly in your code like we </st><st c="15141">just did.</st></p>
			<p><st c="15150">At this point, you have just established your first agent tool! </st><st c="15215">This is a search engine tool that your agent can use to retrieve more information from the internet to help it achieve its goal of answering the question your user poses </st><st c="15385">to it.</st></p>
			<p><st c="15391">The </st><em class="italic"><st c="15396">tool</st></em><st c="15400"> concept in LangChain</st><a id="_idIndexMarker795"/><st c="15421"> and when building agents comes from the idea that you want to make actions available to your agent so that it can carry out its tasks. </st><st c="15557">Tools are the mechanism that allows this to happen. </st><st c="15609">You define a tool like we just did for the web search, and then you later add it to a list of tools that the agent can use to accomplish its tasks. </st><st c="15757">Before we set up that list though, we want to create another tool that is central for a RAG application: a </st><st c="15864">retriever tool:</st></p>
			<pre class="source-code"><st c="15879">
from langchain.tools.retriever import create_retriever_tool
retriever_tool = create_retriever_tool(
    ensemble_retriever,
    "retrieve_google_environmental_question_answers",
    "Extensive information about Google environmental
     efforts from 2023.",
)
retriever_tool_name = retriever_tool.name</st></pre>
			<p><st c="16164">Note that with the web search</st><a id="_idIndexMarker796"/><st c="16194"> tool, we imported it from </st><code><st c="16221">langchain_community.tools.tavily_search</st></code><st c="16260">, whereas with this tool, we use </st><code><st c="16293">langchain.tools.retriever</st></code><st c="16318">. This reflects the fact that Tavily is a third-party tool, whereas the retriever tool we create here is part of the core LangChain functionality. </st><st c="16465">After importing the </st><code><st c="16485">create_retriever_tool</st></code><st c="16506"> function, we use it to create the </st><code><st c="16541">retriever_tool</st></code><st c="16555"> tool for our agent. </st><st c="16576">Again, like with </st><code><st c="16593">web_search_name</st></code><st c="16608">, we pull out the </st><code><st c="16626">retriever_tool.name</st></code><st c="16645"> variable we can reference later when we want to refer to it for the agent. </st><st c="16721">You may notice the name of the actual retriever this tool will use, the </st><code><st c="16793">ensemble_retriever</st></code><st c="16811"> retriever, which we created in </st><a href="B22475_08.xhtml#_idTextAnchor152"><em class="italic"><st c="16843">Chapter 8</st></em></a><st c="16852">’s </st><em class="italic"><st c="16856">8.3 </st></em><em class="italic"><st c="16860">code lab</st></em><st c="16868">!</st></p>
			<p><st c="16869">You should also note that the name that we are giving this tool, as far as the agent is concerned, is found in the second field, and we are calling it </st><code><st c="17020">retrieve_google_environmental_question_answers</st></code><st c="17066">. When we name variables in code, we normally try to keep them smaller, but for tools that agents will use, it is helpful to provide more verbose names that will help the agent understand what can be </st><st c="17266">used fully.</st></p>
			<p><st c="17277">We now have two tools for our agent! </st><st c="17315">However, we still need to tell the agent about them eventually; so, we package them up into a list that we can later share with </st><st c="17443">the agent:</st></p>
			<pre class="source-code"><st c="17453">
tools = [web_search, retriever_tool]</st></pre>
			<p><st c="17490">You see here the two tools we created previously, the </st><code><st c="17545">web_search</st></code><st c="17555"> tool and the </st><code><st c="17569">retriever_tool</st></code><st c="17583"> tool, getting added to the tools list. </st><st c="17623">If we had other tools we wanted to make available to the agent, we could add those to the list as well. </st><st c="17727">In the LangChain</st><a id="_idIndexMarker797"/><st c="17743"> ecosystem, there are hundreds of tools </st><st c="17783">available: </st><a href="https://python.langchain.com/v0.2/docs/integrations/tools/"><st c="17794">https://python.langchain.com/v0.2/docs/integrations/tools/</st></a></p>
			<p><st c="17852">You will want to make sure the LLM you are using is “good” at reasoning and using tools. </st><st c="17942">In general, chat models tend to have been fine-tuned for tool calling and will be better at using tools. </st><st c="18047">Non-chat-fine-tuned models may not be able to use tools, especially if the tools are complex or require multiple calls. </st><st c="18167">Using well-written names and descriptions can play an important role in setting your agent LLM up for success </st><st c="18277">as well.</st></p>
			<p><st c="18285">In the agent we are building, we have all the tools we need, but you will also want to look at the toolkits, which are convenient groups of tools. </st><st c="18433">LangChain provides a list of the current toolkits</st><a id="_idIndexMarker798"/><st c="18482"> available on their </st><st c="18502">website: </st><a href="https://python.langchain.com/v0.2/docs/integrations/toolkits/"><st c="18511">https://python.langchain.com/v0.2/docs/integrations/toolkits/</st></a></p>
			<p><st c="18572">For example, if you have a data infrastructure that uses pandas DataFrames, you could use the pandas DataFrame toolkit to offer your agent various tools to access those DataFrames in different ways. </st><st c="18772">Drawing straight from the LangChain website, toolkits are described as </st><st c="18843">follows: (</st><a href="https://python.langchain.com/v0.1/docs/modules/agents/concepts/#toolkits"><st c="18853">https://python.langchain.com/v0.1/docs/modules/agents/concepts/#toolkits</st></a><st c="18926">)</st></p>
			<p class="author-quote"><st c="18928">For many common tasks, an agent will need a set of related tools. </st><st c="18994">For this LangChain provides the concept of toolkits - groups of around 3-5 tools needed to accomplish specific objectives. </st><st c="19117">For example, the GitHub toolkit has a tool for searching through GitHub issues, a tool for reading a file, a tool for commenting, etc.</st></p>
			<p><st c="19251">So, basically, if you are focusing on a set of common tasks for your agent or a popular integration partner with LangChain (such as a Salesforce integration), there is likely a toolkit that will give you access to all the tools you need all </st><st c="19493">at once.</st></p>
			<p><st c="19501">Now that we have the tools established, let’s start building the components of our agent, starting with the </st><st c="19610">agent state.</st></p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor251"/><st c="19622">Agent state</st></h2>
			<p><st c="19634">The </st><code><st c="19740">AgentState</st></code><st c="19750"> class that establishes the “state” for your agent and tracks it over time. </st><st c="19826">This state is a local mechanism to the agent that you make available to all parts of the graph and can be stored in a </st><st c="19944">persistence layer.</st></p>
			<p><st c="19962">Here, we set up this state for our </st><st c="19998">RAG agent:</st></p>
			<pre class="source-code"><st c="20008">
from typing import Annotated, Literal, Sequence, TypedDict
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage],
                        add_messages]</st></pre>
			<p><st c="20250">This imports relevant packages for setting up </st><code><st c="20297">AgentState</st></code><st c="20307">. For example, </st><code><st c="20322">BaseMessage</st></code><st c="20333"> is a base class for representing messages in the conversation between the user and the AI agent. </st><st c="20431">It will be used to define the structure and properties of messages in the state of the conversation. </st><st c="20532">It then defines a graph and a </st><code><st c="20562">"state"</st></code><st c="20569"> object that it passes around to each node. </st><st c="20613">You can set the state to be a variety of types of objects that you can store different types of data, but for our RAG agent, we set up our state to be a list </st><st c="20771">of </st><code><st c="20774">"messages"</st></code><st c="20784">.</st></p>
			<p><st c="20785">We then need to import another round of packages to set up other parts of </st><st c="20860">our agent:</st></p>
			<pre class="source-code"><st c="20870">
from langchain_core.messages import HumanMessage
from langchain_core.pydantic_v1 import BaseModel, Field
from langgraph.prebuilt import tools_condition</st></pre>
			<p><st c="21022">In this code, we start with importing </st><code><st c="21061">HumanMessage</st></code><st c="21073">. </st><code><st c="21075">HumanMessage</st></code><st c="21087"> is a specific type of message that represents a message sent by the human user. </st><st c="21168">It will used when constructing the prompt for the agent to generate a response. </st><st c="21248">We also import </st><code><st c="21263">BaseModel</st></code><st c="21272"> and </st><code><st c="21277">Field</st></code><st c="21282">. </st><code><st c="21284">BaseModel</st></code><st c="21293"> is a class from the </st><code><st c="21314">Pydantic</st></code><st c="21322"> library that is used to define data models and validate data. </st><code><st c="21385">Field</st></code><st c="21390"> is a class from </st><code><st c="21407">Pydantic</st></code><st c="21415"> that is used to define the properties and validation rules for fields in a data model. </st><st c="21503">Last, we import </st><code><st c="21519">tools_condition</st></code><st c="21534">. The </st><code><st c="21540">tools_condition</st></code><st c="21555"> function is a pre-built function provided by the </st><code><st c="21605">LangGraph</st></code><st c="21614"> library. </st><st c="21624">It is used to assess the agent’s decision on whether to use specific tools based on the current state of </st><st c="21729">the conversation.</st></p>
			<p><st c="21746">These imported classes and </st><a id="_idIndexMarker800"/><st c="21774">functions are used throughout the code to define the structure of messages, validate data, and control the flow of the conversation based on the agent’s decisions. </st><st c="21938">They provide the necessary building blocks and utilities for constructing the language model application using the </st><code><st c="22053">LangGraph</st></code><st c="22062"> library.</st></p>
			<p><st c="22071">We then define our primary prompt (representing what the user would input) </st><st c="22147">like this:</st></p>
			<pre class="source-code"><st c="22157">
generation_prompt = PromptTemplate.from_template(
    """You are an assistant for question-answering tasks.
    </st><st c="22262">Use the following pieces of retrieved context to answer
    the question. </st><st c="22332">If you don't know the answer, just say
    that you don't know. </st><st c="22392">Provide a thorough description to
    fully answer the question, utilizing any relevant
    information you find.
    </st><st c="22498">Question: {question}
    Context: {context}
    Answer:"""
)</st></pre>
			<p><st c="22550">This is a replacement for the code that we were using in the past </st><st c="22617">code labs:</st></p>
			<pre class="console"><st c="22627">
prompt = hub.pull("jclemens24/rag-prompt")</st></pre>
			<p><st c="22670">We alter the name to </st><code><st c="22692">generation_prompt</st></code><st c="22709"> to make this prompt’s use </st><st c="22736">more clear.</st></p>
			<p><st c="22747">Our graph usage is about to pick up in our code, but first, we need to cover some basic graph </st><st c="22842">theory concepts.</st></p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor252"/><st c="22858">Core concepts of graph theory</st></h1>
			<p><st c="22888">To better understand how we are going to use LangGraph in the next few blocks of code, it is helpful to review some key concepts</st><a id="_idIndexMarker801"/><st c="23017"> in </st><strong class="bold"><st c="23021">graph theory</st></strong><st c="23033">. </st><strong class="bold"><st c="23035">Graphs</st></strong><st c="23041"> are mathematical</st><a id="_idIndexMarker802"/><st c="23058"> structures that can be used to represent relationships between different objects. </st><st c="23141">The objects are</st><a id="_idIndexMarker803"/><st c="23156"> called </st><strong class="bold"><st c="23164">nodes</st></strong><st c="23169"> and the relationships between them, typically drawn</st><a id="_idIndexMarker804"/><st c="23221"> with a line, are called </st><strong class="bold"><st c="23246">edges</st></strong><st c="23251">. You have already seen these concepts in </st><em class="italic"><st c="23293">Figure 12</st></em><em class="italic"><st c="23302">.1</st></em><st c="23304">, but it is important to understand how they relate to any graph and how that is used </st><st c="23390">in LangGraph.</st></p>
			<p><st c="23403">With LangGraph, there are also specific types of edges representing different types of these relationships. </st><st c="23512">The “conditional edge” that we mentioned along with </st><em class="italic"><st c="23564">Figure 12</st></em><em class="italic"><st c="23573">.1</st></em><st c="23575">, for example, represents when you need to make a decision about which node you should go to next; so, they represent the decisions. </st><st c="23708">When talking about the ReAct paradigm, this has also been called</st><a id="_idIndexMarker805"/><st c="23772"> the </st><strong class="bold"><st c="23777">action edge</st></strong><st c="23788">, as it is where the action takes place, relating to the </st><em class="italic"><st c="23845">reason + action</st></em><st c="23860"> approach of ReAct. </st><em class="italic"><st c="23880">Figure 12</st></em><em class="italic"><st c="23889">.3</st></em><st c="23891"> shows a basic graph consisting of nodes </st><st c="23932">and edges:</st></p>
			<div><div><img src="img/B22475_12_03.jpg" alt="Figure 12.3 – Basic graph representing our RAG application"/><st c="23942"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="23955">Figure 12.3 – Basic graph representing our RAG application</st></p>
			<p><st c="24013">In this cyclical graph shown in </st><em class="italic"><st c="24046">Figure 12</st></em><em class="italic"><st c="24055">.3</st></em><st c="24057">, you see nodes representing the start, agent, retrieve tool, generation, observation, and end. </st><st c="24153">The key edges are where the LLM makes the decision of what tool to use (retrieve is the only one available here), observes if what is retrieved is sufficient, and then pushes to generation. </st><st c="24343">If it is decided that the retrieved data is not sufficient, there is an edge that sends the observation back to the agent to decide if it wants to try again. </st><st c="24501">These decision points are the </st><em class="italic"><st c="24531">conditional edges</st></em> <st c="24548">we discussed.</st></p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor253"/><st c="24562">Nodes and edges in our agent</st></h1>
			<p><st c="24591">OK, so let’s</st><a id="_idIndexMarker806"/><st c="24604"> review. </st><st c="24613">We’ve mentioned that an agentic RAG graph has three key components: the </st><em class="italic"><st c="24685">state</st></em><st c="24690"> that we already talked about, the </st><em class="italic"><st c="24725">nodes</st></em><st c="24730"> that append to or update the </st><a id="_idIndexMarker807"/><st c="24760">state, and the </st><em class="italic"><st c="24775">conditional edges</st></em><st c="24792"> that decide which node to visit next. </st><st c="24831">We are now to the point where we can step through each of these in code blocks, seeing how the three components interact with </st><st c="24957">each other.</st></p>
			<p><st c="24968">Given this background, the first thing we will add to the code is the conditional edge, where the decisions are made. </st><st c="25087">In this case, we are going to define an edge that determines if the retrieved documents are relevant to the question. </st><st c="25205">This is the function that will decide whether to move on to the generation stage or to go back and </st><st c="25304">try again:</st></p>
			<ol>
				<li><st c="25314">We will step through this code in multiple steps, but keep in mind that this is one large function, starting with </st><st c="25429">the definition:</st><pre class="source-code"><st c="25444">
def score_documents(state) -&gt; Literal[</st></pre><pre class="source-code"><st c="25483">
    "generate", "improve"]:</st></pre><p class="list-inset"><st c="25507">This code starts by defining a function called </st><code><st c="25555">score_documents</st></code><st c="25570"> that determines whether the retrieved documents are relevant to the given question. </st><st c="25655">The function takes the state we’ve been discussing as a parameter, which is a set of messages that have been collected. </st><st c="25775">This is how we make the state </st><code><st c="25805">available</st></code><st c="25814"> to this conditional </st><st c="25835">edge function.</st></p></li>
				<li><st c="25849">Now, we build the </st><st c="25868">data model:</st><pre class="source-code"><st c="25879">
    class scoring(BaseModel):</st></pre><pre class="source-code"><st c="25905">
        binary_score: str = Field(</st></pre><pre class="source-code"><st c="25932">
          description="Relevance score 'yes' or 'no'")</st></pre><p class="list-inset"><st c="25977">This defines a data model class called </st><code><st c="26017">scoring</st></code><st c="26024"> using </st><code><st c="26031">Pydantic</st></code><st c="26039">’s </st><code><st c="26043">BaseModel</st></code><st c="26052">. The </st><code><st c="26058">scoring</st></code><st c="26065"> class has a single field called </st><code><st c="26098">binary_score</st></code><st c="26110">, which is a string representing the relevance score as either </st><code><st c="26173">yes</st></code> <st c="26176">or </st><code><st c="26180">no</st></code><st c="26182">.</st></p></li>
				<li><st c="26183">Next, we add the LLM that will make </st><st c="26220">this decision:</st><pre class="source-code"><st c="26234">
    llm_with_tool = llm.with_structured_output(</st></pre><pre class="source-code"><st c="26278">
        scoring)</st></pre><p class="list-inset"><st c="26287">This creates an instance of </st><code><st c="26316">llm_with_tool</st></code><st c="26329"> by calling </st><code><st c="26341">llm.with_structured_output(scoring)</st></code><st c="26376">, combining the LLM with the scoring data model for structured </st><st c="26439">output validation.</st></p></li>
				<li><st c="26457">As we have</st><a id="_idIndexMarker808"/><st c="26468"> seen in the past, we need to set up a </st><code><st c="26507">PromptTemplate</st></code><st c="26521"> class to pass to the </st><a id="_idIndexMarker809"/><st c="26543">LLM. </st><st c="26548">Here is </st><st c="26556">that prompt:</st><pre class="source-code"><st c="26568">
    prompt = PromptTemplate(</st></pre><pre class="source-code"><st c="26593">
        template="""You are assessing relevance of a retrieved document to a user question with a binary grade. </st><st c="26698">Here is the retrieved document:</st></pre><pre class="source-code"><st c="26729">
{context}</st></pre><pre class="source-code"><st c="26739">
Here is the user question: {question}</st></pre><pre class="source-code"><st c="26777">
If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. </st><st c="26886">Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.""",</st></pre><pre class="source-code"><st c="26991">
        input_variables=["context", "question"],</st></pre><pre class="source-code"><st c="27032">
    )</st></pre><p class="list-inset"><st c="27034">This defines a prompt using the </st><code><st c="27066">PromptTemplate</st></code><st c="27080"> class, providing instructions to the LLM for applying a binary score for the relevance of the retrieved document based on the </st><st c="27207">given question.</st></p></li>
				<li><st c="27222">We can then use LCEL to build the chain that combines the prompt with the </st><code><st c="27297">llm_with_tool</st></code><st c="27310"> tool we just </st><st c="27324">set up:</st><pre class="source-code"><st c="27331">
    chain = prompt | llm_with_tool</st></pre><p class="list-inset"><st c="27362">This chain represents the pipeline for scoring the documents. </st><st c="27425">This defines the chain, but we haven’t invoked </st><st c="27472">it yet.</st></p></li>
				<li><st c="27479">First, we want to </st><a id="_idIndexMarker810"/><st c="27498">pull in the state. </st><st c="27517">Next, we pull the state (</st><code><st c="27542">"messages"</st></code><st c="27553">) into the</st><a id="_idIndexMarker811"/><st c="27564"> function so that we can use it, and we take the </st><st c="27613">last message:</st><pre class="source-code"><st c="27626">
    messages = state["messages"]</st></pre><pre class="source-code"><st c="27655">
    last_message = messages[-1]</st></pre><pre class="source-code"><st c="27683">
    question = messages[0].content</st></pre><pre class="source-code"><st c="27714">
    docs = last_message.content</st></pre><p class="list-inset"><st c="27742">This extracts the necessary information from the </st><code><st c="27792">"state"</st></code><st c="27799"> parameter and then preps the state/message as the context we are going to pass to our agent brain (LLM). </st><st c="27905">The specific components extracted here include </st><st c="27952">the following:</st></p><ul><li><code><st c="27966">messages</st></code><st c="27975">: The list of messages in </st><st c="28002">the conversation</st></li><li><code><st c="28018">last_message</st></code><st c="28031">: The last message in </st><st c="28054">the conversation</st></li><li><code><st c="28070">question</st></code><st c="28079">: The content of the first message, which is assumed to be the </st><st c="28143">user’s question</st></li><li><code><st c="28158">docs</st></code><st c="28163">: The content of the last message, which is assumed to be the </st><st c="28226">retrieved documents</st></li></ul><p class="list-inset"><st c="28245">Then, finally, we invoke the chain with the prompt filled (if you remember, we call this </st><code><st c="28362">question</st></code><st c="28370"> and context </st><code><st c="28383">docs</st></code><st c="28387"> to get the </st><st c="28399">scored result:</st></p><pre class="source-code"><st c="28413">
    scored_result = chain.invoke({"question":</st></pre><pre class="source-code"><st c="28455">
        question, "context": docs})</st></pre><pre class="source-code"><st c="28483">
    score = scored_result.binary_score</st></pre><p class="list-inset"><st c="28518">This extracts</st><a id="_idIndexMarker813"/><st c="28532"> the </st><code><st c="28537">binary_score</st></code><st c="28549"> variable from the </st><code><st c="28568">scored_result</st></code><st c="28581"> object and assigns it to the </st><code><st c="28611">score</st></code><st c="28616"> variable. </st><st c="28627">The </st><code><st c="28631">llm_with_tool</st></code><st c="28644"> step, which</st><a id="_idIndexMarker814"/><st c="28656"> is the last step in the LangChain chain, aptly called </st><code><st c="28711">chain</st></code><st c="28716">, is going to return a string-based binary result based on the response from the </st><st c="28797">scoring function:</st></p><pre class="source-code"><st c="28814">
    if score == "yes":</st></pre><pre class="source-code"><st c="28833">
        print("---DECISION: DOCS RELEVANT---")</st></pre><pre class="source-code"><st c="28872">
        return "generate"</st></pre><pre class="source-code"><st c="28890">
    else:</st></pre><pre class="source-code"><st c="28896">
        print("---DECISION: DOCS NOT RELEVANT---")</st></pre><pre class="source-code"><st c="28939">
        print(score)</st></pre><pre class="source-code"><st c="28952">
        return "improve"</st></pre><p class="list-inset"><st c="28969">This checks the value of the score. </st><st c="29006">If the </st><code><st c="29013">score</st></code><st c="29018"> value is </st><code><st c="29028">yes</st></code><st c="29031">, it prints a message indicating that the documents are relevant and returns </st><code><st c="29108">generate</st></code><st c="29116"> as the final output from the </st><code><st c="29146">score_documents</st></code><st c="29161"> function, suggesting that the next step is to generate a response. </st><st c="29229">If the </st><code><st c="29236">score</st></code><st c="29241"> value is </st><code><st c="29251">no</st></code><st c="29253">, or, technically, anything other than </st><code><st c="29292">yes</st></code><st c="29295">, it prints messages indicating that the documents are not relevant and returns </st><code><st c="29375">improve</st></code><st c="29382">, suggesting that the next step is to improve the query from </st><st c="29443">the user.</st></p><p class="list-inset"><st c="29452">Overall, this function acts as a decision point in the workflow, determining whether the retrieved documents are relevant to the question and directing the flow to either generate a response or rewrite the question based on the </st><st c="29681">relevance score.</st></p></li>
				<li><st c="29697">Now that we</st><a id="_idIndexMarker815"/><st c="29709"> have our conditional edge defined, we are going to move on to </st><a id="_idIndexMarker816"/><st c="29772">defining our nodes, starting with </st><st c="29806">the agent:</st><pre class="source-code"><st c="29816">
def agent(state):</st></pre><pre class="source-code"><st c="29834">
    print("---CALL AGENT---")</st></pre><pre class="source-code"><st c="29860">
    messages = state["messages"]</st></pre><pre class="source-code"><st c="29889">
    llm = llm.bind_tools(tools)</st></pre><pre class="source-code"><st c="29917">
    response = llm.invoke(messages)</st></pre><pre class="source-code"><st c="29949">
    return {"messages": [response]}</st></pre><p class="list-inset"><st c="29981">This function represents the agent node on our graph and invokes the agent model to generate a response based on the current state. </st><st c="30114">The </st><code><st c="30118">agent</st></code><st c="30123"> function takes the current state (</st><code><st c="30158">"state"</st></code><st c="30166">) as input, which contains the messages in the conversation, prints a message indicating that it is calling the agent, extracts the messages from the state dictionary, uses the </st><code><st c="30344">agent_llm</st></code><st c="30353"> instance of the </st><code><st c="30370">ChatOpenAI</st></code><st c="30380"> class we defined earlier, representing the agent </st><em class="italic"><st c="30430">brain</st></em><st c="30435">, and then binds the tools to the model using the </st><code><st c="30485">bind_tools</st></code><st c="30495"> method. </st><st c="30504">We then invoke the agent’s </st><code><st c="30531">llm</st></code><st c="30534"> instance with the messages and assign the result to the </st><code><st c="30591">response</st></code><st c="30599"> variable.</st></p></li>
				<li><st c="30609">Our next node, </st><code><st c="30625">improve</st></code><st c="30632">, is responsible for transforming </st><code><st c="30666">user_query</st></code><st c="30676"> to produce a better question if the agent determines this </st><st c="30735">is needed:</st><pre class="source-code"><st c="30745">
def improve(state):</st></pre><pre class="source-code"><st c="30765">
    print("---TRANSFORM QUERY---")</st></pre><pre class="source-code"><st c="30796">
    messages = state["messages"]</st></pre><pre class="source-code"><st c="30825">
    question = messages[0].content</st></pre><pre class="source-code"><st c="30856">
    msg = [</st></pre><pre class="source-code"><st c="30864">
        HumanMessage(content=f"""\n</st></pre><pre class="source-code"><st c="30892">
            Look at the input and try to reason about</st></pre><pre class="source-code"><st c="30934">
            the underlying semantic intent / meaning.</st></pre><pre class="source-code"><st c="30976">
            \n</st></pre><pre class="source-code"><st c="30979">
            Here is the initial question:</st></pre><pre class="source-code"><st c="31009">
            \n ------- \n</st></pre><pre class="source-code"><st c="31023">
            {question}</st></pre><pre class="source-code"><st c="31034">
            \n ------- \n</st></pre><pre class="source-code"><st c="31048">
            Formulate an improved question:</st></pre><pre class="source-code"><st c="31080">
            """,</st></pre><pre class="source-code"><st c="31085">
        )</st></pre><pre class="source-code"><st c="31087">
    ]</st></pre><pre class="source-code"><st c="31089">
    response = llm.invoke(msg)</st></pre><pre class="source-code"><st c="31116">
    return {"messages": [response]}</st></pre><p class="list-inset"><st c="31148">This function, like all </st><a id="_idIndexMarker817"/><st c="31173">of our node and edge-related functions, takes the</st><a id="_idIndexMarker818"/><st c="31222"> current state (</st><code><st c="31238">"state"</st></code><st c="31246">) as input. </st><st c="31259">The function returns a dictionary with the response appended to the messages list. </st><st c="31342">The function prints a message indicating that it is transforming the query, extracts the messages from the state dictionary, retrieves the content of the first message (</st><code><st c="31511">messages[0].content</st></code><st c="31531">), which is assumed to be the initial question, and assigns it to the </st><code><st c="31602">question</st></code><st c="31610"> variable. </st><st c="31621">We then set up a message using the </st><code><st c="31656">HumanMessage</st></code><st c="31668"> class, indicating that we want the </st><code><st c="31704">llm</st></code><st c="31707"> instance to reason about the underlying semantic intent of the question and formulate an improved question. </st><st c="31816">The result from the </st><code><st c="31836">llm</st></code><st c="31839"> instance is assigned to the </st><code><st c="31868">response</st></code><st c="31876"> variable. </st><st c="31887">Finally, it returns a dictionary with the response appended to the </st><st c="31954">messages list.</st></p></li>
				<li><st c="31968">Our next node function is the </st><code><st c="31999">generate</st></code><st c="32007"> function:</st><pre class="source-code"><st c="32017">
def generate(state):</st></pre><pre class="source-code"><st c="32038">
    print("---GENERATE---")</st></pre><pre class="source-code"><st c="32062">
    messages = state["messages"]</st></pre><pre class="source-code"><st c="32091">
    question = messages[0].content</st></pre><pre class="source-code"><st c="32122">
    last_message = messages[-1]</st></pre><pre class="source-code"><st c="32150">
    question = messages[0].content</st></pre><pre class="source-code"><st c="32181">
    docs = last_message.content</st></pre><pre class="source-code"><st c="32209">
    rag_chain = generation_prompt | llm |</st></pre><pre class="source-code"><st c="32247">
        str_output_parser</st></pre><pre class="source-code"><st c="32265">
    response = rag_chain.invoke({"context": docs,</st></pre><pre class="source-code"><st c="32311">
        "question": question})</st></pre><pre class="source-code"><st c="32334">
    return {"messages": [response]}</st></pre><p class="list-inset"><st c="32366">This function is </st><a id="_idIndexMarker819"/><st c="32384">similar to our generation step in the previous chapter’s code </st><a id="_idIndexMarker820"/><st c="32446">labs but simplified to provide just the response. </st><st c="32496">It generates an answer based on the retrieved documents and the question. </st><st c="32570">The function takes the current state (</st><code><st c="32608">"state"</st></code><st c="32616">) as input, which contains the messages in the conversation, prints a message indicating that it is generating an answer, extracts the messages from the state dictionary, retrieves the content of the first message (</st><code><st c="32832">messages[0].content</st></code><st c="32852">), which is assumed to be the question, and assigns it to the </st><code><st c="32915">question</st></code><st c="32923"> variable.</st></p><p class="list-inset"><st c="32933">The function then retrieves the last message (</st><code><st c="32980">messages[-1]</st></code><st c="32993">) and assigns it to the </st><code><st c="33018">last_message</st></code><st c="33030"> variable. </st><st c="33041">The </st><code><st c="33045">docs</st></code><st c="33049"> variable is assigned the content of </st><code><st c="33086">last_message</st></code><st c="33098">, which is assumed to be the retrieved documents. </st><st c="33148">At this point, we create a chain called </st><code><st c="33188">rag_chain</st></code><st c="33197"> by combining the </st><code><st c="33215">generation_prompt</st></code><st c="33232">, </st><code><st c="33234">llm</st></code><st c="33237">, and </st><code><st c="33243">str_output_parser</st></code><st c="33260"> variables using the </st><code><st c="33281">|</st></code><st c="33282"> operator. </st><st c="33293">As with other LLM prompting, we hydrate the predefined </st><code><st c="33348">generation_prompt</st></code><st c="33365"> as the prompt for generating the answer, which returns a</st><a id="_idIndexMarker821"/><st c="33422"> dictionary </st><a id="_idIndexMarker822"/><st c="33434">with the </st><code><st c="33443">response</st></code><st c="33451"> variable appended to the </st><code><st c="33477">messages</st></code><st c="33485"> list.</st></p></li>
			</ol>
			<p><st c="33491">Next, we want to set up our cyclical graphs using LangGraph and assign our nodes and edges </st><st c="33583">to them.</st></p>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor254"/><st c="33591">Cyclical graph setup</st></h1>
			<p><st c="33612">The next </st><a id="_idIndexMarker823"/><st c="33622">big step in our code </st><a id="_idIndexMarker824"/><st c="33643">is setting up our graphs </st><st c="33668">using LangGraph:</st></p>
			<ol>
				<li><st c="33684">First, we import some important packages to get </st><st c="33733">us started:</st><pre class="source-code"><st c="33744">
from langgraph.graph import END, StateGraph</st></pre><pre class="source-code"><st c="33788">
from langgraph.prebuilt import ToolNode</st></pre><p class="list-inset"><st c="33828">This code imports the following necessary classes and functions from the </st><code><st c="33902">langgraph</st></code><st c="33911"> library:</st></p><ul><li><code><st c="33920">END</st></code><st c="33924">: A special node representing the end of </st><st c="33966">the workflow</st></li><li><code><st c="33978">StateGraph</st></code><st c="33989">: A class for defining the state graph of </st><st c="34032">the workflow</st></li><li><code><st c="34044">ToolNode</st></code><st c="34053">: A class for defining a node that represents a tool </st><st c="34107">or action</st></li></ul></li>
				<li><st c="34116">We then pass </st><code><st c="34130">AgentState</st></code><st c="34140"> as an argument to the </st><code><st c="34163">StateGraph</st></code><st c="34173"> class we just imported for defining the state graph of </st><st c="34229">the workflow:</st><pre class="source-code"><st c="34242">
workflow = StateGraph(AgentState)</st></pre><p class="list-inset"><st c="34276">This creates a new instance of </st><code><st c="34308">StateGraph</st></code><st c="34318"> called </st><code><st c="34326">workflow</st></code><st c="34334"> and defines a new graph for that </st><code><st c="34368">workflow</st></code> <code><st c="34376">StateGraph</st></code><st c="34387"> instance.</st></p></li>
				<li><st c="34397">Next, we define the nodes we will cycle between and assign our node functions </st><st c="34476">to them:</st><pre class="source-code"><st c="34484">
workflow.add_node("agent", agent)  # agent</st></pre><pre class="source-code"><st c="34526">
retrieve = ToolNode(tools)</st></pre><pre class="source-code"><st c="34553">
workflow.add_node("retrieve", retrieve)</st></pre><pre class="source-code"><st c="34593">
# retrieval from web and or retriever</st></pre><pre class="source-code"><st c="34631">
workflow.add_node("improve", improve)</st></pre><pre class="source-code"><st c="34669">
 # Improving the question for better retrieval</st></pre><pre class="source-code"><st c="34715">
workflow.add_node("generate", generate)  # Generating a response after we know the documents are relevant</st></pre><p class="list-inset"><st c="34820">This code adds </st><a id="_idIndexMarker825"/><st c="34836">multiple nodes to the </st><code><st c="34858">workflow</st></code><st c="34866"> instance </st><a id="_idIndexMarker826"/><st c="34876">using the </st><code><st c="34886">add_node</st></code><st c="34894"> method:</st></p><ul><li><code><st c="34902">"agent"</st></code><st c="34910">: This node represents the agent node, which invokes the </st><st c="34968">agent function.</st></li><li><code><st c="34983">"retrieve"</st></code><st c="34994">: This node represents the retrieval node, which is a special </st><code><st c="35057">ToolNode</st></code><st c="35065"> containing the tools list we defined early with the </st><code><st c="35118">web_search</st></code><st c="35128"> and </st><code><st c="35133">retriever_tool</st></code><st c="35147"> tools. </st><st c="35155">In this code, to aid in readability, we explicitly break out the </st><code><st c="35220">ToolNode</st></code><st c="35228"> class instance and define the </st><code><st c="35259">retrieve</st></code><st c="35267"> variable with it, which indicates the “retrieve” focus of this node more explicitly. </st><st c="35353">We then pass that </st><code><st c="35371">retrieve</st></code><st c="35379"> variable into the </st><code><st c="35398">add_node</st></code><st c="35406"> function.</st></li><li><code><st c="35416">"improve"</st></code><st c="35426">: This node represents the node for improving the question, which invokes the </st><code><st c="35505">improve</st></code><st c="35512"> function.</st></li><li><code><st c="35522">"generate"</st></code><st c="35533">: This node represents the node for generating a response, which invokes the </st><code><st c="35611">generate</st></code><st c="35619"> function.</st></li></ul></li>
				<li><st c="35629">Next, we need to define our starting point for </st><st c="35677">our workflow:</st><pre class="source-code"><st c="35690">
workflow.set_entry_point("agent")</st></pre><p class="list-inset"><st c="35724">This sets the entry point of the </st><code><st c="35758">workflow</st></code><st c="35766"> instance to the </st><code><st c="35783">"agent"</st></code><st c="35790"> node </st><st c="35796">using </st><code><st c="35802">workflow.set_entry_point("agent")</st></code><st c="35835">.</st></p></li>
				<li><st c="35836">Next, we call the </st><code><st c="35855">"agent"</st></code><st c="35862"> node to decide whether to retrieve </st><st c="35898">or not:</st><pre class="source-code"><st c="35905">
workflow.add_conditional_edges("agent", tools_condition,</st></pre><pre class="source-code"><st c="35962">
    {</st></pre><pre class="source-code"><st c="35964">
        "tools": "retrieve",</st></pre><pre class="source-code"><st c="35985">
        END: END,</st></pre><pre class="source-code"><st c="35995">
    },</st></pre><pre class="source-code"><st c="35998">
)</st></pre><p class="list-inset"><st c="36000">In this code, </st><code><st c="36014">tools_condition</st></code><st c="36029"> is used as a conditional edge in the workflow graph. </st><st c="36083">It determines whether the agent should proceed to the retrieval step (</st><code><st c="36153">"tools": "retrieve"</st></code><st c="36173">) or end the conversation (</st><code><st c="36201">END: END</st></code><st c="36210">) based on the agent’s decision. </st><st c="36244">The retrieval step represents both of the tools that we made available for the agent to use where needed, and the other option, to end the conversation simply ends </st><st c="36408">the workflow.</st></p></li>
				<li><st c="36421">Here, we add more </st><a id="_idIndexMarker827"/><st c="36440">edges, which are used </st><a id="_idIndexMarker828"/><st c="36462">after the </st><code><st c="36472">"action"</st></code><st c="36480"> node </st><st c="36486">is called:</st><pre class="source-code"><st c="36496">
workflow.add_conditional_edges("retrieve",</st></pre><pre class="source-code"><st c="36539">
    score_documents)</st></pre><pre class="source-code"><st c="36556">
workflow.add_edge("generate", END)</st></pre><pre class="source-code"><st c="36591">
workflow.add_edge("improve", "agent")</st></pre><p class="list-inset"><st c="36629">After the </st><code><st c="36640">"retrieve"</st></code><st c="36650"> node is called, it adds conditional edges using </st><code><st c="36699">workflow.add_conditional_edges("retrieve", score_documents)</st></code><st c="36758">. This assesses the retrieved documents using the </st><code><st c="36808">score_documents</st></code><st c="36823"> function and determines the next node based on the score. </st><st c="36882">This also adds an edge from the </st><code><st c="36914">"generate"</st></code><st c="36924"> node to the </st><code><st c="36937">END</st></code><st c="36940"> node using </st><code><st c="36952">workflow.add_edge("generate", END)</st></code><st c="36986">. This indicates that, after generating a response, the workflow ends. </st><st c="37057">Last, this adds an edge from the </st><code><st c="37090">"improve"</st></code><st c="37099"> node back to the </st><code><st c="37117">"agent"</st></code><st c="37124"> node using </st><code><st c="37136">workflow.add_edge("improve", "agent")</st></code><st c="37173">. This creates a loop where the improved question is sent back to the agent for </st><st c="37253">further processing.</st></p></li>
				<li><st c="37272">We are now ready to compile </st><st c="37301">the graph:</st><pre class="source-code"><st c="37311">
graph = workflow.compile()</st></pre><p class="list-inset"><st c="37338">This line compiles the workflow graph using </st><code><st c="37383">workflow.compile</st></code><st c="37399"> and assigns the compiled graph to the </st><code><st c="37438">graph</st></code><st c="37443"> variable, which now represents a compiled version of the </st><code><st c="37501">StateGraph</st></code><st c="37511"> graph instance we </st><st c="37530">started with.</st></p></li>
				<li><st c="37543">We have already shown you the visualization of what this graph looks like earlier in this chapter in </st><em class="italic"><st c="37645">Figure 12</st></em><em class="italic"><st c="37654">.1</st></em><st c="37656">, but if you want to run the visualization yourself, you </st><a id="_idIndexMarker829"/><st c="37713">can use </st><a id="_idIndexMarker830"/><st c="37721">this code:</st><pre class="source-code"><st c="37731">
from IPython.display import Image, display</st></pre><pre class="source-code"><st c="37774">
try:</st></pre><pre class="source-code"><st c="37779">
    display(Image(graph.get_graph(</st></pre><pre class="source-code"><st c="37810">
        xray=True).draw_mermaid_png()))</st></pre><pre class="source-code"><st c="37842">
except:</st></pre><pre class="source-code"><st c="37850">
    pass</st></pre><p class="list-inset"><st c="37855">We can use </st><code><st c="37867">IPython</st></code><st c="37874"> to generate </st><st c="37887">this visualization.</st></p></li>
				<li><st c="37906">Last, we are going to finally put our agent </st><st c="37951">to work:</st><pre class="source-code"><st c="37959">
import pprint</st></pre><pre class="source-code"><st c="37973">
inputs = {</st></pre><pre class="source-code"><st c="37984">
    "messages": [</st></pre><pre class="source-code"><st c="37998">
        ("user", user_query),</st></pre><pre class="source-code"><st c="38020">
    ]</st></pre><pre class="source-code"><st c="38022">
}</st></pre><p class="list-inset"><st c="38024">This imports the </st><code><st c="38041">pprint</st></code><st c="38047"> module, which provides a pretty-print function for formatting and printing data structures, allowing us to see a more human-readable version of our agent output. </st><st c="38210">We then define a dictionary called </st><code><st c="38245">inputs</st></code><st c="38251"> that represents the initial input to the workflow graph. </st><st c="38309">The inputs dictionary contains a </st><code><st c="38342">"messages"</st></code><st c="38352"> key with a list of tuples. </st><st c="38380">In this case, it has a single tuple, </st><code><st c="38417">("user", user_query)</st></code><st c="38437">, where the </st><code><st c="38449">"user"</st></code><st c="38455"> string represents the role of the message sender (</st><code><st c="38506">user</st></code><st c="38511">) and </st><code><st c="38518">user_query</st></code><st c="38528"> is the user’s query </st><st c="38549">or question.</st></p></li>
				<li><st c="38561">We then initialize an empty string variable called </st><code><st c="38613">final_answer</st></code><st c="38625"> to store the final answer generated by </st><st c="38665">the workflow:</st><pre class="source-code"><st c="38678">
final_answer = ''</st></pre></li>
				<li><st c="38696">We then start our agent </st><a id="_idIndexMarker831"/><st c="38721">loop using the graph instance </st><a id="_idIndexMarker832"/><st c="38751">as </st><st c="38754">the basis:</st><pre class="source-code"><st c="38764">
for output in graph.stream(inputs):</st></pre><pre class="source-code"><st c="38800">
    for key, value in output.items():</st></pre><pre class="source-code"><st c="38834">
        pprint.pprint(f"Output from node '{key}':")</st></pre><pre class="source-code"><st c="38878">
        pprint.pprint("---")</st></pre><pre class="source-code"><st c="38899">
        pprint.pprint(value, indent=2, width=80,</st></pre><pre class="source-code"><st c="38940">
            depth=None)</st></pre><pre class="source-code"><st c="38952">
        final_answer = value</st></pre><p class="list-inset"><st c="38973">This starts a double loop using the output in </st><code><st c="39020">graph.stream(inputs)</st></code><st c="39040">. This iterates over the outputs generated by the </st><code><st c="39090">graph</st></code><st c="39095"> instance as it processes the inputs. </st><st c="39133">The </st><code><st c="39137">graph.stream(inputs)</st></code><st c="39157"> method streams the outputs from the </st><code><st c="39194">graph</st></code> <st c="39199">instance execution.</st></p><p class="list-inset"><st c="39219">Inside the outer loop, it starts another loop for two variables, </st><code><st c="39285">key</st></code><st c="39288"> and </st><code><st c="39293">value</st></code><st c="39298">, representing the key-value pairs in the </st><code><st c="39340">output.items</st></code><st c="39352"> variable. </st><st c="39363">This iterates over each of those key-value pairs, where the </st><code><st c="39423">key</st></code><st c="39426"> variable represents the node name and the </st><code><st c="39469">value</st></code><st c="39474"> variable represents the output generated by that node. </st><st c="39530">This will print the node name using </st><code><st c="39566">pprint.pprint(f"Output from node '{key}':")</st></code><st c="39609"> to indicate which node generated </st><st c="39643">the output.</st></p><p class="list-inset"><st c="39654">The code pretty-prints the value (output) using </st><code><st c="39703">pprint.pprint(value, indent=2, width=80, depth=None)</st></code><st c="39755">. The </st><code><st c="39761">indent</st></code><st c="39767"> parameter specifies the indentation level, </st><code><st c="39811">width</st></code><st c="39816"> specifies the maximum width of the output, and </st><code><st c="39864">depth</st></code><st c="39869"> specifies the maximum depth of nested data structures to print (</st><code><st c="39934">None</st></code><st c="39939"> means </st><st c="39946">no limit).</st></p><p class="list-inset"><st c="39956">It assigns the value (output) to the </st><code><st c="39994">final_answer</st></code><st c="40006"> variable, overwriting it in each iteration. </st><st c="40051">After the loop ends, </st><code><st c="40072">final_answer</st></code><st c="40084"> will contain the output generated by the last node in </st><st c="40139">the workflow.</st></p><p class="list-inset"><st c="40152">A nice </st><a id="_idIndexMarker833"/><st c="40160">feature of this code is that it allows you to see the</st><a id="_idIndexMarker834"/><st c="40213"> intermediate outputs generated by each node in the graph and track the progress of the query processing. </st><st c="40319">These print outputs represent the agent’s “thoughts” as it makes decisions within the loop. </st><st c="40411">The pretty-printing helps in formatting the outputs for </st><st c="40467">better readability.</st></p><p class="list-inset"><st c="40486">When we start the agent and start seeing the output, we can see that a lot is </st><st c="40565">going on!</st></p><p class="list-inset"><st c="40574">I will truncate a lot of the printout, but this will give you an idea of what </st><st c="40653">is provided:</st></p><pre class="source-code"><st c="40665">
---CALL AGENT---</st></pre><pre class="source-code"><st c="40682">
"Output from node 'agent':"</st></pre><pre class="source-code"><st c="40710">
'---'</st></pre><pre class="source-code"><st c="40716">
{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_46NqZuz3gN2F9IR5jq0MRdVm', 'function': {'arguments': '{"query":"Google\'s environmental initiatives"}', 'name': 'retrieve_google_environmental_question_answers'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-eba27f1e-1c32-4ffc-a161-55a32d645498-0', tool_calls=[{'name': 'retrieve_google_environmental_question_answers', 'args': {'query': "Google's environmental initiatives"}, 'id': 'call_46NqZuz3gN2F9IR5jq0MRdVm'}])]}</st></pre><pre class="source-code"><st c="41270">
'\n---\n'</st></pre><p class="list-inset"><st c="41280">This is the first part of our printout. </st><st c="41321">Here, we see the agent is deciding to use the </st><code><st c="41367">retrieve_google_environmental_question_answers</st></code><st c="41413"> tool. </st><st c="41420">If you will recall, that is the text-based name we gave to the retriever tool when defining it. </st><st c="41516">Good choice!</st></p></li>
				<li><st c="41528">Next, the agent is going to determine if it thinks the documents retrieved </st><st c="41604">are relevant:</st><pre class="source-code"><st c="41617">
---CHECK RELEVANCE---</st></pre><pre class="source-code"><st c="41639">
---DECISION: DOCS RELEVANT---</st></pre><p class="list-inset"><st c="41669">The decision is that they are. </st><st c="41701">Again, smart thinking, </st><st c="41724">Mr. </st><st c="41728">Agent.</st></p></li>
				<li><st c="41734">Last, we see the</st><a id="_idIndexMarker835"/><st c="41751"> output of what the agent is looking at, retrieved from</st><a id="_idIndexMarker836"/><st c="41806"> the PDF document and the ensemble retriever we have been using (there was a lot of retrieved data here, so I truncated most of the </st><st c="41938">actual content):</st><pre class="source-code"><st c="41954">
"Output from node 'retrieve':"</st></pre><pre class="source-code"><st c="41985">
'---'</st></pre><pre class="source-code"><st c="41991">
{ 'messages': [ ToolMessage(content='iMasons Climate AccordGoogle is a founding member and part of the governing body of the iMasons Climate Accord, a coalition united on carbon reduction in digital infrastructure.\nReFEDIn 2022, to activate industry-wide change…[TRUNCATED]', tool_call_id='call_46NqZuz3gN2F9IR5jq0MRdVm')]}</st></pre><pre class="source-code"><st c="42316">
'\n---\n'</st></pre><p class="list-inset"><st c="42326">When you look at the actual printout for this portion, you see that the retrieved data is concatenated together and provides substantial and in-depth data for our agent to </st><st c="42499">work with.</st></p></li>
				<li><st c="42509">At this point, just like our original RAG application was doing, the agent takes the question, retrieved data, and formulates a response based on the generation </st><a id="_idIndexMarker837"/><st c="42671">prompt we </st><a id="_idIndexMarker838"/><st c="42681">gave it:</st><pre class="source-code"><st c="42689">
---GENERATE---</st></pre><pre class="source-code"><st c="42704">
"Output from node 'generate':"</st></pre><pre class="source-code"><st c="42735">
'---'</st></pre><pre class="source-code"><st c="42741">
{ 'messages': [ 'Google has a comprehensive and multifaceted approach to '</st></pre><pre class="source-code"><st c="42816">
'environmental sustainability, encompassing various '</st></pre><pre class="source-code"><st c="42870">
'initiatives aimed at reducing carbon emissions, promoting'</st></pre><pre class="source-code"><st c="42930">
'sustainable practices, and leveraging technology for '</st></pre><pre class="source-code"><st c="42986">
"environmental benefits. </st><st c="43012">Here are some key aspects of Google's "</st></pre><pre class="source-code"><st c="43051">
'environmental initiatives:\n''\n'</st></pre><pre class="source-code"><st c="43086">
'1. </st><st c="43091">**Carbon Reduction and Renewable Energy**…']}</st></pre><pre class="source-code"><st c="43136">
'\n---\n'</st></pre><p class="list-inset"><st c="43146">We included a mechanism here to print out the final message separately </st><st c="43218">for readability:</st></p><pre class="source-code"><st c="43234">
final_answer['messages'][0]</st></pre><p class="list-inset"><st c="43262">This will print </st><st c="43279">this out:</st></p><pre class="source-code">
<strong class="bold"><st c="43288">"Google has a comprehensive and multifaceted approach to environmental sustainability, encompassing various initiatives aimed at reducing carbon emissions, promoting sustainable practices, and leveraging technology for environmental benefits. </st><st c="43532">Here are some key aspects of Google's environmental initiatives:\n\n1. </st><st c="43603">**Carbon Reduction and Renewable Energy**:\n   - **iMasons Climate Accord**: Google is a founding member and part of the governing body of this coalition focused on reducing carbon emissions in digital infrastructure.\n   - **Net-Zero Carbon**: Google is committed to operating sustainably with a focus on achieving net-zero carbon emissions. </st><st c="43942">This includes investments in carbon-free energy and energy-efficient facilities, such as their all-electric, net water-positive Bay View campus..."</st></strong></pre></li>
			</ol>
			<p><st c="44089">That is the full output of </st><st c="44117">our agent!</st></p>
			<h1 id="_idParaDest-256"><a id="_idTextAnchor255"/><st c="44127">Summary</st></h1>
			<p><st c="44135">In this chapter, we explored how AI agents and LangGraph can be combined to create more powerful and sophisticated RAG applications. </st><st c="44269">We learned that an AI agent is essentially an LLM with a loop that allows it to reason and break tasks down into simpler steps, improving the chances of success in complex RAG tasks. </st><st c="44452">LangGraph, an extension built on top of LCEL, provides support for building composable and customizable agentic workloads, enabling developers to orchestrate agents using a </st><st c="44625">graph-based approach.</st></p>
			<p><st c="44646">We dove into the fundamentals of AI agents and RAG integration, discussing the concept of tools that agents can use to carry out tasks, and how LangGraph’s </st><code><st c="44803">AgentState</st></code><st c="44813"> class tracks the state of the agent over time. </st><st c="44861">We also covered the core concepts of graph theory, including nodes, edges, and conditional edges, which are crucial for understanding how </st><st c="44999">LangGraph works.</st></p>
			<p><st c="45015">In the code lab, we built a LangGraph retrieval agent for our RAG application, demonstrating how to create tools, define the agent state, set up prompts, and establish the cyclical graphs using LangGraph. </st><st c="45221">We saw how the agent uses its reasoning capabilities to determine which tools to use, how to use them, and what data to feed them, ultimately providing a more thorough response to the </st><st c="45405">user’s question.</st></p>
			<p><st c="45421">Looking ahead, the next chapter will focus on how prompt engineering can be used to improve </st><st c="45514">RAG applications.</st></p>
		</div>
	<div></body></html>