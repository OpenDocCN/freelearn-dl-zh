<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Getting Started with Text Classification</h1>
                </header>
            
            <article>
                
<p class="mce-root">There are several ways that you can learn new ideas and learn new skills. In an art class students study colors, but aren't allowed to actually paint until college. Sound absurd?</p>
<p class="mce-root">Unfortunately, this is how most modern machine learning is taught. The experts are doing something similar. They tell you that need to know linear algebra, calculus and deep learning. This is before they'll teach you how to use <strong>natural language Processing</strong> (<strong>NLP</strong>).</p>
<p class="mce-root">In this book, I want us to learn by teaching the the whole game. In every section, we see how to solve real-world problems and learn the tools along the way. Then, we will dig deeper and deeper into understanding how to make these toolks. This learning and teaching style is very much inspired by Jeremy Howard of fast.ai fame.</p>
<p class="mce-root">The next focus is to have code examples wherever possible. This is to ensure that there is a clear and motivating purpose behind learning a topic. This helps us understand with intuition, beyond math formulae with algebraic notation.</p>
<p class="mce-root">In this opening chapter, we will focus on an introduction to NLP. And, then jump into a text classification example with code.</p>
<p>This is what our journey will briefly look like:</p>
<ul>
<li>What is NLP?</li>
<li>What does a good NLP workflow look like? This is to improve your success rate when working on any NLP project.</li>
<li>Text classification as a motivating example for a good NLP pipeline/workflow.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is NLP?</h1>
                </header>
            
            <article>
                
<p>Natural language processing is the use of machines to manipulate natural language. In this book, we will focus on written language, or in simpler words: text.</p>
<p>In effect, this is a practitioner's guide to text processing in English.</p>
<p>Humans are the only known species to have developed written languages. Yet, children don't learn to read and write on their own. This is to highlight the complexity of text processing and NLP.</p>
<p>The study of natural language processing has been around for more than 50 years. The famous Turing test for general artificial intelligence uses this language. This field has grown both in regard to linguistics and its computational techniques.</p>
<p>In the spirit of being able to build things first, we will learn how to build a simple text classification system using Python's scikit-learn and no other dependencies.</p>
<p>We will also address if this book is a good pick for you.</p>
<p>Let's get going!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why learn about NLP?</h1>
                </header>
            
            <article>
                
<p>The best way to get the most about of this book is by knowing what you want NLP to do for you.</p>
<p>A variety of reasons might draw you to NLP. It might be the higher earning potential. Maybe you've noticed and are excited by the potential of NLP, for example, regarding Uber's customer Service bots. Yes, they mostly use bots to answer your complaints instead of humans.</p>
<p>It is useful to know your motivation and write it down. This will help you select problems and projects that excite you. It will also help you be selective when reading this book. This is not an NLP Made Easy or similar book. Let's be honest: this is a challenging topic. Writing down your motivations is a helpful reminder. </p>
<p>As a legal note, the accompanying code has a permissive MIT License. You can use it at your work without legal hassle. That being said, each dependent library is from a third party, and you should <strong>definitely check</strong> if they <strong>allow commercial use or not.</strong></p>
<p>I don't expect you to be able to use all of the tools and techniques mentioned here. Cherry-pick things that make sense. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">You have a problem in mind</h1>
                </header>
            
            <article>
                
<p>You already have a problem in mind, such as an academic project or a problem at your work.</p>
<p>Are you looking for the best tools and techniques that you could use to get off the ground?</p>
<p>First, flip through to the book's index to check if I have covered your problem here. I have shared end-to-end solutions for some of the most common use cases here. If it is not shared, fret not—you are still covered. The underlying techniques for a lot of tasks are common. I have been careful to select methods that are useful to a wider audience.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical achievement</h1>
                </header>
            
            <article>
                
<p>Is learning a mark of achievement for you?</p>
<p>NLP and, more generally, data science, are popular terms. You are someone who wants to keep up. You are someone who takes joy from learning new tools, techniques, and technologies. This is your next big challenge. This is your chance to prove your ability to self-teach and meet mastery.</p>
<p>If this sounds like you, you may be interested in using this as a reference book. I have dedicated sections where we give you enough understanding of a method. I show you how to use it without having to dive down into the latest papers. This is an invitation to learning more, and you are not encouraged to stop here. Try these code samples out for yourself!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Do something new</h1>
                </header>
            
            <article>
                
<p>You have some domain expertise. Now, you want to do things in your domain that are not possible without these skills. One way to figure out new possibilities is to combine your domain expertise with what you learn here. There are several very large opportunities that I saw as I wrote this book, including the following:</p>
<ul>
<li>NLP for non-English languages such as Hindi, Tamil, or Telugu.</li>
<li>Specialized NLP for your domain, for example, finance and Bollywood have different languages in their own ways. Your models that have been trained on Bollywood news are not expected to work for finance.</li>
</ul>
<p>If this sounds like you, you want to pay attention to the text pre-processing sections in this book. These sections will help you understand how we make text ready for machine consumption.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Is this book for you?</h1>
                </header>
            
            <article>
                
<p>This book has been written so that it keeps the preceding use cases and mindsets in mind. The methods, technologies, tools, and techniques selected here are a fine balance of industry-grade stability and academia-grade results quality. There are several tools, such as parfit, and Flashtext, and ideas such as LIME, that have never been written about in the context of NLP.</p>
<p>Lastly, I understand the importance and excitement of deep learning methods and have a dedicated chapter on deep learning for NLP methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NLP workflow template</h1>
                </header>
            
            <article>
                
<p>Some of us would love to work on Natural Language Processing for its sheer intellectual challenges <span>–</span> across research and engineering. To measure our progress, having a workflow with rough time estimates is really valuable. In this short section, we will briefly outline what a usual NLP or even most applied machine learning processes look like.</p>
<p>Most people I've learned from like to use a (roughly) five-step process:</p>
<ul>
<li>Understanding the problem</li>
<li>Understanding and preparing data</li>
<li>Quick wins: proof of concepts</li>
<li>Iterating and improving the results</li>
<li>Evaluation and deployment</li>
</ul>
<p>This is just a process template. It has a lot of room for customization regarding the engineering culture in your company. Any of these steps can be broken down further. For instance, data preparation and understanding can be split further into analysis and cleaning. Similarly, the proof of concept step may involve multiple experiments, and a demo or a report submission of best results from those.</p>
<p>Although this appears to be a strictly linear process, it is not so. More often than not, you will want to revisit a previous step and change a parameter or a particular data transform to see the effect on later performance.</p>
<p>In order to do so, it is important to factor in the cyclic nature of this process in your code. <strong>Write code with well-designed abstractions with each component being independently reusable.</strong></p>
<p class="mce-root"/>
<div class="packt_tip">If you are interested in how to write better NLP code, especially for research or experimentation, consider looking up the slide deck titled <em>Writing Code for NLP Research</em>, by Joel Grus of AllenAI.</div>
<p>Let's expand a little bit into each of these sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the problem</h1>
                </header>
            
            <article>
                
<p>We will begin by understanding the requirements and constraints from a practical business view point. This tends to answer the following the questions:</p>
<ul>
<li>What is the main problem? We will try to understand <span>–</span> formally and informally <span>– </span>the assumptions and expectations from our project.</li>
<li>How will I solve this problem? List some ideas that you might have seen earlier or in this book. This is the list that you will use to plan your work ahead.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding and preparing the data</h1>
                </header>
            
            <article>
                
<p>Text and language is inherently unstructured. We might want to clean it in certain ways, such as expanding abbreviations and acronyms, removing punctuation, and so on. We also want to select a few samples that are the best representatives of the data we might see in the wild.</p>
<p>The other common practice is to prepare a gold dataset. A gold dataset is the best available data under reasonable conditions. This is not the best available data under ideal conditions. Creating the gold dataset often involves manual tagging and cleaning processes.</p>
<p>The next few sections are dedicated to text cleaning and text representations at this stage of the NLP workflow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quick wins – proof of concept</h1>
                </header>
            
            <article>
                
<p>We want to quickly spot the types of algorithms and dataset combinations that sort of work for us. We can then focus on them and study them in greater detail.</p>
<p>The results from here will help you estimate the amount of work ahead of you. For instance, if you are going to develop a search system for documents based exclusively on keywords, your main effort will probably be deploying an open source solution such as ElasticSearch.</p>
<p>Let's say that you now want to add a similar documents feature. Depending on the expected quality of results, you will want to look into techniques such as doc2vec and word2vec, or even some convolutional neural network solution using Keras/Tensorflow or PyTorch.</p>
<p>This step is essential to get a greater buy-in from others around you, such as your boss, to invest more energy and resources into this. In an engineering role, this demo should highlight parts of your work that the shelf systems usually can't do. These are your unique strengths. These are usually insights, customization, and control that other systems can't provide.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iterating and improving</h1>
                </header>
            
            <article>
                
<p>At this point, we have a selected list of algorithms, data, and methods that have encouraging results for us.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Algorithms</h1>
                </header>
            
            <article>
                
<p>If your algorithms are machine learning or statistical in nature, you will quite often have a lot of juice left.</p>
<p>There are quite often parameters for which you simply pick a good enough default during the earlier stage. Here, you might want to double down and check for the best value of those parameters. This idea is sometimes referred to as parameter search, or hyperparameter tuning in machine learning parlance.</p>
<p>You might want to combine the results of one technique with the other in particular ways. For instance, some statistical methods might be very good for finding noun phrases in your text and using them to classify it, while a deep learning method (let's call it DL-LSTM) might be the best suited for text classification of the entire document. In that case, you might want to pass the extra information from both your noun phrase extraction and DL-LSTM to another model. This will allow it to the use the best of both worlds. This idea is sometimes referred to as stacking in machine learning parlance. This was quite successful on the machine learning contest platform Kaggle until very recently.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pre-processing</h1>
                </header>
            
            <article>
                
<p>Simple changes in data pre-processing or the data cleaning stage can quite often give you dramatically better results. For instance, making sure that your entire corpus is in lowercase can help you reduce the number of unique words (your vocabulary size) by a significant fraction.</p>
<p class="mce-root"/>
<div class="packt_infobox">If your numeric representation of words is skewed by the word frequency, sometimes it helps to normalize and/or scale the same. The laziest hack is to simply divide by the frequency.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation and deployment</h1>
                </header>
            
            <article>
                
<p>Evaluation and deployment are critical components in making your work widely available. The quality of your evaluation determines how trustworthy your work is by other people. Deployment varies widely, but quite often is abstracted out in single function calls or REST API calls. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>Let's say you have a model with 99% accuracy in classifying brain tumors. Can you trust this model? No.<br/>
<br/>
If your model had said that no-one has a brain tumor, it would still have 99%+ accuracy. Why?</p>
<p>Because luckily 99% or more of the population does not have a brain tumor!</p>
<p>To use our models for practical use, we need to look beyond accuracy. We need to understand what the model gets right or wrong in order to improve it. A minute spent understanding the confusion matrix will stop us from going ahead with such dangerous models.</p>
<p>Additionally, we will want to develop an intuition of what the model is doing underneath the black box optimization algorithms. Data visualization techniques such as t-SNE can assist us with this.</p>
<p>For continuously running NLP applications such as email spam classifiers or chatbots, we would want the evaluation of the model quality to happen continuously as well. This will help us ensure that the model's performance does not degrade with time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>This book is written with a programmer-first mindset. We will learn how to deploy any machine learning or NLP application as a REST API which can then be used for the web and mobile. This architecture is quite prevalent in the industry. For instance, we know that this is how data science teams such as those at Amazon and LinkedIn deploy their work to the web.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example – text classification workflow</h1>
                </header>
            
            <article>
                
<p>The preceding process is fairly generic. What would it look like for one of the most common natural language applications <span>–</span> text classification?</p>
<p>The following<span> flow diagram was built by Microsoft Azure, and is used here to explain how their own technology fits directly into our workflow template. There are several new words that they have introduced to feature engineering, such as unigrams, TF-IDF, TF, n-grams, and so on:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0ff7cb56-e4a8-40d3-b1fc-42f8ae3eb381.png"/></div>
<p>The main steps in their flow diagram are as follows:</p>
<ol>
<li><strong>Step 1</strong>: Data preparation</li>
<li><strong>Step 2</strong>: Text pre-processing</li>
<li><strong>Step 3</strong>: Feature engineering:
<ul>
<li>Unigrams TF-IDF extraction</li>
<li>N-grams TF extraction</li>
</ul>
</li>
<li><strong>Step 4</strong>: Train and evaluate models</li>
<li><strong>Step 5</strong>: Deploy trained models as web services</li>
</ol>
<p>This means that it's time to stop talking and start programming. Let's quickly set up the environment first and then we will work on building our first text classification system in 30 lines of code or less.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launchpad – programming environment setup</h1>
                </header>
            
            <article>
                
<p>We will use the fast.ai machine learning setup for this exercise. Their setup environment is great for personal experimentation and industry-grade proof-of-concept projects. I have used the fast.ai environment on both Linux and Windows. We will use Python 3.6 here since our code will not run for other Python versions.</p>
<p class="mce-root"/>
<p>A quick search on their forums will also take you to the latest instructions on how to set up the same on most cloud computing solutions including AWS, Google Cloud Platform, and Paperspace. </p>
<p>This environment covers the tools that we will use across most of the major tasks that we will perform: text processing (including cleaning), feature extraction, machine learning and deep learning models, model evaluation, and deployment.</p>
<p>It includes spaCy out of the box. spaCy is an open source tool that was made for an industry-grade NLP toolkit. If someone recommends that you use NLTK for a task, use spaCy instead. The demo ahead works out of the box in their environment.</p>
<p>There are a few more packages that we will need for later tasks. We will install and set them up as and when required. We don't want to bloat your installation with unnecessary packages that you might not even use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text classification in 30 lines of code</h1>
                </header>
            
            <article>
                
<p>Let's divide the classification problem into the following steps:</p>
<ol>
<li>Getting the data</li>
<li>Text to numbers</li>
<li>Running ML algorithms with sklearn</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p>The 20 newsgroups dataset is a fairly well-known dataset among the NLP community. It is near-ideal for demonstration purposes. This dataset has a near-uniform distribution across 20 classes. This uniform distribution makes iterating rapidly on classification and clustering techniques easy.</p>
<p>We will use the famous 20 newsgroups dataset for our demonstrations as well:</p>
<pre>from sklearn.datasets import fetch_20newsgroups  # import packages which help us download dataset <br/>twenty_train = fetch_20newsgroups(subset='train', shuffle=True, download_if_missing=True)<br/>twenty_test = fetch_20newsgroups(subset='test', shuffle=True, download_if_missing=True)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Most modern NLP methods rely heavily on machine learning methods. These methods need words that are written as strings of text to be converted into a numerical representation. This numerical representation can be as simple as assigning a unique integer ID to slightly more comprehensive vector of float values. In the case of the latter, this is sometimes referred to as vectorization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text to numbers</h1>
                </header>
            
            <article>
                
<p class="mce-root">We will be using a bag of words model for our example. We simply convert the number of times every word occurs per document. Therefore, each document is a bag and we count the frequency of each word in that bag. This also means that we lose any <em>ordering</em> information that's present in the text. Next, we assign each unique word an integer ID. All of these unique words become our vocabulary. Each word in our vocabulary is treated as a machine learning feature. Let's make our vocabulary first.</p>
<p>Scikit-learn has a high-level component that will create feature vectors for us. This is called <kbd>CountVectorizer</kbd>. We recommend reading more about it from the scikit-learn docs:</p>
<pre># Extracting features from text files<br/>from sklearn.feature_extraction.text import CountVectorizer<br/><br/>count_vect = CountVectorizer()<br/>X_train_counts = count_vect.fit_transform(twenty_train.data)<br/><br/>print(f'Shape of Term Frequency Matrix: {X_train_counts.shape}')</pre>
<p class="mce-root">By using <kbd>count_vect.fit_transform(twenty_train.data)</kbd>, we are learning the vocabulary dictionary, which returns a Document-Term matrix of shape [<kbd>n_samples</kbd>, <kbd>n_features</kbd>]. This means that we have <kbd>n_samples</kbd> documents or bags with <kbd>n_features</kbd> unique words across them.</p>
<p>We will now be able to extract a meaningful relationship between these words and the tags or classes they belong to. One of the simplest ways to do this is to count the number of times a word occurs in each class.</p>
<p>We have a small issue with this <span>–</span> long documents then tend to influence the result a lot more. We can normalize this effect by dividing the word frequency by the total words in that document. We call this Term Frequency, or simply TF.</p>
<p>Words like <em>the</em>, <em>a</em>, and <em>of</em> are common across all documents and don't really help us distinguish between document classes or separate them. We want to emphasize rarer words, such as <em>Manmohan</em> and <em>Modi</em>, over common words. One way to do this is to use inverse document frequency, or IDF. Inverse document frequency is a measure of whether the term is common or rare in all documents.</p>
<p class="mce-root"/>
<p>We multiply TF with IDF to get our TF-IDF metric, which is always greater than zero. TF-IDF is calculated for a triplet of term t, document d, and vocab dictionary D.<br/>
<br/>
We can directly calculate TF-IDF using the following lines of code:</p>
<pre>from sklearn.feature_extraction.text import TfidfTransformer<br/><br/>tfidf_transformer = TfidfTransformer()<br/>X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)<br/><br/>print(f'Shape of TFIDF Matrix: {X_train_tfidf.shape}')</pre>
<p>The last line will output the dimension of the Document-Term matrix, which is (11314, 130107).</p>
<p>Please note that in the preceding example we used each word as a feature, so the TF-IDF was calculated for each word. When we use a single word as a feature, we call it a unigram. If we were to use two consecutive words as a feature instead, we'd call it a bigram. In general, for n-words, we would call it an n-gram.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning</h1>
                </header>
            
            <article>
                
<p>Various algorithms can be used for text classification. You can build a classifier in scikit using the following code:</p>
<pre>from sklearn.linear_model import LogisticRegression as LR<br/>from sklearn.pipeline import Pipeline</pre>
<p>Let's dissect the preceding code, line by line.</p>
<p>The initial two lines are simple imports. We import the fairly well-known Logistic Regression model and rename the import LR. The next is a pipeline import:</p>
<div class="packt_quote">"Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be "transforms", that is, they must implement fit and transform methods. The final estimator only needs to implement fit."</div>
<div class="packt_quote CDPAlignRight CDPAlign"><span> - from </span><a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">sklearn docs</a><a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"/></div>
<p class="mce-root"/>
<p>Scikit-learn pipelines are, logistically, lists of operations that are applied, one after another. First, we applied the two operations we have already seen: <kbd>CountVectorizer()</kbd> and <kbd>TfidfTransformer()</kbd>. This was followed by <kbd>LR()</kbd>. The pipeline was created with <kbd>Pipeline(...)</kbd>, but hasn't been executed. It is only executed when we call the <kbd>fit()</kbd> function from the <kbd>Pipeline</kbd> object:</p>
<pre>text_lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])<br/>text_lr_clf = text_lr_clf.fit(twenty_train.data, twenty_train.target)</pre>
<p>When this is called, it calls the transform function of all but the last object. For the last object <span>–</span> our Logistic Regression classifier <span>–</span> its <kbd>fit()</kbd> function is called. These transforms and classifiers are also referred to as estimators:</p>
<div class="packt_quote"><q>"All estimators in a pipeline, except the last one, must be transformers (that is, they must have a transform method). The last estimator may be any type (transformer, classifier, and so on)."<br/></q></div>
<div class="packt_quote CDPAlignRight CDPAlign"><q>- f</q><q>rom<span> </span><a href="http://scikit-learn.org/stable/modules/pipeline.html">sklearn pipeline docs</a></q></div>
<p>Let's calculate the accuracy of this model on the test data. For calculating the means on a large number of values, we will be using a scientific library called <kbd>numpy</kbd><em>: </em></p>
<pre>import numpy as np<br/>lr_predicted = text_lr_clf.predict(twenty_test.data)<br/>lr_clf_accuracy = np.mean(lr_predicted == twenty_test.target) * 100.<br/><br/>print(f'Test Accuracy is {lr_clf_accuracy}')</pre>
<p>This prints out the following output:</p>
<pre>Test Accuracy is 82.79341476367499</pre>
<p>We used the LR default parameters here. We can later optimize these using <kbd>GridSearch</kbd> or <kbd>RandomSearch</kbd> to improve the accuracy even more.</p>
<div class="packt_tip">If you're going to remember only one thing from this section, remember to try a linear model such as logistic regression. They are often quite good for sparse high-dimensional data such as text, bag-of-words, or TF-IDF.</div>
<p>In addition to accuracy, it is useful to understand which categories of text are being confused for which other categories. We will call this a confusion matrix.</p>
<p>The following code uses the same variables we used to calculate the test accuracy for finding out the confusion matrix:</p>
<pre>from sklearn.metrics import confusion_matrix<br/>cf = confusion_matrix(y_true=twenty_test.target, y_pred=lr_predicted)<br/>print(cf)</pre>
<p>This prints a giant list of numbers which is not very interpretable. Let's try pretty printing this by using the <kbd>print-json</kbd> hack:</p>
<pre>import json<br/>print(json.dumps(cf.tolist(), indent=2))</pre>
<p>This returns the following code:</p>
<pre>[<br/>  [<br/>    236,<br/>    2,<br/>    0,<br/>    0,<br/>    1,<br/>    1,<br/>    3,<br/>    0,<br/>    3,<br/>    3,<br/>    1,<br/>    1,<br/>    2,<br/>    9,<br/>    2,<br/>    35,<br/>    3,<br/>    4,<br/>    1,<br/>    12<br/>  ],<br/> ...<br/>  [<br/>    38,<br/>    4,<br/>    0,<br/>    0,<br/>    0,<br/>    0,<br/>    4,<br/>    0,<br/>    0,<br/>    2,<br/>    2,<br/>    0,<br/>    0,<br/>    8,<br/>    3,<br/>    48,<br/>    17,<br/>    2,<br/>    9,<br/>    114<br/>  ]<br/> ]</pre>
<p>This is slightly better. We now understand that this is a 20 × 20 grid of numbers. However, interpreting these numbers is a tedious task unless we can bring some visualization into this game. Let's do that next:</p>
<pre># this line ensures that the plot is rendered inside the Jupyter we used for testing this code<br/>%matplotlib inline <br/><br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/><br/>plt.figure(figsize=(20,10))<br/>ax = sns.heatmap(cf, annot=True, fmt="d",linewidths=.5, center = 90, vmax = 200)<br/># plt.show() # optional, un-comment if the plot does not show</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This gives us the following amazing plot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-369 image-border" src="assets/3ef56a99-2179-4327-8a60-336aa7dd16df.png" style=""/></div>
<p>This plot highlights information of interest to us in different color schemes. For instance, the light diagonal from the lupper-left corner to the lower-right corner shows everything we got right. The other grids are darker-colored if we confused those more. For instance, 97 samples of one class got wrongly tagged, which is quickly visible by the dark black color in row <span class="packt_screen">18</span> and column <span class="packt_screen">16</span>.</p>
<p>We will dive deeper into both parts of this section <span>–</span> model interpretation and data visualization <span>– </span>in slightly more detail later in this book.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you got a feel for the broader things we need to make the project work. We saw the steps that are involved in this process by using a text classification example. We saw how to prepare text for machine learning with scikit-learn. We saw Logistic Regression for ML. We also saw a confusion matrix, which is a quick and powerful tool for making sense of results in all machine learning, beyond NLP.</p>
<p>We are just getting started. From here on out, we will dive deeper into each of these steps and see what other methods exist out there. In the next chapter, we will look at some common methods for text cleaning and extraction. Since this is what we will spend up to 80% of our total time on, it's worth the time and energy learning it. </p>


            </article>

            
        </section>
    </body></html>