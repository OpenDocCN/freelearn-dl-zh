<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer259">
			<h1 id="_idParaDest-343"><em class="italic"><a id="_idTextAnchor315"/>Chapter 14</em>: Line-Following with a Camera in Python</h1>
			<p>In the last chapter, we saw how to use a camera to follow and track objects. In this chapter, we will be extending the camera code to create line-sensing behavior.</p>
			<p>We will look at where robots use line following and how it is useful. We will also learn about some of the different approaches taken to following paths in different robots, along with their trade-offs. You will see how to build a simple line-following track.</p>
			<p>We will learn about some different algorithms to use and then choose a simple one. We will make a data flow diagram to see how it works, collect sample images to test it with, and then tune its performance based on the sample images. Along the way, we'll see more ways to approach computer vision and extract useful data from it. </p>
			<p>We will enhance our PID code, build our line detection algorithm into robot driving behavior, and see the robot running with this. The chapter closes with ideas on how you can take this further.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Introduction to line following</li>
				<li>Making a line-follower test track</li>
				<li>A line-following computer vision pipeline</li>
				<li>Trying computer vision with test images</li>
				<li>Line following with the PID algorithm</li>
				<li>Finding a line again</li>
			</ul>
			<h1 id="_idParaDest-344"><a id="_idTextAnchor316"/>Technical requirements</h1>
			<p>For this chapter, you will need the following:</p>
			<ul>
				<li>The robot and code from <a href="B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283"><em class="italic">Chapter 13</em></a>, <em class="italic">Robot Vision – Using a Pi Camera and OpenCV</em></li>
				<li>Some white or some black insulating tape</li>
				<li>Some A2 paper or boards – the opposite color to the insulating tape</li>
				<li>A pair of scissors</li>
				<li>Good lighting</li>
			</ul>
			<p>The code for this section can be found at <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter14">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter14</a>.</p>
			<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/3slLzbQ">https://bit.ly/3slLzbQ</a></p>
			<h1 id="_idParaDest-345"><a id="_idTextAnchor317"/>Introduction to line following</h1>
			<p>Before we start building code, let's find out about line-following robot behaviors, where and how systems use them, and the different techniques for doing so.</p>
			<h2 id="_idParaDest-346"><a id="_idTextAnchor318"/>What is line following?</h2>
			<p>Some robots are <a id="_idIndexMarker870"/>required to stay on specific paths within their tasks. It is simpler for a robot to navigate a line than to plan and map whole rooms or buildings. </p>
			<p>In simple terms, line following is being able to follow a marked path autonomously. These can be visual markers, such as blue tape or a white line on a black road. As the robot drives along the line, it will continually be looking for where the line ahead is and correcting its course to follow that line.</p>
			<p>In robot competitions, racing on lines is a common challenge, with speed being critical after accuracy.</p>
			<h2 id="_idParaDest-347"><a id="_idTextAnchor319"/>Usage in industry</h2>
			<p>By far the <a id="_idIndexMarker871"/>most common usage of line-following behavior <a id="_idIndexMarker872"/>is in industry. Robots known as <strong class="bold">automated guided vehicles</strong> (<strong class="bold">AGVs</strong>) need to follow set paths for many reasons. These tasks can be warehouse robots staying on tracks between aisles of stacked products or factory robots staying on paths clear of other work areas. The line may mark a route between a storage shelf and a loading bay or a robot charging station and the robot's work area:</p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="Images/B15660_14_01.jpg" alt="" width="1024" height="768"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – IntellCart – a line-following industrial robot by Mukeshhrs [Public domain]</p>
			<p>IntelliCart, shown in <em class="italic">Figure 14.1</em>, uses bright blue guide tape, although, in most industrial applications, robots use under-floor magnetic tracks.</p>
			<p>The route <a id="_idIndexMarker873"/>may include choice points, with multiple lines coming from a particular location. Depending on their task, the robot may need extra clues to sense that it has reached these points. An engineer can set up a repeated path for a fully automated system. </p>
			<p>Having these demarcated means that you can set safety boundaries and be clear on where humans and robots do or do not interact; this means that robots will rarely operate outside of well-understood areas.</p>
			<h2 id="_idParaDest-348"><a id="_idTextAnchor320"/>Types of line following</h2>
			<p>There are <a id="_idIndexMarker874"/>a few major branches of line following and related systems. </p>
			<p><strong class="bold">Visual line following</strong> is by far <a id="_idIndexMarker875"/>the most commonly <a id="_idIndexMarker876"/>practiced and easy-to-set-up line following technique. It consists of a painted, drawn, or taped visual line that robots detect. Optical lines are simple, but surface dirt and light conditions can make this unreliable. How it is detected falls into a couple of major categories:</p>
			<ul>
				<li><strong class="bold">Detected with light sensors</strong>: In this case, we'd attach small sensors to a robot's <a id="_idIndexMarker877"/>underside close to the line. They are tuned to output a binary on/off signal or analog signal. They usually have lights to shine off the surface. These are small and cheap but require extra I/O.</li>
				<li><strong class="bold">Detected with a camera</strong>: This will save space if you already use a camera, along <a id="_idIndexMarker878"/>with I/O pins. It saves complexity in mounting them and wiring them. However, it comes at a trade-off cost of software complexity, as your robot needs computer vision algorithms to analyze this.</li>
			</ul>
			<p><strong class="bold">Magnetic line following</strong> is used <a id="_idIndexMarker879"/>when the line <a id="_idIndexMarker880"/>needs to be protected against the elements. Also, for some variations of this, you can guide a robot on multiple paths. There are <a id="_idIndexMarker881"/>the following variants:</p>
			<ul>
				<li>Running a magnetic strip along a floor allows Hall-effect sensors (such as the magnetometer in <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>) to detect where the strip is. A series of these sensors can determine the direction of a line and follow it. This can be easier to alter than painting a line but can be a trip hazard.</li>
				<li>Running a wire with some current through it along or under a floor will achieve the same effect. With multiple wires and some different circuits for them, systems can steer a robot onto different paths.</li>
				<li>Concealing the line under a floor removes the trip hazard but means that you need to paint warnings for humans on paths that industrial robots follow.</li>
			</ul>
			<p>Now, you have <a id="_idIndexMarker882"/>seen the two major types of line following; it's worth giving an honorable mention to some other ways to determine a robot's path in the real world:</p>
			<ul>
				<li><strong class="bold">Beacons</strong>: Ultrasonic, light-emitting, or radio-emitting beacons can be placed around an <a id="_idIndexMarker883"/>environment to determine the path of a robot. These could just be reflectors for laser or other light. </li>
				<li><strong class="bold">Visual clues</strong>: If you <a id="_idIndexMarker884"/>place QR codes or other visible markers on walls and posts, they can encode an exact position.</li>
			</ul>
			<p>You've seen how a robot can perform line sensing with visible lines and hidden lines, such as wires under a floor and magnetic sensors. Because it is easier, we will use visible lines. </p>
			<p>The simple optical sensors require additional wiring, but if we already have a camera capable of this, why not make use of it?</p>
			<p>In this chapter, we will be focusing on using the camera we already have with a visual track and following the lines there. We will accept the code complexity while simplifying the hardware aspects of our robot.</p>
			<p>Now that you have some idea of the different types of line following and where to use them, let's create a test track that our robot can follow.</p>
			<h1 id="_idParaDest-349"><a id="_idTextAnchor321"/>Making a line-follower test track</h1>
			<p>Since you will be making your robot follow a line, we need to start with a section of line to follow. The track <a id="_idIndexMarker885"/>will be used at the beginning to test our line detection algorithm and can then be extended to more exciting tracks when we turn on the motors and start driving along the line. What I will show you in this section is easy to make and extendable. It allows you to experiment with different line shapes and curves and see how the robot responds.</p>
			<p>You can even experiment with different color and contrast options.</p>
			<h2 id="_idParaDest-350"><a id="_idTextAnchor322"/>Getting the test track materials in place</h2>
			<p>The following <a id="_idIndexMarker886"/>photo shows the main materials required:</p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="Images/B15660_14_02.jpg" alt="" width="509" height="609"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – Materials for making a test track</p>
			<p>The photo in <em class="italic">Figure 14.2</em> shows a roll of black electrical tape on a large sheet of white paper. For this section, you'll need the following:</p>
			<ul>
				<li>Some A2 plain white paper or board.</li>
				<li>Some black electrical insulation tape or painter's tape. Make sure this tape is opaque.</li>
				<li>A pair of scissors.</li>
			</ul>
			<p>You could replace the paper with boards if they are white-painted.</p>
			<p>You can also swap things around by using dark or black paper and white tape. This tape must not be see-through so that it makes a good strong contrast against the background. </p>
			<h2 id="_idParaDest-351"><a id="_idTextAnchor323"/>Making a line</h2>
			<p>Lay the <a id="_idIndexMarker887"/>sheet of paper flat. Then, make a line along the middle of the paper with the tape:</p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="Images/B15660_14_03.jpg" alt="" width="912" height="448"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.3 – Smoothing the tape on the paper</p>
			<p>The photos in <em class="italic">Figure 14.3</em> show the paper with the tape line and me smoothing the tape with my finger. Be sure to smooth the tape down. You do not need to worry about making it perfectly straight, as the whole point of this system is to follow lines even when they curve.</p>
			<p>Once you have a few lengths of this tape on sheet, why not make a few interesting pieces, such as the ones in the following figure:</p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="Images/B15660_14_04.jpg" alt="" width="1264" height="267"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – Some different shapes adjoining a straight line</p>
			<p>As <em class="italic">Figure 14.4</em> shows, you can experiment with curves and intentionally not-quite-straight lines. You can join these together with the straight lines to make whole sections, like a robotic <a id="_idIndexMarker888"/>train set! These will be fun to test with later as you further tune and play with the line-following code. </p>
			<p>Now that we have a test track ready, we can think about how we can visually process the line.</p>
			<h1 id="_idParaDest-352"><a id="_idTextAnchor324"/>Line-following computer vision pipeline</h1>
			<p>As we did <a id="_idIndexMarker889"/>with the previous computer vision tasks, we will visualize this as a pipeline. Before we do, there are many methods for tracking a line with computer vision. </p>
			<h2 id="_idParaDest-353"><a id="_idTextAnchor325"/>Camera line-tracking algorithms</h2>
			<p>It is in our interests to pick one of the simplest ones, but as always, there is a trade-off, in that <a id="_idIndexMarker890"/>others will cope with more tricky situations or anticipate curves better than ours.</p>
			<p>Here is a small selection of methods we could use:</p>
			<ul>
				<li><strong class="bold">Using edge detection</strong>: An edge detection algorithm, such as the Canny edge detector, can be run across the image, turning any transitions it finds into edges. OpenCV has a built-in edge detection system if we wanted to use this. The system can detect dark-to-light and light-to-dark edges. It is more tolerant of less sharp edges.</li>
				<li><strong class="bold">Finding differences along lines</strong>: This is like cheeky edge detection, but only on a particular row. By finding the difference between each pixel along a row in the image, any edges will show significant differences. It's simpler and cheaper than the Canny algorithm; it can cope with edges going either way but requires sharp contrasts.</li>
				<li><strong class="bold">Finding brightness and using a region over an absolute brightness as the line</strong>: This is very cheap but a little too simplistic to give good results. It's not tolerant to inversions but isn't tracking edges, so doesn't need sharp contrasts. </li>
			</ul>
			<p>Using one of the preceding three methods, you can find the line in one picture area and simply aim at that. This means you won't be able to pre-empt course changes. It is the easiest way. The chosen area could be a single row near the bottom of the screen.</p>
			<p>Alternatively, you can use the preceding methods to detect the line throughout the camera image and make a trajectory for it. This is more complex but better able to cope with steeper turns.</p>
			<p>It's worth <a id="_idIndexMarker891"/>noting that we could make a more efficient but more complicated algorithm using the raw YUV data from the Pi Camera. For simplicity, we will stick to the simple one. As you trade further up in complexity and understanding, you can find far faster and more accurate methods for this.</p>
			<p>Another major limitation of our system is the view width of the camera. You could use lenses to let a camera take in a wide visual field so that the robot will not lose the line so often.</p>
			<p>The method we will use is finding the differences along lines due to its simplicity and ability to cope with different line colors. We are also going to simply look along a single column, which results in more straightforward math.</p>
			<h2 id="_idParaDest-354"><a id="_idTextAnchor326"/>The pipeline</h2>
			<p>We can visualize the way we process data as a pipeline. Before we can, let's quickly explain <a id="_idIndexMarker892"/>discrete differences. The brightness of each pixel is a number between 0 (black) and 255 (white). To get the difference, you subtract each pixel from the pixel to the right of it:</p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="Images/B15660_14_05.jpg" alt="" width="352" height="412"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.5 – Discrete differences between pixels</p>
			<p>In <em class="italic">Figure 14.5</em>, there <a id="_idIndexMarker893"/>are six sets of pixels in varying shades:</p>
			<ol>
				<li>The first shows two white pixels. There is no difference between them. </li>
				<li>Where there is a gray pixel followed by a white one, it produces a small difference. </li>
				<li>A black pixel followed by a white pixel produces a large difference.</li>
				<li>A white pixel followed by a black pixel produces a large negative difference.</li>
				<li>A black pixel followed by a black pixel will produce no difference.</li>
				<li>A gray pixel followed by a black pixel will produce a small negative difference.</li>
			</ol>
			<p>It should be easy to see that a contrasting line edge will produce the largest differences, positive or negative. Our code will look for these.</p>
			<p>The following diagram shows how we process camera data for this method:</p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="Images/B15660_14_06.jpg" alt="" width="613" height="371"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.6 – Image processing pipeline for finding a line</p>
			<p>In <em class="italic">Figure 14.6</em>, we show the process of finding a line to follow. It starts with the <strong class="bold">camera</strong>, from which we <strong class="bold">capture images</strong> at a 320-by-240-pixel resolution. The next step in the pipeline is to <strong class="bold">convert to grayscale</strong> – we are only interested in brightness right now.</p>
			<p>Because <a id="_idIndexMarker894"/>images can have noise or grain, we <strong class="bold">blur</strong> it; this is not strictly necessary and depends on how clear the environment you are taking pictures from is. </p>
			<p>We take that image and <strong class="bold">slice out a candidate row</strong>; this shouldn't be too high in the picture, as that line may be too far away or there may be random things above the horizon depending on the camera position. The row shouldn't be too low as it will then be too close to the robot for it to react in time. Above the <strong class="bold">slice out candidate row</strong> box is an example showing the sliced-out row and the image it came from.</p>
			<p>We then treat this row as a set of numbers and <strong class="bold">get the discrete difference across</strong> them. The graph above the <strong class="bold">discrete difference</strong> box shows a large negative spike as the row goes from light gray to black, followed by a large positive spike as the row goes from black to light gray again. Notice that much of the graph shows a line along zero as patches of color have no difference.</p>
			<p>The next step is to <strong class="bold">find the maximum and minimum positions</strong>, specifically where in the row <a id="_idIndexMarker895"/>they are. We want the position/index of the highest point above zero and the lowest point below zero. We now know where the boundaries of our line probably are.</p>
			<p>We can <strong class="bold">find the position between</strong> these boundaries to find the center of the line, by adding them together and dividing by 2; this would be an <em class="italic">X</em> position of the line relative to the middle of the camera image.</p>
			<p>Now, you've seen the pipeline with some test images. It's time to get some test images of your own and try this algorithm out with some code.</p>
			<h1 id="_idParaDest-355"><a id="_idTextAnchor327"/>Trying computer vision with test images</h1>
			<p>In this section, we will <a id="_idIndexMarker896"/>look out how and why <a id="_idIndexMarker897"/>to use test images. We will write our first chunk of code for this behavior and try it on test images from our robot's camera. These tests will prepare us for using the code to drive the robot.</p>
			<h2 id="_idParaDest-356"><a id="_idTextAnchor328"/>Why use test images?</h2>
			<p>So far, our <a id="_idIndexMarker898"/>computer vision work has been written directly with robot behaviors; this is the end goal of them, but sometimes, you want to try the visual processing code in isolation. </p>
			<p>Perhaps you want to get it working or work out bugs in it, or you may want to see whether you can make the code faster and time it. To do this, it makes sense to run that particular code away from the robot control systems. </p>
			<p>It also makes sense to use test images. So, instead of running the camera and needing light conditions, you can run with test images you've already captured and compare them against the result you expected from them.</p>
			<p>For performance testing, trying the same image 100 times or the same set of images will give consistent results for performance measures to be meaningful. Avoid using new data every time, as these could result in unexpected or potentially noisy results. However, adding new test images to see what would happen is fascinating.</p>
			<p>So now that we know why we use them, let's try capturing some test images.</p>
			<h2 id="_idParaDest-357"><a id="_idTextAnchor329"/>Capturing test images</h2>
			<p>You may recall, from the previous chapter, using <strong class="source-inline">raspistill</strong> to capture an image. We are going to <a id="_idIndexMarker899"/>do the same here. First, we want to put our camera into a new position, facing down, so we are looking down onto the line.</p>
			<p>This section requires the setup from the chapter <a href="B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283"><em class="italic">Chapter 13</em></a>, <em class="italic">Robot Vision – Using a Pi Camera and OpenCV</em> and code from <a href="B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Programming RGB Strips in Python</em>.</p>
			<p>Turn the motor power on to the Raspberry Pi, then with an <strong class="source-inline">ssh</strong> session into the Raspberry Pi on the robot, type the following:</p>
			<ol>
				<li value="1">We start Python by typing <strong class="source-inline">python3</strong>:<p class="source-code"><strong class="bold">pi@myrobot:~ $ python3</strong></p><p class="source-code"><strong class="bold">Python 3.7.3 (default, Dec 20 2019, 18:57:59) </strong></p><p class="source-code"><strong class="bold">[GCC 8.3.0] on linux</strong></p><p class="source-code"><strong class="bold">Type "help", "copyright", "credits" or "license" for more information.</strong></p><p class="source-code"><strong class="bold">&gt;&gt;&gt; </strong></p></li>
				<li>Now, we need to import our robot object and create it so that we can interact with it:<p class="source-code"><strong class="bold">&gt;&gt;&gt; import robot</strong></p></li>
				<li>Let's create the robot object:<p class="source-code"><strong class="bold">&gt;&gt;&gt; r = robot.Robot()</strong></p></li>
				<li>Now, use this to set the pan servo to the middle:<p class="source-code"><strong class="bold">&gt;&gt;&gt; r.set_pan(0)</strong></p><p>The pan servo should center the camera.</p></li>
				<li>Next, we set the tilt servo to face down to look at the line:	<p class="source-code"><strong class="bold">&gt;&gt;&gt; r.set_tilt(90)</strong></p><p>The servo should look straight down here. It should not be straining or clicking.</p></li>
				<li>Now, you can exit Python (and release the motors) by pressing <em class="italic">Ctrl</em> + <em class="italic">D</em>.<p>The camera is facing downward. You can now turn off the motor switch and put this robot onto your test track. Try to position the robot so that the camera is right above the line.</p></li>
				<li>In the <strong class="source-inline">ssh</strong> terminal, type the following to capture a test image:<p class="source-code"><strong class="bold">$ raspistill -o line1.jpg</strong></p></li>
			</ol>
			<p>You can <a id="_idIndexMarker900"/>now download this image to your PC using FileZilla, as discussed in the book's earlier chapters. The next figure shows a test image, also used for the preceding examples:</p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="Images/B15660_14_07.jpg" alt="" width="320" height="240"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.7 – A test image of a line</p>
			<p><em class="italic">Figure 14.7</em> shows one of my test images. Note that the line is roughly starting in the middle of the picture, but it isn't exact and doesn't need to be. Note also that the lighting is a bit rough and is creating shadows. These are worth watching out for as they could confuse the system.</p>
			<p>Capture a few images of the line at different angles to the robot and slightly left or slightly right of the camera.</p>
			<p>Now that we have test images, we can write code to test them with!</p>
			<h2 id="_idParaDest-358"><a id="_idTextAnchor330"/>Writing Python to find the edges of the line</h2>
			<p>We are <a id="_idIndexMarker901"/>ready to start writing code, using our test images and the preceding pipeline diagram. We can make the results quite visual so that we can see what the algorithm is doing.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In computer vision, it's useful to use the lowest resolution you can to do the job. Every additional pixel adds more memory and processing to cope with. At 320*200, this is 76,800 pixels. The Raspberry Pi camera can record at 1920 x 1080 – 2,073,600 pixels – 27 times as much data! We need this to be quick, so we keep the resolution low.</p>
			<p>The code in this section will run on the Raspberry Pi, but you can also run it on a PC with Python 3, NumPy, Matplotlib, and Python OpenCV installed:</p>
			<ol>
				<li value="1">Create a file called <strong class="source-inline">test_line_find.py</strong>.</li>
				<li>We will need to import NumPy to process the image numerically, OpenCV to manipulate the image, and Matplotlib to graph the results:<p class="source-code"><strong class="bold">import cv2</strong></p><p class="source-code"><strong class="bold">import numpy as np</strong></p><p class="source-code"><strong class="bold">from matplotlib import pyplot as plt</strong></p></li>
				<li>Now, we load the image. OpenCV can load <strong class="source-inline">jpg</strong> images, but if there is a problem in doing so, it produces an empty image. So, we need to check that it loaded something:<p class="source-code"><strong class="bold">image = cv2.imread("line1.jpg")</strong></p><p class="source-code"><strong class="bold">assert image is not None, "Unable to read file"</strong></p><p>I am assuming the image is called <strong class="source-inline">line1.jpg</strong> and is in the same directory that we will run this file from.</p></li>
				<li>The captured image will be at the large default resolution of the camera. To keep this fast, we resize it to a smaller image:<p class="source-code"><strong class="bold">resized = cv2.resize(image, (320, 240))</strong></p></li>
				<li>We also only want grayscale; we aren't interested in the other colors for this exercise:<p class="source-code"><strong class="bold">gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)</strong></p></li>
				<li>Now, we'll pick out the row; for now, we'll use 180 as that is fairly low on an image with a height of 240. The images are stored such that row 0 is the top. Note that we are telling NumPy to convert this into an <strong class="source-inline">int32</strong> type:<p class="source-code"><strong class="bold">row = gray[180].astype(np.int32)</strong></p><p>We convert to <strong class="source-inline">int32</strong> with a sign (plus or minus) so that our differences can be negative.</p></li>
				<li>We can <a id="_idIndexMarker902"/>get a list of differences for every pixel of this row. NumPy makes this easy:<p class="source-code"><strong class="bold">diff = np.diff(row)</strong></p></li>
				<li>We are going to plot this <strong class="source-inline">diff</strong> list. We will need the <em class="italic">x</em>-axis to be the pixel number; let's create a NumPy range from 0 to that range:<p class="source-code"><strong class="bold">x = np.arange(len(diff))</strong></p></li>
				<li>Let's plot the <strong class="source-inline">diff</strong> variable against the pixel index (<strong class="source-inline">x</strong>), and save the result:<p class="source-code"><strong class="bold">plt.plot(x, diff)</strong></p><p class="source-code"><strong class="bold">plt.savefig("not_blurred.png")</strong></p><p>You'll note that I've called this file <strong class="source-inline">not_blurred</strong>. This is because we've not added the optional blurring step. With the graph, we'll be able to see the difference. </p></li>
			</ol>
			<p>Pointing at my test picture, I get the following graph:</p>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="Images/B15660_14_08.jpg" alt="" width="563" height="421"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.8 – Graph of differences without blurring</p>
			<p>The graph in <em class="italic">Figure 14.8</em> has the column number as the <em class="italic">x</em> axis and the difference as the <em class="italic">y</em> axis. The line has a lot of noise in it. There are two distinct peaks – one below the zero line at <a id="_idIndexMarker903"/>around column 145 and one above the line at around 240. The noise here doesn't affect this too much as the peaks are very distinct:</p>
			<ol>
				<li value="1">Let's try adding the blurring to see how that changes things. Make the following change to the code. The bold areas show changed sections:<p class="source-code">gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)</p><p class="source-code"><strong class="bold">blurred = cv2.blur(gray, (5, 5))</strong></p><p class="source-code">row = <strong class="bold">blurred</strong>[180].astype(np.int32)</p><p class="source-code">diff = np.diff(row)</p><p>In this code, we add the additional blurring step, blurring 5-by-5 chunks a little. </p></li>
				<li>So that we can see different graphs, let's change the name of our output file:<p class="source-code">plt.savefig("<strong class="bold">blurred</strong>.png")</p></li>
			</ol>
			<p>Blurring <a id="_idIndexMarker904"/>a little should reduce noise without affecting our sharp peaks too much. Indeed, the following figure shows how effective this is:</p>
			<div>
				<div id="_idContainer253" class="IMG---Figure">
					<img src="Images/B15660_14_09.jpg" alt="" width="570" height="424"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.9 – The diff graph after blurring</p>
			<p>The graph in <em class="italic">Figure 14.9</em> is similar to <em class="italic">Figure 14.8</em>. The axes are the same, and it has the same peaks. However, there is far less noise around the line at 0, showing that blurring makes the difference clearer. The question about this will be whether it changes the outcome. Looking at the position and size of the peaks, I would say not so much. So, we can leave it out of the final follow for a little extra speed. Every operation will cost a little time.</p>
			<p>Now <a id="_idIndexMarker905"/>that we have the two peaks, let's use them to find the location of the line.</p>
			<h2 id="_idParaDest-359"><a id="_idTextAnchor331"/>Locating the line from the edges</h2>
			<p>Those peaks <a id="_idIndexMarker906"/>are markers of our line edges. To find the middle of something, you add its left and right coordinates, then divide them by 2:</p>
			<ol>
				<li value="1">First, we must pick up the coordinates. Let's write code to ask for the maximum and minimum. We'll add this code between the analysis code and the chart output code:<p class="source-code">diff = np.diff(row)</p><p class="source-code"><strong class="bold">max_d = np.amax(diff, 0)</strong></p><p class="source-code"><strong class="bold">min_d = np.amin(diff, 0)</strong></p><p>This code finds the values of the array maximum and minimum. I've called them <strong class="source-inline">min_d</strong> and <strong class="source-inline">max_d</strong>, abbreviating the difference as <strong class="source-inline">d</strong>. Note that they cannot be called <strong class="source-inline">min</strong> and <strong class="source-inline">max</strong> as those names already belong to Python.</p></li>
				<li>These are values, but not locations. We now need to find the index of the locations. NumPy has an <strong class="source-inline">np.where</strong> function to get indexes from arrays:<p class="source-code"><strong class="bold">highest = np.where(diff == max_d)[0][0]</strong></p><p class="source-code"><strong class="bold">lowest = np.where(diff == min_d)[0][0]</strong></p><p>NumPy's <strong class="source-inline">where</strong> function returns an array of answers for each dimension – so, although <strong class="source-inline">diff</strong> is a one-dimensional array, we will still get a list of lists. The first <strong class="source-inline">[0]</strong> selects this first dimension's results list, and the second <strong class="source-inline">[0]</strong> selects the first item in the results. Multiple results mean it's found more than one peak, but we assume that there's only one for now.</p></li>
				<li>To find the middle, we need to add these together and divide them by 2:<p class="source-code"><strong class="bold">middle = (highest + lowest) // 2</strong></p></li>
				<li>Now that we have found it, we should display it in some way. We can plot this on our graph with three lines. Matplotlib can specify the color and style for a plot. Let's start with the middle line:<p class="source-code"><strong class="bold">plt.plot([middle, middle], [max_d, min_d], "r-")</strong></p><p>The line is specified as a pair of <em class="italic">X</em> coordinates and a pair of <em class="italic">Y</em> coordinates, namely because Matplotlib expects data series. We use <strong class="source-inline">max_d</strong> and <strong class="source-inline">min_d</strong> for the Y coordinates, so the line draws from the highest peak to the lowest. The <strong class="source-inline">r-</strong> style specifier means to draw a solid red line.</p></li>
				<li>We can <a id="_idIndexMarker907"/>do the same for the <strong class="source-inline">highest</strong> and <strong class="source-inline">lowest</strong> locations, this time using <strong class="source-inline">g--</strong> for a green dashed line: <p class="source-code"><strong class="bold">plt.plot([lowest, lowest], [max_d, min_d], "g--")</strong></p><p class="source-code"><strong class="bold">plt.plot([highest, highest], [max_d, min_d], "g--")</strong></p></li>
				<li>As we did for blurring, let's change the name of the output graph so that we can compare them:<p class="source-code">plt.savefig("<strong class="bold">located_lines</strong>.png")</p></li>
			</ol>
			<p>Running this should output the following figure:</p>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<img src="Images/B15660_14_10.jpg" alt="" width="565" height="420"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.10 – Graph showing the highest, lowest, and middle line</p>
			<p>The graph in <em class="italic">Figure 14.10</em> shows that we have found the middle line and the two nice clear <a id="_idIndexMarker908"/>peaks. This code looks usable for our robot.</p>
			<p>However, what happens when things are not so clear?</p>
			<h2 id="_idParaDest-360"><a id="_idTextAnchor332"/>Trying test pictures without a clear line</h2>
			<p>Let's see <a id="_idIndexMarker909"/>what our line-finding code does with a very different test picture. We will see what happens here, so we aren't so surprised by how the robot will behave and weed out some simple bugs.</p>
			<p>For example, what about putting our line on a very noisy surface, such as a carpet? Or how about the paper without a line, or the carpet without a line?</p>
			<div>
				<div id="_idContainer255" class="IMG---Figure">
					<img src="Images/B15660_14_11.jpg" alt="" width="1080" height="780"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.11 – diff graphs under noisier conditions</p>
			<p>With a set of graphs such as those in <em class="italic">Figure 14.11</em>, we learn much about our system. </p>
			<p>The top three images show the original photos. The next three graphs show what those images look like <a id="_idIndexMarker910"/>when finding the difference and middles without blurring. The bottom three graphs show what happens when enabling the blur.</p>
			<p>First, when things get as noisy as the first image (and this is pushing it past what line following should cope with), the blur makes the difference between finding the line and a random artifact; although, in the second graph, a random artifact with a similar downward peak size was a close contender. In this case, making a larger <em class="italic">Y</em> blur might smooth out that artifact, leaving only the line.</p>
			<p>Looking closely, the scale of those graphs is also not the same. The plain paper graph measures a difference with peaks of +10/-10 without blurring, and +1/-1 with blurring. So, when the differences are that low, should we even be looking for a peak? The story is similar in the carpet-only graphs.</p>
			<p>We can make a few changes to our system to make it consider these as not-lines. The simplest is to add a condition that filters out a minimum above -5 and a maximum below 10. I say -5 since this would otherwise filter out the line in the first graph completely. However, a larger blur area might help with that.</p>
			<p>Depending <a id="_idIndexMarker911"/>on the noisiness of the conditions, we will want to enable the blur. On a nicely lit track, the blur is probably not needed.</p>
			<p>The next figure shows our line on the carpet, with a blur set to (5, 40), blurring further between rows and filtering out noise further:</p>
			<div>
				<div id="_idContainer256" class="IMG---Figure">
					<img src="Images/B15660_14_12.jpg" alt="" width="843" height="420"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.12 – The line on carpet with a larger blur</p>
			<p>The graph in <em class="italic">Figure 14.12</em> has far less noise than before, with the blur smoothing out noise spikes a lot, while the actual line spikes remain. We would only want to do this in a noisy environment, as it risks being slower.</p>
			<p>As you can see, testing the code on test images has allowed us to learn a lot about the system. By taking the same pictures and trying different parameters and pipeline changes, you can <a id="_idIndexMarker912"/>optimize this for different scenarios. As you experiment more with computer vision, make this a habit.</p>
			<p>Now we have tried our visual processing code on test images, it's time to put it on a robot behavior!</p>
			<h1 id="_idParaDest-361"><a id="_idTextAnchor333"/>Line following with the PID algorithm</h1>
			<p>In this section, we will combine the visual processing seen previously with the PID control <a id="_idIndexMarker913"/>loops and camera streaming seen in <a href="B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283"><em class="italic">Chapter 13</em></a>, <em class="italic">Robot Vision – Using a Pi Camera and OpenCV</em>. Please start from the code in that chapter.</p>
			<p>The files you will need are as follows:</p>
			<ul>
				<li><strong class="source-inline">pid_controller.py</strong></li>
				<li><strong class="source-inline">robot.py</strong></li>
				<li><strong class="source-inline">servos.py</strong></li>
				<li><strong class="source-inline">camera_stream.py</strong></li>
				<li><strong class="source-inline">image_app_core.py</strong></li>
				<li><strong class="source-inline">leds_led_shim.py</strong></li>
				<li><strong class="source-inline">encoder_counter.py</strong></li>
				<li>The templates folder</li>
			</ul>
			<p>We will use the same template for displaying this, but we are going to add a quick and cheeky way of rendering the <strong class="source-inline">diff</strong> graphs in OpenCV onto our output frame. Matplotlib would be too slow for this.</p>
			<h2 id="_idParaDest-362"><a id="_idTextAnchor334"/>Creating the behavior flow diagram</h2>
			<p>Before we build a new behavior, creating a data flow diagram will help us get a picture of what <a id="_idIndexMarker914"/>happens to the data after we've processed it.</p>
			<p>The system will look familiar, as it is very similar to those we made in <a href="B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283"><em class="italic">Chapter 13</em></a>, <em class="italic">Robot Vision – Using a Pi Camera and OpenCV</em>. Take a look at the following figure:</p>
			<div>
				<div id="_idContainer257" class="IMG---Figure">
					<img src="Images/B15660_14_13_NEW.jpg" alt="" width="1594" height="1002"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.13 – The line-following behavior</p>
			<p>In <em class="italic">Figure 14.13</em>, we have camera images going through to the <strong class="bold">get line from image</strong> block. This block outputs an object's <em class="italic">X</em> position (the middle of the line), which goes to our PID. Note that image data also goes from the get line to the image queue so that you can see these in a browser.</p>
			<p>The PID control also takes a reference middle point, where the middle of the camera should be. It uses the error between these to calculate the offset and uses that to drive the motors.</p>
			<p>The figure shows the motors with a feedback line to the camera, as the indirect effects of moving those are that the view changes, so we will see a different line.</p>
			<p>Before that, we are going to make our PID controller a little smarter again.</p>
			<h2 id="_idParaDest-363"><a id="_idTextAnchor335"/>Adding time to our PID controller</h2>
			<p>Our robot behaviors have involved processing frames, then sending error data to the PID whenever <a id="_idIndexMarker915"/>the process has completed a cycle. There is much going on in a cycle, and that timing might vary. When we create the integral, we have been adding the data as if the time was constant. For a somewhat more accurate picture, we should be multiplying that by the time:</p>
			<ol>
				<li value="1">Open up the <strong class="source-inline">pid_controller.py</strong> file.</li>
				<li>In the <strong class="source-inline">handle_integral</strong> method, change the parameters to take <strong class="source-inline">delta_time</strong>:<p class="source-code">    def handle_integral(self, error<strong class="bold">, delta_time</strong>):</p></li>
				<li>We will then use this when adding in the integral term:<p class="source-code">            self.integral_sum += error<strong class="bold"> * delta_time</strong></p></li>
				<li>We usually update the PID with the <strong class="source-inline">get_value</strong> method; however, since we already have code using this, we should make it behave as it did before for them. To do this, we will add a <strong class="source-inline">delta_time</strong> parameter but with a default value of <strong class="source-inline">1</strong>:<p class="source-code">    def get_value(self, error<strong class="bold">, delta_time=1</strong>):</p></li>
				<li>When this <strong class="source-inline">get_value</strong> method calls <strong class="source-inline">handle_integral</strong>, it should always pass the new <strong class="source-inline">delta_time</strong> parameter:<p class="source-code">        p = self.handle_proportional(error)</p><p class="source-code">        i = self.handle_integral(error<strong class="bold">, delta_time</strong>)</p><p class="source-code">        logger.debug(f"P: {p}, I: {i:.2f}")</p><p class="source-code">        return p + i</p></li>
			</ol>
			<p>While this was not a big change, it will mean we can account for time variations between updates to the PID code.</p>
			<p>We can now use this in our behavior.</p>
			<h2 id="_idParaDest-364"><a id="_idTextAnchor336"/>Writing the initial behavior</h2>
			<p>We can <a id="_idIndexMarker916"/>take all the elements we have and combine them to create our line-following behavior: </p>
			<ol>
				<li value="1">Create a file named <strong class="source-inline">line_follow_behavior.py</strong>.</li>
				<li>Start this with imports for <strong class="source-inline">image_app_core</strong>, NumPy, OpenCV, the camera stream, the PID controller, and the robot. We also have <strong class="source-inline">time</strong>, so we can later compute the delta time:<p class="source-code"><strong class="bold">import time</strong></p><p class="source-code"><strong class="bold">from image_app_core import start_server_process, get_control_instruction, put_output_image</strong></p><p class="source-code"><strong class="bold">import cv2</strong></p><p class="source-code"><strong class="bold">import numpy as np</strong></p><p class="source-code"><strong class="bold">import camera_stream</strong></p><p class="source-code"><strong class="bold">from pid_controller import PIController</strong></p><p class="source-code"><strong class="bold">from robot import Robot</strong></p></li>
				<li>Let's make the behavior class. The constructor, as before, takes the robot:<p class="source-code"><strong class="bold">class LineFollowingBehavior:</strong></p><p class="source-code"><strong class="bold">    def __init__(self, robot):</strong></p><p class="source-code"><strong class="bold">        self.robot = robot</strong></p></li>
				<li>Now, we need variables in the constructor to track our behavior. First, we should set the row we will look for the differences in and a threshold (under which we will not consider it a line):<p class="source-code"><strong class="bold">        self.check_row = 180</strong></p><p class="source-code"><strong class="bold">        self.diff_threshold = 10</strong></p></li>
				<li>As with our previous camera behaviors, we have a set point for the center, a variable to say whether the motors should be running, and a speed to go forward at:<p class="source-code"><strong class="bold">        self.center = 160</strong></p><p class="source-code"><strong class="bold">        self.running = False</strong></p><p class="source-code"><strong class="bold">        self.speed = 60</strong></p></li>
				<li>We are going to make some interesting displays. We will store the colors we plan to use here too – a green crosshair, red for the middle line, and light blue for the graph. These are BGR as OpenCV expects that:<p class="source-code"><strong class="bold">        self.crosshair_color = [0, 255, 0]</strong></p><p class="source-code"><strong class="bold">        self.line_middle_color = [128, 128, 255]</strong></p><p class="source-code"><strong class="bold">        self.graph_color = [255, 128, 128]</strong></p><p>That is the constructor complete for the behavior.</p></li>
				<li>Now, we need <a id="_idIndexMarker917"/>the control to say whether the system is running or should exit. This code should be familiar as it is similar to the other camera control behaviors:<p class="source-code"><strong class="bold">    def process_control(self):</strong></p><p class="source-code"><strong class="bold">        instruction = get_control_instruction()</strong></p><p class="source-code"><strong class="bold">        if instruction:</strong></p><p class="source-code"><strong class="bold">            command = instruction['command']</strong></p><p class="source-code"><strong class="bold">            if command == "start":</strong></p><p class="source-code"><strong class="bold">                self.running = True</strong></p><p class="source-code"><strong class="bold">            elif command == "stop":</strong></p><p class="source-code"><strong class="bold">                self.running = False</strong></p><p class="source-code"><strong class="bold">            if command == "exit":</strong></p><p class="source-code"><strong class="bold">                print("Stopping")</strong></p><p class="source-code"><strong class="bold">                exit()</strong></p></li>
				<li>Next, we'll make the <strong class="source-inline">run</strong> method, which will perform the main PID loop and drive the robot. We are setting the tilt servo to <strong class="source-inline">90</strong> and the pan servo to <strong class="source-inline">0</strong>, so it is looking straight down. We'll set up the camera too:<p class="source-code"><strong class="bold">    def run(self):</strong></p><p class="source-code"><strong class="bold">        self.robot.set_pan(0)</strong></p><p class="source-code"><strong class="bold">        self.robot.set_tilt(90)</strong></p><p class="source-code"><strong class="bold">        camera = camera_stream.setup_camera()</strong></p></li>
				<li>Now, we set up the PID for the direction. These values aren't final and may need tuning. We have a low proportional value as the directional error can be quite large compared with the motor speeds:<p class="source-code"><strong class="bold">        direction_pid = PIController( proportional_constant=0.4, integral_constant=0.01, windup_limit=400)</strong></p></li>
				<li>We sleep for a second so that the camera can initialize and the servos reach their position: <p class="source-code"><strong class="bold">        time.sleep(1)</strong></p><p class="source-code"><strong class="bold">        self.robot.servos.stop_all()</strong></p><p class="source-code"><strong class="bold">        print("Setup Complete")</strong></p><p>We stop the servos so that they won't be pulling further power once they've reached position.</p></li>
				<li>Since <a id="_idIndexMarker918"/>we are going to be keeping track of time, we store the last time value here. The time is a floating-point number in seconds:<p class="source-code"><strong class="bold">        last_time = time.time()</strong></p></li>
				<li>We start the camera loop and feed the frame to a <strong class="source-inline">process_frame</strong> method (which we'll write shortly). We can also process a control instruction:<p class="source-code"><strong class="bold">        for frame in camera_stream.start_stream(camera):</strong></p><p class="source-code"><strong class="bold">            x, magnitude = self.process_frame(frame)</strong></p><p class="source-code"><strong class="bold">            self.process_control()</strong></p><p>From processing a frame, we expect to get an <em class="italic">X</em> value, and the magnitude is the difference between the highest and lowest value in the differences. The gap between the peaks helps detect whether it's actually a line and not just noise.</p></li>
				<li>Now, for the movement, we need to check that the robot is running and that the magnitude we found was bigger than the threshold:<p class="source-code"><strong class="bold">            if self.running and magnitude &gt; self.diff_threshold:</strong></p></li>
				<li>If so, we start the PID behavior:<p class="source-code"><strong class="bold">                direction_error = self.center – x</strong></p><p class="source-code"><strong class="bold">                new_time = time.time()</strong></p><p class="source-code"><strong class="bold">                dt = new_time - last_time</strong></p><p class="source-code"><strong class="bold">                direction_value = direction_pid.get_value(direction_error, delta_time=dt)</strong></p><p class="source-code"><strong class="bold">                last_time = new_time</strong></p><p>We calculate a direction error by subtracting what we got from the middle of the camera. We then get a new time so that we can calculate the difference in time, <strong class="source-inline">dt</strong>. This error and time delta are fed to the PID, getting a new value. So, we are ready for the next calculation: <strong class="source-inline">last_time</strong> now gets the <strong class="source-inline">new_time</strong> value.</p></li>
				<li>We <a id="_idIndexMarker919"/>now log this and use the value to change the heading of the robot. We set the motor speeds to our base speed, and then add/subtract the motors' PID output:<p class="source-code"><strong class="bold">                print(f"Error: {direction_error}, Value:{direction_value:2f}, t: {new_time}")</strong></p><p class="source-code"><strong class="bold">                self.robot.set_left(self.speed - direction_value)</strong></p><p class="source-code"><strong class="bold">                self.robot.set_right(self.speed + direction_value)</strong></p></li>
				<li>Now we've handled what happens when we have detected a line. What about when we don't? <strong class="source-inline">else</strong> stops the motors running and resets the PID, so it doesn't accumulate odd values:<p class="source-code"><strong class="bold">            else:</strong></p><p class="source-code"><strong class="bold">                self.robot.stop_motors()</strong></p><p class="source-code"><strong class="bold">                if not self.running:</strong></p><p class="source-code"><strong class="bold">                    direction_pid.reset()</strong></p><p class="source-code"><strong class="bold">                last_time = time.time()</strong></p><p>Notice how we are still keeping the last time up to date here. Otherwise, there would be a big gap between stops and starts, which would feed odd values into the PID.</p></li>
				<li>Next, we need to fill in what happens when we process a frame. Let's add our <strong class="source-inline">process_frame</strong> method:<p class="source-code"><strong class="bold">    def process_frame(self, frame):</strong></p><p class="source-code"><strong class="bold">        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</strong></p><p class="source-code"><strong class="bold">        blur = cv2.blur(gray, (5, 5))</strong></p><p class="source-code"><strong class="bold">        row = blur[self.check_row].astype(np.int32)</strong></p><p class="source-code"><strong class="bold">        diff = np.diff(row)</strong></p><p class="source-code"><strong class="bold">        max_d = np.amax(diff, 0)</strong></p><p class="source-code"><strong class="bold">        min_d = np.amin(diff, 0)</strong></p><p>This code should all look familiar; it is the code we made previously for our test code. </p></li>
				<li>We should test to see that our readings have put us on either side of the zero line, and that we found two different locations. The maximum should not be below zero, and the minimum should not be above it. If they fail this, stop here – the main loop will consider this not a line:<p class="source-code"><strong class="bold">        if max_d &lt; 0 or min_d &gt; 0:</strong></p><p class="source-code"><strong class="bold">            return 0, 0</strong></p></li>
				<li>We <a id="_idIndexMarker920"/>will find the locations on the row as we did before, along with their midpoint:<p class="source-code"><strong class="bold">        highest = np.where(diff == max_d)[0][0]</strong></p><p class="source-code"><strong class="bold">        lowest = np.where(diff == min_d)[0][0]</strong></p><p class="source-code"><strong class="bold">        middle = (highest + lowest) // 2</strong></p></li>
				<li>So that we can use it to determine that we've got a positive match, we'll calculate the magnitude of the difference between the min and max, making sure we aren't picking up something faint:<p class="source-code"><strong class="bold">        mag = max_d - min_d</strong></p></li>
				<li>We will want to display something useful to the user here. So, this method calls a <strong class="source-inline">make_display</strong> method, just like the other camera behaviors. We pass it some variables to plot onto that display:<p class="source-code"><strong class="bold">        self.make_display(frame, middle, lowest, highest, diff)</strong></p></li>
				<li>We then return the middle point and the magnitude:<p class="source-code"><strong class="bold">        return middle, mag</strong></p></li>
				<li>This code will drive our robot, but we'll have a hard time tuning it if we can't see what is going on. So, let's create the <strong class="source-inline">make_display</strong> method to handle that:<p class="source-code"><strong class="bold">    def make_display(self, frame, middle, lowest, highest, diff):</strong></p><p>The parameters here are the original <strong class="source-inline">frame</strong>, the <strong class="source-inline">middle</strong> position for the line, the <strong class="source-inline">lowest</strong> difference position in the line, the <strong class="source-inline">highest</strong> difference position, and <strong class="source-inline">diff</strong> as the whole difference row.</p></li>
				<li>The first thing we want in the display is the center reference. Let's make a crosshair about the center and the chosen row: <p class="source-code"><strong class="bold">        cv2.line(frame, (self.center - 4, self.check_row), (self.center + 4, self.check_row), self.crosshair_color)</strong></p><p class="source-code"><strong class="bold">        cv2.line(frame, (self.center, self.check_row - 4), (self.center, self.check_row + 4), self.crosshair_color)</strong></p></li>
				<li>Next, we show where we found the middle in another color:<p class="source-code"><strong class="bold">        cv2.line(frame, (middle, self.check_row - 8), (middle, self.check_row + 8), self.line_middle_color)</strong></p></li>
				<li>So <a id="_idIndexMarker921"/>that we can find it, we also plot the bars for the <strong class="source-inline">lowest</strong> and <strong class="source-inline">highest</strong> around it, in a different color again: <p class="source-code"><strong class="bold">        cv2.line(frame, (lowest, self.check_row - 4), (lowest, self.check_row + 4), self.line_middle_color)</strong></p><p class="source-code"><strong class="bold">        cv2.line(frame, (highest, self.check_row - 4), (highest, self.check_row + 4), self.line_middle_color)</strong></p></li>
				<li>Now, we are going to graph <strong class="source-inline">diff</strong> across a new empty frame. Let's make an empty frame – this is just a NumPy array:<p class="source-code"><strong class="bold">        graph_frame = np.zeros((camera_stream.size[1], camera_stream.size[0], 3), np.uint8)</strong></p><p>The array dimensions are rows then columns, so we swap the camera size <em class="italic">X</em> and <em class="italic">Y</em> values.</p></li>
				<li>We will then use a method to make a simple graph. We'll implement this further down. Its parameters are the frame to draw the graph into and the <em class="italic">Y</em> values for the graph. The simple graph method implies the <em class="italic">X</em> values as column numbers:<p class="source-code"><strong class="bold">        self.make_cv2_simple_graph(graph_frame, diff)</strong></p></li>
				<li>Now that we have the frame and the graph frame, we need to concatenate these, as we did for our frames in the color-detecting code:<p class="source-code"><strong class="bold">        display_frame = np.concatenate((frame, graph_frame), axis=1)</strong></p></li>
				<li>We can now encode these bytes and put them on the output queue:<p class="source-code"><strong class="bold">        encoded_bytes = camera_stream.get_encoded_bytes_for_frame(display_frame)</strong></p><p class="source-code"><strong class="bold">        put_output_image(encoded_bytes)</strong></p></li>
				<li>The next thing we will need to implement is this <strong class="source-inline">make_cv2_simple_graph</strong> method. It's a bit cheeky but draws lines between <em class="italic">Y</em> points along an <em class="italic">x</em> axis:<p class="source-code"><strong class="bold">    def make_cv2_simple_graph(self, frame, data):</strong></p></li>
				<li>We <a id="_idIndexMarker922"/>need to store the last value we were at, so the code plots the next value relative to this – giving a line graph. We start with item 0. We also set a slightly arbitrary middle <em class="italic">Y</em> point for the graph. Remember that we know the <strong class="source-inline">diff</strong> values can be negative:<p class="source-code"><strong class="bold">        last = data[0]</strong></p><p class="source-code"><strong class="bold">        graph_middle = 100</strong></p></li>
				<li>Next, we should enumerate the data to plot each item:<p class="source-code"><strong class="bold">        for x, item in enumerate(data):</strong></p></li>
				<li>Now, we can plot a line from the last item <em class="italic">Y</em> position to the current position on the next <em class="italic">X</em> location. Notice how we offset each item by that graph middle:<p class="source-code"><strong class="bold">            cv2.line(frame, (x, last + graph_middle), (x + 1, item + graph_middle), self.graph_color)</strong></p></li>
				<li>We then need to update the last item to this current one:<p class="source-code"><strong class="bold">            last = item</strong></p><p>Okay – nearly there; that will plot the graph on our frame. </p></li>
				<li>Our behavior is complete; we just need the outer code to run it! This code should also be similar to the previous camera examples:<p class="source-code"><strong class="bold">print("Setting up")</strong></p><p class="source-code"><strong class="bold">behavior = LineFollowingBehavior(Robot())</strong></p><p class="source-code"><strong class="bold">process = start_server_process('color_track_behavior.html')</strong></p><p class="source-code"><strong class="bold">try:</strong></p><p class="source-code"><strong class="bold">    behavior.run()</strong></p><p class="source-code"><strong class="bold">finally:</strong></p><p class="source-code"><strong class="bold">    process.terminate()</strong></p><p>Notice that we still use the <strong class="source-inline">color_track_behavior.html</strong> template here.</p></li>
			</ol>
			<p>You can <a id="_idIndexMarker923"/>now upload this to your robot. Then, switch the motors on and run it. Because this is web-based, point a browser at <strong class="source-inline">http://myrobot.local:5001</strong>.</p>
			<p>You should see the following:</p>
			<div>
				<div id="_idContainer258" class="IMG---Figure">
					<img src="Images/B15660_14_14.jpg" alt="" width="1262" height="649"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.14 – Screenshot of the line-following behavior output</p>
			<p>The screenshot in <em class="italic">Figure 14.14</em> shows the title above two pictures. On the left is the camera picture of the line. Drawn onto this frame is a green crosshair showing where the center point is. Also, there is a large red bar showing the middle of the line, and either side of this, two shorter red bars showing the sides of it. To the right is the graph plotting the intensity differences after blurring. The up peak and down peak are visible in the graph.</p>
			<p>Below <a id="_idIndexMarker924"/>this are the <strong class="bold">Start</strong>, <strong class="bold">Stop</strong>, and <strong class="bold">Exit</strong> buttons.</p>
			<p>Place the robot onto the line, with good lighting. If it looks like the preceding display, press the <strong class="bold">Start</strong> button to see it go. It should start shakily driving along the line.</p>
			<h2 id="_idParaDest-365"><a id="_idTextAnchor337"/>Tuning the PID</h2>
			<p>You can <a id="_idIndexMarker925"/>get a little bolder with trying to track curved lines and find its limit. The robot will sometimes overshoot or understeer, which is where the PID tuning comes in: </p>
			<ul>
				<li>If it seems to be turning far too slowly, try increasing the proportional constant a little. Conversely, if it is oversteering, try lowering the proportional constant a fraction.</li>
				<li>If it had a slight continuous error, try increasing the integral constant. </li>
			</ul>
			<p>PID tuning is a repeating process and requires a lot of patience and testing.</p>
			<h2 id="_idParaDest-366"><a id="_idTextAnchor338"/>Troubleshooting</h2>
			<p>If the <a id="_idIndexMarker926"/>behavior isn't quite working, please try the following steps:</p>
			<ul>
				<li>If the tilt servo doesn't look straight down when set to 90 degrees, it may not be calibrated correctly. Change the <strong class="source-inline">deflect_90_in_ms</strong> value parameter to the <strong class="source-inline">Servos</strong> object – increase in 0.1 increments to get this to 90 degrees.</li>
				<li>If it is having trouble getting a clear line, ensure that the lighting is adequate, that the surface it is on is plain, such as paper, and the line is well contrasting.</li>
				<li>If it is still struggling to find a line, increase the vertical blurring amount in steps of 5.</li>
				<li>If it's struggling to turn in time for the line, try reducing the speed in increments of 10.</li>
				<li>If you find the camera is wobbling horizontally, you can remove the <strong class="source-inline">self.robot.servos.stop_all()</strong> line from <strong class="source-inline">line_follow_behavior</strong>. Beware: this comes at the cost of motor battery life.</li>
				<li>If the robot is finding too much other random junk that isn't the line, try increasing <a id="_idIndexMarker927"/>the vertical blurring. Also, try increasing the threshold in steps of 1 or 2. The sharper the contrast in brightness, the less you should need to do this.</li>
				<li>Ensure that you double-check the code and that you have got the previous examples here and from <a href="B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283"><em class="italic">Chapter 13</em></a>, <em class="italic">Robot Vision – Using a Pi Camera and OpenCV</em>, to work.</li>
			</ul>
			<h1 id="_idParaDest-367"><a id="_idTextAnchor339"/>Finding a line again</h1>
			<p>An important thing to consider is what the robot should do if it has lost the line. Coming back to our examples of an industrial setting, this could be a safety measure. </p>
			<p>Our current robot stops. That requires you to put it back on the line. However, when you do so, the robot immediately starts moving again. This behavior is fine for our little robot, but it could be a dangerous hazard for a large robot.</p>
			<p>Another behavior that <a id="_idIndexMarker928"/>you could consider is to spin until the robot finds the line again. Losing a line can be because the robot has under/oversteered off the line and couldn't find it again, or it could be because the robot has gone past the end of a line. This behavior is suitable perhaps for small robot competitions.  </p>
			<p>We need to consider things like this carefully and where you would use the robot. Note that for competition-type robots, or industrial robots, they will have either multiple sensors at different angles or a wider angled sensor – so, they are far less likely to lose the line like ours. Also, spinning for a larger robot, even slowly, could be very hazardous behavior. For this reason, let's implement a simple additional safety-type feature. </p>
			<p>When it fails to find the line, it doesn't just stop the motors; it will set the running flag to false, so you need to start it manually again:</p>
			<ol>
				<li value="1">Open the <strong class="source-inline">line_follow_behavior.py</strong> file again.</li>
				<li>Go to the <strong class="source-inline">run</strong> method and find the <strong class="source-inline">else:</strong> statement.</li>
				<li>Now, we can modify the content of this statement:<p class="source-code">            else:</p><p class="source-code">                self.robot.stop_motors()</p><p class="source-code"><strong class="bold">                self.running = False</strong></p><p class="source-code"><strong class="bold">                direction_pid.reset()</strong></p><p class="source-code">                last_time = time.time()</p><p>We have made two small changes here. Instead of resetting the PID if <strong class="source-inline">running</strong> is false, we now set <strong class="source-inline">running</strong> to <strong class="source-inline">False</strong> every time. We also reset the PID every time.</p></li>
			</ol>
			<p>Save the <a id="_idIndexMarker929"/>code to the robot, and run it until it loses the line. This could be by going off course or by reaching the end of the line. The robot should stop. It should wait for you to press the start button before trying to move again. Notice that you'll need to place it back o<a id="_idTextAnchor340"/>n the line and press start for it to go again.</p>
			<p>This robot now handles a lost line condition more predictably.</p>
			<h1 id="_idParaDest-368"><a id="_idTextAnchor341"/>Summary</h1>
			<p>In this chapter, you saw how to use the camera to detect a line and how to plot data showing what it found. You then saw how to take this data and put it into driving behavior so that the robot follows the line. You added to your OpenCV knowledge, and I showed you a sneaky way to put graphs into frames rendered on the camera stream output. You saw how to tune the PID to make the line following more accurate and how to ensure the robot stops predictably when it has lost the line. </p>
			<p>In the next chapter, we will see how to communicate with our robot via a voice agent, Mycroft. You will add a microphone and speakers to a Raspberry Pi, then add speech recognition software. This will let us speak commands to a Raspberry Pi to send to the robot, and Mycroft will respond to let us know what it has done. </p>
			<h1 id="_idParaDest-369"><a id="_idTextAnchor342"/>Exercises</h1>
			<p>Now that we've got this to work, there are ways we could enhance the system and make it more interesting:</p>
			<ul>
				<li>Could you use <strong class="source-inline">cv2.putText</strong> to draw values such as the PID data onto the frames in the <strong class="source-inline">make_display</strong> method?</li>
				<li>Consider writing the PID and error data versus time to a file, then loading it into another Python file, using Matplotlib to show what happened. This change might make the under/oversteer clearer in retrospect.</li>
				<li>You could modify the motor handling code to go faster when the line is closer to the middle and slow down when it is further.</li>
				<li>A significant enhancement would be to check two rows and find the angle between them. You then know how far the line is from the middle, but you also know which way the line is headed and could use that to guide your steering further.</li>
			</ul>
			<p>These exercises should give you some interesting ways to play and experiment with the things you've built and learned in this chapter.</p>
			<h1 id="_idParaDest-370"><a id="_idTextAnchor343"/>Further reading</h1>
			<p>The following should help you look further into line following:</p>
			<ul>
				<li>Read about an alternative approach for line processing in the Go language on the Raspberry Pi from Pi Wars legend Brian Starkey at <a href="https://blog.usedbytes.com/2019/02/autonomous-challenge-blast-off/">https://blog.usedbytes.com/2019/02/autonomous-challenge-blast-off/</a>.</li>
				<li>Here is another line-following robot, using an approach like ours but more sophisticated: <a href="https://www.raspberrypi.org/blog/an-image-processing-robot-for-robocup-junior/">https://www.raspberrypi.org/blog/an-image-processing-robot-for-robocup-junior/</a>.</li>
			</ul>
		</div>
	</div></body></html>