["```py\npip install torch torchvision torchaudio\npip install bitsandbytes\npip install transformers\npip install accelerate\npip install diffusers\npip install peft\npip install datasets\n```", "```py\npip install torch==2.1.2 torchvision==0.16.1 torchaudio==2.1.1\npip install bitsandbytes==0.41.0\npip install transformers==4.36.1\npip install accelerate==0.24.1\npip install diffusers==0.26.0\npip install peft==0.6.2\npip install datasets==2.16.0\n```", "```py\nimport numpy as np\nw_list = np.array([2,3,4,7])\n```", "```py\nimport random\nx_list = []\nfor _ in range(10):\n    x_sample = np.array([random.randint(1,100) for _ in range(\n        len(w_list))])\n    x_list.append(x_sample)\n```", "```py\ny_list = []\nfor x_sample in x_list:\n    y_temp = x_sample@w_list\n    y_list.append(y_temp)\n```", "```py\nimport torch\nimport torch.nn as nn\nclass MyLinear(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.w = nn.Parameter(torch.randn(4))\n    def forward(self, x:torch.Tensor):\n        return self.w @ x\n```", "```py\nmodel = MyLinear()\n```", "```py\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.00001)\n```", "```py\nx_input = torch.tensor(x_list, dtype=torch.float32)\ny_output = torch.tensor(y_list, dtype=torch.float32)\n```", "```py\n# start train model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    for i, x in enumerate(x_input):\n        # forward\n        y_pred = model(x)\n        # calculate loss\n        loss = loss_fn(y_pred,y_output[i])\n        # zero out the cached parameter.\n        optimizer.zero_grad()\n        # backward\n        loss.backward()\n        # update paramters\n        optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \n            num_epochs, loss.item()))\nprint(\"train done\")\n```", "```py\nEpoch [10/100], Loss: 201.5572\nEpoch [20/100], Loss: 10.8380\nEpoch [30/100], Loss: 3.5255\nEpoch [40/100], Loss: 1.7397\nEpoch [50/100], Loss: 0.9160\nEpoch [60/100], Loss: 0.4882\nEpoch [70/100], Loss: 0.2607\nEpoch [80/100], Loss: 0.1393\nEpoch [90/100], Loss: 0.0745\nEpoch [100/100], Loss: 0.0398\ntrain done\n```", "```py\nmodel.w\n```", "```py\nParameter containing:\ntensor([1.9761, 3.0063, 4.0219, 6.9869], requires_grad=True)\n```", "```py\n    from accelerate import utils\n    ```", "```py\n    utils.write_basic_config()\n    ```", "```py\n    from accelerate import Accelerator\n    ```", "```py\n    accelerator = Accelerator()\n    ```", "```py\n    device = accelerator.device\n    ```", "```py\n    x_input.to(device)\n    ```", "```py\n    y_output.to(device)\n    ```", "```py\n    model.to(device)\n    ```", "```py\n    # loss.backward\n    ```", "```py\n    accelerator.backward(loss)\n    ```", "```py\n# start train model using Accelerate\nfrom accelerate import utils\nutils.write_basic_config()\nfrom accelerate import Accelerator\naccelerator = Accelerator()\ndevice = accelerator.device\nx_input.to(device)\ny_output.to(device)\nmodel.to(device)\nmodel, optimizer = accelerator.prepare(\n    model, optimizer\n)\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    for i, x in enumerate(x_input):\n        # forward\n        y_pred = model(x)\n        # calculate loss\n        loss = loss_fn(y_pred,y_output[i])\n        # zero out the cached parameter.\n        optimizer.zero_grad()\n        # backward\n        #loss.backward()\n        accelerator.backward(loss)\n        # update paramters\n        optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1,\n            num_epochs, loss.item()))\nprint(\"train done\")\n```", "```py\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel\nmodel = MyLinear()\nddp_model = DistributedDataParallel(model)\n# Hugging Face Accelerate wraps this operation automatically using the prepare() function like this:\nfrom accelerate import Accelerator\naccelerator = Accelerator()\nmodel = MyLinear()\nmodel = accelerator.prepare(model)\n```", "```py\ndef allreduce(data):\n    for i in range(1, len(data)):\n        data[0][:] += data[i].to(data[0].device)\n    for i in range(1, len(data)):\n        data[i][:] = data[0].to(data[i].device)\n```", "```py\nimport numpy as np\nw_list = np.array([2,3,4,7])\nimport random\nx_list = []\nfor _ in range(10):\n    x_sample = np.array([random.randint(1,100)\n        for _ in range(len(w_list))]\n    )\n    x_list.append(x_sample)\ny_list = []\nfor x_sample in x_list:\n    y_temp = x_sample@w_list\n    y_list.append(y_temp)\ntrain_obj = {\n    'w_list':w_list.tolist(),\n    'input':x_list,\n    'output':y_list\n}\nimport pickle\nwith open('train_data.pkl','wb') as f:\n    pickle.dump(train_obj,f)\n```", "```py\nimport torch\nimport torch.nn as nn\nfrom accelerate import utils\nfrom accelerate import Accelerator\n# start a accelerate instance\nutils.write_basic_config()\naccelerator = Accelerator()\ndevice = accelerator.device\ndef main():\n    # define the model\n    class MyLinear(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.w = nn.Parameter(torch.randn(len(w_list)))\n        def forward(self, x:torch.Tensor):\n            return self.w @ x\n    # load training data\n    import pickle\n    with open(\"train_data.pkl\",'rb') as f:\n        loaded_object = pickle.load(f)\n    w_list = loaded_object['w_list']\n    x_list = loaded_object['input']\n    y_list = loaded_object['output']\n    # convert data to torch tensor\n    x_input = torch.tensor(x_list, dtype=torch.float32).to(device)\n    y_output = torch.tensor(y_list, dtype=torch.float32).to(device)\n    # initialize model, loss function, and optimizer\n    Model = MyLinear().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr = 0.00001)\n    # wrap model and optimizer using accelerate\n    model, optimizer = accelerator.prepare(\n        model, optimizer\n    )\n    num_epochs = 100\n    for epoch in range(num_epochs):\n        for i, x in enumerate(x_input):\n            # forward\n            y_pred = model(x)\n            # calculate loss\n            loss = loss_fn(y_pred,y_output[i])\n            # zero out the cached parameter.\n            optimizer.zero_grad()\n            # backward\n            #loss.backward()\n            accelerator.backward(loss)\n            # update paramters\n            optimizer.step()\n        if (epoch+1) % 10 == 0:\n            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \n                num_epochs, loss.item()))\n    # take a look at the model weights after trainning\n    model = accelerator.unwrap_model(model)\n    print(model.w)\nif __name__ == \"__main__\":\n    main()\n```", "```py\naccelerate launch --num_processes=2 train_model_in_2gpus.py\n```", "```py\nParameter containing:\ntensor([1.9875, 3.0020, 4.0159, 6.9961], device='cuda:0', requires_grad=True)\n```", "```py\n# import packages\nimport torch\nfrom accelerate import utils\nfrom accelerate import Accelerator\nfrom diffusers import DDPMScheduler,StableDiffusionPipeline\nfrom peft import LoraConfig\nfrom peft.utils import get_peft_model_state_dict\nfrom datasets import load_dataset\nfrom torchvision import transforms\nimport math\nfrom diffusers.optimization import get_scheduler\nfrom tqdm.auto import tqdm\nimport torch.nn.functional as F\nfrom diffusers.utils import convert_state_dict_to_diffusers\n# train code\ndef main():\n    accelerator = Accelerator(\n        gradient_accumulation_steps = gradient_accumulation_steps,\n        mixed_precision = \"fp16\"\n    )\n    Device = accelerator.device\n    ...\n    # almost all training code will be land inside of this main function.\nif __name__ == \"__main__\":\n    main()\n```", "```py\n# hyperparameters\noutput_dir = \".\"\npretrained_model_name_or_path   = \"runwayml/stable-diffusion-v1-5\"\nlora_rank = 4\nlora_alpha = 4\nlearning_rate = 1e-4\nadam_beta1, adam_beta2 = 0.9, 0.999\nadam_weight_decay = 1e-2\nadam_epsilon = 1e-08\ndataset_name = None\ntrain_data_dir = \"./train_data\"\ntop_rows = 4\noutput_dir = \"output_dir\"\nresolution = 768\ncenter_crop = True\nrandom_flip = True\ntrain_batch_size = 4\ngradient_accumulation_steps = 1\nnum_train_epochs = 200\n# The scheduler type to use. Choose between [\"linear\", \"cosine\", # \"cosine_with_restarts\", \"polynomial\",\"constant\", \"constant_with_ \n# warmup\"]\nlr_scheduler_name = \"constant\" #\"cosine\"#\nmax_grad_norm = 1.0\ndiffusion_scheduler = DDPMScheduler\n```", "```py\nnoise_scheduler = DDPMScheduler.from_pretrained(\n    pretrained_model_name_or_path, subfolder=\"scheduler\")\nweight_dtype = torch.float16\npipe = StableDiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_path,\n    torch_dtype = weight_dtype\n).to(device)\ntokenizer, text_encoder = pipe.tokenizer, pipe.text_encoder\nvae, unet = pipe.vae, pipe.unet\n```", "```py\n# freeze parameters of models, we just want to train a LoRA only\nunet.requires_grad_(False)\nvae.requires_grad_(False)\ntext_encoder.requires_grad_(False)\n```", "```py\n# configure LoRA parameters use PEFT\nunet_lora_config = LoraConfig(\n    r = lora_rank,\n    lora_alpha = lora_alpha,\n    init_lora_weights = \"gaussian\",\n    target_modules = [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n)\n```", "```py\n# Add adapter and make sure the trainable params are in float32.\nunet.add_adapter(unet_lora_config)\nfor param in unet.parameters():\n    # only upcast trainable parameters (LoRA) into fp32\n    if param.requires_grad:\n        param.data = param.to(torch.float32)\n```", "```py\nif dataset_name:\n    # Downloading and loading a dataset from the hub. data will be \n    # saved to ~/.cache/huggingface/datasets by default\n    dataset = load_dataset(dataset_name)\nelse:\n    dataset = load_dataset(\n        \"imagefolder\",\n        data_dir = train_data_dir\n    )\ntrain_data = dataset[\"train\"]\ndataset[\"train\"] = train_data.select(range(top_rows))\n# Preprocessing the datasets. We need to tokenize inputs and targets.\ndataset_columns = list(dataset[\"train\"].features.keys())\nimage_column, caption_column = dataset_columns[0],dataset_columns[1]\n```", "```py\ndef tokenize_captions(examples, is_train=True):\n    '''Preprocessing the datasets.We need to tokenize input captions and transform the images.'''\n    captions = []\n    for caption in examples[caption_column]:\n        if isinstance(caption, str):\n            captions.append(caption)\n    inputs = tokenizer(\n        captions,\n        max_length = tokenizer.model_max_length,\n        padding = \"max_length\",\n        truncation = True,\n        return_tensors = \"pt\"\n    )\n    return inputs.input_ids\n```", "```py\n# Preprocessing the datasets.\ntrain_transforms = transforms.Compose(\n    [\n        transforms.Resize(\n            resolution,\n            interpolation=transforms.InterpolationMode.BILINEAR\n        ),\n        transforms.CenterCrop(resolution) if center_crop else \n            transforms.RandomCrop(resolution),\n        transforms.RandomHorizontalFlip() if random_flip else \n            transforms.Lambda(lambda x: x),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]) # [0,1] -> [-1,1]\n    ]\n)\n```", "```py\ndef preprocess_train(examples):\n    '''prepare the train data'''\n    images = [image.convert(\"RGB\") for image in examples[\n        image_column]]\n    examples[\"pixel_values\"] = [train_transforms(image) \n        for image in images]\n    examples[\"input_ids\"] = tokenize_captions(examples)\n    return examples\n# only do this in the main process\nwith accelerator.main_process_first():\n    # Set the training transforms\n    train_dataset = dataset[\"train\"].with_transform(preprocess_train)\ndef collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] \n        for example in examples])\n    pixel_values = pixel_values.to(memory_format = \\\n        torch.contiguous_format).float()\n    input_ids = torch.stack([example[\"input_ids\"] \n        for example in examples])\n    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n# DataLoaders creation:\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    shuffle = True\n    collate_fn = collate_fn\n    batch_size = train_batch_size\n)\n```", "```py\n# initialize optimizer\nlora_layers = filter(lambda p: p.requires_grad, unet.parameters())\noptimizer = torch.optim.AdamW(\n    lora_layers,\n    lr = learning_rate,\n    betas = (adam_beta1, adam_beta2),\n    weight_decay = adam_weight_decay,\n    eps = adam_epsilon\n)\n```", "```py\n# learn rate scheduler from diffusers's get_scheduler\nlr_scheduler = get_scheduler(\n    lr_scheduler_name,\n    optimizer = optimizer\n)\n```", "```py\n# set step count and progress bar\nmax_train_steps = num_train_epochs*len(train_dataloader)\nprogress_bar = tqdm(\n    range(0, max_train_steps),\n    initial = 0,\n    desc = \"Steps\",\n    # Only show the progress bar once on each machine.\n    Disable = not accelerator.is_local_main_process,\n)\n```", "```py\n# start train\nfor epoch in range(num_train_epochs):\n    unet.train()\n    train_loss = 0.0\n    for step, batch in enumerate(train_dataloader):\n        # step 1\\. Convert images to latent space\n        # latents = vae.encode(batch[\"pixel_values\"].to(\n            dtype=weight_dtype)).latent_dist.sample()\n        latents = latents * vae.config.scaling_factor\n        # step 2\\. Sample noise that we'll add to the latents, \n        latents provide the shape info.\n        noise = torch.randn_like(latents)\n        # step 3\\. Sample a random timestep for each image\n        batch_size = latents.shape[0]\n        timesteps = torch.randint(\n            low = 0,\n            high = noise_scheduler.config.num_train_timesteps,\n            size = (batch_size,),\n            device = latents.device\n        )\n        timesteps = timesteps.long()\n        # step 4\\. Get the text embedding for conditioning\n        encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n        # step 5\\. Add noise to the latents according to the noise \n        # magnitude at each timestep\n        # (this is the forward diffusion process), \n        # provide to unet to get the prediction result\n        noisy_latents = noise_scheduler.add_noise(\n            latents, noise, timesteps)\n        # step 6\\. Get the target for loss depend on the prediction \n        # type\n        if noise_scheduler.config.prediction_type == \"epsilon\":\n            target = noise\n        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n            target = noise_scheduler.get_velocity(\n                latents, noise, timesteps)\n        else:\n            raise ValueError(f\"Unknown prediction type {\n                noise_scheduler.config.prediction_type}\")\n        # step 7\\. Predict the noise residual and compute loss\n        model_pred = unet(noisy_latents, timesteps, \n            encoder_hidden_states).sample\n        # step 8\\. Calculate loss\n        loss = F.mse_loss(model_pred.float(), target.float(), \n            reduction=\"mean\")\n        # step 9\\. Gather the losses across all processes for logging \n        # (if we use distributed training).\n        avg_loss = accelerator.gather(loss.repeat(\n            train_batch_size)).mean()\n        train_loss += avg_loss.item() / gradient_accumulation_steps\n        # step 10\\. Backpropagate\n        accelerator.backward(loss)\n        if accelerator.sync_gradients:\n            params_to_clip = lora_layers\n            accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        # step 11\\. check optimization step and update progress bar\n        if accelerator.sync_gradients:\n            progress_bar.update(1)\n            train_loss = 0.0\n        logs = {\"epoch\": epoch,\"step_loss\": loss.detach().item(), \n            \"lr\": lr_scheduler.get_last_lr()[0]}\n        progress_bar.set_postfix(**logs)\n```", "```py\n# Save the lora layers\naccelerator.wait_for_everyone()\nif accelerator.is_main_process:\n    unet = unet.to(torch.float32)\n    unwrapped_unet = accelerator.unwrap_model(unet)\n    unet_lora_state_dict = convert_state_dict_to_diffusers(\n        get_peft_model_state_dict(unwrapped_unet))\n    weight_name = f\"\"\"lora_{pretrained_model_name_or_path.split('/')[-1]}_rank{lora_rank}_s{max_train_steps}_r{resolution}_{diffusion_scheduler.__name__}_{formatted_date}.safetensors\"\"\"\n    StableDiffusionPipeline.save_lora_weights(\n        save_directory = output_dir,\n        unet_lora_layers = unet_lora_state_dict,\n        safe_serialization = True,\n        weight_name = weight_name\n    )\naccelerator.end_training()\n```", "```py\naccelerate launch --num_processes=1 ./train_sd16_lora.py\n```", "```py\naccelerate launch --num_processes=2 ./train_sd16_lora.py\n```", "```py\nCUDA_VISIBLE_DEVICES=1,2 accelerate launch --num_processes=2 ./train_sd16_lora.py\n```", "```py\nCUDA_VISIBLE_DEVICES=0,2 accelerate launch --num_processes=2 ./train_sd16_lora.py\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\nfrom diffusers.utils import make_image_grid\nfrom diffusers import EulerDiscreteScheduler\nlora_name = \"lora_file_name.safetensors\"\nlora_model_path = f\"./output_dir/{lora_name}\"\ndevice = \"cuda:0\"\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype = torch.bfloat16\n).to(device)\npipe.load_lora_weights(\n    pretrained_model_name_or_path_or_dict=lora_model_path,\n    adapter_name = \"az_lora\"\n)\nprompt = \"a toy bike. macro photo. 3d game asset\"\nnagtive_prompt = \"low quality, blur, watermark, words, name\"\npipe.set_adapters(\n    [\"az_lora\"],\n    adapter_weights = [0.0]\n)\npipe.scheduler = EulerDiscreteScheduler.from_config(\n    pipe.scheduler.config)\nimages = pipe(\n    prompt = prompt,\n    nagtive_prompt = nagtive_prompt,\n    num_images_per_prompt = 4,\n    generator = torch.Generator(device).manual_seed(12),\n    width = 768,\n    height = 768,\n    guidance_scale = 8.5\n).images\npipe.to(\"cpu\")\ntorch.cuda.empty_cache()\nmake_image_grid(images, cols = 2, rows = 2)\n```"]