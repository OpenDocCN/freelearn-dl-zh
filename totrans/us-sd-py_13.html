<html><head></head><body>
		<div><h1 id="_idParaDest-157" class="chapter-number"><a id="_idTextAnchor257"/>13</h1>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor258"/>Generating Images with ControlNet</h1>
			<p>Stable Diffusion’s ControlNet<a id="_idIndexMarker404"/> is a neural network plugin that allows you to control diffusion models by adding extra conditions. It was first introduced in a paper called Adding Conditional Control to Text-to-Image Diffusion Models [1] by Lvmin Zhang and Maneesh Agrawala, published in 2023.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>What is ControlNet and how is it different?</li>
				<li>Usage of ControlNet</li>
				<li>Using multiple ControlNets in one pipeline</li>
				<li>How ControlNet works</li>
				<li>More ControlNet usage</li>
			</ul>
			<p>By the end of this chapter, you will understand how ControlNet works and how to use Stable Diffusion V1.5 and Stable Diffusion XL ControlNet models.<a id="_idTextAnchor259"/></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor260"/>What is ControlNet and how is it different?</h1>
			<p>In terms of “control,” you may recall textual embedding, LoRA, and the image-to-image diffusion pipeline. But what makes ControlNet different and useful?</p>
			<p>Unlike other solutions, ControlNet is<a id="_idIndexMarker405"/> a model that works on the UNet diffusion process directly. We compare these solutions in <em class="italic">Table 13.1</em>:</p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Control Method</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Functioning Stage</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Usage Scenario</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Textual Embedding</p>
						</td>
						<td class="No-Table-Style">
							<p>Text encoder</p>
						</td>
						<td class="No-Table-Style">
							<p>Add a new style, a new concept, or a new face</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>LoRA</p>
						</td>
						<td class="No-Table-Style">
							<p>Merge LoRA weights to the UNet model (and the CLIP text encoder, optional)</p>
						</td>
						<td class="No-Table-Style">
							<p>Add a set of styles, concepts, and generate content</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Image-to-Image</p>
						</td>
						<td class="No-Table-Style">
							<p>Provide the initial latent image</p>
						</td>
						<td class="No-Table-Style">
							<p>Fix images, or add styles and concepts to images</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>ControlNet</p>
						</td>
						<td class="No-Table-Style">
							<p>ControlNet participant denoising together with a checkpoint model UNet</p>
						</td>
						<td class="No-Table-Style">
							<p>Control shape, pose, content detail</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 13.1: A comparison of textual embedding, LoRA, image-to-image, and ControlNet</p>
			<p>In many ways, ControlNet<a id="_idIndexMarker406"/> is similar to the image-to-image pipeline, as we discussed in <a href="B21263_11.xhtml#_idTextAnchor214"><em class="italic">Chapter 11</em></a>. Both image-to-image and ControlNet can be used to enhance images.</p>
			<p>However, ControlNet can “control” the image in a more precise way. Imagine you want to generate an image that uses a specific pose from another image or perfectly align objects within the scene to a specific reference point. This kind of precision is impossible with the out-of-the-box Stable Diffusion model. ControlNet is the tool that can help you achieve these goals.</p>
			<p>Besides, ControlNet models work with all other open source checkpoint models, unlike some other solutions, which work only with one base model provided by their author. The team that created ControlNet not only open-sourced the model but also open-sourced the code to train a new model. In other words, we can train a ControlNet model and make it work with any other model. This is what the original paper says [1]:</p>
			<p class="author-quote">Since Stable Diffusion is a typical UNet structure, this ControlNet architecture is likely to be applicable with other models.</p>
			<p>Note that ControlNet models will only work with models using the same base model. A <strong class="bold">Stable Diffusion</strong> (<strong class="bold">SD</strong>) v1.5 ControlNet model<a id="_idIndexMarker407"/> works with all other SD v1.5 models. For <strong class="bold">Stable Diffusion XL</strong> (<strong class="bold">SDXL</strong>) models, we will<a id="_idIndexMarker408"/> need a ControlNet model that is trained with SDXL. This is because SDXL models use a different architecture, a larger UNet than the SD v1.5. Without additional work, a ControlNet model is trained with one architecture and only works with this type of model.</p>
			<p>I used “<em class="italic">without additional work</em>” because in December 2023, to bridge this gap, a paper from Lingmin Ran et al, called <em class="italic">X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model</em> was published [8]. This paper details an adapter that enables us to use SD V1.5 LoRA and ControlNet in a new SDXL model.</p>
			<p>Next, let’s start using ControlNet with SD model<a id="_idTextAnchor261"/>s.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor262"/>Usage of ControlNet</h1>
			<p>Before diving into the backend of <a id="_idIndexMarker409"/>ControlNet, in this section, we will start using ControlNet to help control image generation.</p>
			<p>In the following example, we will first generate an image using SD, take the Canny shape of the object, and then use the Canny shape to generate a new image with the help of ControlNet.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A Canny image<a id="_idIndexMarker410"/> refers to an image that has undergone Canny edge detection, which is a popular edge detection algorithm. It was developed by John F. Canny in 1986. [7]</p>
			<p>Let’s use SD to generate an image using the following code:</p>
			<ol>
				<li>Generate a sample image using SD:<pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
# load model</pre><pre class="source-code">
text2img_pipe = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    "stablediffusionapi/deliberate-v2",</pre><pre class="source-code">
     torch_dtype = torch.float16</pre><pre class="source-code">
).to("cuda:0")</pre><pre class="source-code">
# generate sample image</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
high resolution photo,best quality, masterpiece, 8k</pre><pre class="source-code">
A cute cat stand on the tree branch, depth of field, detailed body</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = """</pre><pre class="source-code">
paintings,ketches, worst quality, low quality, normal quality, lowres,</pre><pre class="source-code">
monochrome, grayscale</pre><pre class="source-code">
"""</pre><pre class="source-code">
image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(7)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
image</pre><p class="list-inset">We will see an<a id="_idIndexMarker411"/> image of a cat, as shown in <em class="italic">Figure 13</em><em class="italic">.1</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_13_01.jpg" alt="Figure 13.1: A cat, generated by SD"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1: A cat, generated by SD</p>
			<ol>
				<li value="2">Then we will <a id="_idIndexMarker412"/>get the Canny shape of the sample image.<p class="list-inset">We will need another package, <code>controlnet_aux</code>, to create a Canny image from an image. Simply execute the following two lines of <code>pip</code> commands to install <code>controlnet_aux</code>:</p><pre class="source-code">
pip install opencv-contrib-python</pre><pre class="source-code">
pip install controlnet_aux</pre><p class="list-inset">We can generate the image Canny edge shape with three lines of code:</p><pre class="source-code">
from controlnet_aux import CannyDetector</pre><pre class="source-code">
canny = CannyDetector()</pre><pre class="source-code">
image_canny = canny(image, 30, 100)</pre><p class="list-inset">Here’s a<a id="_idIndexMarker413"/> breakdown of the code:</p><ul><li><code>from controlnet_aux import CannyDetector</code>: This line imports the <code>CannyDetector</code> class from the <code>controlnet_aux</code> module. There are many other detectors.</li><li><code>image_canny = canny(image, 30, 100)</code>: This line calls the <code>__call__</code> method of the <code>CannyDetector</code> class (which is implemented as a callable object) with the following arguments:<ul><li><code>image</code>: This is the input image to which the Canny edge detection algorithm will be applied.</li><li><code>30</code>: This is the lower threshold value for the edges. Any edges with an intensity gradient below this value will be discarded.</li><li><code>100</code>: This is the upper threshold value for the edges. Any edges with an intensity gradient above this value will be considered strong edges.</li></ul></li></ul><p class="list-inset">The preceding code will generate the Canny image shown in <em class="italic">Figure 13</em><em class="italic">.2</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_13_02.jpg" alt="Figure 13.2: Canny image of a cat"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2: Canny image of a cat</p>
			<ol>
				<li value="3">We will now <a id="_idIndexMarker414"/>use the ControlNet model to generate a new image based on this Canny image. First, let’s load up the ControlNet model:<pre class="source-code">
from diffusers import ControlNetModel</pre><pre class="source-code">
canny_controlnet = ControlNetModel.from_pretrained(</pre><pre class="source-code">
    'takuma104/control_v11',</pre><pre class="source-code">
    subfolder='control_v11p_sd15_canny',</pre><pre class="source-code">
    torch_dtype=torch.float16</pre><pre class="source-code">
)</pre><p class="list-inset">The code will download the ControlNet model from Hugging Face automatically at your first run. If you have the ControlNet <code>safetensors</code> model in your storage and want to use your own model, you can convert the file to the diffuser format first. You can find the conversion code in <a href="B21263_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>. Then, replace <code>takuma104/control_v11</code> with the path to the ControlNet model.</p></li>
				<li>Initialize a <a id="_idIndexMarker415"/>ControlNet pipeline:<pre class="source-code">
from diffusers import StableDiffusionControlNetImg2ImgPipeline</pre><pre class="source-code">
cn_pipe = \</pre><pre class="source-code">
    StableDiffusionControlNetImg2ImgPipeline.from_pretrained(</pre><pre class="source-code">
    "stablediffusionapi/deliberate-v2",</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    controlnet = canny_controlnet</pre><pre class="source-code">
)</pre><p class="list-inset">Note that you can freely swap <code>stablediffusionapi/deliberate-v2</code> with any other SD v1.5 models from the community.</p></li>
				<li>Generate the new image using the ControlNet pipeline. In the following example, we will replace the cat with a dog:<pre class="source-code">
prompt = """</pre><pre class="source-code">
high resolution photo,best quality, masterpiece, 8k</pre><pre class="source-code">
A cute dog stand on the tree branch, depth of field, detailed body</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = """</pre><pre class="source-code">
paintings,ketches, worst quality, low quality, normal quality, lowres,</pre><pre class="source-code">
monochrome, grayscale</pre><pre class="source-code">
"""</pre><pre class="source-code">
image_from_canny = single_cn_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    image = canny_image,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(2),</pre><pre class="source-code">
    num_inference_steps = 30,</pre><pre class="source-code">
    guidance_scale = 6.0</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
image_from_canny</pre><p class="list-inset">These lines of <a id="_idIndexMarker416"/>code will generate a new image following the Canny edge, but the little cat is now a dog, as shown in <em class="italic">Figure 13</em><em class="italic">.3</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_13_03.jpg" alt="Figure 13.3: A dog, generated using the cat Canny image with ControlNet"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3: A dog, generated using the cat Canny image with ControlNet</p>
			<p>The cat’s body<a id="_idIndexMarker417"/> structure and shape are preserved. Feel free to change the prompt and settings to explore the amazing capabilities of the model. One thing to note is that if you don’t provide a prompt to the ControlNet pipeline, the pipeline will still output a meaningful image, maybe another style of cat, which means the ControlNet model learned the underlying meaning of a certain Canny edge.</p>
			<p>In this example, we <a id="_idIndexMarker418"/>used only one ControlNet model, but we can also provide multiple ControlNet models to one <a id="_idTextAnchor263"/>pipeline.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor264"/>Using multiple ControlNets in one pipeline</h1>
			<p>In this section, we will <a id="_idIndexMarker419"/>initialize one more ControlNet, NormalBAE, and then feed the Canny and NormalBAE ControlNet models together to form a pipeline.</p>
			<p>Let’s generate a Normal BAE as one additional control image. Normal BAE is a model that’s used to estimate a normal map using the normal uncertainty method [4] proposed by Bae et al:</p>
			<pre class="source-code">
from controlnet_aux import NormalBaeDetector
normal_bae = \
    NormalBaeDetector.from_pretrained("lllyasviel/Annotators")
image_canny = normal_bae(image)
image_canny</pre>
			<p>This code will generate the <a id="_idIndexMarker420"/>original image’s Normal BAE map, as shown in <em class="italic">Figure 13</em><em class="italic">.4</em>:</p>
			<div><div><img src="img/B21263_13_04.jpg" alt="Figure 13.4: Normal BAE image of the generated cat"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4: Normal BAE image of the generated cat</p>
			<p>Now, let’s initialize two ControlNet models for one pipeline: one Canny ControlNet model, and another NormalBae ControlNet model:</p>
			<pre class="source-code">
from diffusers import ControlNetModel
canny_controlnet = ControlNetModel.from_pretrained(
    'takuma104/control_v11',
    subfolder='control_v11p_sd15_canny',
    torch_dtype=torch.float16
)
bae_controlnet = ControlNetModel.from_pretrained(
    'takuma104/control_v11',
    subfolder='control_v11p_sd15_normalbae',
    torch_dtype=torch.float16
)
controlnets = [canny_controlnet, bae_controlnet]</pre>
			<p>From the code, we can <a id="_idIndexMarker421"/>easily see that all ControlNet models share the same architecture. To load different ControlNet models, we only need to change the model name. Also, note that the two ControlNet models are in a Python <code>controlnets</code> <code>list</code>. We can provide these ControlNet models to the pipeline directly, as shown here:</p>
			<pre class="source-code">
from diffusers import StableDiffusionControlNetPipeline
two_cn_pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "stablediffusionapi/deliberate-v2",
    torch_dtype = torch.float16,
    controlnet = controlnets
).to("cuda")</pre>
			<p>In the inference stage, use one additional parameter, <code>controlnet_conditioning_scale</code>, to control the influence scale of each ControlNet:</p>
			<pre class="source-code">
prompt = """
high resolution photo,best quality, masterpiece, 8k
A cute dog on the tree branch, depth of field, detailed body,
"""
neg_prompt = """
paintings,ketches, worst quality, low quality, normal quality, lowres,
monochrome, grayscale
"""
image_from_2cn = two_cn_pipe(
    prompt = prompt,
    image = [canny_image,bae_image],
    controlnet_conditioning_scale = [0.5,0.5],
    generator = torch.Generator("cuda").manual_seed(2),
    num_inference_steps = 30,
    guidance_scale = 5.5
).images[0]
image_from_2cn</pre>
			<p>This code will give us<a id="_idIndexMarker422"/> another image, as shown in <em class="italic">Figure 13</em><em class="italic">.5</em>:</p>
			<div><div><img src="img/B21263_13_05.jpg" alt="Figure 13.5: A dog generated from Canny ControlNet and a Normal BAE ControlNet"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5: A dog generated from Canny ControlNet and a Normal BAE ControlNet</p>
			<p>In <code>controlnet_conditioning_scale = [0.5,0.5]</code>, I give each ControlNet model a <code>0.5</code> scale<a id="_idIndexMarker423"/> value. The two scale values add up to <code>1.0</code>. We should give weights that add up to no more than <code>2</code>. Values that are too will lead to undesired images. For instance, if you give weights of <code>1.2</code> and <code>1.3</code> to each ControlNet model, like <code>controlnet_conditioning_scale = [1.2,1.3]</code>, you <a id="_idIndexMarker424"/>may get an undesired image.</p>
			<p>If we have successfully generated images using the ControlNet models, we have together witnessed the power of ControlNet. In the next section, we will discuss how Cont<a id="_idTextAnchor265"/>rolNet works.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor266"/>How ControlNet works</h1>
			<p>In this section, we will drill down<a id="_idIndexMarker425"/> into the ControlNet structure and see how ControlNet works internally.</p>
			<p>ControlNet works by injecting additional conditions into the blocks of a neural network. As shown in <em class="italic">Figure 13</em><em class="italic">.6</em>, the trainable copy is the ControlNet block that adds additional guidance to the original SD UNet block:</p>
			<div><div><img src="img/B21263_13_06.jpg" alt="Figure 13.6: Adding ControlNet components"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6: Adding ControlNet components</p>
			<p>During the training stage, we take a copy of the target layer block as the<a id="_idIndexMarker426"/> ControlNet block. In <em class="italic">Figure 13</em><em class="italic">.6</em>, it is denoted as a <strong class="bold">trainable copy</strong>. Unlike typical neural network initialization with Gaussian distributions for all parameters, ControlNet utilizes pre-trained weights from the Stable Diffusion base model. Most of these base model parameters are frozen (with the option to unfreeze them later) and only the additional ControlNet components are trained from scratch.</p>
			<p>During training and inference, the input x is usually a 3D dimensional vector, x ∈ ℝ h×w×c, with h, w, c as the height, width, and number of channels. c is a conditioning vector that we will pass into the SD UNet and also to the ControlNet model network.</p>
			<p>The <strong class="bold">zero convolution</strong> plays a <a id="_idIndexMarker427"/>pivotal role in this process. <strong class="bold">Zero convolutions</strong> are 1D<a id="_idIndexMarker428"/> convolutions with weights and biases initialized to zero. The advantage of zero convolution is that, even without a single training step, the value injected from ControlNet will have no effect on image generation. This ensures that the side network does not negatively impact image generation at any stage.</p>
			<p>You might be thinking: if the weight of a convolution layer is zero, wouldn't the gradient also be zero, rendering the network unable to learn? However, as the paper's authors explain [5], the reality is more nuanced.</p>
			<p>Let’s consider one simple case:</p>
			<p>y = wx + b</p>
			<p>Then we have the following:</p>
			<p>∂ y / ∂ w = x, ∂ y / ∂ x = w, ∂ y / ∂ b = 1</p>
			<p>And if w = 0 and x ≠ 0, then we have this:</p>
			<p>∂ y / ∂ w ≠ 0, ∂ y / ∂ x = 0, ∂ y / ∂ b ≠ 0</p>
			<p>This means as long as x ≠ 0, one gradient descent iteration will make w non-zero. Then, we have:</p>
			<p>∂ y / ∂ x ≠ 0</p>
			<p>So, the zero convolutions will progressively become a common convolution layer with non-zero weights. What a genius design!</p>
			<p>The SD UNet is <a id="_idIndexMarker429"/>only connected with a ControlNet at the encoder blocks and the middle blocks. The trainable blue blocks and the white zero convolution layers are added to build a ControlNet. It’s simple and effective.</p>
			<p>In the original paper— Adding Conditional Control to Text-to-Image Diffusion Models [1] by Lvmin Zhang et al — its authors also provided an ablative study and discussed lots of different cases, such as swapping<a id="_idIndexMarker430"/> the <strong class="bold">zero convolution</strong> layer with a <strong class="bold">traditional convolution</strong> layer and <a id="_idIndexMarker431"/>comparing the differences. It is a great pape<a id="_idTextAnchor267"/>r and fun to read.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor268"/>Further usage</h1>
			<p>In this section, we will introduce more usage of ControlNet, covering SD V<a id="_idTextAnchor269"/>1.5 and also SDXL.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor270"/>More ControlNets with SD</h2>
			<p>The author of the<a id="_idIndexMarker432"/> ControlNet-v1-1-nightly repository [3] lists all the currently available V1.1 ControlNet models for SD. As of the time I am writing this chapter, the list is as follows:</p>
			<pre class="source-code">
control_v11p_sd15_canny
control_v11p_sd15_mlsd
control_v11f1p_sd15_depth
control_v11p_sd15_normalbae
control_v11p_sd15_seg
control_v11p_sd15_inpaint
control_v11p_sd15_lineart
control_v11p_sd15s2_lineart_anime
control_v11p_sd15_openpose
control_v11p_sd15_scribble
control_v11p_sd15_softedge
control_v11e_sd15_shuffle
control_v11e_sd15_ip2p
control_v11f1e_sd15_tile</pre>
			<p>You can simply swap the ControlNet model’s name with one from this list to start using it. Generate the control image using one of the annotators from the open source ControlNet auxiliary models[6].</p>
			<p>Considering the speed of development in the field of AI, when you are reading this, the version may have <a id="_idIndexMarker433"/>increased to v1.1+. However, the underlying mechanism<a id="_idTextAnchor271"/> should be the same.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor272"/>SDXL ControlNets</h2>
			<p>As I am writing this chapter, SDXL <a id="_idIndexMarker434"/>has just been released, and this new model generates excellent images with shorter prompts than before. The Hugging Face Diffusers team trained and provided several ControlNet models for the XL models. Its usage is almost the same as the previous version. Here, let’s use the <code>controlnet-openpose-sdxl-1.0</code> open pose ControlNet for SDXL.</p>
			<p>Note that you will need a dedicated GPU with more than 15 GB of VRAM to run the following example.</p>
			<p>Let’s initialize an SDXL pipeline using the following code:</p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionXLPipeline
sdxl_pipe = StableDiffusionXLPipeline.from_pretrained(
    "RunDiffusion/RunDiffusion-XL-Beta",
    torch_dtype = torch.float16,
    load_safety_checker = False
)
sdxl_pipe.watermark = None</pre>
			<p>Then, generate an image with a man in it:</p>
			<pre class="source-code">
from diffusers import EulerDiscreteScheduler
prompt = """
full body photo of young man, arms spread
white blank background,
glamour photography,
upper body wears shirt,
wears suit pants,
wears leather shoes
"""
neg_prompt = """
worst quality,low quality, paint, cg, spots, bad hands,
three hands, noise, blur, bad anatomy, low resolution, blur face, bad face
"""
sdxl_pipe.to("cuda")
sdxl_pipe.scheduler = EulerDiscreteScheduler.from_config(
    sdxl_pipe.scheduler.config)
image = sdxl_pipe(
    prompt = prompt,
    negative_prompt = neg_prompt,
    width = 832,
    height = 1216
).images[0]
sdxl_pipe.to("cpu")
torch.cuda.empty_cache()
image</pre>
			<p>The code will <a id="_idIndexMarker435"/>generate an image, as shown in <em class="italic">Figure 13</em><em class="italic">.7</em>:</p>
			<div><div><img src="img/B21263_13_07.jpg" alt="Figure 13.7: A man in a suit, generated by SDXL"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7: A man in a suit, generated by SDXL</p>
			<p>We <a id="_idIndexMarker436"/>can use <code>OpenposeDetector</code> from <code>controlnet_aux</code> [6] to extract the pose:</p>
			<pre class="source-code">
from controlnet_aux import OpenposeDetector
open_pose = \
    OpenposeDetector.from_pretrained("lllyasviel/Annotators")
pose = open_pose(image)
pose</pre>
			<p>We will<a id="_idIndexMarker437"/> get the pose image shown in <em class="italic">Figure 13</em><em class="italic">.8</em>:</p>
			<div><div><img src="img/B21263_13_08.jpg" alt="Figure 13.8: Pose image of the man in a suit"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8: Pose image of the man in a suit</p>
			<p>Now, let’s start an SDXL pipeline with the SDXL ControlNet open pose model:</p>
			<pre class="source-code">
from diffusers import StableDiffusionXLControlNetPipeline
from diffusers import ControlNetModel
sdxl_pose_controlnet = ControlNetModel.from_pretrained(
    "thibaud/controlnet-openpose-sdxl-1.0",
    torch_dtype=torch.float16,
)
sdxl_cn_pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    "RunDiffusion/RunDiffusion-XL-Beta",
    torch_dtype = torch.float16,
    load_safety_checker = False,
    add_watermarker = False,
    controlnet = sdxl_pose_controlnet
)
sdxl_cn_pipe.watermark = None</pre>
			<p>Now we can use the<a id="_idIndexMarker438"/> new ControlNet pipeline to generate a new image from the pose image with the same style. We will reuse the prompt but replace <strong class="bold">man</strong> with <strong class="bold">woman</strong>. We are aiming to generate a new image of a woman in a suit but in the same pose as the previous image of a man:</p>
			<pre class="source-code">
from diffusers import EulerDiscreteScheduler
prompt = """
full body photo of young woman, arms spread
white blank background,
glamour photography,
wear sunglass,
upper body wears shirt,
wears suit pants,
wears leather shoes
"""
neg_prompt = """
worst quality,low quality, paint, cg, spots, bad hands,
three hands, noise, blur, bad anatomy, low resolution,
blur face, bad face
"""
sdxl_cn_pipe.to("cuda")
sdxl_cn_pipe.scheduler = EulerDiscreteScheduler.from_config(
    sdxl_cn_pipe.scheduler.config)
generator = torch.Generator("cuda").manual_seed(2)
image = sdxl_cn_pipe(
    prompt = prompt,
    negative_prompt = neg_prompt,
    width = 832,
    height = 1216,
    image = pose,
    generator = generator,
    controlnet_conditioning_scale = 0.5,
    num_inference_steps = 30,
    guidance_scale = 6.0
).images[0]
sdxl_cn_pipe.to("cpu")
torch.cuda.empty_cache()
image</pre>
			<p>The code <a id="_idIndexMarker439"/>generates a new image with the same pose, exactly matching the expectations, as shown in <em class="italic">Figure 13</em><em class="italic">.9</em>:</p>
			<div><div><img src="img/B21263_13_09.jpg" alt="Figure 13.9: A woman in a suit, generated using an SDXL ControlNet"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.9: A woman in a suit, generated using an SDXL ControlNet</p>
			<p>We will further discuss Stable Diff<a id="_idTextAnchor273"/>usion XL in <a href="B21263_16.xhtml#_idTextAnchor309"><em class="italic">Chapter 16</em></a>.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor274"/>Summary</h1>
			<p>In this chapter, we introduced a way to precisely control image generation using SD ControlNets. From the detailed samples we have provided, you can start using one or multiple ControlNet models with SD v1.5 and also SDXL.</p>
			<p>We also drilled down into the internals of ControlNet, explaining how it works in a nutshell.</p>
			<p>We can use ControlNet in lots of applications, including applying a style to an image, applying a shape to an image, merging two images into one, and generating a human body using a posed image. It is powerful and amazingly useful in many ways. Our imagination is the only limitation.</p>
			<p>However, there is one other limitation: it is hard to align the background and overall context between two generations (with different seeds). You may want to use ControlNet to generate a video from the extracted frames from a source video, but the results are still not ideal.</p>
			<p>In the next chapter, we will cover a solution to generate video <a id="_idTextAnchor275"/>and animation using SD.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor276"/>References</h1>
			<ol>
				<li>Adding conditional control to text-to-image diffusion models: <a href="https://arxiv.org/abs/2302.05543">https://arxiv.org/abs/2302.05543</a></li>
				<li>ControlNet v1.0 GitHub repository: <a href="https://github.com/lllyasviel/ControlNet">https://github.com/lllyasviel/ControlNet</a> </li>
				<li>ControlNet v1.1 GitHub repository: <a href="https://github.com/lllyasviel/ControlNet-v1-1-nightly">https://github.com/lllyasviel/ControlNet-v1-1-nightly</a></li>
				<li><code>surface_normal_uncertainty</code>: <a href="https://github.com/baegwangbin/surface_normal_uncertainty">https://github.com/baegwangbin/surface_normal_uncertainty</a></li>
				<li>Zero convolution FAQ: <a href="https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md">https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md</a> </li>
				<li>ControlNet AUX: <a href="https://github.com/patrickvonplaten/controlnet_aux">https://github.com/patrickvonplaten/controlnet_aux</a> </li>
				<li>Canny edge detector: <a href="https://en.wikipedia.org/wiki/Canny_edge_detector">https://en.wikipedia.org/wiki/Canny_edge_detector</a> </li>
				<li> X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model: <a href="https://showlab.github.io/X-Adapter/">https://showlab.github.io/X-Adapter/</a></li>
			</ol>
		</div>
	</body></html>