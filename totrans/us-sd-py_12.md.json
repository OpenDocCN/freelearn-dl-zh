["```py\nA photo with half cat and half dog\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from_pretrained(\n    \"stablediffusionapi/deliberate-v2\",\n    torch_dtype = torch.float16,\n    safety_checker = None\n).to(\"cuda:0\")\nimage = pipeline(\n    prompt = \"A photo with half cat and half dog\",\n    generator = torch.Generator(\"cuda:0\").manual_seed(3)\n).images[0]\nimage\n```", "```py\npip install compel\n```", "```py\nfrom comp\ncompel = Compel(\n    tokenizer = pipeline.tokenizer,\n    text_encoder = pipeline.text_encoder\n)\n```", "```py\nprompt = '(\"A photo of cat\", \"A photo of dog\").blend(0.5, 0.5)'\nprompt_embeds = compel(prompt)\n```", "```py\nimage = pipeline(\n    prompt_embeds = prompt_embeds,\n    generator = torch.Generator(\"cuda:0\").manual_seed(1)\n).images[0]\nimage\n```", "```py\nprompt = '(\"A photo of cat\", \"A photo of dog\").blend(0.7, 0.3)'\n```", "```py\nprompt = '(\"A photo of cat\", \"A photo of dog\").blend(0.7, 0.3)'\n```", "```py\n[A photo of cat:A photo of dog:0.5]\n```", "```py\n[A photo of cat|A photo of dog]\n```", "```py\n[A photo of cat:A photo of dog:0.5]\n```", "```py\npip install -U lark\n```", "```py\nimport lark\nschedule_parser = lark.Lark(r\"\"\"\n!start: (prompt | /[][():]/+)*\nprompt: (emphasized | scheduled | alternate | plain | WHITESPACE)*\n!emphasized: \"(\" prompt \")\"\n        | \"(\" prompt \":\" prompt \")\"\n        | \"[\" prompt \"]\"\nscheduled: \"[\" [prompt \":\"] prompt \":\" [WHITESPACE] NUMBER \"]\"\nalternate: \"[\" prompt (\"|\" prompt)+ \"]\"\nWHITESPACE: /\\s+/\nplain: /([^\\\\\\[\\]():|]|\\\\.)+/\n%import common.SIGNED_NUMBER -> NUMBER\n\"\"\")\n```", "```py\ndef get_learned_conditioning_prompt_schedules(prompts, steps):\n    def collect_steps(steps, tree):\n        l = [steps]\n        class CollectSteps(lark.Visitor):\n            def scheduled(self, tree):\n                tree.children[-1] = float(tree.children[-1])\n                if tree.children[-1] < 1:\n                    tree.children[-1] *= steps\n                tree.children[-1] = min(steps, int(tree.children[-1]))\n                l.append(tree.children[-1])\n            def alternate(self, tree):\n                l.extend(range(1, steps+1))\n        CollectSteps().visit(tree)\n        return sorted(set(l))\n    def at_step(step, tree):\n        class AtStep(lark.Transformer):\n            def scheduled(self, args):\n                before, after, _, when = args\n                yield before or () if step <= when else after\n            def alternate(self, args):\n                yield next(args[(step - 1)%len(args)])\n            def start(self, args):\n                def flatten(x):\n                    if type(x) == str:\n                        yield x\n                    else:\n                        for gen in x:\n                            yield from flatten(gen)\n                return ''.join(flatten(args))\n            def plain(self, args):\n                yield args[0].value\n            def __default__(self, data, children, meta):\n                for child in children:\n                    yield child\n        return AtStep().transform(tree)\n    def get_schedule(prompt):\n        try:\n            tree = schedule_parser.parse(prompt)\n        except lark.exceptions.LarkError as e:\n            if 0:\n                import traceback\n                traceback.print_exc()\n            return [[steps, prompt]]\n        return [[t, at_step(t, tree)] for t in collect_steps(steps, \n            tree)]\n    promptdict = {prompt: get_schedule(prompt) for prompt in \n        set(prompts)}\n    return [promptdict[prompt] for prompt in prompts]\n```", "```py\nsteps = 10\ng = lambda p: get_learned_conditioning_prompt_schedules([p], steps)[0]\n```", "```py\n    g(\"cat\")\n    ```", "```py\n    [[10, 'cat']]\n    ```", "```py\n    g('[cat:dog:0.5]')\n    ```", "```py\n    [[5, 'cat'], [10, 'dog']]\n    ```", "```py\n    g('[cat|dog]')\n    ```", "```py\n    [[1, 'cat'],\n    ```", "```py\n     [2, 'dog'],\n    ```", "```py\n     [3, 'cat'],\n    ```", "```py\n     [4, 'dog'],\n    ```", "```py\n     [5, 'cat'],\n    ```", "```py\n     [6, 'dog'],\n    ```", "```py\n     [7, 'cat'],\n    ```", "```py\n     [8, 'dog'],\n    ```", "```py\n     [9, 'cat'],\n    ```", "```py\n     [10, 'dog']]\n    ```", "```py\ndef parse_scheduled_prompts(text, steps=10):\n    text = text.strip()\n    parse_result = None\n    try:\n        parse_result = get_learned_conditioning_prompt_schedules(\n            [text],\n            steps = steps\n        )[0]\n    except Exception as e:\n        print(e)\n    if len(parse_result) == 1:\n        return parse_result\n    prompts_list = []\n    for i in range(steps):\n        current_prompt_step, current_prompt_content = \\\n            parse_result[0][0],parse_result[0][1]\n        step = i + 1\n        if step < current_prompt_step:\n            prompts_list.append(current_prompt_content)\n            continue\n        if step == current_prompt_step:\n            prompts_list.append(current_prompt_content)\n            parse_result.pop(0)\n    return prompts_list\n```", "```py\nprompt_list = parse_scheduled_prompts(\"[cat:dog:0.5]\")\nprompt_list\n```", "```py\n['cat',\n 'cat',\n 'cat',\n 'cat',\n 'cat',\n 'dog',\n 'dog',\n 'dog',\n 'dog',\n 'dog']\n```", "```py\n...\nprompt_embeds = self._encode_prompt(\n    prompt,\n    device,\n    num_images_per_prompt,\n    do_classifier_free_guidance,\n    negative_prompt,\n    negative_prompt_embeds=negative_prompt_embeds,\n)\n...\n```", "```py\nfrom typing import List, Callable, Dict, Any\nfrom torch import Generator,FloatTensor\nfrom diffusers.pipelines.stable_diffusion import (\n    StableDiffusionPipelineOutput)\nfrom diffusers import (\n    StableDiffusionPipeline,EulerDiscreteScheduler)\nclass StableDiffusionPipeline_EXT(StableDiffusionPipeline):\n    @torch.no_grad()\n    def scheduler_call(\n        self,\n        prompt: str | List[str] = None,\n        height: int | None = 512,\n        width: int | None = 512,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_prompt: str | List[str] | None = None,\n        num_images_per_prompt: int | None = 1,\n        eta: float = 0,\n        generator: Generator | List[Generator] | None = None,\n        latents: FloatTensor | None = None,\n        prompt_embeds: FloatTensor | None = None,\n        negative_prompt_embeds: FloatTensor | None = None,\n        output_type: str | None = \"pil\",\n        callback: Callable[[int, int, FloatTensor], None] | None = None,\n        callback_steps: int = 1,\n        cross_attention_kwargs: Dict[str, Any] | None = None,\n    ):\n        ...\n        # 6\\. Prepare extra step kwargs. TODO: Logic should ideally \n        # just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(\n            generator, eta)\n        # 7\\. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * \\\n            self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as \\\n            progress_bar:\n            for i, t in enumerate(timesteps):\n                # AZ code to enable Prompt Scheduling, \n                # will only function when\n                # when there is a prompt_embeds_l provided.\n                prompt_embeds_l_len = len(embedding_list)\n                if prompt_embeds_l_len > 0:\n                    # ensure no None prompt will be used\n                    pe_index = (i)%prompt_embeds_l_len\n                    prompt_embeds = embedding_list[pe_index]\n                # expand the latents if we are doing classifier \n                #free guidance\n                latent_model_input = torch.cat([latents] * 2) \\\n                    if do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler. \\ \n                    scale_model_input(latent_model_input, t)\n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=prompt_embeds,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                ).sample\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = \\\n                        noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * \\\n                        (noise_pred_text - noise_pred_uncond)\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, \n                    **extra_step_kwargs).prev_sample\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > \\\n                num_warmup_steps and (i + 1) % \\\n                self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps== 0:\n                        callback(i, t, latents)\n        if output_type == \"latent\":\n            image = latents\n        elif output_type == \"pil\":\n            # 8\\. Post-processing\n            image = self.decode_latents(latents)\n            image = self.numpy_to_pil(image)\n        else:\n            # 8\\. Post-processing\n            image = self.decode_latents(latents)\n        if hasattr(self, \"final_offload_hook\") and \\\n            self.final_offload_hook is not None:\n            self.final_offload_hook.offload()\n        return StableDiffusionPipelineOutput(images=image)\n```", "```py\n    if self.scheduler._class_name == \"PNDMScheduler\":\n    ```", "```py\n        self.scheduler = EulerDiscreteScheduler.from_config(\n    ```", "```py\n            self.scheduler.config\n    ```", "```py\n        )\n    ```", "```py\n    device = self._execution_device\n    ```", "```py\n    do_classifier_free_guidance = guidance_scale > 1.0\n    ```", "```py\n    prompt_list = parse_scheduled_prompts(prompt)\n    ```", "```py\n    embedding_list = []\n    ```", "```py\n    if len(prompt_list) == 1:\n    ```", "```py\n        prompt_embeds = self._encode_prompt(\n    ```", "```py\n            prompt,\n    ```", "```py\n            device,\n    ```", "```py\n            num_images_per_prompt,\n    ```", "```py\n            do_classifier_free_guidance,\n    ```", "```py\n            negative_prompt,\n    ```", "```py\n            negative_prompt_embeds=negative_prompt_embeds,\n    ```", "```py\n        )\n    ```", "```py\n    else:\n    ```", "```py\n        for prompt in prompt_list:\n    ```", "```py\n            prompt_embeds = self._encode_prompt(\n    ```", "```py\n                prompt,\n    ```", "```py\n                device,\n    ```", "```py\n                num_images_per_prompt,\n    ```", "```py\n                do_classifier_free_guidance,\n    ```", "```py\n                negative_prompt,\n    ```", "```py\n                negative_prompt_embeds=negative_prompt_embeds,\n    ```", "```py\n            )\n    ```", "```py\n            embedding_list.append(prompt_embeds)\n    ```", "```py\n    self.scheduler.set_timesteps(num_inference_steps, device=device)\n    ```", "```py\n    timesteps = self.scheduler.timesteps\n    ```", "```py\n    num_channels_latents = self.unet.in_channels\n    ```", "```py\n    batch_size = 1\n    ```", "```py\n    latents = self.prepare_latents(\n    ```", "```py\n        batch_size * num_images_per_prompt,\n    ```", "```py\n        num_channels_latents,\n    ```", "```py\n        height,\n    ```", "```py\n        width,\n    ```", "```py\n        prompt_embeds.dtype,\n    ```", "```py\n        device,\n    ```", "```py\n        generator,\n    ```", "```py\n        latents,\n    ```", "```py\n    )\n    ```", "```py\n    num_warmup_steps = len(timesteps) - num_inference_steps * \\ \n    ```", "```py\n        self.scheduler.order\n    ```", "```py\n    with self.progress_bar(total=num_inference_steps) as \\ \n    ```", "```py\n        progress_bar:\n    ```", "```py\n        for i, t in enumerate(timesteps):\n    ```", "```py\n            # custom code to enable Prompt Scheduling, \n    ```", "```py\n            # will only function when\n    ```", "```py\n            # when there is a prompt_embeds_l provided.\n    ```", "```py\n            prompt_embeds_l_len = len(embedding_list)\n    ```", "```py\n            if prompt_embeds_l_len > 0:\n    ```", "```py\n                # ensure no None prompt will be used\n    ```", "```py\n                pe_index = (i)%prompt_embeds_l_len\n    ```", "```py\n                prompt_embeds = embedding_list[pe_index]\n    ```", "```py\n            # expand the latents if we are doing \n    ```", "```py\n            # classifier free guidance\n    ```", "```py\n            latent_model_input = torch.cat([latents] * 2) \n    ```", "```py\n                if do_classifier_free_guidance else latents\n    ```", "```py\n            latent_model_input = \n    ```", "```py\n                self.scheduler.scale_model_input(\n    ```", "```py\n                    latent_model_input, t)\n    ```", "```py\n            # predict the noise residual\n    ```", "```py\n            noise_pred = self.unet(\n    ```", "```py\n                latent_model_input,\n    ```", "```py\n                t,\n    ```", "```py\n                encoder_hidden_states=prompt_embeds,\n    ```", "```py\n                cross_attention_kwargs=cross_attention_kwargs,\n    ```", "```py\n            ).sample\n    ```", "```py\n            # perform guidance\n    ```", "```py\n            if do_classifier_free_guidance:\n    ```", "```py\n                noise_pred_uncond, noise_pred_text = \\\n    ```", "```py\n                    noise_pred.chunk(2)\n    ```", "```py\n                noise_pred = noise_pred_uncond + guidance_scale * \\\n    ```", "```py\n                    (noise_pred_text - noise_pred_uncond)\n    ```", "```py\n            # compute the previous noisy sample x_t -> x_t-1\n    ```", "```py\n            latents = self.scheduler.step(noise_pred, t, \n    ```", "```py\n                latents).prev_sample\n    ```", "```py\n            # call the callback, if provided\n    ```", "```py\n            if i == len(timesteps) - 1 or ((i + 1) > \n    ```", "```py\n                num_warmup_steps and (i + 1) % \n    ```", "```py\n                self.scheduler.order == 0):\n    ```", "```py\n                progress_bar.update()\n    ```", "```py\n                if callback is not None and i % callback_steps == 0:\n    ```", "```py\n                    callback(i, t, latents)\n    ```", "```py\n    image = self.decode_latents(latents)\n    ```", "```py\n    image = self.numpy_to_pil(image)\n    ```", "```py\n    return StableDiffusionPipelineOutput(images=image, \n    ```", "```py\n        nsfw_content_detected=None)\n    ```", "```py\n    pipeline = StableDiffusionPipeline_EXT.from_pretrained(\n    ```", "```py\n        \"stablediffusionapi/deliberate-v2\",\n    ```", "```py\n        torch_dtype = torch.float16,\n    ```", "```py\n        safety_checker = None\n    ```", "```py\n    ).to(\"cuda:0\")\n    ```", "```py\n    prompt = \"high quality, 4k, details, A realistic photo of cute \\ [cat:dog:0.6]\"\n    ```", "```py\n    neg_prompt = \"paint, oil paint, animation, blur, low quality, \\ bad glasses\"\n    ```", "```py\n    image = pipeline.scheduler_call(\n    ```", "```py\n        prompt = prompt,\n    ```", "```py\n        negative_prompt = neg_prompt,\n    ```", "```py\n        generator = torch.Generator(\"cuda\").manual_seed(1)\n    ```", "```py\n    ).images[0]\n    ```", "```py\n    image\n    ```", "```py\nprompt = \"high quality, 4k, details, A realistic photo of white \\\n[cat|dog]\"\nneg_prompt = \"paint, oil paint, animation, blur, low quality, bad \\\nglasses\"\nimage = pipeline.scheduler_call(\n    prompt = prompt,\n    negative_prompt = neg_prompt,\n    generator = torch.Generator(\"cuda\").manual_seed(3)\n).images[0]\nimage\n```"]