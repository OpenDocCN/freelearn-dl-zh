<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-210"><a id="_idTextAnchor253"/>12</h1>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor254"/>Correcting and Optimizing Your Generative AI Application</h1>
			<p>Until this point, you’ve read about how to build a <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>) application, its various components, and how they fit together. You've gained a solid understanding of what makes them work (and not work) well. You’re also aware of some of the challenges of GenAI applications and how to identify them.</p>
			<p>In this chapter, you’ll begin unraveling the mystery of how to <em class="italic">improve</em> your GenAI application once you’ve identified its shortcomings. You will also learn about optimizing and fine-tuning your GenAI application, so it’s a reliable, effective, and stable machine working in your favor, instead of a rogue actor bringing chaos.</p>
			<p>This chapter will discuss several well-known techniques to improve your GenAI application, so you can be confident in your finished product. Ideally, you will perform all of these techniques. The chapter will define each of these and explain how they can improve your application. Then, you will complete a robust example of each of these as an activity. By the end of this chapter, you will have many ideas on how to improve your application.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>Baselining</li>
				<li>Training and evaluation datasets</li>
				<li>Few-shot prompting</li>
				<li>Retrieval and reranking</li>
				<li>Late interaction strategies, including in-application feedback and user feedback loops</li>
				<li>Query rewriting</li>
				<li>Testing and red teaming</li>
				<li>Information post-processing</li>
			</ul>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor255"/>Technical requirements</h1>
			<p>This chapter does not contain any coding. However, it builds upon all the previous chapters to describe various methodologies for improving and optimizing your GenAI application output. To recreate some of the examples, you’ll simply need to use your favorite <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>) provider and recreate the attempts yourself. This chapter uses ChatGPT.</p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor256"/>Baselining</h1>
			<p><strong class="bold">Baselining</strong>, in the context of GenAI, refers to the process of defining a standard or a reference output for the AI model to compare future outputs. This standard serves as a crucial benchmark for evaluating the model’s performance, consistency, and improvements over time. By establishing a baseline, developers and stakeholders can objectively measure how the AI performs relative to a predefined set of expectations, ensuring that the model meets and maintains desired standards.</p>
			<p>In GenAI, baselining is essential for several reasons. Firstly, it provides a clear metric for assessing the quality and performance of the AI model. Secondly, it helps in tracking the model’s progress and improvements over time. Finally, baselining is a tool to help ensure consistency in the model’s outputs, via detection of output variability. All of these are vital for maintaining reliability and trust in the AI system.</p>
			<p>The aspects of the AI model that can be baselined are numerous and highly dependent on the specific application and its goals. Some common elements that might be baselined include the following:</p>
			<ul>
				<li><strong class="bold">Accuracy</strong>: This involves measuring the correctness of the model’s outputs. For instance, in a language model, accuracy can be gauged by how well the generated text matches the expected text or how often it provides the correct information.</li>
				<li><strong class="bold">Speed of response</strong>: This refers to the time it takes for the model to generate an output after receiving an input. Faster response times are generally preferred, especially in real-time applications.</li>
				<li><strong class="bold">Effectiveness</strong>: This can be a measure of how well the AI meets its intended purpose. For example, in a recommendation system, effectiveness might be assessed by the relevance and personalization of the recommendations provided.</li>
				<li><strong class="bold">User satisfaction</strong>: This subjective metric can be gauged through user feedback and surveys, reflecting how satisfied users are with the AI’s performance and outputs.</li>
			</ul>
			<p>Establishing a baseline standard alongside your current performance also helps you—the engineer—determine whether you are improving results over time. This knowledge is crucial for ensuring that your application is not degrading in performance. In some industries, baseline performance indicators may be required to meet industry or regulatory standards and may be a reporting requirement for your application or organization.</p>
			<p>Once you evaluate the initial performance of your application, you’ll want to document these results. Subsequently, ensure that you consistently compare the model’s outputs to the baseline during each training and update cycle. Comprehensive documentation provides a reference that can be used to compare future outputs and identify trends or issues in the model’s performance.</p>
			<p>Regular evaluation of the model’s outputs against the baseline is also critical. During subsequent iterations of training and updates, these evaluations can help in detecting deviations from the expected (baseline) performance. If the model’s performance drops below the baseline, it can indicate a problem that needs to be addressed, such as data drift, changes in user behavior, or issues with the training dataset.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor257"/>Training and evaluation datasets</h2>
			<p>To create your baseline, you will need to create an <strong class="bold">evaluation dataset</strong>. An evaluation dataset is a series of questions asked of your application to determine whether it meets the standards you have identified. Note that the evaluation dataset is not to be confused with the <strong class="bold">training dataset</strong>, which is the data that you used to <em class="italic">train</em> your model. The evaluation dataset should be a wholly different set of questions and answers. Effectively, the training dataset is akin to the notes and sources that you’d give to a student to learn, while the evaluation dataset is like the final exam. You don’t want to make that exam too easy!</p>
			<h3>Training datasets</h3>
			<p>As its name suggests, a training dataset is a collection of data used to teach or <em class="italic">train</em> a machine learning model. It contains input-output pairs where the input data is fed to the model, and the model learns to produce the correct output. This process involves adjusting the model’s parameters so that it can generalize well to new, unseen data. The quality and diversity of the training dataset directly impact the performance and accuracy of the trained model.</p>
			<p>High-quality training data ensures that the model can recognize patterns and make accurate predictions or generate appropriate responses. Therefore, your training dataset should be representative of the problem domain, covering a wide range of scenarios that the model would be expected to encounter in real-world applications. This helps in reducing biases and improving the model’s generalizability.</p>
			<p>The types of data in the training dataset might include the following:</p>
			<ul>
				<li><strong class="bold">Labeled data</strong>: This is the primary type of data used in supervised learning. Each data point consists of an input and a corresponding correct output, or label. For instance, in a text classification task, labeled data might include sentences paired with their respective categories.</li>
				<li><strong class="bold">Unlabeled data</strong>: Used in unsupervised learning, this data does not come with predefined labels. The model tries to find patterns and structures in the data. For example, clustering algorithms use unlabeled data to group similar data points together.</li>
				<li><strong class="bold">Mixed data</strong>: Semi-supervised learning uses a combination of labeled and unlabeled data. This approach leverages the large amounts of unlabeled data available while benefiting from the smaller labeled dataset to guide the learning process.</li>
				<li><strong class="bold">Diverse data</strong>: Including diverse data ensures that the model can handle various inputs. This might include different languages, dialects, formats, and contexts. For certain types of applications, this might include training data that is both human-readable documentation as well as code bases.</li>
			</ul>
			<p>Despite all that, you might wish to also include <strong class="bold">supplemental training data</strong>. Supplemental training data refers to additional data used to fine-tune or enhance the performance of an already trained model. There are many reasons to do this, but let’s talk about three that are particularly compelling:</p>
			<ul>
				<li>Supplemental data can help adapt a general model to a specific domain. For example, a language model trained on general text might be fine-tuned with medical literature to perform better in healthcare applications.</li>
				<li>Supplemental training data can be used to enhance the model’s ability in particular areas where it might be weak. For example, adding more data related to financial transactions can help a fraud detection model become more accurate.</li>
				<li>As new information becomes available, supplemental training data can be used to update the model’s knowledge. This is especially relevant for applications requiring up-to-date information, such as news generation or where the industry is rapidly evolving (such as technology).</li>
			</ul>
			<h3>Evaluation datasets</h3>
			<p>In addition to your training data and supplemental data, you’ll also need an evaluation dataset. Evaluation datasets are crucial because they provide a controlled and consistent way to measure the performance of your AI model. They serve as a benchmark for comparison, ensuring that the model’s outputs can be objectively assessed against predefined criteria. By using a standard dataset, you can reliably track improvements, identify weaknesses, and maintain the quality of the model over time. It helps in validating that the model is not only performing well during the development phase but also generalizing effectively to new, unseen data.</p>
			<p>The content of an evaluation dataset depends on the specific application and its goals. Generally, it should include the following:</p>
			<ul>
				<li><strong class="bold">Representative queries</strong>: A variety of questions or inputs that the AI is likely to encounter in real-world usage. These should cover different scenarios and edge cases to ensure a comprehensive evaluation.</li>
				<li><strong class="bold">Expected outputs</strong>: Corresponding correct or ideal responses for each query, against which the AI’s responses will be compared.</li>
				<li><strong class="bold">Diverse data</strong>: Data that reflects the diversity of inputs the model will face, including variations in language, format, and context. This helps in assessing the model’s robustness and ability to handle different types of input.</li>
			</ul>
			<p>For example, the evaluation dataset for the MongoDB documentation chatbot includes questions and answers to the top 250 search terms, top 250 support questions by volume, and some of the most common questions asked about MongoDB. This can take the form of simple keywords or actual phrases in full-sentence format, like so:</p>
			<pre class="source-code">
Mongodb install
Install mongodb ubuntu
Mongodb connection string
$in mongodb
How to create a collection in mongodb
What is an aggregation pipeline
Cannot deserialize a 'String' from BsonType 'ObjectId' in C#</pre>			<p>These terms and questions were retrieved from a combination of sources, which will vary depending on your infrastructure. For MongoDB, this infrastructure comes from the Google search console for <a href="http://mongodb.com">mongodb.com</a> as well as the support chat, community forums, and Stack Overflow.</p>
			<p>Determining the right amount of evaluation involves balancing thoroughness with practicality. You should have enough data to cover a wide range of scenarios and ensure the outputs of your GenAI application are consistently accurate and reliable. Typically, this involves hundreds or even thousands of data points, depending on the complexity of the application.</p>
			<p>That said, while more data can provide a more comprehensive assessment, there is a point of diminishing returns where additional data does not significantly improve the evaluation but adds to the complexity and resource requirements. <strong class="bold">Over-evaluation</strong> can also lead to overfitting of the evaluation dataset rather than improving overall performance. Returning to the earlier student/exam analogy, you don’t want your evaluation exam to be an exact replica of the training materials because all you would be testing then is whether the student was able to memorize a question and response. You would not be testing how well the student has learned the material.</p>
			<p>In summary, ensuring high-quality, accurate, and comprehensive training and evaluation datasets can reduce the likelihood of the model learning incorrect patterns. This requires significant effort upfront before deployment of the GenAI application but can dramatically improve your GenAI accuracy and depth of response as well as ensure its quality of responses to your users.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor258"/>Few-shot prompting</h2>
			<p>In many cases, you will be asking your GenAI application to produce new content or summarize existing content, in which case what you need to do is provide the existing application with a properly engineered prompt. Most of the time, having the user simply request what they need is sufficient. But in cases where the outputs are complex, you will find that the quality and accuracy of the GenAI application’s response are improved by using a technique called <strong class="bold">few-shot prompting</strong>. Few-shot prompting is when you provide an example as part of the input to the LLM so that it can see exactly what type of syntax and response you need. You can also include a definition as part of the example in case you believe the input might be a term with which the LLM would not be familiar, or in case you’re using a business-specific term.</p>
			<p>Let’s try out an example using GPT-4.</p>
			<p><strong class="bold">Example 1</strong>: Let’s see how the LLM responds to a request that does not use few-shot prompting.</p>
			<p>This is the user input:</p>
			<pre class="source-code">
Please generate a linkedin post less than 500 words about the company OpenAI.</pre>			<p>Here is the output:</p>
			<pre class="source-code">
🚀 Pioneering the Future with OpenAI 🚀
In the dynamic world of technology, OpenAI stands as a beacon of innovation and progress. Their mission to advance artificial general intelligence (AGI) is not just envisioning the future; they are actively shaping it.
OpenAI operates under the guidance of visionary leaders like CEO Sam Altman. Their cutting-edge research is pushing the boundaries of what's possible in AI, crafting a future where technology enhances human capabilities and opens new realms of possibility.
From healthcare to finance, and education to entertainment, OpenAI's innovations are leaving an indelible mark. Their models, such as GPT-4, are revolutionizing how we interact with technology. Businesses are leveraging these tools to enhance customer experiences, streamline operations, and achieve unprecedented levels of productivity.
OpenAI thrives on collaboration, partnering with organizations worldwide to accelerate innovation. Their open research culture demonstrates a belief in collective progress. By sharing insights, tools, and findings, OpenAI is democratizing access to AI, empowering developers, researchers, and businesses to push the boundaries of what's possible. Through initiatives like the OpenAI Scholars and Fellows programs, they are nurturing the next generation of AI talent. These programs provide mentorship, resources, and opportunities to individuals from diverse backgrounds, ensuring a wealth of perspectives and ideas in AI development.
Join the conversation and be part of the future OpenAI is creating.
#AI #OpenAI #Innovation #Technology #Future #Collaboration #Leadership</pre>			<p><strong class="bold">Example 2</strong>: Now, let’s try this example with few-shot prompting.</p>
			<p>This is the user input:</p>
			<pre class="source-code">
Please generate a linkedin post focusing on the OpenAI company. It should be less than 500 words in length. Also, please match the tone, syntax, word choices, and style of this passage:"There's a way to do it better. Find it." — Thomas Edison
"Lies. We are being lied to. We are told that technology takes our jobs, reduces our wages, increases inequality, threatens our health, ruins the environment, degrades our society, corrupts our children, impairs our humanity, threatens our future, and is ever on the verge of ruining everything.
We are told to be angry, bitter, and resentful about technology. We are told to be pessimistic. The myth of Prometheus – in various updated forms like Frankenstein, Oppenheimer, and Terminator – haunts our nightmares. We are told to denounce our birthright – our intelligence, our control over nature, our ability to build a better world. We are told to be miserable about the future." –Marc Andreessen</pre>			<p>Here is the output:</p>
			<pre class="source-code">
We are told that artificial intelligence is a threat, that it steals our jobs, that it widens the chasm of inequality, that it's an invisible force eroding our humanity. We are told that AI is an uncontrollable beast, ready to plunge us into dystopia.
OpenAI is here to shatter these myths.
Founded on the principle of advancing digital intelligence for the collective good, OpenAI isn't just another tech company. It's a bold proclamation that the future is ours to shape. OpenAI stands as a beacon against the darkness of misinformation and fearmongering. It's a rallying cry for innovation, for the relentless pursuit of better.
Imagine a world where AI enhances every aspect of our lives—where it amplifies human potential rather than diminishes it. OpenAI's groundbreaking models, like GPT-4, are not instruments of destruction but tools of empowerment. From revolutionizing healthcare to transforming education, from pioneering new frontiers in entertainment to optimizing complex financial systems, OpenAI's innovations are the keystones of a new era.
OpenAI is driven by visionaries who refuse to bow to the pessimism that pervades our discourse. That we are a testament to what can be achieved when we refuse to accept the status quo. We are the architects of tomorrow, wielding AI not as a weapon, but as a bridge.</pre>			<p>You can see how, by providing the example text, you can enhance the output to make it match whatever syntax you prefer. The additional prompt information need not be terribly difficult to produce either. If you can provide an example output to your GenAI application, its results will be much nearer to what you desire.</p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor259"/>Retrieval and reranking</h2>
			<p>Retrieval and reranking are key techniques used to enhance the performance and accuracy of LLMs. First, understand that by retrieving relevant context or documents, an LLM provides more accurate and contextually relevant responses. This is particularly useful when the model’s training data does not cover the specifics of the query or when up-to-date information is required.</p>
			<p>In the context of LLMs, <strong class="bold">retrieval</strong> can involve searching through a vast collection of documents, knowledge bases, or other data sources to find pieces of information that are pertinent to a given query or task. Let’s have a look at the two different types of retrieval:</p>
			<ul>
				<li><code>cars</code> in your query, it returns documents that contain the word <em class="italic">cars</em>.</li>
				<li><strong class="bold">Embedding-based retrieval</strong>: This uses vector embeddings to find matching documents. Both the query and documents are transformed into vectors in a high-dimensional space. Retrieval then involves finding vectors (that is, documents) that are close to the query vector.</li>
			</ul>
			<p><strong class="bold">Reranking</strong> is the process of reordering the retrieved documents or pieces of information to prioritize the most relevant ones. After the initial retrieval, the documents are ranked based on their relevance to the query. Retrieved documents are initially ranked based on their similarity to the query using methods such as cosine similarity in embedding space. However, a more sophisticated model can rerank these initially retrieved documents by considering additional features and context.</p>
			<p>Let’s look at the following examples.</p>
			<p><strong class="bold">Example 1</strong>: Recommending restaurants with a GenAI application.</p>
			<p>You have built a GenAI application that provides restaurant recommendations. A user requests restaurants currently open near them. When examining the potential restaurants to provide to the user, the application looks at the distance from the user’s current location or provided address and the current local time and opening hours.</p>
			<p>It will then rank the results so that the closest restaurant is the first one shown to the user. This is a perfectly fine solution. But you may want to have smarter results that are dynamically reranked based on other criteria, such as user ratings for the restaurants. You may want to show a higher-rated restaurant that is three miles away first, rather than a one-star restaurant that is one mile away. As the user gives feedback on the results, you may want to rerank dynamically, expanding your pool of restaurants as you get more information about what the user would prefer (including, say, the type of cuisine or ambiance).</p>
			<p>By reranking the results, the most relevant and useful information is prioritized, improving the overall quality of the LLM’s output. It helps in filtering out less relevant or redundant information, ensuring the response is precise and useful.</p>
			<p>When combined, retrieval and reranking significantly enhance LLM outputs with the following:</p>
			<ul>
				<li>The model can access and utilize relevant information that might not be present in its training data, providing more accurate and contextually appropriate answers.</li>
				<li>By focusing on the most relevant information through reranking, the model’s responses become more precise, reducing errors and irrelevant content.</li>
				<li>Retrieval can pull in the latest information from updated sources, making the model’s responses more current.</li>
				<li>These techniques allow the model to handle specific, detailed queries efficiently without needing to retrain the entire model frequently.</li>
			</ul>
			<p><strong class="bold">Example 2</strong>: Summarizing the latest research on quantum computing.</p>
			<p>Here’s another practical example. Suppose you ask an LLM about the latest research on quantum computing. The steps of the output would be as follows:</p>
			<ol>
				<li><strong class="bold">Retrieval</strong>: The model searches through a large database of scientific papers and articles to find relevant documents on quantum computing.</li>
				<li><strong class="bold">Reranking</strong>: The initially retrieved documents are then reranked, with the most recent and pertinent studies placed at the top.</li>
				<li><strong class="bold">Response generation</strong>: The LLM uses the top-ranked documents to generate a detailed and accurate response about the latest research trends in quantum computing.</li>
			</ol>
			<p>By incorporating retrieval and reranking, the LLM can provide a well-informed, up-to-date, and contextually accurate answer, vastly improving the user experience.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor260"/>Late interaction strategies</h2>
			<p>Now that you’re ready to take your application into production, there are still a few more things you can do to help improve the user experience and create a feedback loop in order to get a better signal as to the behavior of your GenAI application. This next set of recommendations focuses on <strong class="bold">late interaction strategies</strong>, sometimes referred to as <strong class="bold">contextualized late interaction over </strong><strong class="bold">BERT</strong> (<strong class="bold">ColBERT</strong>).</p>
			<p>First, let’s define <strong class="bold">interaction</strong>. Interaction refers to the process of evaluating the relevance between a query and a document by comparing their representations. A late processing strategy is one where the interaction between the query and document representations occurs later in the process, typically after both have been independently encoded. Early interaction models are where query and document embeddings interact at earlier stages, typically before or during their encoding by the model.</p>
			<p>Second, let’s dig a little bit into the internal workings. When a user interacts with a GenAI application, they input a query that is encoded into a dense vector representation. Potential responses, usually documents or passages, are also encoded into dense vector representations. The system performs similarity matching between the query and document embeddings, returning the documents with the highest similarity scores as the best matches.</p>
			<p>To enhance relevance, you don’t return all matching results to the user. Instead, you aim to provide the most relevant results or a summarized version of the result set. Late interaction models such as ColBERT improve efficiency by focusing on the most promising query-document pairs rather than considering all possible pairs, yielding more precise results and a better user experience. This selective approach allows for more precise and relevant results, enhancing the user experience.</p>
			<p>If you need to focus on improving search results, consider implementing ColBERT or similar techniques to enhance retrieval performance and provide more relevant results for user queries.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor261"/>Query rewriting</h2>
			<p><strong class="bold">Query rewriting</strong>, or <strong class="bold">query reformulation</strong>, is a technique used to improve the quality of the answers provided by LLMs. This process involves modifying the original query to make it clearer, more specific, or more detailed, which can help the model generate better responses. LLMs do not explicitly rewrite queries in the background, so this effort is manual unless you have implemented a workflow that will evaluate and rewrite the user’s query before it’s processed.</p>
			<p>Rewriting a query can make it clearer and more precise, reducing ambiguity and ensuring the model understands exactly what is being asked. Adding relevant context or details to the query can help the model provide more accurate and contextually appropriate answers and can help disambiguate terms that have multiple meanings, ensuring the response aligns with the intended meaning. In addition, reformulating the query to include additional relevant details can lead to more comprehensive answers.</p>
			<p>How does query rewriting work? It’s important to understand user intent for your GenAI application. What is the <em class="italic">purpose</em> of your application, and what kinds of questions will your application attempt to answer? Understanding what sort of response users expect versus what your application might deliver is key. After that, you can do the following activities, which are not mutually exclusive, meaning that you can perform some, just one, or none of these.</p>
			<p>For instance, based on the <strong class="bold">intent</strong>, the user query can be augmented with additional <strong class="bold">context</strong> and details. This activity substantially expands the user query (and increases the token count per query) but will typically yield much better results.</p>
			<p>To take an easy example, imagine that your application generates images. The user requests <code>a picture of a kitten</code>, a quite simple query that could have endless results.</p>
			<p>To help the user get better results, you can add three buttons in the UI so that the user can select a <code>a picture of a kitten</code>, the query is modified to the following:</p>
			<pre class="source-code">
An image of a kitten, in anime style, large eyes, chikai, chibi-style, pixel-style, anime illustration, cute, in the style of Akira Toriyama.</pre>			<p>Here, for each button style, you can add the terms that augment the user query and then apply them before submission.</p>
			<p>As another example, consider this user query:</p>
			<pre class="source-code">
"What's the average revenue?"</pre>			<p>A meaningful rewrite could be as follows:</p>
			<pre class="source-code">
"What's the average revenue for [May 2024] for [sales sku 123]?"</pre>			<p>This rewritten query with additional context helps the system understand that the user is asking for a specific product and time period, leading to a more accurate and useful response.</p>
			<p>Ultimately, when conducting query rewrites, you’ll want to <strong class="bold">simplify the language</strong>. Complex queries can be simplified or broken down into simpler parts, making it easier for the model to process and respond accurately. This method involves taking a large query and breaking it into constituent parts (which typically is achieved via a series of input fields/forms) and then unifying each data entry into a single submitted query. This guides your user into constructing a well-formed query without specialized knowledge.</p>
			<p>As an example, imagine your user has only a single-entry field to input their query. In such a case, they may leave out relevant information or provide irrelevant information that could impact accuracy or increase the possibility of hallucination. Instead, if you were to provide the user with a series of fields, each with clear instructions, and then assemble the inputted information into a query that was fed into the GenAI application, you would get a better outcome than a free-form text entry.</p>
			<p>For practical implementation, you could consider a workflow in which the system itself analyzes the query for intent and context, reviews the query’s complexity, and then rewrites the query to be clearer, more specific, or more detailed. The reformulated query can then be used to generate the response.</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor262"/>Testing and red teaming</h1>
			<p>Testing AI systems is critical to ensure their accuracy, reliability, and overall performance. Typically, in software engineering, automated testing is used as part of the software development process. GenAI applications are no different. You’ll want to routinely and regularly test the outputs to ensure there are no radical shifts in output quality.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor263"/>Testing</h2>
			<p>Just like your typical software engineering features, you’ll want to include the phases of unit testing, integration testing, performance testing, and user acceptance into your test plan. However, the specifics of how this is done vary from one use case to another.</p>
			<p>In the context of GenAI applications, <strong class="bold">unit testing</strong> still has the same basic tenets and involves testing individual components or modules of the application to ensure they function correctly. However, in the case of GenAI applications, your unit tests will need to also include steps such as the following:</p>
			<ul>
				<li><strong class="bold">Input validation</strong>: Ensure that the application correctly handles and validates various input types, formats, and ranges. Test for edge cases, such as empty inputs, excessively large inputs, or malformed data.</li>
				<li><strong class="bold">Pre-processing</strong>: Verify that any pre-processing steps, such as tokenization, normalization, or feature extraction, are performed correctly.</li>
				<li><strong class="bold">Model loading</strong>: Test that the model is correctly loaded from its storage location, and verify that the correct version is being used.</li>
				<li><strong class="bold">Model inference</strong>: Ensure that the model generates outputs without errors given valid inputs. Test the inference function with controlled inputs to verify expected behavior, such as deterministic responses for certain prompts or scenarios.</li>
				<li><strong class="bold">Output format</strong>: Validate that the generated outputs meet the expected format and structure. This includes checking that outputs are complete, correctly formatted, and adhere to any length or content constraints.</li>
				<li><strong class="bold">Post-processing</strong>: Test any post-processing steps that modify or enhance the model’s output, such as cleaning up text, converting formats, or applying additional business logic.</li>
				<li><strong class="bold">Proper functioning</strong>: The outputs should work. If your GenAI application outputs code, you will need to test that the code itself compiles and behaves as intended.</li>
			</ul>
			<p>These are just a few of the items that you should include for unit testing your GenAI application.</p>
			<p><strong class="bold">Integration testing</strong> focuses on verifying that the components of your GenAI system work together as needed. This means you’ll be testing the interactions between components to check the following:</p>
			<ul>
				<li>Whether your data ingestion pipeline pulls the correct data</li>
				<li>How recommendations are presented to the user (formatting, for instance, if this is done by another library or tool)</li>
				<li>API load testing, if you’re using another LLM such as OpenAI or Anthropic</li>
			</ul>
			<p>You’ll want to evaluate processing time, efficiency, and scalability via <strong class="bold">performance testing</strong>. This might include activities such as the following:</p>
			<ul>
				<li>Load testing your application for how it handles a large volume of simultaneous queries.</li>
				<li>Assessing the inference time of self-hosted models on various hardware configurations.</li>
				<li>Measuring how many token limits should be set for input and output to control costs and processing time.</li>
				<li>Measuring the time taken for the model to generate outputs and ensuring it meets performance requirements. This can be especially important for applications with real-time constraints.</li>
			</ul>
			<p>In addition to this routine testing, you have more to add to your test suite. In general, it is also recommended that GenAI applications go through <strong class="bold">additional testing</strong> for the following:</p>
			<ul>
				<li><strong class="bold">Bias and fairness</strong>: If your model is making recommendations that affect lives and livelihoods, you’ll want to carefully consider training data biases for different demographic groups.</li>
				<li><strong class="bold">Robustness</strong>: To ensure your GenAI application is resilient to variations and noise, you’ll want to test with adversarial examples and edge cases to evaluate its ability to handle unexpected inputs.</li>
			</ul>
			<p>Once you’ve gotten through all of that, you’ll want to think about <strong class="bold">user acceptance testing</strong>, which is one of the most exciting parts of the process, as you will see in the next section.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor264"/>Red teaming</h2>
			<p>If your GenAI application will accept natural language prompts and inputs from human beings, then the practice of <strong class="bold">red teaming</strong> cannot be recommended enough. Red teaming involves simulating real-world, challenging, or adversarial situations to identify vulnerabilities and weaknesses in your GenAI application. This approach is borrowed from cybersecurity practices and is particularly important for ensuring your GenAI application meets user expectations.</p>
			<p>This involves having a large pool of <em class="italic">users</em> who will ask real-world questions, but they are not limited by <em class="italic">scripts</em> as to what they may ask. The reason for red teaming is that GenAI applications can, and often do, produce different outputs that vary widely, even with similar or the same input. Not only that but the quality of the generated output is often subjective and depends on human judgment. So, while traditional software applications produce predictable and consistent results, the same is not true of GenAI. Let’s take an example to see how this works.</p>
			<p>For a chatbot application, you might have routine automated testing that would ask your GenAI application the top 200 most common user questions and then evaluate them for correctness. With a red team, you would have 50 users ask whatever questions they wanted, and then record both the questions asked and the responses. This might yield insights such as the following:</p>
			<ul>
				<li>If a user asks a question in a similar way but not with the exact same wording, they receive incorrect or less correct answers.</li>
				<li>Some users will ask malicious questions and the GenAI application will respond poorly.</li>
				<li>Other users ask questions that are not part of the training data, and the GenAI application hallucinates answers (or gives no answer at all), thus identifying the need to expand your training data.</li>
				<li>When users ask many questions in a row, the application stalls.</li>
				<li>When a user asks specific question types, they are dissatisfied with the output because the application lacks high-quality training data or the formatting of the reply is undesirable.</li>
				<li>When properly prompted, the GenAI application will share details of other users’ sessions, thus identifying a security issue.</li>
			</ul>
			<p>To enable the red-teaming phase, it is recommended that you record every question asked by every user, as well as every response given, and then ask testers to rate the response with notes. While this level of detailed user testing is strenuous and uncommon in software development, it is incredibly valuable to see how your application performs in real-world scenarios, with real human beings, before production.</p>
			<p>Due to the scale and scope of some AI systems, fully testing each component is impossible. Effective testing and red teaming rely on using judgment in terms of which parts of the system are most risky. It may be true that giving occasionally not-quite-accurate advice is a non-impactful event. However, the potential harm of a single hallucination could be quite high. You will want to consider the severity of harm, the likelihood of inaccuracy, and the ability to retract or rectify the inaccuracy as your standard measures of risk. Using those simple, albeit subjective, measures can assist you in determining to what extent you test each aspect of the system, and the size of your red team.</p>
			<p>To give yourself a sense of what sorts of harms and incidents you will be testing for—which are too many to enumerate—you will find it helpful to review the AI Incident Database at <a href="https://incidentdatabase.ai/">https://incidentdatabase.ai/</a>. Upon review of this tool, you may find your specific use case (or ones like it) and what incidents have already been reported, so that you can test and think through the repercussions of inaccuracies.</p>
			<p>As an example, one incident that is detailed here involved an application that made staffing-level recommendations. However, the algorithm-based recommendations left facilities understaffed, leading to critical incidents of neglect, injury, and death. Those incidents then prompted lawsuits and even legislation against healthcare providers using AI.</p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor265"/>Information post-processing</h1>
			<p>You might know that the main way in which GenAI differs from previous forms of AI or analytics is that it generates new content efficiently. But did you know that that content is often in <em class="italic">unstructured</em> forms, for example, written text or images? When you see outputs that are nicely formatted, in bulleted lists, multiple fonts, and so on, it is a form of <strong class="bold">information post-processing</strong>.</p>
			<p>Information post-processing refers to the series of steps taken after an AI model generates an initial response, but before that response is sent to the user. This crucial step enhances the output of GenAI models, refining raw responses to make them more useful, accurate, and contextually appropriate. It can take many forms, so this chapter will only discuss some of the most useful ones along with information on how to implement them:</p>
			<ul>
				<li><strong class="bold">Fact-checking</strong>: Verifying the accuracy of the information provided. This can involve checking facts against reliable sources or databases.</li>
				<li><strong class="bold">Formatting</strong>: Structuring the information in a clear and readable format, such as bullet points, paragraphs, or tables. This may also include style changes such as bold, text color, or font to enhance readability and emphasis.</li>
				<li><strong class="bold">Grammar, style, and tone checking</strong>: At times, the resulting text provided by GenAI applications is not up to par or consistent with the exact messaging, tone, and style that one would expect a human being to write. Post-processing tools and vendors can take generated text outputs and markedly improve them for readability, making them match reader expectations.</li>
			</ul>
			<p>Information post-processing is a vital component in the lifecycle of GenAI outputs. It bridges the gap between raw model outputs and polished, user-ready responses, enhancing accuracy, readability, relevance, and overall user satisfaction. By implementing effective post-processing strategies, AI systems can deliver higher-quality and more reliable results.</p>
			<p>There are entire services springing up around this valuable step in the GenAI process, so engineers do not have to build it themselves.</p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor266"/>Other remedies</h1>
			<p>Some other technical remedies can be employed even more easily than the ones detailed in this chapter. Some of these may improve the accuracy and performance of your GenAI application, though the level of effort involved varies. As an example, during MongoDB’s testing of GPT, it was discovered that the accuracy rate for the same set of questions was improved by 7% between GPT-3.5 and GPT-4. Getting such a level of improvement in accuracy via prompting, retrieval augmentation, or late interaction strategies is certainly possible but would have been difficult.</p>
			<p>So, it is worth investigating every avenue of potential improvement, including areas such as hardware upgrades, code optimization, concurrency management, database query optimization, and even just upgrading your software. All of these can improve the results of your GenAI application and should be independently investigated:</p>
			<ul>
				<li><strong class="bold">Hardware and software upgrades</strong>: Upgrade computational resources, such as using more powerful GPUs, scaling horizontally with more servers, or updating to the latest version of the software, to outsize impacts on both accuracy and performance.</li>
				<li><strong class="bold">Code optimization</strong>: Refactor and optimize code to improve efficiency, reduce computational load, and handle data more effectively.</li>
				<li><strong class="bold">Network optimization</strong>: Reduce network latency by optimizing data transfer, caching responses, and minimizing API call overheads.</li>
				<li><strong class="bold">Concurrency management</strong>: Implement concurrency and parallel processing techniques to handle multiple requests efficiently.</li>
				<li><strong class="bold">Database optimization</strong>: Optimize database queries and interactions to reduce I/O overhead.</li>
			</ul>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor267"/>Summary</h1>
			<p>Implementing mechanisms to correct and optimize your GenAI application can have many forms and can be implemented before, during, and after answers are generated. For optimal performance, you’ll want to train your GenAI model with high-quality data, supplement existing models with your specific use case data, and have thorough evaluation datasets and record the model’s performance to establish a baseline of accuracy.</p>
			<p>Once you have that baseline, however, you can immediately begin improving upon it with the techniques discussed in this chapter. Among these techniques is one- or few-shot prompting. It involves providing the GenAI model with a single example or prompt to guide its response, enabling the model to generate relevant and contextually appropriate outputs with minimal training data. You can also try retrieving and reranking relevant documents or data points based on the user’s query, and then reordering these results to prioritize the most relevant and useful information before generating a final response. Query rewriting is another technique that can improve clarity, specificity, or context, helping the AI model understand and respond more accurately to the user’s requests.</p>
			<p>Formatting GenAI responses via structuring and presenting the AI-generated content in a clear, organized, and readable manner can enhance the overall user experience and ensure the information is easily digestible. Similarly, implementing late interaction strategies such as ColBERT can improve the relevance and accuracy of the retrieved information. By testing, red teaming, and recording your results, you can track your progress in improving the performance, security, and quality of responses over time.</p>
			<p>GenAI technologies are changing (and will continue to change) the face of the software industry. With these optimization strategies in place, your GenAI application will be well equipped to adapt and excel in an ever-evolving landscape.<a id="_idTextAnchor268"/></p>
		</div>
	

		<div><h1 id="_idParaDest-225"><a id="_idTextAnchor269"/>Appendix: Further Reading</h1>
			<p>In addition to the links provided within the chapters, here are some resources to take your learning journey forward.</p>
			<p><a href="B22495_01.xhtml#_idTextAnchor009"><strong class="bold">Chapter 1</strong></a><strong class="bold">, Getting Started with </strong><strong class="bold">Generative AI</strong></p>
			<ul>
				<li>Gryka, Maciej. “Invest in RAG” in “Building reliable systems out of unreliable agents.” <em class="italic">The Rainforest Blog</em>, April 3, 2024. <a href="https://www.rainforestqa.com/blog/building-reliable-systems-out-of-unreliable-agents#Invest_in_RAG">https://www.rainforestqa.com/blog/building-reliable-systems-out-of-unreliable-agents#Invest_in_RAG</a>.</li>
				<li>“The Black Box: Even AI’s creators don’t understand it.” July 2023. <em class="italic">Unexplainable</em>. Produced by Vox Creative. Podcast, Spotify, 36:15. <a href="https://open.spotify.com/episode/3npjXNCtUSGRUjVR4EYb4Y?si=-XpudYVzSEKfhD0-2NBjEQ">https://open.spotify.com/episode/3npjXNCtUSGRUjVR4EYb4Y?si=-XpudYVzSEKfhD0-2NBjEQ</a>.</li>
			</ul>
			<p><a href="B22495_02.xhtml#_idTextAnchor021"><strong class="bold">Chapter 2</strong></a><strong class="bold">, Building Blocks of </strong><strong class="bold">Intelligent Applications</strong></p>
			<ul>
				<li>Naveed et al. “A Comprehensive Overview of Large Language Models.” arXiv, July 12, 2023. <a href="https://arxiv.org/abs/2307.06435">https://arxiv.org/abs/2307.06435</a>.</li>
			</ul>
			<p><a href="B22495_03.xhtml#_idTextAnchor041"><strong class="bold">Chapter 3</strong></a><strong class="bold">, Large </strong><strong class="bold">Language Models</strong></p>
			<ul>
				<li>“Speech and Language Processing,” n.d., <a href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a>.</li>
				<li>Hochreiter, Sepp, and Jürgen Schmidhuber. “Long Short-Term Memory.” <em class="italic">Neural Computation</em> 9, no. 8 (November 1, 1997): 1735–80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a>.</li>
				<li>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” <em class="italic">arXiv (Cornell University)</em>, January 1, 2017. <a href="https://doi.org/10.48550/arxiv.1706.03762">https://doi.org/10.48550/arxiv.1706.03762</a>.</li>
				<li>“Prompt Engineering Guide – Nextra,” n.d., <a href="https://www.promptingguide.ai/">https://www.promptingguide.ai/</a>.</li>
			</ul>
			<p><a href="B22495_04.xhtml#_idTextAnchor061"><strong class="bold">Chapter 4</strong></a><strong class="bold">, </strong><strong class="bold">Embedding Models</strong></p>
			<ul>
				<li>A. Aruna Gladys and V. Vetriselvi, “Survey on multimodal approaches to emotion recognition,” <em class="italic">Neurocomputing</em> 556 (November 1, 2023): 126693, <a href="https://doi.org/10.1016/j.neucom.2023.126693">https://doi.org/10.1016/j.neucom.2023.126693</a>.</li>
				<li>Sumit Kumar, “Positive and Negative Sampling Strategies for Representation Learning in Semantic Search,” Sumit’s Diary, March 22, 2023, <a href="https://blog.reachsumit.com/posts/2023/03/pairing-for-representation">https://blog.reachsumit.com/posts/2023/03/pairing-for-representation</a>.</li>
				<li>Tomas Mikolov et al., “Efficient Estimation of Word Representations in Vector Space,” arXiv.org, January 16, 2013, <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.</li>
				<li>OpenAI, “GPT-4”. GPT-4 Research, March 14, 2023. <a href="https://openai.com/index/gpt-4-research">https://openai.com/index/gpt-4-research</a>.</li>
				<li>Jeffrey Pennington, “GloVe: Global Vectors for Word Representation,” n.d., <a href="https://nlp.stanford.edu/projects/glove">https://nlp.stanford.edu/projects/glove</a>.</li>
				<li>Jacob Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” arXiv.org, October 11, 2018, <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>.</li>
				<li>“fastText,” n.d., <a href="https://fasttext.cc/">https://fasttext.cc/</a>.</li>
				<li>Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. “Deep contextualized word representations,” arXiv:1802.05365, March 22, 2018. <a href="https://arxiv.org/pdf/1802.05365">https://arxiv.org/pdf/1802.05365</a>.</li>
				<li> Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” arXiv.org, September 4, 2014, <a href="https://arxiv.org/abs/1409.1556v6">https://arxiv.org/abs/1409.1556v6</a>.</li>
				<li>Kaiming He et al., “Deep Residual Learning for Image Recognition,” arXiv.org, December 10, 2015, <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>.</li>
				<li>Aurora Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo Bello, “OpenL3 — OpenL3 0.4.2 documentation,” n.d., <a href="https://openl3.readthedocs.io/en/latest/#">https://openl3.readthedocs.io/en/latest/#</a>.</li>
				<li>“Google | vggish | Kaggle,” n.d., <a href="https://www.kaggle.com/models/google/vggish">https://www.kaggle.com/models/google/vggish</a>.</li>
				<li>Tran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri, M., “Learning Spatiotemporal Features with 3D Convolutional Networks.” arXiv:1412.0767, October 7, 2015. <a href="https://arxiv.org/pdf/1412.0767">https://arxiv.org/pdf/1412.0767</a>.</li>
				<li>Grover, A., and Leskovec, J. “Node2Vec: Scalable Feature Learning for Networks.” <em class="italic">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 2016. <a href="https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf">https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf</a>.</li>
				<li>Bryan Perozzi, Rami Al-Rfou, and Steven Skiena, “DeepWalk,” August 24, 2014, <a href="https://doi.org/10.1145/2623330.2623732">https://doi.org/10.1145/2623330.2623732</a>.</li>
				<li>Zhang, S., and Xu, Y. “Json2Vec: A Representation Learning Method for JSON Data.” arXiv:2002.05707, February 13, 2020. <a href="https://arxiv.org/pdf/2002.05707">https://arxiv.org/pdf/2002.05707</a>.</li>
				<li>Alec Radford et al., “Learning Transferable Visual Models From Natural Language Supervision,” arXiv.org, February 26, 2021, <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a>.</li>
			</ul>
			<p><a href="B22495_05.xhtml#_idTextAnchor115"><strong class="bold">Chapter 5</strong></a><strong class="bold">, </strong><strong class="bold">Vector Databases</strong></p>
			<ul>
				<li>Yu. A. Malkov and D. A. Yashunin, “Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs,” arXiv.org, March 30, 2016, <a href="http://arxiv.org/abs/1603.09320">http://arxiv.org/abs/1603.09320</a>.</li>
				<li>Yikun Han, Chunjiang Liu, and Pengfei Wang, “A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge,” arXiv.org, October 18, 2023, <a href="http://arxiv.org/abs/2310.11703">http://arxiv.org/abs/2310.11703</a>.</li>
				<li>Zhi Jing et al., “When Large Language Models Meet Vector Databases: A Survey,” arXiv.org, January 30, 2024, <a href="http://arxiv.org/abs/2402.01763">http://arxiv.org/abs/2402.01763</a>.</li>
				<li>Doug Turnbull, “What Is a Judgment List?,” Doug Turnbull’s Blog, February 21, 2021, <a href="https://softwaredoug.com/blog/2021/02/21/what-is-a-judgment-list">https://softwaredoug.com/blog/2021/02/21/what-is-a-judgment-list</a>.</li>
				<li>“Building RAG-based LLM Applications for Production,” Anyscale, n.d., <a href="https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1">https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1</a>.</li>
				<li>“How to Perform Hybrid Search - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/">https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/</a>.</li>
				<li>“Review Deployment Options - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/deployment-options/">https://www.mongodb.com/docs/atlas/atlas-vector-search/deployment-options/</a>.</li>
			</ul>
			<p><a href="B22495_06.xhtml#_idTextAnchor137"><strong class="bold">Chapter 6</strong></a><strong class="bold">, AI/ML </strong><strong class="bold">Application Design</strong></p>
			<ul>
				<li>“How to Index Fields for Vector Search - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#considerations">https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#considerations</a>.</li>
				<li>Lauren Schaefer Daniel Coupal, “Bloated Documents | MongoDB,” May 31, 2022, <a href="https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-bloated-documents/">https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-bloated-documents/</a>.</li>
				<li>Daniel Coupal, “Building with Patterns: The Extended Reference Pattern,” MongoDB, March 19, 2019, <a href="https://www.mongodb.com/blog/post/building-with-patterns-the-extended-reference-pattern">https://www.mongodb.com/blog/post/building-with-patterns-the-extended-reference-pattern</a>.</li>
				<li>“Atlas Cluster Sizing and Tier Selection - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/sizing-tier-selection/">https://www.mongodb.com/docs/atlas/sizing-tier-selection/</a>.</li>
				<li>“Customize Cluster Storage - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/customize-storage/">https://www.mongodb.com/docs/atlas/customize-storage/</a>.</li>
				<li>“Amazon EBS volume types - Amazon EBS,” n.d., <a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html#gp3-ebs-volume-type">https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html#gp3-ebs-volume-type</a>.</li>
				<li>“Customize Cluster Storage - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/customize-storage/">https://www.mongodb.com/docs/atlas/customize-storage/</a>.</li>
			</ul>
			<p><a href="B22495_07.xhtml#_idTextAnchor162"><strong class="bold">Chapter 7</strong></a><strong class="bold">, Useful Frameworks, Libraries, </strong><strong class="bold">and APIs</strong></p>
			<ul>
				<li>“MongoDB Atlas,” LangChain, n.d., <a href="https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/">https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/</a>.</li>
				<li>“How to Index Fields for Vector Search - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/manage-indexes/">https://www.mongodb.com/docs/atlas/atlas-vector-search/manage-indexes/</a>.</li>
				<li>“Get Started with the LangChain Integration - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/ai-integrations/langchain/">https://www.mongodb.com/docs/atlas/atlas-vector-search/ai-integrations/langchain/</a>.</li>
				<li>“MongoDB with Python - MongoDB Documentation,” n.d., <a href="https://www.mongodb.com/docs/languages/python/#integrations">https://www.mongodb.com/docs/languages/python/#integrations</a>.</li>
				<li>“Transformers,” n.d., <a href="https://huggingface.co/docs/transformers/en/index">https://huggingface.co/docs/transformers/en/index</a>.</li>
				<li>“OpenAI developer platform,” OpenAI Platform, n.d., <a href="https://platform.openai.com/docs/overview">https://platform.openai.com/docs/overview</a>.</li>
			</ul>
			<p><a href="B22495_08.xhtml#_idTextAnchor180"><strong class="bold">Chapter 8</strong></a><strong class="bold">, Implementing Vector Search in </strong><strong class="bold">AI Applications</strong></p>
			<ul>
				<li>Yunfan Gao et al., “Retrieval-Augmented Generation for Large Language Models: A Survey,” arXiv.org, December 18, 2023, <a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a>.</li>
				<li>Rupak Roy, “Harness LLM Output-parsers like CommaSeparatedListOutputParser, PydanticOutputParser and more for a Structured Ai | by Rupak (Bob) Roy - II | Medium | Medium,” <em class="italic">Medium</em>, August 14, 2024, <a href="https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834">https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834</a>.</li>
				<li>Mirjam Minor and Eduard Kaucher, “Retrieval Augmented Generation with LLMs for Explaining Business Process Models,” in <em class="italic">Lecture Notes in Computer Science</em>, 2024, 175–90, <a href="https://doi.org/10.1007/978-3-031-63646-2_12">https://doi.org/10.1007/978-3-031-63646-2_12</a>.</li>
			</ul>
			<p><a href="B22495_09.xhtml#_idTextAnchor193"><strong class="bold">Chapter 9</strong></a><strong class="bold">, LLM </strong><strong class="bold">Output Evaluation</strong></p>
			<ul>
				<li>“Papers with Code - Measuring Massive Multitask Language Understanding,” September 7, 2020, <a href="https://paperswithcode.com/paper/measuring-massive-multitask-language">https://paperswithcode.com/paper/measuring-massive-multitask-language</a>.</li>
				<li>“Papers with Code - HellaSwag: Can a Machine Really Finish Your Sentence?,” May 19, 2019, <a href="https://paperswithcode.com/paper/hellaswag-can-a-machine-really-finish-your">https://paperswithcode.com/paper/hellaswag-can-a-machine-really-finish-your</a>.</li>
				<li>“Papers with Code - Evaluating Large Language Models Trained on Code,” July 7, 2021, <a href="https://paperswithcode.com/paper/evaluating-large-language-models-trained-on">https://paperswithcode.com/paper/evaluating-large-language-models-trained-on</a>.</li>
				<li>“Introduction | Ragas,” n.d., <a href="https://docs.ragas.io/en/stable/index.html">https://docs.ragas.io/en/stable/index.html</a>.</li>
			</ul>
			<p><a href="B22495_10.xhtml#_idTextAnchor214"><strong class="bold">Chapter 10</strong></a><strong class="bold">, Refining the Semantic Data Model to </strong><strong class="bold">Improve Accuracy</strong></p>
			<ul>
				<li>“SentenceTransformers Documentation — Sentence Transformers documentation,” n.d., <a href="https://sbert.net/">https://sbert.net/</a>.</li>
				<li>“Train and Fine-Tune Sentence Transformers Models,” n.d., <a href="https://huggingface.co/blog/how-to-train-sentence-transformers">https://huggingface.co/blog/how-to-train-sentence-transformers</a>.</li>
				<li>“Run Vector Search Queries - MongoDB Atlas,” n.d., <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search-pre-filter">https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search-pre-filter</a>.</li>
				<li>“Knowledge Graph RAG Query Engine - LlamaIndex,” n.d., <a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/">https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/</a>.</li>
			</ul>
			<p><a href="B22495_11.xhtml#_idTextAnchor232"><strong class="bold">Chapter 11</strong></a><strong class="bold">, Common Failures of </strong><strong class="bold">Generative AI</strong></p>
			<ul>
				<li>Lance Eliot, “Doctors Relying On Generative AI To Summarize Medical Notes Might Unknowingly Be Taking Big Risks,” <em class="italic">Forbes</em>, July 2, 2024, <a href="https://www.forbes.com/sites/lanceeliot/2024/02/05/doctors-relying-on-generative-ai-to-summarize-medical-notes-might-unknowingly-be-taking-big-risks/">https://www.forbes.com/sites/lanceeliot/2024/02/05/doctors-relying-on-generative-ai-to-summarize-medical-notes-might-unknowingly-be-taking-big-risks/</a>.</li>
				<li>Markman, Ofer. “Time to Strategize: 85% of Data is Garbage or Siloed.” <em class="italic">Filo Focus</em>, February 11, 2024. <a href="https://www.filo.systems/blog/85-percent-of-data-is-not-actionable-time-to-restrategize">https://www.filo.systems/blog/85-percent-of-data-is-not-actionable-time-to-restrategize</a>.</li>
				<li>Neeman, Ella, Roee Aharoni, Or Honovich, et al. “DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering.” arXiv.org, November 10, 2022. <a href="https://arxiv.org/pdf/2211.05655">https://arxiv.org/pdf/2211.05655</a>.</li>
				<li>Sharma, Mrinank, Meg Tong, Tomasz Korbak, et al. “Towards Understanding Sycophancy in Language Models.” arXiv.org, October 20, 2023. <a href="https://arxiv.org/abs/2310.13548">https://arxiv.org/abs/2310.13548</a>.</li>
				<li>Sparkes, Matthew. “AI chatbots become more sycophantic as they get more advanced.” <em class="italic">New Scientist</em>, August 17, 2023. <a href="https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/">https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/</a>.</li>
				<li>Wei, Jerry, Da Huang, Yifeng Lu, et al. “Simple synthetic data reduces sycophancy in large language models.” arXiv.org, August 7, 2023. <a href="https://arxiv.org/abs/2308.03958">https://arxiv.org/abs/2308.03958</a>.</li>
			</ul>
			<p><a href="B22495_12.xhtml#_idTextAnchor253"><strong class="bold">Chapter 12</strong></a><strong class="bold">, Correcting and Optimizing Your Generative </strong><strong class="bold">AI Application</strong></p>
			<ul>
				<li>Chui, Michael, Roger Roberts, Tanya Rodchenko, et al. “What every CEO should know about generative AI.” McKinsey Digital, May 12, 2023. <a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/what-every-ceo-should-know-about-generative-ai">https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/what-every-ceo-should-know-about-generative-ai</a>.</li>
				<li>Xiao, Han. “What is ColBERT and Late Interaction and Why They Matter in Search?,” Jina AI, February 20, 2024. <a href="https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/">https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/</a>.</li>
			</ul>
		</div>
	</body></html>