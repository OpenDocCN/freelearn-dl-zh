<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building your Own Chatbot</h1>
                </header>
            
            <article>
                
<p>Chatbots, better referred to as conversation software, are amazing tools for a lot of businesses. They help businesses serve their client's server 24/7 without increasing effort, with consistent quality, and the built-in option to defer to a human when bots are not enough.</p>
<p>They are a great example of where technology and AI has come together to improve the impact of human effort.</p>
<p class="mce-root">They range from voice-based solutions such as Alexa, to text-based Intercom chat boxes, to menu-based navigation in Uber.</p>
<p class="mce-root">A common misconception is that building chatbots needs large teams and a lot of machine learning expertise, though this is true if you are trying to build a <em>generic</em> chatbot platform like Microsoft or Facebook (or even Luis, Wit.ai, and so on).</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li class="mce-root">Why build a chatbot?</li>
<li>Figuring out the right user intent</li>
<li>Bot responses</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why chatbots as a learning example?</h1>
                </header>
            
            <article>
                
<p class="mce-root">So far, we have built an application for every NLP topic that we have seen:</p>
<ul>
<li class="mce-root">Text cleaning using grammar and vocabulary insights</li>
<li class="mce-root">Linguistics (and statistical parsers), to mine questions from text</li>
<li class="mce-root">Entity recognition for information extraction</li>
<li>Supervised text classification using both machine learning and deep learning</li>
<li class="mce-root">Text similarity using text-based vectors such as GloVe/word2vec</li>
</ul>
<p class="mce-root"/>
<p>We will now combine all of them into a much more complicated setup and write our own chatbot from scratch. But, before you build anything from scratch, you should ask why.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why build a chatbot?</h1>
                </header>
            
            <article>
                
<p>A related questions is why should we build our own chatbots? <strong>Why can't I use FB/MSFT/some other cloud service?</strong></p>
<p>Perhaps, a better question to ask is <em>when</em> to build on your own? These are the factors to keep in mind when making this decision:<br/>
<br/>
<strong>Privacy and competition</strong>:<strong> </strong>As a business, is it a good idea to share information about your users with Facebook or Microsoft? Or even a smaller company?</p>
<p><strong>Cost and constraints</strong>: Your funky cloud limits your design choices that are made by a particular intelligence provider to those that are made by the likes of Google or Facebook. Additionally, you are now paying for each HTTP call you make, which is slower than running code locally.</p>
<p><strong>Freedom to customize and extend</strong>: You can develop a solution that performs better for you! You don't have to cure world hunger <span>–</span>just keep shipping an everi-ncreasing business value via quality software. If you are at a big company, you have all the more reason to invest in extendible software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quick code means word vectors and heuristics</h1>
                </header>
            
            <article>
                
<p>For the sake of simplicity, we will assume that our bot does not need to remember the context of any question. Therefore it sees input, responds to it, and is done. No links are established with the previous input.</p>
<p>Let's start by simply loading the word vectors using <kbd>gensim</kbd>:</p>
<pre>import numpy as np<br/>import gensim<br/>print(f"Gensim version: {gensim.__version__}")<br/><br/>from tqdm import tqdm<br/>class TqdmUpTo(tqdm):<br/>    def update_to(self, b=1, bsize=1, tsize=None):<br/>        if tsize is not None: self.total = tsize<br/>        self.update(b * bsize - self.n)<br/><br/>def get_data(url, filename):<br/>    """<br/>    Download data if the filename does not exist already<br/>    Uses Tqdm to show download progress<br/>    """<br/>    import os<br/>    from urllib.request import urlretrieve<br/>    <br/>    if not os.path.exists(filename):<br/><br/>        dirname = os.path.dirname(filename)<br/>        if not os.path.exists(dirname):<br/>            os.makedirs(dirname)<br/><br/>        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:<br/>            urlretrieve(url, filename, reporthook=t.update_to)<br/>    else:<br/>        print("File already exists, please remove if you wish to download again")<br/><br/>embedding_url = 'http://nlp.stanford.edu/data/glove.6B.zip'<br/>get_data(embedding_url, 'data/glove.6B.zip')</pre>
<p>Phew, this might take a minute depending on your download speed. Once this is done, let's unzip the file, get it to the data directory, and convert it into <kbd>word2vec</kbd> format:</p>
<pre># !unzip data/glove.6B.zip <br/># !mv -v glove.6B.300d.txt data/glove.6B.300d.txt <br/># !mv -v glove.6B.200d.txt data/glove.6B.200d.txt <br/># !mv -v glove.6B.100d.txt data/glove.6B.100d.txt <br/># !mv -v glove.6B.50d.txt data/glove.6B.50d.txt <br/><br/>from gensim.scripts.glove2word2vec import glove2word2vec<br/>glove_input_file = 'data/glove.6B.300d.txt'<br/>word2vec_output_file = 'data/glove.6B.300d.txt.word2vec'<br/>import os<br/>if not os.path.exists(word2vec_output_file):<br/>    glove2word2vec(glove_input_file, word2vec_output_file)</pre>
<p>By the end of the preceding code block, we have the 300-dimension GloVe embedding from the official Stanford source converted into the word2vec format.</p>
<p class="mce-root"/>
<p>Let's load this into our working memory:</p>
<pre>%%time<br/>from gensim.models import KeyedVectors<br/>filename = word2vec_output_file<br/>embed = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)</pre>
<p>Let's quickly check whether we can vectorize any word by checking for word embeddings for any word, for example, <kbd>awesome</kbd>:</p>
<pre>assert embed['awesome'] is not None</pre>
<p><kbd>awesome</kbd>, this works!</p>
<p class="mce-root">Now, let's take a look at our first challenge.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Figuring out the right user intent</h1>
                </header>
            
            <article>
                
<p class="mce-root">This is commonly referred to as the problem of intent categorization.</p>
<p class="mce-root">As a toy example, we will try to build an order bot that someone like DoorDash/Swiggy/Zomato might use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case – food order bot</h1>
                </header>
            
            <article>
                
<p class="mce-root">Consider the following sample sentence: <em>I'm looking for a cheap Chinese place in Indiranagar</em>.</p>
<p class="mce-root">We want to pick out Chinese as a cuisine type in the sentence. We can obviously take simple approaches, like exact substring matching (search <em>Chinese</em>) or TF-IDF-based matches.</p>
<p class="mce-root">Instead, we will generalize the model to discover cuisine types that we might not have identified yet, but that can learn about via the GloVe embedding.</p>
<p class="mce-root">We'll keep it as simple as possible: we'll provide some example cuisine types to tell the model that we need cuisines, and look for the most similar words in the sentence.</p>
<p class="mce-root">We'll loop through the words in the sentence and pick out the ones whose similarity to the reference words is above a certain threshold.</p>
<p class="mce-root"/>
<p class="mce-root"><strong>Do word vectors even work for this?</strong></p>
<pre>cuisine_refs = ["mexican", "thai", "british", "american", "italian"]<br/>sample_sentence = "I’m looking for a cheap Indian or Chinese place in Indiranagar"</pre>
<p><span>For simplicity's sake, the following code is written as <kbd>for</kbd> loops, but can be vectorized for speed.</span></p>
<div class="text_cell_render rendered_html">
<p>We iterate over each word in the input sentence and find the similarity score with respect to known cuisine words.</p>
<p>The higher the value, the more likely the word is to be something related to our cuisine references or <kbd>cuisine_refs</kbd>:</p>
</div>
<pre>tokens = sample_sentence.split()<br/>tokens = [x.lower().strip() for x in tokens] <br/>threshold = 18.3<br/>found = []<br/>for term in tokens:<br/>    if term in embed.vocab:<br/>        scores = []<br/>        for C in cuisine_refs:<br/>            scores.append(np.dot(embed[C], embed[term].T))<br/>            # hint replace above above np.dot with: <br/>            # scores.append(embed.cosine_similarities(&lt;vector1&gt;, &lt;vector_all_others&gt;))<br/>        mean_score = np.mean(scores)<br/>        print(f"{term}: {mean_score}")<br/>        if mean_score &gt; threshold:<br/>            found.append(term)<br/>print(found)</pre>
<p>The following is the corresponding output:</p>
<pre>looking: 7.448504447937012
for: 10.627421379089355
a: 11.809560775756836
cheap: 7.09670877456665
indian: 18.64516258239746
or: 9.692893981933594
chinese: 19.09498405456543
place: 7.651237487792969
in: 10.085711479187012
['indian', 'chinese']</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The threshold is determined empirically. Notice that we are able to infer <em>Indian</em> and <em>Chinese</em> as cuisines, even if they are not part of the original set.<br/>
<br/>
Of course, exact matches will have a much higher score.<br/>
<br/>
This is a good example where there's a better problem formulation in terms of the <em>generic</em> cuisine type that can be learned. This is more helpful than a dictionary-based cuisine type. This also proves that we can rely on word-vector-based approaches.<br/>
<br/>
Can we extend this for user intent classification? Let's try this next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying user intent</h1>
                </header>
            
            <article>
                
<p class="mce-root">We want to be able to put sentences into categories by user <em>intents</em>. Intents are a generic mechanism that combine multiple individual examples into one semantic umbrella. For example, <em>hi</em>, <em>hey</em>, <em>good morning</em>, and <em>wassup!</em> are all examples of the <kbd>_greeting_</kbd> intent.</p>
<p class="mce-root">Using <em>greeting</em> as an input, the backend logic can then determine how to respond to the user.</p>
<p class="mce-root">There are many ways we could combine word vectors to represent a sentence, but again we're going to do the simplest thing possible: add them up.</p>
<p class="mce-root">This is definitely a less-than-ideal solution, but works in practice because of the simple, unsupervised approach we use with this:</p>
<pre>def sum_vecs(embed,text):<br/><br/>    tokens = text.split(' ')<br/>    vec = np.zeros(embed.vector_size)<br/><br/>    for idx, term in enumerate(tokens):<br/>        if term in embed.vocab:<br/>            vec = vec + embed[term]<br/>    return vec<br/><br/>sentence_vector = sum_vecs(embed, sample_sentence)<br/>print(sentence_vector.shape)<br/>&gt;&gt; (300,)</pre>
<p>Let's define a data dictionary with some examples for each intent.</p>
<p>We will be using the data dictionary written by <a href="https://medium.com/rasa-blog/do-it-yourself-nlp-for-bot-developers-2e2da2817f3d">Alan at the Rasa Blog</a> for this.</p>
<p class="mce-root"/>
<p>This dictionary can be updated since we have more user input:</p>
<pre>data={<br/>  "greet": {<br/>    "examples" : ["hello","hey you","howdy","hello","hi","hey there","hey ho", "ssup?"],<br/>    "centroid" : None<br/>  },<br/>  "inform": {<br/>    "examples" : [<br/>        "i'd like something asian",<br/>        "maybe korean",<br/>        "what swedish options do i have",<br/>        "what italian options do i have",<br/>        "i want korean food",<br/>        "i want vegetarian food",<br/>        "i would like chinese food",<br/>        "what japanese options do i have",<br/>        "vietnamese please",<br/>        "i want some chicken",<br/>        "maybe thai",<br/>        "i'd like something vegetarian",<br/>        "show me British restaurants",<br/>        "show me a cool malay spot",<br/>        "where can I get some spicy food"<br/>    ],<br/>    "centroid" : None<br/>  },<br/>  "deny": {<br/>    "examples" : [<br/>      "no thanks"<br/>      "any other places ?",<br/>      "something else",<br/>      "naah",<br/>      "not that one",<br/>      "i do not like that",<br/>      "something else",<br/>      "please nooo"<br/>      "show other options?"<br/>    ],<br/>    "centroid" : None<br/>  },<br/>    "affirm":{<br/>        "examples":[<br/>            "yeah",<br/>            "that works",<br/>            "good, thanks",<br/>            "this works",<br/>            "sounds good",<br/>            "thanks, this is perfect",<br/>            "just what I wanted"<br/>        ],<br/>        "centroid": None<br/>    }<br/><br/>}</pre>
<p>The approach we have is simple: we find the centroid of each <em>user intent</em>. A centroid is just a central point to denote each intent. Then, the incoming text is assigned to the user intent that's nearest to the corresponding cluster.</p>
<p>Let's write a simple function to find the centroid and update the dictionary:</p>
<pre>def get_centroid(embed,examples):<br/>     C = np.zeros((len(examples),embed.vector_size))<br/>     for idx, text in enumerate(examples):<br/>         C[idx,:] = sum_vecs(embed,text)<br/> <br/>     centroid = np.mean(C,axis=0)<br/>     assert centroid.shape[0] == embed.vector_size<br/>     return centroid</pre>
<p>Let's add the centroid to the data dictionary:</p>
<pre>for label in data.keys():<br/>    data[label]["centroid"] = get_centroid(embed,data[label]["examples"])</pre>
<p>Let's write a simple function to find the nearest user intent cluster now. We will use the L2 norm that we already implemented in <kbd>np.linalg</kbd>:</p>
<pre>def get_intent(embed,data, text):<br/>    intents = list(data.keys())<br/>    vec = sum_vecs(embed,text)<br/>    scores = np.array([ np.linalg.norm(vec-data[label]["centroid"]) for label in intents])<br/>    return intents[np.argmin(scores)]</pre>
<p>Let's run this on some user text that is <strong>not</strong> in the <strong>data dictionary</strong>:</p>
<pre>for text in ["hey ","i am looking for chinese food","not for me", "ok, this is good"]:<br/>    print(f"text : '{text}', predicted_label : '{get_intent(embed, data, text)}'")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The corresponding code generalizes well, and is convincing regarding the fact that this is good enough for the roughly 10-15 minutes it took for us to get to this point:</p>
<div class="output_subarea output_text output_stream output_stdout">
<pre>text : 'hey ', predicted_label : 'greet'
text : 'i am looking for chinese food', predicted_label : 'inform'
text : 'not for me', predicted_label : 'deny'
text : 'ok, this is good', predicted_label : 'affirm'</pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bot responses</h1>
                </header>
            
            <article>
                
<p>We now know how to understand and categorize user intent. We now need to simply respond to each user intent with some corresponding responses. Let's get these <em>template</em> bot responses in one place:</p>
<pre>templates = {<br/>        "utter_greet": ["hey there!", "Hey! How you doin'? "],<br/>        "utter_options": ["ok, let me check some more"],<br/>        "utter_goodbye": ["Great, I'll go now. Bye bye", "bye bye", "Goodbye!"],<br/>        "utter_default": ["Sorry, I didn't quite follow"],<br/>        "utter_confirm": ["Got it", "Gotcha", "Your order is confirmed now"]<br/>    }</pre>
<p>Storing the <kbd>Response</kbd> map in a separate entity is helpful. This means that you can generate responses at a separate service from your intent understanding module and then glue them together:</p>
<pre>response_map = {<br/>    "greet": "utter_greet",<br/>    "affirm": "utter_goodbye",<br/>    "deny": "utter_options",<br/>    "inform": "utter_confirm",<br/>    "default": "utter_default",<br/>}</pre>
<p>If we think about this a little bit more, there is no need for the response map to be depend only on the intent that's categorized. You can convert this response map into a separate function that generates the map using related context and then picks a bot template.<br/>
<br/>
But here, for simplicity, let's keep it as a dictionary/JSON-style structure.</p>
<p class="mce-root"/>
<p>Let's write a simple <kbd>get_bot_response</kbd> function that takes in the response mapping, templates, and the intent as input and returns the actual bot response:</p>
<pre>import random<br/>def get_bot_response(bot_response_map, bot_templates, intent):<br/>    if intent not in list(response_map):<br/>        intent = "default"<br/>    select_template = bot_response_map[intent]<br/>    templates = bot_templates[select_template]<br/>    return random.choice(templates)</pre>
<p>Let's quickly try this with one sentence:</p>
<pre>user_intent = get_intent(embed, data, "i want indian food")<br/>get_bot_response(response_map, templates, user_intent)</pre>
<p>The code is free of syntax errors at this point. This seems good to go for more performance testing. But before that, how can we make this better?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Better response personalization</h1>
                </header>
            
            <article>
                
<p>You'll notice that the function picks one template at random for any particular <em>bot intent</em>, so to say. While this is for simplicity here, in practice, you can train an ML model to pick a response that's personalized to a user.</p>
<p>A simple personalization to make is to adapt with the talking/typing of the user's style. For example, one user might be formal with, <em>Hello, how are you today?</em>, while another might be more informal with, <em>Y</em><em>o</em>.</p>
<p>Therefore, <em>Hello</em> gets <em>Goodbye!</em> in response while <em>Yo!</em> gets <em>Bye bye</em> or even <em>TTYL</em> in the same conversation.</p>
<p>For now, let's go ahead and check the bot response for the sentences that we have already seen:</p>
<pre>for text in ["hey","i am looking for italian food","not for me", "ok, this is good"]:<br/>    user_intent = get_intent(embed, data, text)<br/>    bot_reply = get_bot_response(response_map, templates, user_intent)<br/>    print(f"text : '{text}', intent: {user_intent}, bot: {bot_reply}")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The responses can vary due to randomness; here is an example:</p>
<div class="output_subarea output_text output_stream output_stdout">
<pre>text : 'hey', intent: greet, bot: Hey! How you doin'? 
text : 'i am looking for italian food', intent: inform, bot: Gotcha
text : 'not for me', intent: deny, bot: ok, let me check some more
text : 'ok, this is good', intent: affirm, bot: Goodbye!</pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter on chatbots, we learned about <em>intent</em>, which usually refers to the user input, <em>response</em>, which is via the bot, <em>templates</em>, which defines the nature of bot responses, and <em>entities</em>, such as cuisine type, in our example.</p>
<p>Additionally, to understand the user intent—and even find entities—we used <strong>unsupervised approaches</strong> , that is, we did not have training examples this time. In practice, most commercial systems use a hybrid system, combining supervised and unsupervised systems.</p>
<p>The one thing you should take away from here is that we don't need a lot of training data to make the first usable version of a bot for a specific use case.</p>


            </article>

            
        </section>
    </body></html>