- en: <st c="0">14</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: <st c="3">Advanced RAG-Related Techniques for Improving Results</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于改进结果的RAG相关高级技术
- en: <st c="57">In this final chapter, we explore several advanced techniques to
    improve</st> **<st c="131">retrieval-augmented generation</st>** <st c="161">(</st>**<st
    c="163">RAG</st>**<st c="166">) applications.</st> <st c="183">These techniques
    go beyond the fundamental RAG approaches to tackle</st> <st c="251">more complex
    challenges and achieve even better results.</st> <st c="308">Our starting point
    will be techniques we have already used in previous chapters.</st> <st c="389">We
    will build off those techniques, learning where they fall short so that we can
    introduce new techniques that can make up the difference and take your RAG efforts</st>
    <st c="554">even further.</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们探讨了几个高级技术来改进**<st c="131">检索增强生成</st>** <st c="161">(**<st c="163">RAG</st>**<st
    c="166">)应用。<st c="183">这些技术超越了基本的RAG方法，以应对更复杂的挑战并实现更好的结果。</st> <st c="308">我们的起点将是我们在前几章中已经使用过的技术。</st>
    <st c="389">我们将在此基础上构建，了解它们的不足之处，以便我们可以引入新的技术来弥补差距，并将你的RAG工作推进得更远。</st>
- en: <st c="567">Throughout this chapter, you will gain hands-on experience implementing
    these advanced techniques through a series of code labs.</st> <st c="697">Our
    topics will include</st> <st c="721">the following:</st>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将通过一系列代码实验室获得通过实现这些高级技术的实践经验。<st c="697">我们的主题将包括以下内容：</st> <st c="721">以下：</st>
- en: <st c="735">Naïve RAG and</st> <st c="750">its limitations</st>
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单RAG及其局限性
- en: <st c="765">Hybrid RAG/multi-vector RAG for</st> <st c="798">improved retrieval</st>
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合RAG/多向量RAG以提高检索
- en: <st c="816">Re-ranking in</st> <st c="831">hybrid RAG</st>
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在混合RAG中进行重新排序
- en: <st c="841">Code lab 14.1 –</st> <st c="858">Query expansion</st>
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码实验室14.1 – <st c="858">查询扩展</st>
- en: <st c="873">Code lab 14.2 –</st> <st c="890">Query decomposition</st>
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码实验室14.2 – <st c="890">查询分解</st>
- en: <st c="909">Code lab</st> <st c="918">14.3 –</st> **<st c="926">Multi-modal</st>**
    **<st c="938">RAG</st>** <st c="941">(</st>**<st c="943">MM-RAG</st>**<st c="949">)</st>
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码实验室<st c="909">14.3 –</st> **<st c="926">多模态</st>** **<st c="938">RAG</st>**
    <st c="941">(**<st c="943">MM-RAG</st>**<st c="949">)</st>
- en: <st c="951">Other advanced RAG techniques</st> <st c="981">to explore</st>
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他高级RAG技术<st c="981">以供探索</st>
- en: <st c="991">These techniques enhance retrieval and generation by augmenting
    queries, breaking down questions into subproblems, and incorporating multiple
    data modalities.</st> <st c="1151">We also discuss a range of other advanced RAG
    techniques covering indexing, retrieval, generation, and the entire RAG pipeline.</st>
    <st c="1279">We start with a discussion of naïve RAG, the primary approach for
    RAG that we reviewed back in</st> [*<st c="1374">Chapter 2</st>*](B22475_02.xhtml#_idTextAnchor035)
    <st c="1383">and that you should feel very familiar with</st> <st c="1428">by
    now.</st>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术通过增强查询、将问题分解为子问题以及结合多种数据模态来增强检索和生成。<st c="1151">我们还讨论了一系列其他高级RAG技术，包括索引、检索、生成以及整个RAG管道。</st>
    <st c="1279">我们从讨论简单RAG开始，这是我们在第<st c="1374">第2章</st> <st c="1383">中回顾的RAG的主要方法，你应该现在感到非常熟悉</st>
    <st c="1428">。</st>
- en: <st c="1435">Technical requirements</st>
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: <st c="1458">The code for this chapter is placed in the following GitHub</st>
    <st c="1519">repository:</st> [<st c="1531">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_14</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_14
    )
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码放置在以下GitHub<st c="1519">存储库</st> <st c="1531">中：</st> [<st c="1531">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_14</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_14
    )
- en: <st c="1628">Naïve RAG and its limitations</st>
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单RAG及其局限性
- en: <st c="1658">So far, we have worked with three types of RAG approaches,</st>
    **<st c="1718">naïve RAG</st>**<st c="1727">,</st> **<st c="1729">hybrid RAG</st>**<st
    c="1739">, and</st> **<st c="1745">re-ranking</st>**<st c="1755">. Initially,
    we</st> <st c="1771">were working with what is called naïve RAG.</st> <st c="1815">This
    is the basic RAG approach that we had in our starter code in</st> [*<st c="1881">Chapter
    2</st>*](B22475_02.xhtml#_idTextAnchor035) <st c="1890">and multiple code labs
    after.</st> <st c="1921">Naive RAG models, the initial</st> <st c="1951">iterations
    of RAG technology, provide a foundational framework for integrating retrieval
    mechanisms with generative models, albeit with limitations in flexibility</st>
    <st c="2114">and scalability.</st>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1658">到目前为止，我们已经研究了三种类型的RAG方法，</st> **<st c="1718">朴素RAG</st>**<st c="1727">，</st>
    **<st c="1729">混合RAG</st>**<st c="1739">，以及</st> **<st c="1745">重排序</st>**<st
    c="1755">。最初，我们</st> <st c="1771">正在使用所谓的朴素RAG。</st> <st c="1815">这是我们在第二章</st>
    [*<st c="1881">第2章</st>*](B22475_02.xhtml#_idTextAnchor035) <st c="1890">以及随后的多个代码实验室中使用的RAG基本方法。</st>
    <st c="1921">朴素RAG模型，RAG技术的最初迭代，为将检索机制与生成模型集成提供了一个基础框架，尽管在灵活性和可扩展性方面存在限制。</st>
- en: '<st c="2130">Naïve RAG retrieves numerous fragmented context chunks, the chunks
    of text that we vectorize, to put into the LLM context window.</st> <st c="2261">If
    you do not use large enough chunks of text, your context will</st> <st c="2326">experience
    higher levels of fragmentation.</st> <st c="2369">This fragmentation leads to
    decreased understanding and capture of the context and semantics within your chunks,
    reducing the effectiveness of the retrieval mechanism of your RAG application.</st>
    <st c="2561">In the typical naïve RAG application, you are using some type of
    semantics search and are therefore exposed to these limitations by only using
    that type of search.</st> <st c="2725">As a result, we introduced a more advanced
    type of retrieval: the</st> <st c="2791">hybrid search.</st>'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2130">朴素RAG检索大量的碎片化上下文块，这些块是我们矢量化并放入LLM上下文窗口的文本块。</st> <st c="2261">如果你不使用足够大的文本块，你的上下文将</st>
    <st c="2326">经历更高的碎片化程度。</st> <st c="2369">这种碎片化导致对上下文和块内语义的理解和捕获减少，从而降低了你的RAG应用检索机制的有效性。</st>
    <st c="2561">在典型的朴素RAG应用中，你正在使用某种类型的语义搜索，因此仅使用该类型的搜索就会暴露出这些限制。</st> <st c="2725">因此，我们引入了一种更高级的检索类型：混合搜索。</st>
- en: <st c="2805">Hybrid RAG/multi-vector RAG for improved retrieval</st>
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="2805">混合RAG/多向量RAG以改进检索</st>
- en: <st c="2856">Hybrid RAG expands on the concept of naïve RAG by utilizing multiple
    vectors for the retrieval process, as opposed to relying on a single vector representation
    of queries and documents.</st> <st c="3043">We explored</st> <st c="3054">hybrid
    RAG in depth and in code in</st> [*<st c="3090">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="3099">, not only utilizing the mechanism recommended within LangChain but by
    re-creating that</st> <st c="3187">mechanism ourselves so that we could see its
    inner workings.</st> <st c="3248">Also called multi-vector RAG, hybrid RAG can
    involve not just semantic and keyword search, as we saw in our code lab, but the
    mix of any different vector retrieval techniques that make sense for your</st>
    <st c="3448">RAG application.</st>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2856">混合RAG通过在检索过程中使用多个向量来扩展朴素RAG的概念，而不是依赖于查询和文档的单个向量表示。</st> <st c="3043">我们在</st>
    <st c="3054">第8章</st> <st c="3090">中深入探讨了混合RAG，不仅在代码实验室中使用了LangChain推荐的机制，而且还自己重新创建了该机制，以便我们可以看到其内部工作原理。</st>
    <st c="3099">也称为多向量RAG，混合RAG不仅涉及语义和关键词搜索，正如我们在代码实验室中看到的那样，还可以混合任何适合你的</st> <st
    c="3448">RAG应用的不同向量检索技术。</st>
- en: <st c="3464">Our hybrid RAG code lab introduced a keyword search, which expanded
    our search capabilities, leading to more effective retrieval, particularly when
    dealing with content that has a weaker context (such as names, codes, internal
    acronyms, and similar text).</st> <st c="3721">This multi-vector approach allows
    us to consider broader facets of the query and the content in the database.</st>
    <st c="3831">This, in turn, can achieve higher relevance and accuracy in the information
    it retrieves to support the generation process.</st> <st c="3955">This results
    in generated content that is not only more relevant</st> <st c="4020">and informative
    but also more aligned with the nuances of the input query.</st> <st c="4095">Multi-vector
    RAG is particularly useful in applications requiring a high degree of precision
    and nuance in generated content, such as technical writing, academic</st> <st
    c="4256">research assistance, internal company documentation with significant
    amounts of internal code and entity references, and complex question-answering
    systems.</st> <st c="4414">But multi-vector RAG is not the only advanced technique
    we explored in</st> [*<st c="4485">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="4494">; we also</st> <st c="4505">applied</st> **<st c="4513">re-ranking</st>**<st
    c="4523">.</st>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的混合RAG代码实验室引入了关键词搜索，这扩大了我们的搜索能力，导致更有效的检索，尤其是在处理具有较弱上下文的内容（如名称、代码、内部缩写和类似文本）时。<st
    c="3721">这种多向量方法使我们能够考虑查询和数据库内容更广泛的方面。</st> <st c="3831">这反过来可以提高检索信息的关联性和准确性，以支持生成过程。</st>
    <st c="3955">这导致生成的文本不仅更相关、更信息丰富，而且与输入查询的细微差别也更加一致。</st> <st c="4095">多向量RAG在需要生成内容具有高度精确性和细微差别的应用中特别有用，例如技术写作、学术研究辅助、包含大量内部代码和实体引用的内部公司文档以及复杂的问答系统。</st>
    <st c="4414">但多向量RAG并不是我们在[第八章](B22475_08.xhtml#_idTextAnchor152)中探索的唯一先进技术；我们还<st
    c="4505">应用了</st> **<st c="4513">重新排序</st>**<st c="4523">。</st>
- en: <st c="4524">Re-ranking in hybrid RAG</st>
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="4524">混合RAG中的重新排序</st>
- en: <st c="4549">In</st> [*<st c="4553">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="4562">, in addition to our hybrid RAG approach, we also introduced a form of
    re-ranking, another common</st> <st c="4659">advanced RAG technique.</st> <st
    c="4684">After the semantic search and keyword searches complete their retrieval,
    we re-rank the results based on the rankings across both sets depending on if
    they appear in both and where they</st> <st c="4870">ranked</st> <st c="4876">initially.</st>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第八章](B22475_08.xhtml#_idTextAnchor152)中，除了我们的混合RAG方法，我们还介绍了一种形式的重新排序，这是另一种常见的先进RAG技术。<st
    c="4684">在语义搜索和关键词搜索完成检索后，我们根据它们是否同时出现在两个集合中以及它们最初排名的位置重新排序结果。</st> <st c="4876">最初排名。</st>
- en: <st c="4887">So, you have already stepped through three RAG techniques, including
    two advanced techniques!</st> <st c="4982">But this</st> <st c="4991">chapter
    is focused on bringing you three more advanced approaches:</st> **<st c="5058">query
    expansion</st>**<st c="5073">,</st> **<st c="5075">query decomposition</st>**<st
    c="5094">, and</st> **<st c="5100">MM-RAG</st>**<st c="5106">. We will also provide
    you a list</st> <st c="5140">of many more approaches you can explore, but</st>
    <st c="5185">we sorted through and picked out these three advanced RAG techniques
    because of their application in a wide variety of</st> <st c="5304">RAG applications.</st>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4887">因此，你已经走过了三种RAG技术，包括两种先进技术！<st c="4982">但本章的重点是向您介绍三种更多的先进方法：</st>
    **<st c="5058">查询扩展</st>**<st c="5073">、**<st c="5075">查询分解</st>**<st c="5094">和**<st
    c="5100">MM-RAG</st>**<st c="5106">。我们还将提供您可以探索的许多其他方法的列表，但<st c="5185">我们筛选并挑选出这些三种先进的RAG技术，因为它们在广泛的RAG应用中得到了应用。</st>
- en: <st c="5321">In our first code lab in this chapter, we will talk about</st>
    <st c="5380">query expansion.</st>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5321">在本章的第一个代码实验室中，我们将讨论</st> <st c="5380">查询扩展。</st>
- en: <st c="5396">Code lab 14.1 – Query expansion</st>
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="5396">代码实验室14.1 – 查询扩展</st>
- en: <st c="5428">The code for this lab can be found in the</st> `<st c="5471">CHAPTER14-1_QUERY_EXPANSION.ipynb</st>`
    <st c="5504">file in the</st> `<st c="5517">CHAPTER14</st>` <st c="5526">directory
    of the</st> <st c="5544">GitHub repository.</st>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5428">本实验室的代码可以在GitHub仓库的<st c="5544">`CHAPTER14`</st> <st c="5526">目录下的<st
    c="5471">`CHAPTER14-1_QUERY_EXPANSION.ipynb`</st> <st c="5504">文件中找到。</st>
- en: <st c="5562">Many techniques for enhancing RAG focus on improving one area,
    such as retrieval or generation, but query expansion has the potential to improve
    both.</st> <st c="5714">We have already talked about the concept of</st> <st c="5758">expansion
    in</st> [*<st c="5771">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st
    c="5781">, but that was focused on the LLM output.</st> <st c="5823">Here, we
    focus the concept on the input to the model, augmenting the original prompt with
    additional keywords or phrases.</st> <st c="5945">This approach can improve the
    retrieval model’s understanding as you add more context to the user query that
    is used for retrieval, increasing the chances of fetching relevant documents.</st>
    <st c="6132">With an improved retrieval, you are already helping to improve the
    generation, giving it better context to work with, but this approach also has
    the potential to produce a more effective query, which in turn also helps the
    LLM deliver an</st> <st c="6370">improved response.</st>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5562">许多增强RAG的技术都集中在改进一个领域，例如检索或生成，但查询扩展有潜力同时改进这两个方面。</st> <st c="5714">我们已经在</st>
    [*<st c="5771">第13章</st>*](B22475_13.xhtml#_idTextAnchor256)<st c="5781">中讨论了扩展的概念，但那主要关注LLM的输出。</st>
    <st c="5823">在这里，我们将这个概念聚焦于模型的输入，通过添加额外的关键词或短语来增强原始提示。</st> <st c="5945">这种方法可以通过向用于检索的用户查询添加更多上下文来提高检索模型的理解，从而增加检索相关文档的机会。</st>
    <st c="6132">通过改进检索，你已经在帮助提高生成，给它提供了更好的工作上下文，但这种方法也有潜力产生一个更有效的查询，这反过来也有助于LLM提供</st>
    <st c="6370">改进的响应。</st>
- en: <st c="6388">Typically, the way query expansion with answers works is that you
    take the user query and immediately send it to the LLM with a prompt focused on
    getting an initial answer to the question, even though you haven’t shown it any
    of the typical contexts you normally show it in RAG applications.</st> <st c="6681">From
    an LLM standpoint, these types of changes can help broaden the search scope without
    losing focus on the</st> <st c="6790">original intent.</st>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6388">通常，带有答案的查询扩展的工作方式是，你将用户查询立即发送到LLM，并附带一个专注于获取问题初始答案的提示，即使你还没有向它展示在RAG应用中通常展示的任何典型上下文。</st>
    <st c="6681">从LLM的角度来看，这类变化可以帮助扩大搜索范围，同时不失对</st> <st c="6790">原始意图的关注。</st>
- en: <st c="6806">Start in a new cell above the cell that creates the</st> `<st c="6859">rag_chain_from_docs</st>`
    <st c="6878">chain.</st> <st c="6886">We are going to introduce a number of prompt
    templates to</st> <st c="6944">accomplish this:</st>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6806">在创建</st> `<st c="6859">rag_chain_from_docs</st>` <st c="6878">链的单元格上方开始一个新的单元格。</st>
    <st c="6886">我们将介绍一系列提示模板来完成这个任务：</st>
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="7071">Let’s review each of these prompt templates and</st> <st c="7120">their
    uses:</st>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7071">让我们回顾一下这些提示模板及其</st> <st c="7120">用途：</st>
- en: '`<st c="7131">ChatPromptTemplate</st>` <st c="7150">class: This provides a
    template for creating chat-based prompts, which we can use to combine our other
    prompt templates into a more</st> <st c="7283">chat-based approach.</st>'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="7131">ChatPromptTemplate</st>` <st c="7150">类：这提供了一个创建基于聊天的提示的模板，我们可以使用它将我们的其他提示模板组合成一个更</st>
    <st c="7283">基于聊天的方法。</st>'
- en: '`<st c="7303">HumanMessagePromptTemplate</st>` <st c="7330">class: This provides
    a prompt template for creating human messages in the chat prompt.</st> <st c="7418">The</st>
    `<st c="7422">HumanMessage</st>` <st c="7434">object represents a message sent
    by a human user in a conversation with the language model.</st> <st c="7527">We
    will hydrate this prompt with our</st> `<st c="7564">user_query</st>` <st c="7574">string,
    which comes from the</st> *<st c="7604">human</st>* <st c="7609">in</st> <st c="7613">this
    scenario!</st>'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="7303">HumanMessagePromptTemplate</st>` <st c="7330">类：这提供了一个创建聊天提示中人类消息的提示模板。</st>
    <st c="7418">`<st c="7422">HumanMessage</st>` <st c="7434">对象代表在语言模型对话中由人类用户发送的消息。</st>
    <st c="7527">我们将使用来自</st> `<st c="7564">user_query</st>` <st c="7574">字符串的提示来填充这个提示，这个字符串来自</st>
    *<st c="7604">人类</st>` <st c="7609">在这个场景中！</st>'
- en: '`<st c="7627">SystemMessagePromptTemplate</st>` <st c="7655">class: The</st>
    *<st c="7667">system</st>* <st c="7673">also gets a prompt, which for a chat-based
    LLM has a different significance compared to human-generated prompts.</st> <st
    c="7787">This provides us with a prompt template for creating these system messages
    in the</st> <st c="7869">chat prompt.</st>'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="7627">SystemMessagePromptTemplate</st>` <st c="7655">类：*<st c="7667">系统</st>`
    <st c="7673">也获得了一个提示，对于一个基于聊天的LLM来说，这与人类生成的提示相比具有不同的意义。</st> <st c="7787">这为我们提供了一个创建聊天提示中的这些系统消息的提示模板。</st>'
- en: <st c="7881">Next, we want to create a</st> <st c="7908">function that will
    handle the query expansion for use, utilizing the different prompt templates we
    just discussed.</st> <st c="8023">This is the system message prompt that we will
    use, which you will want to customize to whatever area of focus your RAG system
    targets – environmental reports in</st> <st c="8185">this case:</st>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7881">接下来，我们想要创建一个</st> <st c="7908">函数，该函数将处理查询扩展，并使用我们刚刚讨论的不同提示模板。</st>
    `<st c="8023">这是我们将使用的系统消息提示，您需要将其定制为您的RAG系统关注的任何领域 - 在这种情况下是环境报告：</st>`
- en: '`<st c="8195">"You are a helpful expert environmental research assistant.</st>
    <st c="8256">Provide an example answer to the given question, that might be found
    in a document like an annual</st>` `<st c="8354">environmental report."</st>`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="8195">"您是一位有用的环境研究专家。</st> <st c="8256">提供一个可能出现在年度环境报告等文档中的问题的示例答案。"</st>`
    `<st c="8354">环境报告。</st>`'
- en: <st c="8376">This will be the first step in the function</st> <st c="8421">we
    create:</st>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8376">这将是我们创建的函数的第一个步骤</st> <st c="8421">：</st>
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <st c="9025">Here, you see us utilizing all three types of prompt templates
    to formulate the overall set of messages we send to the LLM.</st> <st c="9150">Ultimately,
    this results in a response from the LLM, where it does its best to answer</st>
    <st c="9236">our question.</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9025">在这里，您可以看到我们利用了所有三种类型的提示模板来制定发送给LLM的整体消息集。</st> `<st c="9150">最终，这导致LLM给出了一个响应，它尽力回答</st>
    <st c="9236">我们的问题。</st>`
- en: <st c="9249">Let’s provide some code that calls this function so that we can
    talk about the output, representing the</st> *<st c="9354">expansion</st>* <st
    c="9363">in</st> <st c="9367">query expansion:</st>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9249">让我们提供一些调用此函数的代码，这样我们就可以讨论输出，表示</st> *<st c="9354">查询扩展</st>* <st
    c="9363">中的</st> <st c="9367">扩展</st>：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: <st c="9585">Here, we are calling</st> <st c="9606">our user query</st> `<st
    c="9622">original_query</st>`<st c="9636">, indicating that it is our source query
    that will soon go through the expansion.</st> <st c="9718">The</st> `<st c="9722">hypothetical_answer</st>`
    <st c="9741">instance is the response string we get back from the LLM.</st> <st
    c="9800">You then concatenate the original user query with the imagined answer
    as a</st> `<st c="9875">joint_query</st>` <st c="9886">string and use that as
    the new query.</st> <st c="9925">The output will look something like this (truncated</st>
    <st c="9977">for brevity!):</st>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9585">在这里，我们调用</st> <st c="9606">我们的用户查询</st> `<st c="9622">original_query</st>`<st
    c="9636">，表示这是我们即将进行扩展的源查询。</st> `<st c="9718">假设答案</st>` <st c="9741">实例是我们从LLM获取的响应字符串。</st>
    <st c="9800">然后，您将原始用户查询与想象中的答案连接成一个</st> `<st c="9875">联合查询</st>` <st c="9886">字符串，并使用该字符串作为新的查询。</st>
    `<st c="9925">输出将类似于以下内容（为了简洁而截断）：</st>`
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <st c="10849">It is a much longer answer.</st> <st c="10878">Our LLM really
    tried to answer it thoroughly!</st> <st c="10924">This initial answer is going
    to be a hypothetical or imagined answer to the original user query you sent it.</st>
    <st c="11033">Normally, we shy away from imagined answers, but here, we take advantage
    of them as they help us tap into the inner workings of the LLM and pull out the
    concepts that align with the</st> `<st c="11215">user_query</st>` <st c="11225">string
    you are going</st> <st c="11247">to use.</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10849">这是一个更长的答案。</st> `<st c="10878">我们的LLM真的尽力彻底回答了它！</st>` `<st c="10924">这个初始答案是您发送给它的原始用户查询的假设或想象中的答案。</st>`
    `<st c="11033">通常，我们避免使用想象中的答案，但在这里，我们利用了它们，因为它们帮助我们深入了解LLM的内部工作原理，并提取出与您将要使用的</st>
    `<st c="11215">user_query</st>` <st c="11225">字符串相一致的概念。</st>`
- en: <st c="11254">At this point, we will step through the original code, but instead
    of passing it the</st> `<st c="11340">original_query</st>` <st c="11354">string
    as</st> <st c="11364">we have in the past, we will pass in the concatenated original
    answer plus an imagined answer into our previous</st> <st c="11477">RAG pipeline:</st>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11254">在这个阶段，我们将逐步执行原始代码，但与过去不同，我们将传递一个连接了原始答案和一个想象中的答案到我们之前的</st> `<st
    c="11477">RAG管道</st>` <st c="11340">original_query</st> <st c="11354">字符串，而不是我们过去所拥有的</st>
    <st c="11364">字符串：</st>
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <st c="11996">You will see that the query that is passed into our original RAG
    pipeline is the much longer</st> `<st c="12090">joint_query</st>` <st c="12101">string,
    and then we see an expanded set of results that mix in the data we provided with
    the expanded structure the LLM helped</st> <st c="12229">to add.</st>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11996">您将看到传递给我们的原始RAG管道的查询是更长的</st> `<st c="12090">joint_query</st>`
    <st c="12101">字符串，然后我们看到一组扩展的结果，这些结果将我们提供的数据与LLM帮助</st> <st c="12229">添加的扩展结构混合在一起。</st>
- en: <st c="12236">Because the LLM returns the text in a Markdown version, we can
    use IPython to print in a nicely formatted way.</st> <st c="12348">This code prints
    out</st> <st c="12369">the following:</st>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12236">因为LLM以Markdown版本返回文本，我们可以使用IPython以良好的格式打印。</st> <st c="12348">此代码打印出以下内容：</st>
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <st c="12523">Here is</st> <st c="12532">the</st> <st c="12536">output:</st>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12523">以下是</st> <st c="12532">输出：</st>
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: <st c="13859">Compare this to the results we got from the original query and
    see if you think it improved the answer!</st> <st c="13964">As you can see, you
    do get a different response for each one, and you can determine what works best
    for your</st> <st c="14073">RAG applications.</st>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13859">将此与原始查询的结果进行比较，看看您是否认为它改进了答案！</st> <st c="13964">如您所见，您确实会得到不同的响应，您可以确定最适合您的</st>
    <st c="14073">RAG应用的最佳方法。</st>
- en: <st c="14090">One important aspect to acknowledge with this approach is that
    you are bringing the LLM into the retrieval stage, whereas in the past, we only
    used the LLM in the generation stage.</st> <st c="14272">When</st> <st c="14277">doing
    this though, prompt engineering now becomes a concern within the retrieval stage,
    whereas we previously only worried about it in the generation stage.</st> <st
    c="14434">However, the approach is similar to what we discussed in our prompt
    engineering chapter (</st>[*<st c="14523">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st
    c="14534">), where we talked about iterating until we have better results from</st>
    <st c="14604">our LLM.</st>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14090">采用这种方法的一个重要方面是，您将LLM引入了检索阶段，而过去我们只在使用LLM进行生成阶段时使用它。</st> <st c="14272">然而，当这样做的时候，现在prompt工程成为检索阶段的一个关注点，而之前我们只担心它在生成阶段。</st>
    <st c="14434">然而，这种方法与我们讨论的提示工程章节（</st>[*<st c="14523">第13章</st>*](B22475_13.xhtml#_idTextAnchor256)<st
    c="14534">）中讨论的方法相似，我们在那里讨论了迭代直到我们从</st> <st c="14604">我们的LLM中获得更好的结果。</st>
- en: <st c="14612">For more information on query expansion, you can read the original
    paper</st> <st c="14686">here:</st> [<st c="14692">https://arxiv.org/abs/2305.03653</st>](https://arxiv.org/abs/2305.03653
    )
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14612">有关查询扩展的更多信息，您可以在以下位置阅读原始论文：</st> <st c="14686">这里：</st> [<st c="14692">https://arxiv.org/abs/2305.03653</st>](https://arxiv.org/abs/2305.03653
    )
- en: <st c="14724">Query expansion is just one of many approaches that enhance the
    original query to help improve the RAG output.</st> <st c="14836">We have listed
    many more near the end of this chapter, but in our next code lab, we will tackle
    an approach called query decomposition that can be particularly useful in RAG
    scenarios because of its emphasis</st> <st c="15044">on question-answering.</st>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14724">查询扩展只是许多增强原始查询以帮助改进RAG输出的方法之一。</st> <st c="14836">我们在本章末尾列出了更多，但在我们下一个代码实验室中，我们将解决一种称为查询分解的方法，因为它特别强调问答，因此在RAG场景中非常有用。</st>
- en: <st c="15066">Code lab 14.2 – Query decomposition</st>
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="15066">代码实验室 14.2 – 查询分解</st>
- en: <st c="15102">The code for this lab can be found in the</st> `<st c="15145">CHAPTER14-2_DECOMPOSITION.ipynb</st>`
    <st c="15176">file in the</st> `<st c="15189">CHAPTER14</st>` <st c="15198">directory
    of the</st> <st c="15216">GitHub repository.</st>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15102">此实验室的代码可以在</st> `<st c="15145">CHAPTER14-2_DECOMPOSITION.ipynb</st>`
    <st c="15176">文件中找到，该文件位于</st> `<st c="15189">CHAPTER14</st>` <st c="15198">目录下的</st>
    <st c="15216">GitHub仓库中。</st>
- en: <st c="15234">Query decomposition is a strategy focused on improving question-answering
    within the GenAI space.</st> <st c="15333">It falls under</st> <st c="15347">the
    category of query translation, which is a set of approaches that focuses on improving
    the initial stage of the RAG pipeline, retrieval.</st> <st c="15488">With query
    decomposition, we will</st> *<st c="15522">decompose</st>* <st c="15531">or break
    down a question into smaller questions.</st> <st c="15581">These smaller questions
    can either be approached sequentially or independently, depending on your needs,
    giving more flexibility across different scenarios you might use RAG for.</st>
    <st c="15760">After each question is answered, there is a consolidation step that
    delivers a final response that often has a broader perspective than the original
    response with</st> <st c="15923">naïve RAG.</st>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15234">查询分解是一种专注于在GenAI空间内改进问答的策略。</st> <st c="15333">它属于</st> <st c="15347">查询翻译类别，这是一组专注于改进RAG管道初始阶段的检索的方法。</st>
    <st c="15488">使用查询分解，我们将</st> *<st c="15522">分解</st> *<st c="15531">或把一个问题分解成更小的问题。</st>
    <st c="15581">这些小问题可以根据你的需求依次或独立处理，从而在不同场景下使用RAG时提供更多灵活性。</st> <st c="15760">在每个问题回答后，都有一个整合步骤，它提供最终响应，通常比使用原始RAG的响应有更广阔的视角。</st>
- en: <st c="15933">There are other query translation approaches such as RAG-Fusion
    and multi-query, which are focused on sub-questions, but this focuses on decomposing
    the question.</st> <st c="16097">We will talk more about these other techniques
    near the end of</st> <st c="16160">this chapter.</st>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15933">还有其他查询翻译方法，如RAG-Fusion和多查询，它们专注于子问题，但这种方法专注于分解问题。</st> <st c="16097">我们将在本章末尾更多地讨论这些其他技术。</st>
- en: <st c="16173">In the paper that proposed this approach, written by Google researchers,
    they call it Least-to-Most, or decomposition.</st> <st c="16293">LangChain has
    documentation for this approach on its website and calls it query decomposition.</st>
    <st c="16388">So, we are in pretty good company when we talk about this</st> <st
    c="16446">particular approach!</st>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16173">在提出这种方法的论文中，由谷歌研究人员撰写，他们称之为Least-to-Most，或分解。</st> <st c="16293">LangChain在其网站上对此方法有文档，称之为查询分解。</st>
    <st c="16388">因此，当我们谈论这个</st> <st c="16446">特定方法时，我们处于一个非常不错的公司中！</st>
- en: <st c="16466">We are going to introduce a couple more concepts to help us understand
    how to implement</st> <st c="16555">query</st> <st c="16560">decomposition:</st>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16466">我们将介绍更多概念，以帮助我们理解如何实现</st> <st c="16555">查询</st> <st c="16560">分解：</st>
- en: <st c="16575">The first concept is</st> **<st c="16597">chain-of-thought</st>**
    <st c="16613">(</st>**<st c="16615">CoT</st>**<st c="16618">), a prompt engineering
    strategy where we structure the</st> <st c="16674">input prompt in a way that
    mimics human reasoning, with the goal of improving language models’ performance
    on tasks requiring logic, calculation,</st> <st c="16821">and decision-making.</st>
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16575">第一个概念是</st> **<st c="16597">思维链</st>** <st c="16613">(</st>**<st
    c="16615">CoT</st>**<st c="16618">)，这是一种提示工程策略，我们以模仿人类推理的方式结构化</st> <st c="16674">输入提示，目的是提高语言模型在需要逻辑、计算</st>
    <st c="16821">和决策的任务上的性能。</st>
- en: <st c="16841">The second concept is</st> **<st c="16864">interleaving retrieval</st>**<st
    c="16886">, where you step back and forth between CoT-driven</st> <st c="16937">prompts
    and retrieval,</st> *<st c="16960">interleaving</st>* <st c="16972">them, with
    the goal of retrieving more relevant information for later reasoning steps, compared
    to simply passing the user query</st> <st c="17102">for retrieval.</st> <st c="17117">This
    combination is called</st> **<st c="17144">Interleave Retrieval with CoT</st>**
    <st c="17173">or</st> **<st c="17177">IR-CoT</st>**<st c="17183">.</st>
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16841">第二个概念是</st> **<st c="16864">交错检索</st>**<st c="16886">，其中你在由CoT驱动的</st>
    <st c="16937">提示和检索之间来回移动，*<st c="16960">交错</st>*, <st c="16972">与简单地传递用户查询进行检索相比，目的是为了在后续推理步骤中检索到更多相关信息。</st>
    <st c="17102">这种组合被称为</st> **<st c="17144">交错检索与思维链</st>** <st c="17173">或</st>
    **<st c="17177">IR-CoT</st>**<st c="17183">。</st>
- en: <st c="17184">Pulling this all together, you end up with an approach that breaks
    down a problem into subproblems and steps through them with a dynamic retrieval
    process.</st> <st c="17341">Doing this sequentially, after you have broken your
    original user query into sub-questions, you start with the first question, retrieve
    documents, answer that, then do retrieval for the second question, adding the
    answer to the first question to the results, and then using all of that data to
    answer question 2\.</st> <st c="17654">This goes on through all of the sub-questions
    you have until you get to the last answer, which will be your</st> <st c="17762">final
    answer.</st>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17184">将所有这些整合在一起，你最终得到一种方法，它将问题分解为子问题，并通过动态检索过程逐步解决它们。</st> <st c="17341">在将你的原始用户查询分解为子问题之后，你开始处理第一个问题，检索文档，回答它，然后对第二个问题进行检索，将第一个问题的答案添加到结果中，然后使用所有这些数据来回答问题2。</st>
    <st c="17654">这个过程会一直持续到你得到最后一个答案，这将是你的</st> <st c="17762">最终答案。</st>
- en: <st c="17775">With all this explanation, you likely just want to jump into the
    code and see how it works, so let’s</st> <st c="17877">get started!</st>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17775">有了所有这些解释，你可能只想跳入代码看看它是如何工作的，所以让我们</st> <st c="17877">开始吧！</st>
- en: <st c="17889">We will import a couple</st> <st c="17914">new packages:</st>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17889">我们将导入几个</st> <st c="17914">新包：</st>
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: <st c="17967">The</st> `<st c="17972">dumps</st>` <st c="17977">and</st> `<st
    c="17982">loads</st>` <st c="17987">functions imported from</st> `<st c="18012">langchain.load</st>`
    <st c="18026">are used to serialize and deserialize (respectively) a Python object
    into and out of a string representation.</st> <st c="18137">In our code, we will
    use it to convert each</st> `<st c="18181">Document</st>` <st c="18189">object
    into a string representation before deduplication, and then</st> <st c="18257">back
    again.</st>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17967">从</st> `<st c="17972">dumps</st>` <st c="17977">和</st> `<st c="17982">loads</st>`
    <st c="17987">函数导入自</st> `<st c="18012">langchain.load</st>` <st c="18026">用于将Python对象序列化和反序列化为字符串表示。</st>
    <st c="18137">在我们的代码中，我们将使用它来在去重之前将每个</st> <st c="18181">Document</st> <st c="18189">对象转换为字符串表示，然后再转换回来。</st>
- en: <st c="18268">We then jump down past the retriever definitions and add a cell
    where we are going to add our decomposition</st> <st c="18377">prompt, chain,
    and code to run it.</st> <st c="18412">Start with creating a new</st> <st c="18438">prompt
    template:</st>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18268">然后我们跳过检索器定义，添加一个单元格，我们将在这里添加我们的分解</st> <st c="18377">提示、链和代码来运行它。</st>
    <st c="18412">首先创建一个新的</st> <st c="18438">提示模板：</st>
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: <st c="18928">Reading through the string in this</st> `<st c="18964">PromptTemplate</st>`
    <st c="18978">object, we get the prompt version explaining to the LLM how to perform
    the decomposition we are looking for.</st> <st c="19088">It is a very transparent
    request of the LLM explaining the problem we are trying to overcome and what we
    need it to do!</st> <st c="19208">We also prompt the LLM to provide the result
    in a specific format.</st> <st c="19275">This can be risky, as LLMs can sometimes
    return unexpected results, even when prompted specifically for a certain format.</st>
    <st c="19397">In a more robust application, this is a good place to run a check
    to make sure your response is properly formatted.</st> <st c="19513">But for this
    simple example, the ChatGPT-4o-mini model we are using seems to do fine returning
    it in the</st> <st c="19618">proper format.</st>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18928">阅读这个</st> `<st c="18964">PromptTemplate</st>` <st c="18978">对象中的字符串，我们得到一个提示版本，解释给LLM如何执行我们正在寻找的分解。</st>
    <st c="19088">这是一个非常透明的对LLM的请求，解释了我们要克服的问题以及我们需要它做什么！</st> <st c="19208">我们还提示LLM以特定格式提供结果。</st>
    <st c="19275">这可能会有些风险，因为LLM有时会返回意外的结果，即使被明确提示以特定格式返回。</st> <st c="19397">在更健壮的应用中，这是一个运行检查以确保你的响应格式正确的好地方。</st>
    <st c="19513">但在这个简单的例子中，我们使用的ChatGPT-4o-mini模型似乎在以正确格式返回它时表现良好。</st> <st c="19618">正确格式。</st>
- en: <st c="19632">Next, we set up the chain, using the various elements we typically
    use in our chains, but using the prompt</st> <st c="19740">to decompose:</st>
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19632">接下来，我们设置链，使用我们在链中通常使用的各种元素，但使用提示</st> <st c="19740">来分解：</st>
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: <st c="19854">This is a self-explanatory chain; it uses the prompt template,
    an LLM defined earlier in the code, the output parser, and then applies formatting
    for a more</st> <st c="20012">readable result.</st>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19854">这是一个自解释的链；它使用提示模板、代码中之前定义的LLM、输出解析器，然后应用格式化以获得更</st> <st c="20012">易读的结果。</st>
- en: <st c="20028">To call this chain, we</st> <st c="20052">implement the</st> <st
    c="20066">following code:</st>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20028">要调用此链，我们</st> <st c="20052">实现以下代码：</st>
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: <st c="20333">This code invokes the chain we set up and provides us with the
    original query, as well as the five new queries our decomposition prompt and LLM</st>
    <st c="20478">have generated:</st>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20333">此代码调用我们设置的链，并为我们提供原始查询，以及我们的分解提示和LLM生成的五个新查询：</st>
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: <st c="20925">The LLM does a superb job of taking our query and breaking it
    into a number of related questions covering the different aspects that will help
    to answer the</st> <st c="21083">original query.</st>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20925">LLM在将我们的查询分解成一系列相关问题方面做得非常出色，这些问题涵盖了不同方面，有助于回答原始查询。</st>
- en: <st c="21098">But this is only half of the decomposition concept!</st> <st c="21151">Next,
    we are going to run all of our questions through retrieval, giving us a much more
    robust set of retrieved context compared to what we have had in past</st> <st
    c="21308">code labs.</st>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22544">但分解概念只完成了一半！接下来，我们将运行所有问题通过检索，与我们在过去的代码实验室中拥有的相比，这将给我们提供一个更健壮的检索上下文集合：</st>
- en: <st c="21318">We will start by setting up a function to format the documents
    we retrieve based on all of these</st> <st c="21416">new queries:</st>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21318">我们将首先设置一个函数来格式化基于所有这些</st> <st c="21416">新查询检索到的文档：</st>
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <st c="21729">This function will provide a</st> <st c="21759">list of lists,
    representing each list of retrieved sets of documents that are themselves provided
    in a list.</st> <st c="21868">We flatten this list of lists, meaning that we just
    make it one long list.</st> <st c="21943">We then use the</st> `<st c="21959">dumps</st>`
    <st c="21964">function we imported from</st> `<st c="21991">LangChain.load</st>`
    <st c="22005">to convert each</st> `<st c="22022">Document</st>` <st c="22030">object
    to a string; we dedupe based on that string, and then return it to its former
    state as a list.</st> <st c="22133">We also print out how many documents we end
    up with before and after to see how our deduping efforts performed.</st> <st c="22245">In
    this example, after we’ve run the</st> `<st c="22282">decompose_queries_chain</st>`
    <st c="22305">chain, we drop from</st> `<st c="22326">100</st>` <st c="22329">documents</st>
    <st c="22340">to</st> `<st c="22343">67</st>`<st c="22345">:</st>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21729">此函数将提供一个</st> <st c="21759">列表的列表，代表每个检索到的文档集合列表。</st> <st c="21868">我们平铺这个列表的列表，这意味着我们只将其变成一个长列表。</st>
    <st c="21943">然后我们使用从 `<st c="21991">LangChain.load</st>` 导入的 `<st c="21959">dumps</st>`
    <st c="21964">函数将每个</st> `<st c="22022">Document</st>` <st c="22030">对象转换为字符串；我们根据该字符串进行去重，然后将其返回为列表。</st>
    <st c="22133">我们还打印出在去重前后我们最终拥有的文档数量，以查看我们的去重工作表现如何。</st> <st c="22245">在这个例子中，在运行了</st>
    `<st c="22282">decompose_queries_chain</st>` <st c="22305">链之后，我们从</st> `<st c="22326">100</st>`
    <st c="22329">个文档</st> <st c="22340">减少到</st> `<st c="22343">67</st>`<st c="22345">：</st>
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: <st c="22384">Let’s set up the chain that will run our previous decomposition
    chain, the retrieval for all of the new queries, and the final formatting with
    the function we</st> <st c="22544">just created:</st>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22384">让我们设置一个链，该链将运行我们之前的分解链、所有新查询的检索以及我们刚刚创建的函数的最终格式化：</st>
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: <st c="22654">This relatively short line of code accomplishes a lot!</st> <st
    c="22710">The final result is a set of</st> `<st c="22739">67</st>` <st c="22741">documents
    related to all of the queries we generated from our original query and the decomposition.</st>
    <st c="22842">Note that we have added the previous</st> `<st c="22879">decompose_queries_chain</st>`
    <st c="22902">chain to this directly, so there is no need to call that</st> <st
    c="22960">chain separately.</st>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22977">这一相对简短的代码行完成了许多工作！最终结果是包含所有从原始查询和分解生成的查询的</st> `<st c="22739">67</st>`
    <st c="22741">个文档的集合。</st> <st c="22842">请注意，我们已经直接将之前的</st> `<st c="22879">decompose_queries_chain</st>`
    <st c="22902">链添加到其中，因此不需要单独调用该链。</st>
- en: <st c="22977">We assign the results of this chain to a</st> `<st c="23019">docs</st>`
    <st c="23023">variable with this one line</st> <st c="23052">of code:</st>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20478">我们使用这一行代码将此链的结果分配给一个</st> `<st c="23019">docs</st>` <st c="23023">变量：</st>
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: <st c="23115">With the invocation of this chain, we retrieve a significant number
    of documents (</st>`<st c="23198">67</st>`<st c="23201">) compared to previous</st>
    <st c="23224">methods, but we still need to run our final RAG steps with our expanded
    retrieval results.</st> <st c="23316">Most of the code remains the same after
    this, but we do replace the ensemble chain with the</st> `<st c="23408">retrieval_chain</st>`
    <st c="23423">chain we</st> <st c="23433">just built:</st>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用这个链，我们检索到了与之前方法相比的显著数量的文档（</st>`<st c="23198">67</st>`<st c="23201">），但我们仍然需要使用我们扩展的检索结果来运行我们的最终RAG步骤。</st>
    <st c="23316">在此之后，大部分代码保持不变，但我们用我们刚刚构建的</st> `<st c="23408">retrieval_chain</st>`
    <st c="23423">链替换了集成链：</st>
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: <st c="23587">This incorporates our new code into our previous RAG application.</st>
    <st c="23654">Running this line will run all of the chains we just added, so there
    is no need to go back and run them separately as we did for this example.</st>
    <st c="23797">This is one large set of cohesive code that combines our previous
    efforts with this new powerful RAG technique.</st> <st c="23909">We invite you
    to compare the current results from this technique with past code lab results
    to see how the details have been filling in better and giving us broader coverage
    of the topics our</st> `<st c="24101">original_query</st>` <st c="24115">chain</st>
    <st c="24122">asks about:</st>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23587">这把我们的新代码整合到了之前的RAG应用中。</st> <st c="23654">运行这一行将运行我们刚刚添加的所有链，因此没有必要像我们在这个例子中那样单独运行它们。</st>
    <st c="23797">这是一组大型连贯的代码，它将我们之前的努力与这个新的强大RAG技术结合起来。</st> <st c="23909">我们邀请您将此技术当前的结果与过去的代码实验室结果进行比较，以查看细节是如何更好地填充并为我们提供更广泛的关于我们</st>
    `<st c="24101">original_query</st>` <st c="24115">链</st> <st c="24122">询问的主题的覆盖范围：</st>
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: <st c="25127">Advanced techniques</st> <st c="25148">such as this offer very
    promising results depending on the goals of your</st> <st c="25221">RAG application!</st>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25127">像这样的高级技术</st> <st c="25148">根据您RAG应用的目标，可以提供非常有希望的结果！</st>
- en: <st c="25237">For more information on this approach, visit the original</st>
    <st c="25296">paper:</st> [<st c="25303">https://arxiv.org/abs/2205.10625</st>](https://arxiv.org/abs/2205.10625
    )
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25237">有关此方法的更多信息，请访问原始</st> <st c="25296">论文：</st> [<st c="25303">https://arxiv.org/abs/2205.10625</st>](https://arxiv.org/abs/2205.10625
    )
- en: <st c="25335">For our next and last code lab of the entire book, we are going
    to go beyond the world of text and expand our RAG to other modalities, such as
    images and video with a technique</st> <st c="25513">called MM-RAG.</st>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25335">对于本书的下一个也是最后一个代码实验室，我们将超越文本的世界，将我们的RAG扩展到其他模态，例如图像和视频，使用一种称为MM-RAG的技术。</st>
    <st c="25513">MM-RAG。</st>
- en: <st c="25527">Code lab 14.3 – MM-RAG</st>
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="25527">代码实验室 14.3 – MM-RAG</st>
- en: <st c="25550">The code for this lab can be found in the</st> `<st c="25593">CHAPTER14-3_MM_RAG.ipynb</st>`
    <st c="25617">file in the</st> `<st c="25630">CHAPTER14</st>` <st c="25639">directory
    of the</st> <st c="25657">GitHub repository.</st>
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25550">此实验室的代码可以在GitHub存储库的</st> `<st c="25593">CHAPTER14-3_MM_RAG.ipynb</st>`
    <st c="25617">文件中找到，位于</st> `<st c="25630">CHAPTER14</st>` <st c="25639">目录下。</st>
- en: <st c="25675">This is a good example of</st> <st c="25702">when an acronym can
    really help us talk faster.</st> <st c="25750">Try to say</st> *<st c="25761">multi-modal
    retrieval augmented regeneration</st>* <st c="25805">out loud once, and you will
    likely want to use MM-RAG from now on!</st> <st c="25873">But I digress.</st>
    <st c="25888">This is a groundbreaking approach that will likely gain a lot of
    traction in the near future.</st> <st c="25982">It better represents how we as
    humans process information, so it must be amazing, right?</st> <st c="26071">Let’s
    start by revisiting the concept of using</st> <st c="26118">multiple modes.</st>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25675">这是一个很好的例子，说明了</st> <st c="25702">当缩写词真的能帮助我们更快地说话时。</st> <st c="25750">试着大声说出</st>
    *<st c="25761">多模态检索增强再生</st>* <st c="25805">一次，你可能会想从此以后就使用MM-RAG！</st> <st c="25873">但我跑题了。</st>
    <st c="25888">这是一个具有突破性的方法，预计在不久的将来会获得很多关注。</st> <st c="25982">它更好地代表了我们作为人类处理信息的方式，所以它肯定很棒，对吧？</st>
    <st c="26071">让我们首先回顾一下使用</st> <st c="26118">多种模式</st> 的概念。
- en: <st c="26133">Multi-modal</st>
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="26133">多模态</st>
- en: '<st c="26145">Up to this point, everything we have discussed has been focused
    on text: taking the text as input, retrieving text based on that input, and passing
    that retrieved text to an LLM that then generates a final text output.</st> <st
    c="26365">But what about non-text?</st> <st c="26390">As the companies building
    these LLMs have started to offer powerful multi-modal capabilities, how can we
    incorporate those multi-modal capabilities into our</st> <st c="26547">RAG applications?</st>'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26145">到目前为止，我们讨论的一切都集中在文本上：以文本为输入，根据该输入检索文本，然后将检索到的文本传递给LLM，最终生成文本输出。</st>
    <st c="26365">那么非文本呢？</st> <st c="26390">随着构建这些LLM的公司开始提供强大的多模态功能，我们如何将这些多模态功能整合到我们的</st>
    <st c="26547">RAG应用中？</st>
- en: <st c="26564">Multi-modal simply means that you are handling multiple forms
    of “modes,” which include text, images, video, audio, and any other type of input.</st>
    <st c="26710">The multiple modes can be represented in the input, the output,
    or both.</st> <st c="26783">For example, you can pass in text and get an image
    back, and that is</st> <st c="26851">multi-modal.</st> <st c="26865">You can pass
    in an image and get</st> <st c="26897">text back (called captioning), and that
    is</st> <st c="26941">also multi-modal.</st>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26564">多模态简单来说就是处理多种“模式”，包括文本、图像、视频、音频以及其他任何类型的输入。</st> <st c="26710">这些多种模式可以体现在输入、输出或两者之中。</st>
    <st c="26783">例如，你可以传入文本并得到一张图片作为回应，这就是</st> <st c="26851">多模态。</st> <st c="26865">你可以传入一张图片并得到</st>
    <st c="26897">文本作为回应（称为字幕），这也是</st> <st c="26941">多模态。</st>
- en: '<st c="26958">More advanced approaches can also include passing both a text
    prompt of</st> `<st c="27031">"turn this image into a video that goes further
    into the waterfall adding the sounds of the waterfall"</st>` <st c="27133">and
    an image of that waterfall and getting video back that takes the user right into
    the waterfall from the image with an audio track of the waterfall added as well.</st>
    <st c="27300">This would represent four different modes: text, images, video,
    and audio.</st> <st c="27375">Given that models that have these capabilities now
    exist with similar APIs to the ones we have used in this book, it is a short logical
    step to consider how they can be applied to our RAG approach, using RAG to once
    again tap into other types of content we have stored in the data coffers of our
    enterprise.</st> <st c="27683">Let’s discuss the benefits of using a</st> <st
    c="27721">multi-modal approach.</st>'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26958">更高级的方法还可以包括传入一个文本提示</st> `<st c="27031">"将此图像转换为一段视频，进一步展示瀑布，并添加瀑布的声音"`
    <st c="27133">以及一张瀑布的图片，并得到一段视频作为回应，该视频将用户带入图像中的瀑布，并添加了瀑布的声音。</st> <st c="27300">这将代表四种不同的模式：文本、图像、视频和音频。</st>
    <st c="27375">鉴于现在存在具有类似API的模型，这些模型现在具有这些功能，考虑如何将它们应用于我们的RAG方法，使用RAG再次挖掘我们企业数据宝库中存储的其他类型的内容，这是一个短而合理的步骤。</st>
    <st c="27683">让我们讨论使用</st> <st c="27721">多模态方法的好处。</st>
- en: <st c="27742">Benefits of multi-modal</st>
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="27742">多模态的益处</st>
- en: <st c="27766">This approach</st> <st c="27780">leverages the strengths of RAG
    technology in understanding and utilizing multi-modal data sources, allowing for
    the creation of more engaging, informative, and context-rich outputs.</st> <st
    c="27963">By integrating multi-modal data, these RAG systems can provide more
    nuanced and comprehensive answers, generate richer content, and engage in more
    sophisticated interactions with users.</st> <st c="28149">Applications range from
    enhanced conversational agents capable of understanding and generating multimedia
    responses to advanced content-creation tools that can produce complex, multi-modal
    documents and presentations.</st> <st c="28367">MM-RAG represents a significant
    advancement in making RAG systems more versatile and capable of understanding
    the world in a way that mirrors human sensory and</st> <st c="28527">cognitive
    experiences.</st>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27766">这种方法</st> <st c="27780">利用了RAG技术在理解和利用多模态数据源方面的优势，允许创建更具吸引力、信息丰富和上下文丰富的输出。</st>
    <st c="27963">通过整合多模态数据，这些RAG系统可以提供更细微、更全面的答案，生成更丰富的内容，并与用户进行更复杂的交互。</st> <st
    c="28149">应用范围从能够理解和生成多媒体响应的增强型对话代理，到能够生成复杂的多模态文档和演示的高级内容创作工具。</st> <st c="28367">MM-RAG代表了在使RAG系统更加灵活和能够以类似于人类感官和</st>
    <st c="28527">认知体验的方式理解世界方面的一项重大进步。</st>
- en: <st c="28549">Much like with the discussions we had about vectors in</st> *<st
    c="28605">Chapters 7</st>* <st c="28615">and</st> *<st c="28620">8</st>*<st c="28621">,
    it is important to recognize</st> <st c="28651">the important role vector embeddings
    play in MM-RAG</st> <st c="28704">as well.</st>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28549">与我们在第7章和第8章中关于向量的讨论类似，重要的是要认识到向量嵌入在MM-RAG中扮演着重要的角色。</st> <st c="28651">the
    important role vector embeddings play in MM-RAG</st> <st c="28704">as well.</st>
- en: <st c="28712">Multi-modal vector embeddings</st>
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="28712">多模态向量嵌入</st>
- en: <st c="28742">MM-RAG is enabled because vector embeddings can represent more
    than just text; they can represent any kind of data that you pass to it.</st>
    <st c="28879">Some data takes a little more prep work to convert it into something
    that can be vectorized, but all types of data have the potential to be vectorized
    and made available to a RAG application.</st> <st c="29071">If you remember, vectorization
    at its core is</st> <st c="29116">taking your data and turning it into a mathematical
    representation, and math and vectors are the primary language of</st> **<st c="29234">deep
    learning</st>** <st c="29247">(</st>**<st c="29249">DL</st>**<st c="29251">) models</st>
    <st c="29260">that form the foundation for all of our</st> <st c="29301">RAG applications.</st>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28742">MM-RAG被启用，因为向量嵌入不仅能表示文本；它们还能表示你传递给它的任何类型的数据。</st> <st c="28879">某些数据需要做更多准备工作才能将其转换为可以矢量化的事物，但所有类型的数据都有可能被矢量化并供RAG应用使用。</st>
    <st c="29071">如果你还记得，矢量化在本质上是将你的数据转换成数学表示，而数学和向量是</st> **<st c="29234">深度学习</st>**
    <st c="29247">(**<st c="29249">DL</st>**) 模型的**<st c="29251">主要语言</st>**，这些模型构成了我们所有RAG应用的基础。</st>
- en: <st c="29318">Another aspect of vectors that you may remember is the concept
    of vector space, where similar concepts are stored in closer proximity to each
    other in the vector space than dissimilar concepts.</st> <st c="29513">When you
    add multiple modes to the mix, this is still applicable, meaning a concept such
    as a seagull should be represented in a similar fashion whether it is the word</st>
    *<st c="29681">seagull</st>*<st c="29688">, an image of a seagull, a video of
    a seagull, or an audio clip of a seagull squawking.</st> <st c="29776">This multi-modal
    embedding concept of cross-modality representations of the same context is known
    as</st> **<st c="29877">modality independence</st>**<st c="29898">. This extension
    of the vector space</st> <st c="29934">concept forms the basis for how MM-RAG
    serves a similar purpose as single-mode RAG but with multiple modes of data.</st>
    <st c="30051">The key concept is that multi-modal vector embeddings preserve semantic
    similarity across all modalities</st> <st c="30156">they represent.</st>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29318">你可能记得向量的另一个方面是向量空间的概念，其中相似的概念在向量空间中彼此更接近，而不相似的概念则更远。</st> <st
    c="29513">当你将多个模式混合在一起时，这一点仍然适用，这意味着像海鸥这样的概念应该以相似的方式表示，无论是单词**<st c="29681">海鸥</st>**，海鸥的图像，海鸥的视频，还是海鸥尖叫声的音频剪辑。</st>
    <st c="29776">这种多模态嵌入概念，即同一上下文的跨模态表示，被称为**<st c="29877">模态独立性</st>**。这种向量空间概念的扩展是MM-RAG像单模态RAG一样服务于类似目的但具有多种数据模式的基础。</st>
    <st c="29934">关键概念是多模态向量嵌入在它们所代表的所有模态中保持语义相似性。</st> <st c="30051">The key concept
    is that multi-modal vector embeddings preserve semantic similarity across all
    modalities</st> <st c="30156">they represent.</st>
- en: <st c="30171">When it comes to using MM-RAG in the enterprise, it is important
    to recognize that a lot of enterprise data resides in multiple modes, so let’s
    discuss</st> <st c="30324">that next.</st>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30171">当谈到在企业中使用MM-RAG时，重要的是要认识到企业中的大量数据存在于多种模式中，所以让我们接下来讨论这一点。</st>
    <st c="30324">that next.</st>
- en: <st c="30334">Images are not just “pictures”</st>
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="30334">图像不仅仅是“图片”</st>
- en: <st c="30365">Images can be thought of as a lot more than just pretty pictures
    of scenery or those 500 pictures you took on your last vacation!</st> <st c="30496">Images
    in the enterprise can represent things such as charts, flowcharts, text that</st>
    <st c="30579">has at some point been converted to an image, and much more.</st>
    <st c="30641">Images are an important data source for</st> <st c="30681">the enterprise.</st>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30365">图像不仅仅是漂亮的风景画或你在上次度假中拍摄的500张照片那么简单！</st> <st c="30496">在企业中，图像可以代表图表、流程图、某些时候被转换为图像的文本，以及更多。</st>
    <st c="30641">图像是企业的重要数据来源。</st>
- en: <st c="30696">If you haven’t looked at the PDF file representing the</st> *<st
    c="30752">Google Environmental Report 2023</st>* <st c="30784">we have used in
    many of our labs, you may have started to believe that it was just text based.</st>
    <st c="30880">But open it up, and you will see well-designed imagery throughout
    and accompanying the text we have been working with.</st> <st c="30999">Some of
    the charts you see, particularly the highly designed ones, are images.</st> <st
    c="31078">What if we had a RAG application that wanted to utilize the data in</st>
    <st c="31145">those images as well?</st> <st c="31168">Let’s get started in</st>
    <st c="31189">building one!</st>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30696">如果您还没有看过我们许多实验室中使用的代表</st> *<st c="30752">Google Environmental
    Report 2023</st>* <st c="30784">的PDF文件，您可能已经开始相信它只是基于文本的。</st> <st c="30880">但是打开它，您会看到我们一直在工作的文本周围的精心设计的图像。</st>
    <st c="30999">您看到的某些图表，尤其是那些高度设计的图表，实际上是图像。</st> <st c="31078">如果我们有一个想要利用那些图像中数据的RAG应用程序怎么办？</st>
    <st c="31145">让我们开始构建一个吧！</st>
- en: <st c="31202">Introducing MM-RAG in code</st>
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="31202">在代码中介绍MM-RAG</st>
- en: <st c="31229">In this lab, we will do</st> <st c="31254">the following:</st>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31229">在这个实验室中，我们将进行以下操作：</st>
- en: <st c="31268">Extract text and images</st> <st c="31292">from a PDF using a
    powerful open source package</st> <st c="31341">called</st> `<st c="31348">unstructured</st>`<st
    c="31360">.</st>
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="31268">从PDF中提取文本和图像</st> <st c="31292">使用一个强大的开源包</st> <st c="31341">称为</st>
    `<st c="31348">unstructured</st>`<st c="31360">。</st>
- en: <st c="31361">Use a multi-modal LLM to produce text summaries from the</st>
    <st c="31419">images extracted.</st>
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="31361">使用多模态LLM从提取的图像中生成文本摘要。</st>
- en: <st c="31436">Embed and retrieve these image summaries (alongside the text objects
    we have already been using) with a reference to the</st> <st c="31558">raw image.</st>
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="31436">使用对原始图像的引用嵌入和检索这些图像摘要（以及我们之前已经使用的文本对象）。</st>
- en: <st c="31568">Store the image summaries in the multi-vector retriever with Chroma,
    which stores raw text and images along with</st> <st c="31682">their summaries.</st>
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="31568">使用Chroma将图像摘要存储在多向量检索器中，Chroma存储原始文本和图像及其摘要。</st>
- en: <st c="31698">Pass the raw images and text chunks to the same multi-modal LLM
    for</st> <st c="31767">answer synthesis.</st>
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="31698">将原始图像和文本块传递给同一个多模态LLM进行答案合成。</st>
- en: <st c="31784">We start with installing some new packages that you need</st>
    <st c="31842">for the</st> `<st c="31901">unstructured</st>`<st c="31913">:</st>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31784">我们从安装一些您需要用于</st> <st c="31842">unstructured</st>`<st c="31901">的新包开始：</st>
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: <st c="32132">Here is a list of what these packages are going to do for us in</st>
    <st c="32197">our code:</st>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32132">以下是我们将在代码中使用的这些包将为我们做什么的列表：</st>
- en: '`<st c="32206">unstructured[pdf]</st>`<st c="32224">: The</st> `<st c="32231">unstructured</st>`
    <st c="32243">library is a Python library for extracting structured information
    from unstructured data, such as PDFs, images, and HTML pages.</st> <st c="32372">This
    installs only the PDF support from</st> `<st c="32412">unstructured</st>`<st c="32424">.
    There are many other</st> <st c="32447">documents supported that you can include
    if using those types of documents, or you can use</st> `<st c="32538">all</st>`
    <st c="32541">to get support for all documents</st> <st c="32575">they support.</st>'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="32206">unstructured[pdf]</st>`<st c="32224">：The</st> `<st c="32231">unstructured</st>`
    <st c="32243">library is a Python library for extracting structured information
    from unstructured data, such as PDFs, images, and HTML pages.</st> <st c="32372">This
    installs only the PDF support from</st> `<st c="32412">unstructured</st>`<st c="32424">.
    There are many other</st> <st c="32447">documents supported that you can include
    if using those types of documents, or you can use</st> `<st c="32538">all</st>`
    <st c="32541">to get support for all documents</st> <st c="32575">they support.</st>'
- en: '`<st c="32588">pillow</st>`<st c="32595">: The</st> `<st c="32602">pillow</st>`
    <st c="32608">library is a fork</st> <st c="32627">of the</st> `<st c="32669">pillow</st>`
    <st c="32675">library provides support for opening, manipulating, and saving various
    image file formats.</st> <st c="32767">In our code, we are working with images
    when using</st> `<st c="32818">unstructured</st>`<st c="32830">, and</st> `<st
    c="32836">unstructured</st>` <st c="32848">uses</st> `<st c="32854">pillow</st>`
    <st c="32860">to help</st> <st c="32869">with that!</st>'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="32588">pillow</st>`<st c="32595">：The</st> `<st c="32602">pillow</st>`
    <st c="32608">library is a fork</st> <st c="32627">of the</st> `<st c="32669">pillow</st>`
    <st c="32675">library provides support for opening, manipulating, and saving various
    image file formats.</st> <st c="32767">在我们的代码中，我们使用</st> `<st c="32818">unstructured</st>`<st
    c="32830">时正在处理图像，并且</st> `<st c="32836">unstructured</st>` <st c="32848">使用</st>
    `<st c="32854">pillow</st>` <st c="32860">来帮助</st> <st c="32869">完成这项工作！</st>'
- en: '`<st c="32879">pydantic</st>`<st c="32888">: The</st> `<st c="32895">pydantic</st>`
    <st c="32903">library is a data validation and settings management library using
    Python type annotations.</st> <st c="32996">The</st> `<st c="33000">pydantic</st>`
    <st c="33008">library is commonly used for defining data models and validating</st>
    <st c="33074">input data.</st>'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="32879">pydantic</st>`<st c="32888">: The</st> `<st c="32895">pydantic</st>`
    <st c="32903">库是一个使用 Python 类型注解进行数据验证和设置管理的库。</st> <st c="32996">该</st> `<st
    c="33000">pydantic</st>` <st c="33008">库通常用于定义数据模型和验证</st> <st c="33074">输入数据。</st>'
- en: '`<st c="33085">lxml</st>`<st c="33090">: The</st> `<st c="33097">lxml</st>`
    <st c="33101">library is a library for processing XML and HTML documents.</st>
    <st c="33162">We use</st> `<st c="33169">lxml</st>` <st c="33173">alongside the</st>
    `<st c="33188">unstructured</st>` <st c="33200">library or other dependencies
    for parsing and extracting information from</st> <st c="33275">structured documents.</st>'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="33085">lxml</st>`<st c="33090">: The</st> `<st c="33097">lxml</st>`
    <st c="33101">库是一个用于处理 XML 和 HTML 文档的库。</st> <st c="33162">我们使用</st> `<st c="33169">lxml</st>`
    <st c="33173">与</st> `<st c="33188">非结构化</st>` <st c="33200">库或其他依赖项一起用于解析和从</st>
    <st c="33275">结构化文档中提取信息。</st>'
- en: '`<st c="33296">matplotlib</st>`<st c="33307">: The</st> `<st c="33314">matplotlib</st>`
    <st c="33324">library is a well-known plotting library for creating visualizations</st>
    <st c="33394">in Python.</st>'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="33296">matplotlib</st>`<st c="33307">: The</st> `<st c="33314">matplotlib</st>`
    <st c="33324">库是一个用于在 Python 中创建可视化的知名绘图库。</st>'
- en: '`<st c="33404">tiktoken</st>`<st c="33413">: The</st> `<st c="33420">tiktoken</st>`
    <st c="33428">library is a</st> **<st c="33442">Byte-Pair Encoding</st>** <st
    c="33460">(</st>**<st c="33462">BPE</st>**<st c="33465">) tokenizer for use with
    OpenAI’s models.</st> <st c="33508">BPE was</st> <st c="33515">initially developed
    as an algorithm to compress texts and then used by OpenAI for tokenization when
    pre-training the</st> <st c="33633">GPT model.</st>'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="33404">tiktoken</st>`<st c="33413">: The</st> `<st c="33420">tiktoken</st>`
    <st c="33428">库是一个</st> **<st c="33442">字节对编码</st>** <st c="33460">(</st>**<st
    c="33462">BPE</st>**<st c="33465">) 分词器，用于与 OpenAI 的模型一起使用。</st> <st c="33508">BPE
    最初是作为一个用于压缩文本的算法开发的，然后被 OpenAI 用于在预训练</st> <st c="33633">GPT 模型时进行分词。</st>'
- en: '`<st c="33643">poppler-utils</st>`<st c="33657">: The</st> `<st c="33664">poppler</st>`
    <st c="33671">utilities are a set of command-line tools for manipulating PDF files.</st>
    <st c="33742">In our code,</st> `<st c="33755">poppler</st>` <st c="33762">is
    used by</st> `<st c="33774">unstructured</st>` <st c="33786">for extracting elements
    from the</st> <st c="33820">PDF file.</st>'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="33643">poppler-utils</st>`<st c="33657">: The</st> `<st c="33664">poppler</st>`
    <st c="33671">实用工具是一组用于操作 PDF 文件的命令行工具。</st> <st c="33742">在我们的代码中，</st> `<st
    c="33755">poppler</st>` <st c="33762">被</st> `<st c="33774">非结构化</st>` <st c="33786">用于从</st>
    <st c="33820">PDF 文件中提取元素。</st>'
- en: '`<st c="33829">tesseract-ocr</st>`<st c="33843">: The</st> `<st c="33850">tesseract-ocr</st>`
    <st c="33863">engine is an open source OCR engine that can recognize and extract
    text from images.</st> <st c="33949">This is another library required by</st>
    `<st c="33985">unstructured</st>` <st c="33997">for PDF support to pull text</st>
    <st c="34027">from images.</st>'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="33829">tesseract-ocr</st>`<st c="33843">: The</st> `<st c="33850">tesseract-ocr</st>`
    <st c="33863">引擎是一个开源的 OCR 引擎，可以从图像中识别和提取文本。</st> <st c="33949">这是另一个由</st> `<st
    c="33985">非结构化</st>` <st c="33997">库所需的库，用于支持 PDF，从图像中提取文本。</st>'
- en: <st c="34039">These packages provide various functionalities and dependencies
    required by the</st> `<st c="34120">langchain</st>` <st c="34129">and</st> `<st
    c="34134">unstructured</st>` <st c="34146">libraries and their associated modules
    used in the code.</st> <st c="34204">They enable tasks such as PDF parsing, image
    handling, data validation, tokenization, and OCR, which are</st> <st c="34308">essential
    for processing and analyzing PDF files and generating responses to</st> <st c="34386">user
    queries.</st>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34039">这些包提供了 langchain 和 unstructured 库及其在代码中使用的相关模块所需的各项功能和相关依赖。</st>
    <st c="34204">它们使任务如 PDF 解析、图像处理、数据验证、分词和 OCR 成为可能，这些对于处理和分析 PDF 文件以及生成对用户查询的响应是必不可少的。</st>
- en: <st c="34399">We will now add import for these packages and others so that we
    can use them in</st> <st c="34480">our code:</st>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34399">我们现在将添加对这些包以及其他包的导入，以便我们可以在</st> <st c="34480">我们的代码中使用：</st>
- en: '[PRE19]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: <st c="34894">This is a long list of Python packages, so let’s step through
    each of them bullet</st> <st c="34977">by bullet:</st>
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34894">这是一个 Python 包的很长列表，所以让我们逐个列出它们：</st>
- en: '`<st c="34987">MultiVectorRetriever</st>` <st c="35008">from</st> `<st c="35014">langchain.retrievers.multi_vector</st>`<st
    c="35047">: The</st> `<st c="35054">MultiVectorRetriever</st>` <st c="35074">package
    is a retriever that combines multiple vector stores and allows for efficient retrieval
    of documents based on similarity search.</st> <st c="35211">In our code,</st>
    `<st c="35224">MultiVectorRetriever</st>` <st c="35244">is used to create a retriever
    that combines</st> `<st c="35289">vectorstore</st>` <st c="35300">and</st> `<st
    c="35305">docstore</st>` <st c="35313">for retrieving relevant documents based
    on the</st> <st c="35361">user’s query.</st>'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="34987">MultiVectorRetriever</st>` `<st c="35008">from</st>` `<st c="35014">langchain.retrievers.multi_vector</st>`
    `<st c="35047">: The</st> `<st c="35054">MultiVectorRetriever</st>` `<st c="35074">package
    is a retriever that combines multiple vector stores and allows for efficient retrieval
    of documents based on similarity search.</st> `<st c="35211">In our code,</st>`
    `<st c="35224">MultiVectorRetriever</st>` `<st c="35244">is used to create a retriever
    that combines</st> `<st c="35289">vectorstore</st>` `<st c="35300">and</st> `<st
    c="35305">docstore</st>` `<st c="35313">for retrieving relevant documents based
    on the</st> `<st c="35361">user’s query.</st>`'
- en: '`<st c="35374">UnstructuredPDFLoader</st>` <st c="35396">from</st> `<st c="35402">langchain_community.document_loaders</st>`<st
    c="35438">: The</st> `<st c="35445">UnstructuredPDFLoader</st>` <st c="35466">package
    is a document loader that extracts elements, including text and images, from a
    PDF file using the</st> `<st c="35573">unstructured</st>` <st c="35586">library.</st>
    <st c="35595">In our code,</st> `<st c="35608">UnstructuredPDFLoader</st>` <st
    c="35629">is used to load and extract elements from the specified PDF</st> <st
    c="35690">file (</st>`<st c="35696">short_pdf_path</st>`<st c="35711">).</st>'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="35374">UnstructuredPDFLoader</st>` `<st c="35396">from</st>` `<st c="35402">langchain_community.document_loaders</st>`
    `<st c="35438">: The</st> `<st c="35445">UnstructuredPDFLoader</st>` `<st c="35466">package
    is a document loader that extracts elements, including text and images, from a
    PDF file using the</st> `<st c="35573">unstructured</st>` `<st c="35586">library.</st>
    `<st c="35595">In our code,</st>` `<st c="35608">UnstructuredPDFLoader</st>` `<st
    c="35629">is used to load and extract elements from the specified PDF</st> `<st
    c="35690">file (</st>` `<st c="35696">short_pdf_path</st>` `<st c="35711">).</st>`'
- en: '`<st c="35714">RunnableLambda</st>` <st c="35729">from</st> `<st c="35735">langchain_core.runnables</st>`<st
    c="35759">: The</st> `<st c="35766">RunnableLambda</st>` <st c="35780">class is
    a utility class that allows wrapping a function as a runnable component in</st>
    <st c="35864">a LangChain pipeline.</st> <st c="35887">In our code,</st> `<st
    c="35900">RunnableLambda</st>` <st c="35914">is used to wrap the</st> `<st c="35935">split_image_text_types</st>`
    <st c="35957">and</st> `<st c="35962">img_prompt_func</st>` <st c="35977">functions
    as runnable components in the</st> <st c="36018">RAG chain.</st>'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="35714">RunnableLambda</st>` `<st c="35729">from</st>` `<st c="35735">langchain_core.runnables</st>`
    `<st c="35759">: The</st> `<st c="35766">RunnableLambda</st>` `<st c="35780">class
    is a utility class that allows wrapping a function as a runnable component in</st>
    `<st c="35864">a LangChain pipeline.</st> `<st c="35887">In our code,</st>` `<st
    c="35900">RunnableLambda</st>` `<st c="35914">is used to wrap the</st> `<st c="35935">split_image_text_types</st>`
    `<st c="35957">and</st> `<st c="35962">img_prompt_func</st>` `<st c="35977">functions
    as runnable components in the</st> `<st c="36018">RAG chain.</st>`'
- en: '`<st c="36028">InMemoryStore</st>` <st c="36042">from</st> `<st c="36048">langchain.storage</st>`<st
    c="36065">: The</st> `<st c="36072">InMemoryStore</st>` <st c="36085">class is
    a simple in-memory storage class that stores key-value pairs.</st> <st c="36157">In
    our code,</st> `<st c="36169">InMemoryStore</st>` <st c="36183">is used as a document
    store for storing the actual document content associated with each</st> <st c="36273">document
    ID.</st>'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="36028">InMemoryStore</st>` `<st c="36042">from</st>` `<st c="36048">langchain.storage</st>`
    `<st c="36065">: The</st> `<st c="36072">InMemoryStore</st>` `<st c="36085">class
    is a simple in-memory storage class that stores key-value pairs.</st> `<st c="36157">In
    our code,</st>` `<st c="36169">InMemoryStore</st>` `<st c="36183">is used as a
    document store for storing the actual document content associated with each</st>
    `<st c="36273">document ID.</st>`'
- en: '`<st c="36285">HumanMessage</st>` <st c="36298">from</st> `<st c="36304">langchain_core.messages</st>`<st
    c="36327">: We saw this type of prompt in</st> *<st c="36360">Code Lab 14.1</st>*
    <st c="36373">already, representing a message sent by a human user in a conversation
    with the language model.</st> <st c="36470">In this code lab,</st> `<st c="36488">HumanMessage</st>`
    <st c="36500">is used to construct prompt messages for image summarization</st>
    <st c="36562">and description.</st>'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="36285">HumanMessage</st>` `<st c="36298">from</st>` `<st c="36304">langchain_core.messages</st>`
    `<st c="36327">: We saw this type of prompt in</st> *<st c="36360">Code Lab 14.1</st>*
    `<st c="36373">already, representing a message sent by a human user in a conversation
    with the language model.</st> `<st c="36470">In this code lab,</st>` `<st c="36488">HumanMessage</st>`
    `<st c="36500">is used to construct prompt messages for image summarization</st>
    `<st c="36562">and description.</st>`'
- en: '`<st c="36578">base64</st>`<st c="36585">: In our code,</st> `<st c="36601">base64</st>`
    <st c="36607">is used to encode images as</st> `<st c="36636">base64</st>` <st
    c="36642">strings for storage</st> <st c="36663">and retrieval.</st>'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="36578">base64</st>`<st c="36585">: 在我们的代码中，</st> `<st c="36601">base64</st>`
    <st c="36607">用于将图像编码为</st> `<st c="36636">base64</st>` <st c="36642">字符串以进行存储</st>
    <st c="36663">和检索。</st>'
- en: '`<st c="36677">uuid</st>`<st c="36682">: The</st> `<st c="36689">uuid</st>`
    <st c="36693">module provides functions for generating</st> `<st c="36789">uuid</st>`
    <st c="36793">is used to generate unique document IDs for the documents</st> <st
    c="36852">added to</st> `<st c="36861">vectorstore</st>` <st c="36872">and</st>
    `<st c="36877">docstore</st>`<st c="36885">.</st>'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="36677">uuid</st>`<st c="36682">: The</st> `<st c="36689">uuid</st>`
    <st c="36693">模块提供生成</st> `<st c="36789">uuid</st>` <st c="36793">的函数，用于为添加到</st>
    `<st c="36852">vectorstore</st>` <st c="36861">和</st> `<st c="36872">docstore</st>`<st
    c="36877">的文档生成唯一的文档ID。</st>'
- en: '`<st c="36886">HTML</st>` <st c="36891">and</st> `<st c="36896">display</st>`
    <st c="36903">from</st> `<st c="36909">IPython.display</st>`<st c="36924">: The</st>
    `<st c="36931">HTML</st>` <st c="36935">function is used to create HTML representations
    of objects, and the</st> `<st c="37004">display</st>` <st c="37011">function is
    used to display objects in the IPython notebook.</st> <st c="37073">In our code,</st>
    `<st c="37086">HTML</st>` <st c="37090">and</st> `<st c="37095">display</st>`
    <st c="37102">are used in the</st> `<st c="37119">plt_img_base64</st>` <st c="37133">function
    to display</st> `<st c="37154">base64</st>`<st c="37160">-encoded images.</st>'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="36886">HTML</st>` <st c="36891">和</st> `<st c="36896">显示</st>` <st
    c="36903">来自</st> `<st c="36909">IPython.display</st>`<st c="36924">: The</st>
    `<st c="36931">HTML</st>` <st c="36935">函数用于创建对象的HTML表示，而</st> `<st c="37004">显示</st>`
    <st c="37011">函数用于在IPython笔记本中显示对象。</st> <st c="37073">在我们的代码中，</st> `<st c="37086">HTML</st>`
    <st c="37090">和</st> `<st c="37095">显示</st>` <st c="37102">在</st> `<st c="37119">plt_img_base64</st>`
    <st c="37133">函数中用于显示</st> `<st c="37154">base64</st>`<st c="37160">编码的图像。</st>'
- en: '`<st c="37177">Image</st>` <st c="37183">from PIL: PIL provides functions for
    opening, manipulating, and saving various image</st> <st c="37269">file formats.</st>'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="37177">Image</st>` <st c="37183">from PIL: PIL提供打开、操作和保存各种图像</st> <st
    c="37269">文件格式的函数。</st>'
- en: '`<st c="37282">matplotlib.pyplot as plt</st>`<st c="37307">: Matplotlib is
    a plotting library that provides functions for creating visualizations and plots.</st>
    <st c="37406">In the code,</st> `<st c="37419">plt</st>` <st c="37422">is not
    directly used, but it may be used implicitly by other libraries</st> <st c="37494">or
    functions.</st>'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="37282">matplotlib.pyplot as plt</st>`<st c="37307">: Matplotlib是一个提供创建可视化图表函数的绘图库。</st>
    <st c="37406">在代码中，</st> `<st c="37419">plt</st>` <st c="37422">没有直接使用，但它可能被其他库</st>
    <st c="37494">或函数隐式使用。</st>'
- en: <st c="37507">These imported packages</st> <st c="37531">and modules provide
    various functionalities related to document loading, retrieval, storage, messaging,
    image handling, and visualization, which are utilized throughout the code to process
    and analyze the PDF file and generate responses to</st> <st c="37772">user queries.</st>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37507">这些导入的包</st> <st c="37531">和模块提供与文档加载、检索、存储、消息、图像处理和可视化相关的各种功能，这些功能在代码中用于处理和分析PDF文件，并生成对</st>
    <st c="37772">用户查询的响应。</st>
- en: <st c="37785">After our imports, there are several variables we establish that
    are used throughout the code.</st> <st c="37881">Here are a couple</st> <st c="37899">of
    highlights</st><st c="37912">:</st>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37785">在我们的导入之后，我们建立了几个在代码中使用的变量。</st> <st c="37881">这里有一些</st> <st c="37899">亮点</st><st
    c="37912">:</st>
- en: '**<st c="37914">GPT-4o-mini</st>**<st c="37925">: We are going to use</st>
    <st c="37947">GPT-4o-mini, where the last character,</st> **<st c="37987">o</st>**<st
    c="37988">, stands for</st> **<st c="38001">omni</st>**<st c="38005">, which is
    another way to</st> <st c="38030">say it</st> <st c="38038">is multi-modal!</st>'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="37914">GPT-4o-mini</st>**<st c="37925">: 我们将使用</st> <st c="37947">GPT-4o-mini，其中最后一个字符，</st>
    **<st c="37987">o</st>**<st c="37988">，代表</st> **<st c="38001">全功能</st>**<st c="38005">，这也可以说是多模态的另一种说法！</st>'
- en: '[PRE20]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**<st c="38111">Short version of PDF</st>**<st c="38132">: Note we are using
    a different</st> <st c="38165">file now:</st>'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="38111">PDF的简短版本</st>**<st c="38132">: 注意我们现在使用的是不同的</st> <st c="38165">文件：</st>'
- en: '[PRE21]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: <st c="38236">The full file is large, and using the entire file would increase
    the cost to process without providing much value from a demonstration standpoint.</st>
    <st c="38384">So, we encourage you to use this file instead, where we can still
    demonstrate the MM- RAG app but with significantly less inference cost from</st>
    <st c="38526">our LLM.</st>
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="38236">整个文件很大，使用整个文件会增加处理成本，而不会在演示方面提供太多价值。</st> <st c="38384">因此，我们鼓励您使用此文件，我们仍然可以演示
    MM- RAG 应用程序，但与我们的 LLM 相比，推理成本会显著降低。</st> <st c="38526">。</st>
- en: '**<st c="38534">OpenAI embeddings</st>**<st c="38552">: There’s one</st> <st
    c="38566">key thing to note here when using OpenAI embeddings, as</st> <st c="38623">seen
    next:</st>'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="38534">OpenAI 嵌入</st>**<st c="38552">：在使用 OpenAI 嵌入时，这里有一个需要注意的关键点，如下所示：</st>
    <st c="38623">正如所见：</st>'
- en: '[PRE22]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: <st c="38673">This embedding model does not support multi-modal embeddings,
    meaning it will not embed an image of a seagull as very similar to the text word</st>
    *<st c="38817">seagull</st>* <st c="38824">as a true multi-modal embedding model
    should.</st> <st c="38871">To overcome this deficiency, we are embedding the description
    of the image rather than the image itself.</st> <st c="38976">This is still considered
    a multi-modal approach, but keep an eye out for multi-modal embeddings in the
    future that can help us address this at</st> <st c="39118">the embedding level</st>
    <st c="39139">as well!</st>
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38673">此嵌入模型不支持多模态嵌入，这意味着它不会将海鸥的图像嵌入为与文本单词 *<st c="38817">海鸥</st>* <st
    c="38824">非常相似，正如真正的多模态嵌入模型应该做的那样。</st> <st c="38871">为了克服这一不足，我们嵌入的是图像的描述而不是图像本身。</st>
    <st c="38976">这仍然被视为一种多模态方法，但请注意未来可能有助于我们在嵌入级别解决这一问题的多模态嵌入！</st> <st c="39118">。</st>
    <st c="39139">。</st>
- en: <st c="39147">Next, we are going to load the PDF using the</st> `<st c="39193">UnstructuredPDFLoader</st>`
    <st c="39214">document loader:</st>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39147">接下来，我们将使用 `<st c="39193">UnstructuredPDFLoader</st>` <st c="39214">文档加载器来加载
    PDF：</st>
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: <st c="39465">Here, we extract elements from PDF using LangChain and</st> `<st
    c="39521">unstructured</st>`<st c="39533">. This takes a little time, typically
    between 1-5 minutes depending on how powerful your development environment is.</st>
    <st c="39650">So, this is a good time to take a break and read about the parameters
    that make this package work the way we need it</st> <st c="39767">to work!</st>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39465">在这里，我们使用 LangChain 和 `<st c="39521">非结构化</st>`<st c="39533">从
    PDF 中提取元素。这需要一点时间，通常在 1-5 分钟之间，具体取决于您的开发环境有多强大。</st> <st c="39650">因此，这是一个休息和阅读有关使此包按需工作的参数的好时机</st>
    <st c="39767">！</st>
- en: <st c="39775">Let’s talk about what parameters we used with this document loader
    and how they set us up for the rest of this</st> <st c="39887">code lab:</st>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39775">让我们谈谈我们使用此文档加载器的参数以及它们如何为我们设置接下来的代码实验室：</st>
- en: '`<st c="39896">short_pdf_path</st>`<st c="39911">: This is the variable for
    the file path we defined earlier representing the shorter version of our</st>
    <st c="40012">PDF file.</st>'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="39896">short_pdf_path</st>`<st c="39911">：这是代表我们之前定义的 PDF 文件较短版本的文件路径变量。</st>'
- en: '`<st c="40021">mode="elements"</st>`<st c="40037">: This argument sets the
    mode of extraction for</st> `<st c="40086">UnstructuredPDFLoader</st>`<st c="40107">.
    By setting</st> `<st c="40120">mode="elements"</st>`<st c="40135">, the loader
    is instructed to extract individual elements from the PDF file, such as text blocks
    and images.</st> <st c="40244">This mode allows for more fine-grained control
    over the extracted content compared to</st> <st c="40330">other modes.</st>'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="40021">mode="elements"</st>`<st c="40037">：此参数设置 `<st c="40086">UnstructuredPDFLoader</st>`<st
    c="40107"> 的提取模式。通过设置 `<st c="40120">mode="elements"</st>`<st c="40135">，加载器被指示从
    PDF 文件中提取单个元素，例如文本块和图像。</st> <st c="40244">与其它模式相比，此模式允许对提取内容有更精细的控制。</st>'
- en: '`<st c="40342">strategy="hi_res"</st>`<st c="40360">: This argument specifies
    the strategy to be used for extracting elements from the PDF file.</st> <st c="40454">Other
    options include</st> `<st c="40476">auto</st>`<st c="40480">,</st> `<st c="40482">fast</st>`<st
    c="40486">, and</st> `<st c="40492">ocr_only</st>`<st c="40500">. The</st> `<st
    c="40506">"hi_res</st>``<st c="40513">"</st>` <st c="40515">strategy identifies
    the layout of the document and uses it to gain additional information about document
    elements.</st> <st c="40631">If you need to speed this process up considerably,
    try</st> `<st c="40686">fast</st>` <st c="40690">mode, but the extraction will
    not be nearly as effective as what you will see from</st> `<st c="40774">"hi_res"</st>`<st
    c="40782">. We encourage you to try all of the settings to see the differences</st>
    <st c="40851">for yourself.</st>'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="40342">strategy="hi_res"</st>`<st c="40360">: 此参数指定用于从 PDF 文件中提取元素所使用的策略。</st>
    <st c="40454">其他选项包括</st> `<st c="40476">auto</st>`<st c="40480">,</st> `<st c="40482">fast</st>`<st
    c="40486">, 和</st> `<st c="40492">ocr_only</st>`<st c="40500">。其中</st> `<st c="40506">"hi_res</st>``<st
    c="40513">"</st>` <st c="40515">策略识别文档布局并使用它来获取有关文档元素的更多信息。</st> <st c="40631">如果您需要显著加快此过程，请尝试</st>
    `<st c="40686">fast</st>` <st c="40690">模式，但提取效果将远不如您从</st> `<st c="40774">"hi_res"</st>`<st
    c="40782">中看到的效果。我们鼓励您尝试所有设置以亲自查看差异</st> <st c="40851">。</st>'
- en: '`<st c="40864">extract_image_block_types=["Image","Table"]</st>`<st c="40908">:
    The</st> `<st c="40915">extract_image_block_types</st>` <st c="40940">parameter
    is used to specify the types of elements to extract</st> <st c="41003">when processing
    image blocks as</st> `<st c="41035">base64</st>`<st c="41041">-encoded data stored
    in metadata fields.</st> <st c="41083">This parameter allows you to target specific
    elements within images during document processing.</st> <st c="41179">Here, we
    target images</st> <st c="41202">and tables.</st>'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="40864">extract_image_block_types=["Image","Table"]</st>`<st c="40908">:
    `<st c="40915">extract_image_block_types</st>` <st c="40940">参数用于指定在处理图像块时作为</st>
    `<st c="41035">base64</st>`<st c="41041">编码数据存储在元数据字段中时需要提取的元素类型。</st> <st c="41083">此参数允许您在文档处理过程中针对图像中的特定元素进行定位。</st>
    <st c="41179">在此，我们针对图像</st> <st c="41202">和表格。</st>'
- en: '`<st c="41213">extract_image_block_to_payload=True</st>`<st c="41249">: The</st>
    `<st c="41256">extract_image_block_to_payload</st>` <st c="41286">parameter is
    used to specify whether the extracted image blocks should be included in the payload
    as</st> `<st c="41388">base64</st>`<st c="41394">-encoded data.</st> <st c="41410">This
    parameter is relevant when processing documents and extracting image blocks using
    a high-resolution strategy.</st> <st c="41525">We set it to</st> `<st c="41538">True</st>`
    <st c="41542">so that we do not have to actually store any of the images as files;
    the loader will convert the extracted images to</st> `<st c="41660">base64</st>`
    <st c="41666">format and include them in the metadata of the</st> <st c="41714">corresponding
    elements.</st>'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="41213">extract_image_block_to_payload=True</st>`<st c="41249">: `<st
    c="41256">extract_image_block_to_payload</st>` <st c="41286">参数用于指定提取的图像块是否应包含在有效载荷中作为</st>
    `<st c="41388">base64</st>`<st c="41394">编码数据。</st> <st c="41410">此参数在处理文档并使用高分辨率策略提取图像块时相关。</st>
    <st c="41525">我们将它设置为</st> `<st c="41538">True</st>` <st c="41542">，这样我们实际上就不需要存储任何图像作为文件；加载器将提取的图像转换为</st>
    `<st c="41660">base64</st>` <st c="41666">格式，并将它们包含在相应元素的元数据中。</st>'
- en: <st c="41737">When this process is finished, you will have all of the data loaded
    from the PDF into</st> `<st c="41824">pdf_data</st>`<st c="41832">. Let’s add
    some code to help us explore this data that</st> <st c="41888">was loaded:</st>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41737">当此过程完成后，您将拥有所有从 PDF 加载到</st> `<st c="41824">pdf_data</st>`<st
    c="41832">中的数据。让我们添加一些代码来帮助我们探索这些已加载的数据：</st>
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: <st c="42252">Here, we pick out the two most important categories of elements
    for our code lab,</st> `<st c="42335">'</st>``<st c="42336">NarrativeText'</st>`
    <st c="42350">and</st> `<st c="42355">'Image'</st>`<st c="42362">. We use list
    comprehensions to pull those into variables that will hold just</st> <st c="42440">those
    elements.</st>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42252">在此，我们挑选出代码实验室中最重要的两个元素类别，</st> `<st c="42335">'</st>``<st c="42336">NarrativeText'</st>`
    <st c="42350">和</st> `<st c="42355">'Image'</st>`<st c="42362">。我们使用列表推导式将这些元素拉入变量中，这些变量将仅包含</st>
    <st c="42440">这些元素。</st>
- en: <st c="42455">We are about to reduce the number of images to save on processing
    costs, so we print out how many we had beforehand to make sure it works!</st>
    <st c="42595">We also want to see how many element types are</st> <st c="42641">represented
    in the data.</st> <st c="42667">Here is</st> <st c="42675">the output:</st>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42455">我们即将减少图像数量以节省处理成本，因此我们打印出我们之前有多少个以确保它工作！</st> <st c="42595">我们还想看看数据中有多少种元素类型。</st>
    <st c="42667">以下是</st> <st c="42675">输出：</st>
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: <st c="42880">So, right now, we have</st> `<st c="42904">17</st>` <st c="42906">images.</st>
    <st c="42915">We want to reduce that for this demonstration because we are about
    to use an LLM to summarize each of them, and three images cost about six times
    less</st> <st c="43066">than</st> `<st c="43071">17</st>`<st c="43073">!</st>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42880">所以，现在我们有了</st> `<st c="42904">17</st>` <st c="42906">张图片。</st>
    <st c="42915">我们想减少这个演示的数量，因为我们即将使用LLM来总结每一张，三张图片的成本大约是17张的六分之一</st> <st c="43066">！</st>
- en: <st c="43074">We also see that there are many other elements in our data than
    we are using when we just use</st> `<st c="43168">'NarrativeText'</st>`<st c="43183">.
    If we wanted to build a more robust application, we could incorporate the</st>
    `<st c="43259">'Title'</st>`<st c="43266">,</st> `<st c="43268">'Footer'</st>`<st
    c="43276">,</st> `<st c="43278">'Header'</st>`<st c="43286">, and other elements
    into the context we send to the LLM, telling it to emphasize those elements accordingly.</st>
    <st c="43396">For example, we can tell it to give more emphasis to</st> `<st c="43449">'Title'</st>`<st
    c="43456">. The</st> `<st c="43462">unstructured</st>` <st c="43474">library does
    a really great job giving us our PDF data in a way that makes it</st> <st c="43553">more
    LLM-friendly!</st>
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43074">我们还看到，当我们只使用</st> `<st c="43168">'NarrativeText'</st>`<st c="43183">时，我们的数据中包含许多其他元素。</st>
    <st c="43259">如果我们想构建一个更健壮的应用程序，我们可以将</st> `<st c="43259">'Title'</st>`<st c="43266">,</st>
    `<st c="43268">'Footer'</st>`<st c="43276">,</st> `<st c="43278">'Header'</st>`<st
    c="43286">和其他元素纳入我们发送给LLM的上下文中，告诉它相应地强调这些元素。</st> <st c="43396">例如，我们可以告诉它更多地强调</st>
    `<st c="43449">'Title'</st>`<st c="43456">。这个</st> `<st c="43462">非结构化</st>` <st
    c="43474">库在以使它</st> <st c="43553">更适合LLM的方式提供我们的PDF数据方面做得非常出色！</st>
- en: <st c="43571">OK – so, as promised, we are going to reduce the image count to
    save you a little money on</st> <st c="43663">the processing:</st>
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43571">OK – 所以，正如承诺的那样，我们将减少图像数量以节省你在</st> <st c="43663">处理上的费用：</st>
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: <st c="43803">We basically just lop off the first three images and use that
    list in the</st> `<st c="43878">images</st>` <st c="43884">list.</st> <st c="43891">We
    print this out and see we have reduced it to</st> <st c="43939">three images:</st>
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43803">我们基本上只是切掉了前三张图片，并使用这个列表在</st> `<st c="43878">images</st>` <st
    c="43884">列表中使用。</st> <st c="43891">我们打印出来，看到我们已经减少到</st> <st c="43939">三张图片：</st>
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: <st c="44005">The next few code blocks will focus on</st> <st c="44044">image
    summarization, starting with our function to apply the prompt to the image and
    get</st> <st c="44134">a summary:</st>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44005">接下来的几个代码块将专注于</st> <st c="44044">图像摘要，从我们将提示应用于图像并获取</st> <st
    c="44134">摘要的函数开始：</st>
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: <st c="44566">This function takes an</st> `<st c="44590">img_base64</st>` <st
    c="44600">parameter, which represents the</st> `<st c="44633">base64</st>`<st
    c="44639">-encoded string of an image.</st> <st c="44669">The function starts
    with defining a prompt variable that contains a string prompt instructing the
    assistant to summarize the image for retrieval purposes.</st> <st c="44824">The
    function returns a list containing a single</st> `<st c="44872">HumanMessage</st>`
    <st c="44884">object representing the summary of the image.</st> <st c="44931">The</st>
    `<st c="44935">HumanMessage</st>` <st c="44947">object has a</st> `<st c="44961">content</st>`
    <st c="44968">parameter, which is a list containing</st> <st c="45007">two dictionaries:</st>
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44566">此函数接受一个</st> `<st c="44590">img_base64</st>` <st c="44600">参数，它表示图像的</st>
    `<st c="44633">base64</st>`<st c="44639">-编码字符串。</st> <st c="44669">函数首先定义一个包含字符串提示的提示变量，指示助手为了检索目的总结图像。</st>
    <st c="44824">该函数返回一个包含单个</st> `<st c="44872">HumanMessage</st>` <st c="44884">对象的列表，该对象表示图像的摘要。</st>
    <st c="44931">该</st> `<st c="44935">HumanMessage</st>` <st c="44947">对象有一个</st>
    `<st c="44961">content</st>` <st c="44968">参数，它是一个包含</st> <st c="45007">两个字典的列表：</st>
- en: <st c="45024">The first dictionary represents a text message with the prompt
    as</st> <st c="45091">its value</st>
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="45024">第一个字典表示一个文本消息，其提示作为</st> <st c="45091">其值</st>
- en: <st c="45100">The second dictionary represents an image URL message, where the</st>
    `<st c="45165">image_url</st>` <st c="45175">key contains a dictionary with the</st>
    `<st c="45211">url</st>` <st c="45214">key set to the</st> `<st c="45230">base64</st>`<st
    c="45236">-encoded image prefixed with the appropriate data URI</st> <st c="45291">scheme
    (</st>`<st c="45299">data:image/jpeg;base64</st>`<st c="45322">)</st>
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="45100">第二个字典表示一个图像URL消息，其中</st> `<st c="45165">image_url</st>` <st c="45175">键包含一个字典，该字典的</st>
    `<st c="45211">url</st>` <st c="45214">键设置为以适当的data URI方案（</st>`<st c="45299">data:image/jpeg;base64</st>`<st
    c="45322">）为前缀的</st> `<st c="45230">base64</st>`<st c="45236">-编码的图像</st>
- en: <st c="45324">Remember when we set</st> `<st c="45345">extract_image_block_to_payload</st>`
    <st c="45375">to</st> `<st c="45379">True</st>` <st c="45383">when using the</st>
    `<st c="45399">UnstructuredPDFLoader</st>` <st c="45420">document loader function?</st>
    <st c="45447">We already have our image in</st> `<st c="45476">base64</st>` <st
    c="45482">format in our metadata as a result, so we just need to pass that into
    this function!</st> <st c="45568">If you use this approach in other applications
    and have a typical image file, such as a</st> `<st c="45656">.jpg</st>` <st c="45660">or</st>
    `<st c="45664">.png</st>` <st c="45668">file, you would just need to convert it
    to</st> `<st c="45712">base64</st>` <st c="45718">to use</st> <st c="45726">this
    function.</st>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45324">记得当我们使用</st> `<st c="45345">extract_image_block_to_payload</st>`
    <st c="45375">将</st> `<st c="45379">True</st>` <st c="45383">设置为</st> `<st c="45399">UnstructuredPDFLoader</st>`
    <st c="45420">文档加载函数时吗？</st> <st c="45447">因此，我们的元数据中已经有了图像，以</st> `<st c="45476">base64</st>`
    <st c="45482">格式存在，所以我们只需要将这个传递给这个函数！</st> <st c="45568">如果你在其他应用程序中使用这种方法，并且有一个典型的图像文件，比如一个</st>
    `<st c="45656">.jpg</st>` <st c="45660">或</st> `<st c="45664">.png</st>` <st c="45668">文件，你只需要将其转换为</st>
    `<st c="45712">base64</st>` <st c="45718">即可使用此功能。</st>
- en: <st c="45740">For this application, though, because</st> <st c="45779">we extracted
    the images as</st> `<st c="45806">base64</st>` <st c="45813">representations of
    the images, the LLM works with</st> `<st c="45863">base64</st>` <st c="45869">images,
    and this function uses that as the parameter, so we do not need to actually work
    with image files!</st> <st c="45977">Are you disappointed that you will not see
    any images?</st> <st c="46032">Do not be!</st> <st c="46043">We will create a
    helper function in a moment using the HTML function we talked about earlier to
    convert the images from their</st> `<st c="46169">base64</st>` <st c="46175">representations
    into HTML versions that we can display in</st> <st c="46234">our notebook!</st>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45740">但是，对于这个应用程序来说，因为我们</st> <st c="45779">将图像提取为</st> `<st c="45806">base64</st>`
    <st c="45813">表示形式，LLM与</st> `<st c="45863">base64</st>` <st c="45869">图像一起工作，而这个函数使用它作为参数，所以我们实际上不需要处理图像文件！</st>
    <st c="45977">你失望地发现你将看不到任何图像吗？</st> <st c="46032">不要失望！</st> <st c="46043">我们将使用之前讨论过的HTML函数创建一个辅助函数，将图像从它们的</st>
    `<st c="46169">base64</st>` <st c="46175">表示形式转换为HTML版本，这样我们就可以在我们的</st> `<st
    c="46234">笔记本</st>》中显示它们了！</st>
- en: <st c="46247">But first, we prep our texts and images and set up lists to collect
    the summaries as we run the functions we</st> <st c="46357">just discussed:</st>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46247">但首先，我们准备我们的文本和图像，并设置列表以收集我们在运行我们</st> <st c="46357">刚刚讨论过的函数时生成的摘要：</st>
- en: '[PRE29]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: <st c="46711">Note that we are not running the summarization on the texts; we
    are just taking the texts directly as summaries.</st> <st c="46825">You could
    summarize the texts as well, and this may improve retrieval results, as this is
    a common approach in improving RAG retrieval.</st> <st c="46961">However, to save
    some more LLM processing costs, we are focusing on just summarizing the images
    here.</st> <st c="47063">Your wallet will</st> <st c="47080">thank us!</st>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46711">请注意，我们不是在文本上运行摘要；我们只是直接将文本作为摘要。</st> <st c="46825">你也可以对文本进行摘要，这可能会提高检索结果，因为这是改进RAG检索的常见方法。</st>
    <st c="46961">然而，为了节省更多的LLM处理成本，我们在这里只专注于对图像进行摘要。</st> <st c="47063">你的钱包会</st>
    <st c="47080">感谢我们！</st>
- en: <st c="47089">For the images though, this is it – you just went multi-modal,
    using both text and images in your LLM usage!</st> <st c="47199">We cannot yet
    say we used it for MM-RAG, as we are not retrieving anything in a multi-modal
    way.</st> <st c="47296">But we will get there soon – let’s</st> <st c="47331">keep
    going!</st>
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47089">尽管如此，对于图像来说，这就足够了——你刚刚实现了多模态，在你的LLM使用中同时使用了文本和图像！</st> <st c="47199">我们目前还不能说我们使用了MM-RAG，因为我们还没有以多模态的方式检索任何内容。</st>
    <st c="47296">但我们很快就会达到那里——让我们</st> <st c="47331">继续前进！</st>
- en: <st c="47342">Our data preparation has</st> <st c="47368">come to an end; we
    can now go back to adding RAG-related elements such as vector stores and retrievers!</st>
    <st c="47472">Here, we set up the</st> <st c="47492">vector store:</st>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47342">我们的数据准备已经结束；现在我们可以回到添加与RAG相关的元素，例如向量存储和检索器！</st> <st c="47368">在这里，我们设置了向量存储：</st>
- en: '[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: <st c="47614">Here, we set up a new collection name,</st> `<st c="47654">mm_rag_google_environment</st>`<st
    c="47679">, indicative of the multi-modal nature of the contents of this vector
    store.</st> <st c="47756">We add our</st> `<st c="47767">embedding_function</st>`
    <st c="47785">chain that will be used to embed our content similar to what we
    have seen numerous times in our code labs.</st> <st c="47893">However, in the
    past, we typically added the documents to our vector store as we set</st> <st
    c="47978">it up.</st>
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47614">在这里，我们设置了一个新的集合名称，</st> `<st c="47654">mm_rag_google_environment</st>`<st
    c="47679">，这表明了该向量存储内容的多元模态性质。</st> <st c="47756">我们添加了我们的</st> `<st c="47767">embedding_function</st>`
    <st c="47785">链，该链将用于嵌入我们的内容，就像我们在代码实验室中多次看到的那样。</st> <st c="47893">然而，在过去，我们通常在设置向量存储时添加文档。</st>
    <st c="47978">设置它。</st>
- en: <st c="47984">In this case, though, we wait to add the documents not just after
    setting up the vector store but after setting up the retriever too!</st> <st c="48119">How
    can we add them to a retriever, a mechanism for retrieving documents?</st> <st
    c="48193">Well, as we’ve said in the past, a retriever in LangChain is simply
    a wrapper around the vector store, so the vector store is still in there, and
    we can add documents through the retriever in a</st> <st c="48387">similar fashion.</st>
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47984">然而，在这种情况下，我们等待添加文档的时间不仅是在设置向量存储之后，也是在设置检索器之后！</st> <st c="48119">我们如何将它们添加到一个检索器中，这是一个检索文档的机制？</st>
    <st c="48193">嗯，就像我们过去说的那样，LangChain中的检索器只是一个围绕向量存储的包装器，所以向量存储仍然在其中，我们可以通过检索器以类似的方式添加文档。</st>
    <st c="48387">以类似的方式。</st>
- en: <st c="48403">But first, we need to set up the</st> <st c="48437">multi-vector
    retriever:</st>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48403">但是首先，我们需要设置</st> <st c="48437">多向量检索器：</st>
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: <st c="48607">Here, we have applied this</st> `<st c="48635">MultiVectorRetriever</st>`
    <st c="48655">wrapper to our</st> `<st c="48671">vectorstore</st>` <st c="48682">vector
    store.</st> <st c="48697">But what is this other element,</st> `<st c="48729">InMemoryStore</st>`<st
    c="48742">? The</st> `<st c="48748">InMemoryStore</st>` <st c="48761">element
    is an in-memory storage class that stores key-value pairs.</st> <st c="48829">It
    is used as</st> `<st c="48843">docstore</st>` <st c="48851">object for storing
    the actual document content associated with each document ID.</st> <st c="48933">We
    provide those by defining the</st> `<st c="48966">id_key</st>` <st c="48972">with
    the</st> `<st c="48982">doc_id</st>` <st c="48988">string.</st>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48607">在这里，我们应用了这个</st> `<st c="48635">MultiVectorRetriever</st>` <st
    c="48655">包装器到我们的</st> `<st c="48671">vectorstore</st>` <st c="48682">向量存储。</st>
    <st c="48697">但是，这个其他元素，</st> `<st c="48729">InMemoryStore</st>`<st c="48742">，是什么？</st>
    `<st c="48748">InMemoryStore</st>` <st c="48761">元素是一个内存存储类，用于存储键值对。</st> <st
    c="48829">它用作</st> `<st c="48843">docstore</st>` <st c="48851">对象，用于存储与每个文档ID关联的实际文档内容。</st>
    <st c="48933">我们通过定义</st> `<st c="48966">id_key</st>` <st c="48972">与</st> `<st
    c="48982">doc_id</st>` <st c="48988">字符串来提供这些。</st>
- en: <st c="48996">At this point, we pass everything to</st> `<st c="49034">MultiVectorRetriever(...)</st>`<st
    c="49059">, a retriever that combines multiple vector stores and allows for efficient
    retrieval of multiple data types based on similarity search.</st> <st c="49196">We
    have seen the</st> `<st c="49213">vectorstore</st>` <st c="49224">vector store
    many times, but as you can see, you can use a</st> `<st c="49284">docstore</st>`
    <st c="49292">object for storing and retrieving document content.</st> <st c="49345">It
    is set to the</st> `<st c="49362">store</st>` <st c="49367">variable (an instance
    of</st> `<st c="49393">InMemoryStore</st>`<st c="49406">) with the</st> `<st c="49418">id_key</st>`
    <st c="49424">string being set as the</st> `<st c="49449">id_key</st>` <st c="49455">parameter
    in the retriever.</st> <st c="49484">This makes it easy to retrieve additional</st>
    <st c="49525">content that is related to the vectors in the vector store, using
    that</st> `<st c="49597">id_key</st>` <st c="49603">string like a foreign key
    in a relational database across the</st> <st c="49666">two stores.</st>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48996">在这个阶段，我们将所有内容传递给</st> `<st c="49034">MultiVectorRetriever(...)</st>`<st
    c="49059">，这是一个结合多个向量存储并允许基于相似性搜索高效检索多种数据类型的检索器。</st> <st c="49196">我们已经多次看到了</st>
    `<st c="49213">vectorstore</st>` <st c="49224">向量存储，但正如你所见，你可以使用一个</st> `<st c="49284">docstore</st>`
    <st c="49292">对象来存储和检索文档内容。</st> <st c="49345">它被设置为</st> `<st c="49362">store</st>`
    <st c="49367">变量（一个</st> `<st c="49393">InMemoryStore</st>`<st c="49406">的实例）</st>，其中</st>
    `<st c="49418">id_key</st>` <st c="49424">字符串被设置为检索器中的</st> `<st c="49449">id_key</st>`
    <st c="49455">参数。</st> <st c="49484">这使得使用那个</st> `<st c="49597">id_key</st>`
    <st c="49603">字符串，就像在关系数据库中跨越两个存储的键值一样，轻松检索与向量存储中的向量相关的内容。</st>
- en: <st c="49677">We still do not have any data in any of our stores, though!</st>
    <st c="49738">Let’s build a function that will allow us to</st> <st c="49783">add
    data:</st>
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="49677">尽管如此，我们还没有在我们的任何存储中添加任何数据！</st> <st c="49738">让我们构建一个函数，这样我们就可以</st>
    <st c="49783">添加数据：</st>
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '<st c="50251">This function is a helper function that adds documents to the</st>
    `<st c="50314">vectorstore</st>` <st c="50325">vector store and</st> `<st c="50343">docstore</st>`
    <st c="50351">object of the</st> `<st c="50366">retriever</st>` <st c="50375">object.</st>
    <st c="50384">It takes the</st> `<st c="50397">retriever</st>` <st c="50406">object,</st>
    `<st c="50415">doc_summaries</st>` <st c="50428">list, and</st> `<st c="50439">doc_contents</st>`
    <st c="50451">list as arguments.</st> <st c="50471">As we’ve already discussed,
    we have summaries and contents for each of our categories: the texts and images
    that we will pass to</st> <st c="50600">this function.</st>'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50251">这个函数是一个辅助函数，它将文档添加到</st> `<st c="50314">vectorstore</st>` <st
    c="50325">向量存储和</st> `<st c="50343">docstore</st>` <st c="50351">检索器对象的</st> `<st
    c="50366">对象</st>` <st c="50375">。</st> <st c="50384">它接受</st> `<st c="50397">retriever</st>`
    <st c="50406">对象、</st> `<st c="50415">doc_summaries</st>` <st c="50428">列表和</st>
    `<st c="50439">doc_contents</st>` <st c="50451">列表作为参数。</st> <st c="50471">正如我们之前讨论的，我们为每个类别都有摘要和内容：我们将传递给</st>
    `<st c="50600">此函数</st>` <st c="50609">的文本和图像。</st>
- en: <st c="50614">This function generates unique</st> <st c="50646">document IDs
    for each document using</st> `<st c="50683">str(uuid.uuid4())</st>` <st c="50700">and
    then creates a</st> `<st c="50720">summary_docs</st>` <st c="50732">list by iterating
    over the</st> `<st c="50760">doc_summaries</st>` <st c="50773">list and creating</st>
    `<st c="50792">Document</st>` <st c="50800">objects with the summary as the page
    content and the corresponding document ID as metadata.</st> <st c="50893">It also
    creates a</st> `<st c="50911">content_docs</st>` <st c="50923">list of</st> `<st
    c="50932">Document</st>` <st c="50940">objects by iterating over the</st> `<st
    c="50971">doc_contents</st>` <st c="50983">list and creating</st> `<st c="51002">Document</st>`
    <st c="51010">objects with the document content as the page content and the corresponding
    document ID as metadata.</st> <st c="51112">It adds the</st> `<st c="51124">summary_docs</st>`
    <st c="51136">list to the retriever’s</st> `<st c="51161">vectorstore</st>` <st
    c="51172">vector store using the</st> `<st c="51196">retriever.vectorstore.add_documents</st>`
    <st c="51231">function.</st> <st c="51242">It uses the</st> `<st c="51254">retriever.docstore.mset</st>`
    <st c="51277">function to add the</st> `<st c="51298">content_docs</st>` <st c="51310">list
    to the retriever’s</st> `<st c="51335">docstore</st>` <st c="51343">object, associating
    each document ID with its corresponding</st> <st c="51404">document content.</st>
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50614">此函数使用</st> `<st c="50646">str(uuid.uuid4())</st>` <st c="50700">为每个文档生成唯一的</st>
    <st c="50720">文档ID</st> <st c="50732">，然后通过遍历</st> `<st c="50760">doc_summaries</st>`
    <st c="50773">列表并创建具有摘要作为页面内容以及相应的文档ID作为元数据的</st> `<st c="50792">Document</st>`
    <st c="50800">对象来创建一个</st> `<st c="50720">summary_docs</st>` <st c="50732">列表。</st>
    <st c="50893">它还通过遍历</st> `<st c="50971">doc_contents</st>` <st c="50983">列表并创建具有文档内容作为页面内容以及相应的文档ID作为元数据的</st>
    `<st c="51002">Document</st>` <st c="51010">对象来创建一个</st> `<st c="50911">content_docs</st>`
    <st c="50923">列表。</st> <st c="50932">Document</st> <st c="50940">对象。</st> <st
    c="51112">它使用</st> `<st c="51124">summary_docs</st>` <st c="51136">列表通过</st> `<st
    c="51161">retriever.vectorstore.add_documents</st>` <st c="51172">函数将其添加到检索器的</st>
    `<st c="51196">vectorstore</st>` <st c="51231">向量存储中。</st> <st c="51242">它使用</st>
    `<st c="51254">retriever.docstore.mset</st>` <st c="51277">函数将</st> `<st c="51298">content_docs</st>`
    <st c="51310">列表添加到检索器的</st> `<st c="51335">docstore</st>` <st c="51343">对象中，将每个文档ID与其对应的</st>
    <st c="51404">文档内容关联起来。</st>
- en: <st c="51421">We still need to apply the</st> `<st c="51449">add_document</st>`
    <st c="51461">function:</st>
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51421">我们仍然需要应用</st> `<st c="51449">add_document</st>` <st c="51461">函数：</st>
- en: '[PRE33]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: <st c="51636">This will add the appropriate documents and summaries where we
    need them for our MM-RAG pipeline, adding embedding vectors that represent both
    text and</st> <st c="51789">image summaries.</st>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51636">这将添加我们MM-RAG管道所需的相关文档和摘要，包括代表文本和</st> <st c="51789">图像摘要的嵌入向量。</st>
- en: <st c="51805">Next, we will add a final round of helper functions we will need
    in our final MM-RAG chain, starting with one that splits</st> `<st c="51928">base64</st>`<st
    c="51934">-encoded images</st> <st c="51951">and texts:</st>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51805">接下来，我们将添加我们最终MM-RAG链中需要的最后一批辅助函数，首先是用于分割</st> `<st c="51928">base64</st>`<st
    c="51934">-编码的图像</st> <st c="51951">和文本的函数：</st>
- en: '[PRE34]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: <st c="52321">This function takes our list of image-related docs as input and
    splits them into</st> `<st c="52403">base64</st>`<st c="52409">-encoded images
    and texts.</st> <st c="52437">It initializes two empty lists:</st> `<st c="52469">b64_images</st>`
    <st c="52479">and</st> `<st c="52484">texts</st>`<st c="52489">. It iterates over
    each</st> `<st c="52513">doc</st>` <st c="52516">variable in the</st> `<st c="52533">docs</st>`
    <st c="52537">list, checking if it is an instance of the</st> `<st c="52581">Document</st>`
    <st c="52589">class.</st> <st c="52597">If the</st> `<st c="52604">doc</st>` <st
    c="52607">variable is a</st> `<st c="52622">Document</st>` <st c="52630">object
    and its metadata has a</st> `<st c="52661">category</st>` <st c="52669">key with
    the value</st> `<st c="52689">Image</st>`<st c="52694">, it extracts the</st>
    `<st c="52712">base64</st>`<st c="52718">-encoded image from</st> `<st c="52739">doc.metadata["image_base64"]</st>`
    <st c="52767">and appends it to the</st> `<st c="52790">b64_images</st>` <st c="52800">list.</st>
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52321">此函数接收我们的图像相关文档列表作为输入，并将它们拆分为</st> `<st c="52403">base64</st>`<st
    c="52409">编码图像和文本。</st> <st c="52437">它初始化两个空列表：</st> `<st c="52469">b64_images</st>`
    <st c="52479">和</st> `<st c="52484">texts</st>`<st c="52489">。它遍历</st> `<st c="52533">docs</st>`
    <st c="52537">列表中的每个</st> `<st c="52513">doc</st>` <st c="52516">变量，检查它是否是</st>
    `<st c="52581">Document</st>` <st c="52589">类的实例。</st> <st c="52597">如果</st> `<st
    c="52604">doc</st>` <st c="52607">变量是一个</st> `<st c="52622">Document</st>` <st
    c="52630">对象并且其元数据有一个</st> `<st c="52661">category</st>` <st c="52669">键，其值为</st>
    `<st c="52689">Image</st>`<st c="52694">，它从</st> `<st c="52739">doc.metadata["image_base64"]</st>`
    <st c="52767">中提取</st> `<st c="52712">base64</st>`<st c="52718">编码的图像并将其添加到</st>
    `<st c="52790">b64_images</st>` <st c="52800">列表中。</st>
- en: <st c="52806">If the</st> `<st c="52814">doc</st>` <st c="52817">variable is
    a</st> `<st c="52832">Document</st>` <st c="52840">object</st> <st c="52847">but
    does not have the</st> `<st c="52870">Image</st>` <st c="52875">category, it appends</st>
    `<st c="52897">doc.page_content</st>` <st c="52913">to the</st> `<st c="52921">texts</st>`
    <st c="52926">list.</st> <st c="52933">If the</st> `<st c="52940">doc</st>` <st
    c="52943">variable is not a</st> `<st c="52962">Document</st>` <st c="52970">object
    but is a string, it appends the</st> `<st c="53010">doc</st>` <st c="53013">variable
    to the</st> `<st c="53030">texts</st>` <st c="53035">list.</st> <st c="53042">Finally,
    the function returns a dictionary with two keys:</st> `<st c="53100">"images"</st>`<st
    c="53108">, containing a list of</st> `<st c="53131">base64</st>`<st c="53137">-encoded
    images, and</st> `<st c="53159">"texts"</st>`<st c="53166">, containing a list</st>
    <st c="53186">of texts.</st>
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52806">如果</st> `<st c="52814">doc</st>` <st c="52817">变量是一个</st> `<st
    c="52832">Document</st>` <st c="52840">对象</st> <st c="52847">但没有</st> `<st c="52870">Image</st>`
    <st c="52875">类别，它将</st> `<st c="52897">doc.page_content</st>` <st c="52913">添加到</st>
    `<st c="52921">texts</st>` <st c="52926">列表中。</st> <st c="52933">如果</st> `<st
    c="52940">doc</st>` <st c="52943">变量不是一个</st> `<st c="52962">Document</st>` <st
    c="52970">对象但是一个字符串，它将</st> `<st c="53010">doc</st>` <st c="53013">变量添加到</st>
    `<st c="53030">texts</st>` <st c="53035">列表中。</st> <st c="53042">最后，该函数返回一个包含两个键的字典：</st>
    `<st c="53100">"images"</st>`<st c="53108">，包含一个包含</st> `<st c="53131">base64</st>`<st
    c="53137">编码图像的列表，以及</st> `<st c="53159">"texts"</st>`<st c="53166">，包含一个文本列表。</st>
- en: <st c="53195">We also have a function to</st> <st c="53222">generate our image</st>
    <st c="53242">prompt message:</st>
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="53195">我们还有一个函数用于</st> <st c="53222">生成我们的图像</st> <st c="53242">提示信息：</st>
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: <st c="54104">This function takes</st> `<st c="54125">data_dict</st>` <st c="54134">as
    input</st> <st c="54143">and generates a prompt message for image analysis.</st>
    <st c="54195">It extracts texts from</st> `<st c="54218">data_dict["context"]</st>`
    <st c="54238">and joins them into a single string,</st> `<st c="54276">formatted_texts</st>`<st
    c="54291">, using</st> `<st c="54299">"\n".join</st>`<st c="54308">. It initializes
    an empty list</st> <st c="54339">called</st> `<st c="54346">messages</st>`<st
    c="54354">.</st>
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54104">此函数接收</st> `<st c="54125">data_dict</st>` <st c="54134">作为输入</st>
    <st c="54143">并生成一个用于图像分析的提示信息。</st> <st c="54195">它从</st> `<st c="54218">data_dict["context"]</st>`
    <st c="54238">中提取文本，并使用</st> `<st c="54299">"\n".join</st>` <st c="54308">将它们合并成一个字符串，</st>
    `<st c="54276">formatted_texts</st>`<st c="54291">。它初始化一个名为</st> `<st c="54339">messages</st>`<st
    c="54346">的空列表。</st>
- en: <st c="54355">If</st> `<st c="54359">data_dict["context"]["images"]</st>` <st
    c="54389">exists, it iterates over each image in the list.</st> <st c="54439">For
    each image, it creates an</st> `<st c="54469">image_message</st>` <st c="54482">dictionary
    with a</st> `<st c="54501">"type"</st>` <st c="54507">key set to</st> `<st c="54519">"image_url"</st>`
    <st c="54530">and an</st> `<st c="54538">"image_url"</st>` <st c="54549">key containing
    a dictionary with the</st> `<st c="54587">base64</st>`<st c="54593">-encoded image
    URL.</st> <st c="54614">It appends each</st> `<st c="54630">image_message</st>`
    <st c="54643">instance to the</st> `<st c="54660">messages</st>` <st c="54668">list.</st>
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54355">如果</st> `<st c="54359">data_dict["context"]["images"]</st>` <st
    c="54389">存在，它将遍历列表中的每个图像。</st> <st c="54439">对于每个图像，它创建一个包含</st> `<st c="54469">image_message</st>`
    <st c="54482">字典，其中有一个</st> `<st c="54501">"type"</st>` <st c="54507">键设置为</st>
    `<st c="54519">"image_url"</st>` <st c="54530">和一个</st> `<st c="54538">"image_url"</st>`
    <st c="54549">键包含一个包含</st> `<st c="54587">base64</st>`<st c="54593">编码的图像URL的字典。</st>
    <st c="54614">它将每个</st> `<st c="54630">image_message</st>` <st c="54643">实例追加到</st>
    `<st c="54660">messages</st>` <st c="54668">列表中。</st>
- en: <st c="54674">And now, the final touch – before</st> <st c="54709">we run our
    MM-RAG application, we establish an MM-RAG chain, including the use of the two
    functions we just</st> <st c="54817">set up:</st>
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54674">现在，最后的润色——在我们运行我们的MM-RAG应用程序之前，我们建立了一个MM-RAG链，包括使用我们刚刚</st> <st
    c="54709">设置的</st> <st c="54817">两个函数：</st>
- en: '[PRE36]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: <st c="55022">This creates our MM-RAG chain, which consists of the</st> <st
    c="55076">following components:</st>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="55022">这创建了我们自己的MM-RAG链，它由以下</st> `<st c="55076">组件组成：</st>`
- en: '`<st c="55097">{"context": retriever_multi_vector | RunnableLambda(split_image_text_types),
    "question": RunnablePassthrough()}</st>`<st c="55209">: This is similar to other
    retriever components we’ve seen in the past, providing a dictionary with two keys:</st>
    `<st c="55320">"context"</st>` <st c="55329">and</st> `<st c="55334">"question"</st>`<st
    c="55344">. The</st> `<st c="55350">"context"</st>` <st c="55359">key is assigned
    the result of</st> `<st c="55390">retriever_multi_vector | RunnableLambda(split_image_text_types)</st>`<st
    c="55453">. The</st> `<st c="55459">retriever_multi_vector</st>` <st c="55481">function
    retrieves relevant documents based on the question, and those results are then
    passed through</st> `<st c="55585">RunnableLambda(split_image_text_types)</st>`<st
    c="55623">, which is a wrapper around the</st> `<st c="55655">split_image_text_types</st>`
    <st c="55677">function.</st> <st c="55688">As we discussed previously, the</st>
    `<st c="55720">split_image_text_types</st>` <st c="55742">function splits the
    retrieved documents into</st> `<st c="55788">base64</st>`<st c="55794">-encoded
    images and texts.</st> <st c="55822">The</st> `<st c="55826">"question"</st>`
    <st c="55836">key is assigned</st> `<st c="55853">RunnablePassthrough</st>`<st
    c="55872">, which simply passes the question through without</st> <st c="55923">any
    modification.</st>'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="55097">{"context": retriever_multi_vector | RunnableLambda(split_image_text_types),
    "question": RunnablePassthrough()}</st>`<st c="55209">: 这与其他我们过去看到的检索组件类似，提供了一个包含两个键的字典：</st>
    `<st c="55320">"context"</st>` <st c="55329">和</st> `<st c="55334">"question"</st>`<st
    c="55344">。键“</st> `<st c="55350">"context"</st>` <st c="55359">”被分配给</st> `<st
    c="55390">retriever_multi_vector | RunnableLambda(split_image_text_types)</st>`<st
    c="55453">的结果。函数</st> `<st c="55459">retriever_multi_vector</st>` <st c="55481">根据问题检索相关文档，然后这些结果通过</st>
    `<st c="55585">RunnableLambda(split_image_text_types)</st>`<st c="55623">传递，这是一个围绕</st>
    `<st c="55655">split_image_text_types</st>` <st c="55677">函数的包装器。</st> <st c="55688">正如我们之前讨论的，函数</st>
    `<st c="55720">split_image_text_types</st>` <st c="55742">将检索到的文档分割成</st> `<st
    c="55788">base64</st>`<st c="55794">编码的图像和文本。</st> <st c="55822">键“</st> `<st
    c="55826">"question"</st>` <st c="55836">”被分配给</st> `<st c="55853">RunnablePassthrough</st>`<st
    c="55872">，它简单地传递问题而不进行</st> `<st c="55923">任何修改</st>`。</st>'
- en: '`<st c="55940">RunnableLambda(img_prompt_func)</st>`<st c="55972">: The output
    of the previous component (the split images and texts along with the question)
    is passed through</st> `<st c="56083">RunnableLambda(img_prompt_func)</st>`<st
    c="56114">. As we discussed previously, the</st> `<st c="56148">img_prompt_func</st>`
    <st c="56163">function generates a prompt message for image analysis based on
    the retrieved context and the question, so this is what formats the prompt we
    will pass into the next</st> <st c="56330">step:</st> `<st c="56336">llm</st>`<st
    c="56339">.</st>'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="55940">RunnableLambda(img_prompt_func)</st>`<st c="55972">: 上一个组件（分割图像和文本以及问题）的输出通过</st>
    `<st c="56083">RunnableLambda(img_prompt_func)</st>`<st c="56114">传递。正如我们之前讨论的，</st>
    `<st c="56148">img_prompt_func</st>` <st c="56163">函数根据检索到的上下文和问题生成一个用于图像分析的提示信息，因此这就是我们将格式化并传递到下一个</st>
    `<st c="56330">步骤</st>` `<st c="56336">llm</st>`<st c="56339">的内容。</st>'
- en: '`<st c="56340">llm</st>`<st c="56344">: The generated prompt</st> <st c="56367">message,
    which includes an image in</st> `<st c="56404">base64</st>` <st c="56410">format,
    is passed to our LLM for processing.</st> <st c="56456">The LLM generates a response
    based on the multi-modal prompt message and then passes it to the next step: the</st>
    <st c="56566">output parser.</st>'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="56340">llm</st>`<st c="56344">：生成的提示</st> <st c="56367">信息，其中包含一个以</st>
    `<st c="56404">base64</st>` <st c="56410">格式编码的图像，传递给我们的LLM进行处理。</st> <st c="56456">LLM根据多模态提示信息生成响应，然后将其传递到下一步：输出解析器。</st>'
- en: '`<st c="56580">str_output_parser</st>`<st c="56598">: We’ve seen output parsers
    throughout our code labs, and this is the same reliable</st> `<st c="56683">StrOutputParser</st>`
    <st c="56698">class that has served us well in the past, parsing the generated
    response as</st> <st c="56776">a string.</st>'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="56580">str_output_parser</st>`<st c="56598">：我们在代码实验室中看到了输出解析器，这是我们在过去表现良好的相同可靠的</st>
    `<st c="56683">StrOutputParser</st>` <st c="56698">类，它将生成的响应解析为</st> <st c="56776">一个字符串。</st>'
- en: <st c="56785">Overall, this chain represents an MM-RAG pipeline that retrieves
    relevant documents, splits them into images and texts, generates a prompt message,
    processes it with an LLM, and parses the output as</st> <st c="56985">a string.</st>
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56785">总的来说，这个链代表了一个MM-RAG管道，它检索相关文档，将它们分成图像和文本，生成一个提示信息，用LLM处理它，并将输出解析为</st>
    <st c="56985">一个字符串。</st>
- en: <st c="56994">We invoke this chain and implement full</st> <st c="57035">multi-modal
    retrieval:</st>
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56994">我们调用此链并实现完整的</st> <st c="57035">多模态检索：</st>
- en: '[PRE37]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: <st c="57160">Note that we are using a different</st> `<st c="57196">user_query</st>`
    <st c="57206">string than we have used in the past.</st> <st c="57245">We changed
    it to something that would be relevant to the images that we</st> <st c="57317">had
    available.</st>
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57160">请注意，我们使用了一个与过去不同的</st> `<st c="57196">user_query</st>` <st c="57206">字符串。</st>
    <st c="57245">我们将其更改为与我们可用的图像相关的内容。</st>
- en: <st c="57331">Here is the output from our MM-RAG pipeline based on this</st>
    <st c="57390">user query:</st>
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57331">以下是我们的MM-RAG管道基于此</st> <st c="57390">用户查询</st>的输出：
- en: '[PRE38]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: <st c="57955">The response aligns with the</st> `<st c="57985">user_query</st>`
    <st c="57995">string, as well as the prompt we used to explain to the LLM how
    to describe the image that it “sees.” Since we only have three images, it is pretty
    easy to find</st> <st c="58157">which image this is talking about, image</st>
    *<st c="58198">#2</st>*<st c="58200">, which we can retrieve</st> <st c="58224">with
    this:</st>
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57955">响应与</st> `<st c="57985">user_query</st>` <st c="57995">字符串一致，以及我们用来向LLM解释如何描述它“看到”的图像的提示。由于我们只有三张图像，因此很容易找到</st>
    <st c="58157">正在讨论的这张图像，图像</st> *<st c="58198">#2</st>*<st c="58200">，我们可以用这个来检索</st>
    <st c="58224">：</st>
- en: '[PRE39]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: <st c="58394">The function here is the helper function we promised to help you
    see the image.</st> <st c="58475">It takes a</st> `<st c="58486">base64</st>`<st
    c="58492">-encoded image,</st> `<st c="58509">img_base64</st>`<st c="58519">,
    as input and displays it using HTML.</st> <st c="58558">It does this by creating
    an</st> `<st c="58586">image_html</st>` <st c="58596">HTML string that represents
    an</st> `<st c="58628"><img></st>` <st c="58633">tag with the</st> `<st c="58647">src</st>`
    <st c="58650">attribute set to the</st> `<st c="58672">base64</st>`<st c="58678">-encoded
    image URL.</st> <st c="58699">It uses the</st> `<st c="58711">display()</st>`
    <st c="58720">function from IPython to render the HTML string and display the
    image.</st> <st c="58792">Run this in your code lab, and you will see the image
    that was extracted from the PDF to provide the basis for the</st> <st c="58907">MM-RAG
    response!</st>
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="58394">这里的函数是我们承诺的辅助函数，帮助你查看图像。</st> <st c="58475">它接受一个</st> `<st c="58486">base64</st>`<st
    c="58492">编码的图像，</st> `<st c="58509">img_base64</st>`<st c="58519">，作为输入，并使用HTML显示它。</st>
    <st c="58558">它是通过创建一个</st> `<st c="58586">image_html</st>` <st c="58596">HTML字符串来表示一个</st>
    `<st c="58628"><img></st>` <st c="58633">标签，其</st> `<st c="58647">src</st>` <st
    c="58650">属性设置为</st> `<st c="58672">base64</st>`<st c="58678">编码的图像URL。</st> <st
    c="58699">它使用</st> `<st c="58711">display()</st>` <st c="58720">函数从IPython渲染HTML字符串并显示图像。</st>
    <st c="58792">在你的代码实验室中运行此代码，你将看到从PDF中提取的图像，为MM-RAG响应提供基础！</st>
- en: <st c="58923">And just for reference, here is the image summary that was generated
    for this image, using the same index from the</st> `<st c="59039">img_base64_list</st>`
    <st c="59054">list since</st> <st c="59066">they match:</st>
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="58923">仅作参考，以下是为此图像生成的图像摘要，使用与</st> `<st c="59039">img_base64_list</st>`
    <st c="59054">列表相同的索引，因为它们匹配：</st>
- en: '[PRE40]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: <st c="59096">The summary should look something</st> <st c="59131">like this:</st>
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59096">摘要应该看起来像这样：</st> <st c="59131">：</st>
- en: '[PRE41]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: <st c="59244">Given the output description from the MM-RAG chain, which is much
    more robust and descriptive about this image, you can see that the LLM can actually
    “see” this image and tell you about it.</st> <st c="59435">You are</st> <st c="59443">officially
    multi-modal!</st>
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59244">鉴于来自 MM-RAG 链的输出描述，它对此图像的描述更加稳健和详细，您可以看到 LLM 实际上“看到”了这张图像，并告诉您关于它的情况。</st>
    <st c="59435">您现在是</st> <st c="59443">官方的多模态了！</st>
- en: <st c="59466">We selected the three code labs in this chapter because we felt
    they represented the broadest representation of potential improvements across
    most RAG applications.</st> <st c="59632">But these are just the tip of the iceberg
    in terms of techniques that may be applicable to your specific RAG needs.</st>
    <st c="59748">In the</st> <st c="59754">next section, we provide what we would
    consider just a start to the many techniques you should consider incorporating
    into your</st> <st c="59883">RAG pipeline.</st>
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59466">我们选择本章中的三个代码实验室，因为我们认为它们代表了大多数 RAG 应用中潜在改进的最广泛代表性。</st> <st c="59632">但这些都是您特定
    RAG 需要可能适用的技术冰山一角。</st> <st c="59748">在</st> <st c="59754">下一节中，我们提供了我们认为只是许多您应该考虑将其纳入您的</st>
    <st c="59883">RAG 管道的技术之始。</st>
- en: <st c="59896">Other advanced RAG techniques to explore</st>
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="59896">其他要探索的先进 RAG 技术</st>
- en: <st c="59937">As with just about everything else we have discussed with RAG
    and GenAI, the options available for advanced techniques to apply to your RAG
    application are too numerous to list or even keep track of.</st> <st c="60138">We
    have selected techniques that are focused on aspects of RAG specifically, categorizing
    them based on the areas of your RAG application they will likely have the</st>
    <st c="60302">most impact.</st>
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59937">正如我们之前与 RAG 和 GenAI 讨论的大多数其他事情一样，可用于应用于您的 RAG 应用的高级技术的选项太多，无法一一列出或跟踪。</st>
    <st c="60138">我们已选择专注于 RAG 特定方面的技术，根据它们将在您的 RAG 应用中产生最大影响的领域对它们进行分类。</st>
- en: <st c="60314">Let’s walk through them in the same order that our RAG pipelines
    operate, starting</st> <st c="60398">with indexing.</st>
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="60314">让我们按照我们的 RAG 管道操作相同的顺序来探讨它们，从</st> <st c="60398">索引开始。</st>
- en: <st c="60412">Indexing improvements</st>
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="60412">索引改进</st>
- en: <st c="60434">These are advanced RAG techniques that focus on the indexing stage
    of the</st> <st c="60509">RAG pipeline:</st>
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="60434">这些是专注于 RAG 管道索引阶段的先进 RAG 技术：</st>
- en: '**<st c="60522">Deep chunking</st>**<st c="60536">: The quality of retrieved
    results often depends on the way your data is chunked before it’s stored in the</st>
    <st c="60644">retrieval system itself.</st> <st c="60669">With deep chunking,
    you use DL models, including transformers, for optimal and</st> <st c="60748">intelligent
    chunking.</st>'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="60522">深度分块</st>**<st c="60536">：检索结果的质量通常取决于您在检索系统本身存储之前如何分块您的数据。</st>
    <st c="60644">使用深度分块，您使用深度学习模型，包括变压器，进行最优和</st> <st c="60748">智能分块。</st>'
- en: '**<st c="60769">Training and utilizing embedding adapters</st>**<st c="60811">:
    Embedding adapters are lightweight modules trained to adapt pre-existing language
    model embeddings for specific tasks or domains without the need for extensive
    retraining.</st> <st c="60986">When applied to RAG systems, these adapters can
    tailor the model’s understanding and generation capabilities to better align with
    the nuances of the prompt, facilitating more accurate and</st> <st c="61174">relevant
    retrievals.</st>'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="60769">训练和使用嵌入适配器</st>**<st c="60811">：嵌入适配器是轻量级模块，经过训练以适应预存在的语言模型嵌入，用于特定任务或领域，而无需大量重新训练。</st>
    <st c="60986">当应用于 RAG 系统时，这些适配器可以调整模型的理解和生成能力，以更好地与提示的细微差别相匹配，从而促进更准确和</st> <st
    c="61174">相关的检索。</st>'
- en: '**<st c="61194">Multi-representation indexing</st>**<st c="61224">: Proposition
    indexing uses an LLM to produce document summaries (propositions) that are optimized</st>
    <st c="61324">for retrieval.</st>'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="61194">多表示索引</st>**<st c="61224">：命题索引使用 LLM 生成针对检索优化的文档摘要（命题）。</st>'
- en: '**<st c="61338">Recursive Abstractive Processing for Tree Organized Retrieval
    (RAPTOR)</st>**<st c="61409">: RAG systems need to handle “lower-level” questions
    that reference specific facts found in a single</st> <st c="61510">document or
    “higher-level” questions that distill ideas that span many documents.</st> <st
    c="61593">Handling both types of questions can be a challenge with typical kNN
    retrieval where only a finite number of doc chunks are retrieved.</st> <st c="61728">RAPTOR
    addresses this by creating document summaries that capture higher-level concepts.</st>
    <st c="61817">It embeds and clusters documents and then summarizes each cluster.</st>
    <st c="61884">It does this recursively, producing a tree of summaries with increasingly
    high-level concepts.</st> <st c="61979">The summaries and starting docs are indexed
    together, giving coverage across</st> <st c="62056">user questions.</st>'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="61338">用于树状检索的递归抽象处理（RAPTOR）</st>**<st c="61409">：RAG系统需要处理“低级”问题，这些问题引用单个</st>
    <st c="61510">文档中找到的特定事实，或者“高级”问题，这些问题提炼了跨越许多文档的思想。</st> <st c="61593">在典型的kNN检索中，只能检索有限数量的文档块，处理这两种类型的问题可能是一个挑战。</st>
    <st c="61728">RAPTOR通过创建文档摘要来解决这个问题，这些摘要捕获了高级概念。</st> <st c="61817">它嵌入并聚类文档，然后总结每个聚类。</st>
    <st c="61884">它通过递归地这样做，产生了一个具有越来越高级概念的摘要树。</st> <st c="61979">摘要和起始文档一起索引，覆盖了</st>
    <st c="62056">用户问题。</st>'
- en: '**<st c="62071">Contextualized Late Interaction over BERT (ColBERT)</st>**<st
    c="62123">: Embedding models compress text into</st> <st c="62162">fixed-length
    (vector) representations that capture the semantic content of the</st> <st c="62240">document.</st>
    <st c="62251">This compression is very useful for efficient search retrieval but
    puts a heavy burden on that single vector representation to capture all the semantic
    nuance and detail of the documents.</st> <st c="62439">In some cases, irrelevant
    or redundant content can dilute the semantic usefulness of the embedding.</st>
    <st c="62539">ColBERT is an approach to address this with higher granularity embeddings,
    focused on producing a more granular token-wise similarity assessment between
    the document and</st> <st c="62709">the query.</st>'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="62071">BERT上的上下文化后期交互（ColBERT）</st>**<st c="62123">：嵌入模型将文本压缩成</st>
    <st c="62162">固定长度（向量）表示，这些表示捕获了文档的语义内容。</st> <st c="62240">这种压缩对于高效的搜索检索非常有用，但给单个向量表示带来了巨大的负担，以捕捉文档的所有语义细微差别和细节。</st>
    <st c="62439">在某些情况下，无关或冗余的内容可能会稀释嵌入的语义有用性。</st> <st c="62539">ColBERT通过使用更高粒度的嵌入来解决这个问题，专注于在文档和</st>
    <st c="62709">查询之间产生更细粒度的标记相似度评估。</st>'
- en: <st c="62719">Retrieval</st>
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="62719">检索</st>
- en: <st c="62729">Retrieval is our largest category of</st> <st c="62767">advanced
    RAG techniques, reflecting the importance of retrieval in the RAG process.</st>
    <st c="62851">Here are some approaches we recommend you consider for your</st>
    <st c="62911">RAG application:</st>
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62729">检索是我们最大的高级RAG技术类别，反映了检索在RAG过程中的重要性。</st> <st c="62767">以下是我们在您的</st>
    <st c="62851">RAG应用中推荐您考虑的一些方法：</st> <st c="62911">：</st>
- en: '**<st c="62927">Hypothetical Document Embeddings (HyDE)</st>**<st c="62967">:
    HyDE is a retrieval method used to enhance retrieval by</st> <st c="63025">generating
    a hypothetical document for an incoming query.</st> <st c="63084">These documents,
    drawn from the LLM’s knowledge, are</st> <st c="63137">embedded and used to retrieve
    documents from an index.</st> <st c="63192">The idea is that hypothetical documents
    may be better aligned with index documents than the raw</st> <st c="63288">user
    question.</st>'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="62927">假设文档嵌入（HyDE）</st>**<st c="62967">：HyDE是一种检索方法，通过</st> <st c="63025">为传入的查询生成一个假设文档来增强检索。</st>
    <st c="63084">这些文档来自LLM的知识库，被嵌入并用于从索引中检索文档。</st> <st c="63137">其理念是假设文档可能比原始的</st>
    <st c="63288">用户问题与索引文档更匹配。</st>'
- en: '**<st c="63302">Sentence-window retrieval</st>**<st c="63328">: With sentence-window
    retrieval, you perform retrieval based on smaller sentences to</st> <st c="63414">better
    match the relevant context and then synthesize based on an expanded context window
    around</st> <st c="63512">the sentence.</st>'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="63302">句子窗口检索</st>**<st c="63328">：在句子窗口检索中，您基于更小的句子进行检索，以</st> <st
    c="63414">更好地匹配相关上下文，然后基于句子周围的扩展上下文窗口进行综合。</st>'
- en: '**<st c="63525">Auto-merging retrieval</st>**<st c="63548">: Auto-merging retrieval
    tackles the issue you see with naïve RAG where having smaller chunks</st> <st
    c="63642">can lead to fragmentation of our data.</st> <st c="63682">It uses an
    auto-merging heuristic to merge smaller chunks into a bigger parent chunk to help
    ensure more</st> <st c="63787">coherent context.</st>'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="63525">自动合并检索</st>**<st c="63548">：自动合并检索解决了你在简单的RAG中看到的问题，即拥有较小的块可能会导致我们的数据碎片化。<st
    c="63642">它使用自动合并启发式方法将较小的块合并到一个更大的父块中，以确保更</st> <st c="63787">连贯的上下文。</st>'
- en: '**<st c="63804">Multi-query rewriting</st>**<st c="63826">: Multi-query is</st>
    <st c="63844">an approach that rewrites a question from</st> <st c="63886">multiple
    perspectives, performs retrieval on each rewritten question, and takes the unique
    union of</st> <st c="63986">all documents.</st>'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="63804">多查询重写</st>**<st c="63826">：多查询是一种从</st> <st c="63844">多个角度重写问题的方法，对每个重写的问题进行检索，并取所有文档的唯一并集。</st>'
- en: '**<st c="64000">Query translation step-back</st>**<st c="64028">: Step-back
    prompting is an approach to improve retrieval that builds on CoT</st> <st c="64105">reasoning.</st>
    <st c="64117">From a question, it generates a step-back (higher level, more abstract)
    question that can serve as a precondition to correctly answering the original
    question.</st> <st c="64277">This is especially useful in cases where background
    knowledge or more fundamental understanding is helpful in answering a</st> <st
    c="64399">specific question.</st>'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="64000">查询翻译回溯步骤</st>**<st c="64028">：回溯提示是一种基于CoT</st> <st c="64105">推理来改进检索的方法。从一个问题出发，它生成一个回溯（更高层次、更抽象）的问题，该问题可以作为正确回答原始问题的先决条件。<st
    c="64277">这在需要背景知识或更基本的理解来回答特定问题时特别有用。</st>'
- en: '**<st c="64417">Query structuring</st>**<st c="64435">: Query structuring is
    the process of text-to-DSL, where DSL is a domain-specific language</st> <st c="64526">required
    to interact with a given database.</st> <st c="64571">This converts user questions
    into</st> <st c="64605">structured queries.</st>'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="64417">查询结构化</st>**<st c="64435">：查询结构化是将文本转换为DSL的过程，其中DSL是用于与特定数据库交互的领域特定语言。<st
    c="64526">这将用户问题转换为</st> <st c="64605">结构化查询。</st>'
- en: <st c="64624">Post-retrieval/generation</st>
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="64624">检索/生成后</st>
- en: <st c="64650">These are advanced RAG</st> <st c="64673">techniques that focus
    on the generation stage of the</st> <st c="64727">RAG pipeline:</st>
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="64650">这些是高级RAG</st> <st c="64673">技术，专注于RAG管道的生成阶段：</st>
- en: '**<st c="64740">Cross-encoder re-ranking</st>**<st c="64765">: We have already
    seen the improvement that re-ranking can provide in our hybrid RAG code lab, which
    is applied to the retrieved results before they are sent to</st> <st c="64927">the
    LLM.</st> <st c="64936">Cross-encoder re-ranking takes even more advantage of
    this technique by using a more computationally intensive model to reassess and
    reorder the retrieved documents based on their relevance to the original prompt.</st>
    <st c="65150">This fine-grained analysis ensures that the most pertinent information
    is prioritized for the generation phase, enhancing the overall</st> <st c="65284">output
    quality.</st>'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="64740">交叉编码重新排名</st>**<st c="64765">：我们已经在我们的混合RAG代码实验室中看到了重新排名可以带来的改进，它应用于检索结果在发送到</st>
    <st c="64927">LLM之前。交叉编码重新排名通过使用一个计算量更大的模型来重新评估和重新排序检索到的文档，基于它们与原始提示的相关性，从而进一步利用这一技术。<st
    c="64936">这种细粒度分析确保了最相关的信息在生成阶段得到优先考虑，从而提高了整体</st> <st c="65284">输出质量。</st>'
- en: '**<st c="65299">RAG-fusion query rewriting</st>**<st c="65326">: RAG-fusion
    is</st> <st c="65343">an approach that rewrites a question from multiple perspectives,
    performs retrieval on each rewritten question, and performs reciprocal rank fusion
    on the results from each retrieval, giving a</st> <st c="65536">consolidated ranking.</st>'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="65299">RAG-fusion查询重写</st>**<st c="65326">：RAG-fusion是一种从多个角度重写问题的方法，对每个重写的问题进行检索，并对每个检索的结果进行相互排名融合，从而得到一个</st>
    <st c="65536">综合排名。</st>'
- en: <st c="65557">Entire RAG pipeline coverage</st>
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="65557">整个RAG管道覆盖</st>
- en: <st c="65586">These advanced RAG techniques focus on the overall RAG pipeline,
    rather than one particular stage</st> <st c="65685">of it:</st>
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="65586">这些高级RAG技术专注于整个RAG管道，而不仅仅是其中的一个阶段：</st>
- en: '**<st c="65691">Self-reflective RAG</st>**<st c="65711">: The self-reflective</st>
    <st c="65734">RAG with LangGraph technique improves on naïve RAG models by incorporating
    a self-reflective mechanism coupled with a linguistic graph structure from LangGraph.</st>
    <st c="65895">In this approach, LangGraph helps in understanding the context and
    semantics at a deeper level, allowing the RAG system to refine its</st> <st c="66028">responses
    based on a more nuanced understanding of the content and its interconnections.</st>
    <st c="66118">This can be particularly</st> <st c="66142">useful in applications
    such as content creation, question-answering, and conversational agents, as it
    leads to more accurate, relevant, and context-aware outputs, significantly enhancing
    the quality of</st> <st c="66345">generated text.</st>'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="65691">自反式RAG</st>**<st c="65711">：结合LangGraph技术的自反式RAG通过引入与LangGraph的语用图结构相结合的自反机制，改进了简单的RAG模型。</st>
    <st c="65895">在此方法中，LangGraph有助于在更深层次上理解上下文和语义，使RAG系统能够根据对内容和其相互关系的更细致理解来优化其</st>
    <st c="66028">响应。</st> <st c="66118">这在内容创作、问答和对话代理等应用中尤其</st> <st c="66142">有用，因为它导致更准确、相关和上下文感知的输出，显著提高了生成文本的质量。</st>'
- en: '**<st c="66360">Modular RAG</st>**<st c="66372">: Modular RAG uses interchangeable
    components to provide a more flexible architecture</st> <st c="66458">that can
    adjust to your RAG development needs.</st> <st c="66506">This modularity enables
    researchers and developers to experiment with different retrieval mechanisms,
    generative models, and optimization strategies, tailoring the RAG system to specific
    needs and applications.</st> <st c="66717">As you’ve seen throughout the code
    labs in this book, LangChain provides mechanisms that support this approach well,
    where LLMs, retrievers, vector stores, and other components can be swapped out
    and switched easily in many cases.</st> <st c="66948">The goal of modular RAG
    is to move toward a more customizable, efficient, and powerful RAG system capable
    of tackling a wider range of tasks with</st> <st c="67094">greater efficacy.</st>'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="66360">模块化RAG</st>**<st c="66372">：模块化RAG使用可互换的组件来提供更灵活的架构</st> <st
    c="66458">，以满足您的RAG开发需求。</st> <st c="66506">这种模块化使得研究人员和开发者能够尝试不同的检索机制、生成模型和优化策略，根据特定需求和应用程序定制RAG系统。</st>
    <st c="66717">正如您在本书的代码实验室中所见，LangChain提供了支持这种方法的机制，在许多情况下，LLMs、检索器、向量存储和其他组件可以轻松替换和切换。</st>
    <st c="66948">模块化RAG的目标是朝着更可定制、更高效、更强大的RAG系统迈进，能够以更高的效率处理更广泛的任务。</st>'
- en: <st c="67111">With new research</st> <st c="67130">coming out every day, this
    technique list is growing rapidly.</st> <st c="67192">One great source for new
    techniques is the Arxiv.org</st> <st c="67245">website:</st> [<st c="67254">https://arxiv.org/</st>](https://arxiv.org/)<st
    c="67272">.</st>
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="67111">随着每天都有新的研究成果出现，这项技术列表正在迅速增长。</st> <st c="67130">新技术的绝佳来源是Arxiv.org网站：</st>
    [<st c="67254">https://arxiv.org/</st>](https://arxiv.org/)<st c="67272">。</st>
- en: <st c="67273">Visit this website and search for various key terms related to
    your RAG application, including</st> *<st c="67369">RAG</st>*<st c="67372">,</st>
    *<st c="67374">retrieval augmented generation</st>*<st c="67404">,</st> *<st c="67406">vector
    search</st>*<st c="67419">, and other</st> <st c="67431">related terms.</st>
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="67273">访问此网站并搜索与您的RAG应用相关的各种关键词，包括</st> *<st c="67369">RAG</st>**<st
    c="67372">、**<st c="67374">检索增强生成</st>**<st c="67404">、**<st c="67406">向量搜索</st>**<st
    c="67419">以及其他</st> <st c="67431">相关术语。</st>
- en: <st c="67445">Summary</st>
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="67445">总结</st>
- en: <st c="67453">In this final chapter, we explored several advanced techniques
    to improve RAG applications, including query expansion, query decomposition, and
    MM-RAG.</st> <st c="67606">These techniques enhance retrieval and generation by
    augmenting queries, breaking down questions into subproblems, and incorporating
    multiple data modalities.</st> <st c="67765">We also discussed a range of other
    advanced RAG techniques covering indexing, retrieval, generation, and the entire</st>
    <st c="67881">RAG pipeline.</st>
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="67453">在本章的最后一部分，我们探讨了多种提高RAG应用的高级技术，包括查询扩展、查询分解和MM-RAG。</st> <st c="67606">这些技术通过增强查询、将问题分解为子问题以及结合多种数据模态来提升检索和生成能力。</st>
    <st c="67765">我们还讨论了一系列其他高级RAG技术，涵盖了索引、检索、生成以及整个</st> <st c="67881">RAG流程。</st>
- en: <st c="67894">It has been a pleasure to go on this RAG journey with you, exploring
    the world of RAG and its vast potential.</st> <st c="68005">As we conclude this
    book, I hope you feel well equipped with the knowledge and practical experience
    to tackle your own RAG projects.</st> <st c="68138">Good luck in your future RAG
    endeavors – I’m confident you’ll create remarkable applications that push the
    boundaries of what’s possible with this exciting</st> <st c="68294">new technology!</st>
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="67894">与您一同踏上RAG之旅，探索RAG的世界及其巨大的潜力，这是一件令人愉快的事情。</st> <st c="68005">随着我们结束这本书的撰写，我希望您已经具备了足够的知识和实践经验来应对自己的RAG项目。</st>
    <st c="68138">祝您在未来的RAG探索中好运——我坚信您将创造出令人瞩目的应用，推动这一激动人心的</st> <st c="68294">新技术所能达到的边界！</st>
