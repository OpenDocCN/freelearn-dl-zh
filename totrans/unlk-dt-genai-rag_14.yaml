- en: <st c="0">14</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3">Advanced RAG-Related Techniques for Improving Results</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="57">In this final chapter, we explore several advanced techniques to
    improve</st> **<st c="131">retrieval-augmented generation</st>** <st c="161">(</st>**<st
    c="163">RAG</st>**<st c="166">) applications.</st> <st c="183">These techniques
    go beyond the fundamental RAG approaches to tackle</st> <st c="251">more complex
    challenges and achieve even better results.</st> <st c="308">Our starting point
    will be techniques we have already used in previous chapters.</st> <st c="389">We
    will build off those techniques, learning where they fall short so that we can
    introduce new techniques that can make up the difference and take your RAG efforts</st>
    <st c="554">even further.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="567">Throughout this chapter, you will gain hands-on experience implementing
    these advanced techniques through a series of code labs.</st> <st c="697">Our
    topics will include</st> <st c="721">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="735">Naïve RAG and</st> <st c="750">its limitations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="765">Hybrid RAG/multi-vector RAG for</st> <st c="798">improved retrieval</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="816">Re-ranking in</st> <st c="831">hybrid RAG</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="841">Code lab 14.1 –</st> <st c="858">Query expansion</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="873">Code lab 14.2 –</st> <st c="890">Query decomposition</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="909">Code lab</st> <st c="918">14.3 –</st> **<st c="926">Multi-modal</st>**
    **<st c="938">RAG</st>** <st c="941">(</st>**<st c="943">MM-RAG</st>**<st c="949">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="951">Other advanced RAG techniques</st> <st c="981">to explore</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="991">These techniques enhance retrieval and generation by augmenting
    queries, breaking down questions into subproblems, and incorporating multiple
    data modalities.</st> <st c="1151">We also discuss a range of other advanced RAG
    techniques covering indexing, retrieval, generation, and the entire RAG pipeline.</st>
    <st c="1279">We start with a discussion of naïve RAG, the primary approach for
    RAG that we reviewed back in</st> [*<st c="1374">Chapter 2</st>*](B22475_02.xhtml#_idTextAnchor035)
    <st c="1383">and that you should feel very familiar with</st> <st c="1428">by
    now.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1435">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1458">The code for this chapter is placed in the following GitHub</st>
    <st c="1519">repository:</st> [<st c="1531">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_14</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_14
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1628">Naïve RAG and its limitations</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1658">So far, we have worked with three types of RAG approaches,</st>
    **<st c="1718">naïve RAG</st>**<st c="1727">,</st> **<st c="1729">hybrid RAG</st>**<st
    c="1739">, and</st> **<st c="1745">re-ranking</st>**<st c="1755">. Initially,
    we</st> <st c="1771">were working with what is called naïve RAG.</st> <st c="1815">This
    is the basic RAG approach that we had in our starter code in</st> [*<st c="1881">Chapter
    2</st>*](B22475_02.xhtml#_idTextAnchor035) <st c="1890">and multiple code labs
    after.</st> <st c="1921">Naive RAG models, the initial</st> <st c="1951">iterations
    of RAG technology, provide a foundational framework for integrating retrieval
    mechanisms with generative models, albeit with limitations in flexibility</st>
    <st c="2114">and scalability.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="2130">Naïve RAG retrieves numerous fragmented context chunks, the chunks
    of text that we vectorize, to put into the LLM context window.</st> <st c="2261">If
    you do not use large enough chunks of text, your context will</st> <st c="2326">experience
    higher levels of fragmentation.</st> <st c="2369">This fragmentation leads to
    decreased understanding and capture of the context and semantics within your chunks,
    reducing the effectiveness of the retrieval mechanism of your RAG application.</st>
    <st c="2561">In the typical naïve RAG application, you are using some type of
    semantics search and are therefore exposed to these limitations by only using
    that type of search.</st> <st c="2725">As a result, we introduced a more advanced
    type of retrieval: the</st> <st c="2791">hybrid search.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2805">Hybrid RAG/multi-vector RAG for improved retrieval</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2856">Hybrid RAG expands on the concept of naïve RAG by utilizing multiple
    vectors for the retrieval process, as opposed to relying on a single vector representation
    of queries and documents.</st> <st c="3043">We explored</st> <st c="3054">hybrid
    RAG in depth and in code in</st> [*<st c="3090">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="3099">, not only utilizing the mechanism recommended within LangChain but by
    re-creating that</st> <st c="3187">mechanism ourselves so that we could see its
    inner workings.</st> <st c="3248">Also called multi-vector RAG, hybrid RAG can
    involve not just semantic and keyword search, as we saw in our code lab, but the
    mix of any different vector retrieval techniques that make sense for your</st>
    <st c="3448">RAG application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3464">Our hybrid RAG code lab introduced a keyword search, which expanded
    our search capabilities, leading to more effective retrieval, particularly when
    dealing with content that has a weaker context (such as names, codes, internal
    acronyms, and similar text).</st> <st c="3721">This multi-vector approach allows
    us to consider broader facets of the query and the content in the database.</st>
    <st c="3831">This, in turn, can achieve higher relevance and accuracy in the information
    it retrieves to support the generation process.</st> <st c="3955">This results
    in generated content that is not only more relevant</st> <st c="4020">and informative
    but also more aligned with the nuances of the input query.</st> <st c="4095">Multi-vector
    RAG is particularly useful in applications requiring a high degree of precision
    and nuance in generated content, such as technical writing, academic</st> <st
    c="4256">research assistance, internal company documentation with significant
    amounts of internal code and entity references, and complex question-answering
    systems.</st> <st c="4414">But multi-vector RAG is not the only advanced technique
    we explored in</st> [*<st c="4485">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="4494">; we also</st> <st c="4505">applied</st> **<st c="4513">re-ranking</st>**<st
    c="4523">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4524">Re-ranking in hybrid RAG</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="4549">In</st> [*<st c="4553">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="4562">, in addition to our hybrid RAG approach, we also introduced a form of
    re-ranking, another common</st> <st c="4659">advanced RAG technique.</st> <st
    c="4684">After the semantic search and keyword searches complete their retrieval,
    we re-rank the results based on the rankings across both sets depending on if
    they appear in both and where they</st> <st c="4870">ranked</st> <st c="4876">initially.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4887">So, you have already stepped through three RAG techniques, including
    two advanced techniques!</st> <st c="4982">But this</st> <st c="4991">chapter
    is focused on bringing you three more advanced approaches:</st> **<st c="5058">query
    expansion</st>**<st c="5073">,</st> **<st c="5075">query decomposition</st>**<st
    c="5094">, and</st> **<st c="5100">MM-RAG</st>**<st c="5106">. We will also provide
    you a list</st> <st c="5140">of many more approaches you can explore, but</st>
    <st c="5185">we sorted through and picked out these three advanced RAG techniques
    because of their application in a wide variety of</st> <st c="5304">RAG applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5321">In our first code lab in this chapter, we will talk about</st>
    <st c="5380">query expansion.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5396">Code lab 14.1 – Query expansion</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="5428">The code for this lab can be found in the</st> `<st c="5471">CHAPTER14-1_QUERY_EXPANSION.ipynb</st>`
    <st c="5504">file in the</st> `<st c="5517">CHAPTER14</st>` <st c="5526">directory
    of the</st> <st c="5544">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5562">Many techniques for enhancing RAG focus on improving one area,
    such as retrieval or generation, but query expansion has the potential to improve
    both.</st> <st c="5714">We have already talked about the concept of</st> <st c="5758">expansion
    in</st> [*<st c="5771">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st
    c="5781">, but that was focused on the LLM output.</st> <st c="5823">Here, we
    focus the concept on the input to the model, augmenting the original prompt with
    additional keywords or phrases.</st> <st c="5945">This approach can improve the
    retrieval model’s understanding as you add more context to the user query that
    is used for retrieval, increasing the chances of fetching relevant documents.</st>
    <st c="6132">With an improved retrieval, you are already helping to improve the
    generation, giving it better context to work with, but this approach also has
    the potential to produce a more effective query, which in turn also helps the
    LLM deliver an</st> <st c="6370">improved response.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6388">Typically, the way query expansion with answers works is that you
    take the user query and immediately send it to the LLM with a prompt focused on
    getting an initial answer to the question, even though you haven’t shown it any
    of the typical contexts you normally show it in RAG applications.</st> <st c="6681">From
    an LLM standpoint, these types of changes can help broaden the search scope without
    losing focus on the</st> <st c="6790">original intent.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6806">Start in a new cell above the cell that creates the</st> `<st c="6859">rag_chain_from_docs</st>`
    <st c="6878">chain.</st> <st c="6886">We are going to introduce a number of prompt
    templates to</st> <st c="6944">accomplish this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="7071">Let’s review each of these prompt templates and</st> <st c="7120">their
    uses:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="7131">ChatPromptTemplate</st>` <st c="7150">class: This provides a
    template for creating chat-based prompts, which we can use to combine our other
    prompt templates into a more</st> <st c="7283">chat-based approach.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="7303">HumanMessagePromptTemplate</st>` <st c="7330">class: This provides
    a prompt template for creating human messages in the chat prompt.</st> <st c="7418">The</st>
    `<st c="7422">HumanMessage</st>` <st c="7434">object represents a message sent
    by a human user in a conversation with the language model.</st> <st c="7527">We
    will hydrate this prompt with our</st> `<st c="7564">user_query</st>` <st c="7574">string,
    which comes from the</st> *<st c="7604">human</st>* <st c="7609">in</st> <st c="7613">this
    scenario!</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="7627">SystemMessagePromptTemplate</st>` <st c="7655">class: The</st>
    *<st c="7667">system</st>* <st c="7673">also gets a prompt, which for a chat-based
    LLM has a different significance compared to human-generated prompts.</st> <st
    c="7787">This provides us with a prompt template for creating these system messages
    in the</st> <st c="7869">chat prompt.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="7881">Next, we want to create a</st> <st c="7908">function that will
    handle the query expansion for use, utilizing the different prompt templates we
    just discussed.</st> <st c="8023">This is the system message prompt that we will
    use, which you will want to customize to whatever area of focus your RAG system
    targets – environmental reports in</st> <st c="8185">this case:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="8195">"You are a helpful expert environmental research assistant.</st>
    <st c="8256">Provide an example answer to the given question, that might be found
    in a document like an annual</st>` `<st c="8354">environmental report."</st>`'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8376">This will be the first step in the function</st> <st c="8421">we
    create:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="9025">Here, you see us utilizing all three types of prompt templates
    to formulate the overall set of messages we send to the LLM.</st> <st c="9150">Ultimately,
    this results in a response from the LLM, where it does its best to answer</st>
    <st c="9236">our question.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9249">Let’s provide some code that calls this function so that we can
    talk about the output, representing the</st> *<st c="9354">expansion</st>* <st
    c="9363">in</st> <st c="9367">query expansion:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="9585">Here, we are calling</st> <st c="9606">our user query</st> `<st
    c="9622">original_query</st>`<st c="9636">, indicating that it is our source query
    that will soon go through the expansion.</st> <st c="9718">The</st> `<st c="9722">hypothetical_answer</st>`
    <st c="9741">instance is the response string we get back from the LLM.</st> <st
    c="9800">You then concatenate the original user query with the imagined answer
    as a</st> `<st c="9875">joint_query</st>` <st c="9886">string and use that as
    the new query.</st> <st c="9925">The output will look something like this (truncated</st>
    <st c="9977">for brevity!):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="10849">It is a much longer answer.</st> <st c="10878">Our LLM really
    tried to answer it thoroughly!</st> <st c="10924">This initial answer is going
    to be a hypothetical or imagined answer to the original user query you sent it.</st>
    <st c="11033">Normally, we shy away from imagined answers, but here, we take advantage
    of them as they help us tap into the inner workings of the LLM and pull out the
    concepts that align with the</st> `<st c="11215">user_query</st>` <st c="11225">string
    you are going</st> <st c="11247">to use.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11254">At this point, we will step through the original code, but instead
    of passing it the</st> `<st c="11340">original_query</st>` <st c="11354">string
    as</st> <st c="11364">we have in the past, we will pass in the concatenated original
    answer plus an imagined answer into our previous</st> <st c="11477">RAG pipeline:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="11996">You will see that the query that is passed into our original RAG
    pipeline is the much longer</st> `<st c="12090">joint_query</st>` <st c="12101">string,
    and then we see an expanded set of results that mix in the data we provided with
    the expanded structure the LLM helped</st> <st c="12229">to add.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12236">Because the LLM returns the text in a Markdown version, we can
    use IPython to print in a nicely formatted way.</st> <st c="12348">This code prints
    out</st> <st c="12369">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: <st c="12523">Here is</st> <st c="12532">the</st> <st c="12536">output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: <st c="13859">Compare this to the results we got from the original query and
    see if you think it improved the answer!</st> <st c="13964">As you can see, you
    do get a different response for each one, and you can determine what works best
    for your</st> <st c="14073">RAG applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14090">One important aspect to acknowledge with this approach is that
    you are bringing the LLM into the retrieval stage, whereas in the past, we only
    used the LLM in the generation stage.</st> <st c="14272">When</st> <st c="14277">doing
    this though, prompt engineering now becomes a concern within the retrieval stage,
    whereas we previously only worried about it in the generation stage.</st> <st
    c="14434">However, the approach is similar to what we discussed in our prompt
    engineering chapter (</st>[*<st c="14523">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st
    c="14534">), where we talked about iterating until we have better results from</st>
    <st c="14604">our LLM.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14612">For more information on query expansion, you can read the original
    paper</st> <st c="14686">here:</st> [<st c="14692">https://arxiv.org/abs/2305.03653</st>](https://arxiv.org/abs/2305.03653
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14724">Query expansion is just one of many approaches that enhance the
    original query to help improve the RAG output.</st> <st c="14836">We have listed
    many more near the end of this chapter, but in our next code lab, we will tackle
    an approach called query decomposition that can be particularly useful in RAG
    scenarios because of its emphasis</st> <st c="15044">on question-answering.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15066">Code lab 14.2 – Query decomposition</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="15102">The code for this lab can be found in the</st> `<st c="15145">CHAPTER14-2_DECOMPOSITION.ipynb</st>`
    <st c="15176">file in the</st> `<st c="15189">CHAPTER14</st>` <st c="15198">directory
    of the</st> <st c="15216">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15234">Query decomposition is a strategy focused on improving question-answering
    within the GenAI space.</st> <st c="15333">It falls under</st> <st c="15347">the
    category of query translation, which is a set of approaches that focuses on improving
    the initial stage of the RAG pipeline, retrieval.</st> <st c="15488">With query
    decomposition, we will</st> *<st c="15522">decompose</st>* <st c="15531">or break
    down a question into smaller questions.</st> <st c="15581">These smaller questions
    can either be approached sequentially or independently, depending on your needs,
    giving more flexibility across different scenarios you might use RAG for.</st>
    <st c="15760">After each question is answered, there is a consolidation step that
    delivers a final response that often has a broader perspective than the original
    response with</st> <st c="15923">naïve RAG.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15933">There are other query translation approaches such as RAG-Fusion
    and multi-query, which are focused on sub-questions, but this focuses on decomposing
    the question.</st> <st c="16097">We will talk more about these other techniques
    near the end of</st> <st c="16160">this chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16173">In the paper that proposed this approach, written by Google researchers,
    they call it Least-to-Most, or decomposition.</st> <st c="16293">LangChain has
    documentation for this approach on its website and calls it query decomposition.</st>
    <st c="16388">So, we are in pretty good company when we talk about this</st> <st
    c="16446">particular approach!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16466">We are going to introduce a couple more concepts to help us understand
    how to implement</st> <st c="16555">query</st> <st c="16560">decomposition:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16575">The first concept is</st> **<st c="16597">chain-of-thought</st>**
    <st c="16613">(</st>**<st c="16615">CoT</st>**<st c="16618">), a prompt engineering
    strategy where we structure the</st> <st c="16674">input prompt in a way that
    mimics human reasoning, with the goal of improving language models’ performance
    on tasks requiring logic, calculation,</st> <st c="16821">and decision-making.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="16841">The second concept is</st> **<st c="16864">interleaving retrieval</st>**<st
    c="16886">, where you step back and forth between CoT-driven</st> <st c="16937">prompts
    and retrieval,</st> *<st c="16960">interleaving</st>* <st c="16972">them, with
    the goal of retrieving more relevant information for later reasoning steps, compared
    to simply passing the user query</st> <st c="17102">for retrieval.</st> <st c="17117">This
    combination is called</st> **<st c="17144">Interleave Retrieval with CoT</st>**
    <st c="17173">or</st> **<st c="17177">IR-CoT</st>**<st c="17183">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17184">Pulling this all together, you end up with an approach that breaks
    down a problem into subproblems and steps through them with a dynamic retrieval
    process.</st> <st c="17341">Doing this sequentially, after you have broken your
    original user query into sub-questions, you start with the first question, retrieve
    documents, answer that, then do retrieval for the second question, adding the
    answer to the first question to the results, and then using all of that data to
    answer question 2\.</st> <st c="17654">This goes on through all of the sub-questions
    you have until you get to the last answer, which will be your</st> <st c="17762">final
    answer.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17775">With all this explanation, you likely just want to jump into the
    code and see how it works, so let’s</st> <st c="17877">get started!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17889">We will import a couple</st> <st c="17914">new packages:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: <st c="17967">The</st> `<st c="17972">dumps</st>` <st c="17977">and</st> `<st
    c="17982">loads</st>` <st c="17987">functions imported from</st> `<st c="18012">langchain.load</st>`
    <st c="18026">are used to serialize and deserialize (respectively) a Python object
    into and out of a string representation.</st> <st c="18137">In our code, we will
    use it to convert each</st> `<st c="18181">Document</st>` <st c="18189">object
    into a string representation before deduplication, and then</st> <st c="18257">back
    again.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18268">We then jump down past the retriever definitions and add a cell
    where we are going to add our decomposition</st> <st c="18377">prompt, chain,
    and code to run it.</st> <st c="18412">Start with creating a new</st> <st c="18438">prompt
    template:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: <st c="18928">Reading through the string in this</st> `<st c="18964">PromptTemplate</st>`
    <st c="18978">object, we get the prompt version explaining to the LLM how to perform
    the decomposition we are looking for.</st> <st c="19088">It is a very transparent
    request of the LLM explaining the problem we are trying to overcome and what we
    need it to do!</st> <st c="19208">We also prompt the LLM to provide the result
    in a specific format.</st> <st c="19275">This can be risky, as LLMs can sometimes
    return unexpected results, even when prompted specifically for a certain format.</st>
    <st c="19397">In a more robust application, this is a good place to run a check
    to make sure your response is properly formatted.</st> <st c="19513">But for this
    simple example, the ChatGPT-4o-mini model we are using seems to do fine returning
    it in the</st> <st c="19618">proper format.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19632">Next, we set up the chain, using the various elements we typically
    use in our chains, but using the prompt</st> <st c="19740">to decompose:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: <st c="19854">This is a self-explanatory chain; it uses the prompt template,
    an LLM defined earlier in the code, the output parser, and then applies formatting
    for a more</st> <st c="20012">readable result.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20028">To call this chain, we</st> <st c="20052">implement the</st> <st
    c="20066">following code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: <st c="20333">This code invokes the chain we set up and provides us with the
    original query, as well as the five new queries our decomposition prompt and LLM</st>
    <st c="20478">have generated:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <st c="20925">The LLM does a superb job of taking our query and breaking it
    into a number of related questions covering the different aspects that will help
    to answer the</st> <st c="21083">original query.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21098">But this is only half of the decomposition concept!</st> <st c="21151">Next,
    we are going to run all of our questions through retrieval, giving us a much more
    robust set of retrieved context compared to what we have had in past</st> <st
    c="21308">code labs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21318">We will start by setting up a function to format the documents
    we retrieve based on all of these</st> <st c="21416">new queries:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <st c="21729">This function will provide a</st> <st c="21759">list of lists,
    representing each list of retrieved sets of documents that are themselves provided
    in a list.</st> <st c="21868">We flatten this list of lists, meaning that we just
    make it one long list.</st> <st c="21943">We then use the</st> `<st c="21959">dumps</st>`
    <st c="21964">function we imported from</st> `<st c="21991">LangChain.load</st>`
    <st c="22005">to convert each</st> `<st c="22022">Document</st>` <st c="22030">object
    to a string; we dedupe based on that string, and then return it to its former
    state as a list.</st> <st c="22133">We also print out how many documents we end
    up with before and after to see how our deduping efforts performed.</st> <st c="22245">In
    this example, after we’ve run the</st> `<st c="22282">decompose_queries_chain</st>`
    <st c="22305">chain, we drop from</st> `<st c="22326">100</st>` <st c="22329">documents</st>
    <st c="22340">to</st> `<st c="22343">67</st>`<st c="22345">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: <st c="22384">Let’s set up the chain that will run our previous decomposition
    chain, the retrieval for all of the new queries, and the final formatting with
    the function we</st> <st c="22544">just created:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: <st c="22654">This relatively short line of code accomplishes a lot!</st> <st
    c="22710">The final result is a set of</st> `<st c="22739">67</st>` <st c="22741">documents
    related to all of the queries we generated from our original query and the decomposition.</st>
    <st c="22842">Note that we have added the previous</st> `<st c="22879">decompose_queries_chain</st>`
    <st c="22902">chain to this directly, so there is no need to call that</st> <st
    c="22960">chain separately.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22977">We assign the results of this chain to a</st> `<st c="23019">docs</st>`
    <st c="23023">variable with this one line</st> <st c="23052">of code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: <st c="23115">With the invocation of this chain, we retrieve a significant number
    of documents (</st>`<st c="23198">67</st>`<st c="23201">) compared to previous</st>
    <st c="23224">methods, but we still need to run our final RAG steps with our expanded
    retrieval results.</st> <st c="23316">Most of the code remains the same after
    this, but we do replace the ensemble chain with the</st> `<st c="23408">retrieval_chain</st>`
    <st c="23423">chain we</st> <st c="23433">just built:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: <st c="23587">This incorporates our new code into our previous RAG application.</st>
    <st c="23654">Running this line will run all of the chains we just added, so there
    is no need to go back and run them separately as we did for this example.</st>
    <st c="23797">This is one large set of cohesive code that combines our previous
    efforts with this new powerful RAG technique.</st> <st c="23909">We invite you
    to compare the current results from this technique with past code lab results
    to see how the details have been filling in better and giving us broader coverage
    of the topics our</st> `<st c="24101">original_query</st>` <st c="24115">chain</st>
    <st c="24122">asks about:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: <st c="25127">Advanced techniques</st> <st c="25148">such as this offer very
    promising results depending on the goals of your</st> <st c="25221">RAG application!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25237">For more information on this approach, visit the original</st>
    <st c="25296">paper:</st> [<st c="25303">https://arxiv.org/abs/2205.10625</st>](https://arxiv.org/abs/2205.10625
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25335">For our next and last code lab of the entire book, we are going
    to go beyond the world of text and expand our RAG to other modalities, such as
    images and video with a technique</st> <st c="25513">called MM-RAG.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25527">Code lab 14.3 – MM-RAG</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="25550">The code for this lab can be found in the</st> `<st c="25593">CHAPTER14-3_MM_RAG.ipynb</st>`
    <st c="25617">file in the</st> `<st c="25630">CHAPTER14</st>` <st c="25639">directory
    of the</st> <st c="25657">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25675">This is a good example of</st> <st c="25702">when an acronym can
    really help us talk faster.</st> <st c="25750">Try to say</st> *<st c="25761">multi-modal
    retrieval augmented regeneration</st>* <st c="25805">out loud once, and you will
    likely want to use MM-RAG from now on!</st> <st c="25873">But I digress.</st>
    <st c="25888">This is a groundbreaking approach that will likely gain a lot of
    traction in the near future.</st> <st c="25982">It better represents how we as
    humans process information, so it must be amazing, right?</st> <st c="26071">Let’s
    start by revisiting the concept of using</st> <st c="26118">multiple modes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26133">Multi-modal</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<st c="26145">Up to this point, everything we have discussed has been focused
    on text: taking the text as input, retrieving text based on that input, and passing
    that retrieved text to an LLM that then generates a final text output.</st> <st
    c="26365">But what about non-text?</st> <st c="26390">As the companies building
    these LLMs have started to offer powerful multi-modal capabilities, how can we
    incorporate those multi-modal capabilities into our</st> <st c="26547">RAG applications?</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26564">Multi-modal simply means that you are handling multiple forms
    of “modes,” which include text, images, video, audio, and any other type of input.</st>
    <st c="26710">The multiple modes can be represented in the input, the output,
    or both.</st> <st c="26783">For example, you can pass in text and get an image
    back, and that is</st> <st c="26851">multi-modal.</st> <st c="26865">You can pass
    in an image and get</st> <st c="26897">text back (called captioning), and that
    is</st> <st c="26941">also multi-modal.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="26958">More advanced approaches can also include passing both a text
    prompt of</st> `<st c="27031">"turn this image into a video that goes further
    into the waterfall adding the sounds of the waterfall"</st>` <st c="27133">and
    an image of that waterfall and getting video back that takes the user right into
    the waterfall from the image with an audio track of the waterfall added as well.</st>
    <st c="27300">This would represent four different modes: text, images, video,
    and audio.</st> <st c="27375">Given that models that have these capabilities now
    exist with similar APIs to the ones we have used in this book, it is a short logical
    step to consider how they can be applied to our RAG approach, using RAG to once
    again tap into other types of content we have stored in the data coffers of our
    enterprise.</st> <st c="27683">Let’s discuss the benefits of using a</st> <st
    c="27721">multi-modal approach.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27742">Benefits of multi-modal</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="27766">This approach</st> <st c="27780">leverages the strengths of RAG
    technology in understanding and utilizing multi-modal data sources, allowing for
    the creation of more engaging, informative, and context-rich outputs.</st> <st
    c="27963">By integrating multi-modal data, these RAG systems can provide more
    nuanced and comprehensive answers, generate richer content, and engage in more
    sophisticated interactions with users.</st> <st c="28149">Applications range from
    enhanced conversational agents capable of understanding and generating multimedia
    responses to advanced content-creation tools that can produce complex, multi-modal
    documents and presentations.</st> <st c="28367">MM-RAG represents a significant
    advancement in making RAG systems more versatile and capable of understanding
    the world in a way that mirrors human sensory and</st> <st c="28527">cognitive
    experiences.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28549">Much like with the discussions we had about vectors in</st> *<st
    c="28605">Chapters 7</st>* <st c="28615">and</st> *<st c="28620">8</st>*<st c="28621">,
    it is important to recognize</st> <st c="28651">the important role vector embeddings
    play in MM-RAG</st> <st c="28704">as well.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28712">Multi-modal vector embeddings</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="28742">MM-RAG is enabled because vector embeddings can represent more
    than just text; they can represent any kind of data that you pass to it.</st>
    <st c="28879">Some data takes a little more prep work to convert it into something
    that can be vectorized, but all types of data have the potential to be vectorized
    and made available to a RAG application.</st> <st c="29071">If you remember, vectorization
    at its core is</st> <st c="29116">taking your data and turning it into a mathematical
    representation, and math and vectors are the primary language of</st> **<st c="29234">deep
    learning</st>** <st c="29247">(</st>**<st c="29249">DL</st>**<st c="29251">) models</st>
    <st c="29260">that form the foundation for all of our</st> <st c="29301">RAG applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29318">Another aspect of vectors that you may remember is the concept
    of vector space, where similar concepts are stored in closer proximity to each
    other in the vector space than dissimilar concepts.</st> <st c="29513">When you
    add multiple modes to the mix, this is still applicable, meaning a concept such
    as a seagull should be represented in a similar fashion whether it is the word</st>
    *<st c="29681">seagull</st>*<st c="29688">, an image of a seagull, a video of
    a seagull, or an audio clip of a seagull squawking.</st> <st c="29776">This multi-modal
    embedding concept of cross-modality representations of the same context is known
    as</st> **<st c="29877">modality independence</st>**<st c="29898">. This extension
    of the vector space</st> <st c="29934">concept forms the basis for how MM-RAG
    serves a similar purpose as single-mode RAG but with multiple modes of data.</st>
    <st c="30051">The key concept is that multi-modal vector embeddings preserve semantic
    similarity across all modalities</st> <st c="30156">they represent.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30171">When it comes to using MM-RAG in the enterprise, it is important
    to recognize that a lot of enterprise data resides in multiple modes, so let’s
    discuss</st> <st c="30324">that next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30334">Images are not just “pictures”</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="30365">Images can be thought of as a lot more than just pretty pictures
    of scenery or those 500 pictures you took on your last vacation!</st> <st c="30496">Images
    in the enterprise can represent things such as charts, flowcharts, text that</st>
    <st c="30579">has at some point been converted to an image, and much more.</st>
    <st c="30641">Images are an important data source for</st> <st c="30681">the enterprise.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30696">If you haven’t looked at the PDF file representing the</st> *<st
    c="30752">Google Environmental Report 2023</st>* <st c="30784">we have used in
    many of our labs, you may have started to believe that it was just text based.</st>
    <st c="30880">But open it up, and you will see well-designed imagery throughout
    and accompanying the text we have been working with.</st> <st c="30999">Some of
    the charts you see, particularly the highly designed ones, are images.</st> <st
    c="31078">What if we had a RAG application that wanted to utilize the data in</st>
    <st c="31145">those images as well?</st> <st c="31168">Let’s get started in</st>
    <st c="31189">building one!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31202">Introducing MM-RAG in code</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="31229">In this lab, we will do</st> <st c="31254">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31268">Extract text and images</st> <st c="31292">from a PDF using a
    powerful open source package</st> <st c="31341">called</st> `<st c="31348">unstructured</st>`<st
    c="31360">.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="31361">Use a multi-modal LLM to produce text summaries from the</st>
    <st c="31419">images extracted.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="31436">Embed and retrieve these image summaries (alongside the text objects
    we have already been using) with a reference to the</st> <st c="31558">raw image.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="31568">Store the image summaries in the multi-vector retriever with Chroma,
    which stores raw text and images along with</st> <st c="31682">their summaries.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="31698">Pass the raw images and text chunks to the same multi-modal LLM
    for</st> <st c="31767">answer synthesis.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="31784">We start with installing some new packages that you need</st>
    <st c="31842">for the</st> `<st c="31901">unstructured</st>`<st c="31913">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32132">Here is a list of what these packages are going to do for us in</st>
    <st c="32197">our code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="32206">unstructured[pdf]</st>`<st c="32224">: The</st> `<st c="32231">unstructured</st>`
    <st c="32243">library is a Python library for extracting structured information
    from unstructured data, such as PDFs, images, and HTML pages.</st> <st c="32372">This
    installs only the PDF support from</st> `<st c="32412">unstructured</st>`<st c="32424">.
    There are many other</st> <st c="32447">documents supported that you can include
    if using those types of documents, or you can use</st> `<st c="32538">all</st>`
    <st c="32541">to get support for all documents</st> <st c="32575">they support.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="32588">pillow</st>`<st c="32595">: The</st> `<st c="32602">pillow</st>`
    <st c="32608">library is a fork</st> <st c="32627">of the</st> `<st c="32669">pillow</st>`
    <st c="32675">library provides support for opening, manipulating, and saving various
    image file formats.</st> <st c="32767">In our code, we are working with images
    when using</st> `<st c="32818">unstructured</st>`<st c="32830">, and</st> `<st
    c="32836">unstructured</st>` <st c="32848">uses</st> `<st c="32854">pillow</st>`
    <st c="32860">to help</st> <st c="32869">with that!</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="32879">pydantic</st>`<st c="32888">: The</st> `<st c="32895">pydantic</st>`
    <st c="32903">library is a data validation and settings management library using
    Python type annotations.</st> <st c="32996">The</st> `<st c="33000">pydantic</st>`
    <st c="33008">library is commonly used for defining data models and validating</st>
    <st c="33074">input data.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="33085">lxml</st>`<st c="33090">: The</st> `<st c="33097">lxml</st>`
    <st c="33101">library is a library for processing XML and HTML documents.</st>
    <st c="33162">We use</st> `<st c="33169">lxml</st>` <st c="33173">alongside the</st>
    `<st c="33188">unstructured</st>` <st c="33200">library or other dependencies
    for parsing and extracting information from</st> <st c="33275">structured documents.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="33296">matplotlib</st>`<st c="33307">: The</st> `<st c="33314">matplotlib</st>`
    <st c="33324">library is a well-known plotting library for creating visualizations</st>
    <st c="33394">in Python.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="33404">tiktoken</st>`<st c="33413">: The</st> `<st c="33420">tiktoken</st>`
    <st c="33428">library is a</st> **<st c="33442">Byte-Pair Encoding</st>** <st
    c="33460">(</st>**<st c="33462">BPE</st>**<st c="33465">) tokenizer for use with
    OpenAI’s models.</st> <st c="33508">BPE was</st> <st c="33515">initially developed
    as an algorithm to compress texts and then used by OpenAI for tokenization when
    pre-training the</st> <st c="33633">GPT model.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="33643">poppler-utils</st>`<st c="33657">: The</st> `<st c="33664">poppler</st>`
    <st c="33671">utilities are a set of command-line tools for manipulating PDF files.</st>
    <st c="33742">In our code,</st> `<st c="33755">poppler</st>` <st c="33762">is
    used by</st> `<st c="33774">unstructured</st>` <st c="33786">for extracting elements
    from the</st> <st c="33820">PDF file.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="33829">tesseract-ocr</st>`<st c="33843">: The</st> `<st c="33850">tesseract-ocr</st>`
    <st c="33863">engine is an open source OCR engine that can recognize and extract
    text from images.</st> <st c="33949">This is another library required by</st>
    `<st c="33985">unstructured</st>` <st c="33997">for PDF support to pull text</st>
    <st c="34027">from images.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34039">These packages provide various functionalities and dependencies
    required by the</st> `<st c="34120">langchain</st>` <st c="34129">and</st> `<st
    c="34134">unstructured</st>` <st c="34146">libraries and their associated modules
    used in the code.</st> <st c="34204">They enable tasks such as PDF parsing, image
    handling, data validation, tokenization, and OCR, which are</st> <st c="34308">essential
    for processing and analyzing PDF files and generating responses to</st> <st c="34386">user
    queries.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34399">We will now add import for these packages and others so that we
    can use them in</st> <st c="34480">our code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: <st c="34894">This is a long list of Python packages, so let’s step through
    each of them bullet</st> <st c="34977">by bullet:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="34987">MultiVectorRetriever</st>` <st c="35008">from</st> `<st c="35014">langchain.retrievers.multi_vector</st>`<st
    c="35047">: The</st> `<st c="35054">MultiVectorRetriever</st>` <st c="35074">package
    is a retriever that combines multiple vector stores and allows for efficient retrieval
    of documents based on similarity search.</st> <st c="35211">In our code,</st>
    `<st c="35224">MultiVectorRetriever</st>` <st c="35244">is used to create a retriever
    that combines</st> `<st c="35289">vectorstore</st>` <st c="35300">and</st> `<st
    c="35305">docstore</st>` <st c="35313">for retrieving relevant documents based
    on the</st> <st c="35361">user’s query.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="35374">UnstructuredPDFLoader</st>` <st c="35396">from</st> `<st c="35402">langchain_community.document_loaders</st>`<st
    c="35438">: The</st> `<st c="35445">UnstructuredPDFLoader</st>` <st c="35466">package
    is a document loader that extracts elements, including text and images, from a
    PDF file using the</st> `<st c="35573">unstructured</st>` <st c="35586">library.</st>
    <st c="35595">In our code,</st> `<st c="35608">UnstructuredPDFLoader</st>` <st
    c="35629">is used to load and extract elements from the specified PDF</st> <st
    c="35690">file (</st>`<st c="35696">short_pdf_path</st>`<st c="35711">).</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="35714">RunnableLambda</st>` <st c="35729">from</st> `<st c="35735">langchain_core.runnables</st>`<st
    c="35759">: The</st> `<st c="35766">RunnableLambda</st>` <st c="35780">class is
    a utility class that allows wrapping a function as a runnable component in</st>
    <st c="35864">a LangChain pipeline.</st> <st c="35887">In our code,</st> `<st
    c="35900">RunnableLambda</st>` <st c="35914">is used to wrap the</st> `<st c="35935">split_image_text_types</st>`
    <st c="35957">and</st> `<st c="35962">img_prompt_func</st>` <st c="35977">functions
    as runnable components in the</st> <st c="36018">RAG chain.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="36028">InMemoryStore</st>` <st c="36042">from</st> `<st c="36048">langchain.storage</st>`<st
    c="36065">: The</st> `<st c="36072">InMemoryStore</st>` <st c="36085">class is
    a simple in-memory storage class that stores key-value pairs.</st> <st c="36157">In
    our code,</st> `<st c="36169">InMemoryStore</st>` <st c="36183">is used as a document
    store for storing the actual document content associated with each</st> <st c="36273">document
    ID.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="36285">HumanMessage</st>` <st c="36298">from</st> `<st c="36304">langchain_core.messages</st>`<st
    c="36327">: We saw this type of prompt in</st> *<st c="36360">Code Lab 14.1</st>*
    <st c="36373">already, representing a message sent by a human user in a conversation
    with the language model.</st> <st c="36470">In this code lab,</st> `<st c="36488">HumanMessage</st>`
    <st c="36500">is used to construct prompt messages for image summarization</st>
    <st c="36562">and description.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="36578">base64</st>`<st c="36585">: In our code,</st> `<st c="36601">base64</st>`
    <st c="36607">is used to encode images as</st> `<st c="36636">base64</st>` <st
    c="36642">strings for storage</st> <st c="36663">and retrieval.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="36677">uuid</st>`<st c="36682">: The</st> `<st c="36689">uuid</st>`
    <st c="36693">module provides functions for generating</st> `<st c="36789">uuid</st>`
    <st c="36793">is used to generate unique document IDs for the documents</st> <st
    c="36852">added to</st> `<st c="36861">vectorstore</st>` <st c="36872">and</st>
    `<st c="36877">docstore</st>`<st c="36885">.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="36886">HTML</st>` <st c="36891">and</st> `<st c="36896">display</st>`
    <st c="36903">from</st> `<st c="36909">IPython.display</st>`<st c="36924">: The</st>
    `<st c="36931">HTML</st>` <st c="36935">function is used to create HTML representations
    of objects, and the</st> `<st c="37004">display</st>` <st c="37011">function is
    used to display objects in the IPython notebook.</st> <st c="37073">In our code,</st>
    `<st c="37086">HTML</st>` <st c="37090">and</st> `<st c="37095">display</st>`
    <st c="37102">are used in the</st> `<st c="37119">plt_img_base64</st>` <st c="37133">function
    to display</st> `<st c="37154">base64</st>`<st c="37160">-encoded images.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="37177">Image</st>` <st c="37183">from PIL: PIL provides functions for
    opening, manipulating, and saving various image</st> <st c="37269">file formats.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="37282">matplotlib.pyplot as plt</st>`<st c="37307">: Matplotlib is
    a plotting library that provides functions for creating visualizations and plots.</st>
    <st c="37406">In the code,</st> `<st c="37419">plt</st>` <st c="37422">is not
    directly used, but it may be used implicitly by other libraries</st> <st c="37494">or
    functions.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="37507">These imported packages</st> <st c="37531">and modules provide
    various functionalities related to document loading, retrieval, storage, messaging,
    image handling, and visualization, which are utilized throughout the code to process
    and analyze the PDF file and generate responses to</st> <st c="37772">user queries.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37785">After our imports, there are several variables we establish that
    are used throughout the code.</st> <st c="37881">Here are a couple</st> <st c="37899">of
    highlights</st><st c="37912">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="37914">GPT-4o-mini</st>**<st c="37925">: We are going to use</st>
    <st c="37947">GPT-4o-mini, where the last character,</st> **<st c="37987">o</st>**<st
    c="37988">, stands for</st> **<st c="38001">omni</st>**<st c="38005">, which is
    another way to</st> <st c="38030">say it</st> <st c="38038">is multi-modal!</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**<st c="38111">Short version of PDF</st>**<st c="38132">: Note we are using
    a different</st> <st c="38165">file now:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38236">The full file is large, and using the entire file would increase
    the cost to process without providing much value from a demonstration standpoint.</st>
    <st c="38384">So, we encourage you to use this file instead, where we can still
    demonstrate the MM- RAG app but with significantly less inference cost from</st>
    <st c="38526">our LLM.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**<st c="38534">OpenAI embeddings</st>**<st c="38552">: There’s one</st> <st
    c="38566">key thing to note here when using OpenAI embeddings, as</st> <st c="38623">seen
    next:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38673">This embedding model does not support multi-modal embeddings,
    meaning it will not embed an image of a seagull as very similar to the text word</st>
    *<st c="38817">seagull</st>* <st c="38824">as a true multi-modal embedding model
    should.</st> <st c="38871">To overcome this deficiency, we are embedding the description
    of the image rather than the image itself.</st> <st c="38976">This is still considered
    a multi-modal approach, but keep an eye out for multi-modal embeddings in the
    future that can help us address this at</st> <st c="39118">the embedding level</st>
    <st c="39139">as well!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39147">Next, we are going to load the PDF using the</st> `<st c="39193">UnstructuredPDFLoader</st>`
    <st c="39214">document loader:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: <st c="39465">Here, we extract elements from PDF using LangChain and</st> `<st
    c="39521">unstructured</st>`<st c="39533">. This takes a little time, typically
    between 1-5 minutes depending on how powerful your development environment is.</st>
    <st c="39650">So, this is a good time to take a break and read about the parameters
    that make this package work the way we need it</st> <st c="39767">to work!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39775">Let’s talk about what parameters we used with this document loader
    and how they set us up for the rest of this</st> <st c="39887">code lab:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="39896">short_pdf_path</st>`<st c="39911">: This is the variable for
    the file path we defined earlier representing the shorter version of our</st>
    <st c="40012">PDF file.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="40021">mode="elements"</st>`<st c="40037">: This argument sets the
    mode of extraction for</st> `<st c="40086">UnstructuredPDFLoader</st>`<st c="40107">.
    By setting</st> `<st c="40120">mode="elements"</st>`<st c="40135">, the loader
    is instructed to extract individual elements from the PDF file, such as text blocks
    and images.</st> <st c="40244">This mode allows for more fine-grained control
    over the extracted content compared to</st> <st c="40330">other modes.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="40342">strategy="hi_res"</st>`<st c="40360">: This argument specifies
    the strategy to be used for extracting elements from the PDF file.</st> <st c="40454">Other
    options include</st> `<st c="40476">auto</st>`<st c="40480">,</st> `<st c="40482">fast</st>`<st
    c="40486">, and</st> `<st c="40492">ocr_only</st>`<st c="40500">. The</st> `<st
    c="40506">"hi_res</st>``<st c="40513">"</st>` <st c="40515">strategy identifies
    the layout of the document and uses it to gain additional information about document
    elements.</st> <st c="40631">If you need to speed this process up considerably,
    try</st> `<st c="40686">fast</st>` <st c="40690">mode, but the extraction will
    not be nearly as effective as what you will see from</st> `<st c="40774">"hi_res"</st>`<st
    c="40782">. We encourage you to try all of the settings to see the differences</st>
    <st c="40851">for yourself.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="40864">extract_image_block_types=["Image","Table"]</st>`<st c="40908">:
    The</st> `<st c="40915">extract_image_block_types</st>` <st c="40940">parameter
    is used to specify the types of elements to extract</st> <st c="41003">when processing
    image blocks as</st> `<st c="41035">base64</st>`<st c="41041">-encoded data stored
    in metadata fields.</st> <st c="41083">This parameter allows you to target specific
    elements within images during document processing.</st> <st c="41179">Here, we
    target images</st> <st c="41202">and tables.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="41213">extract_image_block_to_payload=True</st>`<st c="41249">: The</st>
    `<st c="41256">extract_image_block_to_payload</st>` <st c="41286">parameter is
    used to specify whether the extracted image blocks should be included in the payload
    as</st> `<st c="41388">base64</st>`<st c="41394">-encoded data.</st> <st c="41410">This
    parameter is relevant when processing documents and extracting image blocks using
    a high-resolution strategy.</st> <st c="41525">We set it to</st> `<st c="41538">True</st>`
    <st c="41542">so that we do not have to actually store any of the images as files;
    the loader will convert the extracted images to</st> `<st c="41660">base64</st>`
    <st c="41666">format and include them in the metadata of the</st> <st c="41714">corresponding
    elements.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41737">When this process is finished, you will have all of the data loaded
    from the PDF into</st> `<st c="41824">pdf_data</st>`<st c="41832">. Let’s add
    some code to help us explore this data that</st> <st c="41888">was loaded:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: <st c="42252">Here, we pick out the two most important categories of elements
    for our code lab,</st> `<st c="42335">'</st>``<st c="42336">NarrativeText'</st>`
    <st c="42350">and</st> `<st c="42355">'Image'</st>`<st c="42362">. We use list
    comprehensions to pull those into variables that will hold just</st> <st c="42440">those
    elements.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42455">We are about to reduce the number of images to save on processing
    costs, so we print out how many we had beforehand to make sure it works!</st>
    <st c="42595">We also want to see how many element types are</st> <st c="42641">represented
    in the data.</st> <st c="42667">Here is</st> <st c="42675">the output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: <st c="42880">So, right now, we have</st> `<st c="42904">17</st>` <st c="42906">images.</st>
    <st c="42915">We want to reduce that for this demonstration because we are about
    to use an LLM to summarize each of them, and three images cost about six times
    less</st> <st c="43066">than</st> `<st c="43071">17</st>`<st c="43073">!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43074">We also see that there are many other elements in our data than
    we are using when we just use</st> `<st c="43168">'NarrativeText'</st>`<st c="43183">.
    If we wanted to build a more robust application, we could incorporate the</st>
    `<st c="43259">'Title'</st>`<st c="43266">,</st> `<st c="43268">'Footer'</st>`<st
    c="43276">,</st> `<st c="43278">'Header'</st>`<st c="43286">, and other elements
    into the context we send to the LLM, telling it to emphasize those elements accordingly.</st>
    <st c="43396">For example, we can tell it to give more emphasis to</st> `<st c="43449">'Title'</st>`<st
    c="43456">. The</st> `<st c="43462">unstructured</st>` <st c="43474">library does
    a really great job giving us our PDF data in a way that makes it</st> <st c="43553">more
    LLM-friendly!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43571">OK – so, as promised, we are going to reduce the image count to
    save you a little money on</st> <st c="43663">the processing:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: <st c="43803">We basically just lop off the first three images and use that
    list in the</st> `<st c="43878">images</st>` <st c="43884">list.</st> <st c="43891">We
    print this out and see we have reduced it to</st> <st c="43939">three images:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: <st c="44005">The next few code blocks will focus on</st> <st c="44044">image
    summarization, starting with our function to apply the prompt to the image and
    get</st> <st c="44134">a summary:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: <st c="44566">This function takes an</st> `<st c="44590">img_base64</st>` <st
    c="44600">parameter, which represents the</st> `<st c="44633">base64</st>`<st
    c="44639">-encoded string of an image.</st> <st c="44669">The function starts
    with defining a prompt variable that contains a string prompt instructing the
    assistant to summarize the image for retrieval purposes.</st> <st c="44824">The
    function returns a list containing a single</st> `<st c="44872">HumanMessage</st>`
    <st c="44884">object representing the summary of the image.</st> <st c="44931">The</st>
    `<st c="44935">HumanMessage</st>` <st c="44947">object has a</st> `<st c="44961">content</st>`
    <st c="44968">parameter, which is a list containing</st> <st c="45007">two dictionaries:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45024">The first dictionary represents a text message with the prompt
    as</st> <st c="45091">its value</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="45100">The second dictionary represents an image URL message, where the</st>
    `<st c="45165">image_url</st>` <st c="45175">key contains a dictionary with the</st>
    `<st c="45211">url</st>` <st c="45214">key set to the</st> `<st c="45230">base64</st>`<st
    c="45236">-encoded image prefixed with the appropriate data URI</st> <st c="45291">scheme
    (</st>`<st c="45299">data:image/jpeg;base64</st>`<st c="45322">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="45324">Remember when we set</st> `<st c="45345">extract_image_block_to_payload</st>`
    <st c="45375">to</st> `<st c="45379">True</st>` <st c="45383">when using the</st>
    `<st c="45399">UnstructuredPDFLoader</st>` <st c="45420">document loader function?</st>
    <st c="45447">We already have our image in</st> `<st c="45476">base64</st>` <st
    c="45482">format in our metadata as a result, so we just need to pass that into
    this function!</st> <st c="45568">If you use this approach in other applications
    and have a typical image file, such as a</st> `<st c="45656">.jpg</st>` <st c="45660">or</st>
    `<st c="45664">.png</st>` <st c="45668">file, you would just need to convert it
    to</st> `<st c="45712">base64</st>` <st c="45718">to use</st> <st c="45726">this
    function.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45740">For this application, though, because</st> <st c="45779">we extracted
    the images as</st> `<st c="45806">base64</st>` <st c="45813">representations of
    the images, the LLM works with</st> `<st c="45863">base64</st>` <st c="45869">images,
    and this function uses that as the parameter, so we do not need to actually work
    with image files!</st> <st c="45977">Are you disappointed that you will not see
    any images?</st> <st c="46032">Do not be!</st> <st c="46043">We will create a
    helper function in a moment using the HTML function we talked about earlier to
    convert the images from their</st> `<st c="46169">base64</st>` <st c="46175">representations
    into HTML versions that we can display in</st> <st c="46234">our notebook!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46247">But first, we prep our texts and images and set up lists to collect
    the summaries as we run the functions we</st> <st c="46357">just discussed:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: <st c="46711">Note that we are not running the summarization on the texts; we
    are just taking the texts directly as summaries.</st> <st c="46825">You could
    summarize the texts as well, and this may improve retrieval results, as this is
    a common approach in improving RAG retrieval.</st> <st c="46961">However, to save
    some more LLM processing costs, we are focusing on just summarizing the images
    here.</st> <st c="47063">Your wallet will</st> <st c="47080">thank us!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47089">For the images though, this is it – you just went multi-modal,
    using both text and images in your LLM usage!</st> <st c="47199">We cannot yet
    say we used it for MM-RAG, as we are not retrieving anything in a multi-modal
    way.</st> <st c="47296">But we will get there soon – let’s</st> <st c="47331">keep
    going!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47342">Our data preparation has</st> <st c="47368">come to an end; we
    can now go back to adding RAG-related elements such as vector stores and retrievers!</st>
    <st c="47472">Here, we set up the</st> <st c="47492">vector store:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: <st c="47614">Here, we set up a new collection name,</st> `<st c="47654">mm_rag_google_environment</st>`<st
    c="47679">, indicative of the multi-modal nature of the contents of this vector
    store.</st> <st c="47756">We add our</st> `<st c="47767">embedding_function</st>`
    <st c="47785">chain that will be used to embed our content similar to what we
    have seen numerous times in our code labs.</st> <st c="47893">However, in the
    past, we typically added the documents to our vector store as we set</st> <st
    c="47978">it up.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47984">In this case, though, we wait to add the documents not just after
    setting up the vector store but after setting up the retriever too!</st> <st c="48119">How
    can we add them to a retriever, a mechanism for retrieving documents?</st> <st
    c="48193">Well, as we’ve said in the past, a retriever in LangChain is simply
    a wrapper around the vector store, so the vector store is still in there, and
    we can add documents through the retriever in a</st> <st c="48387">similar fashion.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48403">But first, we need to set up the</st> <st c="48437">multi-vector
    retriever:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: <st c="48607">Here, we have applied this</st> `<st c="48635">MultiVectorRetriever</st>`
    <st c="48655">wrapper to our</st> `<st c="48671">vectorstore</st>` <st c="48682">vector
    store.</st> <st c="48697">But what is this other element,</st> `<st c="48729">InMemoryStore</st>`<st
    c="48742">? The</st> `<st c="48748">InMemoryStore</st>` <st c="48761">element
    is an in-memory storage class that stores key-value pairs.</st> <st c="48829">It
    is used as</st> `<st c="48843">docstore</st>` <st c="48851">object for storing
    the actual document content associated with each document ID.</st> <st c="48933">We
    provide those by defining the</st> `<st c="48966">id_key</st>` <st c="48972">with
    the</st> `<st c="48982">doc_id</st>` <st c="48988">string.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48996">At this point, we pass everything to</st> `<st c="49034">MultiVectorRetriever(...)</st>`<st
    c="49059">, a retriever that combines multiple vector stores and allows for efficient
    retrieval of multiple data types based on similarity search.</st> <st c="49196">We
    have seen the</st> `<st c="49213">vectorstore</st>` <st c="49224">vector store
    many times, but as you can see, you can use a</st> `<st c="49284">docstore</st>`
    <st c="49292">object for storing and retrieving document content.</st> <st c="49345">It
    is set to the</st> `<st c="49362">store</st>` <st c="49367">variable (an instance
    of</st> `<st c="49393">InMemoryStore</st>`<st c="49406">) with the</st> `<st c="49418">id_key</st>`
    <st c="49424">string being set as the</st> `<st c="49449">id_key</st>` <st c="49455">parameter
    in the retriever.</st> <st c="49484">This makes it easy to retrieve additional</st>
    <st c="49525">content that is related to the vectors in the vector store, using
    that</st> `<st c="49597">id_key</st>` <st c="49603">string like a foreign key
    in a relational database across the</st> <st c="49666">two stores.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49677">We still do not have any data in any of our stores, though!</st>
    <st c="49738">Let’s build a function that will allow us to</st> <st c="49783">add
    data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '<st c="50251">This function is a helper function that adds documents to the</st>
    `<st c="50314">vectorstore</st>` <st c="50325">vector store and</st> `<st c="50343">docstore</st>`
    <st c="50351">object of the</st> `<st c="50366">retriever</st>` <st c="50375">object.</st>
    <st c="50384">It takes the</st> `<st c="50397">retriever</st>` <st c="50406">object,</st>
    `<st c="50415">doc_summaries</st>` <st c="50428">list, and</st> `<st c="50439">doc_contents</st>`
    <st c="50451">list as arguments.</st> <st c="50471">As we’ve already discussed,
    we have summaries and contents for each of our categories: the texts and images
    that we will pass to</st> <st c="50600">this function.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50614">This function generates unique</st> <st c="50646">document IDs
    for each document using</st> `<st c="50683">str(uuid.uuid4())</st>` <st c="50700">and
    then creates a</st> `<st c="50720">summary_docs</st>` <st c="50732">list by iterating
    over the</st> `<st c="50760">doc_summaries</st>` <st c="50773">list and creating</st>
    `<st c="50792">Document</st>` <st c="50800">objects with the summary as the page
    content and the corresponding document ID as metadata.</st> <st c="50893">It also
    creates a</st> `<st c="50911">content_docs</st>` <st c="50923">list of</st> `<st
    c="50932">Document</st>` <st c="50940">objects by iterating over the</st> `<st
    c="50971">doc_contents</st>` <st c="50983">list and creating</st> `<st c="51002">Document</st>`
    <st c="51010">objects with the document content as the page content and the corresponding
    document ID as metadata.</st> <st c="51112">It adds the</st> `<st c="51124">summary_docs</st>`
    <st c="51136">list to the retriever’s</st> `<st c="51161">vectorstore</st>` <st
    c="51172">vector store using the</st> `<st c="51196">retriever.vectorstore.add_documents</st>`
    <st c="51231">function.</st> <st c="51242">It uses the</st> `<st c="51254">retriever.docstore.mset</st>`
    <st c="51277">function to add the</st> `<st c="51298">content_docs</st>` <st c="51310">list
    to the retriever’s</st> `<st c="51335">docstore</st>` <st c="51343">object, associating
    each document ID with its corresponding</st> <st c="51404">document content.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51421">We still need to apply the</st> `<st c="51449">add_document</st>`
    <st c="51461">function:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: <st c="51636">This will add the appropriate documents and summaries where we
    need them for our MM-RAG pipeline, adding embedding vectors that represent both
    text and</st> <st c="51789">image summaries.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51805">Next, we will add a final round of helper functions we will need
    in our final MM-RAG chain, starting with one that splits</st> `<st c="51928">base64</st>`<st
    c="51934">-encoded images</st> <st c="51951">and texts:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: <st c="52321">This function takes our list of image-related docs as input and
    splits them into</st> `<st c="52403">base64</st>`<st c="52409">-encoded images
    and texts.</st> <st c="52437">It initializes two empty lists:</st> `<st c="52469">b64_images</st>`
    <st c="52479">and</st> `<st c="52484">texts</st>`<st c="52489">. It iterates over
    each</st> `<st c="52513">doc</st>` <st c="52516">variable in the</st> `<st c="52533">docs</st>`
    <st c="52537">list, checking if it is an instance of the</st> `<st c="52581">Document</st>`
    <st c="52589">class.</st> <st c="52597">If the</st> `<st c="52604">doc</st>` <st
    c="52607">variable is a</st> `<st c="52622">Document</st>` <st c="52630">object
    and its metadata has a</st> `<st c="52661">category</st>` <st c="52669">key with
    the value</st> `<st c="52689">Image</st>`<st c="52694">, it extracts the</st>
    `<st c="52712">base64</st>`<st c="52718">-encoded image from</st> `<st c="52739">doc.metadata["image_base64"]</st>`
    <st c="52767">and appends it to the</st> `<st c="52790">b64_images</st>` <st c="52800">list.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52806">If the</st> `<st c="52814">doc</st>` <st c="52817">variable is
    a</st> `<st c="52832">Document</st>` <st c="52840">object</st> <st c="52847">but
    does not have the</st> `<st c="52870">Image</st>` <st c="52875">category, it appends</st>
    `<st c="52897">doc.page_content</st>` <st c="52913">to the</st> `<st c="52921">texts</st>`
    <st c="52926">list.</st> <st c="52933">If the</st> `<st c="52940">doc</st>` <st
    c="52943">variable is not a</st> `<st c="52962">Document</st>` <st c="52970">object
    but is a string, it appends the</st> `<st c="53010">doc</st>` <st c="53013">variable
    to the</st> `<st c="53030">texts</st>` <st c="53035">list.</st> <st c="53042">Finally,
    the function returns a dictionary with two keys:</st> `<st c="53100">"images"</st>`<st
    c="53108">, containing a list of</st> `<st c="53131">base64</st>`<st c="53137">-encoded
    images, and</st> `<st c="53159">"texts"</st>`<st c="53166">, containing a list</st>
    <st c="53186">of texts.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53195">We also have a function to</st> <st c="53222">generate our image</st>
    <st c="53242">prompt message:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: <st c="54104">This function takes</st> `<st c="54125">data_dict</st>` <st c="54134">as
    input</st> <st c="54143">and generates a prompt message for image analysis.</st>
    <st c="54195">It extracts texts from</st> `<st c="54218">data_dict["context"]</st>`
    <st c="54238">and joins them into a single string,</st> `<st c="54276">formatted_texts</st>`<st
    c="54291">, using</st> `<st c="54299">"\n".join</st>`<st c="54308">. It initializes
    an empty list</st> <st c="54339">called</st> `<st c="54346">messages</st>`<st
    c="54354">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54355">If</st> `<st c="54359">data_dict["context"]["images"]</st>` <st
    c="54389">exists, it iterates over each image in the list.</st> <st c="54439">For
    each image, it creates an</st> `<st c="54469">image_message</st>` <st c="54482">dictionary
    with a</st> `<st c="54501">"type"</st>` <st c="54507">key set to</st> `<st c="54519">"image_url"</st>`
    <st c="54530">and an</st> `<st c="54538">"image_url"</st>` <st c="54549">key containing
    a dictionary with the</st> `<st c="54587">base64</st>`<st c="54593">-encoded image
    URL.</st> <st c="54614">It appends each</st> `<st c="54630">image_message</st>`
    <st c="54643">instance to the</st> `<st c="54660">messages</st>` <st c="54668">list.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54674">And now, the final touch – before</st> <st c="54709">we run our
    MM-RAG application, we establish an MM-RAG chain, including the use of the two
    functions we just</st> <st c="54817">set up:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: <st c="55022">This creates our MM-RAG chain, which consists of the</st> <st
    c="55076">following components:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="55097">{"context": retriever_multi_vector | RunnableLambda(split_image_text_types),
    "question": RunnablePassthrough()}</st>`<st c="55209">: This is similar to other
    retriever components we’ve seen in the past, providing a dictionary with two keys:</st>
    `<st c="55320">"context"</st>` <st c="55329">and</st> `<st c="55334">"question"</st>`<st
    c="55344">. The</st> `<st c="55350">"context"</st>` <st c="55359">key is assigned
    the result of</st> `<st c="55390">retriever_multi_vector | RunnableLambda(split_image_text_types)</st>`<st
    c="55453">. The</st> `<st c="55459">retriever_multi_vector</st>` <st c="55481">function
    retrieves relevant documents based on the question, and those results are then
    passed through</st> `<st c="55585">RunnableLambda(split_image_text_types)</st>`<st
    c="55623">, which is a wrapper around the</st> `<st c="55655">split_image_text_types</st>`
    <st c="55677">function.</st> <st c="55688">As we discussed previously, the</st>
    `<st c="55720">split_image_text_types</st>` <st c="55742">function splits the
    retrieved documents into</st> `<st c="55788">base64</st>`<st c="55794">-encoded
    images and texts.</st> <st c="55822">The</st> `<st c="55826">"question"</st>`
    <st c="55836">key is assigned</st> `<st c="55853">RunnablePassthrough</st>`<st
    c="55872">, which simply passes the question through without</st> <st c="55923">any
    modification.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="55940">RunnableLambda(img_prompt_func)</st>`<st c="55972">: The output
    of the previous component (the split images and texts along with the question)
    is passed through</st> `<st c="56083">RunnableLambda(img_prompt_func)</st>`<st
    c="56114">. As we discussed previously, the</st> `<st c="56148">img_prompt_func</st>`
    <st c="56163">function generates a prompt message for image analysis based on
    the retrieved context and the question, so this is what formats the prompt we
    will pass into the next</st> <st c="56330">step:</st> `<st c="56336">llm</st>`<st
    c="56339">.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="56340">llm</st>`<st c="56344">: The generated prompt</st> <st c="56367">message,
    which includes an image in</st> `<st c="56404">base64</st>` <st c="56410">format,
    is passed to our LLM for processing.</st> <st c="56456">The LLM generates a response
    based on the multi-modal prompt message and then passes it to the next step: the</st>
    <st c="56566">output parser.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="56580">str_output_parser</st>`<st c="56598">: We’ve seen output parsers
    throughout our code labs, and this is the same reliable</st> `<st c="56683">StrOutputParser</st>`
    <st c="56698">class that has served us well in the past, parsing the generated
    response as</st> <st c="56776">a string.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56785">Overall, this chain represents an MM-RAG pipeline that retrieves
    relevant documents, splits them into images and texts, generates a prompt message,
    processes it with an LLM, and parses the output as</st> <st c="56985">a string.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56994">We invoke this chain and implement full</st> <st c="57035">multi-modal
    retrieval:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: <st c="57160">Note that we are using a different</st> `<st c="57196">user_query</st>`
    <st c="57206">string than we have used in the past.</st> <st c="57245">We changed
    it to something that would be relevant to the images that we</st> <st c="57317">had
    available.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57331">Here is the output from our MM-RAG pipeline based on this</st>
    <st c="57390">user query:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: <st c="57955">The response aligns with the</st> `<st c="57985">user_query</st>`
    <st c="57995">string, as well as the prompt we used to explain to the LLM how
    to describe the image that it “sees.” Since we only have three images, it is pretty
    easy to find</st> <st c="58157">which image this is talking about, image</st>
    *<st c="58198">#2</st>*<st c="58200">, which we can retrieve</st> <st c="58224">with
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: <st c="58394">The function here is the helper function we promised to help you
    see the image.</st> <st c="58475">It takes a</st> `<st c="58486">base64</st>`<st
    c="58492">-encoded image,</st> `<st c="58509">img_base64</st>`<st c="58519">,
    as input and displays it using HTML.</st> <st c="58558">It does this by creating
    an</st> `<st c="58586">image_html</st>` <st c="58596">HTML string that represents
    an</st> `<st c="58628"><img></st>` <st c="58633">tag with the</st> `<st c="58647">src</st>`
    <st c="58650">attribute set to the</st> `<st c="58672">base64</st>`<st c="58678">-encoded
    image URL.</st> <st c="58699">It uses the</st> `<st c="58711">display()</st>`
    <st c="58720">function from IPython to render the HTML string and display the
    image.</st> <st c="58792">Run this in your code lab, and you will see the image
    that was extracted from the PDF to provide the basis for the</st> <st c="58907">MM-RAG
    response!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58923">And just for reference, here is the image summary that was generated
    for this image, using the same index from the</st> `<st c="59039">img_base64_list</st>`
    <st c="59054">list since</st> <st c="59066">they match:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: <st c="59096">The summary should look something</st> <st c="59131">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: <st c="59244">Given the output description from the MM-RAG chain, which is much
    more robust and descriptive about this image, you can see that the LLM can actually
    “see” this image and tell you about it.</st> <st c="59435">You are</st> <st c="59443">officially
    multi-modal!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59466">We selected the three code labs in this chapter because we felt
    they represented the broadest representation of potential improvements across
    most RAG applications.</st> <st c="59632">But these are just the tip of the iceberg
    in terms of techniques that may be applicable to your specific RAG needs.</st>
    <st c="59748">In the</st> <st c="59754">next section, we provide what we would
    consider just a start to the many techniques you should consider incorporating
    into your</st> <st c="59883">RAG pipeline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59896">Other advanced RAG techniques to explore</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="59937">As with just about everything else we have discussed with RAG
    and GenAI, the options available for advanced techniques to apply to your RAG
    application are too numerous to list or even keep track of.</st> <st c="60138">We
    have selected techniques that are focused on aspects of RAG specifically, categorizing
    them based on the areas of your RAG application they will likely have the</st>
    <st c="60302">most impact.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60314">Let’s walk through them in the same order that our RAG pipelines
    operate, starting</st> <st c="60398">with indexing.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60412">Indexing improvements</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="60434">These are advanced RAG techniques that focus on the indexing stage
    of the</st> <st c="60509">RAG pipeline:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="60522">Deep chunking</st>**<st c="60536">: The quality of retrieved
    results often depends on the way your data is chunked before it’s stored in the</st>
    <st c="60644">retrieval system itself.</st> <st c="60669">With deep chunking,
    you use DL models, including transformers, for optimal and</st> <st c="60748">intelligent
    chunking.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="60769">Training and utilizing embedding adapters</st>**<st c="60811">:
    Embedding adapters are lightweight modules trained to adapt pre-existing language
    model embeddings for specific tasks or domains without the need for extensive
    retraining.</st> <st c="60986">When applied to RAG systems, these adapters can
    tailor the model’s understanding and generation capabilities to better align with
    the nuances of the prompt, facilitating more accurate and</st> <st c="61174">relevant
    retrievals.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="61194">Multi-representation indexing</st>**<st c="61224">: Proposition
    indexing uses an LLM to produce document summaries (propositions) that are optimized</st>
    <st c="61324">for retrieval.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="61338">Recursive Abstractive Processing for Tree Organized Retrieval
    (RAPTOR)</st>**<st c="61409">: RAG systems need to handle “lower-level” questions
    that reference specific facts found in a single</st> <st c="61510">document or
    “higher-level” questions that distill ideas that span many documents.</st> <st
    c="61593">Handling both types of questions can be a challenge with typical kNN
    retrieval where only a finite number of doc chunks are retrieved.</st> <st c="61728">RAPTOR
    addresses this by creating document summaries that capture higher-level concepts.</st>
    <st c="61817">It embeds and clusters documents and then summarizes each cluster.</st>
    <st c="61884">It does this recursively, producing a tree of summaries with increasingly
    high-level concepts.</st> <st c="61979">The summaries and starting docs are indexed
    together, giving coverage across</st> <st c="62056">user questions.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="62071">Contextualized Late Interaction over BERT (ColBERT)</st>**<st
    c="62123">: Embedding models compress text into</st> <st c="62162">fixed-length
    (vector) representations that capture the semantic content of the</st> <st c="62240">document.</st>
    <st c="62251">This compression is very useful for efficient search retrieval but
    puts a heavy burden on that single vector representation to capture all the semantic
    nuance and detail of the documents.</st> <st c="62439">In some cases, irrelevant
    or redundant content can dilute the semantic usefulness of the embedding.</st>
    <st c="62539">ColBERT is an approach to address this with higher granularity embeddings,
    focused on producing a more granular token-wise similarity assessment between
    the document and</st> <st c="62709">the query.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="62719">Retrieval</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="62729">Retrieval is our largest category of</st> <st c="62767">advanced
    RAG techniques, reflecting the importance of retrieval in the RAG process.</st>
    <st c="62851">Here are some approaches we recommend you consider for your</st>
    <st c="62911">RAG application:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="62927">Hypothetical Document Embeddings (HyDE)</st>**<st c="62967">:
    HyDE is a retrieval method used to enhance retrieval by</st> <st c="63025">generating
    a hypothetical document for an incoming query.</st> <st c="63084">These documents,
    drawn from the LLM’s knowledge, are</st> <st c="63137">embedded and used to retrieve
    documents from an index.</st> <st c="63192">The idea is that hypothetical documents
    may be better aligned with index documents than the raw</st> <st c="63288">user
    question.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="63302">Sentence-window retrieval</st>**<st c="63328">: With sentence-window
    retrieval, you perform retrieval based on smaller sentences to</st> <st c="63414">better
    match the relevant context and then synthesize based on an expanded context window
    around</st> <st c="63512">the sentence.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="63525">Auto-merging retrieval</st>**<st c="63548">: Auto-merging retrieval
    tackles the issue you see with naïve RAG where having smaller chunks</st> <st
    c="63642">can lead to fragmentation of our data.</st> <st c="63682">It uses an
    auto-merging heuristic to merge smaller chunks into a bigger parent chunk to help
    ensure more</st> <st c="63787">coherent context.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="63804">Multi-query rewriting</st>**<st c="63826">: Multi-query is</st>
    <st c="63844">an approach that rewrites a question from</st> <st c="63886">multiple
    perspectives, performs retrieval on each rewritten question, and takes the unique
    union of</st> <st c="63986">all documents.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="64000">Query translation step-back</st>**<st c="64028">: Step-back
    prompting is an approach to improve retrieval that builds on CoT</st> <st c="64105">reasoning.</st>
    <st c="64117">From a question, it generates a step-back (higher level, more abstract)
    question that can serve as a precondition to correctly answering the original
    question.</st> <st c="64277">This is especially useful in cases where background
    knowledge or more fundamental understanding is helpful in answering a</st> <st
    c="64399">specific question.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="64417">Query structuring</st>**<st c="64435">: Query structuring is
    the process of text-to-DSL, where DSL is a domain-specific language</st> <st c="64526">required
    to interact with a given database.</st> <st c="64571">This converts user questions
    into</st> <st c="64605">structured queries.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="64624">Post-retrieval/generation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="64650">These are advanced RAG</st> <st c="64673">techniques that focus
    on the generation stage of the</st> <st c="64727">RAG pipeline:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="64740">Cross-encoder re-ranking</st>**<st c="64765">: We have already
    seen the improvement that re-ranking can provide in our hybrid RAG code lab, which
    is applied to the retrieved results before they are sent to</st> <st c="64927">the
    LLM.</st> <st c="64936">Cross-encoder re-ranking takes even more advantage of
    this technique by using a more computationally intensive model to reassess and
    reorder the retrieved documents based on their relevance to the original prompt.</st>
    <st c="65150">This fine-grained analysis ensures that the most pertinent information
    is prioritized for the generation phase, enhancing the overall</st> <st c="65284">output
    quality.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="65299">RAG-fusion query rewriting</st>**<st c="65326">: RAG-fusion
    is</st> <st c="65343">an approach that rewrites a question from multiple perspectives,
    performs retrieval on each rewritten question, and performs reciprocal rank fusion
    on the results from each retrieval, giving a</st> <st c="65536">consolidated ranking.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="65557">Entire RAG pipeline coverage</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="65586">These advanced RAG techniques focus on the overall RAG pipeline,
    rather than one particular stage</st> <st c="65685">of it:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="65691">Self-reflective RAG</st>**<st c="65711">: The self-reflective</st>
    <st c="65734">RAG with LangGraph technique improves on naïve RAG models by incorporating
    a self-reflective mechanism coupled with a linguistic graph structure from LangGraph.</st>
    <st c="65895">In this approach, LangGraph helps in understanding the context and
    semantics at a deeper level, allowing the RAG system to refine its</st> <st c="66028">responses
    based on a more nuanced understanding of the content and its interconnections.</st>
    <st c="66118">This can be particularly</st> <st c="66142">useful in applications
    such as content creation, question-answering, and conversational agents, as it
    leads to more accurate, relevant, and context-aware outputs, significantly enhancing
    the quality of</st> <st c="66345">generated text.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="66360">Modular RAG</st>**<st c="66372">: Modular RAG uses interchangeable
    components to provide a more flexible architecture</st> <st c="66458">that can
    adjust to your RAG development needs.</st> <st c="66506">This modularity enables
    researchers and developers to experiment with different retrieval mechanisms,
    generative models, and optimization strategies, tailoring the RAG system to specific
    needs and applications.</st> <st c="66717">As you’ve seen throughout the code
    labs in this book, LangChain provides mechanisms that support this approach well,
    where LLMs, retrievers, vector stores, and other components can be swapped out
    and switched easily in many cases.</st> <st c="66948">The goal of modular RAG
    is to move toward a more customizable, efficient, and powerful RAG system capable
    of tackling a wider range of tasks with</st> <st c="67094">greater efficacy.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="67111">With new research</st> <st c="67130">coming out every day, this
    technique list is growing rapidly.</st> <st c="67192">One great source for new
    techniques is the Arxiv.org</st> <st c="67245">website:</st> [<st c="67254">https://arxiv.org/</st>](https://arxiv.org/)<st
    c="67272">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67273">Visit this website and search for various key terms related to
    your RAG application, including</st> *<st c="67369">RAG</st>*<st c="67372">,</st>
    *<st c="67374">retrieval augmented generation</st>*<st c="67404">,</st> *<st c="67406">vector
    search</st>*<st c="67419">, and other</st> <st c="67431">related terms.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67445">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="67453">In this final chapter, we explored several advanced techniques
    to improve RAG applications, including query expansion, query decomposition, and
    MM-RAG.</st> <st c="67606">These techniques enhance retrieval and generation by
    augmenting queries, breaking down questions into subproblems, and incorporating
    multiple data modalities.</st> <st c="67765">We also discussed a range of other
    advanced RAG techniques covering indexing, retrieval, generation, and the entire</st>
    <st c="67881">RAG pipeline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67894">It has been a pleasure to go on this RAG journey with you, exploring
    the world of RAG and its vast potential.</st> <st c="68005">As we conclude this
    book, I hope you feel well equipped with the knowledge and practical experience
    to tackle your own RAG projects.</st> <st c="68138">Good luck in your future RAG
    endeavors – I’m confident you’ll create remarkable applications that push the
    boundaries of what’s possible with this exciting</st> <st c="68294">new technology!</st>
  prefs: []
  type: TYPE_NORMAL
