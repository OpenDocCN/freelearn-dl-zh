<html><head></head><body>
        

                            
                    <h1 class="header-title">Text Representations - Words to Numbers</h1>
                
            
            
                
<p class="mce-root">Computers today cannot act on words or text directly. They need to be represented by meaningful number sequences. These long sequences of decimal numbers are called vectors, and this step is often referred to as the vectorization of text.</p>
<p class="mce-root">So, where are these word vectors used:</p>
<ul>
<li>In text classification and summarization tasks</li>
<li>During similar word searches, such as synonyms</li>
<li>In machine translation (for example, when translating text from English to German)</li>
<li>When understanding similar texts (for example, Facebook articles)</li>
<li>During question and answer sessions, and general tasks (for example, chatbots used in appointment scheduling)</li>
</ul>
<p class="mce-root">Very frequently, we see word vectors used in some form of categorization task. For instance, using a machine learning or deep learning model for sentiment analysis, with the following text vectorization methods:</p>
<ul>
<li class="mce-root">TF-IDF in sklearn pipelines with logistic regression</li>
<li class="mce-root">GLoVe by Stanford, looked up via Gensim</li>
<li class="mce-root">fastText by Facebook using pre-trained vectors</li>
</ul>
<p>We have already seen TF-IDF examples, and will see several more throughout this book. This chapter will instead cover the other ways in which you can vectorize your text corpus or a part of it.</p>
<p class="mce-root"/>
<p>In this chapter, we will learn about the following topics:</p>
<ul>
<li>How to vectorize a specific dataset</li>
<li>How to make document embedding</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Vectorizing a specific dataset</h1>
                
            
            
                
<p class="mce-root">This section focuses almost exclusively on word vectors and how we can leverage the Gensim library to perform them.</p>
<p class="mce-root">Some of the questions we want to answer in this section include these:</p>
<ul>
<li>How do we use original embedding, such as GLoVe?</li>
<li>How do we handle Out of Vocabulary words? (Hint: fastText)</li>
<li>How do we train our own word2vec vectors on our own corpus?</li>
<li>How do we train our own word2vec vectors?</li>
<li>How do we train our own fastText vectors?</li>
<li>How do we use similar words to compare both of the above?</li>
</ul>
<p>First, let's get started with some simple imports, as follows:</p>
<pre class="mce-root">import gensim<br/>print(f'gensim: {gensim.__version__}')<br/>&gt; gensim: 3.4.0</pre>
<p>Please ensure that your Gensim version is at least 3.4.0. This is a very popular package which is maintained and developed mostly by text processing experts over at RaRe Technologies. They use the same library in their own work for enterprise B2B consulting. Large parts of Gensim's internal implementations are written in Cython for speed. It natively uses multiprocessing.</p>
<p>Here, the caveat is that Gensim is known to make breaking API changes, so consider double-checking the API when you use the code with their documents or tutorials.</p>
<p>If you using a Windows machine, watch out for a warning similar to the following:</p>
<pre>C:\Users\nirantk\Anaconda3\envs\fastai\lib\site-packages\Gensim\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial<br/> warnings.warn("detected Windows; aliasing chunkize to chunkize_serial")</pre>
<p>Now, let's get started by downloading the pre-trained GloVe embedding. While we could do this manually, here we will download it using the following Python code:</p>
<pre>from tqdm import tqdm<br/>class TqdmUpTo(tqdm):<br/>    def update_to(self, b=1, bsize=1, tsize=None):<br/>        if tsize is not None: self.total = tsize<br/>        self.update(b * bsize - self.n)<br/><br/>def get_data(url, filename):<br/>    """<br/>    Download data if the filename does not exist already<br/>    Uses Tqdm to show download progress<br/>    """<br/>    import os<br/>    from urllib.request import urlretrieve<br/>    <br/>    if not os.path.exists(filename):<br/><br/>        dirname = os.path.dirname(filename)<br/>        if not os.path.exists(dirname):<br/>            os.makedirs(dirname)<br/><br/>        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:<br/>            urlretrieve(url, filename, reporthook=t.update_to)</pre>
<p>We will also reuse the <kbd>get_data</kbd> API to download any arbitrary files that we want to use throughout this section. We have also set up <kbd>tqdm</kbd> (Arabic for progress), which provides us with a progress bar by wrapping our <kbd>urlretrieve</kbd> iterable in it.</p>
<p>The following text is from tqdm's README:</p>
<p>tqdm works on any platform (Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS), in any console or in a GUI, and is also friendly with IPython/Jupyter notebooks.<br/>
<br/>
tqdm does not require any dependencies (not even curses!), just Python and an environment supporting carriage return \r and line feed \n control characters.</p>
<p>Right, let's finally download the embedding, shall we?</p>
<pre>embedding_url = 'http://nlp.stanford.edu/data/glove.6B.zip'
get_data(embedding_url, 'data/glove.6B.zip')</pre>
<p>The preceding snippet will download a large file with GLoVe word representations of 6 billion English words.</p>
<p>Let's quickly unzip the file using the Terminal or command-line syntax in Jupyter notebooks. You can also do this manually or by writing code, as follows:</p>
<pre># # We need to run this only once, can unzip manually unzip to the data directory too
# !unzip data/glove.6B.zip 
# !mv glove.6B.300d.txt data/glove.6B.300d.txt 
# !mv glove.6B.200d.txt data/glove.6B.200d.txt 
# !mv glove.6B.100d.txt data/glove.6B.100d.txt 
# !mv glove.6B.50d.txt data/glove.6B.50d.txt</pre>
<p>Here, we have moved all of the <kbd>.txt</kbd> files back to the <kbd>data</kbd> directory. The thing to note here is in the filename, <kbd>glove.6B.50d.txt</kbd>.</p>
<p><kbd>6B</kbd> stands for the 6 billion words or tokens. <kbd>50d</kbd> stands for 50 dimensions, which means that each word is represented by a sequence of 50 numbers, and in this case, that's 50 float numbers.</p>
<p>We'll now deviate a little to give you some context about word representations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Word representations</h1>
                
            
            
                
<p>The most popular names in word embedding are word2vec by Google (Mikolov) and GloVe by Stanford (Pennington, Socher, and Manning). fastText seems to be fairly popular for multilingual sub-word embeddings.</p>
<p>We advise that you don't use word2vec or GloVe. Instead, use fastText vectors, which are much better and from the same authors. word2vec was introduced by T. Mikolov et. al. (<a href="https://scholar.google.com/citations?user=oBu8kMMAAAAJ&amp;hl=en">https://scholar.google.com/citations?user=oBu8kMMAAAAJ&amp;hl=en</a>) when he was with Google, and it performs well on word similarity and analogy tasks.</p>
<p>GloVe was introduced by Pennington, Socher, and Manning from Stanford in 2014 as a statistical approximation for word embedding. The word vectors are created by the matrix factorization of word-word co-occurrence matrices.</p>
<p class="mce-root"/>
<p>If picking between the lesser of two evils, we recommend using GloVe over word2vec. This is because GloVe outperforms word2vec in most machine learning tasks and NLP challenges in academia.</p>
<p>Skipping the original word2vec here, we will now look at the following topics:</p>
<ul>
<li>How do we use original embeddings in GLoVe?</li>
<li>How do we handle out of vocabulary words? (Hint: fastText)</li>
<li>How do we train our own word2vec vectors on our own corpus?</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">How do we use pre-trained embeddings?</h1>
                
            
            
                
<p>We just downloaded these.</p>
<p>The file formats used by word2vec and GloVe are slightly different from each other. We'd like a consistent API to look up any word embedding, and we can do this by converting the embedding format. Note that there are minor differences in how word embedding is stored.</p>
<p class="mce-root">This format conversion can be done using Gensim's API called <kbd>glove2word2vec</kbd>. We will use this to convert our GloVe embedding information to the word2vec format.</p>
<p class="inner_cell">So, let's get the imports out of the way and begin by setting up filenames, as follows:</p>
<div><div><pre>from gensim.scripts.glove2word2vec import glove2word2vec
glove_input_file = 'data/glove.6B.300d.txt'
word2vec_output_file = 'data/glove.6B.300d.word2vec.txt'</pre></div>
</div>
<p>We don't want to repeat this step if we have already done the conversion once. The simplest way to check this is to see if <kbd>word2vec_output_file</kbd> already exists. We run the following conversion only if the file does not exist:</p>
<pre>import os
if not os.path.exists(word2vec_output_file):
    glove2word2vec(glove_input_file, word2vec_output_file)</pre>
<p>The preceding snippet will create a new file in a standard that is compatible with the rest of Gensim's API stack.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">KeyedVectors API</h1>
                
            
            
                
<p class="mce-root">We now have to perform the simple task of loading vectors from a file. We do this using the <kbd>KeyedVectors</kbd> API in Gensim. The word we want to look up is the key, and the numerical representation of that word is the corresponding value.</p>
<p>Let's first import the API and set up the target filename as follows:</p>
<div><div><div><pre>from gensim.models import KeyedVectors
filename = word2vec_output_file</pre></div>
</div>
</div>
<p>We will load the entire text file into our memory, thus including the read from disk time. In most running processes, this is a one-off I/O step and is not repeated for every new data pass. This becomes our Gensim model, detailed as follows:</p>
<pre>%%time
# load the Stanford GloVe model from file, this is Disk I/O and can be slow
pretrained_w2v_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)
# binary=False format for human readable text (.txt) files, and binary=True for .bin files</pre>
<p>A faster SSD should definitely speed this up by an order of magnitude.</p>
<p>We can do some word vector arithmetic to compose and show that this representation captures semantic meaning as well. For instance, let's repeat the following famous word vector example:</p>
<pre>(king - man) + woman = ?</pre>
<p>Let's now perform the mentioned arithmetic operations on the word vectors, as follows:</p>
<pre># calculate: (king - man) + woman = ?
result = pretrained_w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We did this using the <kbd>most_similar</kbd> API. Behind the scenes, Gensim has done the following for us:</p>
<ol>
<li>Looked up the vectors for <kbd>woman</kbd>, <kbd>king</kbd>, and <kbd>man</kbd></li>
<li>Added <kbd>king</kbd> and <kbd>woman</kbd>, and subtracted the vector from <kbd>man</kbd> to find a resultant vector</li>
<li>From the 6 billion tokens in this model, ranked all words by distance and found the closest words</li>
<li>Found the closest word</li>
</ol>
<p>We also added <kbd>topn=1</kbd> to tell the API that we are only interested in the closest match. The expected output is now just one word, <kbd>'queen'</kbd>, as shown in the following snippet:</p>
<div><div><div><div><pre>print(result)<br/>&gt; [('queen', 0.6713277101516724)]</pre></div>
</div>
</div>
</div>
<p>Not only did we get the correct word, but also an accompanying decimal number! We will ignore that for now, but note that the number represents a notion of how close or similar the word is to the resultant vector that the API computed for us.</p>
<p>Let's try a few more examples, say social networks, as shown in the following snippet:</p>
<div><div><div><div><pre>result = pretrained_w2v_model.most_similar(positive=['quora', 'facebook'], negative=['linkedin'], topn=1)
print(result)</pre></div>
</div>
</div>
</div>
<div><div><div><p class="prompt">In this example, we are looking for a social network that is more casual than LinkedIn but more focused on learning than Facebook by adding Quora. As you can see in the following output, it looks like Twitter fits the bill perfectly:</p>
<div><pre>[('twitter', 0.37966805696487427)]</pre></div>
</div>
</div>
</div>
<p>We could have equally expected Reddit to fit this.</p>
<p>So, can we use this approach to simply explore similar words in a larger corpus? It seems so. Let's now look up words most similar to <kbd>india</kbd>, as shown in the following snippet. Notice that we are writing India in lowercase; this is because the model contains only lowercase words:</p>
<div><div><div><div><pre>pretrained_w2v_model.most_similar('india')</pre></div>
</div>
</div>
</div>
<div><div><div><p class="prompt output_prompt">It is worth mentioning that these results might be a little biased because GloVe was primarily trained on a large news corpus called Gigaword:</p>
<div><pre>[('indian', 0.7355823516845703),
 ('pakistan', 0.7285579442977905),
 ('delhi', 0.6846907138824463),
 ('bangladesh', 0.6203191876411438),
 ('lanka', 0.609517514705658),
 ('sri', 0.6011613607406616),
 ('kashmir', 0.5746493935585022),
 ('nepal', 0.5421023368835449),
 ('pradesh', 0.5405811071395874),
 ('maharashtra', 0.518537700176239)]</pre></div>
</div>
</div>
</div>
<p>The preceding result does make sense, keeping in mind that, in the foreign press, India is often mentioned because of its troubled relationships with its geographical neighbours, including Pakistan and Kashmir. Bangladesh, Nepal, and Sri Lanka are neighbouring countries, while Maharashtra is the home of India's business capital, Mumbai.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What is missing in both word2vec and GloVe?</h1>
                
            
            
                
<p class="mce-root">Neither GloVe nor word2vec can handle words they didn't see during training. These words are called <strong>Out of Vocabulary</strong> (OOV), in the literature.</p>
<p class="mce-root">Evidence of this can be seen if you try to look up nouns that are not frequently used, for example an uncommon name. As you can see in the following snippet, the model throws a <kbd>not in vocabulary</kbd> error:</p>
<div><div><div><div><pre>try: <br/>  pretrained_w2v_model.wv.most_similar('nirant')<br/>except Exception as e: <br/>  print(e)  </pre></div>
</div>
</div>
</div>
<div><div><div><p class="prompt">This results in the following output:</p>
<div><pre>"word 'nirant' not in vocabulary"</pre></div>
</div>
</div>
</div>
<p>This result is also accompanied by an API warning that sometimes states the API will change in gensim v4.0.0.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How do we handle Out Of Vocabulary words?</h1>
                
            
            
                
<p>The authors of word2vec (Mikolov et al.) extended it to create fastText at Facebook. It works on character n-grams instead of entire words. Character n-grams are effective in languages with specific morphological properties.</p>
<p>We can create our own fastText embeddings, which can handle OOV tokens as well.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting the dataset</h1>
                
            
            
                
<p>First, we need to download the subtitles of several TED talks from a public dataset. We will train our fastText embeddings on these as well as the word2vec embeddings for comparison, as follows:</p>
<div><div><div><div><div><pre>ted_dataset = "https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&amp;filename=ted_en-20160408.zip"
get_data(ted_dataset, "data/ted_en.zip")</pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">Python empowers us to access files inside a <kbd>.zip</kbd> file, which is easy to do with the <kbd>zipfile</kbd> package. Notice it is the <kbd>zipfile.zipFile</kbd> syntax that enables this.</p>
<p class="prompt input_prompt">We additionally use the <kbd>lxml</kbd> package to <kbd>parse</kbd> the XML file inside the ZIP.</p>
<p class="prompt input_prompt">Here, we manually opened the file to find the relevant <kbd>content</kbd> path and look up <kbd>text()</kbd> from it. In this case, we are interested only in the subtitles and not any accompanying metadata, as follows:</p>
<div><div><div><pre>import zipfile<br/>import lxml.etree<br/>with zipfile.ZipFile('data/ted_en.zip', 'r') as z:<br/>    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))<br/>input_text = '\n'.join(doc.xpath('//content/text()'))<br/></pre></div>
</div>
</div>
</div>
</div>
<div><div><p class="prompt input_prompt">Let's now preview the first 500 characters of the following <kbd>input_text</kbd>:</p>
<div><div><div><pre>input_text[:500]<br/>&gt; "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\nConsider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. A"</pre></div>
</div>
</div>
</div>
</div>
<p class="mce-root">Since we are using subtitles from TED talks, there are some fillers that are not useful. These are often words describing sounds in parentheses and the speaker's name.</p>
<p class="mce-root">Let's remove these fillers using some regex, as follows:</p>
<pre>import re
# remove parenthesis 
input_text_noparens = re.sub(r'\([^)]*\)', '', input_text)

# store as list of sentences
sentences_strings_ted = []
for line in input_text_noparens.split('\n'):
    m = re.match(r'^(?:(?P&lt;precolon&gt;[^:]{,20}):)?(?P&lt;postcolon&gt;.*)$', line)
    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)

# store as list of lists of words
sentences_ted = []
for sent_str in sentences_strings_ted:
    tokens = re.sub(r"[^a-z0-9]+", " ", sent_str.lower()).split()
    sentences_ted.append(tokens)</pre>
<p>Notice that we created <kbd>sentence_strings_ted</kbd> using the <kbd>.split('\n')</kbd> syntax on our entire corpus. Replace this with a better sentence tokenizer, such as that from spaCy or NLTK, as a reader exercise:</p>
<pre>print(sentences_ted[:2])</pre>
<p class="mce-root">Notice that each <kbd>sentences_ted</kbd> is now a list of a lists. Each element of the first list is a sentence, and each sentence is a list of tokens (words).</p>
<p class="mce-root">This is the expected structure for training text embeddings using Gensim. We will write the following code to disk for easy retrieval later:</p>
<div><div><div><div><div><pre>import json<br/># with open('ted_clean_sentences.json', 'w') as fp:<br/>#     json.dump(sentences_ted, fp)<br/><br/>with open('ted_clean_sentences.json', 'r') as fp:<br/>    sentences_ted = json.load(fp)<br/></pre></div>
</div>
</div>
</div>
</div>
<p>I personally prefer JSON serialization over Pickle because it's slightly faster, more inter-operable among languages, and, most importantly, human readable.</p>
<p>Let's now train both fastText and word2vec embedding over this small corpus. Although small, the corpus we are using is representative of the data sizes usually seen in practice. Large annotated text corpora are extremely rare in the industry.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Training fastText embedddings</h1>
                
            
            
                
<p>Setting up imports is actually quite simple in the new Gensim API; just use the following code:</p>
<pre class="mce-root">from gensim.models.fasttext import FastText</pre>
<p>The next step is to feed the text and make our text embedding model, as follows:</p>
<pre class="mce-root">fasttext_ted_model = FastText(sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)<br/> # sg = 1 denotes skipgram, else CBOW is used</pre>
<p class="mce-root">You will probably noticed the parameters we pass to make our model. The following list explains these parameters, as explained in the Gensim documentation:</p>
<ul>
<li><kbd>min_count (int, optional)</kbd>: The model ignores all words with total frequency lower than this</li>
<li><kbd>size (int, optional)</kbd>: This represents the dimensionality of word vectors</li>
<li><kbd>window (int, optional)</kbd>: This represents the maximum distance between the current and predicted word within a sentence</li>
<li><kbd>workers (int, optional)</kbd>: Use these many worker threads to train the model (this enables faster training with multicore machines; <kbd>workers=-1</kbd> means using one worker for each core available in your machine)</li>
<li><kbd>sg ({1, 0}, optional)</kbd>: This is a training algorithm, <kbd>skip-gram if sg=1</kbd> or CBOW</li>
</ul>
<p>The preceding parameters are actually part of a larger list of levers that can move around to improve the quality of your text embedding. We encourage you to play around with the numbers in addition to exploring the other parameters that the Gensim API exposes.</p>
<p>Let's now take a quick peek at the words most similar to India in this corpus, as ranked by fastText embedding-based similarity, as follows:</p>
<pre class="mce-root">fasttext_ted_model.wv.most_similar("india")<br/><br/>[('indians', 0.5911639928817749),<br/> ('indian', 0.5406097769737244),<br/> ('indiana', 0.4898717999458313),<br/> ('indicated', 0.4400438070297241),<br/> ('indicate', 0.4042605757713318),<br/> ('internal', 0.39166826009750366),<br/> ('interior', 0.3871103823184967),<br/> ('byproducts', 0.3752930164337158),<br/> ('princesses', 0.37265270948410034),<br/> ('indications', 0.369659960269928)]</pre>
<p>Here, we notice that fastText has leveraged the sub-word structure, such as <kbd>ind</kbd>, <kbd>ian</kbd>, and <kbd>dian</kbd>, to rank the words. We get both <kbd>indians</kbd> and <kbd>indian</kbd> in the top 3, which is quite good. This is one of the reasons fastText is effective—even for small training text tasks.</p>
<p>Let's now repeat the same process using word2vec and look at the words most similar to <kbd>india</kbd> there.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Training word2vec embeddings</h1>
                
            
            
                
<p>Importing the model is simple, simply use the following command. By now, you should have an intuitive feel of how the Gensim model's API is structured:</p>
<pre class="mce-root">from gensim.models.word2vec import Word2Vec</pre>
<p>Here, we are using an identical configuration for the word2vec model as we did for fastText. This helps to reduce bias in the comparison.</p>
<p>You are encouraged to compare the best fastText model to the best word2vec model with the following:</p>
<pre class="mce-root">word2vec_ted_model = Word2Vec(sentences=sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)</pre>
<p>Right, let's now look at the words most similar to <kbd>india</kbd>, as follows:</p>
<pre class="mce-root">word2vec_ted_model.wv.most_similar("india")<br/><br/>[('cent', 0.38214215636253357),<br/> ('dichotomy', 0.37258434295654297),<br/> ('executing', 0.3550642132759094),<br/> ('capabilities', 0.3549191951751709),<br/> ('enormity', 0.3421599268913269),<br/> ('abbott', 0.34020164608955383),<br/> ('resented', 0.33033430576324463),<br/> ('egypt', 0.32998529076576233),<br/> ('reagan', 0.32638251781463623),<br/> ('squeezing', 0.32618749141693115)]</pre>
<p>The words most similar to <kbd>india</kbd> have no tangible relation to the original word. For this particular dataset, and word2vec's training configuration, the model has not captured any semantic or syntactic information at all. This is not unusual since word2vec is meant to work on large text corpora.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">fastText versus word2vec</h1>
                
            
            
                
<p class="mce-root">According to the following preliminary comparison by Gensim:</p>
<p class="mce-root"><q>fastText embeddings are significantly better than word2vec at encoding syntactic information. This is expected, since most syntactic analogies are morphology based, and the char n-gram approach of fastText takes such information into account. The original word2vec model seems to perform better on semantic tasks, since words in semantic analogies are unrelated to their char n-grams, and the added information from irrelevant char n-grams worsens the embeddings.</q></p>
<p class="mce-root">The source for this is: <em>word2vec fasttext comparison notebook</em> (<a href="https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb">https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb</a>).</p>
<p class="mce-root">In general, we prefer fastText because of its innate ability to handle words that it has not seen in training. It is definitely better than word2vec when working with small data (as we've shown), and is at least as good as word2vec on larger datasets.</p>
<p class="mce-root">fastText is also useful in cases where we are processing text riddled with spelling mistakes. For example, it can leverage sub-word similarity to bring <kbd>indian</kbd> and <kbd>indain</kbd> close in the embedding space.</p>
<p>In most downstream tasks, such as sentiment analysis or text classification, we continue to recommend GloVe over word2vec.</p>
<p>The following is our recommended rule of thumb for text embedding applications: fastText &gt; GloVe &gt; word2vec.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Document embedding</h1>
                
            
            
                
<p>Document embedding is often considered an underrated way of doing things. The key idea in document embedding is to compress an entire document, for example a patent or customer review, into one single vector. This vector in turn can be used for a lot of downstream tasks.</p>
<p>Empirical results show that document vectors outperform bag-of-words models as well as other techniques for text representation.</p>
<p>Among the most useful downstream tasks is the ability to cluster text. Text clustering has several uses, ranging from data exploration to online classification of incoming text in a pipeline.</p>
<p>In particular, we are interested in document modeling using doc2vec on a small dataset. Unlike sequence models such as RNN, where a word sequence is captured in generated sentence vectors, doc2vec sentence vectors are word order independent. This word order independence means that we can process a large number of examples quickly, but it does mean capturing less of a sentence's inherent meaning.<br/></p>
<p>This section is loosely based on the doc2Vec API Tutorial from the Gensim repository.</p>
<p>Let's first get the imports out of the way with the following code:</p>
<pre>from gensim.models.doc2vec import Doc2Vec, TaggedDocument<br/>import gensim<br/>from pprint import pprint<br/>import multiprocessing</pre>
<p>Now, let's pull out the talks from the <kbd>doc</kbd> variable we used earlier, as follows:</p>
<pre>talks = doc.xpath('//content/text()')</pre>
<p>To train the Doc2Vec model, each text sample needs a label or unique identifier. To do this, write a small function like the following:</p>
<pre>def read_corpus(talks, tokens_only=False):<br/>    for i, line in enumerate(talks):<br/>        if tokens_only:<br/>            yield gensim.utils.simple_preprocess(line)<br/>        else:<br/>            # For training data, add tags<br/>            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>There are a few things happening inside the preceding function; they are as follows:</p>
<ul>
<li><strong>Overloaded if condition</strong>: This reads a test corpora and sets <kbd>tokens_only</kbd> to <kbd>True</kbd>.</li>
<li><strong>Target Label:</strong> This assigns an arbitrary index variable, <kbd>i</kbd>, as the target label.</li>
<li><kbd><strong>gensim.utils.simple_preprocess</strong></kbd>: This converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long, which then yields instances of <kbd>TaggedDocument</kbd>. Since we are yielding instead of returning, this entire function is acting as a generator.</li>
</ul>
<p>It is worth mentioning how this changes the function behavior. With a <kbd>return</kbd> in use, when a function is called it would have returned a specific object, such as <kbd>TaggedDocument</kbd> or <kbd>None</kbd> if the return is not specified. A <kbd>generator</kbd> function, on the other hand, only returns a <kbd>generator</kbd> object.</p>
<p>So, what do you expect the following code line to return?</p>
<pre>read_corpus(talks)</pre>
<p>If you guessed correctly, you'll know we expect it to return a <kbd>generator</kbd> object, as follows:</p>
<div><pre>&lt;generator object read_corpus at 0x0000024741DBA990&gt;</pre></div>
<p>The preceding object means that we can read the text corpus element by element as and when it's needed. This is exceptionally useful if a training corpus is larger than your memory size.</p>
<p>Understand how Python iterators and generators work. They make your code memory efficient and easy to read.</p>
<p>In this particular case, we have a rather small training corpus as an example, so let's read this entire corpus into working memory as a list of <kbd>TaggedDocument</kbd> objects, as follows:</p>
<pre>ted_talk_docs = list(read_corpus(talks))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The <kbd>list()</kbd> statement runs over the entire corpora until the function stops yielding. Our variable <kbd>ted_talk_docs</kbd> should look something like the following:</p>
<pre>ted_talk_docs[0]<br/><br/>TaggedDocument(words=['here', 'are', 'two', 'reasons', 'companies', 'fail', ...., 'you', 'already', 'know', 'don', 'forget', 'the', 'beauty', 'is', 'in', 'the', 'balance', 'thank', 'you', 'applause'], tags=[0])</pre>
<p>Let's quickly take a look at how many cores this machine has. We will use the following code to initialize the doc2vec model:</p>
<pre>cores = multiprocessing.cpu_count()<br/>print(cores)<br/>8</pre>
<p>Let's now go and initialize our doc2vec model from Gensim.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the doc2vec API</h1>
                
            
            
                
<pre class="mce-root">model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, iter=5, workers=cores)</pre>
<p>Let's quickly understand the flags we have used in the preceding code:</p>
<ul>
<li><kbd>dm ({1,0}, optional)</kbd>: This defines the training algorithm; if <kbd>dm=1</kbd>, <em>distributed memory</em> (PV-DM) is used; otherwise, a distributed bag of words (PV-DBOW) is employed</li>
<li><kbd>size (int, optional)</kbd>: This is the dimensionality of feature vectors</li>
<li><kbd>window (int, optional)</kbd>: This represents the maximum distance between the current and predicted word within a sentence</li>
<li><kbd>negative (int, optional)</kbd>: If &gt; <kbd>0</kbd>, negative sampling will be used (the int for negative values specifies how many <em>noise words</em> should be drawn, which is usually between 5-20); if set to <kbd>0</kbd>, no negative sampling is used</li>
<li><kbd>hs ({1,0}, optional)</kbd>: If <kbd>1</kbd>, hierarchical softmax will be used for model training, and if set to 0 where the negative is non-zero, negative sampling will be used</li>
<li><kbd>iter (int, optional)</kbd>: This represents the number of iterations (epochs) over the corpus</li>
</ul>
<p>The preceding list has been taken directly from the Gensim documentation. With that in mind, we'll now move on and explain some of the new terms introduced here, including negative sampling and hierarchical softmax.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Negative sampling</h1>
                
            
            
                
<p>Negative sampling started out as a hack to speed up training and is now a well-accepted practice. The click point here is that in addition to training your model on what might be the correct answer, why not show it a few examples of wrong answers?</p>
<p>In particular, using negative sampling speeds up training by reducing the number of model updates required. Instead of updating the model for every single wrong word, we pick a small number, usually between 5 and 25, and train the model on them. So, we have reduced the number of updates from a few million, which is required for training on a large corpus, to a much smaller number. This is a classic software programming hack that works in academia too.</p>
<p> </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hierarchical softmax</h1>
                
            
            
                
<p>The denominator term in our usual softmax is calculated using the sum operator over a large number of words. This normalization is a very expensive operation to do at each update during training.</p>
<p>Instead, we can break this down into a specific sequence of calculations, which saves us from having to calculate expensive normalization over all words. This means that for each word, we use an approximation of sorts.</p>
<p>In practice, this approximation has worked so well that some systems use this in both training and inference time. For training, it can give a speed of up to 50x (as per Sebastian Ruder, an NLP research blogger). In my own experiments, I have seen speed gains of around 15-25x.</p>
<pre class="mce-root">model.build_vocab(ted_talk_docs)</pre>
<p>The API to train a doc2vec model is slightly different. We use the <kbd>build_vocab</kbd> API first to build the vocabulary from a sequence of sentences, as shown in the previous snippet. We also pass our memory variable <kbd>ted_talk_docs</kbd> here, but we could have passed our once-only generator stream from the <kbd>read_corpora</kbd> function as well.</p>
<p>Let's now set up some of the following sample sentences to find out whether our model learns something or not:</p>
<pre class="mce-root">sentence_1 = 'Modern medicine has changed the way we think about healthcare, life spans and by extension career and marriage'<br/><br/>sentence_2 = 'Modern medicine is not just a boon to the rich, making the raw chemicals behind these is also pollutes the poorest neighborhoods'<br/><br/>sentence_3 = 'Modern medicine has changed the way we think about healthcare, and increased life spans, delaying weddings'</pre>
<p class="mce-root">Gensim has an interesting API that allows us to find a similarity value between two unseen documents using the model we just updated with our vocabulary, as follows:</p>
<pre class="mce-root">model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_3.split())<br/>&gt; -0.18353473068679<br/><br/>model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_2.split())<br/>&gt; -0.08177642293252027</pre>
<p class="mce-root">The preceding output doesn't quite make sense, does it? The sentences we wrote should have some reasonable degree of similarity that is definitely not negative.</p>
<p>A-ha! We forgot to train the model on our corpora. Let's do that now with the following code and then repeat the previous comparisons to see how they have changed:</p>
<pre>%time model.train(ted_talk_docs, total_examples=model.corpus_count, epochs=model.epochs)<br/>Wall time: 6.61 s</pre>
<p>On a machine with BLAS set up, this step should take less than a few seconds.</p>
<p>We can actually pull out raw inference vectors for any particular sentence based on the following model:</p>
<pre>model.infer_vector(sentence_1.split())<br/><br/>array([-0.03805782,  0.09805363, -0.07234333,  0.31308332,  0.09668373,<br/>       -0.01471598, -0.16677614, -0.08661497, -0.20852503, -0.14948   ,<br/>       -0.20959479,  0.17605443,  0.15131783, -0.17354141, -0.20173495,<br/>        0.11115499,  0.38531387, -0.39101505,  0.12799   ,  0.0808568 ,<br/>        0.2573657 ,  0.06932276,  0.00427534, -0.26196653,  0.23503092,<br/>        0.07589306, -0.01828301,  0.38289976, -0.04719075, -0.19283117,<br/>        0.1305226 , -0.1426582 , -0.05023642, -0.11381021,  0.04444459,<br/>       -0.04242943,  0.08780348,  0.02872207, -0.23920575,  0.00984556,<br/>        0.0620702 , -0.07004016,  0.15629964,  0.0664391 ,  0.10215732,<br/>        0.19148728, -0.02945088,  0.00786009, -0.05731675, -0.16740018,<br/>       -0.1270729 ,  0.10185472,  0.16655563,  0.13184668,  0.18476236,<br/>       -0.27073956, -0.04078012, -0.12580603,  0.02078131,  0.23821649,<br/>        0.09743162, -0.1095973 , -0.22433399, -0.00453655,  0.29851952,<br/>       -0.21170728,  0.1928157 , -0.06223159, -0.044757  ,  0.02430432,<br/>        0.22560015, -0.06163954,  0.09602281,  0.09183675, -0.0035969 ,<br/>        0.13212039,  0.03829316,  0.02570504, -0.10459486,  0.07317936,<br/>        0.08702451, -0.11364868, -0.1518436 ,  0.04545208,  0.0309107 ,<br/>       -0.02958601,  0.08201223,  0.26910907, -0.19102073,  0.00368607,<br/>       -0.02754402,  0.3168101 , -0.00713515, -0.03267708, -0.03792975,<br/>        0.06958092, -0.03290432,  0.03928463, -0.10203536,  0.01584929],<br/>      dtype=float32)</pre>
<p>Here, the <kbd>infer_vector</kbd> API expects a list of tokens as an input. This should explain why we could have used <kbd>read_corpora</kbd> with <kbd>tokens_only =True</kbd> here as well.</p>
<p>Now that our model is trained, let's compare the following sentences again:</p>
<pre class="mce-root">model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_3.split())<br/>0.9010817740272721<br/><br/>model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_2.split())<br/>0.7461058869759862</pre>
<p>The new preceding output makes sense. The first and third sentences are definitely more similar than the first and second. In the spirit of exploring, let's now see how similar the second and third sentences are, as follows:</p>
<pre>model.docvecs.similarity_unseen_docs(model, sentence_2.split(), sentence_3.split())<br/>0.8189999598358203</pre>
<p>Ah, this is better. Our result is now consistent with our expectations. The similarity value is more than the first and second sentences, but less than that of the first and third, which were also almost identical in intent.</p>
<p class="mce-root"/>
<p>As an anecdotal observation or heuristic, truly similar sentences have a value greater than 0.8 on the similarity scale.</p>
<p>We have mentioned how document or text vectors in general are a good way of exploring a data corpus. Next, we will do that to explore our corpus in a very shallow manner before leaving you with some ideas on how to continue the exploration.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data exploration and model evaluation</h1>
                
            
            
                
<p class="mce-root">One simple technique for assessing any vectorization method is to simply use the training corpus as the test corpus. Of course, we expect that we will overfit our model to the training set, but that's fine.</p>
<p class="mce-root">We can use the training corpus as a test corpus by doing the following:</p>
<ul>
<li class="mce-root">Learning a new result or <em>inference</em> vectors for each document</li>
<li class="mce-root">Comparing the vector to all examples</li>
<li class="mce-root">Ranking the document, sentence, and paragraph vectors according to the similarity score</li>
</ul>
<p class="mce-root">Let's do this in code, as follows:</p>
<pre class="mce-root">ranks = []<br/>for idx in range(len(ted_talk_docs)):<br/>    inferred_vector = model.infer_vector(ted_talk_docs[idx].words)<br/>    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))<br/>    rank = [docid for docid, sim in sims].index(idx)<br/>    ranks.append(rank)</pre>
<p class="mce-root">We have now figured out where each document placed itself in the rank. So, if the highest rank is the document itself, that's good enough. As we said, we might overfit a little on the training corpus, but it's a good sanity test nonetheless. We can find this using the frequency count via <kbd>Counter</kbd> as follows:</p>
<pre class="mce-root">import collections<br/>collections.Counter(ranks) # Results vary due to random seeding + very small corpus<br/>Counter({0: 2079, 1: 2, 4: 1, 5: 2, 2: 1})</pre>
<p class="mce-root">The <kbd>Counter</kbd> object tells us how many documents found themselves at what ranks. So, 2079 documents found themselves first (index 0), but two documents each found themselves second (index 1) and sixth (index 5) ranks. There is one document that ranked fifth (index 4) and third (index 2) respectively. All in all, this is a very good training performance, because 2079 out of 2084 documents ranked themselves first.</p>
<p>This helps us understand that the vectors did represent information in the document in a meaningful manner. If they did not, we would see a lot more rank dispersal.</p>
<p>Let's now quickly take a single document and find the most similar document to it, the least similar document, and a document that is somewhat in between in similarity. Do this with the following code:</p>
<pre class="mce-root">doc_slice = ' '.join(ted_talk_docs[idx].words)[:500]<br/>print(f'Document ({idx}): «{doc_slice}»\n')<br/>print(f'SIMILAR/DISSIMILAR DOCS PER MODEL {model}')<br/>for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:<br/>      doc_slice = ' '.join(ted_talk_docs[sims[index][0]].words)[:500]<br/>      print(f'{label} {sims[index]}: «{doc_slice}»\n')<br/><br/></pre>
<p class="mce-root">Notice how we are choosing to preview a slice of the entire document for exploration. You are free to either do this or use a small text summarization tool to create your preview on the fly instead.</p>
<p>The results are as follows:</p>
<pre class="mce-root">Document (2084): «if you re here today and very happy that you are you've all heard about how sustainable development will save us from ourselves however when we're not at ted we're often told that real sustainability policy agenda is just not feasible especially in large urban areas like new york city and that because most people with decision making powers in both the public and the private sector really don't feel as though they are in danger the reason why here today in part is because of dog an abandoned puppy»<br/><br/>SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)<br/> MOST (2084, 0.893369197845459): «if you are here today and very happy that you are you've all heard about how sustainable development will save us from ourselves however when we are not at ted we are often told that real sustainability policy agenda is just not feasible especially in large urban areas like new york city and that because most people with decision making powers in both the public and the private sector really don feel as though they re in danger the reason why here today in part is because of dog an abandoned puppy»<br/><br/>MEDIAN (1823, 0.42069244384765625): «so going to talk today about collecting stories in some unconventional ways this is picture of me from very awkward stage in my life you might enjoy the awkwardly tight cut off pajama bottoms with balloons anyway it was time when was mainly interested in collecting imaginary stories so this is picture of me holding one of the first watercolor paintings ever made and recently I've been much more interested in collecting stories from reality so real stories and specifically interested in collecting »<br/><br/>LEAST (270, 0.12334088981151581): «on june precisely at in balmy winter afternoon in so paulo brazil typical south american winter afternoon this kid this young man that you see celebrating here like he had scored goal juliano pinto years old accomplished magnificent deed despite being paralyzed and not having any sensation from mid chest to the tip of his toes as the result of car crash six years ago that killed his brother and produced complete spinal cord lesion that left juliano in wheelchair juliano rose to the occasion and»</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter was more than an introduction to the Gensim API. We now know how to load pre-trained GloVe vectors, and you can use these vector representations instead of TD-IDF in any machine learning model.</p>
<p>We looked at why fastText vectors are often better than word2vec vectors on a small training corpus, and learned that you can use them with any ML models.</p>
<p>We learned how to build doc2vec models. You can now extend this doc2vec approach to build sent2vec or paragraph2vec style models as well. Ideally, paragraph2vec will change, simply because each document will be a paragraph instead.</p>
<p>In addition, we now know how we can quickly perform sanity checks on our doc2vec vectors without using an annotated test corpora. We did this by checking the rank dispersal metric.</p>


            

            
        
    </body></html>