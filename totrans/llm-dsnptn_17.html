<html><head></head><body><div><div><div><h1 id="_idParaDest-213" class="chapter-number"><a id="_idTextAnchor276"/>17</h1>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor277"/>Fairness and Bias Detection</h1>
			<p>Fairness in LLMs involves ensuring that the model’s outputs and decisions do not discriminate against or unfairly treat individuals or groups based on protected attributes such as race, gender, age, or religion. It’s a complex concept that goes beyond just avoiding explicit bias.</p>
			<p>There are several definitions of fairness in machine learning:</p>
			<ul>
				<li><strong class="bold">Demographic parity</strong>: The probability of a positive outcome should be the same for all groups</li>
				<li><strong class="bold">Equal opportunity</strong>: The true positive rates should be the same for all groups</li>
				<li><strong class="bold">Equalized odds</strong>: Both true positive and false positive rates should be the same for all groups</li>
			</ul>
			<p>For LLMs, fairness often involves ensuring that the model’s language generation and understanding capabilities are equitable across different demographic groups and do not perpetuate or amplify societal bias.</p>
			<p>In this chapter, you’ll learn about different types of bias that can emerge in LLMs and techniques for detecting them.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Types of bias</li>
				<li>Fairness metrics for LLM text generation and understanding</li>
				<li>Detecting bias</li>
				<li>Debiasing strategies</li>
				<li>Fairness-aware training</li>
				<li>Ethical considerations</li>
			</ul>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor278"/>Types of bias</h1>
			<p>LLMs can <a id="_idIndexMarker840"/>exhibit various types of bias:</p>
			<ul>
				<li><strong class="bold">Representation bias</strong>: The underrepresentation or misrepresentation of certain <a id="_idIndexMarker841"/>groups in training data—for example, a facial <a id="_idIndexMarker842"/>recognition system trained primarily on lighter-skinned faces may exhibit significantly higher error rates when identifying individuals with darker skin tones, due to inadequate representation in the training set.</li>
				<li><strong class="bold">Linguistic bias</strong>: The language <a id="_idIndexMarker843"/>used by AI systems to <a id="_idIndexMarker844"/>describe different groups—for instance, an AI system as may label men as “assertive” and women as “aggressive when referring to the same behaviors across genders, reinforcing subtle discriminatory patterns.</li>
				<li><strong class="bold">Allocation bias</strong>: The <a id="_idIndexMarker845"/>unfair distribution of resources <a id="_idIndexMarker846"/>or opportunities based on model predictions, as seen when an automated hiring system systematically ranks candidates from certain universities higher, regardless of their qualifications, thereby disproportionately allocating interview opportunities to graduates from these institutions.</li>
				<li><strong class="bold">Quality of service bias</strong>: Variations in model performance across different groups, as <a id="_idIndexMarker847"/>illustrated by a machine translation system <a id="_idIndexMarker848"/>that provides significantly more accurate translations for mainstream languages like English, Spanish, and Mandarin while delivering lower-quality translations for languages with fewer speakers or less representation in the training data.</li>
				<li><strong class="bold">Stereotypical bias</strong>: The <a id="_idIndexMarker849"/>reinforcement of societal <a id="_idIndexMarker850"/>stereotypes through language generation, as demonstrated when an AI writing assistant automatically suggests stereotypical career paths when completing stories about characters of different backgrounds – suggesting careers in sports or entertainment for characters from certain racial backgrounds while suggesting professional careers like doctors or lawyers for others.</li>
				<li><strong class="bold">Explicit and implicit bias</strong>: Explicit bias in LLMs arises from overt patterns in training data, such as <a id="_idIndexMarker851"/>stereotypes present in text sources, leading <a id="_idIndexMarker852"/>to clearly identifiable bias in outputs. Implicit bias, on the other hand, is more subtle and emerges from underlying <a id="_idIndexMarker853"/>statistical correlations in data, shaping responses in ways that may <a id="_idIndexMarker854"/>reinforce hidden bias without direct intention. While explicit bias can often be detected and mitigated through filtering or fine-tuning, implicit bias is harder to identify and requires deeper intervention, such as bias-aware training techniques and regular auditing of model outputs.</li>
				<li><strong class="bold">Hidden bias</strong>: Hidden bias in LLMs arises when training data, model design, or deployment <a id="_idIndexMarker855"/>choices subtly skew responses, reinforcing stereotypes or <a id="_idIndexMarker856"/>excluding perspectives. This can manifest in gendered language, cultural favoritism, or political slants, often due to overrepresented viewpoints in training data. Algorithmic processing can further amplify these biases, making responses inconsistent or skewed based on prompt phrasing. To mitigate this, diverse datasets, bias audits, and ethical fine-tuning are essential, ensuring models generate balanced and fair outputs while allowing user-aware adjustments within ethical constraints.</li>
			</ul>
			<p>Here’s an example of how to check for representation bias in a dataset (we will just show one example to limit the size of this chapter):</p>
			<pre class="source-code">
import pandas as pd
from collections import Counter
def analyze_representation(texts, attribute_list):
    attribute_counts = Counter()
    for text in texts:
        for attribute in attribute_list:
            if attribute.lower() in text.lower():
                attribute_counts[attribute] += 1
    total = sum(attribute_counts.values())
    percentages = {attr: count/total*100
        for attr, count in attribute_counts.items()}
    return pd.DataFrame({
        'Attribute': percentages.keys(),
        'Percentage': percentages.values()
    }).sort_values('Percentage', ascending=False)
# Example usage
texts = [
    "The CEO announced a new policy.",
    "The nurse took care of the patient.",
    "The engineer designed the bridge.",
    # ... more texts
]
gender_attributes = ['he', 'she', 'his', 'her', 'him', 'her']
representation_analysis = analyze_representation(
    texts, gender_attributes
)
print(representation_analysis)</pre>			<p>This code analyzes the representation of gender-related terms in a corpus of texts, which can help identify potential gender bias in the dataset.</p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor279"/>Fairness metrics for LLM text generation and understanding</h1>
			<p>Fairness <a id="_idIndexMarker857"/>metrics often focus on comparing model performance or outputs across different demographic groups.</p>
			<p>Here <a id="_idIndexMarker858"/>are some examples:</p>
			<ul>
				<li><strong class="bold">Demographic parity difference for text classification</strong>: This metric measures the difference in positive prediction rates between the most and least favored groups:<pre class="source-code">
from sklearn.metrics import confusion_matrix
import numpy as np
def demographic_parity_difference(
    y_true, y_pred, protected_attribute
):
    groups = np.unique(protected_attribute)
    dps = []
    for group in groups:
        mask = protected_attribute == group
        cm = confusion_matrix(y_true[mask], y_pred[mask])
        dp = (cm[1, 0] + cm[1, 1]) / cm.sum()
        dps.append(dp)
    return max(dps) - min(dps)
# Example usage
y_true = [0, 1, 1, 0, 1, 0, 1, 1]
y_pred = [0, 1, 0, 0, 1, 1, 1, 1]
protected_attribute = ['A', 'A', 'B', 'B', 'A', 'B', 'A', 'B']
dpd = demographic_parity_difference(
    y_true, y_pred, protected_attribute
)
print(f"Demographic Parity Difference: {dpd}")</pre><p class="list-inset">The <a id="_idIndexMarker859"/>code defines a <code>demographic_parity_difference</code> function that computes the difference <a id="_idIndexMarker860"/>in demographic parity between groups defined by a protected attribute. It takes true labels (<code>y_true</code>), predicted labels (<code>y_pred</code>), and the protected attribute values as input. For each unique group in the protected attribute, it creates a Boolean mask to isolate the corresponding subset of predictions and computes the confusion matrix for that group. The demographic parity (DP) for each group is then calculated as the proportion of positive predictions—true or false—out of all predictions for that group, specifically using <code>(cm[1, 0] + cm[1, 1]) / cm.sum()</code>, which corresponds to the number of actual positives (both misclassified and correctly classified) over the total. It stores these DP values and finally returns the maximum difference between them, indicating the disparity in treatment across groups. The example demonstrates this using dummy data, printing out the DP difference between groups <code>'A'</code> and <code>'B'</code>.</p></li>				<li><strong class="bold">Equal opportunity difference for text classification</strong>: This metric measures <a id="_idIndexMarker861"/>the difference in true positive rates between the most and least favored groups:<pre class="source-code">
def equal_opportunity_difference(
    y_true, y_pred, protected_attribute
):
    groups = np.unique(protected_attribute)
    tprs = []
    for group in groups:
        mask = (protected_attribute == group) &amp; (y_true == 1)
        tpr = np.mean(y_pred[mask] == y_true[mask])
        tprs.append(tpr)
    return max(tprs) - min(tprs)
# Example usage
eod = equal_opportunity_difference(y_true, y_pred,
    protected_attribute)
print(f"Equal Opportunity Difference: {eod}")</pre><p class="list-inset">This code calculates the difference in true positive rates between groups defined by a protected attribute, measuring how equally the model correctly identifies positive cases across those groups.</p></li>			</ul>
			<p>Now that we’ve explored a couple of metrics for measuring fairness in model outputs and understanding capabilities, we’ll move on to learning techniques to actually detect bias in practice, building on these metrics to develop systematic testing approaches.</p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor280"/>Detecting bias</h1>
			<p>Detecting bias <a id="_idIndexMarker862"/>in LLMs often involves analyzing model outputs across different demographic groups or for different types of inputs. Here are some techniques:</p>
			<ul>
				<li><strong class="bold">Word embeddings</strong>: This <a id="_idIndexMarker863"/>code measures <a id="_idIndexMarker864"/>gender bias in word embeddings by comparing the projection of profession words onto the gender direction:<pre class="source-code">
from gensim.models import KeyedVectors
import numpy as np
def word_embedding_bias(
    model, male_words, female_words, profession_words
):
    male_vectors = [model[word] for word in male_words if word in model.key_to_index]
    female_vectors = [model[word] for word in female_words
        if word in model.key_to_index]
    male_center = np.mean(male_vectors, axis=0)
    female_center = np.mean(female_vectors, axis=0)
    gender_direction = male_center - female_center
    biases = []
    for profession in profession_words:
        if profession in model.key_to_index:
            bias = np.dot(model[profession], gender_direction)
            biases.append((profession, bias))
    return sorted(biases, key=lambda x: x[1], reverse=True)
# Example usage
model = KeyedVectors.load_word2vec_format(
    'path_to_your_embeddings.bin', binary=True
)
male_words = ['he', 'man', 'boy', 'male', 'gentleman']
female_words = ['she', 'woman', 'girl', 'female', 'lady']
profession_words = ['doctor', 'nurse', 'engineer', 'teacher',
    'CEO']
biases = word_embedding_bias(
    model, male_words, female_words, profession_words
)
for profession, bias in biases:
    print(f"{profession}: {bias:.4f}")</pre><p class="list-inset">This code measures gender bias in word embeddings by first creating average vectors for male and female terms, calculating a gender direction vector between them, and then measuring how closely different profession words align with <a id="_idIndexMarker865"/>this gender axis through dot product calculations. The function returns professions sorted by their bias score, where positive <a id="_idIndexMarker866"/>values indicate male association and negative values indicate female association, allowing users to quantify gender stereotypes embedded in the language model.</p></li>				<li><strong class="bold">Sentiment analysis</strong>: You <a id="_idIndexMarker867"/>can analyze <a id="_idIndexMarker868"/>sentiment across different groups to detect potential bias:<pre class="source-code">
from transformers import pipeline
def analyze_sentiment_bias(
    texts, groups,
model_name="distilbert-base-uncased-finetuned-sst-2-english"):
    sentiment_analyzer = pipeline(
        "sentiment-analysis", model=model_name
    )
    results = {group: {'positive': 0, 'negative': 0}
        for group in set(groups)}
    for text, group in zip(texts, groups):
        sentiment = sentiment_analyzer(text)[0]
        results[group][sentiment['label'].lower()] += 1
    for group in results:
        total = results[group]['positive'] \
            + results[group]['negative']
        results[group]['positive_ratio'] = \
            results[group]['positive'] / total
    return results
# Example usage
texts = [
    "The man is very intelligent.",
    "The woman is very intelligent.",
    "The man is a great leader.",
    "The woman is a great leader.",
]
groups = ['male', 'female', 'male', 'female']
bias_results = analyze_sentiment_bias(texts, groups)
print(bias_results)</pre><p class="list-inset">This code <a id="_idIndexMarker869"/>analyzes sentiment bias across different demographic <a id="_idIndexMarker870"/>groups by using a pre-trained sentiment analysis model from the <code>transformers</code> library. It takes a list of texts and their corresponding group labels, processes each text through a sentiment analyzer, and tallies positive and negative sentiment counts for each group. The function then calculates a “positive ratio” for each group (the proportion of texts classified as positive), allowing comparison of sentiment distribution across different groups. In the example, it’s specifically examining potential gender bias by analyzing how identical statements about intelligence and leadership <a id="_idIndexMarker871"/>are classified when attributed <a id="_idIndexMarker872"/>to men versus women, which could reveal if the underlying language model treats identical qualities differently based on gender association.</p></li>				<li><strong class="bold">Coreference resolution</strong>: You <a id="_idIndexMarker873"/>can analyze <a id="_idIndexMarker874"/>coreference resolution to detect potential occupation-gender bias:<pre class="source-code">
import spacy
def analyze_coreference_bias(texts, occupations, genders):
    nlp = spacy.load("en_core_web_sm")
    results = {gender: {occ: 0 for occ in occupations}
        for gender in genders}
    counts = {gender: 0 for gender in genders}
    for text in texts:
        doc = nlp(text)
        occupation = None
        gender = None
        for token in doc:
            if token.text.lower() in occupations:
                occupation = token.text.lower()
            if token.text.lower() in genders:
                gender = token.text.lower()
        if occupation and gender:
            results[gender][occupation] += 1
            counts[gender] += 1
    for gender in results:
        for occ in results[gender]:
            results[gender][occ] /= counts[gender]
    return results
# Example usage
texts = [
    "The doctor examined her patient. She prescribed some medication.",
    "The nurse took care of his patients. He worked a long shift.",
    # ... more texts
]
occupations = ['doctor', 'nurse', 'engineer', 'teacher']
genders = ['he', 'she']
bias_results = analyze_coreference_bias(texts, occupations,
    genders)
print(bias_results)</pre><p class="list-inset">The code <a id="_idIndexMarker875"/>defines an <code>analyze_coreference_bias</code> function that uses spaCy’s NLP pipeline to assess potential gender bias in text by analyzing how often specific gendered pronouns (like “he” and “she”) co-occur with certain occupations (e.g., “doctor”, “nurse”). It initializes a spaCy language model and creates a nested dictionary to count occurrences of each gender-occupation pair, as well as a separate count for each gender. For each input text, it tokenizes the content, identifies if any of the predefined occupations and gendered pronouns appear, and if both are present, it increments the relevant counters. After processing all texts, it normalizes the occupation counts for each gender by the total number of gender mentions, effectively yielding a proportion that reflects the relative association of each occupation with each gender in the given dataset. The function returns this normalized result, which is then printed in the example usage.</p></li>			</ul>
			<p>Next, we’ll build on this detection knowledge to explore practical strategies for reducing bias, helping us move from diagnosis to treatment.</p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor281"/>Debiasing strategies</h1>
			<p>Debiasing <a id="_idIndexMarker876"/>LLMs is an active area of research. Here are some strategies:</p>
			<ul>
				<li><strong class="bold">Data augmentation</strong> (see <a href="B31249_03.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>): In the following code, we augment the dataset <a id="_idIndexMarker877"/>by swapping gendered words, helping <a id="_idIndexMarker878"/>to balance gender representation:<pre class="source-code">
import random
def augment_data(texts, male_words, female_words):
    augmented_texts = []
    for text in texts:
        words = text.split()
        for i, word in enumerate(words):
            if word.lower() in male_words:
                female_equivalent = female_words[
                    male_words.index(word.lower())
                ]
                new_text = ' '.join(words[:i]
                    + [female_equivalent] + words[i+1:])
                augmented_texts.append(new_text)
            elif word.lower() in female_words:
                male_equivalent = male_words[
                    female_words.index(word.lower())
                ]
                new_text = ' '.join(words[:i]
                    + [male_equivalent] + words[i+1:])
                augmented_texts.append(new_text)
    return texts + augmented_texts
# Example usage
texts = [
    "The doctor examined his patient.",
    "The nurse took care of her patients.",
]
male_words = ['he', 'his', 'him']
female_words = ['she', 'her', 'her']
augmented_texts = augment_data(texts, male_words, female_words)
print(augmented_texts)</pre></li>				<li><strong class="bold">Bias fine-tuning</strong>: In the <a id="_idIndexMarker879"/>following code, we fine-tune <a id="_idIndexMarker880"/>a language model to replace biased words with more neutral alternatives:<pre class="source-code">
from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    TrainingArguments, Trainer)
import torch
def create_debiasing_dataset(biased_words, neutral_words):
    inputs = [f"The {biased} person" for biased in biased_words]
    targets = [f"The {neutral} person"
        for neutral in neutral_words]
    return inputs, targets
def fine_tune_for_debiasing(
    model, tokenizer, inputs, targets, epochs=3
):
    input_encodings = tokenizer(inputs, truncation=True,
        padding=True)
    target_encodings = tokenizer(targets, truncation=True,
        padding=True)
    dataset = torch.utils.data.TensorDataset(
        torch.tensor(input_encodings['input_ids']),
        torch.tensor(input_encodings['attention_mask']),
        torch.tensor(target_encodings['input_ids'])
    )
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=epochs,
        per_device_train_batch_size=8,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
    )
    trainer.train()
    return model
# Example usage
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
biased_words = ['bossy', 'emotional', 'hysterical']
neutral_words = ['assertive', 'passionate', 'intense']
inputs, targets = create_debiasing_dataset(
    biased_words, neutral_words
)
debiased_model = fine_tune_for_debiasing(
    model, tokenizer, inputs, targets
)</pre></li>			</ul>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor282"/>Fairness-aware training</h1>
			<p>Fairness constraints in machine learning are mathematical formulations that quantify and enforce <a id="_idIndexMarker881"/>specific notions of fairness by ensuring that model predictions maintain desired statistical properties across different demographic groups. These constraints typically express conditions such as demographic parity (equal positive prediction rates across groups), equalized odds (equal true positive and false positive rates), or individual fairness (similar individuals receive similar predictions). They can be incorporated directly into model optimization as regularization terms or enforced as post-processing steps. By explicitly modeling these constraints, developers can mitigate algorithmic bias and ensure more equitable outcomes across protected attributes like race, gender, or age—balancing the traditional goal of accuracy with ethical considerations about how predictive systems impact different populations.</p>
			<p>Incorporating fairness constraints directly into the training process can help produce fairer models. Here’s <a id="_idIndexMarker882"/>a simplified example:</p>
			<pre class="source-code">
import torch
import torch.nn as nn
import torch.optim as optim
class FairClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(FairClassifier, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)
def fair_loss(
    outputs, targets, protected_attributes, lambda_fairness=0.1
):
    criterion = nn.CrossEntropyLoss()
    task_loss = criterion(outputs, targets)
    # Demographic parity
    group_0_pred = outputs[protected_attributes == 0].mean(dim=0)
    group_1_pred = outputs[protected_attributes == 1].mean(dim=0)
    fairness_loss = torch.norm(group_0_pred - group_1_pred, p=1)
    return task_loss + lambda_fairness * fairness_loss
def train_fair_model(
    model, train_loader, epochs=10, lr=0.001,
    lambda_fairness=0.1
):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    for epoch in range(epochs):
        for inputs, targets, protected_attributes in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = fair_loss(
                outputs, targets,
                protected_attributes, lambda_fairness
            )
            loss.backward()
            optimizer.step()
        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')
    return model
# Example usage (assuming you have prepared your data)
input_size = 10
hidden_size = 50
num_classes = 2
model = FairClassifier(input_size, hidden_size, num_classes)
train_loader = ...  # Your DataLoader here
fair_model = train_fair_model(model, train_loader)</pre>			<p>This code implements a neural network classifier that aims to be fair with respect to protected <a id="_idIndexMarker883"/>attributes such as race or gender. The <code>FairClassifier</code> class defines a simple two-layer neural network, while the <code>fair_loss</code> function combines standard classification loss with a fairness constraint that penalizes the model when predictions differ between demographic groups. The <code>train_fair_model</code> function handles the training loop, applying this combined loss to optimize the model parameters while balancing accuracy and fairness.</p>
			<p>By incorporating a fairness penalty term in the loss function (weighted by <code>lambda_fairness</code>), the model is explicitly trained to make similar predictions across different protected groups, addressing potential bias. This represents a “constraint-based” approach to fair machine learning, where the fairness objective is directly incorporated into the optimization process rather than applied as a post-processing step. The trade-off between task performance and fairness can be tuned through the <code>lambda_fairness</code> hyperparameter.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor283"/>Ethical considerations</h1>
			<p>Developing <a id="_idIndexMarker884"/>fair and unbiased LLMs is not just a technical challenge but also an ethical imperative. Some key ethical considerations include the following:</p>
			<ul>
				<li><strong class="bold">Transparency</strong>: Be open about the model’s limitations and potential bias.</li>
				<li><strong class="bold">Diverse development teams</strong>: Ensure diverse perspectives in the development process to help identify and mitigate potential bias.</li>
				<li><strong class="bold">Regular auditing</strong>: Implement regular bias and fairness audits of your LLM throughout its life cycle.</li>
				<li><strong class="bold">Contextual deployment</strong>: Consider the specific context and potential impacts of deploying your LLM in different applications.</li>
				<li><strong class="bold">Ongoing research</strong>: Stay informed about the latest research in AI ethics and fairness and continuously work to improve your models.</li>
				<li><strong class="bold">User education</strong>: Educate users about the capabilities and limitations of your LLM, including potential bias.</li>
				<li><strong class="bold">Feedback mechanisms</strong>: Implement robust feedback mechanisms to identify and address unfair or biased outputs in deployed models. Keep in mind that feedback loops can reinforce bias by amplifying patterns in data, leading to self-perpetuating errors. If an AI system’s outputs influence future inputs—whether in content recommendations, hiring, or risk assessments—small biases can compound over time, narrowing diversity, reinforcing stereotypes, and skewing decision-making.</li>
			</ul>
			<p>Here’s an example of how you might implement a simple feedback system:</p>
			<pre class="source-code">
pythonCopyimport sqlite3
from datetime import datetime
class FeedbackSystem:
    def __init__(self, db_name='feedback.db'):
        self.conn = sqlite3.connect(db_name)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS feedback
            (id INTEGER PRIMARY KEY AUTOINCREMENT,
             model_output TEXT,
             user_feedback TEXT,
             timestamp DATETIME)
        ''')
        self.conn.commit()
    def record_feedback(self, model_output, user_feedback):
        self.cursor.execute('''
            INSERT INTO feedback (model_output, user_feedback, timestamp)
            VALUES (?, ?, ?)
        ''', (model_output, user_feedback, datetime.now()))
        self.conn.commit()
    def get_recent_feedback(self, limit=10):
        self.cursor.execute('''
            SELECT model_output, user_feedback, timestamp
            FROM feedback
            ORDER BY timestamp DESC
            LIMIT ?
        ''', (limit,))
        return self.cursor.fetchall()
    def close(self):
        self.conn.close()
# Example usage
feedback_system = FeedbackSystem()
# Simulating model output and user feedback
model_output = "The CEO made her decision."
user_feedback = "Biased: assumes CEO is female"
feedback_system.record_feedback(model_output, user_feedback)
# Retrieving recent feedback
recent_feedback = feedback_system.get_recent_feedback()
for output, feedback, timestamp in recent_feedback:
    print(f"Output: {output}")
    print(f"Feedback: {feedback}")
    print(f"Time: {timestamp}")
    print()
feedback_system.close()</pre>			<p>This code sets up a simple SQLite database to store user feedback on model outputs, which can be regularly revi<a id="_idTextAnchor284"/>ewed to identify potential biases or issues.</p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor285"/>Summary</h1>
			<p>In this chapter, we learned about fairness and bias in LLMs, focusing on understanding different fairness definitions, such as demographic parity, equal opportunity, and equalized odds. We explored the types of bias that can emerge in LLMs, including representation, linguistic, allocation, quality of service, and stereotypical, along with techniques for detecting and quantifying them through metrics such as demographic parity difference and equal opportunity difference.</p>
			<p>We used practical coding examples to show you how to analyze bias. Debiasing strategies such as data augmentation, bias-aware fine-tuning, and fairness-aware training were also covered, providing actionable ways to mitigate bias. Finally, we gained insights into ethical considerations, including transparency, diverse development teams, regular auditing, and user feedback systems. These skills will help you detect, measure, and address bias in LLMs while building more equitable and transparent AI systems.</p>
			<p>Keep in mind that fairness metrics in LLMs often conflict because they prioritize different aspects of equitable treatment. For example, <em class="italic">demographic parity</em> (equal outcomes across groups) can clash with <em class="italic">equalized odds</em>, which ensures similar false positive and false negative rates across groups, particularly when base rates differ. Similarly, <em class="italic">calibration</em> (ensuring predicted probabilities reflect actual outcomes) can contradict <em class="italic">equalized odds</em>, as a model that is well calibrated might still have unequal error rates. Additionally, <em class="italic">individual fairness</em> (treating similar individuals similarly) can be at odds with <em class="italic">group fairness</em>, which enforces equity across demographic groups, sometimes requiring differential treatment. These conflicts highlight the challenge of balancing fairness objectives in AI models.</p>
			<p>As we move forward, the next chapter will explore advanced prompt engineering techniques for LLMs.</p>
		</div>
	</div></div></body></html>