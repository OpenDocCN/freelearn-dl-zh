<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Learning for NLP</h1>
                </header>
            
            <article>
                
<p class="mce-root">n the previous chapter, we used classic machine learning techniques to build our text classifiers. In this chapter, we will replace those with deep learning techniques via the use of <strong>recurrent neural networks</strong> (<strong>RNN</strong>).</p>
<p class="mce-root">In particular, we will use a relatively simple bidirectional LSTM model. If this is new to you, keep reading <span>–</span> if not, please feel free to skip ahead!</p>
<div class="packt_tip">The dataset attribute of the batch variable should point to the <kbd>trn</kbd> variable of the <kbd>torchtext.data.TabularData</kbd> type. This is a useful checkpoint to understand how data flow differs in training deep learning models.</div>
<p class="mce-root">Let's begin by touching upon the overhyped terms, that is, <em>deep</em> in deep learning and <em>neural</em> in deep neural networks. Before we do that, let's take a moment to explain why I use PyTorch and compare it to Tensorflow and Keras—the other popular deep learning frameworks.</p>
<p class="mce-root">I will be building the simplest possible architecture for demonstrative purposes here. Let's assume a general familiarity with RNNs and not introduce the same again.</p>
<p class="mce-root">In this chapter, we will answer the following questions:</p>
<ul>
<li class="mce-root">What is deep learning? How does it differ from what we have seen already?</li>
<li class="mce-root">What are the key ideas in any deep learning model?</li>
<li class="mce-root">Why PyTorch?</li>
<li class="mce-root">How do we tokenize text and set up dataloaders with torchtext?</li>
<li class="mce-root">What are recurrent networks, and how can we use them for text classification?</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is deep learning?</h1>
                </header>
            
            <article>
                
<p class="mce-root">Deep learning is a subset of machine learning: a new take on learning from data that puts an emphasis on learning successive layers of increasingly meaningful representations. But what does the <em>deep</em> in deep learning mean?</p>
<div class="mce-root packt_quote"><q>"The deep in deep learning isn't a reference to any kind of deeper understanding achieved by the approach; rather, it stands for this idea of successive layers of representations."<br/></q></div>
<div class="packt_quote CDPAlignRight CDPAlign"><span>– F. Chollet, Lead Developer of Keras</span></div>
<p class="mce-root">The depth of the model is indicative of how many layers of such representations we use. F Chollet suggested layered representations learning and hierarchical representations learning as better names for this. Another name could have been differentiable programming.</p>
<p class="mce-root">The term <em>differentiable programming</em>, coined by Yann LeCun, stems from the fact that what our <em>deep learning methods</em> have in common is not more layers—<span>it's the fact that</span> all of these models learn via some form of differential calculus <span>– </span>most often stochastic gradient descent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differences between modern machine learning methods</h1>
                </header>
            
            <article>
                
<p class="mce-root">The modern machine learning methods that we have studied shot to being mainstream mainly in the 1990s. The binding factor among them was that they all use one layer of representations. For instance, decision trees just create one set of rules and apply them. Even if you add ensemble approaches, the <em>ensembling</em> is often shallow and only combines several ML models directly.</p>
<p class="mce-root">Here is a better-worded interpretation of these differences:</p>
<div class="mce-root packt_quote">"Modern deep learning often involves tens or even hundreds of successive layers of representations <span>– </span>and they’re all learned automatically from exposure to training data. Meanwhile, other approaches to machine learning tend to focus on learning only one or two layers of representations of the data; hence, they’re sometimes called shallow learning."</div>
<div class="packt_quote CDPAlignRight CDPAlign"><span>– F Chollet</span></div>
<p class="mce-root"/>
<p>Let's look at the key terms behind deep learning, since this way we might come across some key ideas as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding deep learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In a loosely worded manner, machine learning is about mapping inputs (such as images, or <em>movie reviews</em>) to targets (such as the label cat or <em>positive</em>). The model does this by looking at (or training from) several pairs of input and targets.</p>
<p class="mce-root">Deep neural networks do this input-to-target mapping using a long sequence of simple data transformations (layers). This sequence length is referred to as the depth of the network. The entire sequence from input-to-target is referred to as a model that learns about the data. These data transformations are learned by repeated observation of examples. Let's look at how this learning happens.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Puzzle pieces</h1>
                </header>
            
            <article>
                
<p class="mce-root">We are looking at a particular subclass of challenges where we want to learn an input-to-target mapping. This subclass is generally referred to as supervised machine learning. The word supervised denotes that we have target for each input. Unsupervised machine learning includes challenges such as trying to cluster text, where we do not have a target.</p>
<p class="mce-root">To do any supervised machine learning, we need the following in place:</p>
<ul>
<li class="mce-root"><strong>Input Data:</strong> Anything ranging from past stock performance to your vacation pictures</li>
<li class="mce-root"><strong>Target:</strong> Examples of the expected output</li>
<li class="mce-root"><strong>A way to measure whether the algorithm is doing a good job:</strong> This is necessary to determine the distance between the algorithm's current output and its expected output</li>
</ul>
<p class="mce-root">The preceding components are universal to any supervised approach, be it machine learning or deep learning. Deep learning in particular has its own cast of puzzling factors:</p>
<ul>
<li class="mce-root">The model itself</li>
<li class="mce-root">The loss function</li>
<li class="mce-root">The optimizer</li>
</ul>
<p>Since these actors are new to the scene, let's take a minute in understanding what they do.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model</h1>
                </header>
            
            <article>
                
<p class="mce-root">Each model is comprised of several layers. Each layer is a data transformation. This transformation is captured using a bunch of numbers, called layer weights. This is not a complete truth though, since most layers often have a mathematical operation associated with them, for example, convolution or an affine transform. A more precise perspective would be to say that a layer is <strong>parameterized</strong> by its weights. Hence, we use the terms <em>layer parameters</em> and <em>layer weights</em> interchangeably.</p>
<p class="mce-root">The state of all the layer weights together makes the model state captured in model weights. A model can have anywhere between a few thousand to a few million parameters.</p>
<p class="mce-root">Let's try to understand the notion of model <strong>learning</strong> in this context: learning means finding values for the weights of all layers in a network, so that the network will correctly map example inputs to their associated targets.</p>
<div class="mce-root packt_infobox">Note that this value set is for <em>all layers</em> in one place. This nuance is important because changing the weights of one layer can change the behavior and predictions made by the entire model.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss function</h1>
                </header>
            
            <article>
                
<p class="mce-root">One of the pieces that's used to set up a machine learning task is to assess how a model is doing. The simplest answer would be to measure the notional accuracy of the model. Accuracy has few flaws, though:</p>
<ul>
<li class="mce-root">Accuracy is a proxy metric tied to validation data and not training data.</li>
<li class="mce-root">Accuracy measures how correct we are. During training, we want to measure how far our model predicts from the target.</li>
</ul>
<p>These differences mean that we need a different function to meet our preceding criteria. This is fulfilled by the <em>loss function</em> in the context of deep learning. This is sometimes referred to as an <em>objective function</em> as well.</p>
<div class="mce-root packt_infobox"><q>"The loss function takes the predictions of the network and the true target (what you wanted the network to output) and computes a distance score, capturing how well the network has done on this specific example.</q>"<br/>
                                                      <q>- From Deep Learning in Python by F Chollet</q><br/>
<br/>
This distance measurement is called the loss score, or simply loss.</div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizer</h1>
                </header>
            
            <article>
                
<p class="mce-root">This loss is automatically used as a feedback signal to adjust the way the algorithm works. This adjustment step is what we call learning.</p>
<p class="mce-root">This automatic adjustment in model weights is peculiar for deep learning. Each adjustment or <em>update</em> of weights is made in a direction that will lower the loss for the current training pair (input, target).</p>
<p class="mce-root">This adjustment is the job of the optimizer, which implements what's called the backpropagation algorithm: the central algorithm in deep learning.</p>
<p class="mce-root">Optimizers and loss functions are common to all deep learning methods <span>–</span> even the cases where we don't have an input/target pair. All optimizers are based on differential calculus, such as <strong>stochastic gradient descent</strong> (<strong>SGD</strong>), Adam, and so on. Hence, the term <em>differentiable programming</em> is a more precise name for deep learning in my mind.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together – the training loop</h1>
                </header>
            
            <article>
                
<p>We now have a shared vocabulary. You have a notional understanding of what terms like layers, model weights, loss function, and optimizer mean. But how do they work together? How do we train them on arbitrary data? We can train them to give us the ability to recognize cat pictures or fraudulent reviews on Amazon.<br/>
<br/>
Here is the rough outline of the steps that occur inside a training loop:</p>
<ul>
<li>Initialize: 
<ul>
<li>The network/model weights are assigned random values, usually in the form of (-1, 1) or (0, 1).</li>
<li>The model is very far from the target. This is because it is simply executing a series of random transformations.</li>
<li>The loss is very high.</li>
</ul>
</li>
<li>With every example that the network processes, the following occurs:
<ul>
<li>The weights are adjusted a little in the correct direction</li>
<li> The loss score decreases</li>
</ul>
</li>
</ul>
<p>This is the training loop, which is repeated several times. Each pass over the entire training set is often referred to as an <strong>epoch</strong>. Each training set suited for deep learning should typically have thousands of examples. The models are sometimes trained for thousands of epochs, or alternatively millions of <strong>iterations</strong>.</p>
<p>In a training setup (model, optimizer, loop), the preceding loop updates the weight values that minimize the loss function. A trained network is the one with the least possible loss score on the entire training and valid data.<br/>
<br/>
It's a simple mechanism that, when repeated often, just works like magic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kaggle – text categorization challenge</h1>
                </header>
            
            <article>
                
<p>In this particular section, we are going to visit the familiar task of text classification, but with a different dataset. We are going to try to solve the <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">Jigsaw Toxic Comment Classification Challenge.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p>Note that you will need to accept the terms and conditions of the competition and data usage to get this dataset.<br/>
<br/>
For a direct download, you can get the train and test data from the <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data">data tab on the challenge website</a>.</p>
<p>Alternatively, you can use the official Kaggle API (<a href="https://github.com/Kaggle/kaggle-api">github link</a>) to download the data via a Terminal or Python program as well.<br/>
<br/>
In the case of both direct download and Kaggle API, you have to split your train data into smaller train and validation splits for this notebook.<br/>
<br/>
You can create train and validation splits of the train data by using the <br/>
<kbd>sklearn.model_selection.train_test_split</kbd> utility. Alternatively, you can download this directly from the accompanying code repository with this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the data</h1>
                </header>
            
            <article>
                
<p>In case you have any packages missing, you can install them from the notebook itself by using the following commands:</p>
<pre><strong># !conda install -y pandas</strong><br/><strong># !conda install -y numpy</strong></pre>
<p class="mce-root">Let's get the imports out of our way:</p>
<pre class="mce-root"><strong>import pandas as pd</strong><br/><strong>import numpy as np</strong></pre>
<p class="mce-root">Then, read the train file into a pandas DataFrame:</p>
<pre class="mce-root">train_df = pd.read_csv("data/train.csv")<br/>train_df.head()</pre>
<p><span>We get the following output:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<thead>
<tr>
<th style="width: 10%"/>
<th style="width: 8%">id</th>
<th style="width: 18%"><kbd>comment_text</kbd></th>
<th style="width: 7%"><kbd>toxic</kbd></th>
<th style="width: 13%"><kbd>severe_toxic</kbd></th>
<th style="width: 9%"><kbd>obscene</kbd></th>
<th style="width: 8%"><kbd>threat</kbd></th>
<th style="width: 8%"><kbd>insult</kbd></th>
<th style="width: 14%"><kbd>identity_hate</kbd></th>
</tr>
</thead>
<tbody>
<tr>
<td style="width: 10%">0</td>
<td style="width: 8%">0000997932d777bf</td>
<td style="width: 18%">Explanation<kbd>\r\nWhy</kbd> the edits made under my use...</td>
<td style="width: 7%">0</td>
<td style="width: 13%">0</td>
<td style="width: 9%">0</td>
<td style="width: 8%">0</td>
<td style="width: 8%">0</td>
<td style="width: 14%">0</td>
</tr>
<tr>
<td style="width: 10%">1</td>
<td style="width: 8%">000103f0d9cfb60f</td>
<td style="width: 18%">D'aww! He matches this background colour I'm s...</td>
<td style="width: 7%">0</td>
<td style="width: 13%">0</td>
<td style="width: 9%">0</td>
<td style="width: 8%">0</td>
<td style="width: 8%">0</td>
<td style="width: 14%">0</td>
</tr>
<tr>
<td style="width: 10%">2</td>
<td style="width: 8%">000113f07ec002fd</td>
<td style="width: 18%">Hey man, I'm really not trying to edit war. It...</td>
<td style="width: 7%">0</td>
<td style="width: 13%">0</td>
<td style="width: 9%">0</td>
<td style="width: 8%">0</td>
<td style="width: 8%">0</td>
<td style="width: 14%">0</td>
</tr>
<tr>
<td style="width: 10%">3</td>
<td style="width: 8%">0001b41b1c6bb37e</td>
<td style="width: 18%"><kbd>\r\nMore\r\n</kbd> I can't make any real suggestions...</td>
<td style="width: 7%">0</td>
<td style="width: 13%">0</td>
<td style="width: 9%">0</td>
<td style="width: 8%">0</td>
<td style="width: 8%">0</td>
<td style="width: 14%">0</td>
</tr>
<tr>
<td style="width: 10%">4</td>
<td style="width: 8%">0001d958c54c6e35</td>
<td style="width: 18%">You, sir, are my hero. Any chance you remember...</td>
<td style="width: 7%">0</td>
<td style="width: 13%">0</td>
<td style="width: 9%">0</td>
<td style="width: 8%">0</td>
<td style="width: 8%">0</td>
<td style="width: 14%">0</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mce-root">Let's read the validation data and preview the same as well:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">val_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"data/valid.csv"</span><span class="p">)</span>
<span class="n">val_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></pre></div>
</div>
</div>
</div>
</div>
<p>We get the following output:<br/></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 9.28645%"/>
<td style="width: 10%">id</td>
<td style="width: 16.6331%"><kbd>comment_text</kbd></td>
<td style="width: 7.8629%"><kbd>toxic</kbd></td>
<td style="width: 14.1129%"><kbd>severe_toxic</kbd></td>
<td style="width: 9.57661%"><kbd>obscene</kbd></td>
<td style="width: 8.66935%"><kbd>threat</kbd></td>
<td style="width: 8.66935%"><kbd>insult</kbd></td>
<td style="width: 15.0202%"><kbd>identity_hate</kbd></td>
</tr>
<tr>
<td style="width: 9.28645%">0</td>
<td style="width: 10%">000eefc67a2c930f</td>
<td style="width: 16.6331%">Radial symmetry <kbd>\r\n\r\n</kbd> Several now extinct li...</td>
<td style="width: 7.8629%">0</td>
<td style="width: 14.1129%">0</td>
<td style="width: 9.57661%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 15.0202%">0</td>
</tr>
<tr>
<td style="width: 9.28645%">1</td>
<td style="width: 10%">000f35deef84dc4a</td>
<td style="width: 16.6331%">There's no need to apologize. A Wikipedia arti...</td>
<td style="width: 7.8629%">0</td>
<td style="width: 14.1129%">0</td>
<td style="width: 9.57661%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 15.0202%">0</td>
</tr>
<tr>
<td style="width: 9.28645%">2</td>
<td style="width: 10%">000ffab30195c5e1</td>
<td style="width: 16.6331%">Yes, because the mother of the child in the ca...</td>
<td style="width: 7.8629%">0</td>
<td style="width: 14.1129%">0</td>
<td style="width: 9.57661%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 15.0202%">0</td>
</tr>
<tr>
<td style="width: 9.28645%">3</td>
<td style="width: 10%">0010307a3a50a353</td>
<td style="width: 16.6331%"><kbd>\r\nOk</kbd>. But it will take a bit of work but I ...</td>
<td style="width: 7.8629%">0</td>
<td style="width: 14.1129%">0</td>
<td style="width: 9.57661%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 15.0202%">0</td>
</tr>
<tr>
<td style="width: 9.28645%">4</td>
<td style="width: 10%">0010833a96e1f886</td>
<td style="width: 16.6331%"><kbd>== A barnstar</kbd> for you! <kbd>==\r\n\r\n</kbd> The Real L...</td>
<td style="width: 7.8629%">0</td>
<td style="width: 14.1129%">0</td>
<td style="width: 9.57661%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 8.66935%">0</td>
<td style="width: 15.0202%">0</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiple target dataset</h1>
                </header>
            
            <article>
                
<p class="mce-root">The interesting thing about this dataset is that each comment can have multiples labels. For instance, a comment could be insulting and toxic, or it could be obscene and have <kbd>identity_hate</kbd> elements in it.</p>
<p class="mce-root">Hence, we are leveling up here by trying to predict not one label (such as positive or negative), but multiple labels in one go. For each label, we'll predict a value between 0 and 1 to indicate how likely it is to belong to that category.</p>
<p class="mce-root">This is not a probability value in the Bayesian meaning of the word, but represents the same intent.</p>
<p class="mce-root"/>
<div class="mce-root packt_tip">I'd recommend trying out the models that we saw earlier with this dataset, and re-implementing this code for our favourite IMDb dataset.</div>
<p>Let's preview the test dataset as well using the same idea:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">test</span><span class="n">_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"data/test.</span><span class="s2">csv"</span><span class="p">)<br/></span><span class="n">test_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></pre>
<p><span>We get the following output:</span></p>
</div>
</div>
</div>
</div>
</div>
<table style="border-collapse: collapse;width: 100%" border="1">
<thead>
<tr>
<th style="width: 10%"/>
<th style="width: 26%"><kbd>id</kbd></th>
<th style="width: 71%"><kbd>comment_text</kbd></th>
</tr>
</thead>
<tbody>
<tr>
<td style="width: 10%">0</td>
<td style="width: 26%">00001cee341fdb12</td>
<td style="width: 71%">Yo bitch Ja Rule is more succesful then you'll...</td>
</tr>
<tr>
<td style="width: 10%">1</td>
<td style="width: 26%">0000247867823ef7</td>
<td style="width: 71%"><kbd>== From RfC == \r\n\r\n</kbd> The title is fine as i...</td>
</tr>
<tr>
<td style="width: 10%">2</td>
<td style="width: 26%">00013b17ad220c46</td>
<td style="width: 71%"><kbd>\r\n\r\n == Sources == \r\n\r\n *</kbd> Zawe Ashto...</td>
</tr>
<tr>
<td style="width: 10%">3</td>
<td style="width: 26%">00017563c3f7919a</td>
<td style="width: 71%">If you have a look back at the source, the in...</td>
</tr>
<tr>
<td style="width: 10%">4</td>
<td style="width: 26%">00017695ad8997eb</td>
<td style="width: 71%">I don't anonymously edit articles at all.</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>This preview confirms that we have a text challenge. The focus here is on the semantic categorization of text. The test dataset does not have empty headers or columns for the target columns, but we can infer them from the train dataframe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why PyTorch?</h1>
                </header>
            
            <article>
                
<p class="mce-root">PyTorch is a deep learning framework by Facebook, similar to TensorFlow by Google.</p>
<p class="mce-root">Being backed by Google, thousands of dollars have been spent on TensorFlow's marketing, development, and documentation. It also got to a stable 1.0 release almost a year ago, while PyTorch has only recently gotten to 0.4.1. This means that it's usually easier to find a TensorFlow solution to your problem and that you can copy and paste code off the internet.</p>
<p class="mce-root">On the other hand, PyTorch is programmer-friendly. It is semantically similar to NumPy and deep learning operations in one. This means that I can use the Python debugging tools that I am already familiar with.</p>
<p class="mce-root"><strong>Pythonic</strong>: TensorFlow worked like a C program in the sense that the code was all written in one session, compiled, and then executed, thereby destroying its Python flavor altogether. This has been solved by TensorFlow's Eager Execution feature release, which will soon be stable enough to use for most prototyping work.</p>
<p class="mce-root"/>
<p class="mce-root"><strong>Training Loop Visualization:</strong> Up until a while ago, TensorFlow had a good visualization tool called TensorBoard for understanding training and validation performance (and other characteristics), which was absent in PyTorch. For a long while now, tensorboardX makes TensorBoard easy to use with PyTorch.</p>
<p class="mce-root">In short, I recommend using PyTorch because it is easier to debug, more Pythonic, and more programmer-friendly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PyTorch and torchtext</h1>
                </header>
            
            <article>
                
<p class="mce-root">You can install the latest version of Pytorch (<a href="https://pytorch.org/">website</a>) via conda or pip for your target machine. I am running this code on a Windows laptop with a GPU.</p>
<p class="mce-root">I have installed torch using <kbd>conda install pytorch cuda92 -c pytorch</kbd>.</p>
<div class="cell border-box-sizing text_cell rendered">
<p class="prompt input_prompt"><span>For installing</span> <kbd>torchtext</kbd><span>, I recommend using pip directly from their GitHub repository with the latest fixes instead of PyPi, which is not frequently updated. Uncomment the line when running this notebook for the first time:</span></p>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="c1"># !pip install --upgrade git+https://github.com/pytorch/text</span></pre></div>
</div>
</div>
</div>
</div>
<p>Let's set up the imports for <kbd>torch</kbd>, <kbd>torch.nn</kbd> (which is used in modeling), and <kbd>torchtext</kbd>:</p>
<pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torchtext</span></pre>
<p class="mce-root">If you are running this code on a machine with a GPU, leave the <kbd>use_gpu</kbd> flag set to <kbd>True</kbd>; otherwise, set it to <kbd>False</kbd>.</p>
<p class="mce-root">If you set <kbd>use_gpu=True</kbd>, we will check whether the GPU is accessible to PyTorch or not using the <kbd>torch.cuda.is_available()</kbd> utility:</p>
<pre><span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">(),</span> <span class="s1">'You either do not have a GPU or is not accessible to PyTorch'</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Let's see how many GPU devices are available to PyTorch on this machine:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
&gt; 1</pre></div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data loaders with torchtext</h1>
                </header>
            
            <article>
                
<p class="mce-root">Writing good data loaders is the most tedious part in most deep learning applications. This step often combines the preprocessing, text cleaning, and vectorization tasks that we saw earlier.</p>
<p class="mce-root">Additionally, it wraps our static data objects into iterators or generators. This is incredibly helpful in processing data sizes much larger than GPU memory—which is quite often the case. This is done by splitting the data so that you can make batches of batchsize samples that fit your GPU memory.</p>
<p class="mce-root">Batchsizes are often powers of 2, such as 32, 64, 512, and so on. This convention exists because it helps with vector operations on the instruction set level. Anecdotally, using a batchsize that's different from a power of 2 has not helped or hurt my processing speed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conventions and style</h1>
                </header>
            
            <article>
                
<p class="mce-root">The code, iterators, and wrappers that we will be using are from <a href="https://github.com/keitakurita/practical-torchtext/">Practical Torchtext</a>. This is a <kbd>torchtext</kbd> tutorial that was created by Keita Kurita—one of the top five contributors to <kbd>torchtext</kbd>.</p>
<p class="mce-root">The naming conventions and style are loosely inspired from the preceding work and fastai—a deep learning framework based on PyTorch itself.</p>
<p class="mce-root">Let's begin by setting up the required variable placeholders in place:</p>
<pre><span class="kn">from</span> <span class="nn">torchtext.data</span> <span class="k">import</span> <span class="n">Field</span></pre>
<p>The <kbd>Field</kbd> class determines how the data is preprocessed and converted into a numeric format. The <kbd>Field</kbd> class is a fundamental <kbd>torchtext</kbd> data structure and worth looking into. The <kbd>Field</kbd> class models common text processing and sets them up for numericalization (or vectorization):</p>
<pre><span class="n">LABEL</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_vocab</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">By default, all of the fields take in strings of words as input, and then the fields build a mapping from the words to integers later on. This mapping is called the vocab, and is effectively a one-hot encoding of the tokens.</p>
<p class="mce-root">We saw that each label in our case is already an integer marked as 0 or 1. Therefore, we will not one-hot this <span>– </span>we will tell the <kbd>Field</kbd> class that this is already one-hot encoded and non-sequential by setting <kbd>use_vocab=False</kbd> and <kbd>sequential=False</kbd>, respectively:</p>
<pre><span class="n">tokenize</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">TEXT</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></pre>
<p class="mce-root">A few things are happening here, so let's unpack it a bit:</p>
<ul>
<li class="mce-root"><kbd>lower=True</kbd>: All input is converted to lowercase.</li>
<li class="mce-root"><kbd>sequential=True</kbd>: If <kbd>False</kbd>, no tokenization is applied.</li>
<li class="mce-root"><kbd>tokenizer</kbd>: We defined a custom tokenize function that simply splits the string on the space. You should replace this with the spaCy tokenizer (set <kbd>tokenize="spacy"</kbd>) and see if that changes the loss curve or final model's performance.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Knowing the field</h1>
                </header>
            
            <article>
                
<p class="mce-root">Along with the keyword arguments that we've already mentioned, the <kbd>Field</kbd> class will also allow the user to specify special tokens (<kbd>unk_token</kbd> for out-of-vocabulary unknown words, <kbd>pad_token</kbd> for padding, <kbd>eos_token</kbd> for the end of a sentence, and an optional <kbd>init_token</kbd> for the start of the sentence).</p>
<p class="mce-root">The preprocessing and postprocessing parameters accept any <kbd>torchtext.data.Pipeline</kbd> that it receives. Preprocessing is applied after tokenizing but before numericalizing. Postprocessing is applied after numericalizing, but before converting them into a Tensor.</p>
<p class="mce-root">The docstrings for the <kbd>Field</kbd> class are relatively well written, so if you need some advanced preprocessing, you should probe them for more information:</p>
<pre><span class="kn">from</span> <span class="nn">torchtext.data</span> <span class="k">import</span> <span class="n">TabularDataset</span></pre>
<p class="mce-root"><kbd>TabularDataset</kbd> is the class that we use to read <kbd>.csv</kbd>, <kbd>.tsv</kbd>, or <kbd>.json</kbd> files. You can specify the type of file that you are reading, that is, <kbd>.tsv</kbd> or <kbd>.json</kbd>, directly in the API, which is powerful and handy</p>
<p class="mce-root"/>
<p class="mce-root">At first glance, you might think that this class is a bit misplaced because a generic file I/O+processor API should be accessible directly in PyTorch and not in a package dedicated to text processing. Let's see why it is placed where it is.</p>
<p class="mce-root"><kbd>TabularData</kbd> has an interesting <kbd>fields</kbd> input parameter. For the CSV data format, <kbd>fields</kbd> is a list of tuples. Each tuple in turn is the column name and the <kbd>torchtext</kbd> variable we want to associate with it. The fields should be in the same order as the columns in the CSV or TSV file.</p>
<p class="mce-root">We have only two defined fields here: TEXT and LABEL. Therefore, each column is tagged as either one. We can simply mark the column as None if we want to ignore it completely. This is how we are tagging our columns as inputs (TEXT) and targets (LABEL) for the model to learn.</p>
<p class="mce-root">This tight coupling of the fields parameter with <kbd>TabularData</kbd> is why this is part of <kbd>torchtext</kbd> and not PyTorch:</p>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">tv_datafields = [("id", None), # we won't be needing the id, so we pass in None as the field<br/>                 ("comment_text", TEXT), ("toxic", LABEL),<br/>                 ("severe_toxic", LABEL), ("threat", LABEL),<br/>                 ("obscene", LABEL), ("insult", LABEL),<br/>                 ("identity_hate", LABEL)]</span></pre></div>
</div>
</div>
<p class="mce-root">This defines our list of inputs. I have done this manually here, but you could also do this with code by reading the column headers from <kbd>train_df</kbd> and assigning them TEXT or LABEL accordingly.</p>
<p class="mce-root">As a reminder, we will have to define another fields list for our test data because it has a different header. It has no LABEL fields.</p>
<p class="mce-root"><kbd>TabularDataset</kbd> supports two APIs: <kbd>split</kbd> and <kbd>splits</kbd>. We will use the one with the extra s, <kbd>splits</kbd>. The splits API is simple:</p>
<ul>
<li class="mce-root"><kbd>path</kbd>: This is the prefix of filenames</li>
<li class="mce-root"><kbd>train</kbd>, <kbd>validation</kbd>: These are filenames of the corresponding dataset</li>
<li class="mce-root"><kbd>format</kbd>: Either <kbd>.csv</kbd>, <kbd>.tsv</kbd>, or <kbd>.json</kbd>, as stated earlier; this is set to <kbd>.csv</kbd> here</li>
<li class="mce-root"><kbd>skip_header</kbd>: This is set to <kbd>True</kbd> if your <kbd>.csv</kbd> file has column titles in it, as does ours</li>
<li class="mce-root"><kbd>fields</kbd>: We pass the list of fields we just set up previously:</li>
</ul>
<p class="mce-root"/>
<pre><span class="p">trn, vld = TabularDataset.splits(<br/>        path="data", # the root directory where the data lies<br/>        train='train.csv', validation="valid.csv",<br/>        format='csv',<br/>        skip_header=True, # make sure to pass this to ensure header doesn't get proceesed as data!<br/>        fields=tv_datafields)</span></pre>
<p class="mce-root">Let's repeat the same for test data now. We drop the <kbd>id</kbd> column again and set <kbd>comment_text</kbd> to be our label:</p>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="p">tst_datafields = [("id", None), # we won't be needing the id, so we pass in None as the field<br/>                 ("comment_text", TEXT)<br/>                 ]</span></pre></div>
</div>
</div>
<p class="mce-root">We pass the entire relative file path directly into the path, instead of using the <kbd>path</kbd> and <kbd>test</kbd> variable combination here. We used the <kbd>path</kbd> and <kbd>train</kbd> combination when setting up the <kbd>trn</kbd> and <kbd>vld</kbd> variables.</p>
<p class="mce-root">As a note, these filenames are consistent with what Keita used in the <kbd>torchtext</kbd> tutorial:</p>
<pre><span class="n">tst</span> <span class="o">=</span> <span class="n">TabularDataset</span><span class="p">(</span>
        <span class="n">path</span><span class="o">=</span><span class="s2">"data/test.csv"</span><span class="p">,</span> <span class="c1"># the file path</span>
        <span class="nb">format</span><span class="o">=</span><span class="s1">'csv'</span><span class="p">,</span>
        <span class="n">skip_header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!</span>
        <span class="n">fields</span><span class="o">=</span><span class="n">tst_datafields</span><span class="p">)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the dataset objects</h1>
                </header>
            
            <article>
                
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">
<p>Let's look at the dataset objects, that is, <kbd>trn</kbd>, <kbd>vld</kbd>, and <kbd>tst</kbd>:</p>
</div>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">trn</span><span class="p">,</span> <span class="n">vld</span><span class="p">,</span> <span class="n">tst<br/><br/>&gt; </span>(&lt;torchtext.data.dataset.TabularDataset at 0x1d6c86f1320&gt;,
 &lt;torchtext.data.dataset.TabularDataset at 0x1d6c86f1908&gt;,
 &lt;torchtext.data.dataset.TabularDataset at 0x1d6c86f16d8&gt;)</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>They are all objects from the same class. Our dataset objects can be indexed and iterated over like normal lists, so let's see what the first element looks like:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">trn</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vld</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tst</span><span class="p">[</span><span class="mi">0</span><span class="p">]<br/>&gt; </span>(&lt;torchtext.data.example.Example at 0x1d6c86f1940&gt;,
 &lt;torchtext.data.example.Example at 0x1d6c86fed30&gt;,
 &lt;torchtext.data.example.Example at 0x1d6c86fecc0&gt;)</pre>
<p class="mce-root">All our elements are, in turn, objects of the <kbd>example.Example</kbd> class. Each example stores each column as an attribute. But where did our text and labels go?</p>
</div>
</div>
</div>
</div>
</div>
<div class=" highlight hl-ipython3">
<pre><span class="n">trn</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">()<br/></span>&gt; dict_keys(['comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate']</pre></div>
<p>The <kbd>Example</kbd> object bundles the attributes of a single data point together. Our <kbd>comment_text</kbd> and the <kbd>labels</kbd> are now part of the dictionary that makes up each of these example objects. We found all of them by calling <kbd>__dict__.keys()</kbd> on an <kbd>example.Example</kbd> object:</p>
<div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">trn</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">__dict__</span><span class="p">[</span><span class="s1">'comment_text'</span><span class="p">][:</span><span class="mi">5</span><span class="p">]<br/>&gt; </span>['explanation', 'why', 'the', 'edits', 'made']</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The text has already been tokenized for us, but has not yet been vectorized or numericalized. We will use one-hot encoding for all the tokens that exist in our training corpus. This will convert our words into integers.</p>
</div>
</div>
</div>
</div>
<p>We can do this by calling the <kbd>build_vocab</kbd> attribute of our <kbd>TEXT</kbd> field:</p>
<div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">trn</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This statement processes the entire train data <span>–</span> in particular, the <kbd>comment_text</kbd> field. The words are registered in the vocabulary.</p>
<p>To handle the vocabulary, <kbd>torchtext</kbd> has its own class. The <kbd>Vocab</kbd> class can also take options such as <kbd>max_size</kbd> and <kbd>min_freq</kbd> that can let us know the number of words present in the vocabulary or how many times a word has to appear to be registered in the vocabulary.<br/>
Words that are not included in the vocabulary will be converted into <kbd>&lt;unk&gt;</kbd>, a token meaning for <em>unknown</em>. Words that occur that are too rare are also assigned the <kbd>&lt;unk&gt;</kbd> token for ease of processing. This can hurt or help the model's performance, depending on which and how many words we lose to the <kbd>&lt;unk&gt;</kbd> token:</p>
</div>
</div>
</div>
</div>
<div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab<br/>&gt; </span>&lt;torchtext.vocab.Vocab at 0x1d6c65615c0&gt;</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <kbd>TEXT</kbd> field now has a vocab attribute that is a specific instance of the <kbd>Vocab</kbd> class. We can use this in turn to look up the attributes of the vocab object. For instance, we can find the frequency of any word in the training corpus. The <kbd>TEXT.vocab.freqs</kbd> object is actually an object of <kbd>type collections.Counter</kbd>:</p>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="nb">type</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">freqs</span><span class="p">)<br/>&gt; </span>collections.Counter</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This means that it will support all functions, including the <kbd>most_common</kbd> API to sort the words by frequency and find the top k most frequently occurring words for us. Let's take a look at them:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">freqs</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">5</span><span class="p">)<br/>&gt; </span>[('the', 78), ('to', 41), ('you', 33), ('of', 30), ('and', 26)]</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <kbd>Vocab</kbd> class holds a mapping from word to <kbd>id</kbd> in its <kbd>stoi</kbd> attribute and a reverse mapping in its <kbd>itos</kbd> attribute. Let's look at these attributes:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="nb">type</span><span class="p">(</span><span class="n">TEX<br/><br/></span>T<span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">),</span><span> </span><span class="nb">type</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="p">),</span><span> </span><span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">),</span><span> </span><span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">stoi</span><span class="o">.</span><span class="n">keys</span><span class="p">())<br/></span>&gt; <span>(list, collections.defaultdict, 784, 784)</span></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>itos</strong>, or integer to string mapping, is a list of words. The index of each word in the list is its integer mapping. For instance, the 7-indexed word would be <em>and</em> because its integer mapping is 7.</p>
<p><strong>stoi</strong>, or string to integer mapping, is a dictionary of words. Each key is a word in the training corpus, with the value being an integer. For instance, the word "and" might have an integer mapping that can be looked up in this dictionary in O(1) time.</p>
<p>Note that this convention automatically handles the off-by-one problem caused by zero indexing in Python:</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<pre>TEXT.vocab.stoi['and'], TEXT.vocab.itos[7]<br/>&gt; (7, 'and')</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Iterators</h1>
                </header>
            
            <article>
                
<p class="mce-root"><kbd>torchtext</kbd> has renamed and extended the <kbd>DataLoader</kbd> objects from PyTorch and torchvision. In essence, it does the same three jobs:</p>
<ul>
<li class="mce-root">Batching the data</li>
<li class="mce-root">Shuffling the data</li>
<li class="mce-root">Loading the data in parallel using <kbd>multiprocessing</kbd> workers</li>
</ul>
<p class="mce-root">This batch loading of data enables us to process a dataset that's much larger than the GPU RAM. <kbd>Iterators</kbd> extend and specialize the <kbd>DataLoader</kbd> for NLP/text processing applications.</p>
<p>We will use both <kbd>Iterator</kbd> and its cousin, <kbd>BucketIterator</kbd>, here:</p>
<pre><span class="kn">from</span> <span class="nn">torchtext.data</span> <span class="k">import</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">BucketIterator</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">BucketIterator</h1>
                </header>
            
            <article>
                
<p><kbd>BucketIterator</kbd> automatically shuffles and buckets the input sequences into sequences of similar length.</p>
<p>To enable batch processing, we need the input sequences in a batch that's of identical length. This is done by padding the smaller input sequences to the length of the longest sequence in batch. Check out the following code:</p>
<pre>[ [3, 15, 2, 7], 
  [4, 1], 
  [5, 5, 6, 8, 1] ]</pre>
<p>This will need to be padded to become the following:</p>
<pre>[ [3, 15, 2, 7, 0],
  [4, 1, 0, 0, 0],
  [5, 5, 6, 8, 1] ]</pre>
<p>Additionally, the padding operation is most efficient when the sequences are of similar lengths. The <kbd>BucketIterator</kbd> does all of this behind the scenes. This is what makes it an extremely powerful abstraction for text processing.</p>
<p class="mce-root"/>
<p class="mce-root">We want the bucket sorting to be based on the lengths of the <kbd>comment_text</kbd> field, so we pass that in as a keyword argument.</p>
<p class="mce-root">Let's go ahead and initialize the iterators for the train and validation data:</p>
<pre><span class="n">train_iter</span><span class="p">,</span> <span class="n">val_iter</span> <span class="o">=</span> <span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span>
        <span class="p">(</span><span class="n">trn</span><span class="p">,</span> <span class="n">vld</span><span class="p">),</span> <span class="c1"># we pass in the datasets we want the iterator to draw data from</span>
        <span class="n">batch_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
        <span class="n">sort_key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">comment_text</span><span class="p">),</span> <span class="c1"># the BucketIterator needs to be told what function it should use to group the data.</span>
        <span class="n">sort_within_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">repeat</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># we pass repeat=False because we want to wrap this Iterator layer.</span>
<span class="p">)</span></pre>
<p>Let's take a quick glance at the parameters we passed to this function:</p>
<ul>
<li><kbd>batch_size</kbd>: We use a small batch size of 32 for both train and validation. This is because I am using a GTX 1060 with only 3 GB of memory.</li>
<li><kbd>sort_key</kbd>: <kbd>BucketIterator</kbd> is told to use the number of tokens in the <kbd>comment_text</kbd> as the key to sort in any example.</li>
<li><kbd>sort_within_batch</kbd>: When set to <kbd>True</kbd>, this sorts the data within each minibatch in decreasing order, according to the <kbd>sort_key</kbd>.</li>
<li><kbd>repeat</kbd>: When set to True, it allows us to loop over and see a previously seen sample again. We set it to <kbd>False</kbd> here because we are repeating using an abstraction that we will write in a minute.</li>
</ul>
<p>In the meanwhile, let's take a minute to explore the new variable that we just made:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">train_iter<br/><br/>&gt; </span>&lt;torchtext.data.iterator.BucketIterator at 0x1d6c8776518&gt;<br/><br/><span class="n">batch</span> <span class="o">=</span><span> </span><span class="nb">next</span><span class="p">(</span><span class="n">train_iter</span><span class="o">.</span><span class="n">__iter__</span><span class="p">())<br/></span>batch<br/><br/>&gt; <span>[torchtext.data.batch.Batch of size 25]<br/></span>        [.comment_text]:[torch.LongTensor of size 494x25]<br/>        [.toxic]:[torch.LongTensor of size 25]<br/>        [.severe_toxic]:[torch.LongTensor of size 25]<br/>        [.threat]:[torch.LongTensor of size 25]<br/>        [.obscene]:[torch.LongTensor of size 25]<br/>        [.insult]:[torch.LongTensor of size 25]<br/>        [.identity_hate]:[torch.LongTensor of size 25]</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, all that each batch has is torch tensors of exactly the same size (the size is the length of the vector of the vector of the vector of the vector of the vector of the vector of the vector here). These tensors have not been moved to GPU yet, but that's fine.</p>
</div>
</div>
</div>
<p><kbd>batch</kbd> is actually a wrapper over the already familiar example object that we have seen. It bundles all the attributes related to the batch in one variable dict:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">batch</span><span class="o">.</span><span class="n">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">()<br/>&gt; </span>dict_keys(['batch_size', 'dataset', 'fields', 'comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])</pre></div>
</div>
</div>
</div>
</div>
<p>If our preceding understanding is correct, and we know how Python's object passing works, the dataset attribute of the batch variable should point to the <kbd>trn</kbd> variable of the <kbd>torchtext.data.TabularData</kbd> type. Let's check for this:</p>
<pre>batch.__dict__['dataset'], trn, batch.__dict__['dataset']==trn</pre>
<p class="mce-root">Aha! We got this right.</p>
<p class="mce-root">For the test iterator, since we don't need shuffling, we will use the plain <kbd>torchtext</kbd> <kbd>Iterator</kbd>:</p>
<pre><span class="n">test_iter</span> <span class="o">=</span> <span class="n">Iterator</span><span class="p">(</span><span class="n">tst</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sort_within_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></pre>
<p>Let's take a look at this iterator, too:</p>
<pre><span class="nb">next</span><span class="p">(</span><span class="n">test_iter</span><span class="o">.</span><span class="n">__iter__</span><span class="p">())<br/>&gt; </span>[torchtext.data.batch.Batch of size 33]
  [.comment_text]:[torch.LongTensor of size 158x33]</pre>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The sequence length of <kbd>33</kbd> here is different from the input's <kbd>25</kbd>. That's fine. We can see that this is also a torch tensor now.</p>
<p>Next, let's write a wrapper over the batch objects.</p>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">BatchWrapper</h1>
                </header>
            
            <article>
                
<p class="mce-root">Before we delve into <kbd>BatchWrapper</kbd>, let me tell you what the problem with batch objects is. Our batch iterator returns a custom datatype, <kbd>torchtext.data.Batch</kbd>. This has a similar to multiple <kbd>example.Example</kbd>. This returns with a batch of data from each field as attributes. This custom datatype makes code reuse difficult since, each time the column names change, we need to modify the code. This also makes <kbd>torchtext</kbd> hard to use with other libraries such as torchsample and fastai.</p>
<p class="mce-root"/>
<p class="mce-root">So, how do we solve this?</p>
<p class="mce-root">We will convert the batch into a tuple in the form (x, y). x is the input to the model and y is the target <span>– or,</span> more conventionally, x is the independent variable while y is the dependent variable. One way to think about this is that the model will learn the function mapping from x to y.</p>
<p>BatchWrapper helps us reuse the modeling, training, and other code functions across datasets:</p>
<pre><span class="p">class BatchWrapper: <br/>  def __init__(self, dl, x_var, y_vars): <br/>      self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y<br/><br/>  def __iter__(self): <br/>      for batch in self.dl: <br/>          x = getattr(batch, self.x_var) # we assume only one input in this wrapper <br/>          if self.y_vars is not None: <br/>                # we will concatenate y into a single tensor <br/>                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()<br/>                 else: y = torch.zeros((1)) if use_gpu: yield (x.cuda(), y.cuda()) else: yield (x, y) <br/><br/>   def __len__(self): return len(self.dl)<br/></span></pre>
<p class="mce-root">The <kbd>BatchWrapper</kbd> class accepts the iterator variable itself, the variable x name, and the variable y name during initialization. It yields tensor x and y. The x and y values are looked up from the <kbd>batch</kbd> in <kbd>self.dl</kbd> using <kbd>getattr</kbd>.</p>
<p class="mce-root">If GPU is available, this class moves these tensors to the GPU as well with <kbd>x.cuda()</kbd> and <kbd>y.cuda()</kbd>, making it ready for consumption by the model.</p>
<p class="mce-root">Let's quickly wrap our <kbd>train</kbd>, <kbd>val</kbd>, and <kbd>test iter</kbd> objects using this new class:</p>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="p">train_dl = BatchWrapper(train_iter, "comment_text", ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"])<br/><br/>valid_dl = BatchWrapper(val_iter, "comment_text", ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"])<br/><br/>test_dl = BatchWrapper(test_iter, "comment_text", None)</span></pre></div>
</div>
</div>
<p class="mce-root"><span>This returns the simplest iterator, ready for model processing. Note that, in this particular case, the tensor has a "device" attribute set to <kbd>cuda:0</kbd>. Let's preview this:</span></p>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="nb">next</span><span class="p">(</span><span class="n">train_dl</span><span class="o">.</span><span class="n">__iter__</span><span class="p">())<br/><br/>&gt; </span>(tensor([[ 453,   63,   15,  ...,  454,  660,  778],
         [ 523,    4,  601,  ...,   78,   11,  650],
         ...,
         [   1,    1,    1,  ...,    1,    1,    1],
         [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0'),
 tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.],
         ...,
         [ 0.,  0.,  0.,  0.,  0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.]], device='cuda:0'))</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a text classifier</h1>
                </header>
            
            <article>
                
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are now ready for training our text classifier model. Let's start with something simple: we are going to consider this model to be a black box for now.</p>
<p>Model architecture is better explained by other sources, including several YouTube videos such as those by CS224n at Stanford (<a href="http://web.stanford.edu/class/cs224n/">http://web.stanford.edu/class/cs224n/</a><a href="http://web.stanford.edu/class/cs224n/">)</a>. I suggest that you explore and connect it with the know-how that you already have:</p>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">class SimpleLSTMBaseline(nn.Module):<br/>    def __init__(self, hidden_dim, emb_dim=300,<br/>                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=2):<br/>        super().__init__() # don't forget to call this!<br/>        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)<br/>        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=num_linear, dropout=recurrent_dropout)<br/>        self.linear_layers = []<br/>        for _ in range(num_linear - 1):<br/>            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))<br/>        self.linear_layers = nn.ModuleList(self.linear_layers)<br/>        self.predictor = nn.Linear(hidden_dim, 6)<br/>    <br/>    def forward(self, seq):<br/>        hdn, _ = self.encoder(self.embedding(seq))<br/>        feature = hdn[-1, :, :]<br/>        for layer in self.linear_layers:<br/>            feature = layer(feature)<br/>        preds = self.predictor(feature)<br/>        return preds</span></pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<p class="prompt input_prompt">All PyTorch models inherit from <kbd>torch.nn.Module</kbd>. They must all implement the <kbd>forward</kbd> function, which is executed when the model makes a prediction. The corresponding <kbd>backward</kbd> function for training is auto-computed.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing the model</h1>
                </header>
            
            <article>
                
<p>Any Pytorch model is instantiated like a Python object. Unlike TensorFlow, there is no strict notion of a session object inside which the code is compiled and then run. The model class is as we have written previously.</p>
<p>The <kbd>init</kbd> function of the preceding class accepts a few parameters:</p>
<ul>
<li><kbd>hidden_dim</kbd>: These are hidden layer dimensions, that is, the vector length of the hidden layers</li>
<li><kbd>emb_dim=300</kbd>: This is an embedding dimension, that is, the vector length of the first input <em>step</em> to the LSTM</li>
<li><kbd>num_linear=2</kbd>: The other two dropout parameters:
<ul>
<li><kbd>spatial_dropout=0.05</kbd></li>
<li><kbd>recurrent_dropout=0.1</kbd></li>
</ul>
</li>
</ul>
<p>Both dropout parameters act as regularizers. They help prevent the model from overfitting, that is, the state where the model ends up learning the samples in the training set instead of the more generic pattern that can be used to make predictions.</p>
<p>One way to think about the differences between the dropouts is that one of them acts on the input itself. The other acts during backpropagation or the weight update step, as mentioned earlier:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="p">em_sz = 300<br/>nh = 500<br/>model = SimpleLSTMBaseline(nh, emb_dim=em_sz)<br/>print(model)<br/><br/></span>SimpleLSTMBaseline(<br/>  (embedding): Embedding(784, 300)<br/>  (encoder): LSTM(300, 500, num_layers=2, dropout=0.1)<br/>  (linear_layers): ModuleList(<br/>    (0): Linear(in_features=500, out_features=500, bias=True)<br/>  )<br/>  (predictor): Linear(in_features=500, out_features=6, bias=True)<br/>)</pre></div>
</div>
</div>
</div>
<p>We can print any PyTorch model to look at the architecture of the class. It is computed from the forward function implementation, which is exactly what we'd expect. This is really helpful when debugging the model.</p>
<p class="mce-root">Let's write a small utility function to calculate the size of any PyTorch model. By size, we mean the number of model parameters that can be updated during training to learn the input-to-target mapping.</p>
<p class="mce-root">While this function is implemented in Keras, it's simple enough to write it again:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="k">def model_size(model: torch.nn)-&gt;int:<br/>    """<br/>    Calculates the number of trainable parameters in any model<br/>    <br/>    Returns:<br/>        params (int): the total count of all model weights<br/>    """<br/>    model_parameters = filter(lambda p: p.requires_grad, model.parameters())<br/>#     model_parameters = model.parameters()<br/>    params = sum([np.prod(p.size()) for p in model_parameters])<br/>    return params<br/><br/>print(f'{model_size(model)/10**6} million parameters')<br/>&gt; 4.096706 million parameters</span></pre></div>
</div>
</div>
</div>
<p class="mce-root">We can see that even our simple baseline model has more than 4 million parameters. In comparison, a typical decision tree might only have a few hundred decision splits, maximum.</p>
<p class="mce-root">Next, we will move the model weights to the GPU using the familiar <kbd>.cuda()</kbd> syntax:</p>
<pre><span class="p">if use_gpu:<br/>    model = model.cuda()</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting the pieces together again</h1>
                </header>
            
            <article>
                
<p>These are the pieces which we looked at, let's quickly summarize them:</p>
<ul>
<li class="mce-root"><strong>Loss function</strong>: Binary cross entropy with Logit loss. It serves as the quality metric of how far the predictions are from the ground truth.
<ul>
<li class="mce-root"><strong>Optimizer</strong>: We use the Adam optimizer with default parameters, set with a learning rate of 1e-2 or 0.01:</li>
</ul>
</li>
</ul>
<p class="mce-root"/>
<p>This is how we would see these 2 components in PyTorch:</p>
<pre><span class="p">from torch import optim<br/>opt = optim.Adam(model.parameters(), lr=1e-2)<br/>loss_func = nn.BCEWithLogitsLoss().cuda()</span></pre>
<p>We call set the number of epochs for which the model has to be trained here:</p>
<pre><span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span></pre>
<p>This is set to a very small value because this entire notebook, model, and training loop is just for demonstrative purposes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training loop</h1>
                </header>
            
            <article>
                
<p>The training loop is logically split into two sections: <kbd>model.train()</kbd> and <kbd>model.eval()</kbd>. Note the placement of the following lines of code:</p>
<pre><span class="p">from tqdm import tqdm<br/>for epoch in range(1, epochs + 1):<br/>    running_loss = 0.0<br/>    running_corrects = 0<br/>    model.train() # turn on training mode<br/>    for x, y in tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!<br/>        opt.zero_grad()<br/>        preds = model(x)<br/>        loss = loss_func(preds, y)<br/>        loss.backward()<br/>        opt.step()<br/>        <br/>        running_loss += loss.item() * x.size(0)<br/>        <br/>    epoch_loss = running_loss / len(trn)<br/>    <br/>    # calculate the validation loss for this epoch<br/>    val_loss = 0.0<br/>    model.eval() # turn on evaluation mode<br/>    for x, y in valid_dl:<br/>        preds = model(x)<br/>        loss = loss_func(preds, y)<br/>        val_loss += loss.item() * x.size(0)<br/><br/>    val_loss /= len(vld)<br/>    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The first half is the actual learning loop. This is the sequence of steps inside the loop:</p>
<ol>
<li>Set the optimizer's gradient to zero</li>
<li>Make model predictions on this training batch in <kbd>preds</kbd></li>
<li>Find the loss using <kbd>loss_func</kbd></li>
<li>Update the model weights using <kbd>loss.backward()</kbd></li>
<li>Update the optimizer state using <kbd>opt.step()</kbd></li>
</ol>
<p>The entire back propagation hassle is handled in one line of code:</p>
<pre>loss.backward()</pre>
<p>This level of abstraction that exposes the model's internals without worrying about the differential calculus aspects is why frameworks such as PyTorch are so convenient and useful.</p>
<p>The second loop is the evaluation loop. This is run on the validation split of the data. We set the model to <em>eval</em> mode, which locks the model weights. The weights will not be updated by accident as long as <kbd>model.eval()</kbd> is not set back to <kbd>model.train()</kbd>.</p>
<p>The only two things we do inside this second loop are simple:</p>
<ul>
<li>Make predictions on the validation split</li>
<li>Calculate the loss on this split</li>
</ul>
<p>The aggregate loss from all validation batches is then printed at the end of every epoch, along with running training loss.</p>
<p>One training loop will look something like the following:</p>
<pre class="mce-root">100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 2.34it/s]<br/><br/>Epoch: 1, Training Loss: 13.5037, Validation Loss: 4.6498<br/>100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 4.58it/s]<br/><br/>Epoch: 2, Training Loss: 7.8243, Validation Loss: 24.5401<br/><br/>100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 3.35it/s]<br/><br/>Epoch: 3, Training Loss: 57.4577, Validation Loss: 4.0107</pre>
<p class="mce-root"/>
<p>We can see that the training loop ends with a low validation loss but a high training loss. This could be indicative of something wrong with either the model or the train and valid data splits. There is no easy way to debug this.</p>
<p>The good way forward is usually to train the model for a few more epochs until no further change in either loss is observed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prediction mode</h1>
                </header>
            
            <article>
                
<p>Let's use the model we trained to make some predictions on the test data:</p>
<pre><span class="p">test_preds = []<br/>model.eval()<br/>for x, y in tqdm(test_dl):<br/>    preds = model(x)<br/>    # if you're data is on the GPU, you need to move the data back to the cpu<br/>    preds = preds.data.cpu().numpy()<br/>    # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function<br/>    preds = 1 / (1 + np.exp(-preds))<br/>    test_preds.append(preds)<br/>test_preds = np.hstack(test_preds)</span></pre>
<p>The entire loop is now in eval mode, which we use to lock the model weights. Alternatively, we could have set <kbd>model.train(False)</kbd> as well. </p>
<p>We iteratively take batchsize samples from the test iterator, make predictions, and append them to a list. At the end, we stack them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting predictions into a pandas DataFrame</h1>
                </header>
            
            <article>
                
<p class="mce-root">This helps us convert the predictions into a more interpretable format. Let's read the test dataframe and insert the predictions in the correct columns:</p>
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"data/test.csv"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">"toxic"</span><span class="p">,</span> <span class="s2">"severe_toxic"</span><span class="p">,</span> <span class="s2">"obscene"</span><span class="p">,</span> <span class="s2">"threat"</span><span class="p">,</span> <span class="s2">"insult"</span><span class="p">,</span> <span class="s2">"identity_hate"</span><span class="p">]):</span>
    <span class="n">test_df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_preds</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span></pre></div>
</div>
</div>
<p>Now, we can preview a few of the rows of the DataFrame:</p>
<pre>test_df.head(3)</pre>
<p class="mce-root"><span>We get the following output:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<thead>
<tr>
<th/>
<th>id</th>
<th><kbd>comment_text</kbd></th>
<th><kbd>toxic</kbd></th>
<th><kbd>severe_toxic</kbd></th>
<th><kbd>obscene</kbd></th>
<th><kbd>threat</kbd></th>
<th><kbd>insult</kbd></th>
<th><kbd>identity_hate</kbd></th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>00001cee341fdb12</td>
<td>Yo bitch Ja Rule is more succesful then you'll...</td>
<td>0.629146</td>
<td>0.116721</td>
<td>0.438606</td>
<td>0.156848</td>
<td>0.139696</td>
<td>0.388736</td>
</tr>
<tr>
<td>1</td>
<td>0000247867823ef7</td>
<td><kbd>== From RfC == \r\n\r\n</kbd> The title is fine as i...</td>
<td>0.629146</td>
<td>0.116721</td>
<td>0.438606</td>
<td>0.156848</td>
<td>0.139696</td>
<td>0.388736</td>
</tr>
<tr>
<td>2</td>
<td>00013b17ad220c46</td>
<td>"<kbd>\r\n\r\n == Sources == \r\n\r\n *</kbd>. Zawe Ashto...</td>
<td>0.629146</td>
<td>0.116721</td>
<td>0.438606</td>
<td>0.156848</td>
<td>0.139696</td>
<td>0.388736</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This was our first brush with deep learning for NLP. This was very a thorough introduction to <kbd>torchtext</kbd> and how we can leverage it with Pytorch. We also got a very broad view of deep learning as a puzzle of only two or three broad pieces: the model, the optimizer, and the loss functions. This is true irrespective of what framework or dataset you use.</p>
<p>We did skimp a bit on the model architecture explanation in the interest of keeping this short. We will avoid using concepts that have not been explained here in other sections.</p>
<p>When we are working with modern ensembling methods, we don't always know how a particular prediction is being made. That's a black box to us, in the same sense that all deep learning model predictions are a black box.</p>
<p>In the next chapter, we will look at some tools and techniques that will help us look into these boxes <span>– </span>at least a little bit more.</p>


            </article>

            
        </section>
    </body></html>