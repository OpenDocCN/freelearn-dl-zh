["```py\n@registry.architectures(\"spacy.HashEmbedCNN.v2\")\ndef build_hash_embed_cnn_tok2vec(\n    *,\n    width: int,\n    depth: int,\n    embed_size: int,\n    window_size: int,\n    maxout_pieces: int,\n    subword_features: bool,\n    pretrained_vectors: Optional[bool],\n) -> Model[List[Doc], List[Floats2d]]:\n    if subword_features:\n        attrs = [\"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"]\n        row_sizes = [embed_size, embed_size // 2, \n                     embed_size // 2, embed_size // 2]\n    else:\n        attrs = [\"NORM\"]\n        row_sizes = [embed_size]\n    return build_Tok2Vec_model(\n        embed=MultiHashEmbed(\n            width=width,\n            rows=row_sizes,\n            attrs=attrs,\n            include_static_vectors=bool(pretrained_vectors),\n        ),\n        encode=MaxoutWindowEncoder(\n            width=width,\n            depth=depth,\n            window_size=window_size,\n            maxout_pieces=maxout_pieces,\n        ),\n    )\n```", "```py\n@registry.architectures(\"spacy.EntityLinker.v2\")\ndef build_nel_encoder(\n    tok2vec: Model, nO: Optional[int] = None\n) -> Model[List[Doc], Floats2d]:\n    with Model.define_operators({\">>\": chain, \"&\": tuplify}):\n        token_width = tok2vec.maybe_get_dim(\"nO\")\n        output_layer = Linear(nO=nO, nI=token_width)\n        model = (\n            ((tok2vec >> list2ragged()) & build_span_maker())\n            >> extract_spans()\n            >> reduce_mean()\n            >> residual(Maxout(nO=token_width, nI=token_width, \n                               nP=2, dropout=0.0))  # type: ignore\n            >> output_layer\n        )\n        model.set_ref(\"output_layer\", output_layer)\n        model.set_ref(\"tok2vec\", tok2vec)\n    # flag to show this isn't legacy\n    model.attrs[\"include_span_maker\"] = True\n    return model\n```", "```py\n[components.entity_linker.model]\n@architectures = \"spacy.EntityLinker.v2\"\nnO = null\n[components.entity_linker.model.tok2vec]\n@architectures = \"spacy.HashEmbedCNN.v1\"\npretrained_vectors = null\nwidth = 96\ndepth = 2\nembed_size = 2000\nwindow_size = 1\nmaxout_pieces = 3\nsubword_features = true\n```", "```py\n    import spacy\n    nlp = spacy.load(\"en_core_web_md\")\n    ruler = nlp.add_pipe(\"span_ruler\", after=\"ner\")\n    patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"taylor\"}]}]\n    ruler.add_patterns(patterns)\n    ```", "```py\n    kb_loc = \"chapter_10/nel_taylor/my_kb\"\n    nlp_dir = \"chapter_10/nel_taylor/my_nlp\"\n    ```", "```py\n    import os\n    from spacy.kb import InMemoryLookupKB\n    kb = InMemoryLookupKB(vocab=nlp.vocab, entity_vector_length=300)\n    ```", "```py\n    entities = {'Q26876': 'Taylor Swift', 'Q23359': 'Taylor Lautner', 'Q17660516': 'Taylor Fritz'}\n    descriptions = {'Q26876': 'American singer-songwriter (born 1989)', 'Q23359': 'American actor', 'Q17660516': 'American tennis player'}\n    ```", "```py\n    for qid, desc in descriptions.items():\n        desc_doc = nlp(desc)\n        desc_vector = desc_doc.vector\n        kb.add_entity(entity=qid, entity_vector=desc_vector,\n                      freq=111)\n    ```", "```py\n    for qid, name in entities.items():\n        kb.add_alias(alias=name, entities=[qid], probabilities=[1])\n    ```", "```py\n    qids = entities.keys()\n    kb.add_alias(alias=\"Taylor\", entities=qids, \n                 probabilities=[0.3, 0.3, 0.3])\n    ```", "```py\n    print(f\"Entities in the KB: {kb.get_entity_strings()}\")\n    >>> Entities in the KB: ['Q23359', 'Q17660516', 'Q26876']\n    print(f\"Aliases in the KB: {kb.get_alias_strings()}\")\n    >>> Aliases in the KB: ['Taylor Lautner', 'Taylor', 'Taylor Swift', 'Taylor Fritz']\n    ```", "```py\n    kb.to_disk(kb_loc)\n    if not os.path.exists(nlp_dir):\n        os.mkdir(nlp_dir)\n    nlp.to_disk(nlp_dir)\n    ```", "```py\n    import pandas as pd\n    df_labeled = pd.read_csv(\"https://raw.githubusercontent.com/PacktPublishing/Mastering-spaCy-Second-Edition/main/chapter_10/taylor_labeled_dataset.csv\")\n    df_train = df_labeled.sample(frac=0.8, random_state=123)\n    df_test = df_labeled.drop(df_train.index)\n    ```", "```py\n    import spacy\n    from spacy.tokens import Span\n    from collections import Counter\n    nlp_dir = \"chapter_10/nel_taylor/my_nlp\"\n    nlp = spacy.load(nlp_dir)\n    docs = []\n    QIDs = []\n    for _,row in df_train.iterrows():\n        sentence = row[\"text\"]\n        QID = row[\"QID\"]\n        span_start = row[\"ent_start\"]\n        span_end = row[\"ent_end\"]\n        doc = nlp(sentence)\n        QIDs.append(QID)\n        label_ent = \"PERSON\"\n        ent_span = Span(doc, span_start, span_end, label_ent, \n                        kb_id=QID)\n        doc.ents = [ent_span]\n        docs.append(doc)\n    ```", "```py\n    from spacy.tokens import DocBin\n    import math\n    train_docs = DocBin()\n    dev_docs = DocBin()\n    ```", "```py\n    entities = {'Q26876': 'Taylor Swift', \n                'Q23359': 'Taylor Lautner',\n                'Q17660516': 'Taylor Fritz'}\n    for QID in entities.keys():\n        indexes_sentences_qid = [i for i, j in enumerate(QIDs) \n                                 if j == QID]\n        for index in indexes_sentences_qid[0:8]:\n            train_docs.add(docs[index])\n        for index in indexes_sentences_qid[8:]:\n            dev_docs.add(docs[index])\n    ```", "```py\n    train_corpus = \"chapter_10/nel_taylor/train.spacy\"\n    dev_corpus = \"chapter_10/nel_taylor/dev.spacy\"\n    train_docs.to_disk(train_corpus)\n    dev_docs.to_disk(dev_corpus)\n    ```", "```py\n    def read_files(file: Path, nlp: \"Language\") -> Iterable[Example]:\n       with nlp.select_pipes(disable=\"entity_linker\"):\n          doc_bin = DocBin().from_disk(file)\n          docs = doc_bin.get_docs(nlp.vocab)\n    ```", "```py\n    # ...\n          for doc in docs:\n             yield Example(nlp(doc.text), doc)\n    ```", "```py\n    from functools import partial\n    from pathlib import Path\n    from typing import Iterable, Callable\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n    @spacy.registry.readers(\"MyCorpus.v1\")\n    def create_docbin_reader(file: Path) -> Callable[[\"Language\"], Iterable[Example]]:\n        return partial(read_files, file)\n    ```", "```py\n    #config.cfg snippet\n    [corpora]\n    [corpora.train]\n    @readers = \"MyCorpus.v1\"\n    file = ${paths.train}\n    [corpora.dev]\n    @readers = \"MyCorpus.v1\"\n    file = ${paths.dev}\n    ```", "```py\n    from functools import partial\n    from pathlib import Path\n    from typing import Iterable, Callable\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n    @spacy.registry.readers(\"MyCorpus.v1\")\n    def create_docbin_reader(file: Path) -> Callable[[\"Language\"],\n\n    Iterable[Example]]:\n        return partial(read_files, file)\n    def read_files(file: Path, nlp: \"Language\") -> Iterable[Example]:\n        # we run the full pipeline and not just nlp.make_doc to\n        # ensure we have entities and sentences\n        # which are needed during training of the entity linker\n        with nlp.select_pipes(disable=\"entity_linker\"):\n            doc_bin = DocBin().from_disk(file)\n            docs = doc_bin.get_docs(nlp.vocab)\n            for doc in docs:\n                yield Example(nlp(doc.text), doc)\n    ```", "```py\n    !curl https://raw.githubusercontent.com/PacktPublishing/Mastering-spaCy-Second-Edition/main/chapter_10/config.cfg -o config.cfg\n    ```", "```py\n    python -m spacy train ./config.cfg --output entity_linking_taylor --paths.train ./nel_taylor/train.spacy --paths.dev ./nel_taylor/dev.spacy --paths.kb nel_taylor/my_kb --paths.base_nlp ./nel_taylor/my_nlp --code custom_functions.py\n    ```", "```py\n    nlp = spacy.load(\"entity_linking_taylor/model-best\")\n    ```", "```py\n    text = 'Taylor struggled with chilly temperatures in Edinburgh, pausing the show to warm up her hands and to assist a distressed fan.'\n    doc = nlp(text)\n    ```", "```py\n    from spacy import displacy\n    displacy.serve(doc, style=\"ent\")\n    ```", "```py\ntext = 'Now, Taylor has revealed that he had to re-audition for the part because the producers wanted to go in a different direction.'\ndoc = nlp(text)\n```"]