<html><head></head><body>
  <div><h1 class="chapter-number" id="_idParaDest-196">
    <a id="_idTextAnchor198">
    </a>
    
     10
    
   </h1>
   <h1 id="_idParaDest-197">
    <a id="_idTextAnchor199">
    </a>
    
     Improving Agents with the Perception System
    
   </h1>
   <p>
    
     The AI
    
    <strong class="bold">
     
      Perception System
     
    </strong>
    
     is a powerful
    
    <a id="_idIndexMarker501">
    </a>
    
     tool in the Unreal Engine Gameplay Framework, as it allows AI-controlled actors to perceive and react to various stimuli in an environment.
    
    
     It provides a way for AI agents to become aware of the presence of other actors – such as players or enemies – through different senses such as sight, hearing, or touch.
    
    
     By properly configuring and using the Perception System, developers can create AI agents that respond appropriately to events in their surroundings.
    
    
     What’s more, this system allows developers to implement and configure custom senses tailored to their game’s specific needs.
    
    
     This flexibility enables developers to create unique and engaging
    
    
     
      AI experiences.
     
    
   </p>
   <p>
    
     In this chapter, we will be covering the main components of the Unreal Engine Perception System, starting with a bit of theory and then applying this newly acquired knowledge to a real-world example.
    
   </p>
   <p>
    
     In this chapter, we will be covering the
    
    
     
      following topics:
     
    
   </p>
   <ul>
    <li>
     
      Presenting the
     
     
      
       Perception System
      
     
    </li>
    <li>
     
      Adding perception to
     
     
      
       an agent
      
     
    </li>
    <li>
     
      
       Debugging perception
      
     
    </li>
    <li>
     
      Creating
     
     
      
       perception stimuli
      
     
    </li>
   </ul>
   <h1 id="_idParaDest-198">
    <a id="_idTextAnchor200">
    </a>
    
     Technical requirements
    
   </h1>
   <p>
    
     To follow along with the topics presented in this chapter, you should have completed the previous ones and understood
    
    
     
      their content.
     
    
   </p>
   <p>
    
     Additionally, if you would prefer to begin with code from the companion repository for this book, you can download the
    
    <strong class="source-inline">
     
      .zip
     
    </strong>
    
     project files provided in this book’s companion project
    
    
     
      repository:
     
    
    <a href="https://github.com/PacktPublishing/Artificial-Intelligence-in-Unreal-Engine-5">
     
      
       https://github.com/PacktPublishing/Artificial-Intelligence-in-Unreal-Engine-5
      
     
    </a>
    
     
      .
     
    
   </p>
   <p>
    
     To download the files from the end of the last chapter, click the
    
    <strong class="source-inline">
     
      Unreal Agility Arena –
     
    </strong>
    
     <strong class="source-inline">
      
       Chapter 09
      
     </strong>
    
    <strong class="source-inline">
     
      -
     
    </strong>
    
     <strong class="source-inline">
      
       End
      
     </strong>
    
    
     
      link.
     
    
   </p>
   <h1 id="_idParaDest-199">
    <a id="_idTextAnchor201">
    </a>
    
     Presenting the Perception System
    
   </h1>
   <p>
    
     Dr.
    
    
     Markus and Professor Viktoria
    
    <a id="_idIndexMarker502">
    </a>
    
     seem to have a new chapter in
    
    
     
      their story:
     
    
   </p>
   <p>
    <em class="italic">
     
      Dr.
     
     
      Markus and Professor Viktoria knew that allowing sophisticated synthetic beings such as their AI dummy puppets to roam unchecked could prove disastrous.
     
     
      Therefore, they started working tirelessly to develop an intricate network of hidden security cameras that could monitor the movements and actions of their creations at all times.
     
     
      With this vigilant surveillance system in place, they hoped to keep them under strict observation and maintain full control, ensuring the safety of their
     
    </em>
    
     <em class="italic">
      
       controversial research.
      
     </em>
    
   </p>
   <p>
    
     One of the key components when creating intelligent and reactive AI agents in Unreal Engine is the AI Perception System; this powerful system allows AI controllers – and, consequently, AI agents – to perceive and respond to different stimuli in their
    
    
     
      virtual environment.
     
    
   </p>
   <p>
    
     At the core of the AI Perception
    
    <a id="_idIndexMarker503">
    </a>
    
     System are
    
    <strong class="bold">
     
      senses
     
    </strong>
    
     and
    
    <strong class="bold">
     
      stimuli
     
    </strong>
    
     .
    
    
     A sense – such as sight or hearing – represents
    
    <a id="_idIndexMarker504">
    </a>
    
     a way for an AI agent to perceive its surroundings and is configured to detect specific types of stimuli, which are sources of perception data emanating from other actors in the
    
    
     
      game world.
     
    
   </p>
   <p>
    
     As an example,
    
    <em class="italic">
     
      sight sense
     
    </em>
    
     is preconfigured to detect any visible pawn actors, while
    
    <em class="italic">
     
      damage sense
     
    </em>
    
     triggers when the associated AI controller’s pawn takes damage from an
    
    
     
      external source.
     
    
   </p>
   <p class="callout-heading">
    
     Note
    
   </p>
   <p class="callout">
    
     As a developer, you can create custom senses tailored to your game’s specific needs by extending the
    
    <strong class="source-inline">
     
      AISense
     
    </strong>
    
     class, if you are working with C++, or the
    
    <strong class="source-inline">
     
      AISense_Blueprint
     
    </strong>
    
     class, if you are working
    
    
     
      with Blueprints.
     
    
   </p>
   <h2 id="_idParaDest-200">
    <a id="_idTextAnchor202">
    </a>
    
     AI Perception System components
    
   </h2>
   <p>
    
     The AI Perception System
    
    <a id="_idIndexMarker505">
    </a>
    
     consists of the following
    
    
     
      main classes:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       AIPerceptionSystem
      
     </strong>
     
      : This is the core manager that keeps track of all AI
     
     
      
       stimuli sources.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       AIPerceptionComponent
      
     </strong>
     
      : This represents the AI agent’s mind and handles processing perceived stimuli.
     
     
      It needs to be attached to an AI controller to
     
     
      
       properly work.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       AIPerceptionStimuliSourceComponent
      
     </strong>
     
      : This component is added to actors that can generate stimuli and is in charge of broadcasting perception data to
     
     
      
       listening elements.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       AIPerceptionSenseConfig
      
     </strong>
     
      : This defines the properties of a specific sense, what actors can be perceived, and how perception decays over time
     
     
      
       or distance.
      
     
    </li>
   </ul>
   <p>
    
     When an actor with
    
    <strong class="source-inline">
     
      AIPerceptionStimuliSourceComponent
     
    </strong>
    
     generates a stimulus, nearby
    
    <strong class="source-inline">
     
      AIPerceptionComponents
     
    </strong>
    
     detect it through their configured senses.
    
    
     This perceived data is then processed by the AI controller to trigger
    
    
     
      desired behaviors.
     
    
   </p>
   <p>
    
     Once you have added
    
    <strong class="source-inline">
     
      AIPerceptionComponent
     
    </strong>
    
     to an AI controller, you will need to add one or more
    
    <strong class="source-inline">
     
      AIPerceptionSenseConfig
     
    </strong>
    
     elements in order to give your AI agent dedicated senses.
    
    
     <em class="italic">
      
       Figure 10
      
     </em>
    
    <em class="italic">
     
      .1
     
    </em>
    
     shows an example where perception is based
    
    
     
      on touch:
     
    
   </p>
   <div><div><img alt="Figure 10.1 – The touch sense config" src="img/Figure_10.1_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.1 – The touch sense config
    
   </p>
   <p>
    
     From the previous screenshot, you may have noticed a
    
    <strong class="bold">
     
      Dominant Sense
     
    </strong>
    
     property; this property allows you to designate a specific sense that takes priority over others when determining the location of a
    
    
     
      sensed actor.
     
    
   </p>
   <p>
    
     Let’s explore the
    
    <a id="_idIndexMarker506">
    </a>
    
     available sense configs that, as mentioned earlier, define the properties of each
    
    
     
      specific sense:
     
    
   </p>
   <h2 id="_idParaDest-201">
    <a id="_idTextAnchor203">
    </a>
    
     AIPerceptionSenseConfig types
    
   </h2>
   <p>
    
     Unreal Engine offers
    
    <a id="_idIndexMarker507">
    </a>
    
     a range of predefined
    
    <strong class="source-inline">
     
      AIPerceptionSenseConfig
     
    </strong>
    
     classes that are highly likely to meet your specific requirements.
    
    
     Let’s take a look at the
    
    
     
      available options:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       AIDamag
      
     </strong>
     
      e: Use this configuration if your AI agent needs to respond to damage events such as
     
     <em class="italic">
      
       Any Damage
      
     </em>
     
      ,
     
     <em class="italic">
      
       Point Damage
      
     </em>
     
      , or
     
     
      <em class="italic">
       
        Radial Damage
       
      </em>
     
    </li>
    <li>
     <strong class="source-inline">
      
       AIHearing
      
     </strong>
     
      : Use this configuration if you need to detect sounds generated in the
     
     
      
       surrounding environment
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       AIPrediction
      
     </strong>
     
      : Use this configuration when you need to predict the target actor location in the next
     
     
      
       few moments
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       AISight
      
     </strong>
     
      : Use this configuration when you want your AI agent to see things in
     
     
      
       the level
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       AITeam
      
     </strong>
     
      : Use this configuration if you want to notify the AI agent that some ally
     
     
      
       is nearby
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       AITouch
      
     </strong>
     
      : Use this configuration when the AI agent touches some other actor or, vice versa, when something is touching the
     
     
      
       AI agent
      
     
    </li>
   </ul>
   <h2 id="_idParaDest-202">
    <a id="_idTextAnchor204">
    </a>
    
     Stimuli source
    
   </h2>
   <p>
    
     The
    
    <strong class="source-inline">
     
      AIPerceptionStimuliSourceComponent
     
    </strong>
    
     class allows an actor to register itself as a source
    
    <a id="_idIndexMarker508">
    </a>
    
     of stimuli for one or more senses.
    
    
     For instance, you can register an actor as a stimuli source for sight.
    
    
     This registration allows an AI agent to visually perceive the actor in the
    
    
     
      game level.
     
    
   </p>
   <p>
    
     A stimuli source can be registered – or unregistered – for a sense, making it detectable – or undetectable – by the
    
    
     
      Perception System.
     
    
   </p>
   <p class="callout-heading">
    
     Note
    
   </p>
   <p class="callout">
    
     The
    
    <strong class="source-inline">
     
      Pawn
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      Character
     
    </strong>
    
     classes in Unreal Engine are inherently visible to
    
    <strong class="source-inline">
     
      AISight
     
    </strong>
    
     perception due to their default behavior as stimuli sources.
    
    
     This design choice streamlines AI behavior development by eliminating the need to manually configure visibility for each character or pawn.
    
    
     However, if you want the AI to ignore specific characters, you’ll need to take additional steps to configure
    
    
     
      them accordingly.
     
    
   </p>
   <p>
    
     In this section, we were introduced to the Perception System and its main elements.
    
    
     In the next section, we
    
    <a id="_idIndexMarker509">
    </a>
    
     will work on a fully functional AI agent that will let you sense other actors in
    
    
     
      your game.
     
    
   </p>
   <h1 id="_idParaDest-203">
    <a id="_idTextAnchor205">
    </a>
    
     Adding perception to an agent
    
   </h1>
   <p>
    
     In this section, we will
    
    <a id="_idIndexMarker510">
    </a>
    
     create a new AI agent that will use the Perception System.
    
    
     We will create a security camera that will probe nearby surrounding areas, looking for some possible targets for the dummy gunner that we created in
    
    <a href="B31016_09.xhtml#_idTextAnchor170">
     
      <em class="italic">
       
        Chapter 9
       
      </em>
     
    </a>
    
     ,
    
    <em class="italic">
     
      Extending Behavior Trees
     
    </em>
    
     .
    
    
     Think of it as some kind of infrared camera for a dark environment.
    
    
     Once the camera spots a target, it will tag it so that the gunner will be able to locate it in
    
    
     
      the environment.
     
    
   </p>
   <p>
    
     We will start by creating an
    
    <strong class="source-inline">
     
      Actor
     
    </strong>
    
     class that will be used as the
    
    
     
      camera model.
     
    
   </p>
   <h2 id="_idParaDest-204">
    <a id="_idTextAnchor206">
    </a>
    
     Creating the BaseSecurityCam class
    
   </h2>
   <p>
    
     Even though we
    
    <a id="_idIndexMarker511">
    </a>
    
     will be
    
    <a id="_idIndexMarker512">
    </a>
    
     implementing the Perception System inside the AI controller, a nice model to display in the level will help your environment’s look and feel, so let’s start by creating a new C++ class, extending from the
    
    <strong class="source-inline">
     
      Pawn
     
    </strong>
    
     class and named
    
    <strong class="source-inline">
     
      BaseSecurityCam
     
    </strong>
    
     .
    
    
     Once the class has been created, open the
    
    <strong class="source-inline">
     
      BaseSecurityCam.h
     
    </strong>
    
     file and add the following forward declaration after the
    
    <strong class="source-inline">
     
      #
     
    </strong>
    
     <strong class="source-inline">
      
       include
      
     </strong>
    
    
     
      declarations:
     
    
   </p>
   <pre class="source-code">
class UAIPerceptionComponent;</pre>
   <p>
    
     Then, make the class a
    
    <strong class="source-inline">
     
      Blueprintable
     
    </strong>
    
     one by changing the
    
    <strong class="source-inline">
     
      UCLASS()
     
    </strong>
    
     macro to
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
UCLASS(Blueprintable)</pre>
   <p>
    
     After that, remove the
    
    <strong class="source-inline">
     
      BeginPlay()
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      Tick()
     
    </strong>
    
     declarations, as we won’t be
    
    
     
      using them.
     
    
   </p>
   <p>
    
     As a final step, add the following component declarations for the static meshes that will display the model just after the
    
    
     <strong class="source-inline">
      
       GENERATED_BODY()
      
     </strong>
    
    
     
      macro:
     
    
   </p>
   <pre class="source-code">
UPROPERTY(VisibleAnywhere, BlueprintReadOnly, Category="Security Cam",   meta=(AllowPrivateAccess="true"))
   UStaticMeshComponent* SupportMeshComponent;
UPROPERTY(VisibleAnywhere, BlueprintReadOnly, Category="Security Cam",   meta=(AllowPrivateAccess="true"))
UStaticMeshComponent* CamMeshComponent;</pre>
   <p>
    
     You can
    
    <a id="_idIndexMarker513">
    </a>
    
     now
    
    <a id="_idIndexMarker514">
    </a>
    
     open the
    
    <strong class="source-inline">
     
      BaseSecurityCam.cpp
     
    </strong>
    
     file to implement this class; as a first step, remove the
    
    <strong class="source-inline">
     
      BeginPlay()
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      Tick()
     
    </strong>
    
     functions.
    
    
     Then, locate the constructor and change this line
    
    
     
      of code:
     
    
   </p>
   <pre class="source-code">
PrimaryActorTick.bCanEverTick = true;</pre>
   <p>
    
     Change it
    
    
     
      to this:
     
    
   </p>
   <pre class="source-code">
PrimaryActorTick.bCanEverTick = false;</pre>
   <p>
    
     Now, inside the constructor and just after the aforementioned line of code, add the following block
    
    
     
      of code:
     
    
   </p>
   <pre class="source-code">
SupportMeshComponent = CreateDefaultSubobject&lt;UStaticMeshComponent&gt;(TEXT("Support Mesh"));
RootComponent = SupportMeshComponent;
static ConstructorHelpers::FObjectFinder&lt;UStaticMesh&gt; SupportStaticMeshAsset(
    TEXT("/Game/_GENERATED/MarcoSecchi/SM_SecurityCam_Base.SM_SecurityCam_Base"));
if (SupportStaticMeshAsset.Succeeded())
{
    SupportMeshComponent-&gt;SetStaticMesh(SupportStaticMeshAsset.Object);
}
CamMeshComponent = CreateDefaultSubobject&lt;UStaticMeshComponent&gt;(TEXT("Cam Mesh"));
CamMeshComponent-&gt;SetRelativeLocation(FVector(61.f, 0.f, -13.f));
CamMeshComponent-&gt;SetupAttachment(RootComponent);
static ConstructorHelpers::FObjectFinder&lt;UStaticMesh&gt;   CamStaticMeshAsset(
    TEXT("/Game/_GENERATED/MarcoSecchi/SM_SecurityCam.SM_SecurityCam"));
if (CamStaticMeshAsset.Succeeded())
{
    CamMeshComponent-&gt;SetStaticMesh(CamStaticMeshAsset.Object);
}</pre>
   <p>
    
     You already know all about this from the previous chapters of the book, so I suppose there’s no need to explain it again.
    
    
     With the security camera model created, we can now implement the
    
    <a id="_idIndexMarker515">
    </a>
    
     corresponding
    
    <a id="_idIndexMarker516">
    </a>
    
     AI controller, along with its
    
    
     
      perception sense.
     
    
   </p>
   <h2 id="_idParaDest-205">
    <a id="_idTextAnchor207">
    </a>
    
     Creating the BaseSecurityCamAIController class
    
   </h2>
   <p>
    
     To add a proper
    
    <a id="_idIndexMarker517">
    </a>
    
     controller
    
    <a id="_idIndexMarker518">
    </a>
    
     for the security camera, let’s create a C++ class extending
    
    <strong class="source-inline">
     
      AIController
     
    </strong>
    
     and name it
    
    <strong class="source-inline">
     
      BaseSecurityCamAIController
     
    </strong>
    
     .
    
    
     Once the class has been created, open the
    
    <strong class="source-inline">
     
      BaseSecurityCamAIController.h
     
    </strong>
    
     file and add the following forward declarations, just after the
    
    <strong class="source-inline">
     
      #
     
    </strong>
    
     <strong class="source-inline">
      
       include
      
     </strong>
    
    
     
      declarations:
     
    
   </p>
   <pre class="source-code">
struct FAIStimulus;
struct FActorPerceptionUpdateInfo;
class UBehaviorTree;</pre>
   <p>
    
     Then, make the class
    
    <strong class="source-inline">
     
      Blueprintable
     
    </strong>
    
     by changing the
    
    <strong class="source-inline">
     
      UCLASS()
     
    </strong>
    
     macro to
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
UCLASS(Blueprintable)</pre>
   <p>
    
     After that, add this block of code just after the already existing
    
    
     
      constructor declaration:
     
    
   </p>
   <pre class="source-code">
protected:
    UPROPERTY(EditAnywhere, BlueprintReadOnly, Category = "Dummy AI Controller")
    TObjectPtr&lt;UBehaviorTree&gt; BehaviorTree;
   virtual void OnPossess(APawn* InPawn) override;
    UFUNCTION()
    void OnTargetPerceptionUpdate(AActor* Actor, FAIStimulus       Stimulus);</pre>
   <p>
    
     You are already familiar with the behavior tree property and the
    
    <strong class="source-inline">
     
      OnPosses()
     
    </strong>
    
     function from
    
    <a href="B31016_08.xhtml#_idTextAnchor148">
     
      <em class="italic">
       
        Chapter 8
       
      </em>
     
    </a>
    
     ,
    
    <em class="italic">
     
      Setting Up a Behavior Tree
     
    </em>
    
     ; in addition, the
    
    <strong class="source-inline">
     
      OnTargetPerceptionUpdate()
     
    </strong>
    
     function will be used as an event handler when getting information from the
    
    
     
      Perception System.
     
    
   </p>
   <p>
    
     You can now open
    
    <strong class="source-inline">
     
      BaseSecurityCamAIController.cpp
     
    </strong>
    
     and add the following
    
    <strong class="source-inline">
     
      #include
     
    </strong>
    
     declarations to the top of
    
    
     
      the file:
     
    
   </p>
   <pre class="source-code">
#include "Perception/AIPerceptionComponent.h"
#include "Perception/AISenseConfig_Sight.h"</pre>
   <p>
    
     Now, locate the constructor, and inside of it, add the following block
    
    
     
      of code:
     
    
   </p>
   <pre class="source-code">
const auto SenseConfig_Sight = CreateDefaultSubobject&lt;UAISenseConfig_  Sight&gt;("SenseConfig_Sight");
SenseConfig_Sight-&gt;SightRadius = 1600.f;
SenseConfig_Sight-&gt;LoseSightRadius = 3000.f;
SenseConfig_Sight-&gt;PeripheralVisionAngleDegrees = 45.0f;
SenseConfig_Sight-&gt;DetectionByAffiliation.bDetectEnemies = true;
SenseConfig_Sight-&gt;DetectionByAffiliation.bDetectNeutrals = true;
SenseConfig_Sight-&gt;DetectionByAffiliation.bDetectFriendlies = true;</pre>
   <p>
    
     As you can see, we
    
    <a id="_idIndexMarker519">
    </a>
    
     are
    
    <a id="_idIndexMarker520">
    </a>
    
     creating the sense config for the sight perception, along with some of its properties, such as
    
    <strong class="source-inline">
     
      SightRadius
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      LoseSightRadius
     
    </strong>
    
     , which will determine the distance at which the Perception System can detect something and the distance at which detection will be lost, respectively.
    
    
     As redundant as these two attributes may seem, keep in mind that, once a target has been detected, it will be more difficult to lose perception of it, unless both attributes have the same value.
    
    <strong class="source-inline">
     
      PeripheralVisionAngleDegrees
     
    </strong>
    
     will handle the cone that will be used to check whether an actor is in the line of sight or not.
    
    
     Lastly, the
    
    <strong class="source-inline">
     
      DetectionByAffiliation
     
    </strong>
    
     property is used to handle whether the detected actor is an enemy, friend, or neutral; in this case, we want to check all of them in order to detect anything that is in the line
    
    
     
      of sight.
     
    
   </p>
   <p>
    
     Now, it’s time to add the actual perception component, so add the following piece of code just after the
    
    
     
      previous one:
     
    
   </p>
   <pre class="source-code">
PerceptionComponent = CreateDefaultSubobject&lt;UAIPerceptionComponent&gt;(TEXT("Perception"));
PerceptionComponent-&gt;ConfigureSense(*SenseConfig_Sight);
PerceptionComponent-&gt;SetDominantSense(SenseConfig_Sight-  &gt;GetSenseImplementation());
PerceptionComponent-&gt;OnTargetPerceptionUpdated.AddDynamic(this,   &amp;ABaseSecurityCamAIController::OnTargetPerceptionUpdate);</pre>
   <p>
    
     As you can see, we create the
    
    <strong class="source-inline">
     
      AIPerceptionComponent
     
    </strong>
    
     instance, and then we assign the previously created sight configuration.
    
    
     Lastly, we register to the
    
    <strong class="source-inline">
     
      OnTargetPerceptionUpdated
     
    </strong>
    
     delegate that will notify the component of any changes detected by the
    
    
     
      Perception System.
     
    
   </p>
   <p>
    
     Now, it’s time
    
    <a id="_idIndexMarker521">
    </a>
    
     to implement
    
    <a id="_idIndexMarker522">
    </a>
    
     the
    
    <strong class="source-inline">
     
      OnPosses()
     
    </strong>
    
     function, something that we already know how
    
    
     
      to handle:
     
    
   </p>
   <pre class="source-code">
void ABaseSecurityCamAIController::OnPossess(APawn* InPawn)
{
    Super::OnPossess(InPawn);
    if (ensureMsgf(BehaviorTree, TEXT("Behavior Tree is nullptr! Please assign BehaviorTree in your AI Controller.")))
    {
       RunBehaviorTree(BehaviorTree);
    }
}</pre>
   <p>
    
     The last step is to implement the event handler.
    
    
     To do so, add the following block
    
    
     
      of code:
     
    
   </p>
   <pre class="source-code">
void ABaseSecurityCamAIController::OnTargetPerceptionUpdate(AActor* Actor, FAIStimulus Stimulus)
{
    if (Actor-&gt;Tags.Num() &gt; 0) return;
    const auto SightID = UAISense::GetSenseID&lt;UAISense_Sight&gt;();
    if (Stimulus.Type == SightID &amp;&amp; Stimulus.WasSuccessfullySensed())
    {
       Actor-&gt;Tags.Init({}, 1);
       Actor-&gt;Tags[0] = "ShootingTarget";
    }
}</pre>
   <p>
    
     This function
    
    <a id="_idIndexMarker523">
    </a>
    
     first checks
    
    <a id="_idIndexMarker524">
    </a>
    
     whether the target has already been tagged; in this case, it means that it has already been spotted.
    
    
     It then retrieves the ID of the sight sense, calling
    
    <strong class="source-inline">
     
      GetSenseID()
     
    </strong>
    
     , and checks whether the stimulus type is equal to the sight sense ID and whether the stimulus was successfully sensed.
    
    
     If both conditions are
    
    <strong class="source-inline">
     
      true
     
    </strong>
    
     , it initializes the
    
    <strong class="source-inline">
     
      Tags
     
    </strong>
    
     array, with the first element set to a value of
    
    <strong class="source-inline">
     
      ShootingTarget
     
    </strong>
    
     , in order to make it a viable target for the dummy gunner we have at
    
    
     
      our disposal.
     
    
   </p>
   <p>
    
     The security camera is now ready to go; we just need a nice environment to test
    
    
     
      it on.
     
    
   </p>
   <p>
    
     In this section, we’ve shown you how to properly create an AI agent, taking advantage of the Perception System.
    
    
     In the next section, we will test this agent and learn how to properly
    
    <a id="_idIndexMarker525">
    </a>
    
     debug
    
    <a id="_idIndexMarker526">
    </a>
    
     
      perception information.
     
    
   </p>
   <h1 id="_idParaDest-206">
    <a id="_idTextAnchor208">
    </a>
    
     Debugging perception
    
   </h1>
   <p>
    
     It’s now time to test our
    
    <a id="_idIndexMarker527">
    </a>
    
     perception logic and learn how to properly debug the Perception system at runtime.
    
    
     In order to do this, we will need to add some small improvements to the base dummy character.
    
    
     As previously mentioned, the
    
    <strong class="source-inline">
     
      Pawn
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      Character
     
    </strong>
    
     classes are already registered with sight stimuli, so we won’t need to implement this logic.
    
    
     However, we will need to handle damage, as we will be playing around with both
    
    <strong class="source-inline">
     
      BP_RoamerDummyCharacter
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      BP_GunnerDummyCharacter
     
    </strong>
    
     .
    
    
     It appears that exciting and enjoyable times are just around
    
    
     
      the corner!
     
    
   </p>
   <h2 id="_idParaDest-207">
    <a id="_idTextAnchor209">
    </a>
    
     Enhancing the roamer behavior tree
    
   </h2>
   <p>
    
     The first step in
    
    <a id="_idIndexMarker528">
    </a>
    
     improving our AI agents
    
    <a id="_idIndexMarker529">
    </a>
    
     will be adding some logic to handle damage to the dummy roamer behavior tree.
    
    
     In particular, we want the AI agent to sit down when it is hit by a Nerf gun projectile.
    
    
     We will start by adding a new key to the
    
    
     
      dedicated Blackboard.
     
    
   </p>
   <h3>
    
     Improving the Blackboard
    
   </h3>
   <p>
    
     A new flag is required for
    
    <a id="_idIndexMarker530">
    </a>
    
     the Blackboard to effectively monitor and keep a record of the character that has been hit.
    
    
     So, open up the
    
    <strong class="source-inline">
     
      BB_Dummy
     
    </strong>
    
     asset and do
    
    
     
      the following:
     
    
   </p>
   <ol>
    <li>
     
      Click the
     
     <strong class="bold">
      
       New Key
      
     </strong>
     
      button, and from the dropdown menu,
     
     
      
       select
      
     
     
      <strong class="bold">
       
        Bool
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     
      Name the newly created
     
     
      
       key
      
     
     
      <strong class="source-inline">
       
        IsHit
       
      </strong>
     
     
      
       .
      
     
    </li>
   </ol>
   <p>
    
     As you already know, this will expose a new key available to the behavior tree; additionally the key will be exposed to the AI controller, as we will see
    
    
     
      later on.
     
    
   </p>
   <h3>
    
     Improving the behavior tree
    
   </h3>
   <p>
    
     The behavior tree
    
    <a id="_idIndexMarker531">
    </a>
    
     needs to manage the AI agent being hit; in our case, we want to play a montage with the character sitting down, as it has been eliminated from the game.
    
    
     So, let’s start by opening the
    
    <strong class="source-inline">
     
      BT_RoamerDummy
     
    </strong>
    
     asset and doing
    
    
     
      the following:
     
    
   </p>
   <ol>
    <li>
     
      Right-click on the
     
     <strong class="bold">
      
       Root Sequence
      
     </strong>
     
      node and add a
     
     <strong class="bold">
      
       Blackboard
      
     </strong>
     
      decorator, naming it
     
     <strong class="source-inline">
      
       Is
      
     </strong>
     
      <strong class="source-inline">
       
        Not Hit?
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     
      With the decorator selected, do
     
     
      
       the following:
      
     
     <ul>
      <li>
       
        Set the
       
       <strong class="bold">
        
         Notify Observer
        
       </strong>
       
        attribute to
       
       <strong class="bold">
        
         On
        
       </strong>
       
        <strong class="bold">
         
          Value Change
         
        </strong>
       
      </li>
      <li>
       
        Set the
       
       <strong class="bold">
        
         Observer aborts
        
       </strong>
       
        attribute
       
       
        
         to
        
       
       
        <strong class="bold">
         
          Self
         
        </strong>
       
      </li>
      <li>
       
        Set the
       
       <strong class="bold">
        
         Key Query
        
       </strong>
       
        attribute to
       
       <strong class="bold">
        
         Is
        
       </strong>
       
        <strong class="bold">
         
          Not Set
         
        </strong>
       
      </li>
      <li>
       
        Set the
       
       <strong class="bold">
        
         Blackboard Key
        
       </strong>
       
        attribute
       
       
        
         to
        
       
       
        <strong class="bold">
         
          IsHit
         
        </strong>
       
      </li>
     </ul>
    </li>
    <li>
     
      Rename the root sequence
     
     <strong class="source-inline">
      
       In Game Sequence
      
     </strong>
     
      and disconnect it from the
     
     
      <strong class="bold">
       
        ROOT
       
      </strong>
     
     
      
       node.
      
     
    </li>
    <li>
     
      Add a
     
     <strong class="bold">
      
       Selector
      
     </strong>
     
      node to the
     
     <strong class="bold">
      
       ROOT
      
     </strong>
     
      node and name it
     
     
      <strong class="source-inline">
       
        Root Selector
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     
      Connect the
     
     <strong class="bold">
      
       Root Selector
      
     </strong>
     
      node to the
     
     <strong class="bold">
      
       In Game
      
     </strong>
     
      <strong class="bold">
       
        Sequence
       
      </strong>
     
     
      
       node.
      
     
    </li>
    <li>
     
      Connect the
     
     <strong class="bold">
      
       Root Selector
      
     </strong>
     
      node to a
     
     <strong class="bold">
      
       PlayMontage
      
     </strong>
     
      task, and name the newly created node
     
     <strong class="source-inline">
      
       Sit Montage
      
     </strong>
     
      .
     
     
      This node should be at the right of the
     
     <strong class="bold">
      
       In Game
      
     </strong>
     
      <strong class="bold">
       
        Sequence
       
      </strong>
     
     
      
       node.
      
     
    </li>
    <li>
     
      With the
     
     <strong class="bold">
      
       Sit Montage
      
     </strong>
     
      node selected, set the
     
     <strong class="bold">
      
       Anim Montage
      
     </strong>
     
      attribute to
     
     <strong class="source-inline">
      
       AM_Sit
      
     </strong>
     
      .
     
     
      The modified portion of the behavior tree is depicted in
     
     
      <em class="italic">
       
        Figure 10
       
      </em>
     
     
      <em class="italic">
       
        .2
       
      </em>
     
     
      
       :
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.2 – A modified behavior tree" src="img/Figure_10.2_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.2 – A modified behavior tree
    
   </p>
   <p>
    
     As you can see, the behavior tree will keep on working as before, unless the AI agent has been hit; in that
    
    <a id="_idIndexMarker532">
    </a>
    
     case, the character will sit down and
    
    
     
      stop wandering.
     
    
   </p>
   <p>
    
     It’s now time to improve the AI controller, in order to manage
    
    
     
      incoming damage.
     
    
   </p>
   <h2 id="_idParaDest-208">
    <a id="_idTextAnchor210">
    </a>
    
     Enhancing BaseDummyAIController
    
   </h2>
   <p>
    
     The AI controller for
    
    <a id="_idIndexMarker533">
    </a>
    
     any dummy character
    
    <a id="_idIndexMarker534">
    </a>
    
     will need to handle any damage.
    
    
     We are handling all of this inside the AI controller instead of the character for the sake of simplicity; we will need to communicate with the Blackboard, and this is much simpler and more direct when done from the
    
    
     
      controller itself.
     
    
   </p>
   <p>
    
     Let’s start by opening the
    
    <strong class="source-inline">
     
      BaseDummyAIController.h
     
    </strong>
    
     file and adding the following declaration for the
    
    
     
      damage handler:
     
    
   </p>
   <pre class="source-code">
UFUNCTION()
void OnPawnDamaged(AActor* DamagedActor, float Damage, const   UDamageType* DamageType, AController* InstigatedBy, AActor*     DamageCauser);</pre>
   <p>
    
     Now, open the
    
    <strong class="source-inline">
     
      BaseDummyAIController.cpp
     
    </strong>
    
     file, and in the
    
    <strong class="source-inline">
     
      OnPossess()
     
    </strong>
    
     function, add the following line
    
    
     
      of code:
     
    
   </p>
   <pre class="source-code">
GetPawn()-&gt;OnTakeAnyDamage.AddDynamic(this, &amp;ABaseDummyAIController::OnPawnDamaged);</pre>
   <p>
    
     Next, add the
    
    
     
      following implementation:
     
    
   </p>
   <pre class="source-code">
void ABaseDummyAIController::OnPawnDamaged(AActor* DamagedActor, float Damage, const UDamageType* DamageType,
    AController* InstigatedBy, AActor* DamageCauser)
{
    const auto BlackboardComp = GetBlackboardComponent();
    BlackboardComp-&gt;SetValueAsBool("IsHit", true);
    if (DamagedActor-&gt;Tags.Num() &gt; 0)
    {
       DamagedActor-&gt;Tags[0] = "Untagged";
    }
}</pre>
   <p>
    
     This function is called when a pawn associated with the AI controller is damaged; the function retrieves the Blackboard component of the AI controller and sets the
    
    <strong class="bold">
     
      IsHit
     
    </strong>
    
     key to a value of
    
    <strong class="source-inline">
     
      true
     
    </strong>
    
     .
    
    
     Then, it sets the first tag of the actor to a value of
    
    <strong class="source-inline">
     
      Untagged
     
    </strong>
    
     so that it won’t be a
    
    <a id="_idIndexMarker535">
    </a>
    
     viable target anymore
    
    <a id="_idIndexMarker536">
    </a>
    
     for the
    
    
     
      gunner dummy.
     
    
   </p>
   <p>
    
     With this AI controller all set up, it’s time to create a Blueprint for the
    
    
     
      security camera.
     
    
   </p>
   <h2 id="_idParaDest-209">
    <a id="_idTextAnchor211">
    </a>
    
     Creating security camera Blueprints
    
   </h2>
   <p>
    
     Now, get back to the
    
    <a id="_idIndexMarker537">
    </a>
    
     Unreal Engine Editor, and
    
    <a id="_idIndexMarker538">
    </a>
    
     after compilation has finished, create a Blueprint out of the
    
    <strong class="source-inline">
     
      BaseSecurityCamAIController
     
    </strong>
    
     class, naming it
    
    <strong class="source-inline">
     
      AISecurityCamController
     
    </strong>
    
     .
    
    
     You don’t need to add a behavior tree, as all the logic is handled inside the
    
    
     
      controller itself.
     
    
   </p>
   <p>
    
     Now, create a Blueprint class from the
    
    <strong class="source-inline">
     
      BaseSecurityCam
     
    </strong>
    
     class, and name it
    
    <strong class="source-inline">
     
      BP_SecurityCam
     
    </strong>
    
     .
    
    
     Once it has been created, open it, and in the
    
    <strong class="bold">
     
      Details
     
    </strong>
    
     panel, locate the
    
    <strong class="bold">
     
      AI Controller Class
     
    </strong>
    
     attribute and set its value
    
    
     
      to
     
    
    
     <strong class="source-inline">
      
       AISecurityCamController
      
     </strong>
    
    
     
      .
     
    
   </p>
   <div><div><img alt="Figure 10.3 – The security cam Blueprint" src="img/Figure_10.3_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.3 – The security cam Blueprint
    
   </p>
   <p>
    
     We have gathered all the necessary elements to bring our new gym to life, and we are ready to take the
    
    <a id="_idIndexMarker539">
    </a>
    
     next steps and start the
    
    <a id="_idIndexMarker540">
    </a>
    
     process of debugging the
    
    
     
      Perception System.
     
    
   </p>
   <h2 id="_idParaDest-210">
    <a id="_idTextAnchor212">
    </a>
    
     Creating the gym
    
   </h2>
   <p>
    
     We are now going
    
    <a id="_idIndexMarker541">
    </a>
    
     to create a level to test and debug
    
    <a id="_idIndexMarker542">
    </a>
    
     everything.
    
    
     We want to achieve this kind
    
    
     
      of behavior:
     
    
   </p>
   <ul>
    <li>
     
      One or more AI agents will move around
     
     
      
       the level
      
     
    </li>
    <li>
     
      A security cam will try to spot AI agents and tag them as
     
     
      
       viable targets
      
     
    </li>
    <li>
     
      A gunner will wait for the AI agents to be tagged, in order to shoot
     
     
      
       at them
      
     
    </li>
   </ul>
   <p>
    
     So, let’s start by creating
    
    
     
      the gym:
     
    
   </p>
   <ol>
    <li>
     
      Create a level of your choice, starting from the Level Instances and Packed Level Actors I provided in the
     
     
      
       project template.
      
     
    </li>
    <li>
     
      Add a
     
     <strong class="bold">
      
       NavMeshBoundsVolume
      
     </strong>
     
      actor so that it will cover all the
     
     
      
       walkable areas.
      
     
    </li>
    <li>
     
      Add some obstacles to make things
     
     
      
       more interesting.
      
     
    </li>
    <li>
     
      Add a
     
     <strong class="bold">
      
       BP_GunnerDummyCharacter
      
     </strong>
     
      instance to
     
     
      
       the level.
      
     
    </li>
    <li>
     
      Add one or more
     
     
      <strong class="bold">
       
        BP_RoamerDummyCharacter
       
      </strong>
     
     
      
       instances.
      
     
    </li>
    <li>
     
      Add some
     
     <strong class="bold">
      
       NS_Target
      
     </strong>
     
      Niagara actors that will work as target points for the pathfinding system; just remember to tag them
     
     <strong class="source-inline">
      
       TargetPoint
      
     </strong>
     
      .
     
     
      Make sure that the path will bring the AI agents in the line of sight of
     
     
      
       the gunner.
      
     
    </li>
    <li>
     
      Add one or more
     
     <strong class="bold">
      
       BP_SecurityCam
      
     </strong>
     
      instances to the walls.
     
     
      The final result should be similar to
     
     
      <em class="italic">
       
        Figure 10
       
      </em>
     
     
      <em class="italic">
       
        .4
       
      </em>
     
     
      
       :
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.4 – The gym" src="img/Figure_10.4_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.4 – The gym
    
   </p>
   <p>
    
     Considering the type of scenario, where the gunner character will shoot at targets located by the security camera, I have decided to make the gym a bit juicier by adding a post-process
    
    <a id="_idIndexMarker543">
    </a>
    
     volume
    
    <a id="_idIndexMarker544">
    </a>
    
     that simulates an infra-red scenario, as depicted in
    
    
     <em class="italic">
      
       Figure 10
      
     </em>
    
    
     <em class="italic">
      
       .5
      
     </em>
    
    
     
      :
     
    
   </p>
   <div><div><img alt="Figure 10.5 – The gym with a post-process volume" src="img/Figure_10.5_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.5 – The gym with a post-process volume
    
   </p>
   <p>
    
     This is obviously not mandatory, and you are free to set your post-process environment as
    
    
     
      you wish.
     
    
   </p>
   <p>
    
     Now that
    
    <a id="_idIndexMarker545">
    </a>
    
     the gym is finished, it’s time to start testing it and learn how to debug the
    
    
     
      Perception
     
    
    
     <a id="_idIndexMarker546">
     </a>
    
    
     
      System.
     
    
   </p>
   <h2 id="_idParaDest-211">
    <a id="_idTextAnchor213">
    </a>
    
     Enabling perception debugging
    
   </h2>
   <p>
    
     If you start the
    
    <a id="_idIndexMarker547">
    </a>
    
     simulation, you should see the following
    
    
     
      things happening:
     
    
   </p>
   <ul>
    <li>
     
      As the roamers wander around, the gunner, unaware of their presence,
     
     
      
       will cheer
      
     
    </li>
    <li>
     
      As soon as one of the roamers enters the camera’s line of sight, the gunner will start aiming
     
     
      
       at it
      
     
    </li>
    <li>
     
      Every time a roamer is hit, it will sit down and
     
     
      
       stop wandering
      
     
    </li>
   </ul>
   <p>
    
     You can tweak the security camera parameters to make it more or less attentive to what’s happening in
    
    
     
      the level.
     
    
   </p>
   <p>
    
     At this point, you might be curious about how we determine whether an agent is in the camera’s line of sight.
    
    
     Well, it’s actually quite simple to observe once you enable the
    
    
     
      debugging tools!
     
    
   </p>
   <p>
    
     So, let’s start by enabling the debugging tools, as explained in
    
    <a href="B31016_06.xhtml#_idTextAnchor116">
     
      <em class="italic">
       
        Chapter 6
       
      </em>
     
    </a>
    
     ,
    
    <em class="italic">
     
      Optimizing the
     
    </em>
    
     <em class="italic">
      
       Navigation Mesh
      
     </em>
    
    
     
      .
     
    
   </p>
   <p class="callout-heading">
    
     Note
    
   </p>
   <p class="callout">
    
     Once the simulation starts, you may be wondering why the security camera has a tiny red icon on top of it, while all the dummy puppets have a green one, as displayed in
    
    
     <em class="italic">
      
       Figure 10
      
     </em>
    
    <em class="italic">
     
      .6
     
    </em>
    
     .
    
    
     When the AI debugging tools are enabled, a green icon is displayed if a pawn has some AI logic set up and running; otherwise, the icon will be red.
    
    
     In our case, the security cam is possessed by a dedicated AI controller, but it has no behavior tree, so the icon will
    
    
     
      be red.
     
    
   </p>
   <div><div><img alt="img" role="presentation" src="img/Figure_10.6_B31016.jpg"/>
     
    </div>
   </div>
   <p class="callout">
    
     
      Figure 10
     
    
    
     .6 –
    
    
     
      AI icons
     
    
   </p>
   <p>
    
     Now, while the simulation is going on, select a security camera and enable the
    
    <strong class="bold">
     
      Perception
     
    </strong>
    
     and
    
    <strong class="bold">
     
      Perception System
     
    </strong>
    
     tools by pressing the
    
    <em class="italic">
     
      4
     
    </em>
    
     and
    
    <em class="italic">
     
      5
     
    </em>
    
     keys on your numpad, respectively.
    
    
     You
    
    <a id="_idIndexMarker548">
    </a>
    
     should immediately see a visualization of the security camera sight sense, as depicted in
    
    
     <em class="italic">
      
       Figure 10
      
     </em>
    
    
     <em class="italic">
      
       .7
      
     </em>
    
    
     
      :
     
    
   </p>
   <div><div><img alt="Figure 10.7 – Perception debugging tools" src="img/Figure_10.7_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.7 – Perception debugging tools
    
   </p>
   <p>
    
     The display will show some important information on the AI agent perception, such as the active sense and its data.
    
    
     Additionally, you will also see a visual representation of the agent sense.
    
    
     In particular, the sight sense will show
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     
      A green circular area that represents the range of the
     
     
      
       agent sight.
      
     
    </li>
    <li>
     
      A pink circular area that represents the maximum range of the agent sight.
     
     
      Once the spotted agent goes beyond this range, sight contact will
     
     
      
       be lost.
      
     
    </li>
    <li>
     
      A green angle that represents the peripheral vision of
     
     
      
       the agent.
      
     
    </li>
   </ul>
   <p>
    
     Once an AI agent enters the green circle, it will be detected by the Perception System, and you should see a green line, starting from the security camera and ending at the detected pawn, as
    
    <a id="_idIndexMarker549">
    </a>
    
     depicted in
    
    
     <em class="italic">
      
       Figure 10
      
     </em>
    
    
     <em class="italic">
      
       .8
      
     </em>
    
    
     
      :
     
    
   </p>
   <div><div><img alt="Figure 10.8 – A pawn detected" src="img/Figure_10.8_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.8 – A pawn detected
    
   </p>
   <p>
    
     A green wireframe sphere will show the detection point, which will be updated as the detected AI agent
    
    
     
      moves around.
     
    
   </p>
   <p>
    
     Once an AI agent moves out of sight, the detection point will stop following it, and you should see the
    
    <strong class="bold">
     
      age
     
    </strong>
    
     label next to the sphere, updating its value; this is the time that has passed since detection was lost.
    
    
     <em class="italic">
      
       Figure 10
      
     </em>
    
    <em class="italic">
     
      .9
     
    </em>
    
     shows
    
    
     
      this scenario:
     
    
   </p>
   <div><div><img alt="Figure 10.9 – Detection lost" src="img/Figure_10.9_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.9 – Detection lost
    
   </p>
   <p>
    
     Each sense has its own way of displaying info, so my advice is to start experimenting with each of them to get an understanding of how to debug and get information from the
    
    
     
      debugging tools.
     
    
   </p>
   <p>
    
     In this section, we have created a new gym and tested how the Perception System works; what’s more, I have shown you how to enable the debugging tools and get a better understanding of what’s happening in the level at runtime.
    
    
     In the next section, we are going to take
    
    <a id="_idIndexMarker550">
    </a>
    
     a look at perception stimuli, in order to make our levels more articulated
    
    
     
      and engaging.
     
    
   </p>
   <h1 id="_idParaDest-212">
    <a id="_idTextAnchor214">
    </a>
    
     Creating perception stimuli
    
   </h1>
   <p>
    
     In the previous section, we
    
    <a id="_idIndexMarker551">
    </a>
    
     utilized the out-of-the-box pawn feature that enables it to be visible to sight sense.
    
    
     We will now analyze actors that are not perceivable by default; this means we will need to add
    
    <strong class="source-inline">
     
      AIPerceptionStimuliSourceComponent
     
    </strong>
    
     to an actor.
    
    
     What’s more, we will learn how to register or unregister these stimuli, in order to make the actor visible or invisible to the
    
    
     
      Perception System.
     
    
   </p>
   <h2 id="_idParaDest-213">
    <a id="_idTextAnchor215">
    </a>
    
     Creating the target actor
    
   </h2>
   <p>
    
     In this subsection, we
    
    <a id="_idIndexMarker552">
    </a>
    
     will create a new actor that will serve as a target for the dummy gunner puppet, but with a twist – this actor will create some interference in the level and won’t be visible to the security camera.
    
    
     Achieving this kind of feature is quite easy, once you know how to register and unregister an actor from the Perception System.
    
    
     We are basically creating a scrambler device that will disturb – that is, it will be invisible to – the gunner
    
    
     
      sight sense.
     
    
   </p>
   <p>
    
     To keep things simple, I will create a Blueprint class; when working with stimuli, it is often more convenient to configure settings directly from a Blueprint rather than using a C++ class.
    
    
     By utilizing a Blueprint, we can easily adjust and fine-tune various aspects of the stimuli, making the whole process more flexible and accessible.
    
    
     This approach allows for quicker iterations and modifications, ultimately resulting in a smoother and more
    
    
     
      efficient workflow.
     
    
   </p>
   <p>
    
     To create our scrambler, open the
    
    <strong class="source-inline">
     
      Blueprints
     
    </strong>
    
     folder, create a new Blueprint class extending from
    
    <strong class="bold">
     
      Actor
     
    </strong>
    
     , and name it
    
    <strong class="source-inline">
     
      BP_Scrambler
     
    </strong>
    
     .
    
    
     Once the blueprint is opened, follow
    
    
     
      these steps:
     
    
   </p>
   <ol>
    <li>
     
      In the
     
     <strong class="bold">
      
       Components
      
     </strong>
     
      panel, add a static
     
     
      
       mesh component.
      
     
    </li>
    <li>
     
      In the
     
     <strong class="bold">
      
       Details
      
     </strong>
     
      panel, set the
     
     <strong class="bold">
      
       Static Mesh
      
     </strong>
     
      property to
     
     <strong class="source-inline">
      
       SM_RoboGun_BaseRemote
      
     </strong>
     
      and the
     
     <strong class="bold">
      
       Scale
      
     </strong>
     
      property to
     
     <strong class="source-inline">
      
       (3.0,
      
     </strong>
     
      <strong class="source-inline">
       
        3.0, 3.0)
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     
      Add an
     
     
      <strong class="bold">
       
        AIPerceptionStimuliSource
       
      </strong>
     
     
      
       component.
      
     
    </li>
    <li>
     
      In the
     
     <strong class="bold">
      
       Details
      
     </strong>
     
      panel, locate the
     
     <strong class="bold">
      
       Register as Source for Senses
      
     </strong>
     
      attribute, add a new element by clicking the
     
     <strong class="bold">
      
       +
      
     </strong>
     
      button, and set the value
     
     
      
       to
      
     
     
      <strong class="bold">
       
        AISense_Sight
       
      </strong>
     
     
      
       .
      
     
    </li>
    <li>
     
      Leave the
     
     <strong class="bold">
      
       Auto Register as Source
      
     </strong>
     
      
       attribute unchecked
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.10 – The stimuli source" src="img/Figure_10.10_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.10 – The stimuli source
    
   </p>
   <p class="list-inset">
    
     Now, open the Event
    
    <a id="_idIndexMarker553">
    </a>
    
     Graph for this Blueprint and do
    
    
     
      the following:
     
    
   </p>
   <ol>
    <li value="6">
     
      From the outgoing execution pin of the
     
     <strong class="bold">
      
       Event Begin Play
      
     </strong>
     
      node, add a
     
     
      <strong class="bold">
       
        Delay
       
      </strong>
     
     
      
       node.
      
     
    </li>
    <li>
     
      From the
     
     <strong class="bold">
      
       Completed
      
     </strong>
     
      pin of the
     
     <strong class="bold">
      
       Delay
      
     </strong>
     
      node, add a
     
     <strong class="bold">
      
       Register for Sense
      
     </strong>
     
      node; this should automatically add an
     
     <strong class="bold">
      
       AIPerception Stimuli Source
      
     </strong>
     
      reference to the
     
     <strong class="bold">
      
       Target
      
     </strong>
     
      
       incoming pin.
      
     
    </li>
    <li>
     
      From the
     
     <strong class="bold">
      
       Duration
      
     </strong>
     
      pin of the
     
     <strong class="bold">
      
       Delay
      
     </strong>
     
      node, add a
     
     <strong class="bold">
      
       Random Float in Range
      
     </strong>
     
      node, setting its
     
     <strong class="bold">
      
       Min
      
     </strong>
     
      and
     
     <strong class="bold">
      
       Max
      
     </strong>
     
      values to
     
     <strong class="source-inline">
      
       4.0
      
     </strong>
     
      and
     
     
      <strong class="source-inline">
       
        6.0
       
      </strong>
     
     
      
       , respectively.
      
     
    </li>
    <li>
     
      From the dropdown menu of the incoming
     
     <strong class="bold">
      
       Sense Class
      
     </strong>
     
      pin, select
     
     <strong class="bold">
      
       AISense_Sight
      
     </strong>
     
      .
     
     
      The final graph should look like the one depicted in
     
     
      <em class="italic">
       
        Figure 10
       
      </em>
     
     
      <em class="italic">
       
        .11
       
      </em>
     
     
      
       :
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.11 – The Event Graph" src="img/Figure_10.11_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.11 – The Event Graph
    
   </p>
   <p>
    
     This graph will simply register the sight sense for this actor after a random interval, making the actor itself visible to the Perception System in
    
    
     
      the level.
     
    
   </p>
   <p class="callout-heading">
    
     Note
    
   </p>
   <p class="callout">
    
     If you need to unregister a stimuli source, the corresponding Blueprint node is
    
    <strong class="bold">
     
      Unregister
     
    </strong>
    
     <strong class="bold">
      
       from Sense
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Let’s test this
    
    <a id="_idIndexMarker554">
    </a>
    
     functionality in a
    
    
     
      brand-new gym.
     
    
   </p>
   <h2 id="_idParaDest-214">
    <a id="_idTextAnchor216">
    </a>
    
     Testing the gym
    
   </h2>
   <p>
    
     To create the
    
    <a id="_idIndexMarker555">
    </a>
    
     testing level, follow
    
    
     
      these steps:
     
    
   </p>
   <ol>
    <li>
     
      Create a level of your choice, starting from the Level Instances and Packed Level Actors that I provided in the
     
     
      
       project template.
      
     
    </li>
    <li>
     
      Add some obstacles to make things
     
     
      
       more interesting.
      
     
    </li>
    <li>
     
      Add
     
     <strong class="bold">
      
       BP_GunnerDummyCharacter
      
     </strong>
     
      to
     
     
      
       the level.
      
     
    </li>
    <li>
     
      Add one
     
     <strong class="bold">
      
       BP_Scrambler
      
     </strong>
     
      instance so that the gunner puppet can shoot
     
     
      
       at it.
      
     
    </li>
    <li>
     
      Add one
     
     <strong class="bold">
      
       BP_SecurityCam
      
     </strong>
     
      instance to the walls so that it is in the line of sight of the scrambler.
     
     
      The final result should be similar to
     
     
      <em class="italic">
       
        Figure 10
       
      </em>
     
     
      <em class="italic">
       
        .12
       
      </em>
     
     
      
       :
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.12 – The scrambler gym" src="img/Figure_10.12_B31016.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.12 – The scrambler gym
    
   </p>
   <p>
    
     By starting the simulation, you can see that the scrambler will go unnoticed by the security camera until it reveals itself after a random interval.
    
    
     After that, the scrambler will be tagged as a viable target and the gunner will shoot at it.
    
    
     Try enabling the debugging tools to check what’s happening to the
    
    
     
      Perception System.
     
    
   </p>
   <p>
    
     In this final section, I have presented how to make any actor detectable by the Perception System.
    
    
     By following the steps and guidelines provided, you can seamlessly integrate the Perception
    
    <a id="_idIndexMarker556">
    </a>
    
     System into your project, allowing your actors to be accurately recognized and interacted with within the
    
    
     
      virtual environment.
     
    
   </p>
   <h1 id="_idParaDest-215">
    <a id="_idTextAnchor217">
    </a>
    
     Summary
    
   </h1>
   <p>
    
     In this chapter, you learned the basics of the Unreal Engine Perception System.
    
    
     Firstly, we showed the main elements that will let you add senses to your AI agents; after that, you built a pawn with a sight sense that detects moving characters around the level.
    
    
     Then, you learned how to debug the active senses at runtime.
    
    
     Finally, you added a stimuli source to an actor, in order to make it detectable by the Perception System itself.
    
    
     All of this opens up a world of possibilities for creating immersive and dynamic experiences, using the power of the Unreal Engine
    
    
     
      AI Framework.
     
    
   </p>
   <p>
    
     In the upcoming chapter, I’ll unveil a new method to gather data from the environment; brace yourself, as this cutting-edge feature is still in the experimental stage.
    
    
     But fear not, my friend, for it is knowledge well
    
    
     
      worth acquiring!
     
    
   </p>
  </div>
 </body></html>