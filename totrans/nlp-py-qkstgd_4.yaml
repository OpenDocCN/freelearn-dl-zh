- en: Text Representations - Words to Numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computers today cannot act on words or text directly. They need to be represented
    by meaningful number sequences. These long sequences of decimal numbers are called
    vectors, and this step is often referred to as the vectorization of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, where are these word vectors used:'
  prefs: []
  type: TYPE_NORMAL
- en: In text classification and summarization tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During similar word searches, such as synonyms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In machine translation (for example, when translating text from English to German)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When understanding similar texts (for example, Facebook articles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During question and answer sessions, and general tasks (for example, chatbots
    used in appointment scheduling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Very frequently, we see word vectors used in some form of categorization task.
    For instance, using a machine learning or deep learning model for sentiment analysis,
    with the following text vectorization methods:'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF in sklearn pipelines with logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GLoVe by Stanford, looked up via Gensim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastText by Facebook using pre-trained vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already seen TF-IDF examples, and will see several more throughout this
    book. This chapter will instead cover the other ways in which you can vectorize
    your text corpus or a part of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to vectorize a specific dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to make document embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorizing a specific dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section focuses almost exclusively on word vectors and how we can leverage
    the Gensim library to perform them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the questions we want to answer in this section include these:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we use original embedding, such as GLoVe?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How do we handle Out of Vocabulary words? (Hint: fastText)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we train our own word2vec vectors on our own corpus?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we train our own word2vec vectors?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we train our own fastText vectors?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we use similar words to compare both of the above?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let''s get started with some simple imports, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Please ensure that your Gensim version is at least 3.4.0\. This is a very popular
    package which is maintained and developed mostly by text processing experts over
    at RaRe Technologies. They use the same library in their own work for enterprise
    B2B consulting. Large parts of Gensim's internal implementations are written in
    Cython for speed. It natively uses multiprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the caveat is that Gensim is known to make breaking API changes, so consider
    double-checking the API when you use the code with their documents or tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you using a Windows machine, watch out for a warning similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s get started by downloading the pre-trained GloVe embedding. While
    we could do this manually, here we will download it using the following Python
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will also reuse the `get_data` API to download any arbitrary files that we
    want to use throughout this section. We have also set up `tqdm` (Arabic for progress),
    which provides us with a progress bar by wrapping our `urlretrieve` iterable in
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following text is from tqdm''s README:'
  prefs: []
  type: TYPE_NORMAL
- en: tqdm works on any platform (Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS),
    in any console or in a GUI, and is also friendly with IPython/Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: tqdm does not require any dependencies (not even curses!), just Python and an
    environment supporting carriage return \r and line feed \n control characters.
  prefs: []
  type: TYPE_NORMAL
- en: Right, let's finally download the embedding, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet will download a large file with GLoVe word representations
    of 6 billion English words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly unzip the file using the Terminal or command-line syntax in
    Jupyter notebooks. You can also do this manually or by writing code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have moved all of the `.txt` files back to the `data` directory. The
    thing to note here is in the filename, `glove.6B.50d.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: '`6B` stands for the 6 billion words or tokens. `50d` stands for 50 dimensions,
    which means that each word is represented by a sequence of 50 numbers, and in
    this case, that''s 50 float numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll now deviate a little to give you some context about word representations.
  prefs: []
  type: TYPE_NORMAL
- en: Word representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most popular names in word embedding are word2vec by Google (Mikolov) and
    GloVe by Stanford (Pennington, Socher, and Manning). fastText seems to be fairly
    popular for multilingual sub-word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: We advise that you don't use word2vec or GloVe. Instead, use fastText vectors,
    which are much better and from the same authors. word2vec was introduced by T.
    Mikolov et. al. ([https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en))
    when he was with Google, and it performs well on word similarity and analogy tasks.
  prefs: []
  type: TYPE_NORMAL
- en: GloVe was introduced by Pennington, Socher, and Manning from Stanford in 2014
    as a statistical approximation for word embedding. The word vectors are created
    by the matrix factorization of word-word co-occurrence matrices.
  prefs: []
  type: TYPE_NORMAL
- en: If picking between the lesser of two evils, we recommend using GloVe over word2vec.
    This is because GloVe outperforms word2vec in most machine learning tasks and
    NLP challenges in academia.
  prefs: []
  type: TYPE_NORMAL
- en: 'Skipping the original word2vec here, we will now look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we use original embeddings in GLoVe?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How do we handle out of vocabulary words? (Hint: fastText)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we train our own word2vec vectors on our own corpus?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we use pre-trained embeddings?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just downloaded these.
  prefs: []
  type: TYPE_NORMAL
- en: The file formats used by word2vec and GloVe are slightly different from each
    other. We'd like a consistent API to look up any word embedding, and we can do
    this by converting the embedding format. Note that there are minor differences
    in how word embedding is stored.
  prefs: []
  type: TYPE_NORMAL
- en: This format conversion can be done using Gensim's API called `glove2word2vec`.
    We will use this to convert our GloVe embedding information to the word2vec format.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s get the imports out of the way and begin by setting up filenames,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We don''t want to repeat this step if we have already done the conversion once.
    The simplest way to check this is to see if `word2vec_output_file` already exists.
    We run the following conversion only if the file does not exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet will create a new file in a standard that is compatible
    with the rest of Gensim's API stack.
  prefs: []
  type: TYPE_NORMAL
- en: KeyedVectors API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have to perform the simple task of loading vectors from a file. We do
    this using the `KeyedVectors` API in Gensim. The word we want to look up is the
    key, and the numerical representation of that word is the corresponding value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first import the API and set up the target filename as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will load the entire text file into our memory, thus including the read
    from disk time. In most running processes, this is a one-off I/O step and is not
    repeated for every new data pass. This becomes our Gensim model, detailed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: A faster SSD should definitely speed this up by an order of magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do some word vector arithmetic to compose and show that this representation
    captures semantic meaning as well. For instance, let''s repeat the following famous
    word vector example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now perform the mentioned arithmetic operations on the word vectors,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We did this using the `most_similar` API. Behind the scenes, Gensim has done
    the following for us:'
  prefs: []
  type: TYPE_NORMAL
- en: Looked up the vectors for `woman`, `king`, and `man`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Added `king` and `woman`, and subtracted the vector from `man` to find a resultant
    vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the 6 billion tokens in this model, ranked all words by distance and found
    the closest words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Found the closest word
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We also added `topn=1` to tell the API that we are only interested in the closest
    match. The expected output is now just one word, `''queen''`, as shown in the
    following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Not only did we get the correct word, but also an accompanying decimal number!
    We will ignore that for now, but note that the number represents a notion of how
    close or similar the word is to the resultant vector that the API computed for
    us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try a few more examples, say social networks, as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we are looking for a social network that is more casual than
    LinkedIn but more focused on learning than Facebook by adding Quora. As you can
    see in the following output, it looks like Twitter fits the bill perfectly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We could have equally expected Reddit to fit this.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, can we use this approach to simply explore similar words in a larger corpus?
    It seems so. Let''s now look up words most similar to `india`, as shown in the
    following snippet. Notice that we are writing India in lowercase; this is because
    the model contains only lowercase words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It is worth mentioning that these results might be a little biased because
    GloVe was primarily trained on a large news corpus called Gigaword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding result does make sense, keeping in mind that, in the foreign press,
    India is often mentioned because of its troubled relationships with its geographical
    neighbours, including Pakistan and Kashmir. Bangladesh, Nepal, and Sri Lanka are
    neighbouring countries, while Maharashtra is the home of India's business capital,
    Mumbai.
  prefs: []
  type: TYPE_NORMAL
- en: What is missing in both word2vec and GloVe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neither GloVe nor word2vec can handle words they didn't see during training.
    These words are called **Out of Vocabulary** (OOV), in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evidence of this can be seen if you try to look up nouns that are not frequently
    used, for example an uncommon name. As you can see in the following snippet, the
    model throws a `not in vocabulary` error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This result is also accompanied by an API warning that sometimes states the
    API will change in gensim v4.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: How do we handle Out Of Vocabulary words?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors of word2vec (Mikolov et al.) extended it to create fastText at Facebook.
    It works on character n-grams instead of entire words. Character n-grams are effective
    in languages with specific morphological properties.
  prefs: []
  type: TYPE_NORMAL
- en: We can create our own fastText embeddings, which can handle OOV tokens as well.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to download the subtitles of several TED talks from a public
    dataset. We will train our fastText embeddings on these as well as the word2vec
    embeddings for comparison, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Python empowers us to access files inside a `.zip` file, which is easy to do
    with the `zipfile` package. Notice it is the `zipfile.zipFile` syntax that enables
    this.
  prefs: []
  type: TYPE_NORMAL
- en: We additionally use the `lxml` package to `parse` the XML file inside the ZIP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we manually opened the file to find the relevant `content` path and look
    up `text()` from it. In this case, we are interested only in the subtitles and
    not any accompanying metadata, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now preview the first 500 characters of the following `input_text`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Since we are using subtitles from TED talks, there are some fillers that are
    not useful. These are often words describing sounds in parentheses and the speaker's
    name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s remove these fillers using some regex, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we created `sentence_strings_ted` using the `.split(''\n'')` syntax
    on our entire corpus. Replace this with a better sentence tokenizer, such as that
    from spaCy or NLTK, as a reader exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Notice that each `sentences_ted` is now a list of a lists. Each element of the
    first list is a sentence, and each sentence is a list of tokens (words).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the expected structure for training text embeddings using Gensim. We
    will write the following code to disk for easy retrieval later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: I personally prefer JSON serialization over Pickle because it's slightly faster,
    more inter-operable among languages, and, most importantly, human readable.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now train both fastText and word2vec embedding over this small corpus.
    Although small, the corpus we are using is representative of the data sizes usually
    seen in practice. Large annotated text corpora are extremely rare in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Training fastText embedddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Setting up imports is actually quite simple in the new Gensim API; just use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to feed the text and make our text embedding model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You will probably noticed the parameters we pass to make our model. The following
    list explains these parameters, as explained in the Gensim documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_count (int, optional)`: The model ignores all words with total frequency
    lower than this'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size (int, optional)`: This represents the dimensionality of word vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window (int, optional)`: This represents the maximum distance between the
    current and predicted word within a sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workers (int, optional)`: Use these many worker threads to train the model
    (this enables faster training with multicore machines; `workers=-1` means using
    one worker for each core available in your machine)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sg ({1, 0}, optional)`: This is a training algorithm, `skip-gram if sg=1`
    or CBOW'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding parameters are actually part of a larger list of levers that can
    move around to improve the quality of your text embedding. We encourage you to
    play around with the numbers in addition to exploring the other parameters that
    the Gensim API exposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a quick peek at the words most similar to India in this corpus,
    as ranked by fastText embedding-based similarity, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, we notice that fastText has leveraged the sub-word structure, such as
    `ind`, `ian`, and `dian`, to rank the words. We get both `indians` and `indian`
    in the top 3, which is quite good. This is one of the reasons fastText is effective—even
    for small training text tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now repeat the same process using word2vec and look at the words most
    similar to `india` there.
  prefs: []
  type: TYPE_NORMAL
- en: Training word2vec embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Importing the model is simple, simply use the following command. By now, you
    should have an intuitive feel of how the Gensim model''s API is structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using an identical configuration for the word2vec model as we did
    for fastText. This helps to reduce bias in the comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are encouraged to compare the best fastText model to the best word2vec
    model with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Right, let''s now look at the words most similar to `india`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The words most similar to `india` have no tangible relation to the original
    word. For this particular dataset, and word2vec's training configuration, the
    model has not captured any semantic or syntactic information at all. This is not
    unusual since word2vec is meant to work on large text corpora.
  prefs: []
  type: TYPE_NORMAL
- en: fastText versus word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to the following preliminary comparison by Gensim:'
  prefs: []
  type: TYPE_NORMAL
- en: <q>fastText embeddings are significantly better than word2vec at encoding syntactic
    information. This is expected, since most syntactic analogies are morphology based,
    and the char n-gram approach of fastText takes such information into account.
    The original word2vec model seems to perform better on semantic tasks, since words
    in semantic analogies are unrelated to their char n-grams, and the added information
    from irrelevant char n-grams worsens the embeddings.</q>
  prefs: []
  type: TYPE_NORMAL
- en: 'The source for this is: *word2vec fasttext comparison notebook* ([https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb](https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)).'
  prefs: []
  type: TYPE_NORMAL
- en: In general, we prefer fastText because of its innate ability to handle words
    that it has not seen in training. It is definitely better than word2vec when working
    with small data (as we've shown), and is at least as good as word2vec on larger
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: fastText is also useful in cases where we are processing text riddled with spelling
    mistakes. For example, it can leverage sub-word similarity to bring `indian` and
    `indain` close in the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: In most downstream tasks, such as sentiment analysis or text classification,
    we continue to recommend GloVe over word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is our recommended rule of thumb for text embedding applications:
    fastText > GloVe > word2vec.'
  prefs: []
  type: TYPE_NORMAL
- en: Document embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Document embedding is often considered an underrated way of doing things. The
    key idea in document embedding is to compress an entire document, for example
    a patent or customer review, into one single vector. This vector in turn can be
    used for a lot of downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Empirical results show that document vectors outperform bag-of-words models
    as well as other techniques for text representation.
  prefs: []
  type: TYPE_NORMAL
- en: Among the most useful downstream tasks is the ability to cluster text. Text
    clustering has several uses, ranging from data exploration to online classification
    of incoming text in a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we are interested in document modeling using doc2vec on a small
    dataset. Unlike sequence models such as RNN, where a word sequence is captured
    in generated sentence vectors, doc2vec sentence vectors are word order independent.
    This word order independence means that we can process a large number of examples
    quickly, but it does mean capturing less of a sentence's inherent meaning.
  prefs: []
  type: TYPE_NORMAL
- en: This section is loosely based on the doc2Vec API Tutorial from the Gensim repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first get the imports out of the way with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s pull out the talks from the `doc` variable we used earlier, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the Doc2Vec model, each text sample needs a label or unique identifier.
    To do this, write a small function like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few things happening inside the preceding function; they are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overloaded if condition**: This reads a test corpora and sets `tokens_only`
    to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target Label:** This assigns an arbitrary index variable, `i`, as the target
    label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**gensim.utils.simple_preprocess**`: This converts a document into a list
    of lowercase tokens, ignoring tokens that are too short or too long, which then
    yields instances of `TaggedDocument`. Since we are yielding instead of returning,
    this entire function is acting as a generator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth mentioning how this changes the function behavior. With a `return`
    in use, when a function is called it would have returned a specific object, such
    as `TaggedDocument` or `None` if the return is not specified. A `generator` function,
    on the other hand, only returns a `generator` object.
  prefs: []
  type: TYPE_NORMAL
- en: So, what do you expect the following code line to return?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'If you guessed correctly, you''ll know we expect it to return a `generator`
    object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding object means that we can read the text corpus element by element
    as and when it's needed. This is exceptionally useful if a training corpus is
    larger than your memory size.
  prefs: []
  type: TYPE_NORMAL
- en: Understand how Python iterators and generators work. They make your code memory
    efficient and easy to read.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this particular case, we have a rather small training corpus as an example,
    so let''s read this entire corpus into working memory as a list of `TaggedDocument`
    objects, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `list()` statement runs over the entire corpora until the function stops
    yielding. Our variable `ted_talk_docs` should look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s quickly take a look at how many cores this machine has. We will use
    the following code to initialize the doc2vec model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Let's now go and initialize our doc2vec model from Gensim.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the doc2vec API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s quickly understand the flags we have used in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dm ({1,0}, optional)`: This defines the training algorithm; if `dm=1`, *distributed
    memory* (PV-DM) is used; otherwise, a distributed bag of words (PV-DBOW) is employed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size (int, optional)`: This is the dimensionality of feature vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window (int, optional)`: This represents the maximum distance between the
    current and predicted word within a sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative (int, optional)`: If > `0`, negative sampling will be used (the int
    for negative values specifies how many *noise words* should be drawn, which is
    usually between 5-20); if set to `0`, no negative sampling is used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hs ({1,0}, optional)`: If `1`, hierarchical softmax will be used for model
    training, and if set to 0 where the negative is non-zero, negative sampling will
    be used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iter (int, optional)`: This represents the number of iterations (epochs) over
    the corpus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list has been taken directly from the Gensim documentation. With
    that in mind, we'll now move on and explain some of the new terms introduced here,
    including negative sampling and hierarchical softmax.
  prefs: []
  type: TYPE_NORMAL
- en: Negative sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Negative sampling started out as a hack to speed up training and is now a well-accepted
    practice. The click point here is that in addition to training your model on what
    might be the correct answer, why not show it a few examples of wrong answers?
  prefs: []
  type: TYPE_NORMAL
- en: In particular, using negative sampling speeds up training by reducing the number
    of model updates required. Instead of updating the model for every single wrong
    word, we pick a small number, usually between 5 and 25, and train the model on
    them. So, we have reduced the number of updates from a few million, which is required
    for training on a large corpus, to a much smaller number. This is a classic software
    programming hack that works in academia too.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The denominator term in our usual softmax is calculated using the sum operator
    over a large number of words. This normalization is a very expensive operation
    to do at each update during training.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we can break this down into a specific sequence of calculations, which
    saves us from having to calculate expensive normalization over all words. This
    means that for each word, we use an approximation of sorts.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this approximation has worked so well that some systems use this
    in both training and inference time. For training, it can give a speed of up to
    50x (as per Sebastian Ruder, an NLP research blogger). In my own experiments,
    I have seen speed gains of around 15-25x.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The API to train a doc2vec model is slightly different. We use the `build_vocab`
    API first to build the vocabulary from a sequence of sentences, as shown in the
    previous snippet. We also pass our memory variable `ted_talk_docs` here, but we
    could have passed our once-only generator stream from the `read_corpora` function
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now set up some of the following sample sentences to find out whether
    our model learns something or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Gensim has an interesting API that allows us to find a similarity value between
    two unseen documents using the model we just updated with our vocabulary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output doesn't quite make sense, does it? The sentences we wrote
    should have some reasonable degree of similarity that is definitely not negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'A-ha! We forgot to train the model on our corpora. Let''s do that now with
    the following code and then repeat the previous comparisons to see how they have
    changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: On a machine with BLAS set up, this step should take less than a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can actually pull out raw inference vectors for any particular sentence
    based on the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `infer_vector` API expects a list of tokens as an input. This should
    explain why we could have used `read_corpora` with `tokens_only =True` here as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our model is trained, let''s compare the following sentences again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The new preceding output makes sense. The first and third sentences are definitely
    more similar than the first and second. In the spirit of exploring, let''s now
    see how similar the second and third sentences are, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Ah, this is better. Our result is now consistent with our expectations. The
    similarity value is more than the first and second sentences, but less than that
    of the first and third, which were also almost identical in intent.
  prefs: []
  type: TYPE_NORMAL
- en: As an anecdotal observation or heuristic, truly similar sentences have a value
    greater than 0.8 on the similarity scale.
  prefs: []
  type: TYPE_NORMAL
- en: We have mentioned how document or text vectors in general are a good way of
    exploring a data corpus. Next, we will do that to explore our corpus in a very
    shallow manner before leaving you with some ideas on how to continue the exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration and model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One simple technique for assessing any vectorization method is to simply use
    the training corpus as the test corpus. Of course, we expect that we will overfit
    our model to the training set, but that's fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the training corpus as a test corpus by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning a new result or *inference* vectors for each document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the vector to all examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranking the document, sentence, and paragraph vectors according to the similarity
    score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s do this in code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now figured out where each document placed itself in the rank. So,
    if the highest rank is the document itself, that''s good enough. As we said, we
    might overfit a little on the training corpus, but it''s a good sanity test nonetheless.
    We can find this using the frequency count via `Counter` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `Counter` object tells us how many documents found themselves at what ranks.
    So, 2079 documents found themselves first (index 0), but two documents each found
    themselves second (index 1) and sixth (index 5) ranks. There is one document that
    ranked fifth (index 4) and third (index 2) respectively. All in all, this is a
    very good training performance, because 2079 out of 2084 documents ranked themselves
    first.
  prefs: []
  type: TYPE_NORMAL
- en: This helps us understand that the vectors did represent information in the document
    in a meaningful manner. If they did not, we would see a lot more rank dispersal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now quickly take a single document and find the most similar document
    to it, the least similar document, and a document that is somewhat in between
    in similarity. Do this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we are choosing to preview a slice of the entire document for exploration.
    You are free to either do this or use a small text summarization tool to create
    your preview on the fly instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was more than an introduction to the Gensim API. We now know how
    to load pre-trained GloVe vectors, and you can use these vector representations
    instead of TD-IDF in any machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at why fastText vectors are often better than word2vec vectors on
    a small training corpus, and learned that you can use them with any ML models.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to build doc2vec models. You can now extend this doc2vec approach
    to build sent2vec or paragraph2vec style models as well. Ideally, paragraph2vec
    will change, simply because each document will be a paragraph instead.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we now know how we can quickly perform sanity checks on our doc2vec
    vectors without using an annotated test corpora. We did this by checking the rank
    dispersal metric.
  prefs: []
  type: TYPE_NORMAL
