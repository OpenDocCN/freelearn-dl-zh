- en: Text Representations - Words to Numbers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computers today cannot act on words or text directly. They need to be represented
    by meaningful number sequences. These long sequences of decimal numbers are called
    vectors, and this step is often referred to as the vectorization of text.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'So, where are these word vectors used:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In text classification and summarization tasks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During similar word searches, such as synonyms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In machine translation (for example, when translating text from English to German)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When understanding similar texts (for example, Facebook articles)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During question and answer sessions, and general tasks (for example, chatbots
    used in appointment scheduling)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Very frequently, we see word vectors used in some form of categorization task.
    For instance, using a machine learning or deep learning model for sentiment analysis,
    with the following text vectorization methods:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF in sklearn pipelines with logistic regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GLoVe by Stanford, looked up via Gensim
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastText by Facebook using pre-trained vectors
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already seen TF-IDF examples, and will see several more throughout this
    book. This chapter will instead cover the other ways in which you can vectorize
    your text corpus or a part of it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: How to vectorize a specific dataset
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to make document embedding
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorizing a specific dataset
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section focuses almost exclusively on word vectors and how we can leverage
    the Gensim library to perform them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the questions we want to answer in this section include these:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: How do we use original embedding, such as GLoVe?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How do we handle Out of Vocabulary words? (Hint: fastText)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we train our own word2vec vectors on our own corpus?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we train our own word2vec vectors?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we train our own fastText vectors?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we use similar words to compare both of the above?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let''s get started with some simple imports, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Please ensure that your Gensim version is at least 3.4.0\. This is a very popular
    package which is maintained and developed mostly by text processing experts over
    at RaRe Technologies. They use the same library in their own work for enterprise
    B2B consulting. Large parts of Gensim's internal implementations are written in
    Cython for speed. It natively uses multiprocessing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Here, the caveat is that Gensim is known to make breaking API changes, so consider
    double-checking the API when you use the code with their documents or tutorials.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'If you using a Windows machine, watch out for a warning similar to the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s get started by downloading the pre-trained GloVe embedding. While
    we could do this manually, here we will download it using the following Python
    code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will also reuse the `get_data` API to download any arbitrary files that we
    want to use throughout this section. We have also set up `tqdm` (Arabic for progress),
    which provides us with a progress bar by wrapping our `urlretrieve` iterable in
    it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The following text is from tqdm''s README:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: tqdm works on any platform (Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS),
    in any console or in a GUI, and is also friendly with IPython/Jupyter notebooks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: tqdm does not require any dependencies (not even curses!), just Python and an
    environment supporting carriage return \r and line feed \n control characters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Right, let's finally download the embedding, shall we?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding snippet will download a large file with GLoVe word representations
    of 6 billion English words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly unzip the file using the Terminal or command-line syntax in
    Jupyter notebooks. You can also do this manually or by writing code, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we have moved all of the `.txt` files back to the `data` directory. The
    thing to note here is in the filename, `glove.6B.50d.txt`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '`6B` stands for the 6 billion words or tokens. `50d` stands for 50 dimensions,
    which means that each word is represented by a sequence of 50 numbers, and in
    this case, that''s 50 float numbers.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We'll now deviate a little to give you some context about word representations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Word representations
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most popular names in word embedding are word2vec by Google (Mikolov) and
    GloVe by Stanford (Pennington, Socher, and Manning). fastText seems to be fairly
    popular for multilingual sub-word embeddings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: We advise that you don't use word2vec or GloVe. Instead, use fastText vectors,
    which are much better and from the same authors. word2vec was introduced by T.
    Mikolov et. al. ([https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en))
    when he was with Google, and it performs well on word similarity and analogy tasks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: GloVe was introduced by Pennington, Socher, and Manning from Stanford in 2014
    as a statistical approximation for word embedding. The word vectors are created
    by the matrix factorization of word-word co-occurrence matrices.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: If picking between the lesser of two evils, we recommend using GloVe over word2vec.
    This is because GloVe outperforms word2vec in most machine learning tasks and
    NLP challenges in academia.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Skipping the original word2vec here, we will now look at the following topics:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: How do we use original embeddings in GLoVe?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How do we handle out of vocabulary words? (Hint: fastText)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we train our own word2vec vectors on our own corpus?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we use pre-trained embeddings?
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just downloaded these.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The file formats used by word2vec and GloVe are slightly different from each
    other. We'd like a consistent API to look up any word embedding, and we can do
    this by converting the embedding format. Note that there are minor differences
    in how word embedding is stored.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: This format conversion can be done using Gensim's API called `glove2word2vec`.
    We will use this to convert our GloVe embedding information to the word2vec format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s get the imports out of the way and begin by setting up filenames,
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We don''t want to repeat this step if we have already done the conversion once.
    The simplest way to check this is to see if `word2vec_output_file` already exists.
    We run the following conversion only if the file does not exist:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding snippet will create a new file in a standard that is compatible
    with the rest of Gensim's API stack.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: KeyedVectors API
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have to perform the simple task of loading vectors from a file. We do
    this using the `KeyedVectors` API in Gensim. The word we want to look up is the
    key, and the numerical representation of that word is the corresponding value.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first import the API and set up the target filename as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will load the entire text file into our memory, thus including the read
    from disk time. In most running processes, this is a one-off I/O step and is not
    repeated for every new data pass. This becomes our Gensim model, detailed as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A faster SSD should definitely speed this up by an order of magnitude.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do some word vector arithmetic to compose and show that this representation
    captures semantic meaning as well. For instance, let''s repeat the following famous
    word vector example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s now perform the mentioned arithmetic operations on the word vectors,
    as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We did this using the `most_similar` API. Behind the scenes, Gensim has done
    the following for us:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Looked up the vectors for `woman`, `king`, and `man`
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Added `king` and `woman`, and subtracted the vector from `man` to find a resultant
    vector
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the 6 billion tokens in this model, ranked all words by distance and found
    the closest words
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Found the closest word
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We also added `topn=1` to tell the API that we are only interested in the closest
    match. The expected output is now just one word, `''queen''`, as shown in the
    following snippet:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Not only did we get the correct word, but also an accompanying decimal number!
    We will ignore that for now, but note that the number represents a notion of how
    close or similar the word is to the resultant vector that the API computed for
    us.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try a few more examples, say social networks, as shown in the following
    snippet:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this example, we are looking for a social network that is more casual than
    LinkedIn but more focused on learning than Facebook by adding Quora. As you can
    see in the following output, it looks like Twitter fits the bill perfectly:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We could have equally expected Reddit to fit this.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'So, can we use this approach to simply explore similar words in a larger corpus?
    It seems so. Let''s now look up words most similar to `india`, as shown in the
    following snippet. Notice that we are writing India in lowercase; this is because
    the model contains only lowercase words:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It is worth mentioning that these results might be a little biased because
    GloVe was primarily trained on a large news corpus called Gigaword:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding result does make sense, keeping in mind that, in the foreign press,
    India is often mentioned because of its troubled relationships with its geographical
    neighbours, including Pakistan and Kashmir. Bangladesh, Nepal, and Sri Lanka are
    neighbouring countries, while Maharashtra is the home of India's business capital,
    Mumbai.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: What is missing in both word2vec and GloVe?
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neither GloVe nor word2vec can handle words they didn't see during training.
    These words are called **Out of Vocabulary** (OOV), in the literature.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Evidence of this can be seen if you try to look up nouns that are not frequently
    used, for example an uncommon name. As you can see in the following snippet, the
    model throws a `not in vocabulary` error:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This result is also accompanied by an API warning that sometimes states the
    API will change in gensim v4.0.0.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: How do we handle Out Of Vocabulary words?
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors of word2vec (Mikolov et al.) extended it to create fastText at Facebook.
    It works on character n-grams instead of entire words. Character n-grams are effective
    in languages with specific morphological properties.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: We can create our own fastText embeddings, which can handle OOV tokens as well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Getting the dataset
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to download the subtitles of several TED talks from a public
    dataset. We will train our fastText embeddings on these as well as the word2vec
    embeddings for comparison, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Python empowers us to access files inside a `.zip` file, which is easy to do
    with the `zipfile` package. Notice it is the `zipfile.zipFile` syntax that enables
    this.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: We additionally use the `lxml` package to `parse` the XML file inside the ZIP.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we manually opened the file to find the relevant `content` path and look
    up `text()` from it. In this case, we are interested only in the subtitles and
    not any accompanying metadata, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s now preview the first 500 characters of the following `input_text`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Since we are using subtitles from TED talks, there are some fillers that are
    not useful. These are often words describing sounds in parentheses and the speaker's
    name.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s remove these fillers using some regex, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Notice that we created `sentence_strings_ted` using the `.split(''\n'')` syntax
    on our entire corpus. Replace this with a better sentence tokenizer, such as that
    from spaCy or NLTK, as a reader exercise:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Notice that each `sentences_ted` is now a list of a lists. Each element of the
    first list is a sentence, and each sentence is a list of tokens (words).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the expected structure for training text embeddings using Gensim. We
    will write the following code to disk for easy retrieval later:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: I personally prefer JSON serialization over Pickle because it's slightly faster,
    more inter-operable among languages, and, most importantly, human readable.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Let's now train both fastText and word2vec embedding over this small corpus.
    Although small, the corpus we are using is representative of the data sizes usually
    seen in practice. Large annotated text corpora are extremely rare in the industry.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Training fastText embedddings
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Setting up imports is actually quite simple in the new Gensim API; just use
    the following code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next step is to feed the text and make our text embedding model, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You will probably noticed the parameters we pass to make our model. The following
    list explains these parameters, as explained in the Gensim documentation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '`min_count (int, optional)`: The model ignores all words with total frequency
    lower than this'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size (int, optional)`: This represents the dimensionality of word vectors'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window (int, optional)`: This represents the maximum distance between the
    current and predicted word within a sentence'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workers (int, optional)`: Use these many worker threads to train the model
    (this enables faster training with multicore machines; `workers=-1` means using
    one worker for each core available in your machine)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sg ({1, 0}, optional)`: This is a training algorithm, `skip-gram if sg=1`
    or CBOW'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding parameters are actually part of a larger list of levers that can
    move around to improve the quality of your text embedding. We encourage you to
    play around with the numbers in addition to exploring the other parameters that
    the Gensim API exposes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a quick peek at the words most similar to India in this corpus,
    as ranked by fastText embedding-based similarity, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, we notice that fastText has leveraged the sub-word structure, such as
    `ind`, `ian`, and `dian`, to rank the words. We get both `indians` and `indian`
    in the top 3, which is quite good. This is one of the reasons fastText is effective—even
    for small training text tasks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Let's now repeat the same process using word2vec and look at the words most
    similar to `india` there.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Training word2vec embeddings
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Importing the model is simple, simply use the following command. By now, you
    should have an intuitive feel of how the Gensim model''s API is structured:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we are using an identical configuration for the word2vec model as we did
    for fastText. This helps to reduce bias in the comparison.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'You are encouraged to compare the best fastText model to the best word2vec
    model with the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Right, let''s now look at the words most similar to `india`, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The words most similar to `india` have no tangible relation to the original
    word. For this particular dataset, and word2vec's training configuration, the
    model has not captured any semantic or syntactic information at all. This is not
    unusual since word2vec is meant to work on large text corpora.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: fastText versus word2vec
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to the following preliminary comparison by Gensim:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: <q>fastText embeddings are significantly better than word2vec at encoding syntactic
    information. This is expected, since most syntactic analogies are morphology based,
    and the char n-gram approach of fastText takes such information into account.
    The original word2vec model seems to perform better on semantic tasks, since words
    in semantic analogies are unrelated to their char n-grams, and the added information
    from irrelevant char n-grams worsens the embeddings.</q>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'The source for this is: *word2vec fasttext comparison notebook* ([https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb](https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: In general, we prefer fastText because of its innate ability to handle words
    that it has not seen in training. It is definitely better than word2vec when working
    with small data (as we've shown), and is at least as good as word2vec on larger
    datasets.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: fastText is also useful in cases where we are processing text riddled with spelling
    mistakes. For example, it can leverage sub-word similarity to bring `indian` and
    `indain` close in the embedding space.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In most downstream tasks, such as sentiment analysis or text classification,
    we continue to recommend GloVe over word2vec.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is our recommended rule of thumb for text embedding applications:
    fastText > GloVe > word2vec.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Document embedding
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Document embedding is often considered an underrated way of doing things. The
    key idea in document embedding is to compress an entire document, for example
    a patent or customer review, into one single vector. This vector in turn can be
    used for a lot of downstream tasks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Empirical results show that document vectors outperform bag-of-words models
    as well as other techniques for text representation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Among the most useful downstream tasks is the ability to cluster text. Text
    clustering has several uses, ranging from data exploration to online classification
    of incoming text in a pipeline.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we are interested in document modeling using doc2vec on a small
    dataset. Unlike sequence models such as RNN, where a word sequence is captured
    in generated sentence vectors, doc2vec sentence vectors are word order independent.
    This word order independence means that we can process a large number of examples
    quickly, but it does mean capturing less of a sentence's inherent meaning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: This section is loosely based on the doc2Vec API Tutorial from the Gensim repository.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first get the imports out of the way with the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let''s pull out the talks from the `doc` variable we used earlier, as
    follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To train the Doc2Vec model, each text sample needs a label or unique identifier.
    To do this, write a small function like the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are a few things happening inside the preceding function; they are as
    follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '**Overloaded if condition**: This reads a test corpora and sets `tokens_only`
    to `True`.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target Label:** This assigns an arbitrary index variable, `i`, as the target
    label.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**gensim.utils.simple_preprocess**`: This converts a document into a list
    of lowercase tokens, ignoring tokens that are too short or too long, which then
    yields instances of `TaggedDocument`. Since we are yielding instead of returning,
    this entire function is acting as a generator.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth mentioning how this changes the function behavior. With a `return`
    in use, when a function is called it would have returned a specific object, such
    as `TaggedDocument` or `None` if the return is not specified. A `generator` function,
    on the other hand, only returns a `generator` object.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: So, what do you expect the following code line to return?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If you guessed correctly, you''ll know we expect it to return a `generator`
    object, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding object means that we can read the text corpus element by element
    as and when it's needed. This is exceptionally useful if a training corpus is
    larger than your memory size.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Understand how Python iterators and generators work. They make your code memory
    efficient and easy to read.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'In this particular case, we have a rather small training corpus as an example,
    so let''s read this entire corpus into working memory as a list of `TaggedDocument`
    objects, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The `list()` statement runs over the entire corpora until the function stops
    yielding. Our variable `ted_talk_docs` should look something like the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s quickly take a look at how many cores this machine has. We will use
    the following code to initialize the doc2vec model:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Let's now go and initialize our doc2vec model from Gensim.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the doc2vec API
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s quickly understand the flags we have used in the preceding code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '`dm ({1,0}, optional)`: This defines the training algorithm; if `dm=1`, *distributed
    memory* (PV-DM) is used; otherwise, a distributed bag of words (PV-DBOW) is employed'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size (int, optional)`: This is the dimensionality of feature vectors'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window (int, optional)`: This represents the maximum distance between the
    current and predicted word within a sentence'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative (int, optional)`: If > `0`, negative sampling will be used (the int
    for negative values specifies how many *noise words* should be drawn, which is
    usually between 5-20); if set to `0`, no negative sampling is used'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hs ({1,0}, optional)`: If `1`, hierarchical softmax will be used for model
    training, and if set to 0 where the negative is non-zero, negative sampling will
    be used'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iter (int, optional)`: This represents the number of iterations (epochs) over
    the corpus'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list has been taken directly from the Gensim documentation. With
    that in mind, we'll now move on and explain some of the new terms introduced here,
    including negative sampling and hierarchical softmax.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Negative sampling
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Negative sampling started out as a hack to speed up training and is now a well-accepted
    practice. The click point here is that in addition to training your model on what
    might be the correct answer, why not show it a few examples of wrong answers?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: In particular, using negative sampling speeds up training by reducing the number
    of model updates required. Instead of updating the model for every single wrong
    word, we pick a small number, usually between 5 and 25, and train the model on
    them. So, we have reduced the number of updates from a few million, which is required
    for training on a large corpus, to a much smaller number. This is a classic software
    programming hack that works in academia too.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical softmax
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The denominator term in our usual softmax is calculated using the sum operator
    over a large number of words. This normalization is a very expensive operation
    to do at each update during training.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we can break this down into a specific sequence of calculations, which
    saves us from having to calculate expensive normalization over all words. This
    means that for each word, we use an approximation of sorts.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this approximation has worked so well that some systems use this
    in both training and inference time. For training, it can give a speed of up to
    50x (as per Sebastian Ruder, an NLP research blogger). In my own experiments,
    I have seen speed gains of around 15-25x.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The API to train a doc2vec model is slightly different. We use the `build_vocab`
    API first to build the vocabulary from a sequence of sentences, as shown in the
    previous snippet. We also pass our memory variable `ted_talk_docs` here, but we
    could have passed our once-only generator stream from the `read_corpora` function
    as well.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now set up some of the following sample sentences to find out whether
    our model learns something or not:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Gensim has an interesting API that allows us to find a similarity value between
    two unseen documents using the model we just updated with our vocabulary, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The preceding output doesn't quite make sense, does it? The sentences we wrote
    should have some reasonable degree of similarity that is definitely not negative.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'A-ha! We forgot to train the model on our corpora. Let''s do that now with
    the following code and then repeat the previous comparisons to see how they have
    changed:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: On a machine with BLAS set up, this step should take less than a few seconds.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'We can actually pull out raw inference vectors for any particular sentence
    based on the following model:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, the `infer_vector` API expects a list of tokens as an input. This should
    explain why we could have used `read_corpora` with `tokens_only =True` here as
    well.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our model is trained, let''s compare the following sentences again:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The new preceding output makes sense. The first and third sentences are definitely
    more similar than the first and second. In the spirit of exploring, let''s now
    see how similar the second and third sentences are, as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Ah, this is better. Our result is now consistent with our expectations. The
    similarity value is more than the first and second sentences, but less than that
    of the first and third, which were also almost identical in intent.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: As an anecdotal observation or heuristic, truly similar sentences have a value
    greater than 0.8 on the similarity scale.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: We have mentioned how document or text vectors in general are a good way of
    exploring a data corpus. Next, we will do that to explore our corpus in a very
    shallow manner before leaving you with some ideas on how to continue the exploration.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration and model evaluation
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One simple technique for assessing any vectorization method is to simply use
    the training corpus as the test corpus. Of course, we expect that we will overfit
    our model to the training set, but that's fine.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the training corpus as a test corpus by doing the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Learning a new result or *inference* vectors for each document
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the vector to all examples
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranking the document, sentence, and paragraph vectors according to the similarity
    score
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s do this in code, as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We have now figured out where each document placed itself in the rank. So,
    if the highest rank is the document itself, that''s good enough. As we said, we
    might overfit a little on the training corpus, but it''s a good sanity test nonetheless.
    We can find this using the frequency count via `Counter` as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `Counter` object tells us how many documents found themselves at what ranks.
    So, 2079 documents found themselves first (index 0), but two documents each found
    themselves second (index 1) and sixth (index 5) ranks. There is one document that
    ranked fifth (index 4) and third (index 2) respectively. All in all, this is a
    very good training performance, because 2079 out of 2084 documents ranked themselves
    first.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: This helps us understand that the vectors did represent information in the document
    in a meaningful manner. If they did not, we would see a lot more rank dispersal.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now quickly take a single document and find the most similar document
    to it, the least similar document, and a document that is somewhat in between
    in similarity. Do this with the following code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Notice how we are choosing to preview a slice of the entire document for exploration.
    You are free to either do this or use a small text summarization tool to create
    your preview on the fly instead.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was more than an introduction to the Gensim API. We now know how
    to load pre-trained GloVe vectors, and you can use these vector representations
    instead of TD-IDF in any machine learning model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: We looked at why fastText vectors are often better than word2vec vectors on
    a small training corpus, and learned that you can use them with any ML models.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to build doc2vec models. You can now extend this doc2vec approach
    to build sent2vec or paragraph2vec style models as well. Ideally, paragraph2vec
    will change, simply because each document will be a paragraph instead.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何构建doc2vec模型。现在，你可以将这种doc2vec方法扩展到构建sent2vec或paragraph2vec风格的模型。理想情况下，paragraph2vec将会改变，仅仅是因为每个文档将变成一个段落。
- en: In addition, we now know how we can quickly perform sanity checks on our doc2vec
    vectors without using an annotated test corpora. We did this by checking the rank
    dispersal metric.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们现在知道如何在不使用标注测试语料库的情况下快速对doc2vec向量进行合理性检查。我们是通过检查排名分散度指标来做到这一点的。
