- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embedding LLMs within Your Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter kickstarts the hands-on portions of this book, focusing on how
    we can **leverage large language models** (**LLMs**) to build powerful AI applications.
    In fact, LLMs have introduced a whole new paradigm in software development, paving
    the way for new families of applications that have the peculiarity of making the
    communication between the user and the machine smooth and conversational. Plus,
    those models enhanced existing applications, such as chatbots and recommendation
    systems, with their unique reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Developing LLM-powered applications is becoming a key factor for enterprises
    to keep themselves competitive in the market, and this leads to the spreading
    of new libraries and frameworks that make it easier to embed LLMs within applications.
    Some examples are Semantic Kernel, Haystack, LlamaIndex, and LangChain. In this
    chapter, we are going to cover LangChain and use its modules to build hands-on
    examples. By the end of this chapter, you will have the technical foundations
    to start developing your LLM-powered applications using LangChain and open-source
    Hugging Face models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief note about LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with LLMs via the Hugging Face Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the hands-on sections of this chapter, the following prerequisites
    are needed:'
  prefs: []
  type: TYPE_NORMAL
- en: A Hugging Face account and user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI account and user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7.1 or later version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python packages: Make sure to have the following Python packages installed:
    `langchain`, `python-dotenv`, `huggingface_hub`, `google-search-results`, `faiss`,
    and `tiktoken`. Those can be easily installed via `pip install` in your terminal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find all the code and examples used in this chapter in the book’s GitHub
    repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_05.xhtml)
  prefs: []
  type: TYPE_NORMAL
- en: A brief note about LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as generative AI has evolved so rapidly over the last year, so has LangChain.
    In the months between the writing of this book and its publication, the AI orchestrator
    has gone through massive changes. The most remarkable traces back to January 2024,
    when the first stable version of LangChain was released, introducing a new organization
    of packages and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A core backbone where all the abstractions and runtime logic are stored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A layer of third-party integrations and components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of pre-built architectures and templates to leverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A serving layer to consume chains as APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An observability layer to monitor your applications in the development, testing,
    and production stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can look at the architecture in greater detail at [https://python.langchain.com/docs/get_started/introduction](https://python.langchain.com/docs/get_started/introduction).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three packages you can install to start using LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '`langchain-core`: This contains the base abstractions and runtime for the whole
    LangChain ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain-experimental`: This holds experimental LangChain code, intended
    for research and experimental uses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain-community`: This contains all third-party integrations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On top of that, there are three additional packages that we’re not going to
    cover in this book, yet can be leveraged to monitor and maintain your LangChain
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '`langserve`: LangServe is a tool that lets you deploy **LangChain runnables
    and chains** as a REST API, making it easier to integrate LangChain applications
    into production environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langsmith`: Think of LangSmith as an **innovative testing framework** for
    evaluating language models and AI applications. It helps visualize inputs and
    outputs at each step in the chain, aiding understanding and intuition during development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain-cli`: The **official command-line interface** for LangChain, it
    facilitates interactions with LangChain projects, including template usage and
    quickstarts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, LangChain introduced the **LangChain Expression Language**
    (**LCEL**) to enhance the efficiency and flexibility of text processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key features of LCEL include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Streaming asynchronous support**: This allows for the efficient handling
    of data streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch support**: This enables processing data in batches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel execution**: This enhances performance by executing tasks concurrently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retries and fallbacks**: This ensures robustness by handling failures gracefully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamically routing logic**: This allows logic flow based on input and output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message history**: This keeps track of interactions for context-aware processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are not going to cover LCEL in this book; however, all the code samples can
    be converted into LCEL if you want to speed up your development and leverage its
    native integration with the end-to-end LangChain development stack.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note**'
  prefs: []
  type: TYPE_NORMAL
- en: Before we start working with LangChain, it is important to note that all packages
    are versioned slightly differently, yet all releases are cut with high frequency
    by a maintainer with a clearer communication strategy for breaking changes.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapters, you will see some packages that have been moved, for
    example, to the `experimental` package, meaning that they are more prone to experimental
    uses. Similarly, some third-party integrations have been moved to the `community`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the next section, we are going to cover the backbone concepts
    – such as memory, VectorDB, and agents – that remain solid in the LangChain framework
    and, more generally, in the landscape of LLM development.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As introduced in *Chapter 2*, LangChain is a lightweight framework meant to
    make it easier to integrate and orchestrate LLMs and their components within applications.
    It is mainly Python based, yet it recently extended its support to JavaScript
    and TypeScript.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to LLM integration (which we will cover in an upcoming dedicated
    section), we saw that LangChain offers the following main components:'
  prefs: []
  type: TYPE_NORMAL
- en: Models and prompt templates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These components are illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: LangChain’s components'
  prefs: []
  type: TYPE_NORMAL
- en: The next sections will take a deep dive into each of these components.
  prefs: []
  type: TYPE_NORMAL
- en: Models and prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain offers more than 50 integrations with third-party vendors and platforms,
    including **OpenAI**, Azure OpenAI, Databricks, and MosaicML, as well as the integration
    with the Hugging Face Hub and the world of open-source LLMs. In *Part 2* of this
    book, we will be trying various LLMs, both proprietary and open-source, and leveraging
    LangChain’s integrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to provide an example, let’s see how easy it is to consume the OpenAI
    GPT-3 model (you can retrieve your OpenAI API key at [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: While running examples with LLMs, the output will vary at each run, due to the
    stochasticity of the models themselves. If you want to reduce the margin of variations
    in your output, you can make your model more “deterministic” by tuning the temperature
    hyperparameter. This parameter ranges from 0 (deterministic) to 1 (stochastic).
  prefs: []
  type: TYPE_NORMAL
- en: By default, the **OpenAI** module uses the `gpt-3.5-turbo-instruct` as a model.
    You can specify the model you want to use by passing the model’s name as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'As said previously, we will dive deeper into LLMs in the next section; so,
    for now, let’s focus on prompts. There are two main components related to LLM
    prompts and prompts design/engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt templates**: A prompt template is a component that defines how to
    generate a prompt for a language model. It can include variables, placeholders,
    prefixes, suffixes, and other elements that can be customized according to the
    data and the task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, suppose you want to use a language model to generate a translation
    from one language to another. You can use a prompt template like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`{sentence}` is a variable that will be replaced by the actual text. `Translation
    in {language}:` is a prefix that indicates the task and the expected output format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily implement this template as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Generally speaking, prompt templates tend to be agnostic with respect to the
    LLM you might decide to use, and it is adaptable to both completion and chat models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: A completion model is a type of LLM that takes a text input and generates a
    text output, which is called a completion. The completion model tries to continue
    the prompt in a coherent and relevant way, according to the task and the data
    it was trained on. For example, a completion model can generate summaries, translations,
    stories, code, lyrics, and more, depending on the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: A chat model is a special kind of completion model that is designed to generate
    conversational responses. A chat model takes a list of messages as input, where
    each message has a role (either system, user, or assistant) and content. The chat
    model tries to generate a new message for the assistant role, based on the previous
    messages and the system instruction.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between completion and chat models is that completion models
    expect a single text input as a prompt, while chat models expect a list of messages
    as input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example selector**: An example selector is a component in LangChain that
    allows you to choose which examples to include in a prompt for a language model.
    A prompt is a text input that guides the language model to produce a desired output.
    Examples are pairs of inputs and outputs that demonstrate the task and the format
    of the output as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The idea recalls the concept of few-shot learning we covered in *Chapter 1*.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain offers the example selector class called `BaseExampleSelector` that
    you can import and modify as you wish. You can find the API reference at [https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/](https://platform.openai.com/account/api-keys).
  prefs: []
  type: TYPE_NORMAL
- en: Data connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data connections refer to the building blocks needed to retrieve the additional
    non-parametric knowledge we want to provide the model with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to cover the typical flow of incorporating user-specific data into
    applications that are made of five main blocks, as illustrated in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![data_connection_diagram](img/B21714_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Incorporating user-specific knowledge into LLMs (source: [https://python.langchain.com/docs/modules/data_connection/](https://python.langchain.com/docs/modules/data_connection/))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Those blocks are addressed with the following LangChain tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document loaders**: They are in charge of loading documents from different
    sources such as CSV, file directory, HTML, JSON, Markdown, and PDF. Document loaders
    expose a `.load` method for loading data as documents from a configured source.
    The output is a `Document` object that contains a piece of text and associated
    metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let’s consider a sample CSV file to be loaded (you can find the
    whole code in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_05.xhtml)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Document transformers**: After importing your documents, it’s common to modify
    them to better match your needs. A basic instance of this is breaking down a lengthy
    document into smaller chunks that fit your model’s context window. Within LangChain,
    there are various pre-built document transformers available called **text splitters**.
    The idea of text splitters is to make it easier to split documents into chunks
    that are semantically related so that we do not lose context or relevant information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With text splitters, you can decide how to split the text (for example, by character,
    heading, token, and so on) and how to measure the length of the chunk (for example,
    by number of characters).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s split a document using the `RecursiveCharacterTextSplitter`
    module, which operates at a character level. For this purpose, we will be using
    a `.txt` file about mountains (you can find the whole code in the book’s GitHub
    repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_05.xhtml)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `chunk_size` refers to the number of characters in each chunk while `chunk_overlap`
    represents the number of characters overlapping between successive chunks. Here
    is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Text embedding models**: In *Chapter 1*, in the *Under the hood of an LLM*
    section, we introduced the concept of embedding as a way to represent words, subwords,
    or characters in a continuous vector space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings are the key step in incorporating non-parametric knowledge into LLMs.
    In fact, once properly stored in a VectorDB (which will be covered in the next
    section), they become the non-parametric knowledge against which we can measure
    the distance of a user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: To get started with embedding, you will need an embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: Then, LangChain offers the `Embedding` class with two main modules, which address
    the embedding of, respectively, the non-parametric knowledge (multiple input text)
    and the user query (single input text).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s consider the embeddings using the **OpenAI** embedding model
    `text-embedding-ada-002` (for more details about OpenAI embedding models, you
    can refer to the official documentation at [https://platform.openai.com/docs/guides/embeddings/what-are-embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once we have both documents and the query embedded, the next step will be to
    compute the similarity between the two elements and retrieve the most suitable
    information from the document embedding. We will see the details of this when
    talking about vector stores.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector stores**: A vector store (or VectorDB) is a type of database that
    can store and search over unstructured data, such as text, images, audio, or video,
    by using embeddings. By using embeddings, vector stores can perform a fast and
    accurate similarity search, which means finding the most relevant data for a given
    query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Similarity is a measure of how close or related two vectors are in a vector
    space. In the context of LLMs, vectors are numerical representations of sentences,
    words, or documents that capture their semantic meaning, and the distance between
    those vectors should be representative of their semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to measure similarity between vectors, and while working
    with LLMs, one of the most popular measures in use is cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: This is the cosine of the angle between two vectors in a multidimensional space.
    It is computed as the dot product of the vectors divided by the product of their
    lengths. Cosine similarity is insensitive to scale and location, and it ranges
    from -1 to 1, where 1 means identical, 0 means orthogonal, and -1 means opposite.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an illustration of the typical flow while using a vector store.
  prefs: []
  type: TYPE_NORMAL
- en: '![vector store diagram](img/B21714_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Sample architecture of a vector store (source: [https://python.langchain.com/docs/modules/data_connection/vectorstores/](https://python.langchain.com/docs/modules/data_connection/vectorstores/))'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain offers more than 40 integrations with third-party vector stores. Some
    examples are **Facebook AI Similarity Search** (**FAISS**), Elasticsearch, MongoDB
    Atlas, and Azure Search. For an exhaustive list and descriptions of all the integrations,
    you can check the official documentation at [https://python.langchain.com/docs/integrations/vectorstores/](https://python.langchain.com/docs/integrations/vectorstores/).
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s leverage the FAISS vector store, which has been developed
    by Meta AI research for efficient similarity search and clustering of dense vectors.
    We are going to leverage the same `dialogue.txt` file saved in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve embedded and saved the non-parametric knowledge, let’s also
    embed a user’s query so that it can be used to search the most similar text chunk
    using cosine similarity as a measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the output is the piece of text that is more likely to contain
    the answer to the question. In an end-to-end scenario, it will be used as context
    to the LLM to generate a conversational response.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrievers**: A retriever is a component in LangChain that can return documents
    relevant to an unstructured query, such as a natural language question or a keyword.
    A retriever does not need to store the documents itself, but only to retrieve
    them from a source. A retriever can use different methods to find relevant documents,
    such as keyword matching, semantic search, or ranking algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between a retriever and a vector store is that a retriever is
    more general and flexible than a vector store. A retriever can use any method
    to find relevant documents, while a vector store relies on embeddings and similarity
    metrics. A retriever can also use different sources of documents, such as web
    pages, databases, or files, while a vector store needs to store the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: However, a vector store can also be used as the backbone of a retriever if the
    data is embedded and indexed by a vector store. In that case, the retriever can
    use the vector store to perform a similarity search over the embedded data and
    return the most relevant documents. This is one of the main types of retrievers
    in LangChain, and it is called a vector store retriever.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s consider the FAISS vector store we previously initialized
    and “mount” a retriever on top of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Overall, data connection modules offer a plethora of integrations and pre-built
    templates that make it easier to manage the flow of your LLM-powered application.
    We will see some concrete applications of these building blocks in the upcoming
    chapters, but in the next section, we are going to take a deep dive into another
    one of LangChain’s main components.
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of LLM-powered applications, memory allows the application to
    keep references to user interactions, both in the short and long term. For example,
    let’s consider the well-known ChatGPT. While interacting with the application,
    you have the possibility to ask follow-up questions referencing previous interactions
    without explicitly telling the model.
  prefs: []
  type: TYPE_NORMAL
- en: Plus, all conversations are saved into threads, so that, if you want to follow
    up on a previous conversation, you can re-open the thread without providing ChatGPT
    with all the contexts. This is made possible thanks to ChatGPT’s ability to store
    users’ interactions into a memory variable and use this memory as context while
    addressing follow-up questions.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain offers several modules for designing your memory system within your
    applications, enabling it with both reading and writing skills.
  prefs: []
  type: TYPE_NORMAL
- en: The first step to do with your memory system is to actually store your human
    interactions somewhere. To do so, you can leverage numerous built-in memory integrations
    with third-party providers, including Redis, Cassandra, and Postgres.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, when it comes to defining how to query your memory system, there are
    various memory types you can leverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conversation buffer memory**: This is the “plain vanilla” memory type available
    in LangChain. It allows you to store your chat messages and extract them in a
    variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversation buffer window memory**: It is identical to the previous one,
    with the only difference being allowing a sliding window over only *K* interactions
    so that you can manage longer chat history over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity memory**: Entity memory is a feature of LangChain that allows the
    language model to remember given facts about specific entities in a conversation.
    An entity is a person, place, thing, or concept that can be identified and distinguished
    from others. For example, in the sentence “Deven and Sam are working on a hackathon
    in Italy,” Deven and Sam are entities (person), as well as hackathon (thing) and
    Italy (place).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity memory works by extracting information on entities from the input text
    using an LLM. It then builds up its knowledge about that entity over time by storing
    the extracted facts in a memory store. The memory store can be accessed and updated
    by the language model whenever it needs to recall or learn new information about
    an entity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conversation knowledge graph memory**: This type of memory uses a knowledge
    graph to recreate memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A knowledge graph is a way of representing and organizing knowledge in a graph
    structure, where nodes are entities and edges are relationships between them.
    A knowledge graph can store and integrate data from various sources, and encode
    the semantics and context of the data. A knowledge graph can also support various
    tasks, such as search, question answering, reasoning, and generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Another example of a knowledge graph is DBpedia, which is a community project
    that extracts structured data from Wikipedia and makes it available on the web.
    DBpedia covers topics such as geography, music, sports, and films, and provides
    links to other datasets like GeoNames and WordNet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can use this type of memory to save the input and output of each conversation
    turn as knowledge triplets (such as subject, predicate, and object) and then use
    them to generate relevant and consistent responses based on the current context.
    You can also query the knowledge graph to get the current entities or the history
    of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conversation summary memory**: When it comes to longer conversations to be
    stored, this type of memory can be very useful, since it creates a summary of
    the conversation over time (leveraging an LLM).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversation summary buffer memory**: This type of memory combines the ideas
    behind buffer memory and conversation summary memory. It keeps a buffer of recent
    interactions in memory, but rather than just completely flushing old interactions
    (as occurs for the conversation buffer memory) it compiles them into a summary
    and uses both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversation token buffer memory**: It is similar to the previous one, with
    the difference that, to determine when to start summarizing the interactions,
    this type of memory uses token lengths rather than the number of interactions
    (as occurs in summary buffer memory).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector store-backed memory**: This type of memory leverages the concepts
    of embeddings and vector stores previously covered. It is different from all the
    previous memories since it stores interactions as vectors, and then retrieves
    the top *K* most similar texts every time it is queried, using a retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangChain provides specific modules for each of those memory types. Let’s consider
    an example with the conversation summary memory, where we will also need an LLM
    to generate the summary of the interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the memory summarized the conversation, leveraging the **OpenAI**
    LLM we initialized.
  prefs: []
  type: TYPE_NORMAL
- en: There is no recipe to define which memory to use within your applications; however,
    there are some scenarios that might be particularly suitable for specific memories.
    For example, a knowledge graph memory is useful for applications that need to
    access information from a large and diverse corpus of data and generate responses
    based on semantic relationships, while a conversation summary buffer memory could
    be suitable for creating conversational agents that can maintain a coherent and
    consistent context over multiple turns, while also being able to compress and
    summarize the previous dialogue history.
  prefs: []
  type: TYPE_NORMAL
- en: Chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chains are predetermined sequences of actions and calls to LLMs that make it
    easier to build complex applications that require combining LLMs with each other
    or with other components.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain offers four main types of chain to get started with:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLMChain**: This is the most common type of chain. It consists of a prompt
    template, an LLM, and an optional **output parser**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: 'An output parser is a component that helps structure language model responses.
    It is a class that implements two main methods: `get_format_instructions` and
    `parse.` The `get_format_instructions` method returns a string containing instructions
    for how the output of a language model should be formatted. The `parse` method
    takes in a string (assumed to be the response from a language model) and parses
    it into some structure, such as a dictionary, a list, or a custom object.'
  prefs: []
  type: TYPE_NORMAL
- en: This chain takes multiple input variables, uses `PromptTemplate` to format them
    into a prompt, passes it to the model, and then uses `OutputParser` (if provided)
    to parse the output of the LLM into a final format.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s retrieve the prompt template we built in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s put it into an LLMChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**RouterChain**: This is a type of chain that allows you to route the input
    variables to different chains based on some conditions. You can specify the conditions
    as functions or expressions that return a Boolean value. You can also specify
    the default chain to use if none of the conditions are met.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, you can use this chain to create a chatbot that can handle different
    types of requests, such as planning an itinerary or booking a restaurant reservation.
    To achieve this goal, you might want to differentiate two different prompts, depending
    on the type of query the user will make:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Thanks to RouterChain, we can build a chain that is able to activate a different
    prompt depending on the user’s query. I won’t post the whole code here (you can
    find the notebook on the book’s GitHub at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_05.xhtml)),
    but you can see a sample output of how the chain reacts to two different user’s
    queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here it is with a second query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**SequentialChain**: This is a type of chain that allows you to execute multiple
    chains in a sequence. You can specify the order of the chains and how they pass
    their outputs to the next chain. The simplest module of a sequential chain, takes
    by default the output of one chain as the input of the next chain. However, you
    can also use a more complex module to have more flexibility to set input and output
    among chains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, let’s consider an AI system that is meant to first generate
    a joke on a given topic, and then translate it in to another language. To do so,
    we will first create two chains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s combine them using the `SimpleSequentialChain` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**TransformationChain**: This is a type of chain that allows you to transform
    the input variables or the output of another chain using some functions or expressions.
    You can specify the transformation as a function that takes the input or output
    as an argument and returns a new value, as well as specify the output format of
    the chain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let’s say we want to summarize a text, but before that, we want
    to rename one of the protagonists of the story (a cat) as “Silvester the Cat.”
    As a sample text, I asked Bing Chat to generate a story about cats and dogs (you
    can find the whole `.txt` file in the GitHub repository of this book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we’ve combined a simple sequential chain with a transformation
    chain, where we set as a transformation function the `rename_cat` function (you
    can see the whole code in the GitHub repository).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Overall, LangChain chains are a powerful way to combine different language
    models and tasks into a single workflow. Chains are flexible, scalable, and easy
    to use, and they enable users to leverage the power of language models for various
    purposes and domains. Starting from the next chapter, we are going to see chains
    in action in concrete use cases, but before getting there, we need to cover the
    last component of LangChain: agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agents are entities that drive decision-making within LLM-powered applications.
    They have access to a suite of tools and can decide which tool to call based on
    the user input and the context. Agents are dynamic and adaptive, meaning that
    they can change or adjust their actions based on the situation or the goal: in
    fact, while in a chain, the sequence of actions is hardcoded, in agents, the LLM
    is used as the reasoning engine with the goal of planning and executing the right
    actions in the right order.'
  prefs: []
  type: TYPE_NORMAL
- en: A core concept while talking about agents is that of tools. In fact, an agent
    might be good at planning all the right actions to fulfill a user’s query, but
    what if it cannot actually execute them, since it is missing information or executive
    power? For example, imagine I want to build an agent that is capable of answering
    my questions by searching the web. By itself, the agent has no access to the web,
    so I need to provide it with this tool. I will do so by using SerpApi (the Google
    Search API) integration provided by LangChain (you can retrieve your API key at
    [https://serpapi.com/dashboard](https://serpapi.com/dashboard)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see it in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, while initializing my agent, I set the agent type as `ZERO_SHOT_REACT_DESCRIPTION`.
    This is one of the configurations we can pick and, specifically, it configures
    the agent to decide which tool to pick based solely on the tool’s description
    with a ReAct approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ReAct approach is a way of using LLMs to solve various language reasoning
    and decision-making tasks. It was introduced in the paper *ReAct: Synergizing
    Reasoning and Acting in Language Models* by Shunyu Yao et al., back in October
    2022.'
  prefs: []
  type: TYPE_NORMAL
- en: The ReAct approach prompts LLMs to generate both verbal reasoning traces and
    text actions in an interleaved manner, allowing for greater synergy between the
    two. Reasoning traces help the model to plan, track, and update its actions, as
    well as handle exceptions. Actions allow the model to interact with external sources,
    such as knowledge bases or environments, to gather additional information.
  prefs: []
  type: TYPE_NORMAL
- en: 'On top of this configuration, LangChain also offers the following types of
    agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured input ReAct**: This is an agent type that uses the ReAct framework
    to generate natural language responses based on structured input data. The agent
    can handle different types of input data, such as tables, lists, or key-value
    pairs. The agent uses a language model and a prompt to generate responses that
    are informative, concise, and coherent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI Functions**: This is an agent type that uses the OpenAI Functions
    API to access various language models and tools from OpenAI. The agent can use
    different functions, such as GPT-3, Codex, DALL-E, CLIP, or ImageGPT. The agent
    uses a language model and a prompt to generate requests to the OpenAI Functions
    API and parse the responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversational**: This is an agent type that uses a language model to engage
    in natural language conversations with the user. The agent can handle different
    types of conversational tasks, such as chit-chat, question answering, or task
    completion. The agent uses a language model and a prompt to generate responses
    that are relevant, fluent, and engaging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self ask with search**: This is an agent type that uses a language model
    to generate questions for itself and then search for answers on the web. The agent
    can use this technique to learn new information or test its own knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReAct document store**: This is an agent type that uses the ReAct framework
    to generate natural language responses based on documents stored in a database.
    The agent can handle different types of documents, such as news articles, blog
    posts, or research papers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plan-and-execute agents**: This is an experimental agent type that uses a
    language model to choose a sequence of actions to take based on the user’s input
    and a goal. The agent can use different tools or models to execute the actions
    it chooses. The agent uses a language model and a prompt to generate plans and
    actions and then uses `AgentExecutor` to run them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain agents are pivotal whenever you want to let your LLMs interact with
    the external world. Plus, it is interesting to see how agents leverage LLMs not
    only to retrieve and generate responses, but also as reasoning engines to plan
    an optimized sequence of actions.
  prefs: []
  type: TYPE_NORMAL
- en: Together with all the LangChain components covered in this section, agents can
    be the core of LLM-powered applications, as we will see in the next chapters.
    In the next section, we are going to shift toward the world of open-source LLMs,
    introducing the Hugging Face Hub and its native integration with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Working with LLMs via the Hugging Face Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are familiar with LangChain components, it is time to start using
    our LLMs. If you want to use open-source LLMs, leveraging the Hugging Face Hub
    integration is extremely versatile. In fact, with just one access token you can
    leverage all the open-source LLMs available in Hugging Face’s repositories.
  prefs: []
  type: TYPE_NORMAL
- en: As it is a non-production scenario, I will be using the free Inference API;
    however, if you are meant to build production-ready applications, you can easily
    scale to the Inference Endpoint, which grants you a dedicated and fully managed
    infrastructure to host and consume your LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s see how to start integrating LangChain with the Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Hugging Face user access token
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To access the free Inference API, you will need a user access token, the credential
    that allows you to run the service. The following are the steps to activate the
    user access token:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a Hugging Face account**: You can create a Hugging Face account for
    free at [https://huggingface.co/join](https://huggingface.co/join).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrieve your user access token**: Once you have your account, go to the
    upper-right corner of your profile and go to **Settings** | **Access Tokens**.
    From that tab, you will be able to copy your secret token and use it to access
    Hugging Face models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Retrieving access tokens from the Hugging Face account (source:
    [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set permissions**: Access tokens enable users, applications, and notebooks
    to perform specific actions based on their assigned roles. There are two available
    roles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Read:** This allows tokens to provide read access to repositories you have
    permission to read. This includes public and private repositories owned by you
    or your organization. This role is suitable for tasks like downloading private
    models or inference.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write:** In addition to read access, tokens with this role grant write access
    to repositories where you have writing privileges. This token is useful for activities
    like training models or updating model cards.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In our series of use cases, we will keep a write permission on our token.
  prefs: []
  type: TYPE_NORMAL
- en: '**Managing your user access token**: Within your profile, you can create and
    manage multiple access tokens, so that you can also differentiate permissions.
    To create a new token, you can click on the **New token** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Creating a new token'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, at any time, you can delete or refresh your token under the **Manage**
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Managing tokens'
  prefs: []
  type: TYPE_NORMAL
- en: It is important not to leak your token, and a good practice is to periodically
    regenerate it.
  prefs: []
  type: TYPE_NORMAL
- en: Storing your secrets in an .env file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With our user access token generated in the previous section, we have the first
    secret to be managed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Secrets are data that needs to be protected from unauthorized access, such as
    passwords, tokens, keys, and credentials. Secrets are used to authenticate and
    authorize requests to API endpoints, as well as to encrypt and decrypt sensitive
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this hands-on portion of the book, we will keep all our secrets within
    an `.env` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Storing Python secrets in an `.env` file is a common practice to enhance security
    and maintainability in projects. To do this, create a file named `.env` in your
    project directory and list your sensitive information as key-value pairs: in our
    scenario, we will have `HUGGINGFACEHUB_API_TOKEN="your_user_access_token"`. This
    file should be added to your project’s `.gitignore` to prevent accidental exposure.'
  prefs: []
  type: TYPE_NORMAL
- en: To access these secrets in your Python code, use the `python-dotenv` library
    to load the `.env` file’s values as environment variables. You can easily install
    it in your terminal via `pip install python-dotenv`.
  prefs: []
  type: TYPE_NORMAL
- en: This approach keeps sensitive data separate from your code base and helps ensure
    that confidential information remains confidential throughout the development
    and deployment processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can see an example of how to retrieve your access token and set it
    as an environmental variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, by default, `load_dotenv` will look for the `.env` file in the current
    working directory; however, you can also specify the path to your secrets file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have all the ingredients to start coding, it is time to try out
    some open-source LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Start using open-source LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The nice thing about the Hugging Face Hub integration is that you can navigate
    its portal and decide, within the model catalog, what to use. Models are also
    clustered per category (**Computer Vision**, **Natural Language Processing**,
    **Audio**, and so on) and, within each category, per capability (within **Natural
    Language Processing**, we have summarization, classification, Q&A, and so on),
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Home page of Hugging Face’s model catalog'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are interested in LLMs, we will focus on the text generation category.
    For this first experiment, let’s try Falcon LLM-7B:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, with just a few lines of code, we integrated an LLM from the
    Hugging Face Hub. With analogous code, you can test and consume all the LLMs available
    in the Hub.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, throughout this book, we will be leveraging specific models for each
    application, both proprietary and open source. However, the idea is that you can
    use the model you prefer by simply initializing it as the main LLM and running
    the code as it is, simply changing the LangChain LLM integration. This is one
    of the main advantages of LLM-powered applications since you don’t have to change
    the whole code to adapt to different LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we dove deeper into the fundamentals of LangChain, since it
    will be the AI orchestrator used in the upcoming chapters: we got familiar with
    LangChain components such as memory, agents, chains, and prompt templates. We
    also covered how to start integrating LangChain with the Hugging Face Hub and
    its model catalog, and how to use the available LLMs and start embedding them
    into your code.'
  prefs: []
  type: TYPE_NORMAL
- en: From now on, we will look at a series of concrete end-to-end use cases, starting
    from a semantic Q&A search app, which we are going to develop in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain’s integration with OpenAI – [https://python.langchain.com/docs/integrations/llms/openai](https://python.langchain.com/docs/integrations/llms/openai)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain’s prompt templates – [https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain’s vector stores – [https://python.langchain.com/docs/integrations/vectorstores/](https://python.langchain.com/docs/integrations/vectorstores/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FAISS index – [https://faiss.ai/](https://faiss.ai/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain’s chains – [https://python.langchain.com/docs/modules/chains/](https://python.langchain.com/docs/modules/chains/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReAct approach – [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain’s agents – [https://python.langchain.com/docs/modules/agents/agent_types/](https://python.langchain.com/docs/modules/agents/agent_types/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face documentation – [https://huggingface.co/docs](https://huggingface.co/docs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain Expression Language (LCEL) – [https://python.langchain.com/docs/expression_language/](https://python.langchain.com/docs/expression_language/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain stable version – [https://blog.langchain.dev/langchain-v0-1-0/](https://blog.langchain.dev/langchain-v0-1-0/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/llm](https://packt.link/llm)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code214329708533108046.png)'
  prefs: []
  type: TYPE_IMG
