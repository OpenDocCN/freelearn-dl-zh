- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Real-World Use Case – Retrieval-Augmented Generation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实世界用例 - 检索增强生成
- en: In the previous chapter, we learned how to augment our kernel with memories,
    which enables our applications to be much more personalized. Cloud-based AI models,
    such as OpenAI’s GPT, usually have knowledge cut-offs that are a few months old.
    They also usually don’t have domain-specific knowledge, such as the user manuals
    of the products your company makes, and don’t know the preferences of your users,
    such as their favorite programming language or their favorite city. The previous
    chapter taught you ways to augment the knowledge of models by keeping small pieces
    of knowledge in memory and retrieving them as needed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何通过在内存中保留小块知识并将其按需检索来增强我们的内核，这使得我们的应用程序能够更加个性化。基于云的 AI 模型，如 OpenAI
    的 GPT，通常具有知识截止日期，大约是几个月前。它们通常也没有特定领域的知识，例如您公司制造产品的用户手册，也不知道用户的偏好，例如他们最喜欢的编程语言或他们最喜欢的城市。上一章向您介绍了通过在内存中保留小块知识并将其按需检索来增强模型知识的方法。
- en: In this chapter, we’re going to show you how to expand the data that’s available
    to your AI application. Instead of using a small amount of data that fits in the
    prompt, we’re going to use a large amount of data with a **retrieval-augmented
    generation** (**RAG**) application that combines the latest generative AI models
    with recent specialized information to answer questions about a specific topic
    – in our case, academic articles about AI.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您展示如何扩展可供您的 AI 应用程序使用的数据。我们不会使用适合提示的小量数据，而是将使用大量数据，通过一个 **检索增强生成**（**RAG**）应用程序来实现，该应用程序结合了最新的生成式
    AI 模型与最近的专业信息，以回答关于特定主题的问题——在我们的案例中，是关于 AI 的学术论文。
- en: RAG takes advantage of the fact that lots of institutions have useful data that
    wasn’t part of the data that was used to train OpenAI’s GPT. This gives these
    institutions a way of putting this data to use while still taking advantage of
    the generative power of GPT.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 利用大量机构拥有有用的数据这一事实，这些数据并未用于训练 OpenAI 的 GPT。这为这些机构提供了一种将这些数据投入使用的同时，还能利用 GPT
    的生成能力的方法。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Creating a document index with the Azure AI Search service
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Azure AI 搜索服务创建文档索引
- en: Loading a large number of documents to the index
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大量文档加载到索引中
- en: Creating an application that searches the index and uses AI to write an answer
    based on the data it found
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个搜索索引并使用 AI 基于找到的数据编写答案的应用程序
- en: By the end of this chapter, you will have created an application that uses a
    large amount of recent data and uses AI to find and combine the data in a user-friendly
    way.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将创建一个应用程序，该应用程序使用大量最近的数据，并使用 AI 以用户友好的方式查找和组合数据。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete this chapter, you will need to have a recent, supported version
    of your preferred Python or C# development environment:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章，您需要拥有您首选的 Python 或 C# 开发环境的最新、受支持的版本：
- en: For Python, the minimum supported version is Python 3.10, and the recommended
    version is Python 3.11
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Python，最低支持的版本是 Python 3.10，推荐版本是 Python 3.11
- en: For C#, the minimum supported version is .NET 8
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 C#，最低支持的版本是 .NET 8。
- en: In this chapter, we will call OpenAI services. Given the amount that companies
    spend on training these LLMs, it’s no surprise that using these services is not
    free. You will need an **OpenAI API** key, obtained either directly through **OpenAI**
    or **Microsoft**, via the **Azure** **OpenAI** service.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将调用 OpenAI 服务。鉴于公司在训练这些大型语言模型（LLM）上所花费的金额，使用这些服务并非免费也就不足为奇了。您需要获取一个 **OpenAI
    API** 密钥，可以通过直接从 **OpenAI** 或 **Microsoft** 获取，或者通过 **Azure OpenAI** 服务。
- en: If you are using .NET, the code for this chapter is at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch7](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch7).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 .NET，本章的代码位于 [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch7](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch7)。
- en: If you are using Python, the code for this chapter is at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch7](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch7).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Python，本章的代码位于[https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch7](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch7)。
- en: To create a document index, you will need a free trial of Microsoft Azure AI
    Search.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建文档索引，您需要Microsoft Azure AI Search的免费试用版。
- en: 'You can install the required packages by going to the GitHub repository and
    using the following: `pip install -``r requirements.txt`.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问GitHub仓库并使用以下命令安装所需的包：`pip install -r requirements.txt`。
- en: Why would you need to customize GPT models?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么您需要自定义GPT模型？
- en: 'GPT models are already very useful without any customizations. When your user
    types a request, you, as a programmer, could simply forward the request to the
    GPT model (such as GPT-3.5 or GPT-4), and, in many cases, the unaltered response
    from the model is good enough. However, in many cases, the responses aren’t good
    enough. There are three categories of problems with responses:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型在未经任何自定义的情况下已经非常有用。当您的用户输入请求时，作为程序员的您可以将请求简单地转发给GPT模型（如GPT-3.5或GPT-4），在许多情况下，模型未经修改的响应已经足够好。然而，在许多情况下，响应并不足够好。响应问题可以分为三类：
- en: '**Non-text functionality**: In some cases, the response you want is not text-based.
    For example, you may want to allow your user to turn a light on or off, perform
    complex math, or insert records into a database.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非文本功能**：在某些情况下，您想要的响应不是基于文本的。例如，您可能希望允许您的用户打开或关闭灯光，执行复杂的数学运算，或将记录插入数据库。'
- en: '**Lack of context**: Models can’t accurately answer questions if they haven’t
    been exposed to the data that contains the answer. Despite being trained with
    immense amounts of data, there’s a lot of data that LLMs haven’t been exposed
    to. At the time of writing, the cut-off date for data used to train GPT 3.5 and
    GPT-4 is September 2021, although there is a preview version of GPT-4 called GPT-4
    Turbo with a cut-off date of December 2023 (you can see the cut-off dates of models
    at [https://platform.openai.com/docs/models/](https://platform.openai.com/docs/models/).)
    In addition, models don’t have access to proprietary data, such as the internal
    documents of your company.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏上下文**：如果模型没有接触过包含答案的数据，它们就无法准确回答问题。尽管经过大量数据的训练，但仍有大量数据LLM尚未接触过。撰写本文时，用于训练GPT
    3.5和GPT-4的数据截止日期为2021年9月，尽管有一个名为GPT-4 Turbo的GPT-4预览版本，其截止日期为2023年12月（您可以在[https://platform.openai.com/docs/models/](https://platform.openai.com/docs/models/)上查看模型的截止日期）。此外，模型无法访问专有数据，例如贵公司的内部文件。'
- en: '`Answer only with Y or N` to your prompt, but some requests return responses
    such as `Yes` (instead of *Y*) or `The answer is yes`, which requires adding code
    to validate the answer.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`仅用Y或N回答您的提示`，但某些请求返回的响应可能是`Yes`（而不是*Y*）或`答案是肯定的`，这需要添加代码来验证答案。'
- en: 'We showed you how to solve the first issue (non-text functionality) using Semantic
    Kernel via native functions, as shown in [*Chapter 3*](B21826_03.xhtml#_idTextAnchor071).
    However, if the problem with the responses you’re getting is a lack of context
    or format, you can use the techniques depicted in the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向您展示了如何使用语义内核通过原生函数解决第一个问题（非文本功能），如[*第3章*](B21826_03.xhtml#_idTextAnchor071)中所示。然而，如果您收到的响应问题在于缺乏上下文或格式，您可以使用以下图表中描述的技术：
- en: '![Figure 7.1 – Techniques to improve responses](img/B21826_07_1.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 提高响应的技术](img/B21826_07_1.jpg)'
- en: Figure 7.1 – Techniques to improve responses
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 提高响应的技术
- en: 'The first technique you should always try is **prompt engineering**, something
    we covered in detail in [*Chapter 2*](B21826_02.xhtml#_idTextAnchor045). Prompt
    engineering is easy to do and test: it can be used both to give new data to the
    LLM (improving context) and to provide some examples of how you want the answer
    to look (improving format).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该始终尝试的第一个技术是**提示工程**，我们在[*第2章*](B21826_02.xhtml#_idTextAnchor045)中详细介绍了这一技术。提示工程既容易做又容易测试：它可以用来向LLM提供新数据（改善上下文），也可以提供一些示例，说明您希望答案看起来如何（改善格式）。
- en: For example, let’s say you’re building an application that gives your team suggestions
    of places to go for lunch, something that’s always a challenge among teams of
    developers. Instead of simply asking `Where should we go for lunch?`, you will
    get much better results by adding context and format specifications, such as `We
    are a team of six developers aged 25-38, two of us are vegetarians, and we are
    looking for places to have lunch near the Eiffel Tower on a Friday. We want to
    spend less than 20 euro per person and we don't want to spend more than 90 minutes
    having lunch. Please provide your answer with the name of the place, their website,
    their average price, and their street address`. The format specification is the
    last sentence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你正在构建一个应用程序，为你的团队提供午餐地点的建议，这对于开发团队来说总是一个挑战。与其简单地问“我们午餐去哪里？”，不如通过添加上下文和格式规范来获得更好的结果，例如：“我们是一支由六个
    25-38 岁的开发者组成的团队，其中两人是素食主义者，我们想在周五靠近埃菲尔铁塔的地方吃午餐。我们希望每人花费不超过 20 欧元，并且我们不想花费超过 90
    分钟吃午餐。请以地点名称、网站、平均价格和街道地址的形式提供你的答案。”格式规范是最后一句话。
- en: The main downside is that the more data you want to provide and the more complex
    the instructions, the larger your prompts will become, resulting in additional
    costs and latency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的缺点是，你想要提供的数据越多，指令越复杂，你的提示就会变得越大，从而导致额外的成本和延迟。
- en: Besides providing examples through prompt engineering, another technique you
    can use to improve the format of your answer is to fine-tune your model. Fine-tuning
    allows you to provide hundreds or thousands of examples of questions and answers
    to an existing model (for example, GPT-3.5) and save a new, fine-tuned model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过提示工程提供示例之外，你还可以使用微调模型来改进你答案的格式。微调允许你向现有的模型（例如 GPT-3.5）提供成百上千个问题和答案的示例，并保存一个新的、经过微调的模型。
- en: One example of successful fine-tuning is to show thousands of examples of the
    way you expect JSON output to look. Since you are providing thousands of examples,
    you can’t pass this on to every prompt because the prompt will become too large.
    You can create a file that contains thousands of questions and JSON answers and
    use the OpenAI fine-tuning API or fine-tuning UI to create a custom GPT model
    that has been trained with your additional examples. The result will be a model
    that is a lot better at providing JSON answers, and worse at everything else.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 成功微调的一个例子是展示成千上万种你期望 JSON 输出应该呈现的方式。由于你提供了成千上万的示例，你不能将这些示例传递给每个提示，因为提示会变得太大。你可以创建一个包含成千上万问题和
    JSON 答案的文件，并使用 OpenAI 微调 API 或微调 UI 创建一个经过你额外示例训练的定制 GPT 模型。结果将是一个在提供 JSON 答案方面表现得更好，而在其他方面表现更差的模型。
- en: 'If your application only needs to provide JSON answers, that’s exactly what
    you need. Microsoft Semantic Kernel does not help with fine-tuning, so techniques
    for fine-tuning are outside the scope of this book. If you want to learn more
    about fine-tuning, this online article from Sebastian Raschka, a Packt author,
    can help: [https://magazine.sebastianraschka.com/p/finetuning-large-language-models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用程序只需要提供 JSON 格式的答案，这正是你所需要的。Microsoft Semantic Kernel 并不帮助进行微调，因此微调技术不在此书的范围之内。如果你想了解更多关于微调的信息，Sebastian
    Raschka（Packt 作者）的这篇在线文章可能会有所帮助：[https://magazine.sebastianraschka.com/p/finetuning-large-language-models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)。
- en: 'In practice, one of the most common problems is that the LLM will not have
    enough context to provide the answers you want. This can happen even if the data
    that’s required to provide the answer has been used to train the model: since
    LLMs are trained with a lot of data, you may need to add relevant data to your
    request to help the model recall the data that’s relevant to your request from
    the large amount of data it was trained with. For example, if you simply ask GPT
    `Who is the best football player of all time?`, it may not know whether you mean
    association football (soccer) or NFL (American) football.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，最常见的问题之一是 LLM 可能没有足够的上下文来提供你想要的答案。即使提供答案所需的数据已经被用于训练模型，这也可能发生：由于 LLM 是用大量数据进行训练的，你可能需要向你的请求中添加相关数据，以帮助模型从它训练的大量数据中回忆起与你的请求相关的数据。例如，如果你只是问
    GPT “谁是史上最佳足球运动员？”，它可能不知道你指的是足球（足球）还是 NFL（美国足球）。
- en: In some other cases, as discussed previously when we mentioned the cut-off date
    and private data examples, the model has never seen the data required to answer
    the question, and you need to show it to the model as you are making the request.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些其他情况下，正如我们之前在提到截止日期和私人数据示例时讨论的那样，模型从未见过回答问题所需的数据，你需要在你提出请求时将其展示给模型。
- en: 'To an extent, you can solve both problems with prompt engineering:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在一定程度上，你可以通过提示工程来解决这两个问题：
- en: You can tell the model to play a role. For example, you can add `you are a Python
    software engineer` to prime the model to respond more technically, or `you are
    a five-year-old child` to prime the model to respond more simply.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以指示模型扮演一个角色。例如，你可以添加 `你是一名Python软件工程师` 来引导模型以更技术性的方式回答，或者添加 `你是一个五岁的孩子` 来引导模型以更简单的方式回答。
- en: You can give the model some data examples. For example, you can add `If the
    user says 'the earth is flat', reply with 'misinformation'; if the user says 'the
    moon landing was fake', reply with 'misinformation'; if the user says 'birds are
    real', reply with "true"` to your prompt, either directly or by using prompt templates
    in semantic functions.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以向模型提供一些数据示例。例如，你可以在提示中直接添加 `如果用户说'地球是平的'，回复'错误信息'；如果用户说'登月是假的'，回复'错误信息'；如果用户说'鸟是真实的'，回复"真实"`，或者通过使用语义函数中的提示模板。
- en: You can add some fields to your prompt template and fill them in real time.
    For example, you can get today’s date from the system and create a prompt that
    states `the difference between $today and July 4th, 1776, in days is…"`, replacing
    `$today` dynamically, and therefore passing recent information to the model.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在提示模板中添加一些字段并在实时填充它们。例如，你可以从系统中获取今天的日期并创建一个提示，声明 `从今天到1776年7月4日，相差的天数是……"`，动态地替换
    `$today`，因此将最近的信息传递给模型。
- en: The first downside of prompt engineering is that the more data you need to pass,
    the larger your prompts will get, which will make the prompts more expensive.
    It will also increase latency as it will take longer for the LLM to process long
    prompts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程的第一个缺点是，你需要传递的数据越多，你的提示就会越大，这会使提示更昂贵。它还会增加延迟，因为LLM处理长提示需要更长的时间。
- en: Even if your budget can support the additional cost and your users are extremely
    patient and don’t mind waiting for the answers, there are still two problems.
    The first is that the accuracy of LLMs decreases [1] as prompts get larger. The
    second is that at some point, you may run out of space in the context window of
    the model. For example, let’s say you work for a company that manufactures cars,
    and you want to help a user find the answer to a question about their car in the
    user manual, but it’s 300 pages long. Even if you were to solve all previous problems,
    you can’t pass the whole manual in the prompt because it doesn’t fit.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的预算可以支持额外的成本，并且你的用户非常耐心，不介意等待答案，仍然存在两个问题。第一个问题是，随着提示的增大，LLMs的准确性会降低[1]。第二个问题是，在某个时候，你可能会耗尽模型上下文窗口的空间。例如，假设你为一家制造汽车的公司工作，你想帮助用户在用户手册中找到关于他们汽车的答案，但手册有300页长。即使你解决了所有之前的问题，你也不能在提示中传递整个手册，因为它放不下。
- en: The solution that works best is to break your user manual into several chunks
    and save these chunks to an index. When the user asks a question, you can use
    a search algorithm to return the most relevant chunks by using something such
    as cosine similarity, as shown in [*Chapter 6*](B21826_06.xhtml#_idTextAnchor120).
    Then, you only need to pass the relevant chunks to the prompt. The name of this
    technique is RAG and it’s widely used. Semantic Kernel makes it easy to implement
    it, but you also need an index. Let’s delve into the details.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的解决方案是将用户手册分成几个部分，并将这些部分保存到索引中。当用户提问时，你可以使用搜索算法通过使用诸如余弦相似度等方法返回最相关的部分，如[*第6章*](B21826_06.xhtml#_idTextAnchor120)所示。然后，你只需要将相关的部分传递给提示。这种技术的名称是RAG，并且它被广泛使用。Semantic
    Kernel使其易于实现，但你还需要一个索引。让我们深入了解细节。
- en: Retrieval-augmented generation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: RAG is an approach that combines the powers of pre-trained language models with
    information retrieval to generate responses based on a large corpus of documents.
    This is particularly useful for generating informed responses that rely on external
    knowledge not contained within the model’s training dataset.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种结合预训练语言模型的力量和信息检索来根据大量文档生成响应的方法。这对于生成基于外部知识（不包含在模型训练数据集中）的知情响应特别有用。
- en: 'RAG involves three steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: RAG涉及三个步骤：
- en: '**Retrieval**: Given an input query (for example, a question or a prompt),
    you use a system to retrieve relevant documents or passages from your data sources.
    This is typically done using embeddings.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**：给定一个输入查询（例如，一个问题或提示），你使用一个系统从你的数据源中检索相关文档或段落。这通常是通过嵌入来完成的。'
- en: '**Augmentation**: The retrieved documents are then used to augment the input
    prompt. Usually, this means creating a prompt that incorporates the data from
    the retrieval step and adds some prompt engineering.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强**：检索到的文档随后被用来增强输入提示。通常这意味着创建一个结合检索步骤中的数据并添加一些提示工程的提示。'
- en: '**Generation**: The augmented prompt is then fed into a generative model, usually
    GPT, which generates the output. Because the prompt contains relevant information
    from the retrieved documents, the model can generate responses that are informed
    by that external knowledge.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**：增强后的提示随后被输入到一个生成模型，通常是GPT，它生成输出。因为提示包含了检索到的文档中的相关信息，模型可以生成受外部知识启发的外部知识。'
- en: In addition to providing additional and more recent information to an AI service,
    RAG can help with **grounding**. Grounding is the process of tying the language
    model’s responses to accurate, reliable, and contextually appropriate knowledge
    or data. This can be particularly important in scenarios where factual accuracy
    and relevance are crucial, such as answering questions about science, history,
    or current events. Grounding helps ensure that the information provided by the
    model is not only plausible but also correct and applicable to the real world.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了向AI服务提供额外的和更近期的信息外，RAG还可以帮助进行**扎根**。扎根是将语言模型的响应与准确、可靠和上下文适当的知识或数据联系起来的过程。这在事实准确性和相关性至关重要的场景中尤为重要，例如回答有关科学、历史或当前事件的问题。扎根有助于确保模型提供的信息不仅合理，而且正确，并且适用于现实世界。
- en: When you use RAG, you give the LLM the data that you want it to use to generate
    your responses. If your data is accurate, reliable, and contextually appropriate,
    the text that’s generated by the LLM using this data has a very high likelihood
    of also being accurate, reliable, and contextually appropriate. You can even ask
    the generator step to provide links to the documents it used. We will see this
    in our example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用RAG时，你向LLM提供你希望它用来生成你响应的数据。如果你的数据是准确、可靠和上下文适当的，那么LLM使用这些数据生成的文本很可能也是准确、可靠和上下文适当的。你甚至可以要求生成步骤提供它使用的文档的链接。我们将在我们的示例中看到这一点。
- en: Let’s say you want to summarize the latest discoveries in models with large
    context windows. First, you need to retrieve information about the latest discoveries
    by doing a web search or using a database of academic papers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要总结关于具有大上下文窗口的模型最新发现的内容。首先，你需要通过进行网络搜索或使用学术论文数据库来检索关于最新发现的信息。
- en: 'To implement RAG, you need a few extra components:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现RAG，你需要一些额外的组件：
- en: '`numpy`, which have the advantage of being free.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`具有免费的优势。'
- en: '**Retrieval system**: The software that’s used to find the most relevant documents
    from the document store based on the input query.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索系统**：用于根据输入查询从文档存储中找到最相关文档的软件。'
- en: Most vector database vendors provide algorithms that work well with their service,
    and lately, most solutions have been using vector comparisons such as cosine similarity.
    For example, services such as Pinecone and Azure AI Search provide document and
    embedding storage and retrieval algorithms.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数向量数据库供应商提供与他们的服务兼容的算法，最近，大多数解决方案都采用了向量比较，如余弦相似度。例如，Pinecone和Azure AI Search等服务提供文档和嵌入存储和检索算法。
- en: 'In our example, we will create an application that allows you to search for
    and ask questions about AI papers from the ArXiV database. We downloaded the list
    of ArXiV IDs, authors, titles, and abstracts for all papers in the *Computation
    and Language* category that were submitted in 2021 and after. This dataset is
    available in this book’s GitHub repository: [https://github.com/PacktPublishing/Microsoft-Semantic-Kernel/blob/b1187f88f46589f14a768e3ee7b89bef733f1263/data/papers/ai_arxiv_202101.json](https://github.com/PacktPublishing/Microsoft-Semantic-Kernel/blob/b1187f88f46589f14a768e3ee7b89bef733f1263/data/papers/ai_arxiv_202101.json).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将创建一个应用程序，允许你搜索和询问来自 ArXiV 数据库的 AI 论文。我们下载了 2021 年及以后提交的 *计算与语言* 类别中所有论文的
    ArXiV ID、作者、标题和摘要列表。这个数据集可以在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Microsoft-Semantic-Kernel/blob/b1187f88f46589f14a768e3ee7b89bef733f1263/data/papers/ai_arxiv_202101.json](https://github.com/PacktPublishing/Microsoft-Semantic-Kernel/blob/b1187f88f46589f14a768e3ee7b89bef733f1263/data/papers/ai_arxiv_202101.json)。
- en: The dataset contains a total of 36,908 scientific articles. The summaries of
    their contents are in the `abstract` field and contain over 40 million characters,
    which would require approximately 10 million tokens, something that’s too large
    for even the largest AI models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含总共 36,908 篇科学文章。它们的摘要内容位于 `abstract` 字段中，包含超过 4000 万个字符，这大约需要 1000 万个标记，即使是最大的
    AI 模型也无法处理。
- en: We are going to load all this data into an Azure AI Search index. But before
    we load the articles, we must create the index.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把所有这些数据加载到 Azure AI 搜索索引中。但在加载文章之前，我们必须创建索引。
- en: Creating an index
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建索引
- en: 'To store and retrieve large amounts of data, we will need to create an index.
    To do so, you must have an Azure account and must create an Azure AI Search service.
    Just search for `Azure AI Search` and click **Create**; you will be asked for
    a name. You will need the endpoint of the service, which you can find in the **Configuration**
    tab, shown in *Figure 7**.2*. or the Azure AI Search service you created. *Figure
    7**.2* shows the endpoint for the service you created in the **Url** field, marked
    in green:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储和检索大量数据，我们需要创建一个索引。为此，你必须有一个 Azure 账户，并必须创建一个 Azure AI 搜索服务。只需搜索 `Azure
    AI Search` 并点击 **创建**；你将被要求输入一个名称。你需要服务的端点，你可以在 **配置** 选项卡中找到它，如图 *7**.2* 所示。或者你创建的
    Azure AI 搜索服务。*图 7**.2* 显示了你在 **Url** 字段中创建的服务端点，用绿色标记：
- en: '![Figure 7.2 – Azure AI Search configuration screen](img/B21826_07_2.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – Azure AI 搜索配置屏幕](img/B21826_07_2.jpg)'
- en: Figure 7.2 – Azure AI Search configuration screen
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – Azure AI 搜索配置屏幕
- en: You will also need an admin key, which you can find under the **Keys** tab for
    your Azure AI Search service.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要一个管理密钥，你可以在 Azure AI 搜索服务的 **密钥** 选项卡下找到。
- en: 'Creating a service is just the first step: the service is just a place to store
    one or more indexes, which are the places where we will store the data. Now that
    we have a service, we need to write code to create the index.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务只是第一步：服务只是一个存储一个或多个索引的地方，而索引是我们将存储数据的地方。现在我们有了服务，我们需要编写代码来创建索引。
- en: The field names deserve mentioning. Your life will be a lot easier if you can
    use some standard names – that is, `Id`, `AdditionalMetadata`, `Text`, `Description`,
    `ExternalSourceName`, `IsReference`, and `Embedding`. The field names should use
    that specific capitalization. If you use these names, you can easily use the preview
    version of the Azure AI Search Semantic Kernel connection, which will make your
    code much smaller. The text you’ll use for searching (abstracts, in our case)
    should be `Text`. In the following code, I’ll map these fields to what we need.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 字段名称值得提及。如果你能使用一些标准名称——即 `Id`、`AdditionalMetadata`、`Text`、`Description`、`ExternalSourceName`、`IsReference`
    和 `Embedding`，你的生活将会轻松很多。字段名称应使用特定的首字母大写。如果你使用这些名称，你可以轻松地使用 Azure AI 搜索语义内核连接的预览版本，这将使你的代码更小。你将用于搜索的文本（在我们的例子中是摘要）应该是
    `Text`。在下面的代码中，我将将这些字段映射到我们需要的内容。
- en: So, let’s see how to do that in Python. Later, we’ll learn how to do this in
    C#.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们看看如何在 Python 中实现这一点。稍后，我们将学习如何在 C# 中实现。
- en: Creating the index with Python
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 创建索引
- en: 'Write the following code in a Python script to create an index:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 脚本中编写以下代码以创建索引：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, you need to import the `AzureKeyCredential` function to read your admin
    key and `SearchIndexClient` to create an object that will allow you to interact
    with the Azure AI Search service.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要导入 `AzureKeyCredential` 函数来读取你的管理密钥，以及 `SearchIndexClient` 来创建一个对象，该对象将允许你与
    Azure AI 搜索服务交互。
- en: 'Next, we will import several classes for the types we will be using in our
    index:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将导入我们将要在索引中使用的一些类：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For fields that we want to search using embeddings, we use the `SearchField`
    type. For other fields, we use the `SimpleField` type if we don’t intend to search
    for content inside of them, and `SearchableField` if we want them to be searchable
    by string comparisons.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们想要使用嵌入进行搜索的字段，我们使用`SearchField`类型。对于其他字段，如果我们不打算在它们内部搜索内容，则使用`SimpleField`类型，如果我们希望它们可以通过字符串比较进行搜索，则使用`SearchableField`类型。
- en: 'Next, let’s create an API client that will add a new index to the index collection
    with the `SearchIndexClient` class:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用`SearchIndexClient`类创建一个API客户端，该客户端将使用`SearchIndexClient`类向索引集合中添加一个新的索引：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When you are in the development phase, it’s not uncommon to have to redesign
    your index by adding or dropping fields, changing the size of the embeddings,
    and so on. Therefore, we usually drop and recreate the fields in the script. To
    drop a field in the preceding snippet, we used the `delete_index` method.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发阶段，需要通过添加或删除字段、更改嵌入的大小等方式重新设计索引的情况并不少见。因此，我们通常在脚本中删除并重新创建字段。在前面的代码片段中，我们使用`delete_index`方法删除了一个字段。
- en: 'The following code specifies the fields and their properties to help describe
    which fields the index will contain:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码指定了字段及其属性，以帮助描述索引将包含哪些字段：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we are adding the same fields we have in our dataset to the index: `id`,
    `authors`, `title`, and `abstract`. In addition, we’re adding a field called `Embedding`,
    where we will put the embedding vectors of the articles’ abstracts. For that field,
    we need to specify a vector search algorithm profile and a vector search dimension.
    The dimension is the size of the embeddings. Since we’re using the new `OpenAI
    text-embeddings-3-small`, the embeddings’ size is 1,536.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将数据集中已有的字段添加到索引中：`id`、`authors`、`title`和`abstract`。此外，我们还在索引中添加了一个名为`Embedding`的字段，我们将在此字段中放置文章摘要的嵌入向量。对于该字段，我们需要指定一个向量搜索算法配置文件和一个向量搜索维度。维度是嵌入的大小。由于我们正在使用新的`OpenAI
    text-embeddings-3-small`，因此嵌入的大小为1,536。
- en: 'These embeddings are used in search algorithms. Azure AI Search uses an algorithm
    called **Hierarchical Navigable Small World** (**HNSW**), a flexible algorithm
    that’s closely related to nearest neighbors for high-dimensional spaces, such
    as the number of dimensions of our embeddings. We’ll use this algorithm later
    to search for items in our index and bring the ones that are more closely related.
    Let’s add it to our embedding field:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入用于搜索算法。Azure AI Search使用一个名为**分层可导航小世界**（**HNSW**）的算法，这是一个灵活的算法，与高维空间（如我们嵌入的维度数）的最近邻算法密切相关。我们将稍后使用此算法在我们的索引中搜索项目，并检索与搜索内容更相关的项目。让我们将其添加到我们的嵌入字段中：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding snippet, we used cosine similarity as the metric that determines
    the items in the index that are more closely related to what the user searched
    for. For now, we’ve used the default parameters of `m=10`, `ef_construction=400`,
    and `ef_search=500`. `ef` in the parameters stands for *exploration factor*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了余弦相似度作为度量标准，该标准决定了索引中与用户搜索内容更相关的项目。目前，我们使用了默认参数`m=10`、`ef_construction=400`和`ef_search=500`。参数中的`ef`代表*探索因子*。
- en: 'The `m` parameter controls the density of the index – in the index, each record
    will have `m` neighbors. The `ef_construction` parameter increases the number
    of candidates being used to find neighbors for each record: the higher this parameter,
    the more thorough the search is going to be. The `ef_search` parameter controls
    the depth of the search during runtime – that is, when a search is executed, how
    many results are retrieved from the index for comparison.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`m`参数控制索引的密度——在索引中，每条记录将具有`m`个邻居。`ef_construction`参数增加了用于为每条记录找到邻居的候选者数量：此参数越高，搜索就越彻底。`ef_search`参数控制运行时搜索的深度——也就是说，当执行搜索时，从索引中检索多少结果进行比较。'
- en: Increasing `ef_construction` causes the index construction to take longer, whereas
    increasing `ef_search` causes runtime searches to take longer. In most cases,
    the numbers can be close to each other, but if you are planning to update the
    index frequently and don’t want the construction time to take longer, you may
    increase `ef_search`. On the other hand, if your searches are already taking long
    enough at runtime and you want to improve their quality, you may increase `ef_construction`
    as it will make the results better and only increase the time it takes to build
    the index, but not the time it takes to execute a search.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 增加`ef_construction`会导致索引构建时间更长，而增加`ef_search`会导致运行时搜索时间更长。在大多数情况下，这些数字可以非常接近，但如果你计划频繁更新索引且不想构建时间变长，你可以增加`ef_search`。另一方面，如果你的搜索在运行时已经足够长，并且你想提高其质量，你可以增加`ef_construction`，因为它会使结果更好，但只会增加构建索引的时间，而不会增加执行搜索的时间。
- en: Higher values for these parameters make the index better at finding records,
    but they also make it take longer to build and search through. The parameters
    we used here work well for this example, but when you are using your own dataset
    for your application, be sure to experiment with the parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数的值越高，索引在查找记录方面表现得越好，但它们也会使构建和搜索索引的时间更长。我们在这里使用的参数对示例来说效果很好，但当你使用自己的数据集为你的应用程序时，务必对参数进行实验。
- en: 'Finally, we simply call `create_or_update_index` with all the parameters we
    specified. This command is what will create the index:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只需调用`create_or_update_index`并传入我们指定的所有参数。这个命令将创建索引：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have an index, we can upload the records (each record is called
    a document) from our dataset into it.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了索引，我们可以将数据集中的记录（每条记录称为文档）上传到其中。
- en: Next, we’ll learn how to create the index with C#.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用C#创建索引。
- en: Creating the index using C#
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用C#创建索引
- en: 'It’s a lot simpler to create the index using C#. First, we must define the
    fields in a class, which I chose to call `SearchModel`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用C#创建索引要简单得多。首先，我们必须在类中定义字段，我选择将其称为`SearchModel`：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we are using the same field names that we used for Python. Note that we
    didn’t create an `Embedding` field like in Python. This will be created later,
    dynamically, when we load the documents.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用与Python相同的字段名。请注意，我们没有创建一个`Embedding`字段，就像在Python中那样。这将在我们加载文档时动态创建。
- en: 'Let’s see how to create an index:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何创建索引：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The code is straightforward: first, we create a list of fields in `SearchModel`
    using the `FieldBuilder` class; then, we create an `index` object with the `SearchIndex`
    class; and finally, we call `CreateOrUpdateIndex` to create the index in the cloud
    service.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 代码很简单：首先，我们使用`FieldBuilder`类在`SearchModel`中创建字段列表；然后，我们使用`SearchIndex`类创建一个`index`对象；最后，我们调用`CreateOrUpdateIndex`在云服务中创建索引。
- en: Uploading documents to the index
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文档上传到索引
- en: While loading documents to the index is also straightforward, there are a couple
    of details that we need to pay attention to.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将文档加载到索引中也很直接，但还有一些细节我们需要注意。
- en: The first detail is the unique identifier of the document. In our case, that
    is the `Id` field. In an ideal case, the data that you want to load will have
    a unique and immutable identifier.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个细节是文档的唯一标识符。在我们的案例中，那就是`Id`字段。在理想情况下，你想要加载的数据将有一个唯一且不可变的标识符。
- en: 'Luckily, that is the case for the ArXiV database: the `Id` field in the ArXiV
    database is unique and immutable and can always be used to search for articles
    online. For example, the article with an ID of `2309.12288` will always be the
    latest version of the *The Reversal Curse: LLMs trained on “A is B” fail to learn
    “B is A”* article [2], which talks about a quirk in LLMs: when asked who Tom Cruise’s
    mother is, it will give the correct answer, Mary Lee Pfeiffer, 79% of the time.
    When asked who Mary Lee Pfeiffer’s famous actor son is, it will give the correct
    answer, Tom Cruise, only 33% of the time.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，ArXiV数据库就是这样：ArXiV数据库中的`Id`字段是唯一且不可变的，并且可以始终用于在线搜索文章。例如，ID为`2309.12288`的文章将始终是*《逆转诅咒：在“A是B”上训练的LLMs无法学习“B是A”》*文章的最新版本[2]，该文章讨论了LLMs的一个怪癖：当被问及汤姆·克鲁斯的母亲是谁时，它有79%的概率给出正确答案，玛丽·李·佩菲弗，79岁。当被问及玛丽·李·佩菲弗的著名演员儿子是谁时，它只有33%的概率给出正确答案，汤姆·克鲁斯。
- en: 'The uniqueness and immutability of the `Id` field allow us to update the index
    with new information as needed. However, there’s one caveat: in the index, the
    `Id` field can only contain numbers, letters, and underscores, so we will need
    to replace the dot with an underscore.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`Id` 字段的唯一性和不可变性使我们能够根据需要更新索引以包含新信息。然而，有一个注意事项：在索引中，`Id` 字段只能包含数字、字母和下划线，因此我们需要将点替换为下划线。'
- en: The second detail is that we need to load the embeddings. For Python, at the
    time of writing, this will require us to calculate the embeddings manually, as
    we did in [*Chapter 6*](B21826_06.xhtml#_idTextAnchor120). Different embedding
    models produce data vectors with different meanings, and usually different sizes,
    but even if the sizes are the same, the embeddings are incompatible unless explicitly
    stated.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个细节是我们需要加载嵌入。对于 Python 来说，在撰写本文时，这将需要我们手动计算嵌入，就像我们在 [*第 6 章*](B21826_06.xhtml#_idTextAnchor120)
    中所做的那样。不同的嵌入模型会产生具有不同含义的数据向量，通常大小也不同，即使大小相同，除非明确说明，否则嵌入是不兼容的。
- en: Therefore, you can’t create your embeddings with a model and later use another
    embedding model to do your searches. Also, this means that whoever is writing
    the code to perform searches needs to know the exact embedding model that was
    used to load the data in the index. In C#, we can use a connector called `Microsoft.SemanticKernel.Connectors.AzureAISearch`.
    That connector, while still in preview, will greatly simplify things. This should
    be available for Python soon but isn’t at the time of writing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你不能使用一个模型创建嵌入，然后使用另一个嵌入模型进行搜索。这也意味着编写执行搜索代码的人需要知道用于加载索引数据的确切嵌入模型。在 C# 中，我们可以使用名为
    `Microsoft.SemanticKernel.Connectors.AzureAISearch` 的连接器。虽然该连接器目前仍处于预览阶段，但它将大大简化事情。这应该很快就会对
    Python 可用，但在撰写本文时还没有。
- en: Now that we know about these two details, let’s write some code that will load
    the documents into the index.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了这些细节，让我们编写一些代码来将文档加载到索引中。
- en: Uploading documents with Python
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 上传文档
- en: 'We start by importing several packages:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入几个包：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first set of packages is for connecting to the Azure AI Search index. The
    packages are similar to the ones we used when creating the index, but note that
    we’re using a different class, `SearchClient`, instead of `SearchIndexClient`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组包用于连接到 Azure AI Search 索引。这些包与我们创建索引时使用的包类似，但请注意，我们正在使用一个不同的类，`SearchClient`，而不是
    `SearchIndexClient`。
- en: 'Now, let’s load the Semantic Kernel packages:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载语义内核包：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These Semantic Kernel packages are going to be used to connect to the OpenAI
    service and generate the embeddings.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语义内核包将被用来连接到 OpenAI 服务并生成嵌入。
- en: 'Finally, we’re going to import some packages to help us control the flow of
    the program:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将导入一些包来帮助我们控制程序的流程：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `tenacity` library is helpful when you need to call functions that may fail
    as it provides you with functionality that allows you to automatically retry.
    The `pandas` library is used to load a CSV file. It’s not strictly necessary;
    you can manipulate CSVs directly without it, but the `pandas` library makes it
    easier.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要调用可能会失败的功能时，`tenacity` 库非常有用，因为它为你提供了自动重试的功能。`pandas` 库用于加载 CSV 文件。它不是严格必要的；即使没有它，你也可以直接操作
    CSV 文件，但 `pandas` 库使这个过程更容易。
- en: 'Next, let’s define a helper function to generate embeddings:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个辅助函数来生成嵌入：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This function assumes we have a kernel with a service named `emb` that can generate
    embeddings for a given text. We used the `retry` decorator to try to generate
    embeddings three times before giving up, waiting between `1` and `5` seconds between
    each try, increasing the interval as the number of tries increased.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数假设我们有一个名为 `emb` 的服务的内核，它可以生成给定文本的嵌入。我们使用了 `retry` 装饰器来尝试生成嵌入三次，如果失败则放弃，每次尝试之间等待
    `1` 到 `5` 秒，随着尝试次数的增加而增加间隔。
- en: Since the OpenAI service that we’re going to use for generating embeddings is
    an online service and we have more than 30,000 articles to generate embeddings
    for, we are going to call it more than 30,000 times. With so many calls, it’s
    not uncommon for some of them to occasionally fail due to network connectivity
    or the service being too busy. Therefore, adding the `retry` functionality can
    help so that you don’t get an error on call number 29,000 that breaks your program.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将要用于生成嵌入的OpenAI服务是一个在线服务，而我们需要为超过30,000篇文章生成嵌入，因此我们将调用它超过30,000次。在如此多的调用中，偶尔由于网络连接问题或服务过于繁忙而导致某些调用失败并不罕见。因此，添加`retry`功能可以帮助你避免在调用第29,000次时出现错误，从而破坏你的程序。
- en: Important – Using the OpenAI services is not free
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示——使用OpenAI服务不是免费的
- en: To generate embeddings, we must call the OpenAI API. These calls require a paid
    subscription, and each call will incur a cost. The costs are usually small per
    request —version 3 of the embedding models costs $0.02 per million tokens at the
    time of writing this book, but costs can add up.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成嵌入，我们必须调用OpenAI API。这些调用需要付费订阅，并且每次调用都会产生费用。通常，每个请求的费用很小——在撰写本书时，嵌入模型版本3每百万个标记的费用为0.02美元，但费用可能会累积。
- en: OpenAI pricing details can be found at [https://openai.com/pricing](https://openai.com/pricing).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI定价详情可在[https://openai.com/pricing](https://openai.com/pricing)找到。
- en: Azure OpenAI pricing details can be found at [https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI定价详情可在[https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)找到。
- en: 'The process we’ll follow to create the index search client for loading documents
    is very similar to what we did when creating the index. The `SearchClient` class
    has one more parameter than `SearchIndexClient`, which we used to create the index:
    the `index_name` property that we created before:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循的过程来创建索引搜索客户端以加载文档，与我们创建索引时所做的非常相似。`SearchClient`类比我们用于创建索引的`SearchIndexClient`多一个参数：我们之前创建的`index_name`属性：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s load the data:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载数据：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we read the data file into a `pandas` DataFrame, and for each record,
    we create a dictionary called `document`. Note that we must replace periods with
    underscores in the `Id` field because Azure AI Search requires key fields to only
    contain numbers, letters, dashes, and underscores.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将数据文件读入一个`pandas` DataFrame，并为每条记录创建一个名为`document`的字典。请注意，我们必须在`Id`字段中将句点替换为下划线，因为Azure
    AI Search要求键字段只能包含数字、字母、破折号和下划线。
- en: 'Now that we have the data in the dictionary, we are ready to upload it, which
    we will do in the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据放入字典中，我们准备上传它，这将在以下代码中完成：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The fields in the document dictionary match the fields that we used when we
    created the index: `Id`, `Text`, `Description`, and `Embedding`. The value for
    the `Embedding` field is generated by calling the `generate_embeddings` function
    we created earlier.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 文档字典中的字段与我们创建索引时使用的字段相匹配：`Id`、`Text`、`Description`和`Embedding`。`Embedding`字段的值是通过调用我们之前创建的`generate_embeddings`函数生成的。
- en: Also, note the additional field, `@search.action`. This field contains instructions
    on what’s going to happen with that item when it’s submitted to the index. `upload`
    is a good default as it creates the record with that ID if it doesn’t exist and
    updates its contents in the index if it does.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意额外的字段`@search.action`。该字段包含有关将该项提交到索引时将发生什么的说明。"upload"是一个好的默认选项，因为它在不存在该ID时创建记录，如果存在则更新索引中的内容。
- en: Lastly, once we’ve created the `document` dictionary item, we append it to the
    `documents` list.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦我们创建了`document`字典项，我们就将其追加到`documents`列表中。
- en: 'Now, we are ready to upload it to the index:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备将其上传到索引：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When uploading data to the index, there’s a limit of 16 MB per operation. Therefore,
    we can only upload a few records at a time. In the preceding code, I limited the
    number of records uploaded to `100`. However, any small enough number works since
    we are going to insert records into the index just once. The upload operation
    doesn’t take very long, and it’s better to upload a few records at a time and
    have a slightly longer upload duration than trying to upload many records at a
    time and risk getting an error.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在向索引上传数据时，每个操作的限制为16 MB。因此，我们一次只能上传少量记录。在先前的代码中，我将上传的记录数限制为`100`。然而，任何足够小的数字都适用，因为我们只将记录插入索引一次。上传操作不会花费很长时间，一次上传少量记录并稍微延长上传时间比一次尝试上传大量记录并冒着出错的风险更好。
- en: 'The final step is to call the `main` function:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是调用`main`函数：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that before calling the `main` function, we called `load_dotenv` to get
    the values of the environment variables that contain the index name, the service
    name, the admin key, and the OpenAI key.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在调用`main`函数之前，我们调用了`load_dotenv`来获取包含索引名称、服务名称、管理员密钥和OpenAI密钥的环境变量的值。
- en: Running this program will cost approximately $1.50 as it will generate the embeddings.
    It will take about two and a half hours to run since we are generating dozens
    of thousands of embeddings. If you want to reduce the cost or time for your experiment,
    you can simply load just a fraction of the documents.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此程序将花费大约1.50美元，因为它将生成嵌入。由于我们将生成数十万个嵌入，所以运行时间大约为两小时半。如果你想减少实验的成本或时间，你可以简单地只加载文档的一部分。
- en: 'Once the program finishes running, you will see the following printed message:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦程序运行完成，你将看到以下打印消息：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, we can use the index to find articles. Later, we will use it to answer
    questions about AI papers. But before we do that, let’s learn how to upload the
    documents to the index using C#.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用索引来查找文章。稍后，我们将用它来回答关于AI论文的问题。但在我们这样做之前，让我们学习如何使用C#将文档上传到索引。
- en: Uploading documents with C#
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用C#上传文档
- en: 'The `Microsoft.SemanticKernel.Connectors.AzureAISearch` package, which is in
    preview at the time of writing, makes it a lot easier to upload documents with
    C#. To use it, we must install it:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时处于预览状态的`Microsoft.SemanticKernel.Connectors.AzureAISearch`包，使得使用C#上传文档变得容易得多。为了使用它，我们必须安装它：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Also, add the OpenAI connectors package:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要添加OpenAI连接器包：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we are going to use these packages to load the following documents into
    the index:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用这些包将以下文档加载到索引中：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Since the package is in prerelease form, we need to add a few `pragma` directives
    to let C# know that we know that we are using prerelease functionality:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该包处于预发布形式，我们需要添加几个`pragma`指令来让C#知道我们正在使用预发布功能：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'At this point, we can get our environment variables. I’ve modified the `Settings.cs`
    file to allow for the additional Azure AI Search variables to be stored and read
    from `config/settings.json`. For brevity, I won’t put the file here, but you can
    check out this chapter’s GitHub repository to see it:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以获取我们的环境变量。我已经修改了`Settings.cs`文件，以便将额外的Azure AI Search变量存储和读取自`config/settings.json`。为了简洁，我不会在这里放置文件，但你可以在本章节的GitHub仓库中查看它：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we must create a `Memory` object with `MemoryBuilder`. We will use the
    `AzureAISearchMemoryStore` class to connect to Azure AI Search:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须使用`MemoryBuilder`创建一个`Memory`对象。我们将使用`AzureAISearchMemoryStore`类连接到Azure
    AI Search：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next step is to read the data from the `ai_arxiv.json` file. Despite its
    extension, it’s not a JSON file; it’s a text file with one JSON object per line,
    so we will parse each line one at a time:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从`ai_arxiv.json`文件中读取数据。尽管它的扩展名是.json，但它不是一个JSON文件；它是一个每行一个JSON对象的文本文件，因此我们将逐行解析：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next step is to use the `SaveInformationAsync` method of the `MemoryStore`
    object to upload the document into the index:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用`MemoryStore`对象的`SaveInformationAsync`方法将文档上传到索引：
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that we’ve loaded the documents into the index, we can learn how to use
    the index to run a simple search. Later, we will use the results of the search
    and Semantic Kernel to assemble an answer.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将文档加载到索引中，我们可以学习如何使用索引来运行简单的搜索。稍后，我们将使用搜索结果和语义内核来组装答案。
- en: Using the index to find academic articles
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用索引查找学术论文
- en: This subsection assumes that the index was loaded in the previous step. The
    index now contains titles, abstracts, and embeddings for thousands of academic
    papers about LLMs from ArXiV. Note that the papers in ArXiV are not necessarily
    peer-reviewed, which means that some articles may contain incorrect information.
    Regardless, ArXiV is generally a reputable data source for academic articles about
    AI, and many classic papers can be found there, including *Attention is All You
    Need* [3], the academic article that introduced GPT to the world. That article
    is not part of our dataset because our dataset starts in 2021, and that article
    is from 2017.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节假设在上一步骤中已加载索引。现在，索引包含来自ArXiV的关于LLMs的数千篇学术论文的标题、摘要和嵌入。请注意，ArXiV中的论文不一定经过同行评审，这意味着一些文章可能包含错误信息。无论如何，ArXiV通常是关于AI学术论文的可靠数据源，许多经典论文都可以在那里找到，包括*Attention
    is All You Need* [3]，这篇论文向世界介绍了GPT。该文章不是我们数据集的一部分，因为我们的数据集始于2021年，而该文章是2017年的。
- en: We’re going to use this index to help us find papers for a given search string.
    This will ensure that the search is working and that we’re comfortable with the
    results. In the next subsection, we’re going to use GPT to combine the search
    results and summarize the findings. Let’s see how to do this in Python and C#.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用此索引来帮助我们找到给定搜索字符串的论文。这将确保搜索正在工作，并且我们对结果感到满意。在下一小节中，我们将使用GPT来组合搜索结果并总结发现。让我们看看如何在Python和C#中实现这一点。
- en: Searching for articles in Python
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Python中搜索文章
- en: 'The first thing we must do is load the required libraries:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须做的第一件事是加载所需的库：
- en: '[PRE26]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: There are no new libraries, and we’re using the same `generate_embeddings` function
    as we did before. The function that’s used to generate embeddings when searching
    must be compatible with the function that was used to store embeddings in the
    vector database. If you use the same model, the function is going to be compatible.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 没有新的库，我们使用与之前相同的`generate_embeddings`函数。用于在搜索时生成嵌入的函数必须与用于在向量数据库中存储嵌入的函数兼容。如果您使用相同的模型，该函数将是兼容的。
- en: 'In the following code, we’re creating a kernel and loading the embedding model
    into it:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们创建一个内核并将嵌入模型加载到其中：
- en: '[PRE27]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Besides loading the kernel with the embeddings model, we’ve also loaded all
    the environment variables with the configuration of our OpenAI connection and
    our Azure AI Search connection.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 除了加载内核中的嵌入模型外，我们还加载了所有环境变量，包括我们的OpenAI连接和Azure AI Search连接的配置。
- en: 'Now, we can execute the query:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以执行查询：
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Executing this query consists of a few steps. First, we calculate the embeddings
    from our query string. This is done with the `generate_embeddings` function. Then,
    we create `VectorizedQuery` with the embeddings before executing the query using
    the `search` method of `SearchClient`. The `k_nearest_neighbors` parameter of
    our query determines how many results we want to bring back. In this case, I’m
    bringing back the first `5`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此查询包括几个步骤。首先，我们使用`generate_embeddings`函数从查询字符串中计算嵌入。然后，在执行查询之前，我们使用`SearchClient`的`search`方法创建`VectorizedQuery`，并使用嵌入。我们的查询的`k_nearest_neighbors`参数决定了我们希望返回多少个结果。在这种情况下，我正在返回前`5`个。
- en: 'The results come in a dictionary with the columns in the index. We’ll also
    retrieve an additional special column called `@search.score` that’s created dynamically
    during the search and shows the cosine similarity of each result:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 结果以字典形式返回，包含索引中的列。我们还将检索一个额外的特殊列`@search.score`，该列在搜索过程中动态创建，显示每个结果的余弦相似度：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The values of the `@search.score` field may be used to sort results by order
    of similarity, and also to drop results that are below a cut-off point.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`@search.score`字段的值按相似度顺序排序结果，也可以用于丢弃低于截止点的结果。
- en: 'Let’s print the results:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印结果：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In the preceding code, I’m loading the results into a `pandas` DataFrame before
    printing them as this makes it easier to sort and filter results when you have
    too many. This isn’t required, though – you can simply use a dictionary. In this
    case, we’re limiting our results to only five, so we could also print them directly
    from the `pd_results` dictionary list we created. For example, let’s say we have
    the following query:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我在打印之前将结果加载到`pandas` DataFrame中，这使得在结果太多时排序和过滤结果变得更容易。这不是必需的——您也可以简单地使用字典。在这种情况下，我们限制结果只包含五个，因此我们也可以直接从我们创建的`pd_results`字典列表中打印它们。例如，假设我们有以下查询：
- en: '[PRE31]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We’ll look at the results after we’ve learned how to implement the search in
    C#.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在学习如何在C#中实现搜索之后查看结果。
- en: Searching for articles with C#
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用C#搜索文章
- en: 'We can create our `memoryWithCustomDb` object in the same way we did to load
    the documents. Up to that point, the code is the same. However, instead of loading
    documents, we will now search for them. We can do that with the `SearchAsync`
    method of the `memoryWithCustomDb` object. All we need to do is pass the name
    of our index, which is stored in the `searchIndexName` variable from the configuration,
    the query we want to make, which we specified in `query_string`, and the number
    of articles we want to retrieve, which we specified in `limit`. We set `minRelevanceScore`
    to `0.0` so that we always retrieve the top five results. You can set it to a
    higher number if you only want to return results that exceed a minimum cosine
    similarity:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像加载文档一样创建我们的`memoryWithCustomDb`对象。到目前为止，代码是相同的。然而，我们现在将搜索文档而不是加载它们。我们可以通过`memoryWithCustomDb`对象的`SearchAsync`方法来完成。我们只需要传递我们的索引名称，该名称存储在配置中的`searchIndexName`变量中，我们想要进行的查询，我们在`query_string`中指定了它，以及我们想要检索的文章数量，我们在`limit`中指定了它。我们将`minRelevanceScore`设置为`0.0`，这样我们总是检索前五个结果。如果您只想返回超过最小余弦相似度的结果，可以将其设置为更高的数字：
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With the dedicated `memoryWithCustomDb` C# object, querying the memory is very
    simple, and we can get our results with a single `SearchAsync` call.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用专门的`memoryWithCustomDb` C#对象，查询内存非常简单，我们只需通过一个`SearchAsync`调用即可获取我们的结果。
- en: Let’s check out the results.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看结果。
- en: Search results
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 搜索结果
- en: 'For both Python and C#, the results we get are as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Python和C#，我们得到的结果如下：
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now that we have seen that the search works in C# and Python, we can use RAG
    to automatically generate a summary of several papers based on a search we’ll
    make.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到搜索在C#和Python中工作，我们可以使用RAG根据我们将要进行的搜索自动生成几篇论文的摘要。
- en: Using RAG to create a summary of several articles on a topic
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用RAG创建关于一个主题的几篇文章的摘要
- en: We’re going to use the search results from the previous step and add them to
    a prompt by using the usual semantic function prompt template. The prompt will
    instruct a model – in our case, GPT-4 – to summarize the papers our search returned.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一步的搜索结果，并通过使用通常的语义函数提示模板将它们添加到提示中。提示将指示一个模型——在我们的情况下，是GPT-4——总结我们搜索返回的论文。
- en: 'Let’s start with the semantic function, which we will call `summarize_abstracts`.
    Here’s its metaprompt:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从语义函数开始，我们将称之为`summarize_abstracts`。这是它的元提示：
- en: skprompt.txt
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: skprompt.txt
- en: '[PRE34]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The key part of the prompt is that when we ask for the summarization, I ask
    GPT to refer to the number of the abstract. For that to work, we will generate
    a list that has a number and an abstract, which is very similar to the results
    we generated in the *Using the index to find academic articles* section. The difference
    is that instead of having a number and the article title, we will have a number
    and the article abstract. Let’s see the configuration file:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 提示的关键部分是，当我们要求摘要时，我要求GPT参考摘要的编号。为了使其工作，我们将生成一个包含编号和摘要的列表，这与我们在*使用索引查找学术论文*部分生成的结果非常相似。不同之处在于，我们不会有一个编号和文章标题，而是一个编号和文章摘要。让我们看看配置文件：
- en: config.json
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: config.json
- en: '[PRE35]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here, the most important thing you must do is make sure that you have enough
    tokens in the `max_tokens` field. You’re going to be sending five abstracts, which
    might easily get to 200 tokens per abstract, so you need at least 1,000 tokens
    just for the abstracts and more for the instructions and the response.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您必须确保的最重要的事情是确保`max_tokens`字段中有足够的标记。您将发送五篇摘要，每篇摘要可能很容易达到200个标记，所以您至少需要1,000个标记来处理摘要，还需要更多来处理指令和响应。
- en: Retrieving the data with Python and calling the semantic function
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python检索数据并调用语义函数
- en: 'The first thing we need to do is add a generative model to our kernel. I’ve
    modified the `create_kernel` function so that it adds a `gpt-4-turbo` model:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是向我们的内核添加一个生成模型。我已经修改了`create_kernel`函数，以便添加`gpt-4-turbo`模型：
- en: '[PRE36]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You can use any model you want, but I decided on gpt-4-turbo because it provides
    a good balance between cost and performance.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用任何您想要的模型，但我决定使用gpt-4-turbo，因为它在成本和性能之间提供了良好的平衡。
- en: 'The next step is to create a function to summarize documents:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个用于总结文档的函数：
- en: '[PRE37]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The first part of the function creates a string that specifies a numbered list
    of documents and their URLs. Because of the way Azure AI Search stores IDs, remember
    that we had to convert dots into underscores. To generate the proper URL, we must
    convert it back.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的第一部分创建一个字符串，指定编号的文档及其 URL。由于 Azure AI Search 存储 ID 的方式，请记住我们不得不将点转换为下划线。为了生成正确的
    URL，我们必须将其转换回来。
- en: 'The second part of the function generates a list of abstracts with the same
    numbers as the papers. When we write the prompt, we can ask the model to refer
    to the numbers, which, in turn, refer to the articles’ titles and URLs:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的第二部分生成一个与论文具有相同数字的摘要列表。当我们编写提示时，我们可以要求模型引用这些数字，这些数字反过来又指向文章的标题和 URL：
- en: '[PRE38]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The next step is to load the semantic function from its configuration directory:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从其配置目录中加载语义函数：
- en: '[PRE39]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The final step is to combine the list of papers and URLs with the generated
    summary and return it:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将论文和 URL 列表与生成的摘要结合起来，并返回它：
- en: '[PRE40]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now that we know how to do the retrieval with Python, let’s see how to do it
    with C#. We’ll look at the results after.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何使用 Python 进行检索，让我们看看如何使用 C# 进行检索。我们将在之后查看结果。
- en: Retrieving the data with C# and calling the semantic function
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 C# 获取数据并调用语义函数
- en: 'To retrieve the data, we’ll start with the same code we used in the *Using
    the index to find academic articles* section until we fill the `memories` variable:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取数据，我们将从与“使用索引查找学术论文”部分相同的代码开始，直到我们填充`memories`变量：
- en: '[PRE41]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'I’ve started the response by listing the documents that were retrieved, their
    numbers, and their URLs, all of which were built from the `Id` field. There’s
    no need to use an AI model for this step:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经通过列出检索到的文档、它们的编号和 URL（所有这些都由`Id`字段构建）开始了响应，这一步不需要使用 AI 模型：
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, instead of creating a string with all the titles, as we did in the *Using
    the index to find academic articles* section, we are going to create a string
    named `input` with the five abstracts, identified by a number in the `i` variable.
    This will be used as the input parameter for our semantic function:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们不再像在“使用索引查找学术论文”部分那样创建包含所有标题的字符串，而是将五个摘要（由`i`变量中的数字标识）创建一个名为`input`的字符串。这将被用作语义函数的输入参数：
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, we can create a Semantic Kernel named `kernel`, add an AI service to it,
    and load the semantic function defined in the previous subsection, which I decided
    to call `rag`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个名为`kernel`的语义内核，向其中添加一个 AI 服务，并加载在上一小节中定义的语义函数，我决定将其称为`rag`：
- en: '[PRE44]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Let’s run our programs and see the results we get when we use the same `query_string`
    value that we used for the test in the previous subsection.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行我们的程序，看看当我们使用与上一小节测试中相同的`query_string`值时，我们得到的结果。
- en: RAG results
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RAG 结果
- en: The query that we will use here is `"models with long context windows lose information
    in` `the middle"`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将使用的查询是：“`具有长上下文窗口的模型在中间会丢失信息`”。
- en: 'The results aren’t deterministic, but you should get something similar to the
    following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 结果不是确定的，但您应该得到以下类似的结果：
- en: '[PRE45]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As you can see, the summary is comprehensive, captures the main idea of each
    paper, and shows how the papers relate to one another.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，总结是全面的，捕捉了每篇论文的主要思想，并展示了论文之间的相互关系。
- en: In this section, we learned how to use an external database to help an LLM work
    with a lot more information than the model can handle with its context window.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用外部数据库帮助 LLM 处理比模型上下文窗口能处理的更多信息。
- en: One advantage of this method is that the generative model primarily uses the
    search data that you supplied to it to generate a response. If you only supply
    it with real, well-curated data, you will substantially lower the chance that
    it will *hallucinate* – that is, generate information that doesn’t exist. If you
    did not use RAG, there’s a possibility that the generative model will make up
    non-existent papers and references just to try to answer the questions and generate
    the summaries we’re asking for.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是生成模型主要使用您提供给它的搜索数据来生成响应。如果您只提供真实、精心整理的数据，您将大大降低它产生*幻觉*的可能性——也就是说，生成不存在的信息。如果您没有使用
    RAG，那么生成模型可能会编造不存在的论文和参考文献，只是为了尝试回答我们提出的问题和生成我们要求的摘要。
- en: Blocking hallucinations completely is theoretically impossible, but using RAG
    can make the chance of hallucinating so low that, in practice, your users may
    never see fake data being generated by your model. This is the reason RAG models
    are used extensively in production applications.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 完全阻止幻觉在理论上是不可行的，但使用RAG可以使幻觉发生的概率极低，以至于在实践中，用户可能永远不会看到模型生成的虚假数据。这就是为什么RAG模型在生产应用中被广泛使用的原因。
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we greatly expanded the data that’s available to our AI models
    by using the RAG methodology. Besides allowing AI models to use large amounts
    of data when building prompts, the RAG methodology also improves the accuracy
    of the model: since the prompt contains a lot of the data that’s required to generate
    the answer, models tend to hallucinate less.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过使用RAG方法大大扩展了我们AI模型可用的数据。除了允许AI模型在构建提示时使用大量数据外，RAG方法还提高了模型的准确性：由于提示包含大量生成答案所需的数据，模型倾向于产生更少的幻觉。
- en: RAG also allows AI to provide references to the material it used to generate
    a response. Many real-world use cases require models to manipulate large quantities
    of data, require references to be provided, and are sensitive to hallucinations.
    RAG can help overcome these issues easily.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: RAG还允许AI提供其用于生成响应的材料的引用。许多现实世界的用例需要模型处理大量数据，需要提供引用，并且对幻觉敏感。RAG可以帮助轻松克服这些问题。
- en: In the next chapter, we will change gears and learn how to integrate a Semantic
    Kernel application with ChatGPT, making it available to hundreds of millions of
    users. In our example, we will use the application we built in [*Chapter 5*](B21826_05.xhtml#_idTextAnchor106)
    for home automation, but you can use the same techniques to do that with your
    own applications.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转换方向，学习如何将语义内核应用程序与ChatGPT集成，使其可供数亿用户使用。在我们的示例中，我们将使用我们在[*第五章*](B21826_05.xhtml#_idTextAnchor106)中构建的应用程序进行家庭自动化，但你也可以使用相同的技巧在你的应用程序中实现这一点。
- en: References
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N. F. Liu et al., “Lost in the Middle: How Language Models Use Long Contexts.”
    arXiv, Nov. 20, 2023\. doi: 10.48550/arXiv.2307.03172.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] N. F. Liu 等人，“迷失在中间：语言模型如何使用长上下文。” arXiv，2023年11月20日。doi: 10.48550/arXiv.2307.03172。'
- en: '[2] L. Berglund et al., “The Reversal Curse: LLMs trained on ‘A is B’ fail
    to learn ‘B is A.’” arXiv, Sep. 22, 2023\. doi: 10.48550/arXiv.2309.12288.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] L. Berglund 等人，“逆转诅咒：在‘A是B’上训练的LLM无法学习‘B是A。’” arXiv，2023年9月22日。doi: 10.48550/arXiv.2309.12288。'
- en: '[3] A. Vaswani et al., “Attention Is All You Need,” Jun. 2017.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] A. Vaswani 等人，“Attention Is All You Need，”2017年6月。'
