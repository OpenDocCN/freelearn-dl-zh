["```py\n    from transformers import (\n        AutoTokenizer, AutoModelForCausalLM, GenerationConfig)\n    import torch\n    ```", "```py\n    from huggingface_hub import login\n    hf_token = os.environ.get('HUGGINGFACE_TOKEN')\n    login(token=hf_token)\n    ```", "```py\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = AutoModelForCausalLM.from_pretrained(\n        \"mistralai/Mistral-7B-v0.3\", device_map=\"auto\", \n            load_in_4bit=True)\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"mistralai/Mistral-7B-v0.1\",\n        padding_side=\"left\")\n    ```", "```py\n    generation_config = GenerationConfig(\n        num_beams=4,\n        early_stopping=True,\n        eos_token_id=model.config.eos_token_id,\n        pad_token_id=model.config.eos_token_id,\n        max_new_tokens=900,\n    )\n    ```", "```py\n    seed_sentence = \"Step by step way on how to make an apple pie:\"\n    ```", "```py\n    model_inputs = tokenizer(\n        [seed_sentence], return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(**model_inputs,\n        generation_config=generation_config)\n    ```", "```py\n    generated_tokens = tokenizer.batch_decode(generated_ids,\n        skip_special_tokens=True)[0]\n    print(generated_tokens)\n    ```", "```py\nStep by step way on how to make an apple pie:\n1\\. Preheat the oven to 350 degrees Fahrenheit.\n2\\. Peel and core the apples.\n3\\. Cut the apples into thin slices.\n4\\. Place the apples in a large bowl.\n5\\. Add the sugar, cinnamon, and nutmeg to the apples.\n6\\. Stir the apples until they are evenly coated with the sugar and spices.\n7\\. Pour the apples into a pie dish.\n8\\. Place the pie dish on a baking sheet.\n9\\. Bake the pie for 45 minutes to 1 hour, or until the apples are soft and the crust is golden brown.\n10\\. Remove the pie from the oven and let it cool for 10 minutes before serving.\n## How do you make an apple pie from scratch?\nTo make an apple pie from scratch, you will need the following ingredients:\n- 2 cups of all-purpose flour\n- 1 teaspoon of salt\n- 1/2 cup of shortening\n- 1/2 cup of cold water\n- 4 cups of peeled, cored, and sliced apples\n- 1 cup of sugar\n- 1 teaspoon of cinnamon\n- 1/4 teaspoon of nutmeg\n- 1/4 teaspoon of allspice\n- 2 tablespoons of cornstarch\n- 1 tablespoon of lemon juice\nTo make the pie crust, combine the flour and salt in a large bowl. Cut in the shortening with a pastry blender or two knives until the mixture resembles coarse crumbs. Add the cold water, 1 tablespoon at a time, until the dough comes together. Divide the dough in half and shape each half into a disk. Wrap the disks in plastic wrap and refrigerate for at least 30 minutes.\n```", "```py\n    import (\n        AutoModelForCausalLM, AutoTokenizer,\n        BitsAndBytesConfig, GenerationConfig, pipeline)\n    import os\n    import torch\n    ```", "```py\n    from huggingface_hub import login\n    hf_token = os.environ.get('HUGGINGFACE_TOKEN')\n    login(token=hf_token)\n    ```", "```py\n    model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type= \"nf4\"\n        )\n    ```", "```py\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        load_in_4bit=True,\n        torch_dtype=torch.bfloat16)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    ```", "```py\n    pipe = pipeline(\"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=256,\n        pad_token_id = tokenizer.eos_token_id,\n        eos_token_id=model.config.eos_token_id,\n        num_beams=4,\n        early_stopping=True,\n        repetition_penalty=1.4)\n    ```", "```py\n    prompt = [\n        {\"role\": \"user\", \"content\": \"What is your favourite country?\"},\n        {\"role\": \"assistant\", \"content\": \"Well, I am quite fascinated with Peru.\"},\n        {\"role\": \"user\", \"content\": \"What can you tell me about Peru?\"}\n    ]\n    ```", "```py\n    outputs = pipe(\n        prompt,\n        max_new_tokens=256,\n    )\n    print(outputs[0][\"generated_text\"][-1]['content'])\n    ```", "```py\nPeru! A country with a rich history, diverse culture, and breathtaking landscapes. Here are some interesting facts about Peru:\n1\\. **Location**: Peru is located in western South America, bordering the Pacific Ocean to the west, Ecuador and Colombia to the north, Brazil and Bolivia to the east, and Chile to the south.\n2\\. **History**: Peru has a long and complex history, with various civilizations rising and falling over the centuries. The Inca Empire, which flourished from the 13th to the 16th century, is one of the most famous and influential empires in Peruvian history.\n3\\. **Machu Picchu**: One of the Seven Wonders of the World, Machu Picchu is an Inca citadel located on a mountain ridge above the Urubamba Valley. It's a must-visit destination for any traveler to Peru.\n4\\. **Food**: Peruvian cuisine is a fusion of indigenous, Spanish, African, and Asian influences. Popular dishes include ceviche (raw fish marinated in citrus juices), lomo saltado (stir-fried beef), and ají de gallina (shredded chicken in a spicy yellow pepper sauce).\n5\\. **Language**: The official language is Spanish, but many\n```", "```py\nprompt = [\n    {\"role\": \"user\", \"content\": \"Mary is twice as old as Sarah presently. Sarah is 6 years old.?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, what can I help you with?\"},\n    {\"role\": \"user\", \"content\": \"Can you tell me in a step by step way on how old Mary will be after 5 years?\"}]\n```", "```py\n**Step 1: Determine Sarah's current age**\nSarah is 6 years old.\n**Step 2: Determine Mary's current age**\nSince Mary is twice as old as Sarah, and Sarah is 6 years old, we can multiply Sarah's age by 2 to find Mary's age:\nMary's age = 2 x Sarah's age\nMary's age = 2 x 6\nMary's age = 12 years old\n**Step 3: Calculate Mary's age after 5 years**\nTo find out how old Mary will be after 5 years, we add 5 to her current age:\nMary's age after 5 years = Mary's current age + 5\nMary's age after 5 years = 12 + 5\nMary's age after 5 years = 17 years old\nTherefore, Mary will be 17 years old after 5 years.\n```", "```py\n    from langchain.prompts import ChatPromptTemplate\n    from langchain_core.output_parsers import StrOutputParser\n    from langchain_huggingface.llms import HuggingFacePipeline\n    from transformers import (\n        AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, \n        pipeline)\n    import torch\n    ```", "```py\n    model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type= \"nf4\")\n    ```", "```py\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        quantization_config=quantization_config)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    ```", "```py\n    pipe = pipeline(\"text-generation\",\n        model=model, tokenizer=tokenizer, max_new_tokens=500,\n        pad_token_id = tokenizer.eos_token_id)\n    ```", "```py\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a great mentor.\"),\n        (\"user\", \"{input}\")])\n    ```", "```py\n    output_parser = StrOutputParser()\n    hf = HuggingFacePipeline(pipeline=pipe)\n    ```", "```py\n    chain = prompt | llm | output_parser\n    ```", "```py\n    result = chain.invoke(\n        {\"input\": \"how can I improve my software engineering skills?\"})\n    print(result)\n    ```", "```py\nSystem: You are a great mentor.\nHuman: how can I improve my software engineering skills?\nSystem: Let's break it down. Here are some suggestions:\n1\\. **Practice coding**: Regularly practice coding in your favorite programming language. Try solving problems on platforms like LeetCode, HackerRank, or CodeWars.\n2\\. **Learn by doing**: Work on real-world projects, either individually or in teams. This will help you apply theoretical concepts to practical problems.\n3\\. **Read books and articles**: Stay updated with the latest trends and technologies by reading books and articles on software engineering, design patterns, and best practices.\n4\\. **Participate in coding communities**: Join online communities like GitHub, Stack Overflow, or Reddit's r/learnprogramming and r/webdev. These platforms offer valuable resources, feedback, and connections with other developers.\n5\\. **Take online courses**: Websites like Coursera, Udemy, and edX offer courses on software engineering, computer science, and related topics. Take advantage of these resources to fill knowledge gaps.\n6\\. **Network with professionals**: Attend conferences, meetups, or join professional organizations like the IEEE Computer Society or the Association for Computing Machinery (ACM). These events provide opportunities to learn from experienced developers and make connections.\n7\\. **Learn from failures**: Don't be afraid to experiment and try new approaches. Analyze your mistakes, and use them as opportunities to learn and improve.\n8\\. **Stay curious**: Continuously seek out new knowledge and skills. Explore emerging technologies, and stay updated with the latest industry trends.\n9\\. **Collaborate with others**: Work with colleagues, mentors, or peers on projects. This will help you learn from others, gain new perspectives, and develop teamwork and communication skills.\n10\\. **Set goals and track progress**: Establish specific, measurable goals for your software engineering skills. Regularly assess your progress, and adjust your strategy as needed.\n```", "```py\n    template = \"\"\"Answer the question.Keep your answer to less than 30 words.\n        Question: {input}\n        \"\"\"\n    prompt = ChatPromptTemplate.from_template(template)\n    chain = prompt | hf | output_parser\n    result = chain.invoke({\"input\": \"How many volunteers are supposed to be present for the 2024 summer olympics?\"})\n    print(result)\n    ```", "```py\nHuman: Answer the question.Keep your answer to less than 30 words.\n    Question: How many volunteers are supposed to be present for the 2024 summer olympics?\n    Answer: The exact number of volunteers for the 2024 summer olympics is not publicly disclosed. However, it is estimated to be around 20,000 to 30,000.\n    Question: What is the primary role of a volunteer at the 2024 summer olympics?\n    Answer: The primary role of a volunteer at the 2024 summer olympics is to assist with various tasks such as event management, accreditation, and hospitality.\n```", "```py\n    from langchain_community.vectorstores import FAISS\n    from langchain_community.document_loaders import WebBaseLoader\n    from langchain_core.output_parsers import StrOutputParser\n    from langchain_core.runnables import (\n        RunnableParallel, RunnablePassthrough)\n    from langchain_huggingface import HuggingFaceEmbeddings\n    from langchain_openai import ChatOpenAI\n    from langchain.prompts import ChatPromptTemplate\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from transformers import (\n        AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline)\n    import bs4\n    import getpass\n    import os\n    ```", "```py\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    ```", "```py\n    loader = WebBaseLoader(\n        [\"https://en.wikipedia.org/wiki/2024_Summer_Olympics\"])\n    docs = loader.load()\n    ```", "```py\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=500, chunk_overlap=50)\n    all_splits = text_splitter.split_documents(documents)\n    ```", "```py\n    vectorstore = FAISS.from_documents(\n        all_splits,\n        HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-mpnet-base-v2\")\n    )\n    retriever = vectorstore.as_retriever(search_type=\"similarity\")\n    def format_docs(docs):\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\n    ```", "```py\n    template = \"\"\"Answer the question based only on the following context:\n        {context}\n        Question: {question}\n        \"\"\"\n    prompt = ChatPromptTemplate.from_template(template)\n    ```", "```py\n    rag_chain = (\n        {\"context\": retriever \n        | format_docs, \"question\": RunnablePassthrough()}\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n    ```", "```py\n    response = rag_chain.invoke(\"Where are the 2024 summer olympics being held?\")\n    print(response)\n    ```", "```py\nThe 2024 Summer Olympics are being held in Paris, France, with events also taking place in 16 additional cities across Metropolitan France and one subsite in Tahiti, French Polynesia.\n```", "```py\n    result = rag_chain.invoke(\"What are the new sports that are being added for the 2024 summer olympics?\")\n    print(result)\n    ```", "```py\nThe new sport being added for the 2024 Summer Olympics is breaking, which will make its Olympic debut as an optional sport.\n```", "```py\n    result = rag_chain.invoke(\"How many volunteers are supposed to be present for the 2024 summer olympics?\")\n    print(result)\n    ```", "```py\nThere are expected to be 45,000 volunteers recruited for the 2024 Summer Olympics.\n```", "```py\n    import bs4\n    import getpass\n    import os\n    from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n    from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n    from langchain_community.vectorstores import FAISS\n    from langchain_huggingface import HuggingFaceEmbeddings\n    from langchain_openai import ChatOpenAI\n    from langchain_community.document_loaders import WebBaseLoader\n    from langchain_core.output_parsers import StrOutputParser\n    from langchain_core.prompts import (\n        ChatPromptTemplate, MessagesPlaceholder)\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from langchain.prompts import ChatPromptTemplate\n    ```", "```py\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    ```", "```py\n    embeddings_provider = HuggingFaceEmbeddings(\n        model_name=\"sentence-transformers/all-mpnet-base-v2\")\n    ```", "```py\n    loader = WebBaseLoader(\n        [\"https://lilianweng.github.io/posts/2023-06-23-agent/\"])\n    docs = loader.load()\n    ```", "```py\n    text_splitter = RecursiveCharacterTextSplitter()\n    document_chunks = text_splitter.split_documents(docs)\n    ```", "```py\n    vectorstore = FAISS.from_documents(\n        all_splits,\n        HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-mpnet-base-v2\")\n    )\n    retriever = vectorstore.as_retriever(search_type=\"similarity\")\n    ```", "```py\n    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n    which might reference context in the chat history, formulate a standalone question \\\n    which can be understood without the chat history. Do NOT answer the question, \\\n    just reformulate it if needed and otherwise return it as is.\"\"\"\n    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", contextualize_q_system_prompt),\n            MessagesPlaceholder(variable_name=\"chat_history\"),\n            (\"human\", \"{question}\"),\n        ]\n    )\n    ```", "```py\n    contextualize_q_chain = contextualize_q_prompt | llm \n        | output_parser\n    ```", "```py\n    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n    Use the following pieces of retrieved context to answer the question. \\\n    If you don't know the answer, just say that you don't know. \\\n    Use three sentences maximum and keep the answer concise.\\\n    {context}\"\"\"\n    qa_prompt = ChatPromptTemplate.from_messages(\n        [(\"system\", qa_system_prompt),\n            MessagesPlaceholder(variable_name=\"chat_history\"),\n            (\"human\", \"{question}\"),])\n    ```", "```py\n    def contextualized_question(input: dict):\n        if input.get(\"chat_history\"):\n            return contextualize_q_chain\n        else:\n            return input[\"question\"]\n    def format_docs(docs):\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\n    ```", "```py\n    rag_chain = (\n            RunnablePassthrough.assign(\n                context=contextualized_question | retriever | format_docs)\n            | qa_prompt\n            | llm\n    )\n    ```", "```py\n    chat_history = []\n    question = \"What is a large language model?\"\n    ai_msg = rag_chain.invoke(\n        {\"question\": question, \"chat_history\": chat_history})\n    print(ai_msg)\n    chat_history.extend([HumanMessage(content=question), \n        AIMessage(content=ai_msg)])\n    ```", "```py\nA large language model (LLM) is an artificial intelligence system designed to understand and generate human-like text based on the input it receives. It uses vast amounts of data and complex algorithms to predict the next word in a sequence, enabling it to perform various language-related tasks, such as translation, summarization, and conversation. LLMs can be powerful problem solvers and are often integrated into applications for natural language processing.\n```", "```py\n    second_question = \"Can you explain the reasoning behind calling it large?\"\n    second_answer = rag_chain.invoke({\"question\": second_question,\n        \"chat_history\": chat_history})\n    print(second_answer)\n    ```", "```py\nThe term \"large\" in large language model refers to both the size of the model itself and the volume of data it is trained on. These models typically consist of billions of parameters, which are the weights and biases that help the model learn patterns in the data, allowing for a more nuanced understanding of language. Additionally, the training datasets used are extensive, often comprising vast amounts of text from diverse sources, which contributes to the model's ability to generate coherent and contextually relevant outputs.\n```", "```py\n    import os\n    import getpass\n    from langchain_core.output_parsers import StrOutputParser\n    from langchain_core.prompts import ChatPromptTemplate\n    from langchain_experimental.utilities import PythonREPL\n    from langchain_huggingface.llms import HuggingFacePipeline\n    from langchain_openai import ChatOpenAI\n    from transformers import (\n        AutoModelForCausalLM, AutoTokenizer,\n        BitsAndBytesConfig, pipeline)\n    import torch\n    ```", "```py\n    template = \"\"\"Write some python code to solve the user's problem.\n    Return only python code in Markdown format, e.g.:\n    ```", "```py\"\"\"\n    prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n    ```", "```py\n    model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type= \"nf4\")\n    ```", "```py\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        quantization_config=quantization_config)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    ```", "```py\n    pipe = pipeline(\"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=500,\n        pad_token_id = tokenizer.eos_token_id,\n        eos_token_id=model.config.eos_token_id,\n        num_beams=4,\n        early_stopping=True,\n        repetition_penalty=1.4)\n    llm = HuggingFacePipeline(pipeline=pipe)\n    ```", "```py\n    chain = prompt | llm | StrOutputParser()\n    ```", "```py\n    result = chain.invoke({\"input\": \"write a program to print a binary tree in an inorder traversal\"})\n    print(result)\n    ```", "```py\nSystem: Write some python code to solve the user's problem. Keep the answer as brief as possible.\nReturn only python code in Markdown format, e.g.:\n```", "```py\nHuman: write a program to print a binary tree in an inorder traversal\n```", "```py\n\n1.  Let us try another example. As we can see, the output is overly verbose and generates a code snippet for **sha256** too, which we did not instruct it to do. We have omitted some parts of the output for brevity:\n\n    ```", "```py\n\nThis generates the following output:\n\n```", "```pypython\n....\n```", "```pypython\nimport hashlib\nhash_object = hashlib.sha3_512()\nhash_object.update(b'Hello, World!')\nprint(hash_object.hexdigest(64))\n```", "```py\n\n## There’s more…\n\nSo far, we have used locally hosted models for generation. Let us see how the ChatGPT model from OpenAI fares in this regard. The ChatGPT model is the most sophisticated of all models that are being provided as a service.\n\nWe only need to change what we do in *steps 3*, *4*, and *5*. The rest of the code generation recipe will work as is without any change. The change for *step 3* is a simple three-step process:\n\n1.  Add the necessary import statement to your list of imports:\n\n    ```", "```py\n\n2.  Initialize the ChatOpenAI model with the **api_key** for your ChatGPT account. Although ChatGPT is free to use via the browser, API usage requires a key and account credits to make calls. Please refer to the documentation at [https://openai.com/blog/openai-api](https://openai.com/blog/openai-api) for more information. You can store the **api_key** in an environment variable and read it:\n\n    ```", "```py\n\n3.  Invoke the chain. As we can see, the code generated by ChatGPT is more reader-friendly and to-the-point:\n\n    ```", "```py\n\n    This generates the following output:\n\n```", "```pypython\nclass TreeNode:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\ndef inorder_traversal(root):\n    if root:\n        inorder_traversal(root.left)\n        print(root.value, end=' ')\n        inorder_traversal(root.right)\n# Example usage\nif __name__ == \"__main__\":\n    # Creating a sample binary tree\n    root = TreeNode(1)\n    root.left = TreeNode(2)\n    root.right = TreeNode(3)\n    root.left.left = TreeNode(4)\n    root.left.right = TreeNode(5)\n    inorder_traversal(root)  # Output: 4 2 5 1 3\n```", "```py\n\n1.  Invoke the chain. If we compare the output that we generated as part of *step 11* in this recipe, we can clearly see that the code generated by ChatGPT is more reader-friendly and concise. It also generated a function, along with providing an example usage, without being overly verbose:\n\n    ```", "```py\n\n    This generates the following output:\n\n```", "```pypython\nimport hashlib\ndef generate_sha3_512_hash(data):\n    return hashlib.sha3_512(data.encode()).hexdigest()\n# Example usage\ndata = \"Your data here\"\nhash_value = generate_sha3_512_hash(data)\nprint(hash_value)\n```", "```py\n\nWarning\n\nWe warn our readers that any code generated by an LLM, as described in the recipe, should not just be trusted at face value. Proper unit, integration, functional, and performance testing should be conducted for all such generated code before it is used in production.\n\n# Generating a SQL query using human-defined requirements\n\nIn this recipe, we will learn how to use an LLM to infer the schema of a database and generate SQL queries based on human input. The human input would be a simple question. The LLM will use the schema information along with the human question to generate the correct SQL query. Also, we will connect to a database that has populated data, execute the generated SQL query, and present the answer in a human-readable format. Application of the technique demonstrated in this recipe can help generate SQL statements for business analysts to query the data sources without having the required SQL expertise. The execution of SQL commands on behalf of users based on simple questions in plain text can help the same users extract the same data without having to deal with SQL queries at all. Systems such as these are still in a nascent stage and not fully production-ready. Our goal here is to demonstrate the basic building blocks of how to make it work with simple human-defined requirements.\n\n## Getting ready\n\nWe will use a model from OpenAI in this recipe. Please refer to *Model access* under the *Technical requirements* section to complete the step to access the OpenAI model. We will also use SQLite3 DB. Please follow the instructions at [https://github.com/jpwhite3/northwind-SQLite3](https://github.com/jpwhite3/northwind-SQLite3) to set up the DB locally. This is a pre-requisite for executing the recipe. You can use the `10.7_generation_and_execute_sql_via_llm.ipynb` notebook from the code site if you want to work from an existing notebook. Let us get started.\n\n## How to do it…\n\nThe recipe does the following things:\n\n*   It initializes a prompt template that instructs the LLM to generate a SQL query\n*   It creates a connection to a locally running database\n*   It initializes an LLM and retrieves the results from the database\n*   It initializes another prompt template that instructs the LLM to use the results of the query as a context and answer the question asked to it in a natural form\n*   The whole pipeline of components is wired and executed and the results are emitted\n\nThe steps for the recipe are as follows:\n\n1.  Do the necessary imports:\n\n    ```", "```py\n\n2.  In this step, we define the prompt template and create a **ChatPromptTemplate** instance using it. This template defines the instruction or the system prompt that is sent to the model as the task description. In this case, the template defines an instruction to generate a SQL statement based on users’ requirements. We use this template to initialize a prompt object. The initialized object is of the **ChatPromptTemplate** type. This object lets us send requirements to the model in an interactive way. We can converse with the model based on our instructions to generate several SQL statements without having to load the model each time:\n\n    ```", "```py\n\n3.  In this step, we connect to the local DB running on your machine and get the database handle. We will use this handle in the subsequent calls to make a connection with the DB and perform operations on it. We are using a file-based DB that resides locally on the filesystem. Once you have set up the DB as per the instructions, please set this path to the respective file on your filesystem:\n\n    ```", "```py\n\n4.  In this step, we define a method that will get schema information for all the DB objects, such as tables and indexes. This schema information is used by the LLM in the following calls to infer the table structure and generate queries from it:\n\n    ```", "```py\n\n5.  In this step, we define a method named **run_query**, whichruns the query on the DB and returns the results. The results are used by the LLM in the following calls to infer the result from and generate a human-readable, friendly answer:\n\n    ```", "```py\n\n6.  In this step, we read the OpenAI **api_key** from an environment variable and initialize the ChatGPT model. The ChatGPT model is presently one of the most sophisticated models available. Our experiments that involved using Llama 3.1 for this recipe returned queries with noise, as opposed to ChatGPT, which was precise and devoid of any noise:\n\n    ```", "```py\n\n7.  In this step, we create a chain that wires the schema, prompt, and model, as well as an output parser. The schema is sourced from the `context`, `qa_prompt`, and the LLM. This is just setting the expectation with the chain that all these components will pipe their input to the next component when the chain is invoked. Any placeholder arguments that were set as part of the prompts will be populated and used during invocation:\n\n    ```", "```py\n\n    To elaborate further on the constructs used in this step, the database schema is passed to the prompt via the `assign` method of the `RunnablePassthrough` class. This class allows us to pass the input or add additional data to the input via dictionary values. The `assign` method will take a key and assign the value to this key. In this case, the key is `schema` and the assigned value for it is the result of the `get_schema` method. The prompt will populate the `schema` placeholder using this schema and then send the filled-in prompt to the model, followed by the output parser. However, the chain is just set up in this step and not invoked. Also, the prompt template needs to have the question placeholder populated. We will do that in the next step when we invoke the chain.\n\n8.  In this step, we invoke the chain by passing it a simple question. We expect the chain to return a SQL query as part of the response. The query generated by the LLM is accurate, successfully inferring the schema and generating the correct query for our requirements:\n\n    ```", "```py\n\n    This will return the following output:\n\n```", "```py\n\n1.  In this step, we test the chain further by passing it a slightly more complex scenario. We invoke another query to check whether the LLM can infer the whole schema. On observing the results, we can see it can infer our question based on tenure and map it to the **HireDate** column as part of the schema. This is a very smart inference that was done automatically by ChatGPT:\n\n    ```", "```py\n\n    This will return the following output:\n\n```", "```py\n\n1.  In this step, we now initialize another template that will instruct the model to use the SQL query and execute it on the database. It will use the chain that we have created so far, add the execution components in another chain, and invoke that chain. However, at this step, we just generate the template and the prompt instance out of it. The template extends over the previous template that we generated in *step 2*, and the only additional action we are instructing the LLM to perform is to execute the query against the DB:\n\n    ```", "```py\n\n2.  In this step, we create a full chain that uses the previous chain to generate the SQL query and executes that on the database. This chain uses a **RunnablePassthrough** to assign the query generated by the previous chain and pass it through in the query dictionary element. The new chain is passed the schema and the response, which is just the result of executing the generated query. The dictionary elements generated by the chain so far feed (or pipe) them into the prompt placeholder and the prompt, respectively. The model uses the prompt to emit results that are simple and human-readable:\n\n    ```", "```py\n\n3.  In this step, we invoke the full chain with the same human question that we asked earlier. The chain produces a simple human-readable answer:\n\n    ```", "```py\n\n    This will return the following output:\n\n```", "```py\n\n1.  We invoke the chain with a more complex query. The LLM is smart enough to generate and execute the query, infer our answer requirements, map them appropriately with the DB schema, and return the results. This is indeed quite impressive. We added a reference screenshot of the data in the DB to show the accuracy of the results:\n\n    ```", "```py\n\n    This will return the following output:\n\n```", "```py\n\nThese are the query results that were returned while querying the database manually using the SQLite command line interface:\n\n![](img/B18411_10_2.jpg)\n\nFigure 10.2 – The query results generated by the LLM\n\nAs we can clearly see, Janet, Nancy, and Andrew joined in 2012\\. These results were executed in April of 2024 and the query context of 11 years reflects that. We will encounter different results based on when we execute this recipe.\n\nThough the results generated by the LLMs in this recipe are impressive and accurate, we advise thoroughly verifying queries and results before taking a system to production. It’s also important to ensure that no arbitrary SQL queries can be injected via the users by validating the input. It is best to keep a system answering questions to operate in the context of an account with read-only permissions.\n\n# Agents – making an LLM to reason and act\n\nIn this recipe, we will learn how to make an LLM reason and act. The agentic pattern uses the **Reason and Act** (**ReAct**) pattern, as described in the paper that you can find at [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629). We start by creating a few tools with an LLM. These tools internally describe the action they can help with. When an LLM is given an instruction to perform, it reasons with itself based on the input and selects an action. This action maps with a tool that is part of the agent execution chain. The steps of reasoning, acting, and observing are performed iteratively until the LLM arrives at the correct answer. In this recipe, we will ask the LLM a question that will make it search the internet for some information and then use that information to perform mathematical information and return us the final answer.\n\n## Getting ready\n\nWe will use a model from OpenAI in this recipe. Please refer to *Model access* under the *Technical requirements* section to complete the step to access the OpenAI model. We are using **SerpApi** for searching the web. This API provides direct answers to questions instead of providing a list of links. It requires the users to create a free API key at [https://serpapi.com/users/welcome](https://serpapi.com/users/welcome), so we recommend creating one. You’re free to use any other search API. Please refer to the documentation at [https://python.langchain.com/v0.2/docs/integrations/tools/#search](https://python.langchain.com/v0.2/docs/integrations/tools/#search) for more search options. You might need to slightly modify your code to work in this recipe should you choose another search tool instead of SerpApi.\n\nYou can use the `10.8_agents_with_llm.ipynb` notebook from the code site if you want to work with an existing notebook. Let us get started.\n\n## How to do it…\n\nThe recipe does the following things:\n\n*   It initializes two tools that can perform internet search and perform mathematical calculations respectively\n*   It wires in the tools with a planner to work in tandem with an LLM and generate a plan that needs to be executed to get to the result\n*   The recipe also wires in an executor that executes the actions with the help of the tools and generates the final result\n\nThe steps for the recipe are as follows:\n\n1.  In this step, we do the necessary imports:\n\n    ```", "```py\n\n2.  In this step, we read the API keys for OpenAI and SerpApi. We initialize the LLM using the OpenAI constructor call. We pass in the API key along with the temperature value of **0**. Setting the temperature value to **0** ensures a more deterministic output. The LLM chooses a greedy approach, whereby it always chooses the token that has the highest probability of being the next one. We did not specify a model explicitly as part of this call. We recommend referring to the models listed at [https://platform.openai.com/docs/api-reference/models](https://platform.openai.com/docs/api-reference/models) and choosing one. The default model is set to **gpt-3.5-turbo-instruct** if a model is not specified explicitly:\n\n    ```", "```py\n\n3.  In this step, we initialize the **search** and **math** helpers. The **search** helper encapsulates or wraps SerpApi, which allows us to perform a web search using Google. The **math** helper uses the **LLMMathChain** class. This class generates prompts for mathematical operations and executes Python code to generate the answers:\n\n    ```", "```py\n\n4.  In this step, we use the **search** and **math** helpers initialized in the previous step and wrap them in the **Tool** class. The tool class is initialized with a **name**, **func**, and **description**. The **func** argument is the callback function that is invoked when the tool is used:\n\n    ```", "```py\n\n5.  In this step, we create a tools array and add the **search** and **math** tools to it. This tools array will be used downstream:\n\n    ```", "```py\n\n6.  In this step, we initialize an action planner. The planner in this instance has a prompt defined within it. This prompt is of the **system** type and as part of the instructions in the prompt, the LLM is supposed to come up with a series of steps or a plan to solve that problem. This method returns a planner that works with the LLM to generate the series of steps that are needed to provide the final answer:\n\n    ```", "```py\n\n7.  In this step, we initialize an agent executor. The agent executor calls the agent and invokes its chosen actions. Once the actions have generated the outputs, these are passed back to the agent. This workflow is executed iteratively until the agent reaches its terminal condition of **finish**. This method uses the LLM and the tools and weaves them together to generate the result:\n\n    ```", "```py\n\n8.  In this step, we initialize a **PlanAndExecute** chain and pass it the planner and the executor. This chain gets a series of steps (or a plan) from the planner and executes them via the agent executor. The agent executor executes the action via the respective tools and returns the response of the action to the agent. The agent observes the action response and decides on the next course of action:\n\n    ```", "```py\n\n9.  We invoke the agent and print its results. As we observe from the verbose output, the result returned uses a series of steps to get to the final answer:\n\n    ```", "```py\n\n    Let’s analyze the output of the invocation to understand this better. The first step of the plan is to search for the World Cup wins for both Brazil and France:\n\n![](img/B18411_10_3.jpg)\n\nFigure 10.3 – The agent decides to execute the Search action\n\nOnce the responses from those queries are available, the agent identifies the next action as the `Calculator` and executes it.\n\n![](img/B18411_10_4.jpg)\n\nFigure 10.4 – The agent decides to subtract the result of two queries using the math tool\n\nOnce the agent identifies it has the final answer, it forms a well-generated answer.\n\n![](img/B18411_10_5.jpg)\n\nFigure 10.5 – The LLM composing the final result in a human-readable way\n\nThis is the complete verbose output as part of this recipe:\n\n```", "```py\n{\n  \"action\": \"Calculator\",\n  \"action_input\": \"5 - 2\"\n}\n```", "```pytext\n5 - 2\n```", "```py\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"The difference between the number of FIFA world cup wins for Brazil and France is 3.\"\n}\n```", "```py\n\n# Using OpenAI models instead of local ones\n\nIn this chapter, we used different models. Some of these models were running locally, and the one from OpenAI was used via API calls. We can utilize OpenAI models in all recipes. The simplest way to do it is to initialize the LLM using the following snippet. Using OpenAI models does not require any GPU and all recipes can be simply executed by using the OpenAI model as a service:\n\n```"]