<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer265">
			<h1 id="_idParaDest-371"><em class="italic"><a id="_idTextAnchor344"/>Chapter 15</em>: Voice Communication with a Robot Using Mycroft</h1>
			<p>Using our voice to ask a robot to do something and receiving a voice response has been seen as a sign of intelligence for a long time. Devices around us, such as those using Alexa and Google Assistant, have these tools. Being able to program our system to integrate with these tools gives us access to a powerful voice assistant system. Mycroft is a Python-based open source voice system. We will get this running on the Raspberry Pi by connecting it to a speaker and microphone, and then we will run instructions on our robot based on the words we speak.</p>
			<p>In this chapter, we will have an overview of Mycroft and then learn how to add a speaker/microphone board to a Raspberry Pi. We will then install and configure a Raspberry Pi to run Mycroft.</p>
			<p>We'll also extend our use of Flask programming, building a Flask API with more control points.</p>
			<p>Toward the end of the chapter, we will create our own skills code to connect a voice assistant to our robot. You will be able to take this knowledge and use it to create further voice agent skills.</p>
			<p>The following topics are covered in this chapter:</p>
			<ul>
				<li>Introducing Mycroft – understanding voice agent terminology</li>
				<li>Limitations of listening for speech on a robot</li>
				<li>How to add a speaker/microphone board to a Raspberry Pi</li>
				<li>How to install and configure a Raspberry Pi to run Mycroft</li>
				<li>Programming a Flask control API</li>
				<li>How to create our own skills code to connect a voice assistant to our robot</li>
			</ul>
			<h1 id="_idParaDest-372"><a id="_idTextAnchor345"/>Technical requirements</h1>
			<p>You will require the following hardware for this chapter:</p>
			<ul>
				<li>An additional Raspberry Pi 4 (model B).</li>
				<li>An SD card (at least 8 GB).</li>
				<li>A PC that can write the card (with the balenaEtcher software).</li>
				<li>The ReSpeaker 2-Mics Pi HAT.</li>
				<li>Mini Audio Magnet Raspberry Pi Speaker—a tiny speaker with a JST connector or a speaker with a 3.5 mm jack.</li>
				<li>It may be helpful to have a Micro-HDMI to HDMI cable for troubleshooting.</li>
				<li>Micro USB power supply.</li>
				<li>The robot from the previous chapters (after all, we intend to get this moving).</li>
			</ul>
			<p>The code for this chapter is available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter15">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter15</a>.</p>
			<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/2N5bXqr">https://bit.ly/2N5bXqr</a></p>
			<h1 id="_idParaDest-373"><a id="_idTextAnchor346"/>Introducing Mycroft – understanding voice agent terminology</h1>
			<p><strong class="bold">Mycroft</strong> is a software<a id="_idIndexMarker930"/> suite known as a <strong class="bold">voice assistant</strong>. Mycroft<a id="_idIndexMarker931"/> listens <a id="_idIndexMarker932"/>for voice commands and takes actions based on those commands. Mycroft code is written in Python and is open source and free. It performs most of its voice processing in the cloud. After the commands are processed, Mycroft will use a voice to respond to the human.</p>
			<p>Mycroft is documented online and has a community of users. There are alternatives that you could consider after you've experimented with Mycroft – for example, Jasper, Melissa-AI, and Google Assistant.</p>
			<p>So, what are the <a id="_idIndexMarker933"/>concepts of a voice assistant? Let's look at them in the following subsections.</p>
			<h2 id="_idParaDest-374"><a id="_idTextAnchor347"/>Speech to text</h2>
			<p><strong class="bold">Speech to text</strong> (<strong class="bold">STT</strong>) describes<a id="_idIndexMarker934"/> systems that<a id="_idIndexMarker935"/> take audio containing human speech and turn it into a series of words that a computer can then process.</p>
			<p>These can run locally, or they can run in the cloud on far more powerful machines.</p>
			<h2 id="_idParaDest-375"><a id="_idTextAnchor348"/>Wake words</h2>
			<p>Voice assistants<a id="_idIndexMarker936"/> usually have a <strong class="bold">wake word</strong> – a phrase or word<a id="_idIndexMarker937"/> that is spoken before the rest of a command to get the attention of the voice assistant. Examples are the <em class="italic">Hey Siri</em>, <em class="italic">Hi Google</em>, and <em class="italic">Alexa</em> utterances. Mycroft uses the word <em class="italic">Mycroft</em> or the phrase <em class="italic">Hey Mycroft,</em> but that can be changed.</p>
			<p>A voice assistant is usually only listening for wake words and ignores all other audio input until woken. The wake word is recognized locally on the device. The sounds it samples after the wake word are sent to a speech-to-text system for recognition.</p>
			<h2 id="_idParaDest-376"><a id="_idTextAnchor349"/>Utterances</h2>
			<p>An <strong class="bold">utterance</strong> is a term for <a id="_idIndexMarker938"/>something a user says. Voice assistants <a id="_idIndexMarker939"/>use vocabulary you define to match an utterance to a skill. The specific vocabulary will cause Mycroft to invoke the intent handler.</p>
			<p>The vocabulary in Mycroft comprises lists of interchangeable phrases in a file.</p>
			<p>A good example of an utterance is asking Mycroft about the weather: <em class="italic">Hey Mycroft, what is the weather?</em></p>
			<h2 id="_idParaDest-377"><a id="_idTextAnchor350"/>Intent</h2>
			<p>An <strong class="bold">intent</strong> is a task that the<a id="_idIndexMarker940"/> voice assistant can do, such as finding<a id="_idIndexMarker941"/> out what today's weather is. We will build intents to interact with our robot. An intent is part of a skill, defining the handler code for what it does and choosing a dialog to respond.</p>
			<p>Using the weather skill as an example, the utterance <em class="italic">What is the weather?</em> triggers an intent to fetch the<a id="_idIndexMarker942"/> current weather for the configured location and then speak the details of this back to the user. An example for our robot is <em class="italic">ask the robot to test LEDs</em>, with an intent that starts the LED rainbow behavior (from <a href="B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Programming RGB Strips in Python</em>) on the robot.</p>
			<h2 id="_idParaDest-378"><a id="_idTextAnchor351"/>Dialog</h2>
			<p>In Mycroft<a id="_idIndexMarker943"/> terminology, <strong class="bold">dialogs</strong> are phrases that Mycroft <a id="_idIndexMarker944"/>speaks to the user. An example would be <em class="italic">OK, the robot has been started</em>, or <em class="italic">Today, the weather is clear</em>.</p>
			<p>A skill has a collection of dialogs. These have sets of synonymous words to say and can use different languages.</p>
			<h2 id="_idParaDest-379"><a id="_idTextAnchor352"/>Vocabulary</h2>
			<p>Utterances you<a id="_idIndexMarker945"/> speak, once<a id="_idIndexMarker946"/> converted into text, are matched to <strong class="bold">vocabulary</strong>. Vocabulary files, like dialogs, are parts of an intent, matching utterances to action. The vocabulary files contain synonymous phrases and can be organized into language sets to make your skill multi-lingual.</p>
			<p>This would make phrases like <em class="italic">what is the weather?</em>, <em class="italic">is it sunny?</em>, <em class="italic">do I need an umbrella?</em> or <em class="italic">will it rain?</em> synonymous. You may have things split – for example, <em class="italic">ask the robot to</em> as one vocabulary item and <em class="italic">drive forward</em> as another.</p>
			<h2 id="_idParaDest-380"><a id="_idTextAnchor353"/>Skills</h2>
			<p><strong class="bold">Skills</strong> are <a id="_idIndexMarker947"/>containers<a id="_idIndexMarker948"/> for a whole set of vocabulary for utterances, dialogs to speak, and <em class="italic">intents</em>. A skill for alarms might contain intents such as setting an alarm, listing the alarms, deleting an alarm, or changing an alarm. It would contain a dialog to say the alarm setting is complete or to confirm each alarm.</p>
			<p>Later in this chapter, we will build a <strong class="source-inline">MyRobot</strong> skill with intents to make it move and stop.</p>
			<p>Now you've learned a bit about the terminology and parts of a voice agent. We next need to consider <a id="_idIndexMarker949"/>what we will build. Where would we put a speaker<a id="_idIndexMarker950"/> and microphone?</p>
			<h1 id="_idParaDest-381"><a id="_idTextAnchor354"/>Limitations of listening for speech on a robot</h1>
			<p>Before we start to build this, we should consider what we are going to make. Should the speaker and <a id="_idIndexMarker951"/>microphone be on the robot or somewhere else? Will the processing be local or in the cloud?</p>
			<p>Here are some considerations to keep in mind:</p>
			<ul>
				<li><strong class="bold">Noise</strong>: A robot with motors is a noisy environment. Having a microphone anywhere near the motors will make it close to useless.</li>
				<li><strong class="bold">Power</strong>: The voice assistant is continuously listening. The robot has many demands for power already with the other sensors that are running on it. This power demand applies both in terms of battery power and the CPU power needed.</li>
				<li><strong class="bold">Size and physical location</strong>: The speaker and voice HAT would add height and wiring complications to an already busy robot.</li>
			</ul>
			<p>A microphone and speaker combination could be on a stalk for a large robot – a tall standoff with a second Raspberry Pi there. But this is unsuitable for this small and simple robot. We will create a separate voice assistant board that will communicate with our robot, but we won't be putting it directly on the robot. The voice assistant will be a second Raspberry Pi.</p>
			<p>We will also be using a system that goes to the cloud to process the speech. While a fully local system would have better privacy and could respond quicker, at the time of writing, there is not a complete packaged voice assistant that works this way for a Raspberry Pi. The Mycroft software gives us flexibility in using our own skills and has a pluggable backend for voice processing, so that one day it may run locally.</p>
			<p>Now we've chosen how we will make our voice agent with Mycroft and a second Raspberry Pi, it's <a id="_idIndexMarker952"/>time to start building it.</p>
			<h1 id="_idParaDest-382"><a id="_idTextAnchor355"/>Adding sound input and output to the Raspberry Pi</h1>
			<p>Before we can use a <a id="_idIndexMarker953"/>voice processing/voice assistant, we<a id="_idIndexMarker954"/> need to give the Raspberry Pi some speakers and a microphone. A few Raspberry Pi add-ons provide this. My recommendation, with a microphone array (for better recognition) and a connection to speakers, is the ReSpeaker 2-Mics Pi HAT, which is widely available.</p>
			<p>The next photograph shows the ReSpeaker 2-Mics Pi HAT:</p>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<img src="Images/B15660_15_01.jpg" alt="" width="974" height="633"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.1 – The ReSpeaker 2-Mics Pi HAT</p>
			<p><em class="italic">Figure 15.1</em> shows a photo of a ReSpeaker 2-Mics Pi HAT mounted on a Raspberry Pi. On the left, I've labeled the left microphone. The hat has two microphones, which are two tiny rectangular metal parts on each side. The next label is for 3 RGB LEDs and a button connected to a GPIO pin. After this are the two ways of connecting speakers – a 3.5mm jack or a JST connector. I recommend you connect a speaker to hear output from this HAT. Then, the last label highlights the right microphone.</p>
			<p>I've chosen the ReSpeaker 2-Mic Pi HAT because it is an inexpensive device to get started on voice recognition. Very cheap USB microphones will not work well for this. There are expensive devices better supported in Mycroft, but they do not sit on the Pi as a hat. This ReSpeaker 2-Mics Pi HAT is a trade-off – great for hardware simplicity and cost but with some <a id="_idIndexMarker955"/>more software setup. Let's now look at how we <a id="_idIndexMarker956"/>physically install this HAT.</p>
			<h2 id="_idParaDest-383"><a id="_idTextAnchor356"/>Physical installation</h2>
			<p>The ReSpeaker 2-Mics HAT will sit directly on the Raspberry Pi 4 headers with the board overhanging<a id="_idIndexMarker957"/> the Pi.</p>
			<p>The speakers will have either a tiny two-pin connector (JST) type that fits the single two-pin socket on the board or a 3.5 mm jack. The next photograph shows the speaker plugged into it:</p>
			<p class="figure-caption"><img src="Images/B15660_15_02.png" alt="" width="473" height="376"/></p>
			<p class="figure-caption">Figure 15.2 – The Mycroft Voice Assistant ReSpeaker setup</p>
			<p><em class="italic">Figure 15.2</em> shows my Mycroft setup with the ReSpeaker 2-Mics Pi HAT set up on my desk. It is powered up, and the Raspberry Pi is lit. I've connected a speaker to it as well.</p>
			<p>You could use a Raspberry Pi case or project box but ensure that the microphones are not covered up.</p>
			<p>You also need an SD card and a power supply. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For the next few sections, I recommend using a mains power supply. Do not plug it in and power it up yet.</p>
			<p>Now we have the hardware prepared, and it has speakers and a microphone. In the next section, we will set <a id="_idIndexMarker958"/>up Raspbian and the voice agent software.</p>
			<h2 id="_idParaDest-384"><a id="_idTextAnchor357"/>Installing a voice agent on a Raspberry Pi</h2>
			<p>Mycroft <a id="_idIndexMarker959"/>has a Raspbian distribution prepared for <a id="_idIndexMarker960"/>this. Let's put that on an SD card:</p>
			<ol>
				<li>Go to the Mycroft website to download the <em class="italic">Picroft</em> image: <a href="https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/get-mycroft/picroft">https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/get-mycroft/picroft</a> – this is based on Raspbian Buster. Select <strong class="bold">stable disk image</strong>.</li>
				<li>Insert the SD card into your computer. Use the procedures from <a href="B15660_03_Final_ASB_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring the Raspberry Pi</em>, in the <em class="italic">Flashing the card in balenaEtcher</em> section. Be sure to select the Picroft image instead of Raspbian.</li>
				<li>Make sure this image works headlessly, enabling SSH and Wi-Fi as we did in <a href="B15660_04_Final_ASB_ePub.xhtml#_idTextAnchor063"><em class="italic">Chapter 4</em></a>, <em class="italic">Preparing a Headless Raspberry Pi for a Robot</em>, in the <em class="italic">Setting up wireless on the Raspberry Pi and enabling SSH</em> section.</li>
			</ol>
			<p>With this SD card ready, it's time to try it out. Insert it into the voice assistant Raspberry Pi and power it up using the USB micro socket on the ReSpeaker 2-Mics Pi HAT (not the Pi).</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Ensure you supply power via the ReSpeaker 2-Mics Pi HAT and not the Pi. This board requires power to drive its speaker. The documentation for the board suggests that if you power it through the Pi you don't get output from the speaker. See <a href="https://wiki.seeedstudio.com/ReSpeaker_2_Mics_Pi_HAT/#hardware-overview">https://wiki.seeedstudio.com/ReSpeaker_2_Mics_Pi_HAT/#hardware-overview</a> for details.</p>
			<p>Its hostname starts as <strong class="source-inline">picroft.local</strong>. You use the username <strong class="source-inline">pi</strong> and password <strong class="source-inline">mycroft</strong>. Ensure it is connected to Wi-Fi, and you can reach it via SSH (PuTTY). With the Raspberry Pi <a id="_idIndexMarker961"/>started, you can start to set up <a id="_idIndexMarker962"/>Mycroft.</p>
			<h2 id="_idParaDest-385"><a id="_idTextAnchor358"/>Installing the ReSpeaker software</h2>
			<p>When you<a id="_idIndexMarker963"/> log on, Mycroft will show you an<a id="_idIndexMarker964"/> installation guide. This will ask you some questions as listed:</p>
			<ol>
				<li value="1">When asked if you want a guided setup, press <em class="italic">Y</em> for yes. The Mycroft installation will download a load of updates. Leave it for 30 minutes to an hour to do so.</li>
				<li>Mycroft will now ask for your audio output device:<p class="source-code"><strong class="bold">HARDWARE SETUP</strong></p><p class="source-code"><strong class="bold">How do you want Mycroft to output audio:</strong></p><p class="source-code"><strong class="bold">  1) Speakers via 3.5mm output (aka 'audio jack' or 'headphone jack')</strong></p><p class="source-code"><strong class="bold">  2) HDMI audio (e.g. a TV or monitor with built-in speakers)</strong></p><p class="source-code"><strong class="bold">  3) USB audio (e.g. a USB soundcard or USB mic/speaker combo)</strong></p><p class="source-code"><strong class="bold">  4) Google AIY Voice HAT and microphone board (Voice Kit v1)</strong></p><p class="source-code"><strong class="bold">  5) ReSpeaker Mic Array v2.0 (speaker plugged in to Mic board)</strong></p><p class="source-code"><strong class="bold">Choice [1-5]:</strong></p><p>This guided setup doesn't directly support the ReSpeaker 2-Mics Pi HAT we are using. Type <strong class="source-inline">3</strong>, to select USB speakers, which sets some basic defaults.</p></li>
				<li>Press <em class="italic">Ctrl</em> + <em class="italic">C</em> to leave the guided setup and return to the <strong class="source-inline">$</strong> prompt.</li>
				<li>For the installation to work, we'll need the software on the SD card to be updated. At the prompt, type <strong class="source-inline">sudo apt update -y &amp;&amp; sudo apt upgrade -y</strong>. The update will take some time. </li>
				<li>Reboot the Pi (with <strong class="source-inline">sudo reboot</strong>) for the updates to take effect. After you reboot the<a id="_idIndexMarker965"/> Pi, <strong class="source-inline">ssh</strong> in. You will be at the guided setup again. Press <em class="italic">Ctrl</em> + <em class="italic">C</em> again.</li>
				<li>Use the following commands to install the audio drivers for the ReSpeaker 2-Mics Pi HAT:<p class="source-code">$ git clone https://github.com/waveshare/WM8960-Audio-HAT.git</p><p class="source-code">$ cd WM8960-Audio-HAT</p><p class="source-code">$ sudo ./install.sh</p><p>The Git clone may take a minute or two. This board uses the WM8960 sound chip. The install script will take 20-30 minutes to finish.</p></li>
				<li>Reboot again. Press <em class="italic">Ctrl</em> + <em class="italic">C</em> after to leave the guided mode.<p>Before we move on, it's a good idea to test that we are getting audio here.</p></li>
				<li>Type <strong class="source-inline">aplay -l</strong> to list playback devices. In the output, you should see the following:<p class="source-code">card 1: wm8960soundcard [wm8960-soundcard], device 0: bcm2835-i2s-wm8960-hifi wm8960-hifi-0 [bcm2835-i2s-wm8960-hifi wm8960-hifi-0]</p><p>This shows that it has found our card.</p></li>
				<li>We can now test this card will play audio by getting it to play an audio file. Use this command: <strong class="source-inline">aplay -Dplayback /usr/share/sounds/alsa/Front_Left.wav</strong>.<p>This command specifies the device named <strong class="source-inline">playback</strong> with the device <strong class="source-inline">-D</strong> flag, and then the file to play. The <strong class="source-inline">playback</strong> device is a default ALSA handler that ensures mixing is done and avoids issues with bitrate and channel number mismatches. There are other test audio files in <strong class="source-inline">/usr/share/sounds/alsa</strong>.</p></li>
				<li>We can then<a id="_idIndexMarker966"/> check for recording devices with <strong class="source-inline">arecord -l</strong>. In the following output, we can see that <strong class="source-inline">arecord</strong> has found the card:</li>
			</ol>
			<p class="source-code">card 1: wm8960soundcard [wm8960-soundcard], device 0: bcm2835-i2s-wm8960-hifi wm8960-hifi-0 [bcm2835-i2s-wm8960-hifi wm8960-hifi-0]</p>
			<p>The card is now ready for use. Next, we need to show the Mycroft system how to choose this card for use.</p>
			<h3 id="_idParaDest-386">Troubleshooting</h3>
			<p>If you haven't got <a id="_idIndexMarker967"/>audio output, there are some things you can check:</p>
			<ol>
				<li value="1">First, type <strong class="source-inline">sudo poweroff</strong> to turn off the Raspberry Pi. When it is off, check the connections. Ensure that the board is connected fully to the GPIO header on the Pi. Make sure you've connected the speaker to the correct port on the ReSpeaker 2-Mics Pi HAT.</li>
				<li>When you power it again, ensure that you are using the power connector on the ReSpeaker 2-Mics Pi HAT, and not the Raspberry Pi.</li>
				<li>If you are using the headphone slot instead of the speaker slot, you may need to increase the volume. Type <strong class="source-inline">alsamixer</strong>, select the WM8960 sound card, and turn the headphone volume up. Then try the playback tests again.</li>
				<li>Make sure you have performed the <strong class="source-inline">apt update</strong> and the <strong class="source-inline">apt upgrade</strong> steps. The installation of the drivers will not work without it. You will need to reboot after this and then try reinstalling the driver.</li>
				<li>When installing the driver, if the Git step fails, double-check the address you have fetched.</li>
				<li>When attempting playback, the <strong class="source-inline">-D</strong> flag is case-sensitive. A lowercase <strong class="source-inline">d</strong> will not work here.</li>
			</ol>
			<p>If these steps still do not help, please go to the <a href="https://github.com/waveshare/WM8960-Audio-HAT">https://github.com/waveshare/WM8960-Audio-HAT</a> website, read their documentation, or raise an issue.</p>
			<p>Now we've checked<a id="_idIndexMarker968"/> this, let's try to link the sound card with Mycroft.</p>
			<h2 id="_idParaDest-387"><a id="_idTextAnchor359"/>Getting Mycroft to talk to the sound card</h2>
			<p>Now you need to <a id="_idIndexMarker969"/>connect Mycroft and the sound card. Do this by editing the Mycroft configuration file: </p>
			<ol>
				<li value="1">Open the Mycroft config file as root using <strong class="source-inline">sudo nano /etc/mycroft/mycroft.conf</strong>.</li>
				<li>The file has lines describing various aspects of Mycroft. However, we are interested in two lines only:<p class="source-code"><strong class="bold">   "play_wav_cmdline": "aplay -Dhw:0,0 %1",</strong></p><p class="source-code"><strong class="bold">   "play_mp3_cmdline": "mpg123 -a hw:0,0 %1",</strong></p><p>The first specifies that Mycroft will play wave audio files using the <strong class="source-inline">aplay</strong> command on device hardware <strong class="source-inline">0,0</strong> (the Pi headphone jack) – written as <strong class="source-inline">hw:0,0</strong>. This will be the wrong device. The second specifies it will play mp3 files using the <strong class="source-inline">mpg123</strong> command and on the same incorrect device. Using a direct hardware device may make assumptions about the format of the sound being played, so it needs to go through the mixer device. Let's fix these.</p></li>
				<li>Edit both occurrences of <strong class="source-inline">hw:0,0</strong> to be the term <strong class="source-inline">playback</strong>. The two lines should look like this:<p class="source-code"><strong class="bold">   "play_wav_cmdline": "aplay -Dplayback %1",</strong></p><p class="source-code"><strong class="bold">   "play_mp3_cmdline": "mpg123 -a playback %1",</strong></p></li>
				<li>Press <em class="italic">Ctrl </em>+ <em class="italic">X</em> to write out and exit. Type <em class="italic">Y</em> for yes when asked to write out the file.</li>
				<li>Reboot one more time; when you return, do not exit the guided mode.</li>
				<li>Mycroft will ask to test the device. Press <em class="italic">T</em> to test the speaker. It may take a few seconds, but you will hear Mycroft speak to you. If it is a little quiet, try typing the number <strong class="source-inline">9</strong>, and test it again. An exciting moment! Press <em class="italic">D</em> to say you have done the test.</li>
				<li>The guided installer will next ask about the microphone. Select <strong class="bold">4</strong> for <strong class="bold">Other USB Microphone</strong> and try the sound test. The installer will ask you to speak to the <a id="_idIndexMarker970"/>microphone, and it should play your voice back to you. Press <em class="italic">1</em> if this sounds good.</li>
				<li>The guided installation will ask you about using the recommendations; select <em class="italic">1</em> to confirm you want that. There will be a series of questions about your password settings. I recommend not adding a sudo password but changing the default password for the Pi to something unique. </li>
				<li>Mycroft will launch with a large section of purple installation text.</li>
			</ol>
			<p>You have Mycroft configured and starting up. It can record your voice and play that back to you, and you have heard it speak a test word too. Now, it's time to start using Mycroft and see what it can do.</p>
			<h2 id="_idParaDest-388"><a id="_idTextAnchor360"/>Starting to use Mycroft</h2>
			<p>Let's get to know <a id="_idIndexMarker971"/>Mycroft a little, and then try talking with it. We will start with the debug interface, the Mycroft client, which shows you what is going on with the system, and then we'll get into talking to it.</p>
			<h3 id="_idParaDest-389">The Mycroft client</h3>
			<p>When you connect to<a id="_idIndexMarker972"/> Mycroft, you will see a display like the following figure:</p>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="Images/B15660_15_03.jpg" alt="" width="564" height="339"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.3 – The Mycroft client interface</p>
			<p>The screenshot in <em class="italic">Figure 15.3</em> is the Mycroft client. It allows you to see what Mycroft is doing, but you don't need to connect to this for Mycroft to listen to you. The top right shows how many messages there are and how many you can see. In the screenshot, you can see messages <strong class="bold">0-10</strong>, out of a total of <strong class="bold">10</strong> messages.</p>
			<p>The main middle section shows the messages. <em class="italic">Red</em> and <em class="italic">purple</em> messages come from the Mycroft system and plugins. If many <em class="italic">purple</em> messages are flashing by, Mycroft is installing plugins and updates, so you may need to leave it until it finishes. <em class="italic">Green</em> messages show Mycroft interacting with a user. It shows when it detects a wake word, when it starts to record, when it ends the recording, and the utterance it thinks you said. The messages are useful as if it isn't quite responding, you can check whether it's picking up the wake word and that the utterance matches what you are trying to say. </p>
			<p>Below this, on the left, is the history. In the history, what Mycroft has processed from your utterance is in <em class="italic">blue</em>. The dialog Mycroft speaks is in <em class="italic">yellow</em>. You should hear <em class="italic">yellow</em> text repeated on the speaker; however, it can take a while if it is very busy. On the right, it shows a legend that matches colors to a log file. Further right is a microphone speaker level meter, and unless Mycroft is busy, or you are very quiet, you should see this moving up and down as it picks up noise in the room. Note – too much noise, and you may have trouble talking to it.</p>
			<p>At the bottom of the screen is an input area, where you can type commands for Mycroft.</p>
			<p>Give the system about 30-40 minutes to finish all the installations. If it is not responsive, it is not hung but is usually installing and compiling additional components.</p>
			<p>Mycroft will then tell you it needs to be paired at <a href="http://mycroft.ai">mycroft.ai</a>. You will need to register the device using the code it gives you; which you can do while Mycroft is installing. You will need to create an <a id="_idIndexMarker973"/>account there to do so (or log in if this is a second device/attempt). Please complete this before proceeding.</p>
			<p>When you've paired Mycroft, and it finishes installing things, you can start to interact.</p>
			<h3 id="_idParaDest-390">Talking to Mycroft</h3>
			<p>Now you should <a id="_idIndexMarker974"/>be able to speak to your voice assistant:</p>
			<ol>
				<li value="1">First, to get its attention, you must use the wake word <em class="italic">Hey Mycroft</em>. If it's ready (and not still busy), it will issue a speaker tone to show <em class="italic">Mycroft</em> is listening. You need to stand within about a meter of the microphones on the Raspberry Pi. It may respond with <em class="italic">Please wait a moment while I finish booting up</em>. Give it a minute and try again.</li>
				<li>If you hear the tone, you can now ask it to do something. A good starting point is to tell it: <em class="italic">Say hello</em>. Mycroft should respond with <em class="italic">Hello</em> from the speaker after about 10 seconds. You will need to speak as clearly as possible. I've found that it needs you to pronounce each syllable; those <em class="italic">t</em> and <em class="italic">n</em> sounds are essential.</li>
			</ol>
			<p>Now that this works, you can have some fun with it! You can shorten <em class="italic">Hey Mycroft</em> to just <em class="italic">Mycroft</em>. Other things you can say include the following:</p>
			<ul>
				<li><em class="italic">Hey Mycroft, what is the weather?</em>: This will use the weather skill and tell you the weather. It may be for the wrong location; use the <a href="http://mycroft.ai">mycroft.ai</a> website to configure your device to your location.</li>
				<li><em class="italic">Mycroft, what is 23 times 76</em>: This will use the Wolfram skill, which can handle mathematical questions.</li>
				<li><em class="italic">Mycroft, wiki banana</em>: This will use a Wikipedia skill, and Mycroft will tell you what it has found out about the banana.</li>
			</ul>
			<p>Try these out to get used to talking to Mycroft so it responds. It may say <em class="italic">I don't understand</em>, and the log will tell you what it heard, which can help you try to tune how you pronounce things for it.</p>
			<p>We can now create a<a id="_idIndexMarker975"/> skill to connect Mycroft to our robot. But first, let's check for problems.</p>
			<h2 id="_idParaDest-391"><a id="_idTextAnchor361"/>Troubleshooting</h2>
			<p>If you are not able <a id="_idIndexMarker976"/>to get Mycroft to speak or recognize talking, try the following:</p>
			<ul>
				<li>Make sure you are close enough to the microphone/loud enough. This can be checked by observing whether the mic (microphone) level goes above the dashed line in the Mycroft console.</li>
				<li>Ensure you have a good network connection from your Raspberry Pi. Mycroft is only going to work where you can reach the internet. See the Mycroft documentation for handling proxies. Mycroft can fail to boot correctly if the internet connection isn't great. Fixing the connection and rebooting it can help.</li>
				<li>Attaching a monitor while the Pi is booting may reveal error messages.</li>
				<li>Mycroft has a troubleshooting system starting with: <em class="italic">Troubleshooting and Known errors</em> (<a href="https://mycroft.ai/documentation/troubleshooting/">https://mycroft.ai/documentation/troubleshooting/</a>).</li>
				<li>Mycroft is under active development. Taking the latest Picroft image and applying the ReSpeaker driver may help. In short, getting this installed and running is subject to change.</li>
			</ul>
			<p>With Mycroft talking and responding, we need to prepare the robot for Mycroft to talk to it.</p>
			<h1 id="_idParaDest-392"><a id="_idTextAnchor362"/>Programming a Flask API</h1>
			<p>This chapter aims<a id="_idIndexMarker977"/> to control our robot with Mycroft. To do so, we need to give our robot some way to receive commands from other systems. An <strong class="bold">Application Programming Interface</strong> (<strong class="bold">API</strong>) on a<a id="_idIndexMarker978"/> server lets us decouple systems like this to send commands across the network to another and receive a response. The Flask system is ideally suited to building this. </p>
			<p>Web-based APIs have endpoints that other systems make their requests to and roughly map to functions or methods in a Python module. As you'll see, we map our API endpoints directly to functions in the Python <strong class="source-inline">robot_modes</strong> module.</p>
			<p>Before we get into building much, let's look at the design of this thing – it will also reveal how Mycroft works.</p>
			<h2 id="_idParaDest-393"><a id="_idTextAnchor363"/>Overview of Mycroft controlling the robot</h2>
			<p>The following <a id="_idIndexMarker979"/>diagram shows how a user controls a robot via Mycroft:</p>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="Images/B15660_15_04.jpg" alt="" width="1650" height="788"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.4 – Overview of the robot skill</p>
			<p>The diagram in <em class="italic">Figure 15.4</em> shows how data flows in this system:</p>
			<ol>
				<li value="1">On the left, it starts with the user speaking an instruction to Mycroft.</li>
				<li>On recognizing the wake word, Mycroft sends the sound to the Google STT engine.</li>
				<li>Google STT returns text, an utterance, which Mycroft matches against vocabulary in skills/intents. We'll dig more into these later.</li>
				<li>This triggers intents in the robot skill, which we will build. The robot skill will send a request to the Raspberry Pi in the robot, on the right, as a request to a Flask control API (web) server.</li>
				<li>That control API <a id="_idIndexMarker980"/>server will start the robot processes and respond to say it's done so.</li>
				<li>The robot skill will choose dialog to say it has completed and sends this to Mycroft.</li>
				<li>Mycroft will then speak this response to the user.</li>
			</ol>
			<p>At this point, we are going to build the Flask server on the robot. You have seen Flask before in the visual processing chapters and have already installed this library.</p>
			<h2 id="_idParaDest-394"><a id="_idTextAnchor364"/>Starting a behavior remotely</h2>
			<p>We will use HTTP and<a id="_idIndexMarker981"/> a web server for this, as it's simple to send requests to, so we can build other ways to control the robot remotely. HTTP sends requests in a URL—first, the <strong class="source-inline">http://</strong> protocol identifier; a server hostname, <strong class="source-inline">myrobot.local</strong>; a path, <strong class="source-inline">/mode/foo</strong>; and it may have additional parameters after that. We use the path of the URL to determine what our robot does.</p>
			<p>As we have done with other systems, we create a few logical sections and blocks to handle different aspects of this:</p>
			<ul>
				<li>Code to manage the robot's modes and to start and stop known scripts. It can also give us a list of those known scripts.</li>
				<li>A web server<a id="_idIndexMarker982"/> to handle requests over the network.</li>
			</ul>
			<p>We'll need to build the mode manager first.</p>
			<h3 id="_idParaDest-395">Managing robot modes</h3>
			<p>We can manage<a id="_idIndexMarker983"/> modes by starting and stopping our behavior scripts as subprocesses. Let's make a configuration to tell the mode manager about the modes. This configuration maps a mode name to a file—a Python file. Note that we are specifying a list of files and not inferring it. Although we could take our mode/path section and add <strong class="source-inline">.py</strong> to get a file, this would be bad for two reasons:</p>
			<ul>
				<li>It would couple us directly to script names; it would be nice if we could change underlying scripts for the same mode name.</li>
				<li>Although the robot is not a secure environment, allowing arbitrary subprocesses to run is very bad; restricting it keeps the robot a little more secure.</li>
			</ul>
			<p>Let's start building it:</p>
			<ol>
				<li value="1">Create a file called <strong class="source-inline">robot_modes.py</strong>. This file contains a class called <strong class="source-inline">RobotModes</strong> that handles robot processes.</li>
				<li>The file starts with some imports and the top of the class definition:<p class="source-code"><strong class="bold">import subprocess</strong></p><p class="source-code"><strong class="bold">class RobotModes(object):</strong></p></li>
				<li>Next, we create a few mode mappings, mapping a mode name to a filename:<p class="source-code"><strong class="bold">    mode_config = {</strong></p><p class="source-code"><strong class="bold">        "avoid_behavior": "avoid_with_rainbows.py",</strong></p><p class="source-code"><strong class="bold">        "circle_head": "circle_pan_tilt_behavior.py",</strong></p><p class="source-code"><strong class="bold">        "test_rainbow": "test_rainbow.py"</strong></p><p class="source-code"><strong class="bold">    }</strong></p><p>The mode name is a short name, also known as a <em class="italic">slug</em>, a compromise between human-readable and machine-readable – they are usually restricted to lowercase and underscore characters and are shorter than a full English description. Our filenames are relatively close to slug names already.</p></li>
				<li>With the fixed configuration aside, this class is also managing running behaviors as processes. It should only run one at a time. Therefore, we need a member variable to keep track of the current process and check whether it is running: <p class="source-code"><strong class="bold">    def __init__(self):</strong></p><p class="source-code"><strong class="bold">        self.current_process = None</strong></p></li>
				<li>We should be able to check whether something is already running or it has completed:<p class="source-code"><strong class="bold">    def is_running(self):</strong></p><p class="source-code"><strong class="bold">        return self.current_process and self.current_process.returncode is None</strong></p><p>Python's <strong class="source-inline">subprocess</strong> is a way <a id="_idIndexMarker984"/>of running other processes and apps from within Python. We check whether we have a current process, and if so, whether it is still running. Processes have a return code, usually to say whether they completed or failed. However, if they are still running, it will be <strong class="source-inline">None</strong>. We can use this to determine that the robot is currently running a process.</p></li>
				<li>The next function is running a process. The function parameters include a mode name. The function checks whether a process is running, and if not, starts a process:<p class="source-code"><strong class="bold">    def run(self, mode_name):</strong></p><p class="source-code"><strong class="bold">        if not self.is_running():</strong></p><p class="source-code"><strong class="bold">            script = self.mode_config[mode_name]</strong></p><p class="source-code"><strong class="bold">            self.current_process = subprocess.Popen(["python3", script])</strong></p><p class="source-code"><strong class="bold">            return True</strong></p><p class="source-code"><strong class="bold">        return False</strong></p><p class="callout-heading">Important note</p><p class="callout">Before we run a new process, we need to check that the previous behavior has stopped. Running two modes simultaneously could have quite strange consequences, so we should be careful not to let that happen.</p><p>We use <strong class="source-inline">self.mode_config</strong> to map <strong class="source-inline">mode_name</strong> to a script name. We then use <strong class="source-inline">subprocess</strong> to start this script with Python. <strong class="source-inline">Popen</strong> creates a process, and the code stores a handle for it in <strong class="source-inline">self.current_process</strong>. This method returns <strong class="source-inline">True</strong> if we started it, and <strong class="source-inline">False</strong> if one was already running.</p></li>
				<li>The class needs a way to ask it to stop a process. Note that this doesn't try to stop a process when it is not running. When we stop the scripts, we can use Unix signals, which let us ask them to stop in a way that allows their <strong class="source-inline">atexit</strong> code<a id="_idIndexMarker985"/> to run. It sends the <strong class="source-inline">SIGINT</strong> signal, which is the equivalent of the <em class="italic">Ctrl</em> + <em class="italic">C</em> keyboard combination:<p class="source-code"><strong class="bold">    def stop(self):</strong></p><p class="source-code"><strong class="bold">        if self.is_running():</strong></p><p class="source-code"><strong class="bold">            self.current_process.send_signal( subprocess.signal.SIGINT)</strong></p><p class="source-code"><strong class="bold">            self.current_process = None</strong></p></li>
			</ol>
			<p>After we have signaled the process, we set the current process to <strong class="source-inline">None</strong> – throwing away the handle.</p>
			<p>We now have code to start and stop processes, which also maps names to scripts. We need to wrap it in a web service that the voice agent can use.</p>
			<h2 id="_idParaDest-396"><a id="_idTextAnchor365"/>Programming the Flask control API server</h2>
			<p>We've used<a id="_idIndexMarker986"/> Flask previously to make the web server for our visual processing behaviors. We are going to use it for something a bit simpler this time, though.  </p>
			<p>As we saw with the start and stop buttons in the image servers, Flask lets us set up handlers for links to perform tasks. Let's make a script that acts as our control web service, which uses <strong class="source-inline">Flask</strong> and our <strong class="source-inline">RobotModes</strong> object.</p>
			<p>Let's build this by following these steps:</p>
			<ol>
				<li value="1">Create a script called <strong class="source-inline">control_server.py</strong>. We can start by importing Flask and our robot modes:<p class="source-code"><strong class="bold">from flask import Flask</strong></p><p class="source-code"><strong class="bold">from robot_modes import RobotModes</strong></p></li>
				<li>Now, we create a Flask app to contain the routes and an instance of our <strong class="source-inline">RobotModes</strong> class from before:<p class="source-code"><strong class="bold">app = Flask(__name__)</strong></p><p class="source-code"><strong class="bold">mode_manager = RobotModes()</strong></p></li>
				<li>Next, we need a route, or API endpoint, to run the app. It takes the mode name as part of the <a id="_idIndexMarker987"/>route:<p class="source-code"><strong class="bold">@app.route("/run/&lt;mode_name&gt;", methods=['POST'])</strong></p><p class="source-code"><strong class="bold">def run(mode_name):</strong></p><p class="source-code"><strong class="bold">    mode_manager.run(mode_name)</strong></p><p class="source-code"><strong class="bold">    return "%s running"</strong></p><p>We return a running confirmation.</p></li>
				<li>We also need another API endpoint to stop the running process:<p class="source-code"><strong class="bold">@app.route("/stop", methods=['POST'])</strong></p><p class="source-code"><strong class="bold">def stop():</strong></p><p class="source-code"><strong class="bold">    mode_manager.stop()</strong></p><p class="source-code"><strong class="bold">    return "stopped"</strong></p></li>
				<li>Finally, we need to start the server up:<p class="source-code"><strong class="bold">app.run(host="0.0.0.0", debug=True)</strong></p><p>This app is ready to start for speech control.</p><p class="callout-heading">Tip</p><p class="callout">What is a URL? You have already used these with other web services in the book. A URL, or uniform resource locator, defines how to reach some kind of resource; it starts with a protocol specification—in this case, <strong class="source-inline">http</strong> for a web (hypertext) service. This is followed by a colon (<strong class="source-inline">:</strong>) and then two slashes <strong class="source-inline">//</strong> with a hostname or host address—the network address of the Raspberry Pi the resource will be on. As a host can have many services running, we can then have a port number, with a colon as a separator—in our case, <strong class="source-inline">:5000</strong>. After this, you could add a slash <strong class="source-inline">/</strong> then select a specific resource in the service.</p><p>We can test this now:</p></li>
				<li>Power up the robot and copy both the <strong class="source-inline">control_server.py</strong> and <strong class="source-inline">robot_modes.py</strong> files to it.</li>
				<li>SSH into the robot and start the control server with <strong class="source-inline">python3 control_server.py</strong>. You <a id="_idIndexMarker988"/>should see the following:<p class="source-code">$ python3 control_server.py</p><p class="source-code"> * Serving Flask app "control_server" (lazy loading)</p><p class="source-code"> * Environment: production</p><p class="source-code">   WARNING: Do not use the development server in a production environment.</p><p class="source-code">   Use a production WSGI server instead.</p><p class="source-code"> * Debug mode: on</p><p class="source-code"> * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)</p></li>
				<li>Now create another <strong class="source-inline">ssh</strong> window into the Mycroft Raspberry Pi – we can test that one talks to the other. Press <em class="italic">Ctrl</em> + <em class="italic">C</em> once into <strong class="source-inline">pi@picroft.local</strong> to get to the Linux command line (the <strong class="source-inline">$</strong> prompt).</li>
				<li>The <strong class="source-inline">curl</strong> command is frequently used on Linux systems like the Raspberry Pi to test servers like this. It makes requests to web servers, sending/receiving data, and displaying the result. It's perfect for testing HTTP control APIs like this.<p>We intend to make a <strong class="source-inline">post</strong> request. Type this command: </p><p class="source-code">curl -X POST http://myrobot.local:5000/run/test_rainbow</p><p>This should start the rainbows turning on and off, using the code from <a href="B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Programming RGB Strips in Python</em>. The <strong class="source-inline">curl</strong> command specifies that we are using the <strong class="source-inline">POST</strong> method to make a request, then a URL with the port, the robot hostname, then the instruction <strong class="source-inline">run</strong>, and then the mode name.</p></li>
				<li>You can stop the LEDs with <strong class="source-inline">curl -X POST http://myrobot.local:5000/stop</strong>. This URL has the instruction <strong class="source-inline">stop</strong>. The robot LED rainbow should stop.<p>Notice how both these URLs have <strong class="source-inline">http://myrobot.local:5000/</strong> at their start. The address may be different for your robot, depending on your hostname. This is a base URL for this control server.</p></li>
				<li>You can press <em class="italic">Ctrl</em> + <em class="italic">C</em> to stop this.</li>
			</ol>
			<p>We can use this <a id="_idIndexMarker989"/>to build our Mycroft behaviors, but let's check for any problems before carrying on.</p>
			<h2 id="_idParaDest-397"><a id="_idTextAnchor366"/>Troubleshooting</h2>
			<p>If this isn't<a id="_idIndexMarker990"/> working for you, we can check a few things to see what happened:</p>
			<ul>
				<li>If you receive any syntax errors, check your code and try again.</li>
				<li>Please verify that your robot and the device you are testing from have internet availability.</li>
				<li>Note that when we are starting the subprocess, we are starting Python 3. Without the <strong class="source-inline">3</strong>, other unexpected things will happen.</li>
				<li>First, remember the control server is running on the Raspberry Pi 3A+ on the robot. You will need to substitute it for your robot's address in the <strong class="source-inline">curl</strong> commands.</li>
				<li>Ensure you have installed Flask, as shown in <a href="B15660_13_Final_ASB_ePub.xhtml#_idTextAnchor283"><em class="italic">Chapter 13</em></a>, <em class="italic">Robot Vision – Using a Pi Camera and OpenCV</em>.</li>
				<li>Make sure you have copied both the control server and the robot mode scripts to the robot. You will also need the code from <a href="B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Programming RGB Strips in Python</em> installed on the robot to run this test.</li>
			</ul>
			<p>Now we've tested the <a id="_idIndexMarker991"/>control server, you can power down the Pi. There's some more code to write! Let's tie this into Mycroft.</p>
			<h1 id="_idParaDest-398"><a id="_idTextAnchor367"/>Programming a voice agent with Mycroft on the Raspberry Pi</h1>
			<p>The robot backend<a id="_idIndexMarker992"/> provided by the Flask control system is good enough to create our Mycroft skill with. </p>
			<p>In <em class="italic">Figure 15.4</em>, you saw that after you say something with the wake word, upon waking, Mycroft will transmit the sound you made to the Google STT system. Google STT will then return the text.</p>
			<p>Mycroft will then match this against vocabulary files for the region you are in and match that with intents set up in the skills. Once matched, Mycroft will invoke an intent in a skill. Our robot skill has intents that will make network (HTTP) requests to the Flask control server we created for our robot. When the Flask server responds to say that it has processed the request (perhaps the behavior is started), the robot skill will choose a dialog to speak back to the user to confirm that it has successfully carried out the request or found a problem.</p>
			<p>We'll start with a simple skill, with a basic intent, and then you can expand this to perform more. I've picked the rainbow LEDs test (<strong class="source-inline">test_leds</strong> from <a href="B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Programming RGB Strips in Python</em>) because it is simple.</p>
			<p>It's worth noting that the time taken to get the speech processed by Google means that this is not suitable for stopping a robot in a hurry; the voice recognition can take some time. You could consider using <strong class="source-inline">GPIOZero</strong> in the intent and a <strong class="source-inline">when_pressed</strong> handler to trigger the control server's stop handler.</p>
			<h2 id="_idParaDest-399"><a id="_idTextAnchor368"/>Building the intent</h2>
			<p>We can start<a id="_idIndexMarker993"/> with the<a id="_idIndexMarker994"/> intent, then look at some vocabulary. To build it, we will use a library built into Mycroft named <strong class="source-inline">adapt</strong>:</p>
			<ol>
				<li value="1">Create a folder called <strong class="source-inline">my-robot-skill</strong>, which we will work in to build the Mycroft skill. </li>
				<li>The main intent file will be an <strong class="source-inline">__init__.py</strong> file in this folder. This filename means that Python will treat the whole folder like a Python library, called a <strong class="bold">package</strong>. Let's start putting some imports in <strong class="source-inline">my-robot-skill/__init__.py</strong>:<p class="source-code"><strong class="bold">from adapt.intent import IntentBuilder</strong></p><p class="source-code"><strong class="bold">from mycroft import MycroftSkill, intent_handler</strong></p><p class="source-code"><strong class="bold">from mycroft.util.log import LOG</strong></p><p class="source-code"><strong class="bold">import requests</strong></p><p>The imports include <strong class="source-inline">IntentBuilder</strong> to build and define intents around vocabulary. <strong class="source-inline">MycroftSkill</strong> is a base class to plug our code into Mycroft. <strong class="source-inline">intent_handler</strong> marks which parts of our code are intents, associating the code with <strong class="source-inline">IntentBuilder</strong>. We import <strong class="source-inline">LOG</strong> to write information out to the Mycroft console and see problems there.</p><p>The last import, <strong class="source-inline">requests</strong>, is a tool to talk to our control server in Python remotely.</p></li>
				<li>Next, we will define our skill from the <strong class="source-inline">MycroftSkill</strong> base. It needs to set up its parent and prepare settings:<p class="source-code"><strong class="bold">class MyRobot(MycroftSkill):</strong></p><p class="source-code"><strong class="bold">    def __init__(self):</strong></p><p class="source-code"><strong class="bold">        super().__init__()</strong></p><p class="source-code"><strong class="bold">        self.base_url = self.settings.get("base_url")</strong></p><p>The Python keyword <strong class="source-inline">super</strong> calls a method from a class we've made our base; in this case, <strong class="source-inline">__init__</strong> so we can let it set things up.</p><p>The only setting we have is a <strong class="source-inline">base_url</strong> member for our control server on the robot. It is consulting a settings file, which we'll see later. It's usually a good idea to separate the configuration from the code.</p></li>
				<li>The next thing we need is to define an intent. We do so with a <strong class="source-inline">handle_test_rainbow</strong> method – but you need to decorate it using <strong class="source-inline">@intent_handler</strong>. In Python, decorating wraps a method in further handling – in <a id="_idIndexMarker995"/>this <a id="_idIndexMarker996"/>case, making it suitable for Mycroft:<p class="source-code"><strong class="bold">    @intent_handler(IntentBuilder("")</strong></p><p class="source-code"><strong class="bold">                    .require("Robot")</strong></p><p class="source-code"><strong class="bold">                    .require("TestRainbow"))</strong></p><p class="source-code"><strong class="bold">    def handle_test_rainbow(self, message):</strong></p><p>The <strong class="source-inline">intent_handler</strong> decorator takes some parameters to configure the vocabulary we will use. We will define vocabulary in files later. We require a vocabulary matching <em class="italic">robot</em> first, then another part matching <em class="italic">TestRainbow</em> – which could match a few phrases.</p></li>
				<li>Next, this skill should make the request to the robot – using <strong class="source-inline">requests.post</strong>:<p class="source-code"><strong class="bold">        try:</strong></p><p class="source-code"><strong class="bold">            requests.post(self.base_url + "/run/test_rainbow")</strong></p><p>This segment posts to the URL in the <strong class="source-inline">base_url</strong> variable, plus the <strong class="source-inline">run</strong> instruction and the <strong class="source-inline">test_rainbow</strong> mode.</p></li>
				<li>We need Mycroft to say something, to say that it has told the robot to do something here:<p class="source-code"><strong class="bold">            self.speak_dialog('Robot')</strong></p><p class="source-code"><strong class="bold">            self.speak_dialog('TestingRainbow')</strong></p><p>The <strong class="source-inline">speak_dialog</strong> method tells Mycroft to pick something to say from dialog files, which allows it to have variations on things to say.</p></li>
				<li>This request could fail for a few reasons, hence the <strong class="source-inline">try</strong> in the code snippet before last. We need an <strong class="source-inline">except</strong> to handle this and speak a dialog for the user. We also <strong class="source-inline">LOG</strong> an exception to the Mycroft console:<p class="source-code"><strong class="bold">        except:</strong></p><p class="source-code"><strong class="bold">            self.speak_dialog("UnableToReach")</strong></p><p class="source-code"><strong class="bold">            LOG.exception("Unable to reach the robot")</strong></p><p>We are treating many error types as <strong class="source-inline">Unable to reach the robot</strong>, while not inspecting<a id="_idIndexMarker997"/> the <a id="_idIndexMarker998"/>result code from the server other than if the voice skill contacted the robot.</p></li>
				<li>This file then needs to provide a <strong class="source-inline">create_skill</strong> function outside of the class, which Mycroft expects to find in skill files:<p class="source-code"><strong class="bold">def create_skill():</strong></p><p class="source-code"><strong class="bold">    return MyRobot()</strong></p></li>
			</ol>
			<p>The code is one part of this system, but we need to configure this before using it.</p>
			<h3 id="_idParaDest-400">The settings file</h3>
			<p>Our intent started by loading a<a id="_idIndexMarker999"/> setting. We will put this in <strong class="source-inline">my-robot-skill/settingsmeta.json</strong>, and it defines the base URL for our control server.</p>
			<p>Please use the hostname/address of your robot Raspberry Pi if it is different. This file is a little long for this one setting, but will mean that you can configure the URL later if need be:</p>
			<p class="source-code">{</p>
			<p class="source-code">    "skillMetadata": {</p>
			<p class="source-code">        "sections": [</p>
			<p class="source-code">            {</p>
			<p class="source-code">                "name": "Robot",</p>
			<p class="source-code">                "fields": [</p>
			<p class="source-code">                    {</p>
			<p class="source-code">                        "name": "base_url",</p>
			<p class="source-code">                        "type": "text",</p>
			<p class="source-code">                        "label": "Base URL for the robot control server",</p>
			<p class="source-code">                        "value": "http://myrobot.local:5000"</p>
			<p class="source-code">                    }</p>
			<p class="source-code">                ]</p>
			<p class="source-code">            }</p>
			<p class="source-code">        ]</p>
			<p class="source-code">    } </p>
			<p class="source-code">}</p>
			<p>We have now set <a id="_idIndexMarker1000"/>which base URL to use, but we need to configure Mycroft to load our skill.</p>
			<h3 id="_idParaDest-401">The requirements file</h3>
			<p>Our skill <a id="_idIndexMarker1001"/>uses the <strong class="source-inline">requests</strong> library. When Mycroft encounters our skill, we should tell it to expect this. In Python, requirements files are the standard way to do this. Put the following in <strong class="source-inline">my-robot-skill/requirements.txt</strong>:</p>
			<p class="source-code">requests</p>
			<p>This file is not unique to Mycroft and is used with many Python systems to install libraries needed by an application.</p>
			<p>Now we need to tell Mycroft what to listen for, with vocabulary.</p>
			<h3 id="_idParaDest-402">Creating the vocabulary files</h3>
			<p>To define<a id="_idIndexMarker1002"/> vocabularies, we need to define vocabulary files. You need to put them in a folder following the format <strong class="source-inline">my-robot-skill/vocab/&lt;IETF language and locale&gt;</strong>. A language/locale means we should be able to define a vocabulary for variants such as <strong class="source-inline">en-us</strong> for American English and <strong class="source-inline">zn-cn</strong> for simplified Chinese; however, at the time of writing, <strong class="source-inline">en-us</strong> is the most supported Mycroft language. Parts of the community are working on support for other languages.</p>
			<p>You define each intent with one or more vocabulary parts matching vocabulary files. Vocabulary files have lines representing ways to phrase the intended utterance. These allow a human to naturally vary the way they say things, something people notice when a machine fails to respond to a slightly different way of asking for something. There is a bit of a trick in thinking up similar phrases for the vocabulary files.</p>
			<p>We need two vocabulary files for our intent—one for <strong class="source-inline">robot</strong> synonyms and one for <strong class="source-inline">TestRainbow</strong> synonyms:</p>
			<ol>
				<li value="1">Create the folder <strong class="source-inline">vocab</strong> under <strong class="source-inline">my-robot-skill</strong>, and then the <strong class="source-inline">en-us</strong> folder under that.</li>
				<li>Make a file there with the path and name <strong class="source-inline">my-robot-skill/vocab/en-us/robot.voc</strong>.</li>
				<li>Add some<a id="_idIndexMarker1003"/> phrases for <em class="italic">asking the robot to do something</em>:<p class="source-code"><strong class="bold">robot</strong></p><p class="source-code"><strong class="bold">my robot</strong></p><p class="source-code"><strong class="bold">ask robot to</strong></p><p class="source-code"><strong class="bold">tell the robot to</strong></p><p>Mycroft will match these phrases where we have said <strong class="source-inline">robot</strong> in the intent handler.</p></li>
				<li>Let's create the vocabulary for testing the rainbow. Put it into <strong class="source-inline">my-robot-skill/vocab/en-us/TestRainbow.voc</strong>:<p class="source-code"><strong class="bold">test rainbow</strong></p><p class="source-code"><strong class="bold">test the leds</strong></p><p class="source-code"><strong class="bold">deploy rainbows</strong></p><p class="source-code"><strong class="bold">turn on the lights</strong></p><p class="callout-heading">Important note</p><p class="callout">Note that the vocabulary filename's capitalization must match the intent builder; I've then used the convention of capitalizing the non-shared vocab parts.</p></li>
			</ol>
			<p>Inevitably, when you test this, you will eventually try to say a sensible sounding phrase that isn't there. Mycroft will tell you <em class="italic">Sorry, I don't understand</em>, and you will add another <a id="_idIndexMarker1004"/>expression to the vocabularies above.</p>
			<h3 id="_idParaDest-403">Dialog files</h3>
			<p>We also want to<a id="_idIndexMarker1005"/> define the phrases Mycroft will say back to you. We have three phrases that our intent requires so far. These go into the <strong class="source-inline">my-robot-skill/dialog/en-us</strong> folder with a similar structure to vocabulary files. Let's build them:</p>
			<ol>
				<li value="1">Under <strong class="source-inline">my-robot-skill</strong>, create the folder <strong class="source-inline">dialog</strong>, and then under this, the folder <strong class="source-inline">en-us</strong>.</li>
				<li>In the folder, create the file with the path <strong class="source-inline">my-robot-skill/dialog/en-us/Robot.dialog</strong>. We can add some phrases for that here:<p class="source-code"><strong class="bold">The Robot</strong></p><p class="source-code"><strong class="bold">Robot</strong></p></li>
				<li>The next dialog we need is <strong class="source-inline">TestRainbow.dialog</strong> in the same folder:<p class="source-code"><strong class="bold">is testing rainbows.</strong></p><p class="source-code"><strong class="bold">is deploying rainbows.</strong></p><p class="source-code"><strong class="bold">is starting rainbows.</strong></p><p class="source-code"><strong class="bold">is lighting up.</strong></p></li>
				<li>Since we have an error handler, we should also create <strong class="source-inline">UnableToReach.dialog</strong>:<p class="source-code"><strong class="bold">Sorry I cannot reach the robot.</strong></p><p class="source-code"><strong class="bold">The robot is unreachable.</strong></p><p class="source-code"><strong class="bold">Have you turned the robot on?</strong></p><p class="source-code"><strong class="bold">Is the control server running on the robot?</strong></p></li>
			</ol>
			<p>By defining multiple possible dialogs, Mycroft will randomly pick one to make itself less repetitive. We've now seen how to make vocabulary phrases and dialog phrases. Let's just recap what we should have.</p>
			<h3 id="_idParaDest-404">Current skill folder</h3>
			<p>Our skill <a id="_idIndexMarker1006"/>folder should look like the following screenshot:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="Images/B15660_15_05.jpg" alt="" width="470" height="518"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.5 – Screenshot of the robot skill folder</p>
			<p>In <em class="italic">Figure 15.5,</em> we see a screenshot showing the skill in a folder called <strong class="source-inline">my-robot-skill</strong>. This skill folder has the <strong class="source-inline">dialog</strong> folder, with the <strong class="source-inline">en-us</strong> subfolder and the three dialog files here. Below that is the <strong class="source-inline">vocab</strong> folder, with the <strong class="source-inline">en-us</strong> folder and two vocab files. Below the <strong class="source-inline">vocab</strong> folder, we have <strong class="source-inline">__init__.py</strong> defining the intents, requirements for Mycroft to install it, and a settings file. Whew – we've created a lot here, but it will be worth it!</p>
			<p>We are going to now need to upload this whole folder structure to our robot:</p>
			<ol>
				<li value="1">Using SFTP (FileZilla), upload this folder to your Mycroft Pi, in the <strong class="source-inline">/opt/mycroft/skills</strong> folder.</li>
				<li>Mycroft will automatically load this skill; you will see purple text for this flash past as it does the install.</li>
				<li>If you need to update the code, uploading the files to this location again will cause Mycroft to reload it.<p>Any problems loading or using the skill will be shown on the Mycroft output. You can also find the result in <strong class="source-inline">/var/log/mycroft/skills.log</strong>—the <strong class="source-inline">less</strong> Linux tool is useful for looking at log output like this, using <em class="italic">Shift</em> + <em class="italic">G</em> to jump to the end of the file or typing <strong class="source-inline">/myrobot</strong> to jump to its output. </p><p>You can also use <strong class="source-inline">tail -f /var/log/mycroft/skills.log</strong> to see problems as they happen. Use <em class="italic">Ctrl</em> + <em class="italic">C</em> to stop.</p></li>
				<li>Now, power up the robot, <strong class="source-inline">ssh</strong> in, and start the control server with <strong class="source-inline">python3 control_server.py</strong>.</li>
				<li>You can then try out your skill with Mycroft: <em class="italic">Tell the robot to turn on the lights</em>. </li>
				<li>Mycroft should beep to show the user it's awake and, once it has got the words from speech<a id="_idIndexMarker1007"/> to text, it will send <strong class="source-inline">/run/test_rainbow</strong> to the control server on the robot. You should hear Mycroft say one of the dialog phrases, such as <em class="italic">The robot is testing rainbows</em> and see the LEDs light up.</li>
			</ol>
			<h2 id="_idParaDest-405"><a id="_idTextAnchor369"/>Troubleshooting</h2>
			<p>If you encounter <a id="_idIndexMarker1008"/>problems making the intent respond, please try the following:</p>
			<ul>
				<li>First, check the syntax and indenting of the previous Python code.</li>
				<li>Ensure that your robot and the voice assistant Raspberry Pi are on the same network; I've found this problematic with some Wi-Fi extenders, and IP addresses are needed instead of <strong class="source-inline">myrobot.local</strong>. Use the <strong class="source-inline">settingsmeta.json</strong> file to configure this.</li>
				<li>Ensure you have copied over the whole structure – with the <strong class="source-inline">vocab</strong>, <strong class="source-inline">dialog</strong>, <strong class="source-inline">settingsmeta.json</strong>, and <strong class="source-inline">__init__.py</strong> – to the <strong class="source-inline">/opt/mycroft/skills</strong> folder on the voice assistant Raspberry Pi.</li>
				<li>If your settings were incorrect, you will need to change them on the <a href="https://account.mycroft.ai/skills">https://account.mycroft.ai/skills</a> page. Look for the <strong class="source-inline">My Robot</strong> skill and change it here. You will need to save the change and may need to restart Mycroft or wait a few minutes for this to take effect.</li>
				<li>Ensure the way you have spoken to Mycroft matches your vocabulary files – it will not recognize your words otherwise.</li>
				<li>You can also type phrases into the Mycroft console if you are having trouble with it recognizing your voice.</li>
			</ul>
			<p>We've got our first intent to work! You've been able to speak to a voice assistant, and it has instructed the robot what to do. However, we've now started the LEDs flashing, and the only way to stop <a id="_idIndexMarker1009"/>them is with that inconvenient <strong class="source-inline">curl</strong> command. We should probably fix that by adding another intent.</p>
			<h2 id="_idParaDest-406"><a id="_idTextAnchor370"/>Adding another intent</h2>
			<p>Now we have our<a id="_idIndexMarker1010"/> skill, adding a second intent for it to stop becomes relatively easy, using another of the<a id="_idIndexMarker1011"/> endpoints in our robot's control server. </p>
			<h3 id="_idParaDest-407">Vocabulary and dialog</h3>
			<p>We need to add the<a id="_idIndexMarker1012"/> vocabulary and dialog so our new intent can understand what we are saying and has a few things to say back:</p>
			<ol>
				<li value="1">We will need to create the <strong class="source-inline">stop</strong> vocabulary; we can put this in <strong class="source-inline">my-robot-skill/vocab/en-us/stop.voc</strong>:<p class="source-code"><strong class="bold">stop</strong></p><p class="source-code"><strong class="bold">cease</strong></p><p class="source-code"><strong class="bold">turn off</strong></p><p class="source-code"><strong class="bold">stand down</strong></p></li>
				<li>We need a dialog file for Mycroft to tell us the robot is stopping in <strong class="source-inline">my-robot-skill/dialog/en-us/stopping.dialog</strong>:<p class="source-code"><strong class="bold">is stopping.</strong></p><p class="source-code"><strong class="bold">will stop.</strong></p></li>
			</ol>
			<p>These will do, but you can add more synonyms if you think of them.</p>
			<h3 id="_idParaDest-408">Adding the code</h3>
			<p>Now we <a id="_idIndexMarker1013"/>need to add the intent code to our skill:</p>
			<ol>
				<li value="1">We will put this into the <strong class="source-inline">MyRobot</strong> class in <strong class="source-inline">my-robot-skill/__init__.py</strong>:<p class="source-code"><strong class="bold">    @intent_handler(IntentBuilder("")</strong></p><p class="source-code"><strong class="bold">                    .require("Robot")</strong></p><p class="source-code"><strong class="bold">                    .require("stop"))</strong></p><p class="source-code"><strong class="bold">    def handle_stop(self, message):</strong></p><p class="source-code"><strong class="bold">        try:</strong></p><p class="source-code"><strong class="bold">            requests.post(self.base_url + "/stop")</strong></p><p class="source-code"><strong class="bold">            self.speak_dialog('Robot')</strong></p><p class="source-code"><strong class="bold">            self.speak_dialog('stopping')</strong></p><p class="source-code"><strong class="bold">        except:</strong></p><p class="source-code"><strong class="bold">            self.speak_dialog("UnableToReach")</strong></p><p class="source-code"><strong class="bold">            LOG.exception("Unable to reach the robot")</strong></p><p>This code is almost identical to the test rainbows intent, with the <strong class="source-inline">stop</strong> vocabulary, the handler name (which could be anything – but must not be the same as another handler), and the URL endpoint. </p><p>Identical code like that is ripe for refactoring. Refactoring is changing the appearance of code without affecting what it does. This is useful for dealing with common/repeating code sections or improving how readable code is. Both the intents have the same try/catch and similar dialog with some small differences. </p></li>
				<li>In the same file, add the following:<p class="source-code"><strong class="bold">    def handle_control(self, end_point, dialog_verb):</strong></p><p class="source-code"><strong class="bold">        try:</strong></p><p class="source-code"><strong class="bold">            requests.post(self.base_url + end_point)</strong></p><p class="source-code"><strong class="bold">            self.speak_dialog('Robot')</strong></p><p class="source-code"><strong class="bold">            self.speak_dialog(dialog_verb)</strong></p><p class="source-code"><strong class="bold">        except:</strong></p><p class="source-code"><strong class="bold">            self.speak_dialog("UnableToReach")</strong></p><p class="source-code"><strong class="bold">            LOG.exception("Unable to reach the robot")</strong></p><p>This will be a common handler. It takes <strong class="source-inline">end_point</strong> as a parameter and uses that in its request. It takes a <strong class="source-inline">dialog_verb</strong> parameter to say after the <strong class="source-inline">Robot</strong> bit. All of the other dialog and error handling we saw before is here. </p></li>
				<li>The two intents now become far simpler. Change them to the following:<p class="source-code"><strong class="bold">    @intent_handler(IntentBuilder("")</strong></p><p class="source-code"><strong class="bold">                    .require("Robot")</strong></p><p class="source-code"><strong class="bold">                    .require("TestRainbow"))</strong></p><p class="source-code"><strong class="bold">    def handle_test_rainbow(self, message):</strong></p><p class="source-code"><strong class="bold">        self.handle_control('/run/test_rainbow', 'TestingRainbow')</strong></p><p class="source-code"><strong class="bold">    @intent_handler(IntentBuilder("")</strong></p><p class="source-code"><strong class="bold">                    .require("Robot")</strong></p><p class="source-code"><strong class="bold">                    .require("stop"))</strong></p><p class="source-code"><strong class="bold">    def handle_stop(self, message):</strong></p><p class="source-code"><strong class="bold">        self.handle_control('/stop', 'stopping')</strong></p></li>
			</ol>
			<p>Adding new<a id="_idIndexMarker1014"/> intents is now easier as we can reuse <strong class="source-inline">handle_control</strong>.</p>
			<h3 id="_idParaDest-409">Running with the new intent</h3>
			<p>You can now <a id="_idIndexMarker1015"/>upload the folder structure again—since the <strong class="source-inline">vocab</strong>, <strong class="source-inline">dialog</strong>, and <strong class="source-inline">__init__</strong> files have changed. When you do so, note that Mycroft will automatically reload the changed skill (or show any problems trying to do so), so it is immediately ready to use.</p>
			<p>Try this out by saying <em class="italic">Mycroft, tell the robot to stop</em>.</p>
			<p>You've now added a second intent to the system, defining further vocabulary and dialogs. You've also refactored this code, having seen some repetition. You've now got the beginnings of voice control for your robot.</p>
			<h1 id="_idParaDest-410"><a id="_idTextAnchor371"/>Summary</h1>
			<p>In this chapter, you learned about voice assistant terminology, speech to text, wake words, intents, skills, utterances, vocabulary, and dialog. You considered where you would install microphones and speakers and whether they should be on board a robot.</p>
			<p>You then saw how to physically install a speaker/microphone combination onto a Raspberry Pi, then prepare software to get the Pi to use it. You installed Picroft – a Mycroft Raspbian environment, getting the voice agent software.</p>
			<p>You were then able to play with Mycroft and get it to respond to different voice commands and register it with its base.</p>
			<p>You then saw how to make a robot ready for an external agent, such as a voice agent to control it with a Flask API. You were able to create multiple skills that communicate with a robot, with a good starting point for creating more.</p>
			<p>In the next chapter, we will bring back out the IMU we introduced in <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>, and get it to do more interesting things – we will smooth and calibrate the sensors and then combine them to get a heading for the robot, programming the robot to always turn north.</p>
			<h1 id="_idParaDest-411"><a id="_idTextAnchor372"/>Exercises</h1>
			<p>Try these exercises to get more out of this chapter and expand your experience:</p>
			<ul>
				<li>Try installing some other Mycroft skills from the Mycroft site and playing with them. Hint: say <em class="italic">Hey Mycroft, install pokemon</em>.</li>
				<li>The robot mode system has a flaw; it assumes that a process you've asked to stop does stop. Should it wait and check the return code to see if it has stopped?</li>
				<li>An alternative way to implement the robot modes might be to update all the behaviors to exit cleanly so you could import them instead of running in subprocesses. How tricky would this be?</li>
				<li>While testing the interactions, did you find the vocabulary wanting? Perhaps extend it with phrases you might find more natural to start the different behaviors. Similarly, you could make dialogs more interesting too.</li>
				<li>Add more intents to the skill, for example, wall avoiding. You could add a stop intent, although the response time may make this less than ideal.</li>
				<li>Could the RGB LEDs on the ReSpeaker 2-Mics Pi HAT be used? The project <a href="https://github.com/respeaker/mic_hat">https://github.com/respeaker/mic_hat</a> has an LED demonstration.</li>
			</ul>
			<p>With these ideas, there is plenty of room to explore this concept more. Further reading will help too.</p>
			<h1 id="_idParaDest-412"><a id="_idTextAnchor373"/>Further reading</h1>
			<p>Please refer to the following for more information:</p>
			<ul>
				<li><em class="italic">Raspberry Pi Robotic Projects</em>, <em class="italic">Dr. Richard Grimmett</em>, <em class="italic">Packt Publishing</em>, has a chapter on providing speech input and output.</li>
				<li><em class="italic">Voice User Interface Projects</em>, <em class="italic">Henry Lee</em>, <em class="italic">Packt Publishing</em>, focuses entirely on voice interfaces to systems. It shows you how to build chatbots and applications with the Alexa and Google Home voice agents.</li>
				<li><em class="italic">Mycroft AI – Introduction Voice Stack</em> – a whitepaper from Mycroft AI gives more detail on how the Mycroft stack works and its components.</li>
				<li>Mycroft has a large community that supports and discusses the technology at <a href="https://community.mycroft.ai/">https://community.mycroft.ai/</a>. I recommend consulting the troubleshooting information of this community. Mycroft is under active development and has both many quirks and many new features. It's also an excellent place to share skills you build for it.</li>
				<li>Seeed Studio, the ReSpeaker 2-Mics Pi HAT creators, host documentation and code for this device, along with bigger four and six-microphone versions at <a href="https://github.com/respeaker/seeed-voicecard">https://github.com/respeaker/seeed-voicecard</a>.</li>
			</ul>
		</div>
	</div></body></html>