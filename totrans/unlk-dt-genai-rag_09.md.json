["```py\n    <st c=\"21354\">$ pip install ragas</st>\n    ```", "```py\n    <st c=\"21374\">$ pip install tqdm -q â€“user</st>\n    ```", "```py\n    <st c=\"21515\">tqdm</st> package, which is used by ragas, is a popular Python library used for creating progress bars and displaying progress information for iterative processes. You have probably come across the <st c=\"21708\">matplotlib</st> package before, as it is a widely used plotting library for Python. We will be using it to provide visualizations for our evaluation metric results.\n    ```", "```py\n    <st c=\"21939\">import tqdm as notebook_tqdm</st>\n    ```", "```py\n    <st c=\"21968\">import pandas as pd</st>\n    ```", "```py\n    <st c=\"21988\">import matplotlib.pyplot as plt</st>\n    ```", "```py\n    <st c=\"22020\">from datasets import Dataset</st>\n    ```", "```py\n    <st c=\"22049\">from ragas import evaluate</st>\n    ```", "```py\n    <st c=\"22076\">from ragas.testset.generator import TestsetGenerator</st>\n    ```", "```py\n    <st c=\"22129\">from ragas.testset.evolutions import (</st>\n    ```", "```py\n     <st c=\"22168\">simple, reasoning, multi_context)</st>\n    ```", "```py\n    <st c=\"22202\">from ragas.metrics import (</st>\n    ```", "```py\n     <st c=\"22230\">answer_relevancy,</st>\n    ```", "```py\n     <st c=\"22248\">faithfulness,</st>\n    ```", "```py\n     <st c=\"22262\">context_recall,</st>\n    ```", "```py\n     <st c=\"22278\">context_precision,</st>\n    ```", "```py\n    **<st c=\"22297\">answer_correctness,</st>**\n    ```", "```py\n     **<st c=\"22317\">answer_similarity</st>**\n    ```", "```py\n    `<st c=\"22347\">tqdm</st>` <st c=\"22351\">will give our ragas platform the ability to use progress bars during the time-consuming processing tasks it implements.</st> <st c=\"22472\">We are going to use the</st> <st c=\"22496\">popular pandas data manipulation and analysis library to pull our data into DataFrames as part of our analysis.</st> <st c=\"22608\">The</st> `<st c=\"22612\">matplotlib.pyplot as plt</st>` <st c=\"22636\">import gives us the ability to add visualizations (charts in this case) for our metric results.</st> <st c=\"22733\">We also import</st> `<st c=\"22748\">Dataset</st>` <st c=\"22755\">from</st> `<st c=\"22761\">datasets</st>`<st c=\"22769\">. The</st> `<st c=\"22775\">datasets</st>` <st c=\"22783\">library is an open source library developed and maintained by Hugging Face.</st> <st c=\"22860\">The</st> `<st c=\"22864\">datasets</st>` <st c=\"22872\">library provides</st> <st c=\"22890\">a standardized interface for accessing and manipulating a wide variety of datasets, typically focused on the field of</st> `<st c=\"23157\">from ragas import evaluate</st>`<st c=\"23184\">: The</st> `<st c=\"23191\">evaluate</st>` <st c=\"23199\">function takes a dataset in the ragas format, along with optional metrics, language models, embeddings, and other configurations, and runs the evaluation on the RAG pipeline.</st> <st c=\"23375\">The</st> `<st c=\"23379\">evaluate</st>` <st c=\"23387\">function returns a</st> `<st c=\"23407\">Result</st>` <st c=\"23413\">object containing the scores for each metric, providing a convenient way to assess the performance of RAG pipelines using various metrics</st> <st c=\"23552\">and configurations.</st>\n    ```", "```py\n embedding_ada = \"text-embedding-ada-002\"\nmodel_gpt35=\"gpt-3.5-turbo\"\nmodel_gpt4=\"gpt-4o-mini\"\nembedding_function = OpenAIEmbeddings(\n    model=embedding_ada, openai_api_key=openai.api_key)\nllm = ChatOpenAI(model=model_gpt35,\n    openai_api_key=openai.api_key, temperature=0.0)\ngenerator_llm = ChatOpenAI(model=model_gpt35,\n    openai_api_key=openai.api_key, temperature=0.0)\ncritic_llm = ChatOpenAI(model=model_gpt4,\n    openai_api_key=openai.api_key, temperature=0.0)\n```", "```py\n llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n```", "```py\n<st c=\"27627\">rag_chain_similarity</st> = RunnableParallel(\n        {\"context\": dense_retriever,\n        \"question\": RunnablePassthrough()\n}).assign(answer=rag_chain_from_docs)\n```", "```py\n<st c=\"27845\">rag_chain_hybrid</st> = RunnableParallel(\n        {\"context\": ensemble_retriever,\n        \"question\": RunnablePassthrough()\n}).assign(answer=rag_chain_from_docs)\n```", "```py\n user_query = \"What are Google's environmental initiatives?\" result = rag_chain_similarity.invoke(user_query)\nretrieved_docs = result['context']\nprint(f\"Original Question to Similarity Search: {user_query}\\n\")\nprint(f\"Relevance Score: {result['answer']['relevance_score']}\\n\")\nprint(f\"Final Answer:\\n{result['answer']['final_answer']}\\n\\n\")\nprint(\"Retrieved Documents:\")\nfor i, doc in enumerate(retrieved_docs, start=1):\n    print(f\"Document {i}: Document ID: {doc.metadata['id']}\n        source: {doc.metadata['source']}\")\n    print(f\"Content:\\n{doc.page_content}\\n\")\n```", "```py\n user_query = \"What are Google's environmental initiatives?\" result = rag_chain_hybrid.invoke(user_query)\nretrieved_docs = result['context']\nprint(f\"Original Question to Dense Search:: {user_query}\\n\")\nprint(f\"Relevance Score: {result['answer']['relevance_score']}\\n\")\nprint(f\"Final Answer:\\n{result['answer']['final_answer']}\\n\\n\")\nprint(\"Retrieved Documents:\")\nfor i, doc in enumerate(retrieved_docs, start=1):\n    print(f\"Document {i}: Document ID: {doc.metadata['id']}\n        source: {doc.metadata['source']}\")\n    print(f\"Content:\\n{doc.page_content}\\n\")\n```", "```py\n Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, water stewardship, and promoting a circular economy. They have implemented sustainability features in products like Google Maps, Google Nest thermostats, and Google Flights to help individuals make more sustainable choices. Google also supports various environmental organizations and initiatives, such as the iMasons Climate Accord, ReFED, and The Nature Conservancy, to accelerate climate action and address environmental challenges. Additionally, Google is involved in public policy advocacy and is committed to reducing its environmental impact through its operations and value chain.\n```", "```py\n Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, focusing on water stewardship, promoting a circular economy, engaging with suppliers to reduce energy consumption and greenhouse gas emissions, and reporting environmental data. They also support public policy and advocacy for low-carbon economies, participate in initiatives like the iMasons Climate Accord and ReFED, and support projects with organizations like The Nature Conservancy. Additionally, Google is involved in initiatives with the World Business Council for Sustainable Development and the World Resources Institute to improve well-being for people and the planet. They are also working on using technology and platforms to organize information about the planet and make it actionable to help partners and customers create a positive impact.\n```", "```py\n generator = TestsetGenerator.from_langchain(\n    generator_llm,\n    critic_llm,\n    embedding_function\n)\ndocuments = [Document(page_content=chunk) for chunk in splits]\ntestset = generator.generate_with_langchain_docs(\n    documents,\n    test_size=10,\n    distributions={\n        simple: 0.5,\n        reasoning: 0.25,\n        multi_context: 0.25\n    }\n)\ntestset_df = testset.to_pandas()\ntestset_df.to_csv(\n    os.path.join('testset_data.csv'), index=False)\nprint(\"testset DataFrame saved successfully in the local directory.\")\n```", "```py\n saved_testset_df = pd.read_csv(os.path.join('testset_data.csv'))\nprint(\"testset DataFrame loaded successfully from local directory.\")\nsaved_testset_df.head(5)\n```", "```py\n saved_testing_data = \\\n    saved_testset_df.astype(str).to_dict(orient='list')\nsaved_testing_dataset = Dataset.from_dict(saved_testing_data)\nsaved_testing_dataset_sm = saved_testing_dataset.remove_columns(\n    [\"evolution_type\", \"episode_done\"])\n```", "```py\n saved_testing_dataset_sm\n```", "```py\n Dataset({\n    features: ['question', 'contexts', 'ground_truth', 'metadata'],\n    num_rows: 7\n})\n```", "```py\n def generate_answer(question, ground_truth, rag_chain):\n    result = rag_chain.invoke(question)\n    return {\n        \"question\": question,\n        \"answer\": result[\"answer\"][\"final_answer\"],\n        \"contexts\": [doc.page_content for doc in result[\"context\"]],\n        \"ground_truth\": ground_truth\n    }\n```", "```py\n testing_dataset_similarity = saved_testing_dataset_sm.map(\n    lambda x: generate_answer(x[\"question\"],\n        x[\"ground_truth\"], rag_chain_similarity),\n    remove_columns=saved_testing_dataset_sm.column_names)\ntesting_dataset_hybrid = saved_testing_dataset_sm.map(\n    lambda x: generate_answer(x[\"question\"],\n        x[\"ground_truth\"], rag_chain_hybrid),\n    remove_columns=saved_testing_dataset_sm.column_names)\n```", "```py\n score_similarity = evaluate(\n    testing_dataset_similarity,\n    metrics=[\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall,\n        answer_correctness,\n        answer_similarity\n    ]\n)\nsimilarity_df = score_similarity.to_pandas()\n```", "```py\n score_hybrid = evaluate(\n    testing_dataset_hybrid,\n    metrics=[\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall,\n        answer_correctness,\n        answer_similarity\n    ]\n)\nhybrid_df = score_hybrid.to_pandas()\n```", "```py\n key_columns = [\n    'faithfulness',\n    'answer_relevancy',\n    'context_precision',\n    'context_recall',\n    'answer_correctness',\n    'answer_similarity'\n]\nsimilarity_means = similarity_df[key_columns].mean()\nhybrid_means = hybrid_df[key_columns].mean()\ncomparison_df = pd.DataFrame(\n        {'Similarity Run': similarity_means,\n        'Hybrid Run': hybrid_means})\ncomparison_df['Difference'] = comparison_df['Similarity Run'] \\\n        - comparison_df['Hybrid Run']\nsimilarity_df.to_csv(\n    os.path.join('similarity_run_data.csv'), index=False)\nhybrid_df.to_csv(\n    os.path.join('hybrid_run_data.csv'), index=False)\ncomparison_df.to_csv(os.path.join('comparison_data.csv'), index=True)\nprint(\"Dataframes saved successfully in the local directory.\")\n```", "```py\n sem_df = pd.read_csv(os.path.join('similarity_run_data.csv'))\nrec_df = pd.read_csv(os.path.join('hybrid_run_data.csv'))\ncomparison_df = pd.read_csv(\n    os.path.join('comparison_data.csv'), index_col=0)\nprint(\"Dataframes loaded successfully from the local directory.\")\nprint(\"Performance Comparison:\")\nprint(\"\\n**Retrieval**:\")\nprint(comparison_df.loc[['context_precision', 'context_recall']])\nprint(\"\\n**Generation**:\")\nprint(comparison_df.loc[['faithfulness', 'answer_relevancy']])\nprint(\"\\n**End-to-end evaluation**:\")\nprint(comparison_df.loc[['answer_correctness', 'answer_similarity']])\n```", "```py\n fig, axes = plt.subplots(3, 1, figsize=(12, 18), sharex=False)\nbar_width = 0.35\ncategories = ['Retrieval', 'Generation', 'End-to-end evaluation']\nmetrics = [\n    ['context_precision', 'context_recall'],\n    ['faithfulness', 'answer_relevancy'],\n    ['answer_correctness', 'answer_similarity']\n]\n```", "```py\n for i, (category, metric_list) in enumerate(zip(categories, metrics)):\n    ax = axes[i]\n    x = range(len(metric_list))\n    similarity_bars = ax.bar(\n    x, comparison_df.loc[metric_list, 'Similarity Run'],\n    width=bar_width, label='Similarity Run',\n    color='#D51900')\n    for bar in similarity_bars:\n        height = bar.get_height()\n        ax.text(\n            bar.get_x() + bar.get_width() / 2,\n            height, f'{height:.1%}', ha='center',\n            va='bottom', fontsize=10)\n    hybrid_bars = ax.bar(\n        [i + bar_width for i in x],\n        comparison_df.loc[metric_list, 'Hybrid Run'],\n        width=bar_width, label='Hybrid Run',\n        color='#992111')\n    for bar in hybrid_bars:\n        height = bar.get_height()\n        ax.text(\n            bar.get_x() + bar.get_width() / 2,\n            height, f'{height:.1%}', ha='center',\n            va='bottom', fontsize=10)\n    ax.set_title(category, fontsize=14, pad=20)\n    ax.set_xticks([i + bar_width / 2 for i in x])\n    ax.set_xticklabels(metric_list, rotation=45,\n        ha='right', fontsize=12)\n    ax.legend(fontsize=12, loc='lower right',\n        bbox_to_anchor=(1, 0))\n```", "```py\n fig.text(0.04, 0.5, 'Scores', va='center',\n    rotation='vertical', fontsize=14)\nfig.suptitle('Performance Comparison', fontsize=16)\nplt.tight_layout(rect=[0.05, 0.03, 1, 0.95])\nplt.subplots_adjust(hspace=0.6, top=0.92)\nplt.show()\n```", "```py\n Performance Comparison:\n**Retrieval**:\n                   Similarity Run  Hybrid Run  Difference\ncontext_precision        0.906113    0.841267    0.064846\ncontext_recall           0.950000    0.925000    0.025000\n```", "```py\n **Generation**:\n                  Similarity Run  Hybrid Run  Difference\nfaithfulness            0.977500    0.945833    0.031667\nanswer_relevancy        0.968222    0.965247    0.002976\n```", "```py\n **End-to-end evaluation**:\n                    Similarity Run  Hybrid Run  Difference\nanswer_correctness        0.776018    0.717365    0.058653\nanswer_similarity         0.969899    0.969170    0.000729\n```"]