- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text Classification Reimagined: Delving Deep into Deep Learning Language Models'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delve into the realm of **deep learning** (**DL**) and its
    application in **natural language processing** (**NLP**), specifically focusing
    on the groundbreaking transformer-based models such as **Bidirectional Encoder
    Representations from Transformers** (**BERT**) and **generative pretrained transformer**
    (**GPT**). We begin by introducing the fundamentals of DL, elucidating its powerful
    capability to learn intricate patterns from large amounts of data, making it the
    cornerstone of state-of-the-art NLP systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we delve into transformers, a novel architecture that has revolutionized
    NLP by offering a more effective method of handling sequence data compared to
    traditional **recurrent neural networks** (**RNNs**) and **convolutional neural
    networks** (**CNNs**). We unpack the transformer’s unique characteristics, including
    its attention mechanisms, which allow it to focus on different parts of the input
    sequence to better understand the context.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Then, we turn our attention to BERT and GPT, transformer-based language models
    that leverage these strengths to create highly nuanced language representations.
    We provide a detailed breakdown of the BERT architecture, discussing its innovative
    use of bidirectional training to generate contextually rich word embeddings. We
    will demystify the inner workings of BERT and explore its pretraining process,
    which leverages a large corpus of text to learn language semantics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discuss how BERT can be fine-tuned for specific tasks, such as text
    classification. We walk you through the steps, from data preprocessing and model
    configuration to training and evaluation, providing a hands-on understanding of
    how to leverage BERT’s power for text classification.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides a thorough exploration of DL in NLP, moving from foundational
    concepts to practical applications, equipping you with the knowledge to harness
    the capabilities of BERT and transformer models for your text classification tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Understanding deep learning basics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of different neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenges of training neural networks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use language models for classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP-ML system design example
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To successfully navigate through this chapter, certain technical prerequisites
    are necessary, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**Programming knowledge**: A strong understanding of Python is essential, as
    it’s the primary language used for most DL and NLP libraries.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning fundamentals**: A good grasp of basic ML concepts such as
    training/testing data, overfitting, underfitting, accuracy, precision, recall,
    and F1 score will be valuable.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DL basics**: Familiarity with **DL** concepts and architectures, including
    neural networks, backpropagation, activation functions, and loss functions, will
    be essential. Knowledge of RNNs and CNNs would be advantageous but not strictly
    necessary as we will focus more on transformer architectures.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLP basics**: Some understanding of basic NLP concepts such as tokenization,
    stemming, lemmatization, and word embeddings (such as **Word2Vec** or **GloVe**)
    would be beneficial.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Libraries and frameworks**: Experience with libraries such as **TensorFlow**
    and **PyTorch** for building and training neural models is crucial. Familiarity
    with NLP libraries such as **NLTK** or **SpaCy** can also be beneficial. For working
    with BERT specifically, knowledge of the **transformers** library from **Hugging
    Face** would be very helpful.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware requirements**: DL models, especially transformer-based models such
    as BERT, are computationally intensive and typically require a modern **graphics
    processing unit** (**GPU)** to train in a reasonable amount of time. Access to
    a high-performance computer or cloud-based solutions with GPU capabilities is
    highly recommended.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mathematics**: A good understanding of linear algebra, calculus, and probability
    is helpful for understanding the inner workings of these models, but most of the
    chapter can be understood without in-depth mathematical knowledge.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These prerequisites are intended to equip you with the necessary background
    to understand and implement the concepts discussed in the chapter. With these
    in place, you should be well-prepared to delve into the fascinating world of DL
    for text classification using **BERT**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Understanding deep learning basics
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we explain what neural network and deep neural networks are, what
    is the motivation for using them, and the different types (architectures) of deep
    learning models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: What is a neural network?
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks are a subfield of **artificial intelligence** (**AI**) and ML
    that focuses on algorithms inspired by the structure and function of the brain.
    It is also known as “deep” learning because these neural networks often consist
    of many repetitive layers, creating a deep architecture.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: These DL models are capable of “learning” from large volumes of complex, high-dimensional,
    and unstructured data. The term “learning” refers to the ability of the model
    to automatically learn and improve from experience without being explicitly programmed
    to do so for any one particular task of the tasks it learns.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: DL can be supervised, semi-supervised, or unsupervised. It’s used in numerous
    applications, including NLP, speech recognition, image recognition, and even playing
    games. The models can identify patterns and make data-driven predictions or decisions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: One of the critical advantages of DL is its ability to process and model data
    of various types, including text, images, sound, and more. This versatility has
    led to a vast range of applications, from self-driving cars to sophisticated web
    search algorithms and highly responsive speech recognition systems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的一个关键优势是它能够处理和建模各种类型的数据，包括文本、图像、声音等。这种多功能性导致了从自动驾驶汽车到复杂的网络搜索算法以及高度响应的语音识别系统等广泛的应用。
- en: It’s worth noting that DL, despite its high potential, also requires significant
    computational power and large amounts of high-quality data to train effectively,
    which can be a challenge.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管深度学习具有很高的潜力，但它也要求有显著的计算能力和大量高质量的数据来有效地训练，这可能是一个挑战。
- en: In essence, DL is a powerful and transformative technology that is at the forefront
    of many of today’s technological advancements.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，深度学习（DL）是一种强大且变革性的技术，它是许多当今技术进步的前沿。
- en: The motivation for using neural networks
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用神经网络的动机
- en: 'Neural networks are used for a variety of reasons in the field of ML and artificial
    intelligence. Here are some of the key motivations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和人工智能领域，神经网络被用于各种原因。以下是一些关键动机：
- en: '**Nonlinearity**: Neural networks, with their intricate structure and use of
    activation functions, can capture nonlinear relationships in data. Many real-world
    phenomena are nonlinear in nature, and neural networks offer a way to model these
    complexities.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非线性**：神经网络通过其复杂的结构和激活函数的使用，可以捕捉数据中的非线性关系。许多现实世界现象在本质上是非线性的，神经网络提供了一种建模这些复杂性的方法。'
- en: '**Universal approximation theorem**: This theorem states that a neural network
    with enough hidden units can approximate virtually any function with a high degree
    of accuracy. This makes them highly flexible and adaptable to a wide range of
    tasks.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用逼近定理**：这个定理表明，具有足够隐藏单元的神经网络可以以高精度逼近几乎任何函数。这使得它们非常灵活，能够适应广泛的任务。'
- en: '**Ability to handle high dimensional data**: Neural networks can handle data
    with a large number of features or dimensions effectively, which makes them useful
    for tasks such as image or speech recognition, where data is highly dimensional.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理高维数据的能力**：神经网络可以有效地处理具有大量特征或维度的数据，这使得它们在图像或语音识别等高度维度的任务中非常有用。'
- en: '**Pattern recognition and prediction**: Neural networks excel at identifying
    patterns and trends within large datasets, making them especially useful for prediction
    tasks, such as forecasting sales or predicting stock market trends.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式识别和预测**：神经网络在识别大型数据集中的模式和趋势方面表现出色，这使得它们在预测任务中特别有用，例如预测销售或预测股市趋势。'
- en: '**Parallel processing**: Neural networks’ architecture allows them to perform
    many operations simultaneously, making them highly efficient when implemented
    on modern hardware.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行处理**：神经网络的结构允许它们同时执行许多操作，这使得它们在现代硬件上实现时效率非常高。'
- en: '**Learning from data**: Neural networks can improve their performance as they
    are exposed to more data. This ability to learn from data makes them highly effective
    for tasks where large amounts of data are available.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从数据中学习**：随着神经网络接触到更多的数据，它们可以提高自己的性能。这种从数据中学习的能力使它们在大量数据可用的任务中非常有效。'
- en: '**Robustness**: Neural networks can handle noise in the input data and are
    robust to small variations in the input.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：神经网络可以处理输入数据中的噪声，对输入的小幅变化具有鲁棒性。'
- en: 'Additionally, neural networks are extensively used in NLP tasks due to several
    reasons. Here are some of the primary motivations:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于几个原因，神经网络在自然语言处理（NLP）任务中被广泛使用。以下是一些主要动机：
- en: '**Handling sequential data**: Natural language is inherently sequential (words
    follow one another to make coherent sentences). RNNs and their advanced versions,
    such as **long short-term memory** (**LSTM**) and **gated recurrent units** (**GRUs**),
    are types of neural networks that are capable of processing sequential data by
    maintaining a form of internal state or memory about the previous steps in the
    sequence.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理序列数据**：自然语言本质上是序列性的（单词依次排列以形成连贯的句子）。循环神经网络（RNN）及其高级版本，如**长短期记忆**（**LSTM**）和**门控循环单元**（**GRUs**），是能够通过保持关于序列中先前步骤的某种内部状态或记忆来处理序列数据的神经网络类型。'
- en: '**Context understanding**: Neural networks, especially recurrent types, are
    capable of understanding the context in a sentence by taking into account the
    surrounding words or even previous sentences, which is crucial in NLP tasks.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic hashing**: Neural networks, through the use of word embeddings (such
    as Word2Vec and GloVe), can encode words in a way that preserves their semantic
    meaning. Words with similar meanings are placed closer together in the vector
    space, which is highly valuable for many NLP tasks.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**End-to-end learning**: Neural networks can learn directly from raw data.
    For example, in image classification, a neural network can learn features from
    the pixel level without needing any manual feature extraction steps. This is a
    significant advantage, as the feature extraction process can be time-consuming
    and require domain expertise.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, neural networks can learn to perform NLP tasks from raw text data
    without the need for manual feature extraction. This is a big advantage in NLP,
    where creating hand-engineered features can be challenging and time-consuming.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Performance**: Neural networks, especially with the advent of transformer-based
    architectures such as BERT, GPT, and so on., have been shown to achieve state-of-the-art
    results in many NLP tasks, including but not limited to machine translation, text
    summarization, sentiment analysis, and question answering.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling large vocabularies**: Neural networks can effectively handle large
    vocabularies and continuous text streams, which is typical in many **NLP** problems.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning hierarchical features**: Deep neural networks can learn hierarchical
    representations. In the context of NLP, lower layers often learn to represent
    simple things such as n-grams, whereas higher layers can represent complex concepts
    such as sentiment.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite these advantages, it’s worth noting that neural networks also have their
    challenges, including their “black box” nature, which makes their decision-making
    process difficult to interpret, and their need for large amounts of data and computational
    resources for training. However, the benefits they provide in terms of performance
    and their ability to learn from raw text data and model complex relationships
    make them a go-to choice for many NLP tasks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The basic design of a neural network
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network consists of multiple layers of interconnected nodes, or “neurons,”
    each of which performs a simple computation on the data it receives, passing its
    output to the neurons of the next layer. Each connection between neurons has an
    associated weight that is adjusted during the learning process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of a basic neural network consists of three types of layers,
    as shown in *Figure 6**.1*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Basic architecture of neural networks](img/B18949_06_001.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Basic architecture of neural networks
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following list, we explain each layer of the model in more detail:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: This is where the network receives its input. If the network
    is designed to process an image with dimensions of 28x28 pixels, for instance,
    there would be 784 neurons in the input layer, each representing the value of
    one pixel.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer(s)**: These are the layers between the input and output layers.
    Each neuron in a hidden layer takes the outputs of the neurons from the previous
    layer, multiplies each of these by the weight of the respective connection, and
    sums these values up. This sum is then passed through an “activation function”
    to introduce nonlinearity into the model, which helps the network learn complex
    patterns. There can be any number of hidden layers in a neural network, and a
    network with many hidden layers is often referred to as a “deep” neural network.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: This is the final layer in the network. The neurons in this
    layer produce the final output of the network. For a classification problem, for
    instance, you might design the network to have one output neuron for each class
    in the problem, with each neuron outputting a value indicating the probability
    that the input belongs to its respective class.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neurons in the network are interconnected. The weights of these connections,
    which are initially set to random values, represent what the network has learned
    once it has been trained on data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, an algorithm such as backpropagation is used to
    adjust the weights of the connections in the network in response to the difference
    between the network’s output and the desired output. This process is repeated
    many times, and the network gradually improves its performance on the training
    data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: To provide a simple visual idea, imagine three sets of circles (representing
    neurons) arranged in columns (representing layers). The first column is the input
    layer, the last column is the output layer and any columns in between are the
    hidden layers. Then, imagine lines connecting every circle in each column to every
    circle in the next column, representing the weighted connections between neurons.
    That’s a basic visual representation of a neural network.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: In the next part, we are going to describe the common terms related to neural
    networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Neural network common terms
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following subsections, we'll look at some of the most commonly used terms
    in the context of neural networks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Neuron (or node)
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the basic unit of computation in a neural network; typically, a simple
    computation involves inputs, weights, a bias, and an activation function. A neuron,
    also known as a node or unit, is a fundamental element in a neural network. It
    receives input from some other nodes or from an external source if the neuron
    is in the input layer. The neuron then computes an output based on this input.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Each input has an associated weight (*w*), which is assigned based on its relative
    importance to other inputs. The neuron applies a weight to the inputs, sums them
    up, and then applies an activation function to the sum plus a bias value (*b*).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a step-by-step breakdown:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighted sum**: Each input (*x*) to the neuron is multiplied by a corresponding
    weight (*w*). These weighted inputs are then summed together with a bias term
    (*b*). The bias term allows for the activation function to be shifted to the left
    or the right, helping the neuron model a wider range of patterns. Mathematically,
    this step can be represented as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>z</mi><mo>=</mo><mi>w</mi><mn>1</mn><mi
    mathvariant="normal">*</mi><mi>x</mi><mn>1</mn><mo>+</mo><mi>w</mi><mn>2</mn><mi
    mathvariant="normal">*</mi><mi>x</mi><mn>2</mn><mo>+</mo><mo>…</mo><mo>+</mo><mi>w</mi><mi
    mathvariant="normal">n</mi><mi mathvariant="normal">*</mi><mi>x</mi><mi mathvariant="normal">n</mi><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/297.png)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Activation function**: The result of the weighted sum is then passed through
    an activation function. The purpose of the activation function is to introduce
    nonlinearity into the output of a neuron. This nonlinearity allows the network
    to learn from errors and make adjustments, which is essential when it comes to
    performing complex tasks such as language translation or image recognition. Common
    choices for activation functions include the sigmoid function, hyperbolic **tangent**
    (**tanh**), and **rectified linear unit** (**ReLU**), among others.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the neuron is the result of the activation function. It serves
    as the input to the neurons in the next layer of the network.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The weights and bias in the neuron are learnable parameters. In other words,
    their values are learned over time as the neural network is trained on data:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Weights**: The strength or amplitude of the connection between two neurons.
    During the training phase, the neural network learns the correct weights that
    better map inputs to outputs. Weight is used in the neuron, as explained previously.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias**: An additional parameter in the neuron that allows for the activation
    function to be shifted to the left or right, which can be critical for successful
    learning (also used in the neuron).'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The function (in each neuron) that determines the output a neuron should produce
    given its input is called an activation function. Common examples include sigmoid,
    ReLU and tanh.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the most common types of activation functions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid function**: This is where we’re essentially classifying the input
    as either 0 or 1\. The sigmoid function takes a real-valued input and squashes
    it to range between 0 and 1\. It’s often used in the output layer of a binary
    classification network:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mfrac></mrow></mrow></mrow></math>](img/298.png)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'However, it has two major drawbacks: **the vanishing gradients problem** (gradients
    are very small for large positive or negative inputs, which can slow down learning
    during backpropagation) and the **outputs are** **not zero-centered**.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Hyperbolic tanh function**: The tanh function also takes a real-valued input
    and squashes it to range between -1 and 1\. Unlike the sigmoid function, its output
    is zero-centered because its range is symmetric around the origin:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mo>(</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow><mrow><mo>(</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>−</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mfrac></mrow></mrow></mrow></math>](img/299.png)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: It also suffers from the vanishing gradients problem, as does the sigmoid function.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ReLU function**: The ReLU function has become very popular in recent years.
    It computes the function as follows:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/300.png)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: In other words, the activation is simply the input if the input is positive;
    otherwise, it’s zero.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It doesn’t activate all the neurons at the same time, meaning that the neurons
    will only be deactivated if the output of the linear transformation is less than
    0\. This makes the network sparse and efficient. However, ReLU units can be fragile
    during training and can “die” (they stop learning completely) if a large gradient
    flows through them.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Leaky ReLU**: Leaky ReLU is a variant of ReLU that addresses the “dying ReLU”
    problem. Instead of defining the function as *0* for negative *x*, we define it
    as a small linear component of *x*:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mn>0.01</mn><mi>x</mi><mo>,</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/301.png)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This allows the function to “leak” some information when the input is negative
    and helps to mitigate the dying ReLU problem.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Exponential linear unit (ELU)**: ELU is also a variant of ReLU that modifies
    the function to be a non-zero value for negative *x*, which can help the learning
    process:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: f(x) = x if x > 0, else
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: α(exp(x) − 1)
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here alpha (*α*) is a constant that defines function smoothness when inputs
    are negative. ELU tends to converge cost to zero faster and produce more accurate
    results. However, it can be slower to compute because of the use of the exponential
    operation.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Softmax function**: The softmax function is often used in the output layer
    of a classifier where we’re trying to assign the input to one of several classes.
    It gives the probability that any given input belongs to each of the possible
    classes:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow></mrow></math>](img/302.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: The denominator normalizes the probabilities, so they all sum up to 1 across
    all classes. The softmax function is also used in multinomial logistical regression.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Each of these activation functions has pros and cons, and the choice of activation
    function can depend on the specific application and context of the problem at
    hand.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Layer
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A set of neurons that process signals at the same level of abstraction. The
    first layer is the input layer, the last layer is the output layer, and all layers
    in between are called hidden layers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Epoch
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of training a neural network, an epoch is a term used to denote
    one complete pass through the entire training dataset. During an epoch, the neural
    network’s weights are updated in an attempt to minimize the loss function.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The number of epochs hyperparameter sets how many times the deep learning algorithm
    processes the entire training dataset. Too many epochs can cause overfitting,
    where the model performs well on training data but poorly on new data. Conversely,
    training for too few epochs may mean the model is underfitting—it could improve
    with further training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to note that the concept of an epoch is more relevant in
    the batch and mini-batch variants of gradient descent. In stochastic gradient
    descent, the model’s weights are updated after seeing each individual example,
    so the concept of an epoch is less straightforward.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number of training instances used in one iteration. Batch size refers to
    the number of training examples used in one iteration.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'When you start training a neural network, you have a couple of options for
    how you feed your data into the model:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch gradient descent**: Here, the entire training dataset is used to compute
    the gradient of the loss function for each iteration of the optimizer (as with
    gradient descent). In this case, the batch size is equal to the total number of
    examples in the training dataset.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent (SGD)**: SGD uses a single example at each iteration
    of the optimizer. Therefore, the batch size for SGD is *1*.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-batch gradient descent**: This is a compromise between batch gradient
    descent and SGD. In mini-batch gradient descent, the batch size is usually between
    10 and 1,000 and is chosen depending on the computational resources you have.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch size can significantly impact the learning process. Larger batch sizes
    result in faster progress in training but don’t always converge as fast. Smaller
    batch sizes update the model frequently but the progress in training is slower.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, smaller batch sizes have a regularizing effect and can help the model
    generalize better, leading to better performance on unseen data. However, using
    a batch size that is too small can lead to unstable training, less accurate estimates
    of the gradient, and, ultimately, a model with worse performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the right batch size is a matter of trial and error and depends on
    the specific problem and the computational resources at hand:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterations**: The number of batches of data the algorithm has seen (or the
    number of passes it has made on the dataset).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: A hyperparameter that controls the speed of convergence
    of the learning algorithm by adjusting the weight update rate based on the loss
    gradient.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function (cost function)**: The loss function evaluates the neural network’s
    performance on the dataset. Higher deviations between predictions and actual results
    result in a larger output from the loss function. The goal is to minimize this
    output, which will give the model more accurate predictions.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backpropagation**: The primary algorithm for performing gradient descent
    on neural networks. It calculates the gradient of the loss function at the output
    layer and distributes it back through the layers of the network, updating the
    weights and biases in a way that minimizes the loss.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: A situation where a model learns the detail and noise in the
    training data to the extent that it performs poorly on new, unseen data.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Underfitting**: A situation where a model is too simple to learn the underlying
    structure of the data and, thus, performs poorly on both training and new data.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: A technique used to prevent overfitting by adding a penalty
    term to the loss function, which, in turn, constrains the weights of the network.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: A regularization technique where randomly selected neurons are
    ignored during training, which helps to prevent overfitting.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CNN**: A type of neural network well-suited to image processing and computer
    vision tasks.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RNN**: A type of neural network designed to recognize patterns in sequences
    of data, such as time series or text.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to the architecture of different neural networks next.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of different neural networks
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks come in various types, each with a specific architecture suited
    to a different kind of task. The following list contains general descriptions
    of some of the most common types:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedforward neural network (FNN)**: This is the most straightforward type
    of neural network. Information in this network moves in one direction only, from
    the input layer through any hidden layers to the output layer. There are no cycles
    or loops in the network; it’s a straight, “feedforward” path.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Feedforward neural network](img/B18949_06_002.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Feedforward neural network
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '**Multilayer perceptron (MLP)**: An MLP is a type of feedforward network that
    has at least one hidden layer in addition to its input and output layers. The
    layers are fully connected, meaning each neuron in a layer connects with every
    neuron in the next layer. MLPs can model complex patterns and are widely used
    for tasks such as image recognition, classification, speech recognition, and other
    types of machine learning tasks. The MLP is a feedforward network with layers
    of neurons arranged sequentially. Information flows from the input layer through
    hidden layers to the output layer in one direction:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Multilayer perceptron](img/B18949_06_003.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Multilayer perceptron
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '**CNN**: A CNN is particularly well-suited to tasks involving spatial data,
    such as images. Its architecture includes three main types of layers: convolutional
    layers, pooling layers, and fully connected layers. The convolutional layers apply
    a series of filters to the input, which allows the network to automatically and
    adaptively learn spatial hierarchies of features. Pooling layers decrease the
    spatial size of the representation, thereby reducing parameters and computation
    in the network to control overfitting and decrease the computation cost in the
    following layers. Fully connected layers get the output of the pooling layer and
    conduct high-level reasoning on the output.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 6.4 – \uFEFFConvolutional neural network](img/B18949_06_004.jpg)"
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Convolutional neural network
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent neural network (RNN)**: Unlike feedforward networks, RNNs have
    connections that form directed cycles. This architecture allows them to use information
    from their previous outputs as inputs, making them ideal for tasks involving sequential
    data, such as time series prediction or NLP. A significant variation of RNNs is
    the LSTM network, which uses special units in addition to standard units. RNN
    units include a "memory cell" that can maintain information in memory for long
    periods of time, a feature that is particularly useful for tasks that require
    learning from long-distance dependencies in the data, such as handwriting or speech
    recognition.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Recurrent neural network](img/B18949_06_005.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Recurrent neural network
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**Autoencoder (AE)**: An AE is a type of neural network used to learn the efficient
    coding of input data. It has a symmetrical architecture and is designed to apply
    backpropagation, setting the target values to be equal to the inputs. Autoencoders
    are typically used for feature extraction, learning representations of data, and
    dimensionality reduction. They’re also used in generative models, noise removal,
    and recommendation systems.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器 (AE)**: AE 是一种用于学习输入数据有效编码的神经网络。它具有对称架构，并设计用于应用反向传播，将目标值设置为等于输入。自动编码器通常用于特征提取、学习数据的表示和降维。它们还用于生成模型、噪声去除和推荐系统。'
- en: '![Figure 6.6 – Autoencoder architecture](img/B18949_06_006.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 自动编码器架构](img/B18949_06_006.jpg)'
- en: Figure 6.6 – Autoencoder architecture
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 自动编码器架构
- en: '**Generative adversarial network (GAN)**: A GAN consists of two parts, a generator
    and a discriminator, which are both neural networks. The generator creates data
    instances that aim to come from the same distribution as the training dataset.
    The discriminator’s goal is to distinguish between instances from the true distribution
    and instances from the generator. The generator and the discriminator are trained
    together, with the goal that the generator produces better instances as training
    progresses, whereas the discriminator becomes better at distinguishing true instances
    from generated ones.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络 (GAN)**: GAN 由两部分组成，一个生成器和一个判别器，它们都是神经网络。生成器创建数据实例，旨在与训练数据集的分布相同。判别器的目标是区分来自真实分布的实例和来自生成器的实例。生成器和判别器一起训练，目标是随着训练的进行，生成器产生更好的实例，而判别器则变得更好地区分真实实例和生成实例。'
- en: '![Figure 6.7 – Generative adversarial network in computer vision](img/B18949_06_007.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 计算机视觉中的生成对抗网络](img/B18949_06_007.jpg)'
- en: Figure 6.7 – Generative adversarial network in computer vision
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 计算机视觉中的生成对抗网络
- en: These are just a few examples of neural network architectures, and many variations
    and combinations exist. The architecture you choose for a task will depend on
    the specific requirements and constraints of your task.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是神经网络架构的几个例子，存在许多变体和组合。你为任务选择架构将取决于任务的具体要求和限制。
- en: The challenges of training neural networks
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络训练的挑战
- en: 'Training neural networks is a complex task and comes with challenges during
    the training, such as local minima and vanishing/exploding gradients, as well
    as computational costs and interpretability. All challenges are explained in detail
    in the following points:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络是一项复杂的工作，在训练过程中会面临挑战，如局部最小值和梯度消失/爆炸，以及计算成本和可解释性。所有挑战都在以下各点中详细解释：
- en: '**Local minima**: The objective of training a neural network is to find the
    set of weights that minimizes the loss function. This is a high-dimensional optimization
    problem, and there are many points (sets of weights) where the loss function has
    local minima. A suboptimal local minimum is a point where the loss is lower than
    for the nearby points but higher than the global minimum, which is the overall
    lowest possible loss. The training process can get stuck in such suboptimal local
    minima. It’s important to remember that the local minima problem exists even in
    convex loss functions due to the discrete representation that is a part of digital
    computation.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部最小值**: 训练神经网络的目的是找到使损失函数最小化的权重集。这是一个高维优化问题，存在许多点（权重集）的损失函数具有局部最小值。次优局部最小值是损失低于附近点的点，但高于全局最小值，全局最小值是整体可能的最小损失。训练过程可能会陷入这种次优局部最小值。重要的是要记住，即使是在凸损失函数中，由于数字计算中的一部分离散表示，局部最小值问题也存在。'
- en: '**Vanishing/exploding gradients**: This is a difficulty encountered, especially
    when training deep neural networks. The gradients of the loss function may become
    very small (vanish) or very large (explode) in deeper layers of the network during
    the backpropagation process. Vanishing gradients make it hard for the network
    to learn from the data because the weight updates become very small. Exploding
    gradients can cause the training process to fail because weight updates become
    too large, and the loss becomes undefined (e.g., NaN).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失/爆炸**：这是在训练深度神经网络时遇到的一个困难。在反向传播过程中，损失函数的梯度可能在网络的深层中变得非常小（消失）或非常大（爆炸）。梯度消失使得网络难以从数据中学习，因为权重更新变得非常小。梯度爆炸可能导致训练过程失败，因为权重更新变得过大，损失变得未定义（例如，NaN）。'
- en: '**Overfitting**: One of the common problems in training machine learning models
    is when our model is too complex, and we train it too much. In this case, the
    model learns even the noises in the training data and works very well on training
    data but poorly on the unseen test data.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：在训练机器学习模型时，一个常见的问题是我们的模型过于复杂，训练过度。在这种情况下，模型甚至学习了训练数据中的噪声，在训练数据上表现良好，但在未见过的测试数据上表现不佳。'
- en: '**Underfitting**: Conversely, underfitting occurs when the model is too simple
    and can’t capture the underlying structure of the data. Both overfitting and underfitting
    can be mitigated by using proper model complexity, regularization techniques,
    and a sufficient amount of training data.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：相反，当模型过于简单，无法捕捉数据的潜在结构时，就会发生欠拟合。通过使用适当的模型复杂度、正则化技术和足够数量的训练数据，可以减轻过拟合和欠拟合。'
- en: '**Computational resources**: Training neural networks, particularly deep networks,
    requires significant computational resources (CPU/GPU power and memory). They
    also often require a large amount of training data to perform well, which can
    be a problem when such data are not available.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源**：训练神经网络，尤其是深度网络，需要大量的计算资源（CPU/GPU功率和内存）。它们通常还需要大量的训练数据才能表现良好，当这些数据不可用时，这可能成为一个问题。'
- en: '**Lack of interpretability**: While not strictly a training issue, the lack
    of interpretability of neural networks is a significant problem. They are often
    referred to as “black boxes” because it is challenging to understand why they
    are making the predictions they do.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏可解释性**：虽然这不是一个严格意义上的训练问题，但神经网络缺乏可解释性是一个重大问题。它们通常被称为“黑盒”，因为很难理解它们为什么会做出这样的预测。'
- en: '**Difficulty in selecting appropriate architecture and hyperparameters**: There
    are many types of neural network architectures to choose from (such as CNN and
    RNN), and each has a set of hyperparameters that need to be tuned (such as learning
    rate, batch size, number of layers, and number of units per layer). Selecting
    the best architecture and tuning these hyperparameters for a given problem can
    be a challenging and time-consuming task.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择适当架构和超参数的困难**：有众多类型的神经网络架构可供选择（例如CNN和RNN），每种架构都有一组需要调整的超参数（例如学习率、批量大小、层数和每层的单元数）。为特定问题选择最佳架构并调整这些超参数可能是一个具有挑战性和耗时的工作。'
- en: '**Data preprocessing**: Neural networks often require the input data to be
    in a specific format. For instance, data might need to be normalized, categorical
    variables might need to be one-hot encoded, and missing values might need to be
    imputed. This preprocessing can be a complex and time-consuming step.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理**：神经网络通常需要输入数据以特定格式。例如，数据可能需要归一化，分类变量可能需要独热编码，缺失值可能需要填充。这个预处理步骤可能既复杂又耗时。'
- en: These challenges make training neural networks a non-trivial task, often requiring
    a combination of technical expertise, computational resources, and trial and error.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战使得训练神经网络成为一个非平凡的任务，通常需要技术专长、计算资源和试错法的结合。
- en: Language models
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型
- en: A language model is a statistical model in NLP that is designed to learn and
    understand the structure of human language. More specifically, it is a probabilistic
    model that is trained to estimate the likelihood of words when provided with a
    given word scenario. For instance, a language model could be trained to predict
    the next word in a sentence, given the previous words.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是NLP中的一种统计模型，旨在学习和理解人类语言的结构。更具体地说，它是一种概率模型，经过训练可以估计在给定一个单词场景的情况下单词的可能性。例如，语言模型可以被训练来预测句子中的下一个单词，给定前面的单词。
- en: Language models are fundamental to many NLP tasks. They are used in machine
    translation, speech recognition, part-of-speech tagging, and named entity recognition,
    among other things. More recently, they have been used to create conversational
    AI models such as chatbots and personal assistants and to generate human-like
    text.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Traditional language models were often based on explicitly statistical methods,
    such as n-gram models, which consider only the previous n words when predicting
    the next word, or **hidden Markov** **models** (**HMMs**).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: More recently, neural networks have become popular for creating language models,
    leading to the rise of neural language models. These models use the power of neural
    networks to consider the context of each word when making predictions, resulting
    in higher accuracy and fluency. Examples of neural language models include RNNs,
    the transformer model, and various transformer-based architectures such as BERT
    and GPT.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Language models are essential for understanding, generating, and interpreting
    human language in a computational setting, and they play a vital role in many
    applications of NLP.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are several motivations for using language models:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine translation**: Language models are a crucial component in systems
    that translate text from one language to another. They can assess the fluency
    of translated sentences and help choose between multiple possible translations.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition**: Language models are used in speech recognition systems
    to help distinguish between words and phrases that sound similar. By predicting
    what word is likely to come next in a sentence, they can improve the accuracy
    of transcription.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information retrieval**: When you search for something on the internet, language
    models help to determine what documents are relevant to your query. They can understand
    the semantic similarity between your search terms and potential results.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generation**: Language models can generate human-like text, which is
    useful in various applications such as chatbots, writing assistants, and content
    creation tools. For example, a chatbot can use a language model to generate appropriate
    responses to user queries.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: By understanding the structure of language, language
    models can help determine whether the sentiment of a piece of text is positive,
    negative, or neutral. This is useful in areas such as social media monitoring,
    product reviews, and customer feedback.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grammar checking**: Language models can predict what word should come next
    in a sentence, which can help identify grammatical errors or awkward phrasing.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Named entity recognition**: Language models can help identify named entities
    in text, such as people, organizations, locations, and more. This can be useful
    for tasks such as information extraction and automated summarization.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understanding context**: Language models, especially recent models based
    on **DL**, such as transformers, are excellent at understanding the context of
    words and sentences. This capability is vital for many **NLP** tasks, such as
    question answering, summarization, and dialogue systems.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All these motivations stem from a central theme: language models help machines
    understand and generate human language more effectively, which is crucial for
    many applications in today’s data-driven world.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we introduce the different types of learning and then
    explain how one can use self-supervised learning to train language models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semi-supervised learning is a type of ML approach that utilizes both labeled
    and unlabeled data for training. It is particularly useful when you have a small
    amount of labeled data and a large amount of unlabeled data. The strategy here
    is to use the labeled data to train an initial model and then use this model to
    predict labels for the unlabeled data. The model is then retrained using the newly
    labeled data, improving its accuracy in the process.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning, on the other hand, involves training models entirely
    on unlabeled data. The goal here is to find underlying patterns or structures
    in the data. Unsupervised learning includes techniques such as clustering (where
    the aim is to group similar instances together) and dimensionality reduction (where
    the aim is to simplify the data without losing too much information).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Using self-supervised learning to train language models
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-supervised learning is a form of unsupervised learning where the data provides
    the supervision. In other words, the model learns to predict certain parts of
    the input data from other parts of the same input data. It does not require explicit
    labels provided by humans, hence the term “self-supervised.”
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: In the context of language models, self-supervision is typically implemented
    by predicting parts of a sentence when given other parts. For example, given the
    sentence “The cat is on the __,” the model would be trained to predict the missing
    word (“mat,” in this case).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a couple of popular self-supervised learning strategies for training
    language models next.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Masked language modeling (MLM)
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This strategy, used in the training of BERT, randomly masks some percentage
    of the input tokens and tasks the model with predicting the masked words based
    on the context provided by the unmasked words. For instance, in the sentence “The
    cat is on the mat,” we could mask “cat,” and the model’s job would be to predict
    this word. Please note that more than one word can also be masked.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the objective of an MLM is to maximize the following likelihood:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/303.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: where *w*_i is a masked word, *w*_{-i} are the non-masked words, and *θ* represents
    the model parameters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Autoregressive language modeling
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In autoregressive language modeling, which is used in models such as GPT, the
    model predicts the next word in a sentence given all the preceding words. It’s
    trained to maximize the likelihood of a word given its previous words in the sentence.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The objective of an autoregressive language model is to maximize
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>L</mi><mo>=</mo><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>log</mi><mfenced
    open="(" close=")"><mrow><mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></mrow></mrow></mrow></math>](img/304.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: where *w_*i is the current word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/305.png)
    are the previous words, and *θ* represents the model parameters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: These strategies enable language models to obtain a rich understanding of language
    syntax and semantics directly from raw text without the need for explicit labels.
    The models can then be fine-tuned for various tasks such as text classification,
    sentiment analysis, and more, leveraging the language understanding gained from
    the self-supervised pretraining phase.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning is an ML technique where a pretrained model is reused as the
    starting point for a different but related problem. Compared to traditional ML
    approaches, where you start with initializing your model with random weights,
    transfer learning has the advantage of kick-starting the learning process from
    patterns that have been learned from a related task, which can both speed up the
    training process and improve the performance of the model, especially when you
    have limited labeled training data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: In transfer learning, a model is typically trained on a large-scale task, and
    then parts of the model are used as a starting point for another task. The large-scale
    task is often chosen to be broad enough that the learned representations are useful
    for many different tasks. This process works particularly well when the input
    data for both tasks are of the same type and the tasks are related.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to apply transfer learning, and the best approach can
    depend on how much data you have for your task and how similar your task is to
    the original task the model was trained on.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pretrained model acts as a feature extractor. You remove the last layer
    or several layers of the model, leaving the rest of the network intact. Then,
    you pass your data through this truncated model and use the output as input to
    a new, smaller model that is trained for your specific task.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You use the pretrained model as a starting point and update all or some of the
    model’s parameters for your new task. In other words, you continue the training
    where it left off, allowing the model to adjust from generic feature extraction
    to features more specific to your task. Often, a lower learning rate is used during
    fine-tuning to avoid overwriting the prelearned features entirely during training.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is a powerful technique that can be used to improve the performance
    of ML models. It is particularly useful for tasks where there are limited labeled
    data available. It is commonly used in DL applications. For instance, it’s almost
    a standard in image classification problems where pretrained models on ImageNet,
    a large-scale annotated image dataset (ResNet, VGG, Inception, and so on), are
    used as the starting point. The features learned by these models are generic for
    image classification and can be fine-tuned on a specific image classification
    task with a smaller amount of data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of how transfer learning can be used:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: A model trained to classify images of cats and dogs can be used to fine-tune
    a model to classify images of other animals, such as birds or fish
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model trained to translate text from English to Spanish can be used to fine-tune
    a model to translate text from Spanish to French
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model trained to predict the price of a house can be used to fine-tune a model
    to predict the price of a car
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, in natural language processing, large pretrained models, such as
    BERT or GPT, are often used as the starting point for a wide range of tasks. These
    models are pretrained on a large corpus of text and learn a rich representation
    of language that can be fine-tuned for specific tasks such as text classification,
    sentiment analysis, question answering, and more.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Understanding transformers
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers are a type of neural network architecture that was introduced in
    a paper called *Attention is All You Need* by Ashish Vaswani, Noam Shazeer, Niki
    Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia
    Polosukhin (*Advances in neural information processing systems 30* (2017), Harvard).
    They have been very influential in the field of NLP and have formed the basis
    for state-of-the-art models such as BERT and GPT.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The key innovation in transformers is the self-attention mechanism, which allows
    the model to weigh the relevance of each word in the input when producing an output,
    thereby considering the context of each word. This is unlike previous models such
    as RNNs or RNNs, which process the input sequentially and, therefore, have a harder
    time capturing the long-range dependencies between words.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of transformers
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A transformer is composed of an encoder and a decoder, both of which are made
    up of several identical layers, as shown in *Figure 6**.8*. Each layer in the
    encoder contains two sub-layers: a self-attention mechanism and a position-wise
    fully connected feedforward network. A residual connection is employed around
    each of the two sub-layers, followed by layer normalization:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Self-attention mechanism](img/B18949_06_008.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Self-attention mechanism
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, each layer in the decoder has three sub-layers. The first is a self-attention
    layer, the second is a cross-attention layer that attends to the output of the
    encoder stack, and the third is a position-wise fully connected feedforward network.
    Like the encoder, each of these sub-layers has a residual connection around it,
    followed by layer normalization. Please note that in the figure, just one head
    is being shown, and we can have multiple heads working in parallel (*N* heads).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention mechanism
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The self-attention mechanism, or scaled dot-product attention, calculates the
    relevance of each word in the sequence to the current word being processed. The
    input to the self-attention layer is a sequence of word embeddings, each of which
    is split into a **query** (*Q*), a **key** (*K*), and a **value** (*V*) using
    separately learned linear transformations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'The attention score for each word is then calculated as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: "![<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow><mrow><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>Q</mi><mi>K</mi><mi\
    \ mathvariant=\"normal\">\uFEFF</mi><mi mathvariant=\"normal\">T</mi><mo>/</mo><mi>s</mi><mi>q</mi><mi>r</mi><mi>t</mi><mo>(</mo><mi>d</mi><mo>_</mo><mi\
    \ mathvariant=\"normal\">k</mi><mo>)</mo><mo>)</mo><mi>V</mi></mrow></mrow></mrow></math>](img/306.png)"
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Where *d_k* is the dimensionality of the queries and keys, which is used to
    scale the dot product to prevent it from growing too large. The softmax operation
    ensures that the attention scores are normalized and sum to 1\. These scores represent
    the weight given to each word’s value when producing the output for the current
    word.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The output of the self-attention layer is a new sequence of vectors, where the
    output for each word is a weighted sum of all the input values, with the weights
    determined by the attention scores.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the self-attention mechanism does not take into account the position of
    the words in the sequence, the transformer adds a positional encoding to the input
    embeddings at the bottom of the encoder and decoder stacks. This encoding is a
    fixed function of the position and allows the model to learn to use the order
    of the words.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: In the original transformer paper, positional encoding is a sinusoidal function
    of the position and the dimension, although learned positional encodings have
    also been used effectively.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Applications of transformers
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since their introduction, transformers have been used to achieve state-of-the-art
    results on a wide range of NLP tasks, including machine translation, text summarization,
    sentiment analysis, and more. They have also been adapted for other domains, such
    as computer vision and reinforcement learning.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of transformers has led to a shift in the NLP field towards
    pretraining large transformer models on a large corpus of text and then fine-tuning
    them on specific tasks, which is an effective form of transfer learning. This
    approach has been used in models such as BERT, GPT-2, GPT-3, and GPT-4.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Learning more about large language models
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large language models are a class of ML models that have been trained on a broad
    range of internet text.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The term “large” in “large language models” refers to the number of parameters
    that these models have. For example, GPT-3 has 175 billion parameters. These models
    are trained using self-supervised learning on a large corpus of text, which means
    they predict the next word in a sentence (such as GPT) or a word based on surrounding
    words (such as BERT, which is also trained to predict whether a pair of sentences
    is sequential). Because they are exposed to such a large amount of text, these
    models learn grammar, facts about the world, reasoning abilities, and also biases
    in the data they’re trained on.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: “大型语言模型”中的“大型”一词指的是这些模型所拥有的参数数量。例如，GPT-3有1750亿个参数。这些模型使用自监督学习在大量的文本语料库上进行训练，这意味着它们预测句子中的下一个单词（例如GPT）或基于周围单词的单词（例如BERT，它也被训练来预测一对句子是否连续）。由于它们接触到了如此大量的文本，这些模型学习了语法、关于世界的知识、推理能力，以及它们在训练数据中存在的偏见。
- en: These models are transformer-based, meaning they leverage the transformer architecture,
    which uses self-attention mechanisms to weigh the importance of words in input
    data. This architecture allows these models to process long-range dependencies
    in text, making them very effective for a wide range of NLP tasks.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型基于转换器架构，这意味着它们利用了转换器架构，该架构使用自注意力机制来衡量输入数据中单词的重要性。这种架构使得这些模型能够处理文本中的长距离依赖关系，使它们在广泛的自然语言处理任务中非常有效。
- en: Large language models can be fine-tuned on specific tasks to achieve high performance.
    Fine-tuning involves additional training on a smaller, task-specific dataset and
    allows the model to adapt its general language understanding abilities to the
    specifics of the task. This approach has been used to achieve state-of-the-art
    results on many NLP benchmarks.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型可以通过在特定任务上进行微调来实现高性能。微调涉及在较小的、特定任务的语料库上进行额外的训练，这使得模型能够将其通用的语言理解能力适应到任务的特定细节。这种方法已被用于在许多自然语言处理基准测试中实现最先进的结果。
- en: While large language models have demonstrated impressive abilities, they also
    raise important challenges. For example, because they’re trained on internet text,
    they can reproduce and amplify biases present in the data. They can also generate
    outputs that are harmful or misleading. Additionally, due to their size, these
    models require significant computational resources to train and deploy, which
    raises issues around cost and environmental impact.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型展示了令人印象深刻的能力，但它们也提出了重要的挑战。例如，由于它们是在互联网文本上训练的，它们可以复制和放大数据中存在的偏见。它们还可以生成有害或误导性的输出。此外，由于它们的规模，这些模型在训练和部署时需要大量的计算资源，这引发了成本和环境影响的问题。
- en: Despite these challenges, large language models represent a significant advance
    in the field of AI and are a powerful tool for a wide range of applications, including
    translation, summarization, content creation, question answering, and more.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，大型语言模型在人工智能领域代表了重大进步，并且是广泛应用于翻译、摘要、内容创作、问答等众多领域的强大工具。
- en: The challenges of training language models
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练语言模型的挑战
- en: 'Training large language models is a complex and resource-intensive task that
    poses several challenges. Here are some of the key issues:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型语言模型是一项复杂且资源密集型的工作，面临着诸多挑战。以下是其中一些关键问题：
- en: '**Computational resources**: The training of large language models requires
    substantial computational resources. These models have billions of parameters
    that need to be updated during training, which involves performing a large amount
    of computation over an extensive dataset. This computation is usually carried
    out on high-performance GPUs or **tensor processing units** (**TPUs**), and the
    costs associated can be prohibitive.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源**：大型语言模型的训练需要大量的计算资源。这些模型拥有数十亿个参数，需要在训练过程中进行更新，这涉及到在庞大的数据集上执行大量的计算。这种计算通常在高性能GPU或**张量处理单元**（**TPUs**）上执行，相关的成本可能非常昂贵。'
- en: '**Memory limitations**: As the size of the model increases, the amount of memory
    required to store the model parameters, intermediate activations, and gradients
    during training also increases. This can lead to memory issues on even the most
    advanced hardware. Techniques such as model parallelism, gradient checkpointing,
    and offloading can be used to mitigate these issues, but they add complexity to
    the training process.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset size and quality**: Large language models are trained on extensive
    text corpora. Finding, cleaning, and structurally organizing such massive datasets
    can be challenging. Moreover, the quality of the dataset directly impacts the
    performance of the model. Since these models learn from the data they’re trained
    on, biases or errors in the data can lead to a biased or error-prone model.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: While large models have a high capacity to learn complex patterns,
    they can also be overfitted to the training data, especially when the amount of
    available data is limited compared to the size of the model. Overfitting leads
    to poor generalization of unseen data. Regularization techniques, such as weight
    decay, dropout, and early stopping, can be used to combat overfitting.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training stability**: As models get larger, stably training them becomes
    more difficult. The challenges include managing learning rates and batch sizes
    and dealing with issues such as vanishing or exploding gradients.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation and fine-tuning**: Evaluating the performance of these models
    can also be challenging due to their size. Moreover, fine-tuning these models
    on a specific task can be tricky, as it can lead to “catastrophic forgetting,”
    where the model forgets the pretraining knowledge.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical and safety concerns**: Large language models can generate content
    that is harmful or inappropriate. They can also propagate and amplify biases present
    in the training data. These issues necessitate the development of robust methods
    to control the behavior of the model, both during training and at runtime.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite these challenges, progress continues in the field of large language
    models. Researchers are developing new strategies to mitigate these issues and
    to train large models more effectively and responsibly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Specific designs of language models
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we are going to explain two popular architectures of language models,
    BERT and GPT, in detail.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BERT, which we mentioned already and will now expand on, is a transformer-based
    ML technique for NLP tasks. It was developed by Google and introduced in a paper
    by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova titled *Bert:
    Pre-training of deep bidirectional transformers for language understanding*, arXiv
    preprint arXiv:1810.04805 (2018).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: BERT is designed to pretrain deep bidirectional representations from the unlabeled
    text by joint conditioning on both left and right contexts in all layers. This
    is in contrast to previous methods, such as GPT and ELMo, which pretrain text
    representations from only the left context or from left and right contexts separately.
    This bi-directionality allows BERT to understand the context and the semantic
    meaning of a word more accurately.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: BERT’s design
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: BERT is based on the transformer model architecture, which is shown in *Figure
    6**.8*, originally introduced by Vaswani et al. in the paper *Attention is All
    You Need*. The model architecture consists of stacked self-attention and point-wise
    fully connected layers.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT comes in two sizes: **BERT Base** and **BERT Large**. BERT Base is composed
    of 12 transformer layers, each with 12 self-attention heads, and a total of 110
    million parameters. BERT Large is much bigger and has 24 transformer layers, each
    with 16 self-attention heads, for a total of 340 million parameters.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT’s training process involves two steps: **pretraining** and **fine-tuning**.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The very first step in training or using a language model is to create or load
    its dictionary. We usually use a tokenizer to achieve this goal.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to use the language models efficiently, we need to use a tokenizer
    that converts the input text into a limited number of tokens. Subword tokenization
    algorithms, such as **byte pair encoding** (**BPE**), **unigram language model**
    (**ULM**), and **WordPiece**, split words into smaller subword units. This is
    useful for handling out-of-vocabulary words and allows the model to learn meaningful
    representations for subword parts that often carry semantic meaning.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: The BERT tokenizer is a critical component of the BERT model, performing the
    initial preprocessing of text data necessary for input into the model. BERT uses
    WordPiece tokenization, a subword tokenization algorithm that breaks words into
    smaller parts, allowing BERT to handle out-of-vocabulary words, reduce the size
    of the vocabulary, and deal with the richness and diversity of languages.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a detailed breakdown of how the BERT tokenizer works:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic tokenization**: First, the BERT tokenizer performs basic tokenization,
    breaking text into individual words by splitting on whitespace and punctuation.
    This is similar to what you might find in other tokenization methods.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**WordPiece tokenization**: After basic tokenization, the BERT tokenizer applies
    WordPiece tokenization. This step breaks words into smaller subword units or “WordPieces.”
    If a word isn’t in the BERT vocabulary, the tokenizer will iteratively break the
    word down into smaller sub words until it finds a match in the vocabulary or until
    it has to resort to character-level representation.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, the word “unhappiness” might be broken down into two WordPieces:
    “un” and “##happiness”. The “##” symbol is used to denote sub-words that are part
    of a larger word and not a whole word on their own.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Special tokens addition**: The **BERT** tokenizer then adds special tokens
    necessary for specific **BERT** functionalities. The [**CLS**] token is appended
    at the beginning of each sentence, serving as an aggregate representation for
    classification tasks. The [**SEP**] token is added at the end of each sentence
    to signify sentence boundaries. If two sentences are inputted (for tasks that
    require sentence pairs), they are separated by this [**SEP**] token.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Token to ID conversion**: Finally, each token is mapped to an integer ID
    corresponding to its index in the **BERT** vocabulary. These IDs are what the
    **BERT** model actually uses as input.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, in summary, the BERT tokenizer works by first tokenizing the text into words,
    then further breaking these words down into WordPieces (if necessary), adding
    special tokens, and finally converting these tokens into IDs. This process allows
    the model to understand and generate meaningful representations for a wide variety
    of words and sub-words, contributing to BERT’s powerful performance on various
    NLP tasks.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'During pretraining, **BERT** was trained on a large corpus of text (the entire
    English Wikipedia and BooksCorpus are used in the original paper). The model was
    trained to predict masked words in a sentence (masked language model) and to distinguish
    whether two sentences come in order in the text (next sentence prediction), as
    explained here:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked language model**: In this task, 15% of the words in a sentence are
    replaced by a [**MASK**] token, and the model is trained to predict the original
    word from the context provided by the non-masked words.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next sentence prediction**: When the model is given a pair of two sentences,
    it is also trained to predict whether sentence *B* is the next sentence following
    sentence *A*.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After pretraining, BERT can be fine-tuned on a specific task with a significantly
    smaller amount of training data. Fine-tuning involves adding an additional output
    layer to BERT and training the entire model end-to-end on the specific task. This
    approach has been shown to achieve state-of-the-art results on a wide range of
    NLP tasks, including question answering, named entity recognition, sentiment analysis,
    and more.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: BERT’s design and its pretraining/fine-tuning approach revolutionized the field
    of NLP and have led to a shift toward training large models on a broad range of
    data and then fine-tuning them on specific tasks.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: How to fine-tune BERT for text classification
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned, BERT has been pretrained on a large corpus of text data, and
    the learned representations can be fine-tuned for specific tasks, including text
    classification. Here is a step-by-step process on how to fine-tune BERT for text
    classification:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing input data**: BERT requires a specific format for input data.
    The sentences need to be tokenized into sub-words using BERT’s own tokenizer,
    and special tokens such as [CLS] (classification) and [SEP] (separation) need
    to be added. The [CLS] token is added at the beginning of each example and is
    used as the aggregate sequence representation for classification tasks. The [SEP]
    token is added at the end of each sentence to denote sentence boundaries. All
    sequences are then padded to a fixed length to form a uniform input.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理输入数据**：BERT需要特定的输入数据格式。句子需要使用BERT自己的分词器将句子分词成子词，并添加特殊标记，如[CLS]（分类）和[SEP]（分隔）。[CLS]标记添加在每个示例的开头，用作分类任务的聚合序列表示。[SEP]标记添加在每个句子的末尾，以表示句子边界。然后，所有序列都填充到固定长度，以形成一个统一的输入。'
- en: '**Loading the pretrained BERT model**: BERT has several pretrained models,
    and the right one should be chosen based on the task at hand. The models differ
    in terms of the size of the model and the language of the pretraining data. Once
    the pretrained BERT model is loaded, it can be used to create contextualized word
    embeddings for the input data.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载预训练BERT模型**：BERT有多个预训练模型，应根据任务选择正确的模型。这些模型在模型大小和预训练数据的语言方面有所不同。一旦加载预训练BERT模型，就可以用它为输入数据创建上下文化的词嵌入。'
- en: '**Adding a classification layer**: A classification layer, also known as the
    classification head, is added on top of the pretrained BERT model. This layer
    will be trained to make predictions for the text classification task. Usually,
    this layer is a fully connected neural network layer that takes the representation
    corresponding to the [CLS] token as input and outputs the probability distribution
    over the classes.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加分类层**：在预训练BERT模型之上添加一个分类层，也称为分类头。这个层将被训练以对文本分类任务进行预测。通常，这个层是一个全连接神经网络层，它以对应于[CLS]标记的表示作为输入，并输出类别的概率分布。'
- en: '**Fine-tuning the model**: Fine-tuning involves training the model on the specific
    task (in this case, text classification) using the labeled data. This process
    can be done in multiple ways. The more common approach is to update the weights
    of the pretrained BERT model and the newly added classification layer to minimize
    a loss function, typically the cross-entropy loss for classification tasks. It
    is important to use a lower learning rate during fine-tuning, as larger rates
    can destabilize the prelearned weights. Additionally, the number of recommended
    epochs is two to four, so the model learns the task but does not overfit. The
    benefit of this approach is that the model weights will be adjusted to perform
    well on specific tasks. Alternatively, we can freeze BERT layers and just update
    the classifier layer weights.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调模型**：微调涉及使用标记数据在特定任务（在这种情况下，文本分类）上训练模型。这个过程可以通过多种方式完成。更常见的方法是更新预训练BERT模型和新添加的分类层权重，以最小化损失函数，通常是分类任务的交叉熵损失。在微调期间使用较低的学习率很重要，因为较大的学习率可能会使预学习的权重不稳定。此外，建议的epoch数通常是两到四个，这样模型就能学习任务但不会过拟合。这种方法的好处是模型权重将被调整以在特定任务上表现良好。或者，我们可以冻结BERT层，只更新分类层权重。'
- en: '**Evaluating the model**: Once the model has been fine-tuned, it can be evaluated
    on a validation set to assess its performance. This involves calculating metrics
    such as accuracy, precision, recall, and F1 score. During the training and evaluation
    task, similar to other ML and DL models, we can perform hyperparameter tuning.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型**：一旦模型经过微调，就可以在验证集上评估其性能，以评估其性能。这包括计算准确率、精确率、召回率和F1分数等指标。在训练和评估任务期间，与其他ML和DL模型类似，我们可以执行超参数调整。'
- en: '**Applying the model**: The fine-tuned model can now be used to make predictions
    on new, unseen text data. As with the training data, this new data also need to
    be preprocessed into the format that BERT expects.'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用模型**：经过微调的模型现在可以用于对新、未见过的文本数据进行预测。与训练数据一样，这些新数据也需要预处理成BERT期望的格式。'
- en: Important note
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: Note that working with **BERT** requires considerable computational resources,
    as the model has a large number of parameters. A GPU is typically recommended
    for fine-tuning and applying BERT models. There are some models that are lighter
    than BERT with slightly lower performance, such as DistilBERT, that we can use
    in the case of being constrained by the computation or memory resources. Additionally,
    BERT is able to process 512 tokens, which limits the length of our input text.
    If we want to process longer text, Longformer or BigBird are good choices. What
    we explained here works for similar language models such as RoBERTa, XLNet, and
    so on.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: In summary, fine-tuning BERT for text classification involves preprocessing
    the input data, loading the pretrained BERT model, adding a classification layer,
    fine-tuning the model on the labeled data, and then evaluating and applying the
    model.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: We will demonstrate the preceding paradigm of fine-tuning BERT and then apply
    it at the end of this chapter. You will have the opportunity to employ it firsthand
    and adjust it to your needs.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**GPT-3**, short for **generative** **pretrained transformer 3**, is an autoregressive
    language model developed by OpenAI that uses DL techniques to generate human-like
    text. It is the third version of the GPT series. The GPT versions that followed
    it, GPT-3.5 and GPT-4, will be covered in the next chapter, as we will expand
    on large language models.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Design and architecture of GPT-3
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GPT-3 extends the transformer model architecture used by its predecessors. The
    architecture is based on a transformer model that uses layers of transformer blocks,
    where each block is composed of self-attention and feedforward neural network
    layers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 is massive compared to the previous versions. It consists of 175 billion
    ML parameters. These parameters are learned during the training phase, where the
    model learns to predict the next word in a sequence of words.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3’s transformer model is designed to process sequences of data (in this
    case, sequences of words or tokens in text), making it well-suited for language
    tasks. It processes input data sequentially from left to right and generates predictions
    for the next item in the sequence. This is the difference between BERT and GPT,
    where, in BERT, words from both sides are used to predict masked words, but in
    GPT, just the previous words are used for prediction, which makes it a good choice
    for generative tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining and fine-tuning
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similar to BERT and other transformer-based models, GPT-3 also involves a two-step
    process: **pretraining** and **fine-tuning**.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this phase, GPT-3 is trained on a large corpus of text data. It learns to
    predict the next word in a sentence. However, unlike BERT, which uses a bidirectional
    context for prediction, GPT-3 only uses the left context (i.e., the previous words
    in the sentence).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After the pretraining phase, GPT-3 can be fine-tuned on a specific task using
    a smaller amount of task-specific training data. This could be any NLP task, such
    as text completion, translation, summarization, question answering, and so on.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot, one-shot, and few-shot learning
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the impressive features of GPT-3 is its capability to perform few-shot
    learning. When given a task and a few examples of that task, GPT-3 can often learn
    to perform the task accurately.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: In the zero-shot setting, the model is given a task without any prior examples.
    In the one-shot setting, it’s given one example, and in the few-shot setting,
    it’s given a few examples to learn from.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of using GPT-3
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite its impressive capabilities, GPT-3 also presents some challenges. Due
    to its large size, it requires substantial computational resources to train. It
    can sometimes generate incorrect or nonsensical responses, and it can reflect
    biases present in the training data. It also struggles with tasks that require
    a deep understanding of the world or common sense reasoning beyond what can be
    learned from text.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing our use case – ML/DL system design for NLP classification in a Jupyter
    Notebook
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to work on a real-world problem and see how we
    can use an NLP pipeline to solve it. The code for this part is shared as a Google
    Colab notebook at [Ch6_Text_Classification_DL.ipynb](https://colab.research.google.com/drive/1HVD2fvxHup6OsPi2mKxNS_nfCRZ0iGCw?usp=sharing).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The business objective
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this scenario, we are in the healthcare sector. Our objective is to develop
    a general medical knowledge engine that is very up to date with recent findings
    in the world of healthcare.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: The technical objective
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CTO derives several technical objectives from the business objective. One
    objective is for the ML team: given the growing collection of conclusions that
    correspond to medical publications, identify the ones that represent advice. This
    will allow us to identify the medical advice that stems from the underlying research.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s review the parts of the pipeline, as depicted in *Figure 6**.9*:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.9 – The structure of a typical exploration and model pipeline\uFEFF\
    ](img/B18949_06_009.jpg)"
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – The structure of a typical exploration and model pipeline
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Notice how this design is different from the design we saw in *Figure 5**.2*.
    There, the exploration and evaluation parts leverage the same feature engineering
    technique that is later used by the ML models. Here, with LMs, feature engineering
    is not a part of the preparation for the modeling. The pretrained model, and particularly
    the tokenizer, performs feature engineering, which yields very different and less
    interpretable features than the binary, BoW, or TF-IDF features.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Code parts: From “Settings” through “Generating Results of the Traditional
    ML Models.”'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: These parts are identical in their nature to the analog parts discussed in [*Chapter
    5*](B18949_05_split_000.xhtml#_idTextAnchor130). The only differences relate to
    the differences in the data.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这些部分在本质上与第[*第5章*](B18949_05_split_000.xhtml#_idTextAnchor130)中讨论的模拟部分相同。唯一的区别与数据的不同有关。
- en: Deep learning
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习
- en: In this part of the code, we employ a deep learning language model.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用了一个深度学习语言模型。
- en: When looking to apply transfer learning via LMs and fine-tuning them per our
    objective and data, there are several stacks to choose from. The ones that stand
    out the most are Google’s TensorFlow, and Meta’s PyTorch. A package called **Transformers**
    was built as a wrapper around these stacks to allow for a simpler implementation
    of the code. In this example, we leverage the simplicity and richness of transformers
    models.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑通过LM应用迁移学习并针对我们的目标和数据进行微调时，有多个堆栈可供选择。最突出的是Google的TensorFlow和Meta的PyTorch。一个名为**Transformers**的包被构建为这些堆栈的包装器，以允许代码的更简单实现。在这个例子中，我们利用了transformers模型的简洁性和丰富性。
- en: 'It is worth highlighting the company that built and supports the Transformers
    package: Hugging Face. Hugging Face took it upon themselves to create an entire
    ecosystem around the collection and sharing of free, open source DL models, which
    includes the many components that accommodate for implementing these models. The
    most actionable tool is the Transformers package, which is a Python package dedicated
    to picking, importing, training, and employing a large and growing set of DL models.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 值得强调的是构建并支持Transformers包的公司：Hugging Face。Hugging Face承担起创建一个围绕免费、开源DL模型收集和共享的整个生态系统，这包括许多适应实现这些模型的组件。最实用的工具是Transformers包，这是一个Python包，致力于选择、导入、训练和部署一个庞大且不断增长的DL模型集合。
- en: The code we are reviewing here provides more than just an example of ML/DL system
    design in the real world; it also showcases Hugging Face’s Transformers.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里审查的代码不仅提供了一个现实世界中的ML/DL系统设计的示例；还展示了Hugging Face的Transformers。
- en: Formatting the data
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据格式化
- en: Here, we set the data up in a format that suits the Transformers library. The
    column names must be very specific.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将数据设置成适合Transformers库的格式。列名必须非常具体。
- en: Evaluation metric
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: We decided which metric we wished to optimize and plugged it into the training
    process. For this problem of binary classification, we optimized for accuracy
    and evaluated our result in comparison to the dataset’s baseline accuracy, also
    known as the prior.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定要优化的指标，并将其插入到训练过程中。对于这个二元分类问题，我们优化了准确率，并将我们的结果与数据集的基线准确率进行了比较，也称为先验。
- en: Trainer object
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练器对象
- en: 'This is the core object for training the LM in Transformers. It holds a set
    of predefined configurations. Some of the key training configurations are the
    following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于在Transformers中训练LM的核心对象。它包含一组预定义的配置。一些关键的训练配置如下：
- en: 'The neural net’s mathematical learning hyperparameters, such the following:'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的数学学习超参数，例如以下：
- en: The learning rate
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: The gradient decent settings
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降设置
- en: The number of training epochs
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮数
- en: The computation hardware usage
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算硬件使用情况
- en: Logging setting for capturing the progression of the objective metric throughout
    the training process
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程中记录目标指标进度的日志设置
- en: Fine-tuning the neural network parameters
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调神经网络参数
- en: The fundamental concept around fine-tuning LMs is transfer learning. Neural
    networks lend themselves so well to transfer learning because one can simply strip
    any number of layers from the end of the structure and replace them with untrained
    layers that would be trained based on the underlying problem. The rest of the
    layers that weren’t removed and aren’t trained continue to operate exactly in
    the same way they did when the LM was originally trained (when it was originally
    built). If we replace the last layer but leave the rest of the original layers,
    then we could view those layers as supervised feature engineering or, conversely,
    as an embedding mechanism. This trait reflects the concept of transfer learning.
    Ideally, the model is expected to lend itself well to our underlying problem so
    that we will choose to keep the vast majority of the original layers, and only
    a small minority would be replaced and trained. In this way, a large DL model
    that took many weeks to be pretrained can be transferred and adapted to a new
    problem in minutes.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 关于微调语言模型的基本概念是迁移学习。神经网络非常适合迁移学习，因为可以从结构的末端简单地剥离任意数量的层，并用未训练的层替换它们，这些层将基于底层问题进行训练。未移除且未训练的其余层将继续以与语言模型最初训练时（当它最初构建时）完全相同的方式运行。如果我们替换最后一层但保留其余的原始层，那么我们可以将这些层视为监督特征工程，或者相反，作为嵌入机制。这种特性反映了迁移学习的概念。理想情况下，模型预计将很好地适应我们的底层问题，因此我们将选择保留绝大多数的原始层，只有一小部分会被替换并训练。这样，一个需要几周时间预训练的大型深度学习模型可以在几分钟内迁移和适应新的问题。
- en: In our code, we set the model up in a way that we dictate exactly which of its
    layers we are looking to fine-tune. It is a design choice for us for this to be
    based on performance and also computation resources. One choice is to fine-tune
    the last layer right before the final output, also known as the classification
    head. The alternative is to fine-tune all the layers. In our code, we explicitly
    call the model’s configuration, which controls which layer is fine-tuned, so the
    code can be changed in any way that suits the design.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码中，我们以这种方式设置模型，即我们确切地指定我们想要微调其哪些层。这是我们的设计选择，基于性能和计算资源。一个选择是微调紧接最终输出的最后一层，也称为分类头。另一种选择是微调所有层。在我们的代码中，我们明确调用模型的配置，该配置控制哪些层被微调，因此代码可以以任何适合设计的方式更改。
- en: We configure the trainer to log the performance of the training in real time.
    It prints those logs out for us in a table so we can observe and monitor them.
    When the training is complete, we plot the progress of the training and the evaluation.
    This helps us see the relation between the evolution of the training results and
    the evaluation results. Since the evaluation set that the trainer uses can be
    viewed as a held-out set in the context of the trainer, this plot allows us to
    investigate underfitting and overfitting.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练器配置为实时记录训练性能。它将这些日志以表格形式打印出来，以便我们观察和监控。当训练完成后，我们绘制训练和评估的进度。这有助于我们了解训练结果和评估结果之间的关联。由于训练器使用的评估集可以被视为训练器上下文中的保留集，这个图表使我们能够研究欠拟合和过拟合。
- en: Generating the training results – used for design choices
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成训练结果——用于设计选择
- en: We reviewed the results of the training set, along with the logs that the trainer
    printed out. We compared them to the baseline accuracy and observed an increase
    in accuracy. We learned about the quality of our design by iterating over several
    different design choices and comparing them. That process of iterating over many
    sets of design parameters would be automated into code to allow for a systematic
    evaluation of the optimal setting. We didn’t do that in our notebook just to keep
    things simple in the example. Once we believed we had found the optimal setting,
    we could say that the process was finished.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了训练集的结果，以及训练器打印出的日志。我们将它们与基线准确率进行了比较，并观察到准确率的提高。通过迭代多个不同的设计选择并进行比较，我们了解了我们设计的质量。将这个过程自动化为代码，以便对最佳设置进行系统评估。我们只是在笔记本中这样做，以保持示例的简单性。一旦我们相信我们已经找到了最佳设置，我们就可以说这个过程已经完成。
- en: Generating the testing results – used for presenting performance
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成测试结果——用于展示性能
- en: As with the code in [*Chapter 5*](B18949_05_split_000.xhtml#_idTextAnchor130),
    here, too, we finished by reviewing the test results. It is worth noting the difference
    between the evaluation set and the test set. One could suggest that since the
    trainer doesn’t use the evaluation set for training, it could be used as a held-out
    test set, thus saving the need to exclude so many observations from training and
    supplying the model with more labeled data. However, while the trainer didn’t
    use the evaluation set, we did use it to make our design decisions. For instance,
    we observed the plot from the preceding section and judged which number of epochs
    is optimal to achieve optimal fitting. In [*Chapter 5*](B18949_05_split_000.xhtml#_idTextAnchor130),
    an evaluation set was used too, but we didn’t need to explicitly define it; it
    was carried out as a part of the K-fold cross-validation mechanism.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 [*第 5 章*](B18949_05_split_000.xhtml#_idTextAnchor130) 中的代码一样，在这里，我们也以回顾测试结果结束。值得注意的是评估集和测试集之间的差异。有人可能会建议，由于训练者没有使用评估集进行训练，它可以作为一个保留的测试集，从而节省了从训练中排除许多观察结果的需要，并为模型提供更多标记数据。然而，尽管训练者没有使用评估集，我们确实使用了它来做出我们的设计决策。例如，我们观察了前一个部分的图表，并判断哪个数量的周期数是达到最佳拟合的最优选择。在
    [*第 5 章*](B18949_05_split_000.xhtml#_idTextAnchor130) 中也使用了评估集，但我们不需要明确定义它；它是作为
    K 折交叉验证机制的一部分执行的。
- en: Summary
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this enlightening chapter, we embarked on a comprehensive exploration of
    DL and its remarkable application to text classification tasks through language
    models. We began with an overview of DL, revealing its profound ability to learn
    complex patterns from vast amounts of data and its indisputable role in advancing
    state-of-the-art NLP systems.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一启发性的章节中，我们全面探索了深度学习及其在文本分类任务中通过语言模型应用的显著应用。我们从深度学习的概述开始，揭示了其从大量数据中学习复杂模式的能力，以及在推进最先进自然语言处理系统中的无可争议的作用。
- en: We then delved into the transformative world of transformer models, which have
    revolutionized NLP by providing an effective alternative to traditional RNNs and
    CNNs for processing sequence data. By unpacking the attention mechanism—a key
    feature in transformers—we highlighted its capacity to focus on different parts
    of the input sequence, hence facilitating a better understanding of context.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入到变换器模型的变革性世界，这些模型通过提供传统 RNN 和 CNN 处理序列数据的有效替代方案，已经彻底改变了自然语言处理。通过解开注意力机制——变换器中的一个关键特性——我们突出了其专注于输入序列不同部分的能力，从而促进了上下文理解的更好。
- en: Our journey continued with an in-depth exploration of the BERT model. We detailed
    its architecture, emphasizing its pioneering use of bidirectional training to
    generate contextually rich word embeddings, and we highlighted its pretraining
    process, which learns language semantics from a large text corpus.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程继续深入探索 BERT 模型。我们详细介绍了其架构，强调其开创性地使用双向训练来生成语境丰富的词嵌入，并突出了其预训练过程，该过程从大量文本语料库中学习语言语义。
- en: However, our exploration did not end there; we also introduced GPT, another
    transformative model that leverages the power of transformers in a slightly different
    way—focusing on generating human-like text. By comparing BERT and GPT, we shed
    light on their distinct strengths and use cases.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的探索并未就此结束；我们还介绍了 GPT，另一个利用变换器能力以略有不同方式变革的模型——专注于生成类似人类的文本。通过比较 BERT 和 GPT，我们阐明了它们的独特优势和用例。
- en: The chapter culminated in a practical guide on how to design and implement a
    text classification model using these advanced models. We walked you through all
    the stages of this process, from data preprocessing and model configuration to
    training, evaluation, and finally, making predictions on unseen data.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以一个关于如何使用这些高级模型设计和实现文本分类模型的实用指南结束。我们引导您经历了这个过程的各个阶段，从数据预处理和模型配置到训练、评估，最后是在未见过的数据上做出预测。
- en: In essence, this chapter provided a well-rounded understanding of DL in NLP,
    transitioning from fundamental principles to hands-on applications. With this
    knowledge, you are now equipped to leverage the capabilities of transformer models,
    BERT, and GPT for your text classification tasks. Whether you are looking to delve
    further into the world of NLP or apply these skills in a practical setting, this
    chapter has equipped you with a firm foundation on which to build.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，本章提供了对自然语言处理中深度学习的全面理解，从基本原理过渡到实际应用。凭借这些知识，您现在可以充分利用变压器模型、BERT和GPT的能力来处理您的文本分类任务。无论您是想进一步深入研究自然语言处理的世界，还是在实际环境中应用这些技能，本章都为您奠定了坚实的基础。
- en: In this chapter, we introduced you to large language models. In the next chapter,
    we dive deeper into these models to learn more about them.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了大型语言模型。在下一章中，我们将更深入地探讨这些模型，以了解更多关于它们的信息。
