- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introduction to LLM Design Patterns
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 设计模式简介
- en: '**Large language models** (**LLMs**) are machine learning models capable of
    understanding and producing human-like text across diverse domains. They have
    opened up unprecedented possibilities while also presenting unique challenges.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型**（LLMs）是能够理解和生成类似人类文本的机器学习模型，涵盖多个领域。它们开辟了前所未有的可能性，同时也带来了独特的挑战。'
- en: In this chapter, we will introduce the world of LLMs and the critical role of
    **design patterns** in their development. You will learn about the evolution of
    language models, explore the core principles that power modern LLMs, and examine
    their impressive capabilities, as well as their limitations. We’ll uncover the
    importance of design patterns – time-tested solutions to recurring problems in
    software development – and how they are being adapted and applied to address the
    specific challenges of LLM projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍 LLMs 的世界以及**设计模式**在它们发展中的关键作用。您将了解语言模型的演变，探索推动现代 LLMs 的核心原则，并检查它们的惊人能力和局限性。我们将揭示设计模式的重要性——经过时间考验的软件开发中常见问题的解决方案——以及它们如何被调整和应用来解决
    LLM 项目特有的挑战。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding LLMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 LLMs
- en: Understanding design patterns
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解设计模式
- en: Design patterns for LLM development
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 开发的设计模式
- en: Future directions in LLM patterns and their development
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 模式及其发展的未来方向
- en: Understanding LLMs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 LLMs
- en: In this section, we will highlight the core concepts of LLMs, exploring their
    evolution, underlying principles, and the transformative impact they have had
    on the AI landscape. We will examine the key components that make LLMs so powerful,
    the challenges they present, and the ongoing developments shaping their future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将突出 LLMs 的核心概念，探讨它们的演变、潜在原则以及它们对人工智能领域产生的变革性影响。我们将检查构成 LLMs 那么强大的关键组件，它们所提出的挑战，以及塑造它们未来的持续发展。
- en: The evolution of language models
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型的发展
- en: 'The journey toward modern LLMs has been marked by significant paradigm shifts
    in natural language processing, as illustrated in the timeline shown in *Figure
    1**.1*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现代LLMs的旅程被自然语言处理中的重大范式转变所标记，如图 *图 1**.1* 所示的时间线所示：
- en: '![Figure 1.1 – Evolution of language models](img/B31249_01_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 语言模型演变](img/B31249_01_01.jpg)'
- en: Figure 1.1 – Evolution of language models
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 语言模型演变
- en: Early statistical approaches, while groundbreaking, were limited in capturing
    the nuances of human language. The advent of **neural networks**, particularly
    **recurrent neural networks** (**RNNs**) and **long short-term memory** (**LSTM**)
    networks, allowed for better handling of sequential data and improved the ability
    to capture longer-term dependencies in text. Capturing longer-term dependencies
    in text is crucial for understanding the broader context and maintaining coherence
    over extended passages. Early statistical approaches struggled with this due to
    their inability to account for the relationships between words or concepts spread
    across long sequences. The development of neural networks, particularly RNNs and
    LSTM networks, significantly improved the ability to capture these dependencies.
    However, even with these advancements, capturing long-term dependencies alone
    is not sufficient; these models still face challenges in managing complex contexts
    and ensuring consistency across larger text sequences.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管早期的统计方法具有开创性，但在捕捉人类语言细微差别方面存在局限性。**神经网络**的出现，尤其是**循环神经网络**（RNNs）和**长短期记忆**（LSTM）网络，使得处理序列数据的能力得到提升，并改善了捕捉文本中长期依赖关系的能力。在文本中捕捉长期依赖关系对于理解更广泛的上下文和保持长段落的一致性至关重要。由于无法考虑跨越长序列的单词或概念之间的关系，早期的统计方法在这方面遇到了困难。神经网络的发展，尤其是
    RNNs 和 LSTM 网络，显著提高了捕捉这些依赖关系的能力。然而，即使有了这些进步，仅仅捕捉长期依赖关系是不够的；这些模型在管理复杂上下文和确保长文本序列的一致性方面仍然面临挑战。
- en: In 2017, the introduction of the **transformer architecture** revolutionized
    the field, paving the way for larger, more powerful language models. (For more
    on the transformer architecture, see the next section.) This breakthrough ushered
    in the era of pre-trained models such as **BERT** and the **GPT** series, which
    leveraged vast amounts of unlabeled text data to achieve unprecedented performance
    across various NLP tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，**变换器架构**的引入彻底改变了该领域，为更大、更强大的语言模型铺平了道路。（关于变换器架构的更多内容，请参阅下一节。）这一突破迎来了预训练模型如**BERT**和**GPT**系列的时代，它们利用大量未标记的文本数据，在各种自然语言处理（NLP）任务上实现了前所未有的性能。
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For a comprehensive overview of the evolution of language models, including
    detailed discussions of statistical models, neural networks, and transformer-based
    approaches, see the book *Speech and Language Processing* by Dan Jurafsky and
    James H. Martin. The online manuscript is updated frequently and can be found
    at [https://web.stanford.edu/~jurafsky/slp3](https://web.stanford.edu/~jurafsky/slp3).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 想要全面了解语言模型的发展历程，包括对统计模型、神经网络和基于变换器方法的详细讨论，请参阅丹·朱尔法斯基（Dan Jurafsky）和詹姆斯·H·马丁（James
    H. Martin）合著的书籍《语音与语言处理》（Speech and Language Processing）。在线手稿经常更新，可在[https://web.stanford.edu/~jurafsky/slp3](https://web.stanford.edu/~jurafsky/slp3)找到。
- en: Core features of LLMs
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的核心特性
- en: This section introduces the core features of LLMs, focusing on their transformer
    architecture, scale, few-shot learning, language understanding and generation,
    and multilingual capabilities.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了LLM的核心特性，重点关注其变换器架构、规模、少样本学习、语言理解和生成以及多语言能力。
- en: The transformer architecture
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换器架构
- en: The key component of any LLM is its **transformer architecture**. The transformer
    architecture leverages a **self-attention mechanism**, which allows the model
    to weigh the importance of different parts of the input when processing each element.
    In a transformer-based LLM, the input text is first tokenized into smaller units,
    typically words or subwords. These tokens are then embedded into a high-dimensional
    vector space, where each token is represented as a dense vector.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 任何大型语言模型（LLM）的关键组成部分是其**变换器架构**。变换器架构利用**自注意力机制**，允许模型在处理每个元素时，根据输入的不同部分的重要性进行加权。在基于变换器的LLM中，输入文本首先被标记化为更小的单元，通常是单词或子词。然后，这些标记被嵌入到一个高维向量空间中，其中每个标记都表示为一个密集向量。
- en: A dense vector is a mathematical object that’s used in various fields, including
    AI, to represent data in a compact, high-dimensional space. In simple terms, it’s
    a list of numbers (or values) that, when combined, form a representation of something,
    such as a word, an image, or any other type of data. These numbers in the vector
    can be thought of as the coordinates in a multidimensional space, where each number
    contributes to the description of the data point.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密向量是一种数学对象，在包括人工智能在内的多个领域中使用，用于在紧凑、高维空间中表示数据。简单来说，它是一系列数字（或值），当结合在一起时，形成对某物（如单词、图像或其他类型的数据）的表示。向量中的这些数字可以被视为多维空间中的坐标，其中每个数字都对数据点的描述做出贡献。
- en: The self-attention mechanism operates on these vector representations, allowing
    the model to capture complex relationships between different parts of the input
    sequence. This is achieved through the process of computing attention scores between
    each pair of tokens in the sequence. These scores determine how much each token
    should attend to every other token when computing its contextual representation.
    This allows the model to capture long-range dependencies and complex relationships
    within the text, overcoming the limitations of previous sequential models (*Attention
    Is All You* *Need*, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制作用于这些向量表示，允许模型捕捉输入序列中不同部分之间的复杂关系。这是通过计算序列中每对标记之间的注意力分数来实现的。这些分数决定了每个标记在计算其上下文表示时应该关注其他每个标记的程度。这允许模型捕捉文本中的长距离依赖和复杂关系，克服了先前顺序模型的局限性（*Attention
    Is All You Need*，[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)）。
- en: The transformer architecture consists of multiple layers of self-attention and
    feedforward neural networks. Each layer refines the representations of the input
    tokens, capturing increasingly abstract and contextual information. The **multi-head
    attention mechanism**, another key component of transformers, allows the model
    to attend to different aspects of the input simultaneously, further enhancing
    its ability to capture complex patterns in the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构由多层自注意力机制和前馈神经网络组成。每一层都细化了输入标记的表示，捕捉越来越抽象和上下文相关的信息。**多头注意力机制**是Transformer的另一个关键组件，它允许模型同时关注输入的不同方面，进一步增强了其捕捉数据中复杂模式的能力。
- en: Multi-head attention in transformers is a mechanism that allows the model to
    focus on different positions of the input sequence simultaneously for better representation
    learning. Instead of performing a single attention function, the model projects
    the queries, keys, and values into multiple lower-dimensional spaces (heads),
    performs attention in each of these spaces independently, and then concatenates
    the results before performing a final linear transformation. This approach enables
    the model to jointly attend to information from different representation subspaces
    and positions, capturing various aspects of the relationships between sequence
    elements – such as syntactic dependencies, semantic similarities, or contextual
    relevance – which significantly enhances the model’s ability to understand complex
    patterns and relationships in the data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer中的多头注意力是一种机制，允许模型同时关注输入序列的不同位置，以实现更好的表示学习。模型不是执行单个注意力函数，而是将查询、键和值投影到多个低维空间（头），在每个这些空间中独立执行注意力操作，然后在对这些结果进行最终线性变换之前将它们连接起来。这种方法使得模型能够同时关注来自不同表示子空间和位置的信息，捕捉序列元素之间关系的各个方面——如句法依赖、语义相似性或上下文相关性——这显著增强了模型理解数据中的复杂模式和关系的能力。
- en: Scale and computational resources
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规模和计算资源
- en: A defining characteristic of LLMs is their unprecedented scale, both in terms
    of model size and the amount of data they are trained on. The *large* in LLMs
    refers not just to the complexity of these models but also to the vast computational
    resources required to train and run them. Modern LLMs can have hundreds of billions
    of parameters, which require enormous amounts of memory and processing power.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的一个显著特征是其前所未有的规模，这既体现在模型大小上，也体现在它们训练所使用的数据量上。在LLMs中，“大”不仅指这些模型的复杂性，还包括训练和运行它们所需的庞大计算资源。现代LLMs可以拥有数百亿个参数，这需要巨大的内存和处理能力。
- en: This scaling up of model size and training data has been driven by empirical
    observations of consistent improvements in performance across various tasks as
    models become larger. These improvements often follow predictable scaling laws,
    where performance metrics such as perplexity or accuracy improve as a **power-law**
    function of model size and compute budget (*Scaling Laws for Neural Language Models*,
    [https://arxiv.org/pdf/2001.08361](https://arxiv.org/pdf/2001.08361)). This phenomenon
    has led to a race to build ever-larger models, with some recent LLMs boasting
    trillions of parameters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小和训练数据的这种扩展是由对各种任务性能持续改进的实证观察所驱动的，随着模型变得更大，这些改进通常遵循可预测的扩展定律，其中性能指标如困惑度或准确度随着模型大小和计算预算的**幂律**函数而提高（参见《神经语言模型的扩展定律》[https://arxiv.org/pdf/2001.08361](https://arxiv.org/pdf/2001.08361)）。这一现象导致了一场构建更大模型的竞赛，一些最近的LLMs甚至拥有万亿个参数。
- en: Few-shot capabilities
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 少样本学习能力
- en: The **few-shot learning** capabilities of LLMs represent an advancement in the
    field of NLP. Traditional machine learning approaches typically require large
    amounts of labeled data for each specific task. In contrast, LLMs can often perform
    new tasks with just a few examples or even with just a natural language description
    of the task (**zero-shot learning**). This flexibility stems from the models’
    broad understanding of language and their ability to generalize patterns across
    different contexts.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的**少样本学习能力**代表了自然语言处理领域的一项进步。传统的机器学习方法通常需要为每个特定任务收集大量标记数据。相比之下，LLMs通常只需要几个示例或甚至仅需要任务的天然语言描述（**零样本学习**）就能执行新任务。这种灵活性源于模型对语言的广泛理解以及它们在不同上下文中泛化模式的能力。
- en: For example, a pre-trained LLM might be able to perform a sentiment analysis
    task on product reviews without ever being explicitly trained on sentiment analysis,
    simply by being provided with a few examples of positive and negative reviews.
    This capability has opened up new possibilities for applying AI to a wide range
    of language tasks, particularly in domains where large amounts of task-specific
    labeled data are not available.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个预训练的LLM可能能够在没有明确进行情感分析训练的情况下，通过提供一些正面和负面评论的例子，对产品评论进行情感分析。这种能力为将AI应用于广泛的语言任务开辟了新的可能性，尤其是在那些没有大量特定任务标记数据的领域。
- en: Language understanding and generation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言理解和生成
- en: One of the most striking capabilities of LLMs is their ability to understand
    and generate human-like text across a wide range of styles, topics, and formats.
    In terms of understanding, these models can process and interpret complex textual
    inputs, extracting meaning and context with a level of sophistication that mimics
    human-like comprehension in many scenarios. This ability extends to various subtasks,
    such as sentiment analysis, named entity recognition, and topic classification.
    LLMs can often discern nuanced differences in tone, identify implicit information,
    and recognize complex linguistic patterns.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs最引人注目的能力之一是它们理解和生成类似人类文本的能力，涵盖广泛的风格、主题和格式。在理解方面，这些模型能够处理和解释复杂的文本输入，以高度复杂的方式提取意义和上下文，在很多情况下模仿人类类似的理解。这种能力扩展到各种子任务，如情感分析、命名实体识别和主题分类。LLMs通常能够辨别细微的语气差异，识别隐含信息，并识别复杂的语言模式。
- en: On the generation side, LLMs have shown an unprecedented ability to produce
    coherent, contextually appropriate text. They can generate everything from creative
    fiction and poetry to technical documentation and code. The quality of this generated
    text often exhibits a high degree of fluency, grammatical correctness, and contextual
    relevance. This generative capability has opened up new possibilities in areas
    such as content creation, automated writing assistance, and conversational AI.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成方面，LLMs展示了前所未有的能力，能够生成连贯、上下文适当的文本。它们可以生成从创意小说和诗歌到技术文档和代码的一切内容。这种生成文本的质量通常表现出高度的流畅性、语法正确性和上下文相关性。这种生成能力为内容创作、自动写作辅助和对话式人工智能等领域开辟了新的可能性。
- en: Multilingual and cross-lingual abilities
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多语言和跨语言能力
- en: Many modern LLMs exhibit strong multilingual and cross-lingual abilities. When
    trained on diverse multilingual corpora, these models can understand and generate
    text in multiple languages. Some models have demonstrated the ability to perform
    cross-lingual tasks, such as translating between language pairs they were not
    explicitly trained on, or answering questions in one language based on context
    provided in another.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代大型语言模型（LLMs）展现出强大的多语言和跨语言能力。当在多样化的多语言语料库上训练时，这些模型能够理解和生成多种语言中的文本。一些模型已经展示了执行跨语言任务的能力，例如在它们没有明确训练的语言对之间进行翻译，或者根据另一语言中提供的内容来回答问题。
- en: These capabilities open up possibilities for breaking down language barriers
    and enabling more inclusive global communication. However, it’s important to note
    that the performance of LLMs can vary significantly across different languages.
    Models tend to perform best in languages that are well-represented in their training
    data, which often favors widely spoken languages such as English. Efforts are
    ongoing to develop more equitable multilingual models and to improve performance
    in low-resource languages.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些能力为打破语言障碍和促进更具包容性的全球沟通开辟了可能性。然而，需要注意的是，LLMs在不同语言上的性能可能会有很大差异。模型在它们训练数据中表现最好的通常是那些广泛使用的语言，这通常有利于英语等广泛使用的语言。目前正在努力开发更公平的多语言模型，并提高在低资源语言上的性能。
- en: Having examined the core features of LLMs, the next section turns to the role
    of design patterns in structuring and guiding LLM projects. Drawing from their
    origins in software engineering, design patterns offer reusable solutions that
    help manage complexity, improve collaboration, and support scalable, maintainable
    architectures. Understanding their evolution and principles sets the foundation
    for applying them effectively in the context of LLM development.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 经过对LLM的核心特征进行考察，下一节将转向设计模式在LLM项目结构和指导中的作用。设计模式源于软件工程，提供了可重用的解决方案，有助于管理复杂性、提高协作，并支持可扩展、可维护的架构。理解它们的演变和原则为在LLM开发背景下有效地应用它们奠定了基础。
- en: Understanding design patterns
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解设计模式
- en: Design patterns originated as a way to capture and share solutions to recurring
    design problems. Initially rooted in object-oriented programming, they offered
    a structured approach to building software by identifying repeatable strategies
    that enhance code clarity, reusability, and maintainability. Over time, design
    patterns have evolved beyond their original context, influencing a wide range
    of development practices and system architectures, including LLM development.
    The following discussion traces the origins of design patterns and outlines the
    principles that have shaped their continued relevance across different programming
    paradigms and application domains.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 设计模式最初作为一种捕捉和共享重复性设计问题解决方案的方法而出现。最初根植于面向对象编程，它们通过识别增强代码清晰度、可重用性和可维护性的可重复策略，提供了一种构建软件的结构化方法。随着时间的推移，设计模式已经超越了其原始的背景，影响了包括LLM开发在内的广泛开发实践和系统架构。以下讨论将追溯设计模式的起源，并概述塑造它们在不同编程范式和应用领域持续相关性的原则。
- en: Origins and evolution
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 起源和演变
- en: 'The concept of design patterns in software engineering gained prominence in
    the 1990s, largely popularized by the book *Design Patterns: Elements of Reusable
    Object-Oriented Software* by Erich Gamma, Richard Helm, Ralph Johnson, and John
    Vlissides, often referred to as the **Gang of Four**. This seminal work identified
    and cataloged common patterns in object-oriented software design, providing a
    vocabulary and set of best practices that quickly became foundational in the field
    ([https://books.google.com/books/about/Design_Patterns.html?id=6oHuKQe3TjQC](https://books.google.com/books/about/Design_Patterns.html?id=6oHuKQe3TjQC)).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程中的设计模式概念在20世纪90年代获得了显著的关注，这主要归功于Erich Gamma、Richard Helm、Ralph Johnson和John
    Vlissides合著的书籍《设计模式：可重用面向对象软件元素》，通常被称为**四人帮**。这部开创性的工作识别并编目了面向对象软件设计中的常见模式，提供了一种词汇和最佳实践集合，这些很快成为该领域的基石([https://books.google.com/books/about/Design_Patterns.html?id=6oHuKQe3TjQC](https://books.google.com/books/about/Design_Patterns.html?id=6oHuKQe3TjQC))。
- en: These patterns emerged from the collective experience of software developers,
    representing solutions that had proven effective across various projects and contexts.
    They offered a way to capture and communicate complex design ideas efficiently,
    enabling developers to build on the wisdom of their predecessors rather than reinventing
    solutions to recurring problems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式源于软件开发者的集体经验，代表了在各种项目和环境中证明有效的解决方案。它们提供了一种高效地捕捉和传达复杂设计思想的方法，使开发者能够建立在前辈的智慧之上，而不是重新发明解决重复问题的方案。
- en: Initially focused on object-oriented programming, the concept of design patterns
    has since expanded to encompass a wide range of software development paradigms
    and domains. As software systems have grown in complexity and scale, the importance
    of design patterns has only increased, providing a means to manage this complexity
    and promote more maintainable, scalable, and robust software architectures.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 设计模式的概念最初专注于面向对象编程，但现在已经扩展到涵盖广泛的软件开发范式和领域。随着软件系统在复杂性和规模上的增长，设计模式的重要性也只增不减，提供了一种管理这种复杂性的手段，并促进更可维护、可扩展和稳健的软件架构。
- en: Core principles of design patterns
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计模式的核心原则
- en: At their core, design patterns embody several key principles that make them
    valuable in software development. First, they promote code reuse and modularity.
    By encapsulating solutions to common problems, patterns allow developers to apply
    proven approaches without having to duplicate code or reinvent solutions. This
    modularity also enhances the maintainability of software systems as changes can
    often be localized to specific components implementing a pattern.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，设计模式体现了几个关键原则，使它们在软件开发中具有价值。首先，它们促进了代码重用和模块化。通过封装常见问题的解决方案，模式允许开发者应用经过验证的方法，而无需重复代码或重新发明解决方案。这种模块化也增强了软件系统的可维护性，因为变化通常可以局部化到实现模式的特定组件。
- en: Second, design patterns provide a shared vocabulary among developers. This common
    language facilitates communication within development teams and across projects.
    When a developer describes a solution using a well-known pattern, it immediately
    conveys a wealth of information about the structure and behavior of that solution
    to other developers familiar with the pattern.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，设计模式为开发者提供了一种共享的词汇。这种共同的语言促进了开发团队内部以及项目之间的沟通。当开发者使用一个广为人知的设计模式来描述解决方案时，它立即向熟悉该模式的其他开发者传达了大量关于该解决方案结构和行为的信息。
- en: Third, patterns often embody principles of good software design, such as loose
    coupling and high cohesion. They encourage developers to think about the relationships
    between components and the overall structure of their systems, leading to more
    thoughtful and well-architected solutions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，模式通常体现了良好的软件设计原则，例如松散耦合和高内聚。它们鼓励开发者思考组件之间的关系以及他们系统的整体结构，从而产生更深思熟虑且结构良好的解决方案。
- en: Lastly, design patterns are typically flexible and adaptable. While they provide
    a general structure for solving a problem, they are not rigid prescriptions. Developers
    can – and should – adapt patterns so that they fit the specific context and requirements
    of their projects, allowing for creativity within a proven framework.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，设计模式通常是灵活和可适应的。虽然它们提供了一个解决问题的通用结构，但它们并不是僵化的规定。开发者可以——并且应该——根据项目的具体上下文和需求调整模式，以便在经过验证的框架内发挥创造力。
- en: Design patterns for LLM development
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM开发的设计模式
- en: As the need to develop intelligent LLM-based applications grows, we see the
    emergence of specific design patterns tailored to address the unique challenges
    posed by these complex systems. These patterns differ significantly from traditional
    software design patterns, focusing on aspects inherent to the entire life cycle
    of LLMs – from data preparation and model training to evaluation, deployment,
    and sophisticated application design.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随着开发基于智能LLM的应用需求增长，我们看到出现了专门针对这些复杂系统独特挑战的设计模式。这些模式与传统软件设计模式有显著差异，专注于LLM整个生命周期内在的方面——从数据准备和模型训练到评估、部署和复杂应用设计。
- en: 'This book delves into **29 practical LLM design patterns**, explored in detail
    across *Chapters 2* through *30*. Developers and researchers can navigate the
    complexities of building LLM systems using these design patterns:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本书深入探讨了**29个实用的LLM设计模式**，这些模式在*第2章*至*第30章*中进行了详细探讨。开发者和研究人员可以使用这些设计模式来导航构建LLM系统的复杂性：
- en: '**Establishing a solid data foundation (Chapters 2–6)**: Lay the groundwork
    for high-quality models by mastering patterns for **data cleaning** ([*Chapter
    2*](B31249_02.xhtml#_idTextAnchor035)), **data augmentation** ([*Chapter 3*](B31249_03.xhtml#_idTextAnchor049)),
    **handling large datasets** ([*Chapter 4*](B31249_04.xhtml#_idTextAnchor072)),
    implementing **data versioning** ([*Chapter 5*](B31249_05.xhtml#_idTextAnchor084)),
    and ensuring effective **dataset annotation** ([*Chapter 6*](B31249_06.xhtml#_idTextAnchor095)).
    These practices enhance input quality and manageability, thereby directly impacting
    model performance.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立坚实的数据基础（第2章至第6章）**：通过掌握数据清洗（[第2章](B31249_02.xhtml#_idTextAnchor035)）、数据增强（[第3章](B31249_03.xhtml#_idTextAnchor049)）、处理大型数据集（[第4章](B31249_04.xhtml#_idTextAnchor072)）、实施数据版本控制（[第5章](B31249_05.xhtml#_idTextAnchor084)）和确保有效的数据集标注（[第6章](B31249_06.xhtml#_idTextAnchor095)）的模式来为高质量模型打下基础。这些实践提高了输入质量和可管理性，从而直接影响了模型性能。'
- en: '**Optimizing training and model efficiency (Chapters 7–13**): Streamline the
    core model-building process with patterns for robust **training pipelines** ([*Chapter
    7*](B31249_07.xhtml#_idTextAnchor108)), effective **hyperparameter tuning** ([*Chapter
    8*](B31249_08.xhtml#_idTextAnchor120)), **regularization** techniques ([*Chapter
    9*](B31249_09.xhtml#_idTextAnchor141)), reliable **checkpointing** ([*Chapter
    10*](B31249_10.xhtml#_idTextAnchor162)), task-specific **fine-tuning** ([*Chapter
    11*](B31249_11.xhtml#_idTextAnchor181)), and efficiency gains through **model
    pruning** ([*Chapter 12*](B31249_12.xhtml#_idTextAnchor191)) and **quantization**
    ([*Chapter 13*](B31249_13.xhtml#_idTextAnchor209)).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化训练和模型效率（第7-13章）**：通过用于健壮**训练流程**([*第7章*](B31249_07.xhtml#_idTextAnchor108))、有效的**超参数调整**([*第8章*](B31249_08.xhtml#_idTextAnchor120))、**正则化**技术([*第9章*](B31249_09.xhtml#_idTextAnchor141))、可靠的**检查点**([*第10章*](B31249_10.xhtml#_idTextAnchor162))、特定任务的**微调**([*第11章*](B31249_11.xhtml#_idTextAnchor181))，以及通过**模型剪枝**([*第12章*](B31249_12.xhtml#_idTextAnchor191))和**量化**([*第13章*](B31249_13.xhtml#_idTextAnchor209))提高效率的模式，简化核心模型构建过程。'
- en: '**Addressing model quality and alignment (Chapters 14–19)**: Build confidence
    in your models by applying rigorous **evaluation metrics** ([*Chapter 14*](B31249_14.xhtml#_idTextAnchor230))
    and **cross-validation** ([*Chapter 15*](B31249_15.xhtml#_idTextAnchor247)), enhancing
    **interpretability** ([*Chapter 16*](B31249_16.xhtml#_idTextAnchor265)), proactively
    addressing **fairness and bias** ([*Chapter 17*](B31249_17.xhtml#_idTextAnchor276)),
    improving **adversarial robustness** ([*Chapter 18*](B31249_18.xhtml#_idTextAnchor286)),
    and aligning models with human preferences using **Reinforcement Learning from
    Human Feedback** (**RLHF**) ([*Chapter 19*](B31249_19.xhtml#_idTextAnchor295)).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解决模型质量和对齐问题（第14-19章）**：通过应用严格的**评估指标**([*第14章*](B31249_14.xhtml#_idTextAnchor230))和**交叉验证**([*第15章*](B31249_15.xhtml#_idTextAnchor247))建立对模型的信心，增强**可解释性**([*第16章*](B31249_16.xhtml#_idTextAnchor265))，积极解决**公平性和偏差**([*第17章*](B31249_17.xhtml#_idTextAnchor276))，提高**对抗鲁棒性**([*第18章*](B31249_18.xhtml#_idTextAnchor286))，并使用**从人类反馈中学习强化学习**(**RLHF**)([*第19章*](B31249_19.xhtml#_idTextAnchor295))将模型与人类偏好对齐。'
- en: '**Enhancing reasoning and problem-solving capabilities (Chapters 20–25)**:
    Unlock more sophisticated model behaviors with advanced prompting and reasoning
    strategies such as **chain-of-thought** ([*Chapter 20*](B31249_20.xhtml#_idTextAnchor305)),
    **tree-of-thoughts** ([*Chapter 21*](B31249_21.xhtml#_idTextAnchor315)), **Reason
    and Act** (**ReAct**) **patterns** ([*Chapter 22*](B31249_22.xhtml#_idTextAnchor325)),
    **Reasoning** **WithOut** **Observation** ([*Chapter 23*](B31249_23.xhtml#_idTextAnchor339)),
    **reflection** techniques ([*Chapter 24*](B31249_24.xhtml#_idTextAnchor346)),
    and enabling **automatic multi-step reasoning and tool use** ([*Chapter 25*](B31249_25.xhtml#_idTextAnchor355)).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强推理和解决问题的能力（第20-25章）**：通过高级提示和推理策略，如**思维链**([*第20章*](B31249_20.xhtml#_idTextAnchor305))、**思维树**([*第21章*](B31249_21.xhtml#_idTextAnchor315))、**Reason
    and Act** (**ReAct**) **模式**([*第22章*](B31249_22.xhtml#_idTextAnchor325))、**无需观察的推理**([*第23章*](B31249_23.xhtml#_idTextAnchor339))、**反思**技术([*第24章*](B31249_24.xhtml#_idTextAnchor346))，以及启用**自动多步推理和工具使用**([*第25章*](B31249_25.xhtml#_idTextAnchor355))，解锁更复杂的模型行为。'
- en: '**Integrating external knowledge with RAG (Chapters 26–29)**: Ground model
    responses in factual, up-to-date information by using **retrieval-augmented generation**
    (**RAG**) ([*Chapter 26*](B31249_26.xhtml#_idTextAnchor366)), exploring variations
    such as **graph-based RAG** ([*Chapter 27*](B31249_27.xhtml#_idTextAnchor378))
    and **advanced RAG techniques** ([*Chapter 28*](B31249_28.xhtml#_idTextAnchor389)),
    and learning how to **evaluate RAG systems** ([*Chapter* *29*](B31249_29.xhtml#_idTextAnchor400))
    effectively.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将外部知识与RAG集成（第26-29章）**：通过使用**检索增强生成**(**RAG**)([*第26章*](B31249_26.xhtml#_idTextAnchor366))，探索如**基于图的RAG**([*第27章*](B31249_27.xhtml#_idTextAnchor378))和**高级RAG技术**([*第28章*](B31249_28.xhtml#_idTextAnchor389))的变体，以及学习如何有效地**评估RAG系统**([*第29章*](B31249_29.xhtml#_idTextAnchor400))。'
- en: '**Developing agentic AI applications (**[*Chapter 30*](B31249_30.xhtml#_idTextAnchor469)**)**:
    Move toward creating more independent applications by understanding and implementing
    **agentic patterns** ([*Chapter 30*](B31249_30.xhtml#_idTextAnchor469)), enabling
    LLMs to plan, use tools, and execute tasks autonomously.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发具有代理能力的AI应用([*第30章*](B31249_30.xhtml#_idTextAnchor469))**：通过理解和实现**代理模式**([*第30章*](B31249_30.xhtml#_idTextAnchor469))，朝着创建更独立的应用迈进，使大型语言模型能够自主规划、使用工具和执行任务。'
- en: Benefits of LLM design patterns
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM设计模式的益处
- en: The design patterns for LLM development offer significant benefits, starting
    with the establishment of a robust data foundation. Data cleaning ensures improved
    data quality, resulting in increased model accuracy, reduced training time, and
    mitigation of biases. Data augmentation enhances model robustness and generalization,
    leading to better performance on unseen data, while handling large datasets unlocks
    the potential for capturing complex patterns and improved model capabilities.
    Data versioning enables reproducibility of experiments and model training runs,
    while dataset annotation provides high-quality labels for supervised learning
    tasks, improving model accuracy and efficiency.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 开发的设计模式提供了显著的好处，首先是从建立强大的数据基础开始。数据清洗确保了数据质量的提升，从而提高了模型精度，减少了训练时间，并减轻了偏差。数据增强增强了模型的鲁棒性和泛化能力，使得模型在未见过的数据上表现更佳，同时处理大数据集释放了捕捉复杂模式和提升模型能力潜力。数据版本控制使得实验和模型训练运行的可重复性成为可能，而数据集标注为监督学习任务提供了高质量的标签，提高了模型精度和效率。
- en: Furthermore, optimizing training and model efficiency offers substantial advantages.
    Robust training pipelines automate the training process, leading to faster development
    cycles and consistent performance. Hyperparameter tuning optimizes model performance,
    improving accuracy and generalization, while regularization techniques prevent
    overfitting and improve robustness. Reliable checkpointing allows for the saving
    of model weights, facilitating experimentation and debugging. Task-specific fine-tuning
    optimizes a pre-trained LLM for a specific task, improving performance with few
    resources. Model pruning reduces the size and complexity of the LLM, leading to
    faster inference and improved deployment efficiency, and quantization further
    reduces model size and speeds up inference, enabling deployment on edge devices.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，优化训练和模型效率提供了实质性的优势。鲁棒的训练流程自动化了训练过程，导致开发周期更快，性能更一致。超参数调整优化了模型性能，提高了精度和泛化能力，而正则化技术防止过拟合并提高了鲁棒性。可靠的检查点允许保存模型权重，便于实验和调试。针对特定任务的微调优化了预训练的
    LLM 以适应特定任务，以少量资源提高性能。模型剪枝减少了 LLM 的大小和复杂性，导致推理更快，提高了部署效率，而量化进一步减少了模型大小并加快了推理速度，使得在边缘设备上部署成为可能。
- en: Addressing model quality and alignment is crucial for building trustworthy LLMs.
    Rigorous evaluation metrics provide a comprehensive assessment of model performance,
    enabling informed decision-making. Cross-validation improves the reliability of
    model evaluation and provides a more accurate estimate of generalization performance.
    Interpretability makes the model’s decision-making process more transparent and
    understandable, while fairness and bias mitigation reduces bias in the model’s
    predictions. Adversarial robustness makes the model more resistant to adversarial
    attacks, improving security, and RLHF aligns the model’s behavior with human preferences,
    improving user satisfaction and trust.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 解决模型质量和对齐问题对于构建可信赖的 LLM 至关重要。严格的评估指标提供了对模型性能的全面评估，使得决策更加明智。交叉验证提高了模型评估的可靠性，并提供了更准确的泛化性能估计。可解释性使得模型的决策过程更加透明和易于理解，而公平性和偏差缓解减少了模型预测中的偏差。对抗鲁棒性使得模型对对抗攻击更具抵抗力，提高了安全性，而强化学习与人类偏好对齐（RLHF）改善了用户满意度和信任度。
- en: Enhancing reasoning and problem-solving capabilities unlocks more sophisticated
    model behaviors. Chain-of-thought enables the model to break down complex problems,
    improving reasoning and accuracy. Tree-of-thoughts extends chain-of-thought by
    allowing the model to explore multiple reasoning paths, enhancing problem-solving
    capabilities for more complex tasks. ReAct integrates reasoning and action capabilities,
    enabling the model to interact with its environment and solve real-world problems.
    Reasoning WithOut Observation allows the model to apply reasoning skills even
    in the absence of explicit data, while reflection techniques empower the model
    to evaluate its own reasoning process and improve. Automatic multi-step reasoning
    and tool use automates the process of reasoning and tool usage, enabling the model
    to solve complex tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 增强推理和解决问题的能力可以解锁更复杂的模型行为。思维链使模型能够分解复杂问题，提高推理和准确性。思维树通过允许模型探索多个推理路径来扩展思维链，增强复杂任务的解决问题的能力。ReAct集成了推理和行动能力，使模型能够与环境交互并解决现实世界的问题。无观察推理允许模型在没有明确数据的情况下应用推理技能，而反思技术赋予模型评估自身推理过程并改进的能力。自动多步推理和工具使用自动化推理和工具使用的过程，使模型能够解决复杂任务。
- en: Finally, integrating external knowledge with RAG enhances the model’s knowledge
    and accuracy. RAG retrieves relevant information from external sources, overcoming
    the limitations of the model’s pre-trained knowledge. Graph-based RAG uses knowledge
    graphs to represent and retrieve information, enabling more sophisticated reasoning.
    Advanced RAG techniques further refine RAG systems and improve the quality, relevance,
    and accuracy of the retrieved information. Evaluating RAG systems involves methods
    for assessing the performance of RAG systems, enabling optimization and improvement.
    The use of agentic patterns enables the creation of autonomous AI agents that
    can plan, use tools, and execute tasks independently, leading to more powerful
    and versatile applications.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将外部知识整合到RAG中可以增强模型的知识和准确性。RAG从外部来源检索相关信息，克服了模型预训练知识的局限性。基于图的RAG使用知识图谱来表示和检索信息，从而实现更复杂的推理。高级RAG技术进一步精炼RAG系统，并提高检索信息的质量、相关性和准确性。评估RAG系统涉及评估RAG系统性能的方法，从而实现优化和改进。使用代理模式可以创建自主的AI代理，这些代理可以独立地规划、使用工具和执行任务，从而带来更强大和通用的应用。
- en: '*Table 1.1* summarizes the benefits of the LLM design patterns, organized by
    category.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*表1.1* 总结了LLM设计模式的优势，按类别组织。'
- en: '| **Category** | **Design pattern** | **Key benefits** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **设计模式** | **关键优势** |'
- en: '| --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Data Foundation** | Data cleaning | Higher quality insights; more accurate
    predictions; faster model iteration; reduced bias in outcomes. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **数据基础** | 数据清洗 | 更高质量的见解；更准确的预测；更快的模型迭代；降低结果偏差。 |'
- en: '|  | Data augmentation | More reliable and generalizable models; improved performance
    in diverse situations; greater resilience to noisy data. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据增强 | 更可靠和可泛化的模型；在多种情况下的性能改进；对噪声数据的更强鲁棒性。 |'
- en: '|  | Handling large datasets | Ability to extract deeper insights; higher performance
    potential; broader range of applications; more robust models. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | 处理大型数据集 | 能够提取更深入的见解；更高的性能潜力；更广泛的应用范围；更健壮的模型。 |'
- en: '|  | Data versioning | Increased confidence in results; easier debugging and
    auditing; reduced risk of data corruption; faster recovery from errors; improved
    data-driven decision making. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据版本控制 | 结果的信心增加；更容易调试和审计；降低数据损坏的风险；更快地从错误中恢复；改进数据驱动的决策。 |'
- en: '|  | Dataset annotation | More precise and effective models; faster learning
    rates; better alignment with desired outcomes. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据集标注 | 更精确和有效的模型；更快的学习率；与预期结果的更好对齐。 |'
- en: '| **Training** **and Efficiency** | Robust training pipelines | Faster model
    development; more consistent results; reduced manual effort; higher productivity.
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **训练** **和效率** | 强健的训练流程 | 更快的模型开发；更一致的结果；减少人工努力；更高的生产力。 |'
- en: '|  | Hyperparameter tuning | Optimized model performance; higher accuracy;
    faster training convergence; more efficient resource utilization. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 超参数调整 | 优化模型性能；更高的准确性；更快的训练收敛速度；更有效的资源利用。 |'
- en: '|  | Regularization techniques | More stable and generalizable models; reduced
    risk of overfitting; improved performance on unseen data. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 正则化技术 | 更稳定和可泛化的模型；降低过拟合的风险；在未见数据上的性能改进。 |'
- en: '|  | Reliable checkpointing | Reduced risk of losing progress; faster experimentation;
    improved model development workflows. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 可靠的检查点 | 降低丢失进度的风险；加快实验速度；改进模型开发工作流程。 |'
- en: '|  | Task-specific fine-tuning | Significantly improved performance on target
    tasks; faster time to market; more efficient use of resources. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | 任务特定微调 | 显著提高目标任务的性能；缩短上市时间；更有效地使用资源。 |'
- en: '|  | Model pruning | Faster inference speeds; reduced storage requirements;
    lower computational costs; enabling deployment on resource-constrained devices.
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型剪枝 | 加快的推理速度；降低存储需求；降低计算成本；使资源受限设备上的部署成为可能。 |'
- en: '|  | Quantization | Reduced model size; accelerated inference; lower memory
    footprint; improved energy efficiency; wider deployment possibilities. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 量化 | 减少模型大小；加速推理；降低内存占用；提高能源效率；更广泛的部署可能性。 |'
- en: '| **Quality** **and Alignment** | Rigorous evaluation metrics | Data-driven
    decision making; improved model selection; better understanding of model strengths
    and weaknesses. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **质量和对齐** | 严格的评估指标 | 数据驱动决策；改进模型选择；更好地理解模型的优势和劣势。 |'
- en: '|  | Cross-validation | More reliable performance estimates; reduced risk of
    overfitting; improved model generalization. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 交叉验证 | 更可靠的性能估计；降低过拟合风险；提高模型泛化能力。 |'
- en: '|  | Interpretability | Increased trust in model predictions; easier identification
    of errors; improved model understanding; facilitates debugging and refinement.
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 增加对模型预测的信任；更容易识别错误；提高模型理解；便于调试和改进。 |'
- en: '|  | Fairness and bias mitigation | More equitable and ethical outcomes; reduced
    risk of discrimination; increased user trust. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | 公平性和偏见缓解 | 更公平和道德的结果；降低歧视风险；提高用户信任。 |'
- en: '|  | Adversarial robustness | Enhanced security; improved reliability in unpredictable
    environments; protection against malicious attacks. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 对抗鲁棒性 | 提高安全性；在不可预测环境中的可靠性改善；抵御恶意攻击。 |'
- en: '|  | Reinforcement Learning from Human Feedback | Models aligned with human
    values; improved user experience; increased safety and trustworthiness. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | 从人类反馈中进行强化学习 | 与人类价值观一致的模式；改善用户体验；提高安全性和可靠性。 |'
- en: '| **Reasoning and** **Problem Solving** | Chain-of-thought | Enhanced problem-solving
    abilities; improved accuracy; increased transparency in decision-making. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **推理和问题解决** | 思维链 | 增强问题解决能力；提高准确性；决策透明度增加。 |'
- en: '|  | Tree-of-thoughts | Improved ability to handle complex and ambiguous problems;
    more robust solutions. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 思维树 | 提高处理复杂和模糊问题的能力；更稳健的解决方案。 |'
- en: '|  | ReAct | Ability to solve real-world problems effectively; improved adaptability;
    enhanced learning and reasoning. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | ReAct | 有效解决现实世界问题的能力；提高适应性；增强学习和推理。 |'
- en: '|  | Reasoning WithOut Observation | Enhanced problem-solving in data-scarce
    environments; improved decision-making with incomplete information. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 无观察推理 | 在数据稀缺环境中的问题解决能力增强；在信息不完整的情况下改善决策。 |'
- en: '|  | Reflection techniques | More self-aware and reliable models; improved
    accuracy; enhanced learning and adaptation. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 反思技术 | 更自我意识和可靠的模型；提高准确性；增强学习和适应能力。 |'
- en: '|  | Automatic multi-step reasoning | Ability to solve complex tasks autonomously;
    increased efficiency; reduced need for human intervention. |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 自动多步推理 | 能够自主解决复杂任务；提高效率；减少对人工干预的需求。 |'
- en: '| **Knowledge Integration (****RAG)** | Retrieval-augmented generation | Access
    to up-to-date information; reduced reliance on pre-trained knowledge; improved
    accuracy and relevance. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **知识集成（RAG**） | 检索增强生成 | 访问最新信息；减少对预训练知识的依赖；提高准确性和相关性。 |'
- en: '|  | Graph-based RAG | More sophisticated reasoning; improved accuracy in complex
    knowledge domains; enhanced understanding of relationships. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于图的RAG | 更复杂的推理；在复杂知识领域中的准确性提高；增强对关系的理解。 |'
- en: '|  | Advanced RAG techniques | Higher quality and more relevant information;
    improved accuracy and reliability of results. |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 高级RAG技术 | 更高质量和更相关的信息；提高结果准确性和可靠性。 |'
- en: '|  | Evaluating RAG systems | Optimized RAG systems; greater user satisfaction;
    higher quality outcomes. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 评估RAG系统 | 优化的RAG系统；更高的用户满意度；更高质量的结果。 |'
- en: '| **Agentic AI** | Agentic patterns | Ability to create autonomous systems;
    increased efficiency; reduced human intervention; enabling new applications. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **代理式AI** | 代理模式 | 能够创建自主系统；提高效率；减少人工干预；启用新应用。 |'
- en: Table 1.1 – Benefits of LLM design patterns
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 – LLM设计模式的好处
- en: Challenges in applying design patterns to LLMs
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将设计模式应用于LLM的挑战
- en: 'While the benefits of design patterns in LLM development are clear, their application
    is not without significant challenges. The unique nature of LLM systems, their
    rapid evolution, and the breadth of areas these patterns cover – from foundational
    data handling to complex agentic systems – present several obstacles:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然设计模式在LLM开发中的好处是显而易见的，但它们的运用并非没有重大挑战。LLM系统的独特性质、它们的快速演变以及这些模式涵盖的广泛领域（从基础数据处理到复杂的代理系统）都带来了几个障碍：
- en: '**Rapid technological evolution**: One of the primary challenges that remains
    is the breakneck speed of advancement in the LLM field. New model architectures,
    training methodologies, sophisticated prompting strategies, knowledge retrieval
    techniques, and agentic frameworks emerge constantly. This rapid flux means that
    patterns, even recently established ones for optimizing training or enhancing
    reasoning, may require frequent adaptation; otherwise, they can become less optimal
    quickly. Developers need a flexible mindset, balancing the need for stable practices
    such as disciplined data management with the agility to integrate breakthroughs.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速技术进步**：LLM 领域中持续快速发展的速度仍然是主要挑战之一。新的模型架构、训练方法、复杂的提示策略、知识检索技术和代理框架不断涌现。这种快速变化意味着，即使是最近建立的用于优化训练或增强推理的模式，也可能需要频繁的调整；否则，它们可能会迅速变得不那么优化。开发者需要灵活的心态，在需要稳定实践（如纪律性的数据管理）和整合突破的敏捷性之间取得平衡。'
- en: '**Complexity, scale, and unpredictability**: LLMs are inherently complex, operate
    at a massive scale, and often exhibit non-deterministic behavior. This poses challenges
    across the pattern spectrum:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性、规模和不可预测性**：大型语言模型（LLMs）本质上复杂，在巨大规模上运行，并且常常表现出非确定性行为。这在整个模式谱系中带来了挑战：'
- en: '**Data and training**: Applying patterns for managing large datasets, structuring
    training pipelines, or tuning hyperparameters effectively requires managing immense
    computational resources and data volumes.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据和训练**：应用管理大型数据集、构建训练管道或有效调整超参数的模式需要管理巨大的计算资源和数据量。'
- en: '**Behavioral control**: The stochastic nature of LLMs complicates the process
    of applying patterns that aim to ensure desired outcomes, such as those addressing
    fairness, bias, adversarial robustness, or even advanced techniques for step-by-step
    reasoning and action. Achieving consistent, predictable behavior is harder than
    in traditional software.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行为控制**：LLMs的随机性质使得应用旨在确保期望结果的模式（如解决公平性、偏见、对抗鲁棒性，甚至逐步推理和行动的高级技术）变得复杂。实现一致、可预测的行为比传统软件更难。'
- en: '**Error handling and debugging**: Pinpointing failures when using complex patterns,
    such as those involving multi-step reasoning chains or autonomous agent behaviors,
    can be incredibly difficult due to the opaque nature of the models.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误处理和调试**：由于模型的不透明性，在使用涉及多步推理链或自主代理行为的复杂模式时，定位失败可能极其困难。'
- en: '**Evaluation difficulties**: Measuring the effectiveness of applying many LLM
    design patterns is a major challenge. While patterns exist for defining evaluation
    metrics and validation processes, assessing nuanced aspects such as the quality
    of generated reasoning paths, the true helpfulness of retrieved context in RAG
    systems, or the overall robustness and task success of an agent often requires
    more than standard benchmarks. Developing reliable and comprehensive evaluation
    strategies for these advanced patterns is an ongoing research area.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估困难**：衡量应用许多LLM设计模式的有效性是一个主要挑战。虽然存在定义评估指标和验证过程的模式，但评估细微方面（如生成的推理路径的质量、RAG系统中检索到的上下文的真正有用性，或代理的整体鲁棒性和任务成功率）通常需要比标准基准更多的内容。为这些高级模式开发可靠和全面的评估策略是一个持续的研究领域。'
- en: '**Cost and resource constraints**: Implementing many LLM patterns can be resource-intensive
    in various ways:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本和资源限制**：实施许多LLM模式可能在各种方式上消耗资源：'
- en: '**Data costs**: Thorough data annotation and preparation can be expensive and
    time-consuming.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据成本**：彻底的数据标注和准备可能既昂贵又耗时。'
- en: '**Compute costs**: Core model training, extensive fine-tuning, large-scale
    hyperparameter searches, or running inference for complex retrieval-augmented
    or agentic systems requires significant computational power.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本**：核心模型训练、广泛的微调、大规模的超参数搜索或运行复杂检索增强或代理系统的推理需要大量的计算能力。'
- en: '**Optimization trade-offs**: Patterns aimed at model optimization, such as
    pruning or quantization, seek to reduce costs but involve their own complexities
    and potential performance trade-offs. The cost factor can limit the practical
    applicability of certain patterns for teams with constrained budgets.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化权衡**：旨在模型优化的模式，如剪枝或量化，旨在降低成本，但涉及自己的复杂性和潜在的性能权衡。成本因素可能限制了预算受限的团队对某些模式的实际应用性。'
- en: '**The interdisciplinary nature of LLM development**: Building effective LLM
    systems requires collaboration between diverse roles – software engineers, ML
    researchers, data scientists, prompt engineers, domain experts, ethicists, and
    more. Establishing a shared understanding and consistent application of patterns
    across these disciplines is crucial but challenging. For instance, ensuring everyone
    aligns on data management practices, interprets evaluation results similarly,
    or understands the implications of patterns designed to ensure fairness requires
    deliberate effort and clear communication.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM开发的跨学科性质**：构建有效的LLM系统需要不同角色之间的协作——软件工程师、机器学习研究人员、数据科学家、提示工程师、领域专家、伦理学家等。在这些学科之间建立共同的理解和一致的应用模式至关重要但具有挑战性。例如，确保每个人都对数据管理实践达成一致、对评估结果有相同的解读或理解旨在确保公平性的模式的影响，需要刻意努力和清晰的沟通。'
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter provided a foundational understanding of LLMs and introduced the
    role of design patterns in their development. It traced the evolution of language
    models from early statistical approaches to the transformer architecture-based
    LLMs of today, emphasizing key features such as the self-attention mechanism,
    the significance of scale and computational resources, few-shot learning, language
    understanding and generation capabilities, and multilingual abilities.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了对LLMs的基础理解，并介绍了设计模式在它们开发中的作用。它追溯了语言模型从早期的统计方法到今天基于transformer架构的LLMs的演变，强调了诸如自注意力机制、规模和计算资源的重要性、少样本学习、语言理解和生成能力以及多语言能力等关键特征。
- en: Then, this chapter transitioned to the importance of design patterns, drawing
    parallels with their established role in software engineering. This highlighted
    the benefits of applying design patterns to LLM development, outlining a structured
    approach for improving data quality, optimizing training, addressing model quality
    and alignment, enhancing reasoning capabilities, integrating external knowledge
    through RAG, and developing agentic applications. Then, the 29 patterns that will
    be explored throughout this book were outlined, as well as what stage of the LLM
    life cycle they focus on.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，本章转向了设计模式的重要性，将其与软件工程中已确立的角色进行类比。这突出了将设计模式应用于LLM开发的益处，概述了一种结构化的方法来提高数据质量、优化训练、解决模型质量和一致性、增强推理能力、通过RAG（检索增强生成）整合外部知识以及开发代理应用。然后，概述了本书中将探讨的29种模式，以及它们关注的LLM生命周期阶段。
- en: Finally, this chapter acknowledged the challenges in applying design patterns
    to LLMs, all of which stem from rapid technological evolution, complexity and
    scale, evaluation difficulties, cost constraints, and the interdisciplinary nature
    of LLM development.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章承认了将设计模式应用于LLMs（大型语言模型）所面临的挑战，所有这些挑战都源于技术的快速演变、复杂性、规模、评估困难、成本限制以及LLM开发的跨学科性质。
- en: In the rest of this book, we will guide you through the LLM development life
    cycle using design patterns, starting with building a solid data foundation (*Chapters
    2* to *6*) and optimizing model training (*Chapters 7* to *13*). Then, we’ll focus
    on ensuring model quality, alignment, and robustness (*Chapters 14* to *19*) before
    exploring advanced reasoning and problem-solving capabilities (*Chapters 20* to
    *25*). Finally, we’ll cover integrating external knowledge with RAG (*Chapters
    26* to *29*) and delve into the future of LLMs with agentic AI ([*Chapter 30*](B31249_30.xhtml#_idTextAnchor469)),
    thus providing a comprehensive toolkit for building intelligent applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的剩余部分，我们将使用设计模式引导您通过LLM开发的生命周期，从构建坚实的基础数据（第2章至第6章）和优化模型训练（第7章至第13章）开始。然后，我们将专注于确保模型质量、一致性和鲁棒性（第14章至第19章），在探索高级推理和问题解决能力（第20章至第25章）之前。最后，我们将涵盖将外部知识整合到RAG（第26章至第29章），并深入探讨具有代理人工智能的LLM的未来（[第30章](B31249_30.xhtml#_idTextAnchor469)），从而为构建智能应用提供一个全面的工具包。
