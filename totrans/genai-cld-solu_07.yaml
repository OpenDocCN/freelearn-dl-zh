- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: 'Deploying ChatGPT in the Cloud: Architecture Design and Scaling Strategies'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云中部署ChatGPT：架构设计和扩展策略
- en: In the previous chapters, you learned more about how to fine-tune LLMs and add
    external data. You also gained a deep understanding of how prompts and responses
    work under the covers. Then, you learned how to develop applications with GenAI
    while using popular programming frameworks for the various LLMs. As we continue
    building on our learning of GenAI/ChatGPT for cloud solutions, we will realize
    that limits are placed on how these cloud services process tokens for prompts
    and completions. As large-scale deployments need to be “enterprise-ready,” we
    must take advantage of the cloud to provide the necessary services and support
    to enable an enterprise solution, with less effort than creating a service from
    the ground up, on our own. Services, such as security (this topic will be covered
    in more detail in the next chapter) and identity, are pre-baked into a cloud service,
    and thus in the cloud solution we are trying to build. However, limits are imposed
    by a cloud provider and we must understand these limits and design around them
    for a successful cloud solution.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学习了更多关于如何微调LLMs和添加外部数据的方法。你还深入了解了提示和响应在底层是如何工作的。然后，你学习了如何使用各种LLMs的流行编程框架开发应用程序。随着我们继续在GenAI/ChatGPT云解决方案的学习中前进，我们将意识到这些云服务在处理提示和完成令牌时存在限制。大规模部署需要“企业就绪”，我们必须利用云来提供必要的服务和支持，以实现企业解决方案，而无需从头开始创建服务，这样我们付出的努力会更少。例如，安全（这个主题将在下一章中更详细地介绍）和身份等服务已经预置在云服务中，因此在我们试图构建的云解决方案中也是如此。然而，云服务提供商施加了限制，我们必须了解这些限制，并围绕它们设计以实现成功的云解决方案。
- en: In this chapter, we’ll focus on understanding that GenAI can be scaled to support
    many thousands of users, with a large number of concurrent connections, and submitting
    prompts. This is not only limited to users of GenAI and can also include applications
    and other LLMs, to name a few. The entire solution, from architecture design,
    deployment, scaling, performance tuning, monitoring, and logging all combine to
    make a robust, scalable cloud solution for ChatGPT.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于理解GenAI可以扩展以支持成千上万的用户，拥有大量的并发连接，并提交提示。这不仅限于GenAI的用户，还可以包括应用程序和其他LLMs等，仅举几例。整个解决方案，从架构设计、部署、扩展、性能调整、监控和日志记录，都结合在一起，为ChatGPT提供了一个强大、可扩展的云解决方案。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding limits
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解限制
- en: Cloud scaling, design patterns, and error handling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云扩展、设计模式和错误处理
- en: Monitoring, logging, and HTTP response codes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控、日志记录和HTTP响应代码
- en: Costs, training and support
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本、培训和支援
- en: '![Figure 7.1 – Too many requests and too many tokens](img/B21443_07_1.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 请求过多和令牌过多](img/B21443_07_1.jpg)'
- en: Figure 7.1 – Too many requests and too many tokens
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 请求过多和令牌过多
- en: Understanding limits
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解限制
- en: Any large-scale cloud deployment needs to be “enterprise-ready,” ensuring both
    the end user experience is acceptable and the business objectives and requirements
    are met. “Acceptable” is a loose term that can vary per user and workload. To
    understand how to scale to meet any user or business requirements, as the appetite
    for a service increases, we must first understand the basic limits, such as token
    limits. We covered these limits for most of the common generative AI GPT models
    in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098), however, we will quickly revisit
    them here.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 任何大规模云部署都需要“企业就绪”，确保最终用户体验可接受，并且满足业务目标和要求。“可接受”是一个相对宽泛的术语，可能因用户和负载而异。为了了解如何扩展以满足任何用户或业务需求，随着对服务的需求增加，我们必须首先了解基本限制，例如令牌限制。我们在[*第5章*](B21443_05.xhtml#_idTextAnchor098)中涵盖了大多数常见生成式AI
    GPT模型的相关限制，然而，我们在这里将快速回顾它们。
- en: As organizations scale up using an enterprise-ready service, such as Azure OpenAI,
    there are rate limits on how fast tokens are processed in the prompt+completion
    request. There is a limit to how many text prompts can be sent due to these token
    limits for each model that can be consumed in a single prompt+completion. It is
    important to note that the overall size of tokens for rate limiting includes *both*
    the prompt (text sent to the AOAI model) size *plus* the return completion (response
    back from the model) size, and depending on the model, the token limits on the
    model will vary. That is, the number of maximum token numbers used per a single
    prompt, will vary depending on the GenAI model used.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织使用企业级服务（如 Azure OpenAI）进行扩展，提示+完成请求中令牌的处理速度存在速率限制。由于每个模型在单个提示+完成中可以消耗的令牌限制，因此可以发送的文本提示数量有限。重要的是要注意，速率限制的令牌总体大小包括提示（发送到
    AOAI 模型的文本）大小以及返回的完成（模型返回的响应）大小，并且根据模型的不同，模型上的令牌限制也会有所不同。也就是说，每个提示使用的最大令牌数将取决于所使用的通用人工智能模型。
- en: You can see your rate limits on the Azure OpenAI overview page or OpenAI account
    page. You can also view important information about your rate limits, such as
    the remaining requests, tokens, and other metadata in the headers of the HTTP
    response. Please see the reference link at the end of this chapter for details
    on what these header fields contain.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Azure OpenAI 概览页面或 OpenAI 账户页面查看您的速率限制。您还可以在 HTTP 响应头中查看有关速率限制的重要信息，例如剩余请求数、令牌和其他元数据。请参阅本章末尾的参考链接，了解这些头字段包含的详细信息。
- en: 'Here are a few token limits for various GPT models:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些不同 GPT 模型的令牌限制：
- en: '| **Model** | **Token Limit** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **令牌限制** |'
- en: '| --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-3.5-turbo 0301 | 4,096 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo 0301 | 4,096 |'
- en: '| GPT-3.5-turbo-16k | 16,385 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-16k | 16,385 |'
- en: '| GPT-3.5-turbo-0613 | 4,096 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-0613 | 4,096 |'
- en: '| GPT-3.5-turbo-16k-0613 | 16,384 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-16k-0613 | 16,384 |'
- en: '| GPT-4 | 8,192 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 8,192 |'
- en: '| GPT-4-0613 | 32,768 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-0613 | 32,768 |'
- en: '| GPT-4-32K | 32,768 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-32K | 32,768 |'
- en: '| GPT-4-32-0613 | 32,768 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-32-0613 | 32,768 |'
- en: '| GPT-4-Turbo | 128,000 (context) and 4,096 (output) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo | 128,000（上下文）和 4,096（输出）|'
- en: Figure 7.2 – Token limits for some GenAI models
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 一些通用人工智能模型的令牌限制
- en: While we already discussed prompt optimization techniques earlier in this book,
    in this chapter, we will look at some of the other ways to scale an enterprise-ready
    cloud GenAI service for applications and services that can easily exceed the token
    limits for a specific model and scale effectively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经在本书前面讨论了提示优化技术，但在本章中，我们将探讨一些其他方法来扩展企业级云通用人工智能服务，以适应那些可以轻松超过特定模型令牌限制的应用和服务。
- en: Cloud scaling and design patterns
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云扩展和设计模式
- en: Since you learned about some of the limits imposed by Azure OpenAI and OpenAI
    in the previous section, we will now look at how to overcome these limits.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您在上一节中已经了解了 Azure OpenAI 和 OpenAI 施加的一些限制，我们现在将探讨如何克服这些限制。
- en: Overcoming these limits through a well-designed architecture or design pattern
    is critical for businesses to ensure they are meeting any internal **service-level
    agreements** (**SLAs**) and are providing a robust service without a lot of latency,
    or delay, in the user or application experience.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过精心设计的架构或设计模式克服这些限制对于企业来说至关重要，以确保它们满足任何内部**服务级别协议**（**SLAs**），并提供一个没有太多延迟或延迟的用户或应用体验的强大服务。
- en: What is scaling?
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是扩展？
- en: As we described earlier, limits are imposed on any cloud architecture, just
    as there are hardware limits on your laptop (amount of RAM or disk space), on-premises
    data centers, and so on. Resources are finite, so we have come to expect these
    limits, even in cloud services. However, there are a few techniques we can use
    to overcome limitations so that we can meet our business requirements or user
    behavior and appetite.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所描述的，任何云架构都会受到限制，就像您的笔记本电脑（RAM 或磁盘空间）上存在硬件限制一样，本地数据中心等。资源是有限的，因此我们已经在云服务中期待这些限制。然而，我们可以使用一些技术来克服这些限制，以满足我们的业务需求或用户行为和需求。
- en: Understanding TPM, RPM, and PTUs
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 TPM、RPM 和 PTUs
- en: As we scale, we will need to understand some additional terminology, such as
    **tokens per minute** (**TPM**), **request per minute** (**RPM**), and **provisioned
    throughput units** (**PTUs**), as well as other additional services, such as **Azure
    API Management** (**APIM**), which support a cloud environment in Azure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们扩展，我们需要了解一些额外的术语，例如**每分钟令牌数**（**TPM**）、**每分钟请求数**（**RPM**）和**已配置吞吐量单位**（**PTUs**），以及其他一些附加服务，例如**Azure
    API 管理**（**APIM**），这些服务支持 Azure 中的云环境。
- en: TPMs
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TPMs
- en: With a cloud provider such as Microsoft Azure, Azure OpenAI’s quota management
    service built into Azure AI Studio enables you to assign quota limits for your
    deployments, up to whatever amount is the specified limit – that is, your “quota.”
    You can assign a quota to an Azure subscription on a per-region, per-model basis
    in units of TPM. The billing component of TPM is also known as pay-as-you-go,
    where pricing will be based on the pay-as-you-go consumption model, with a price
    per unit specific for each type of model deployed. Please refer to *Figure 7**.2*
    for a list of some models and what their token limit is.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像微软 Azure 这样的云提供商，Azure OpenAI 的配额管理服务内置在 Azure AI Studio 中，使你能够为你的部署分配配额限制，上限为指定的限制金额——即你的“配额”。你可以在每个区域、每个模型的基础上，以
    TPM 为单位为 Azure 订阅分配配额。TPM 的计费组件也称为按量付费，其中定价将基于按量付费的消费模式，每个部署的模型类型都有特定的单价。请参阅*图
    7**.2*以了解一些模型及其令牌限制列表。
- en: When you create an Azure OpenAI service within a subscription, you will receive
    the default TPM quota size. You can then adjust the TPM to that deployment or
    any additional deployment you create, at which point the overall **available**
    quota for that model will be reduced by that amount. TPMs/pay-as-you-go are also
    the default mechanism for billing within the Azure OpenAI (AOAI) service. We will
    cover some of the costs a bit later, but for more details on AOAI quota management,
    take a look at the link provided at the end of this chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在订阅内创建 Azure OpenAI 服务时，你将收到默认的 TPM 配额大小。然后你可以调整 TPM 到那个部署或你创建的任何其他部署，此时该模型的总体**可用**配额将减少相应数量。TPMs/按量付费也是
    Azure OpenAI（AOAI）服务中的默认计费机制。我们将在稍后讨论一些成本，但有关 AOAI 配额管理的更多详细信息，请参阅本章末尾提供的链接。
- en: If you are using OpenAI directly, scaling works very similarly – in OpenAI models,
    you can scale by adjusting the TPM bar to the “max” under the advanced options.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接使用 OpenAI，扩展操作非常相似——在 OpenAI 模型中，你可以通过在高级选项下将 TPM 滑块调整到“最大”来扩展。
- en: Now, let’s look at an example and deep dive into TPMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个例子，并深入了解 TPMs。
- en: In the Microsoft Azure cloud, for example, there is an overall limit (quota)
    of 240,000 TPMs for GPT-35-Turbo in the Azure East US region. This means you can
    have a *single deployment of 240K TPM* per Azure OpenAI account, *two deployments
    of 120K TPM each*, or any number of deployments in one or multiple deployments,
    so long as the TPMs add up to 240K (or less) total in the East US region.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在微软 Azure 云中，Azure 东部美国地区 GPT-35-Turbo 的整体限制（配额）为 240,000 TPM。这意味着每个 Azure
    OpenAI 账户可以有一个 240K TPM 的*单个部署*，*两个各 120K TPM 的部署*，或者在一个或多个部署中任意数量的部署，只要 TPM 总数不超过
    240K（或更少）。
- en: So, *one way to scale up is by adding ADDITIONAL (Azure) OpenAI accounts*. With
    additional AOAI accounts, you can stack or add limits together. So, in this example,
    rather than having a single 240K GPT-35-Turbo limit, we can add an additional
    240K times *X*, where *X* is 30 or less.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*一种扩展的方法是通过添加额外的（Azure）OpenAI 账户*。有了额外的 AOAI 账户，你可以堆叠或合并限制。所以，在这个例子中，我们不是只有一个
    240K 的 GPT-35-Turbo 限制，我们可以添加额外的 240K 倍 *X*，其中 *X* 是 30 或更少。
- en: The maximum number of Azure OpenAI accounts (or resources) *per region per Azure
    subscription* is *30* (at the time of writing) and is also dependent on regional
    capacity *availability*. We expect this number to be increased over time as additional
    GPU-based capacity continues to be made available.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Azure 订阅在每个区域中每个 Azure OpenAI 账户（或资源）的最大数量（截至写作时）为 30，并且也取决于区域容量的*可用性*。我们预计这个数字会随着时间的推移而增加，因为基于
    GPU 的额外容量将继续被提供。
- en: RPM
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RPM
- en: Beyond the TPM limit, an RPM rate limit is also enforced, where the amount of
    RPM available to a model is set proportionally to the TPM assignment using a ratio
    of 6 RPM per 1,000 TPM.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 超过 TPM 限制后，还会实施 RPM 速率限制，其中模型可用的 RPM 数量按比例设置，使用 1,000 TPM 对应 6 RPM 的比率。
- en: 'RPM is not a direct billing component, but it is a component of rate limits.
    It is important to note that while the billing for AOAI is token-based (TPM),
    the actual two triggers in which rate limits occur are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RPM不是一个直接的计费组件，但它是速率限制的一部分。需要注意的是，尽管AOAI的计费基于令牌（TPM），但实际触发速率限制的两个触发器如下：
- en: On a per-second basis, not at the per-minute billing level.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 按每秒计算，而不是按每分钟计费级别。
- en: The rate limit will occur at either **tokens per second** (**TPS**) or RPM evaluated
    over a small period (1-10 seconds). That is, if you exceed the total TPS for a
    specific model, then a rate limit applies. If you exceed the RPM over a short
    period, then a rate limit will also apply, returning limit error codes (429).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 速率限制将在每秒**令牌数**（**TPS**）或RPM（在短时间段内评估，1-10秒）发生。也就是说，如果您超过特定模型的TPS总和，则将应用速率限制。如果您在短时间内超过RPM，则也会应用速率限制，返回限制错误代码（429）。
- en: The throttled rate limits can easily be managed using the scaling special sauce,
    as well as following some of the best practices described later in this chapter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过缩放特殊配方以及遵循本章后面描述的一些最佳实践轻松管理节流速率限制。
- en: You can read more about quota management and the details on how TPM/RPM rate
    limits apply in the *Manage Azure OpenAI Service* link at the end of this chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章末尾的“管理Azure OpenAI服务”链接中了解更多关于配额管理和TPM/RPM速率限制的详细信息。
- en: PTUs
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PTUs
- en: The Microsoft Azure cloud recently introduced the ability to use reserved capacity,
    or PTUs, for AOAI earlier this summer. Beyond the default TPMs described above,
    this new Azure OpenAI service feature, PTUs, defines the model processing capacity,
    using reserved resources, for processing prompts and generating completions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Azure云最近在今年的早些时候引入了使用预留容量，或PTUs，为AOAI的能力。除了上述默认的TPMs之外，这项新的Azure OpenAI服务功能，PTUs，定义了使用预留资源处理提示和生成完成内容的模型处理能力。
- en: '*PTUs are another way an enterprise can scale up to meet business requirements
    as they can provide reserved capacity for your most demanding and complex* *prompt/completion
    scenarios.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*PTUs是企业可以扩展以满足业务需求的一种方式，因为它们可以为您的最苛刻和复杂的* *提示/完成场景* *提供预留容量。*'
- en: Different types of PTUs are available, where size of these PTUs is available
    in smaller increments or larger increments of PTU units. For example, the first
    version of PTUs, which we will call Classic PTUs, and newer PTU offerings, such
    as “managed” PTUs, size offering differs to accommodate various size workloads
    in a more predictable fashion.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 可提供不同类型的PTUs，这些PTUs的大小以较小的增量或较大的PTU单位增量提供。例如，PTUs的第一个版本，我们将称之为经典PTUs，以及较新的PTU产品，如“管理”PTUs，其大小提供不同，以便以更可预测的方式适应各种大小的工作负载。
- en: PTUs are purchased as a monthly commitment with an auto-renewal option, which
    will *reserve* AOAI capacity within an Azure subscription, using a specific model,
    in a specific Azure region. Let’s say you have 300 PTUs provisioned for GPT 3.5
    Turbo. The PTUs are only provisioned for GPT 3.5 Turbo deployments, within a specific
    Azure subscription, not for GPT 4\. You can have separate PTUs for GPT 4, with
    the minimum PTUs described in the following table , for classic PTUs. There are
    also managed PTUs, which can vary in min. size.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: PTUs作为每月承诺购买，具有自动续订选项，这将在Azure订阅中使用特定模型在特定Azure区域中*预留* AOAI容量。假设您为GPT 3.5 Turbo配置了300
    PTUs。PTUs仅针对GPT 3.5 Turbo部署在特定Azure订阅中配置，不适用于GPT 4。您可以为GPT 4拥有单独的PTUs，以下表格中描述了经典PTUs的最小PTUs。还有管理PTUs，其最小大小可能不同。
- en: 'Keep in mind that while having reserved capacity does *provide consistent latency,
    predictable performance and throughput*, this throughput amount is highly dependent
    on your scenario – that is, throughput will be affected by a few items, including
    *the number and ratio of prompts and generation tokens, the number of simultaneous
    requests, and the type and version of the model used*. The following table describes
    the *approximate* TPMs expected concerning PTUs per model. Throughput can vary,
    so an approximate range has been provided:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，虽然预留容量可以提供一致的延迟、可预测的性能和吞吐量，但这种吞吐量量高度依赖于您的场景 – 即吞吐量将受到一些因素的影响，包括*提示和生成令牌的数量和比率、并发请求数量以及使用的模型类型和版本*。以下表格描述了有关PTUs的*大致*TPMs预期值。吞吐量可能会有所不同，因此提供了一个近似范围：
- en: '![Figure 7.3 – Approximate throughput range of Classic PTUs](img/B21443_07_2.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 经典PTUs的大致吞吐量范围](img/B21443_07_2.jpg)'
- en: Figure 7.3 – Approximate throughput range of Classic PTUs
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 经典PTUs的大致吞吐量范围
- en: As you can scale by creating multiple (Azure) OpenAI accounts, you can *also
    scale by increasing the number of PTUs*. For scaling purposes, you can multiply
    the minimum number of PTUs required in terms of whatever your application or service
    requires.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您可以通过创建多个（Azure）OpenAI 账户进行扩展，因此您也可以通过增加 PTU 的数量进行扩展。为了扩展目的，您可以乘以您应用程序或服务所需的最小
    PTU 数量。
- en: 'The following table describes this scaling of classic PTUs:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下表描述了经典 PTU 的这种扩展：
- en: '| **Model** | **Minimum Classic PTUs Required to** **Create Deployment** |
    **Classic PTUs for Incrementally Scaling** **the Deployment** | **Example Deployment**
    **Sizes (PTUs)** |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **创建部署所需的最小经典 PTU 数量** | **部署增量扩展所需的经典 PTU 数量** | **示例部署大小（PTU）**
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT-3.5-Turbo (4K) | 300 | 100 | 300, 400, 500… |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo (4K) | 300 | 100 | 300, 400, 500… |'
- en: '| GPT-3.5-Turbo (16K) | 600 | 200 | 600, 800, 1,000... |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo (16K) | 600 | 200 | 600, 800, 1,000... |'
- en: '| GPT-4 (8K) | 900 | 300 | 900, 1,200, 1,500… |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (8K) | 900 | 300 | 900, 1,200, 1,500… |'
- en: '| GPT-4 (32K) | 1,800 | 600 | 1,800, 2,400, 3,000… |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (32K) | 1,800 | 600 | 1,800, 2,400, 3,000… |'
- en: Figure 7.4 – PTU minimums and incremental scaling (classic PTU)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – PTU 最小值和增量扩展（经典 PTU）
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The PTU size and type are continuously evolving. The two tables above are just
    to give a sense about the approximate scale of the PTUs with respect to TPMs and
    how it differs based on model and version. For more updated information, you can
    visit the Provisioned Throughput Units (PTU) getting started guide.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: PTU 的大小和类型正在不断演变。上面的两个表格只是为了给出 PTU 相对于 TPM 的近似规模以及它如何根据模型和版本而有所不同。有关更多信息，您可以访问“已配置吞吐量单元（PTU）入门指南”。
- en: Now we have understood the essential components for scaling purposes like TPM,
    RPM and PTU. Now let’s delve into the scaling strategies and how to circumvent
    these limits with our special scaling sauce for a large-scale and global enterprise-ready
    application.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了用于扩展目的的基本组件，如 TPM、RPM 和 PTU。现在让我们深入了解扩展策略以及如何使用我们为大型和全球企业级应用程序量身定制的特殊扩展方法来规避这些限制。
- en: Scaling Design patterns
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展设计模式
- en: One area we haven’t covered yet is how these multiple TPMs or PTU-based Azure
    OpenAI accounts can work in unison. That is, once you have set up multiple AOAI
    accounts, how would you send prompts to each? Or, if you are sending too many
    prompts at once, how can you manage the error/response codes?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未涉及的一个领域是，这些基于多个 TPM 或 PTU 的 Azure OpenAI 账户如何协同工作。也就是说，一旦您设置了多个 AOAI 账户，您将如何向每个账户发送提示？或者，如果您一次发送了太多的提示，您如何管理错误/响应代码？
- en: The answer is by using the Azure APIM service. APIs form the basis of an APIM
    service instance. Each API consists of a group of operations that app developers
    can use. Each API has a link to the backend service that provides the API, and
    its operations correspond to backend operations. Operations in APIM have many
    configuration options, with control over URL mapping, query and path parameters,
    request and response content, and operation response caching. We won’t cover these
    additional features, such as URL mapping and response caching, in this book, but
    you can read more about APIM in the reference link at the end of this chapter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是通过使用 Azure APIM 服务。API 是 APIM 服务实例的基础。每个 API 都由一组应用程序开发者可以使用的操作组成。每个 API
    都有一个链接到后端服务的链接，其操作对应于后端操作。APIM 中的操作有许多配置选项，包括对 URL 映射、查询和路径参数、请求和响应内容以及操作响应缓存的控制。我们不会在本章中涵盖这些附加功能，如
    URL 映射和响应缓存，但您可以在本章末尾的参考链接中了解更多关于 APIM 的信息。
- en: '*Using APIM is yet another way to help organizations scale up to meet business
    and* *user requirements.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用 APIM 是帮助组织扩展以满足业务和用户需求的另一种方式。*'
- en: For example, you can also create a “spillover” scenario, where you may be sending
    prompts to PTUs that have been enabled for deploying an AOAI account. Then, if
    you exceed PTU limits, you can spill over to a TPM-enabled AOAI account that is
    used in the pay-as-you-go model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你也可以创建一个“溢出”场景，其中你可能正在向已启用部署 AOAI 账户的 PTU 发送提示。然后，如果你超过了 PTU 限制，你可以溢出到一个用于按量付费模型的
    TPM 启用 AOAI 账户。
- en: 'The following figure shows the basic setup, but this architecture can scale
    and also include many other Azure cloud resources. However, for simplicity and
    focus, only the relevant services are depicted here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了基本设置，但此架构可以扩展并包括许多其他 Azure 云资源。然而，为了简单和专注，这里只描绘了相关的服务：
- en: '![Figure 7.5 – AOAI and APIM in a single Azure region](img/Figure_7.5_-_AOAI_and_APIM_in_a_single_Azure_region.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 单个Azure区域中的AOAI和APIM](img/Figure_7.5_-_AOAI_and_APIM_in_a_single_Azure_region.jpg)'
- en: Figure 7.5 – AOAI and APIM in a single Azure region
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 单个Azure区域中的AOAI和APIM
- en: As described in the single region scenario, you can use APIM to queue and send
    prompts to any AOAI endpoint, so long as those endpoints can be reached. In a
    multi-region example, as shown in the following figure, we have two AOAI accounts
    in one region (one PTU and another TPM), and then a third Azure OpenAI account
    in another Azure region.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如在单个区域场景中所述，您可以使用APIM将提示排队并发送到任何AOAI端点，只要这些端点可以访问。在以下图例的多区域示例中，我们有一个区域中的两个AOAI账户（一个PTU和一个TPM），然后在另一个Azure区域中有一个第三个Azure
    OpenAI账户。
- en: 'Thus, a single APIM service can easily scale and support many AOAI accounts,
    even across multiple regions, as described here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单个APIM服务可以轻松扩展并支持许多AOAI账户，甚至跨多个区域，如以下所述：
- en: '![Figure 7.6 – Multi-region AOAI deployment using a single APIM service](img/Figure_7.6_-_Multi-region_AOAI_deployment_using_a_single_APIM_service.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 使用单个APIM服务的多区域AOAI部署](img/Figure_7.6_-_Multi-region_AOAI_deployment_using_a_single_APIM_service.jpg)'
- en: Figure 7.6 – Multi-region AOAI deployment using a single APIM service
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 使用单个APIM服务的多区域AOAI部署
- en: As you can see, a single APIM service can serve multiple AOAI accounts, both
    in the same Azure region *and* also in multiple regions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，单个APIM服务可以为多个AOAI账户提供服务，无论是在同一Azure区域，还是在多个区域。
- en: 'As we continue our “scaling” journey, it is a good time to mention that APIM
    has three production-level tiers: Basic, Standard, and Premium. With the Premium
    tier, you can use a single APIM instance in as many Azure regions as you need,
    so long as APIM can access the AOAI endpoint in the other region(s). When you
    make an APIM service, the instance has only one unit in a single Azure region
    (the main region). What does this provide? If you have a multi-regional Azure
    OpenAI deployment, does this mean you are required to also have a multi-region
    (Premium) SKU of APIM? No, not necessarily. As shown in the preceding multi-region
    architecture, a single APIM service instance can support multi-region, multi-AOAI
    accounts. Having a single APIM service makes sense when an application using the
    service is in the same region and you do not need **disaster** **recovery** (**DR**).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续“扩展”之旅的过程中，提到APIM有三个生产级层级：基本、标准和高级。使用高级层级，您可以在所需的任意多个Azure区域中使用单个APIM实例，只要APIM可以访问其他区域（s）中的AOAI端点。当您创建APIM服务时，实例在单个Azure区域（主区域）中只有一个单元。这提供了什么？如果您有一个多区域Azure
    OpenAI部署，这意味着您也必须拥有多区域（高级）SKU的APIM吗？不，不一定。如前所述的多区域架构所示，单个APIM服务实例可以支持多区域、多AOAI账户。当使用该服务的应用程序位于同一区域且您不需要**灾难**
    **恢复**（**DR**）时，拥有单个APIM服务是有意义的。
- en: However, as this chapter is about scaling at an enterprise level, we recommend
    multiple APIM service accounts to cover the DR scenario using the APIM Premium
    SKU.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于本章是关于企业级扩展，我们建议使用多个APIM服务账户，以使用APIM高级SKU覆盖DR场景。
- en: The Premium SKU allows you to have one region be the primary and any number
    of regions as secondaries. In this case, you can use a secondary, or multiple
    secondaries, in different scenarios – for example, if you are planning for any
    DR scenarios, which is always recommended for any enterprise architecture. Note
    that your enterprise applications should also be designed for data resiliency
    using DR strategies. Another example is if you are monitoring the APIM services.
    If you are seeing extremely heavy usage and can scale out your application(s)
    across regions, then you may want to deploy APIM service instances across multiple
    regions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 高级SKU允许您将一个区域设为主要区域，并将任意数量的区域设为辅助区域。在这种情况下，您可以在不同的场景中使用辅助区域，或多个辅助区域——例如，如果您正在计划任何灾难恢复（DR）场景，这对于任何企业架构来说都是推荐的。请注意，您的企业应用程序也应设计为使用DR策略实现数据弹性。另一个例子是如果您正在监控APIM服务。如果您看到极端的密集使用，并且可以将您的应用程序（s）扩展到多个区域，那么您可能希望在不同区域部署APIM服务实例。
- en: "For more information on how to deploy an APIM service instance to multiple\
    \ Azure regions, please see How to deploy an Azure API Management service instance\
    \ to multiple Azure regions: [https://](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-reg\uFEFF\
    ion)https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region"
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: "有关如何将 APIM 服务实例部署到多个 Azure 区域的更多信息，请参阅如何将 Azure API 管理服务实例部署到多个 Azure 区域：[https://](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-reg\uFEFF\
    ion)https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region"
- en: Retries with exponential backoff – the scaling special sauce
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指数退避重试——扩展的特殊配方
- en: So, how do we control (or queue) messages when using multiple Azure OpenAI instances
    (accounts)? How do we manage return error codes highly efficiently to optimize
    the AOAI experience?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们使用多个 Azure OpenAI 实例（账户）时，如何控制（或排队）消息？我们如何高效地管理返回的错误代码以优化 AOAI 体验？
- en: As a best practice, Microsoft, and any other cloud vendor, will recommend the
    use of “retry logic” or a “retry pattern” whenever using a cloud service. This
    retry pattern, when used in cloud applications, helps the applications deal with
    temporary (transient) failures while then attempting to re-establish a connection,
    or reconnect, to a service to perform requests on that service, thus automatically
    repeating a failed operation without additional user intervention. As cloud services
    are cloud-based and applications or users are remote to the cloud-based service,
    this retry pattern is paramount. This retry logic can improve the stability of
    the application and provide a better end user experience.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最佳实践，Microsoft 和任何其他云服务提供商都会建议在使用云服务时使用“重试逻辑”或“重试模式”。当在云应用程序中使用此重试模式时，有助于应用程序处理暂时性（瞬态）故障，同时尝试重新建立与服务之间的连接，或重新连接，以在该服务上执行请求，从而自动重复失败的操作，而无需额外的用户干预。由于云服务是基于云的，而应用程序或用户对基于云的服务是远程的，因此此重试模式至关重要。这种重试逻辑可以提高应用程序的稳定性并提供更好的最终用户体验。
- en: Using a cloud-based service, such as ChatGPT on Azure OpenAI, especially at
    scale via an application, is no exception.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于云的服务，例如 Azure OpenAI 上的 ChatGPT，尤其是在通过应用程序进行扩展时，也不例外。
- en: While you can add some retry logic directly to your application, you are quite
    limited as you scale across the enterprise. Are you now using the retry logic
    again and again with every application? What if the application was written by
    a third party? In that scenario, you can’t (usually) edit code directly.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以在应用程序中直接添加一些重试逻辑，但随着企业规模的扩大，你的选择相当有限。你现在是否在每个应用程序中都重复使用重试逻辑？如果应用程序是由第三方编写的呢？在这种情况下，你通常无法直接编辑代码。
- en: Instead, to achieve stability and high scalability, using the APIM service described
    previously will provide the necessary retry pattern/logic. For example, if your
    application sends prompt and if the server is too busy or some other error occurs,
    APIM will be able to resend the same prompt again, without any additional end
    user interaction. This will all happen seamlessly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，为了实现稳定性和高可扩展性，使用之前描述的 APIM 服务将提供必要的重试模式/逻辑。例如，如果您的应用程序发送提示，而服务器过于繁忙或发生其他错误，APIM
    将能够重新发送相同的提示，而无需任何额外的最终用户交互。这一切都将无缝发生。
- en: APIM allows us to do this easily using the scaling special sauce – the concept
    of *retries with exponential backoff*, which allows for extremely high, concurrent
    user loads.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: APIM 允许我们通过使用扩展的特殊配方——即“指数退避重试”的概念，轻松地做到这一点，这允许极高的并发用户负载。
- en: Retries with exponential backoff is a method that tries an operation again,
    with a wait time that grows exponentially, until it reaches a maximum number of
    retries (the exponential backoff). This technique accepts the fact that cloud
    resources may sometimes be unreachable for more than a few seconds for any reason,
    known as a transient error, or if an error is returned due to too many tokens
    per second being processed in a large-scale deployment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 指数退避重试是一种尝试操作的方法，等待时间呈指数增长，直到达到最大重试次数（指数退避）。这种技术接受这样一个事实，即云资源可能由于任何原因在几秒钟内无法访问，这被称为瞬态错误，或者如果由于大规模部署中每秒处理的令牌过多而返回错误。
- en: 'This can be accomplished via APIM’s retry policy. Here’s an example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过 APIM 的重试策略来实现。以下是一个示例：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, the error is specific to an HTTP response status code equal
    to 429, which is the return code for “server busy.” This states that *too many
    concurrent requests* were sent to a particular model, measured at *a per-second
    rate*. This can occur as an enterprise organization is scaling to a large number
    of users.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，错误是针对一个等于 429 的 HTTP 响应状态码的，这是“服务器忙碌”的返回代码。这表明向特定模型发送了太多的并发请求，以每秒的速率来衡量。这可能会在企业组织扩展到大量用户时发生。
- en: 'Here are the detailed values and explanation of the APIM policy statement:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 APIM 策略声明的详细值和解释：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The format and what each value means is fairly evident, however for a deeper
    dive, you can learn more about the parameters by reading the link to the documentation
    provided at the end of this chapter.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 格式和每个值的含义相当明显，然而，为了更深入的了解，你可以通过阅读本章末尾提供的文档链接来了解更多关于参数的信息。
- en: The main and extremely important point to understand is that when the APIM’s
    interval, max interval, and delta parameters are specified, as they are in the
    preceding example, then an *exponential interval retry* algorithm is automatically
    applied by APIM. This is what we call the *scaling special sauce* – that is, the
    exponential interval retry special sauce needed to scale using any combination
    of multiple AOAI accounts to meet the most demanding business/user requirements.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解和掌握的主要且极为重要的观点是，当 APIM 的间隔、最大间隔和 delta 参数被指定，就像前面示例中那样，那么 APIM 会自动应用一个指数间隔重试算法。这就是我们所说的“扩展特殊配方”——也就是说，使用任何组合的多个
    AOAI 账户来满足最苛刻的业务/用户需求的指数间隔重试特殊配方。
- en: 'For those interested in the mathematical logic behind this, here is the calculation
    that’s used by APIM for the exponential interval retry formula:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对背后的数学逻辑感兴趣的人，以下是 APIM 用于指数间隔重试公式的计算方法：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Without the scaling special sauce (APIM using retries with exponential backoff),
    once the initial rate limit is hit, say due to too many concurrent users sending
    too many prompts, then a 429 error return code (server busy) response code is
    sent back.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 没有扩展的特殊配方（APIM 使用指数退避的重试），一旦达到初始速率限制，比如说由于太多并发用户发送了太多的提示，那么就会返回一个 429 错误返回代码（服务器忙碌）。
- en: Furthermore, as additional subsequent prompts/completions are sent, the issue
    can be compounded quickly as more 429 errors are returned, and the error rates
    increase further and further. It is with the retries with exponential backoff
    that you are then able to scale many thousands of concurrent users with very low
    error responses, providing scalability to the AOAI service.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着更多后续提示/完成的发送，问题可能会迅速恶化，因为返回了更多的 429 错误，错误率进一步增加。正是通过指数退避的重试，你才能以非常低的错误响应率扩展数千个并发用户，为
    AOAI 服务提供可扩展性。
- en: In addition to using retries with exponential backoff, APIM also supports content-based
    routing. This is where the message routing endpoint is determined by the content
    of the message at runtime. You can leverage this to send AOAI prompts to multiple
    AOAI accounts, including both PTUs and TPMs, to meet further scaling requirements.
    For example, if your model API request states a specific version, say gpt-35-turbo-16k,
    you can route this request to your GPT 3.5 Turbo (16K) PTUs deployment. This is
    true whether you’re in the same region or a multi-region deployment.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用指数退避的重试之外，APIM 还支持基于内容的路由。这就是在运行时根据消息的内容来确定消息路由端点。你可以利用这一点向多个 AOAI 账户发送
    AOAI 提示，包括 PTUs 和 TPMs，以满足进一步的扩展需求。例如，如果你的模型 API 请求指定了特定的版本，比如 gpt-35-turbo-16k，你可以将此请求路由到你的
    GPT 3.5 Turbo (16K) PTUs 部署。无论是在同一区域还是多区域部署，这都是适用的。
- en: We could write an entire book on all the wonderful scaling features APIM provides,
    but for additional details on APIM, please check out the APIM link at the end
    of this chapter. Alternatively, you can refer to the great book *Enterprise API
    Management*, by Luis Weir, published by Packt Publishing
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以写一本书来介绍 APIM 提供的所有奇妙扩展功能，但关于 APIM 的更多详细信息，请查看本章末尾的 APIM 链接。或者，你可以参考由 Packt
    Publishing 出版的由 Luis Weir 编写的优秀书籍《企业 API 管理》。
- en: Rate Limiting Policy in Azure API Management
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure API Management 中的速率限制策略
- en: Rate limiting in Azure API Management is a policy that restricts the number
    of requests a user can make to an API within a certain timeframe, ensuring cost
    control, fair usage and protecting the API from overuse and abuse. Just as we
    have rate limits at the OpenAI API level discussed above with TPM and RPM, we
    can also set rate limiting policies in Azure API management too. This has several
    benefits as mentioned below –
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Azure API Management中的速率限制是一种策略，限制用户在特定时间段内对API发起请求的数量，确保成本控制、公平使用，并保护API免受过度使用和滥用。正如我们在上面讨论的TPM和RPM级别上的速率限制一样，我们也可以在Azure
    API管理中设置速率限制策略。这有几个好处，如下所述——
- en: '**Prevents Overuse**: Ensures no single user can monopolize API resources by
    making too many requests.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止过度使用**：确保没有单个用户可以通过发起过多请求而垄断API资源。'
- en: '**Manages Resources**: Helps in evenly distributing server resources to maintain
    service reliability.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理资源**：帮助均匀分配服务器资源，以维护服务可靠性。'
- en: '**Controls Costs**: Avoids unexpected spikes in usage that could lead to higher
    operational costs.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制成本**：避免使用量意外激增，可能导致更高的运营成本。'
- en: '**Enhances Security**: Acts as a defense layer against attacks, such as Denial
    of Service (DoS), by limiting request rates.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强安全性**：通过限制请求速率，作为防御层对抗攻击，例如拒绝服务（DoS）攻击。'
- en: '**Ensures Quality of Service**: Guarantees fair resource distribution among
    all users to maintain expected service levels.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保服务质量**：保证所有用户之间公平的资源分配，以维持预期的服务级别。'
- en: '**Promotes Operational Stability**: Contributes to the API’s stability and
    predictability by allowing for effective resource planning.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促进操作稳定性**：通过允许有效的资源规划，有助于API的稳定性和可预测性。'
- en: Now that we have a good grasp on fundamental components of scaling and strategies
    with our special scaling sauce on Azure API Management, let’s turn our attention
    to Monitoring and Logging capabilities that can help build telemetry on our Gen
    AI application that can help you measure critical metrics to determine the performance
    and availability of your application.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地掌握了Azure API Management中扩展的基本组件和策略，让我们将注意力转向监控和日志记录功能，这些功能可以帮助构建Gen
    AI应用程序的遥测，帮助你测量关键指标，以确定应用程序的性能和可用性。
- en: Monitoring, logging, and HTTP return codes
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控、日志记录和HTTP返回代码
- en: As we have learned in the previous sections, both limits and how we manage these
    limits using various scaling techniques can help us provide a robust, enterprise-class,
    highly scalable cloud GenAI service to many thousands of users/demanding enterprise
    applications.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中学到的，限制以及我们如何使用各种扩展技术来管理这些限制，可以帮助我们为成千上万的用户/要求严格的业务应用提供强大、企业级、高度可扩展的云通用人工智能（GenAI）服务。
- en: But as with any good enterprise-class service, it’s important to configure and
    deploy the basic telemetry data provided by monitoring and logging to ensure optimal
    performance and timely notifications in case of issues.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但与任何好的企业级服务一样，配置和部署监控和日志记录提供的基本遥测数据对于确保最佳性能和问题发生时的及时通知非常重要。
- en: Monitoring and logging
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控和日志记录
- en: One of the most critical operational categories that is required for any robust
    enterprise service or solution that’s designed to be enterprise-ready are monitoring/instrumentation/observability
    and logging of the solution.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何旨在企业就绪的强大企业服务或解决方案，最关键的运营类别之一是解决方案的监控/仪表化/可观察性和日志记录。
- en: These components are required for any enterprise-level service, and you may
    already be familiar with the concepts or have a lot of experience in these areas,
    so we will not cover this extensively, only how monitoring and logging pertain
    to running a GenAI/ChatGPT-based cloud service, as well as some best practices.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件对于任何企业级服务都是必需的，你可能已经熟悉这些概念，或者在这些领域有很多经验，所以我们不会对此进行深入探讨，只介绍监控和日志记录如何与运行基于GenAI/ChatGPT的云服务相关，以及一些最佳实践。
- en: Any enterprise monitoring solution can be used for health-checking applications
    and services, as well as setting up alerts to be notified if certain thresholds
    are reached or exceeded, such as protection against automated and high volume
    misuse or other anomalies related to unusual usage patterns. Two very well and
    broadly used services, Azure Monitoring and DataDog, both have operational modules
    for use with OpenAI/Azure OpenAI. These enterprise tools know which metrics are
    important to collect, display, and alert on for the success and optimal health
    of your cloud GenAI service.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 任何企业级监控解决方案都可以用于健康检查应用程序和服务，以及设置警报，以便在达到或超过某些阈值时收到通知，例如防止自动化和大量滥用或其他与异常使用模式相关的异常。两个非常广泛使用的服务，Azure
    监控和 DataDog，都提供了与 OpenAI/Azure OpenAI 一起使用的操作模块。这些企业工具知道哪些指标对于收集、显示和警报以实现云 GenAI
    服务的成功和最佳健康状态是重要的。
- en: 'Monitoring transactional events, such as **TokenTransaction**, **Latency**,
    or **TotalError** to name a few, can provide valuable insight into how your Cloud
    ChatGPT service is operating, or alert you if settings or conditions are not within
    your ideal parameters. The alerting and notification of these available metrics
    are highly configurable. You can find the complete list of metrics here: [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/monitoring#azure-openai-metrics](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/monitoring#azure-openai-metrics).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 监控事务性事件，例如 **TokenTransaction**、**Latency** 或 **TotalError** 等等，可以提供有关您的 Cloud
    ChatGPT 服务运行情况的宝贵见解，或者在设置或条件不在您理想参数范围内时发出警报。这些可用指标的警报和通知是高度可配置的。您可以在以下位置找到完整的指标列表：[https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/monitoring#azure-openai-metrics](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/monitoring#azure-openai-metrics)。
- en: For more information about OpenAI monitoring by Datadog, check out [https://www.datadoghq.com/solutions/openai/](https://www.datadoghq.com/solutions/openai/).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Datadog 对 OpenAI 的监控更多信息，请查看 [https://www.datadoghq.com/solutions/openai/](https://www.datadoghq.com/solutions/openai/)。
- en: On a related note, application logging is critical to the success of reviewing
    events either in real time or after they have occurred. All metrics described
    previously can be collected and stored, reported in real-time for historical analysis,
    and output to visualization tools such as Fabric (Power BI) using Log Analytics
    Workspace in Azure, for example.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的是，应用程序日志对于实时或事件发生后审查事件的成功至关重要。之前描述的所有指标都可以收集和存储，实时报告以进行历史分析，并使用 Azure 中的日志分析工作区等可视化工具输出，例如
    Fabric（Power BI）。
- en: Every cloud GenAI application will have different logging requirements defined
    by the business/organization. As such, Microsoft has created a monitoring and
    logging AOAI best practices guide, a link to which you can find at the end of
    this chapter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个云 GenAI 应用程序都将有不同的日志要求，由业务/组织定义。因此，微软创建了一份监控和日志 AOAI 最佳实践指南，您可以在本章末尾找到该指南的链接。
- en: HTTP return codes
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HTTP 返回代码
- en: HTTP return codes, sometimes generically called “error codes” and briefly mentioned
    in the previous section, provide a way to validate. This is a standard web pattern
    many web developers will easily recognize.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 返回代码，有时被称为“错误代码”，在上一节中简要提及，提供了一种验证方式。这是一个许多网络开发者会很容易识别的标准网络模式。
- en: Remember that when your application is sending prompts, it does so via HTTP
    API calls.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当您的应用程序发送提示时，它是通过 HTTP API 调用来实现的。
- en: As described in the *Retries with exponential backoff – the scaling special
    sauce* section, you can use retries with exponential backoff for any 429 errors
    based on the APIM retry policy document.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如在“指数退避重试 – 规模化特殊酱料”部分所述，您可以根据 APIM 重试策略文档对任何 429 错误使用指数退避重试。
- en: 'However, as a best practice, you should always configure error checking regarding
    the size of the prompt against the model this prompt is intended for first. For
    example, for GPT-4 (8k), this model supports a maximum request token limit of
    8,192 tokens for each prompt+completion. If your prompt has a 10K token size,
    then this will cause the entire prompt to fail due to the token size being too
    large. You can continue retrying but the result will be the same – any subsequent
    retries would fail as well as the token limit size has already been exceeded.
    As a best practice, ensure the size of the prompt does not exceed the maximum
    request token limit immediately, before sending the prompt across the wire to
    the AOAI service. Again, here are the token size limits for each model:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为最佳实践，您应该首先配置有关提示大小与该提示旨在使用的模型大小的错误检查。例如，对于GPT-4（8k），此模型支持每个提示+完成的最多8,192个请求令牌。如果您的提示大小为10K令牌，则这会导致整个提示因令牌大小过大而失败。您可以继续重试，但结果将相同——任何后续的重试都会失败，因为令牌限制大小已经超过。作为最佳实践，确保在将提示发送到AOAI服务之前，立即确保提示的大小不超过最大请求令牌限制。再次强调，以下是每个模型的令牌大小限制：
- en: '| **HTTP** **Response Code** | **Cause** | **Remediation** | **Notes** |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **HTTP** **响应代码** | **原因** | **补救措施** | **备注** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 200 | Processed the prompt. Completion without error. | N/A | Successful
    completion |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 处理了提示。无错误完成 | N/A | 成功完成 |'
- en: '| 429 (v0613 AOAI Models) | Server busy (rate limit reached for requests).
    | APIM – retries with exponential backoff | When the APIM’s interval, max interval,
    and delta are specified, an exponential interval retry algorithm is automatically
    applied |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 429 (v0613 AOAI Models) | 服务器忙碌（请求达到速率限制）。 | APIM – 带指数退避的重试 | 当指定APIM的间隔、最大间隔和增量时，将自动应用指数间隔重试算法
    |'
- en: '| 424 (v0301 AOAI Models) | Server busy (rate limit reached for requests) |
    APIM – retries with exponential backoff | Same as above |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 424 (v0301 AOAI Models) | 服务器忙碌（请求达到速率限制） | APIM – 带指数退避的重试 | 与上面相同 |'
- en: '| 408 | Request timeout | APIM retry with interval | There are many reasons
    why a timeout could occur, such as a network connection/transient error |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 408 | 请求超时 | APIM重试，带间隔 | 发生超时的原因有很多，例如网络连接/短暂错误 |'
- en: '| 50x | Internal server error due to transient backend service error. | APIM
    retry with an interval | Retry policy: [https://learn.microsoft.com/en-us/azure/api-management/retry-policy](https://learn.microsoft.com/en-us/azure/api-management/retry-policy)
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 50x | 由于短暂的后端服务错误导致的内部服务器错误。 | APIM重试，带间隔 | 重试策略：[https://learn.microsoft.com/en-us/azure/api-management/retry-policy](https://learn.microsoft.com/en-us/azure/api-management/retry-policy)
    |'
- en: '| 400 | Another issue with the prompt itself, such as the prompt size being
    too large for the model type | Use APIM logic or application logic to return a
    custom error immediately, without sending it to the model for further processing
    | After immediately evaluating the prompt, a response is sent back, so no further
    processing is needed |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 400 | 与提示本身相关的问题，例如提示大小对于模型类型过大 | 使用APIM逻辑或应用逻辑立即返回自定义错误，而无需将其发送到模型进行进一步处理
    | 立即评估提示后，发送回响应，因此不需要进一步处理 |'
- en: Figure 7.7 – HTTP return codes
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – HTTP返回代码
- en: The preceding table lists the most common HTTP return codes so that you can
    programmatically manage and handle each return code accordingly, based on the
    response. Together with monitoring and logging, your application and services
    can better handle most scaling aspects of generative AI service behaviors.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上表列出了最常见的HTTP返回代码，以便您可以根据响应程序化地管理和处理每个返回代码，从而更好地处理生成式AI服务行为的大多数扩展方面。
- en: Next, we will learn about some additional considerations you should account
    for in your generative AI scaling journey.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解在您的生成式AI扩展之旅中应考虑的一些额外因素。
- en: Costs, training and support
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本、培训和支援
- en: 'To round off this chapter on deploying ChatGPT in the cloud with architecture
    design and scaling strategies, three additional areas are associated with a scaled
    enterprise service: costs, training and support.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束本章关于在云中部署ChatGPT的架构设计和扩展策略，与扩展的企业服务相关联的三个额外领域是：成本、培训和支援。
- en: Costs
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: Throughout this chapter, we discussed many services for a robust, enterprise-ready
    cloud ChatGPT service. While we wanted to focus on technical aspects of architecture
    design and scaling strategies, the topic of costs will (and should) be discussed,
    a critical factor from an ROI perspective that executives invariably weigh. Recognizing
    its significance, this section is dedicated to understanding the various elements
    that influence costs, alongside discussing strategies for cost optimization across
    different architectural layers – namely, the Model, Data, Application, and Infrastructure
    Layers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了许多用于强大、企业级云ChatGPT服务的服务。虽然我们希望关注架构设计和扩展策略的技术方面，但成本问题（以及它应该被讨论）将（并且应该）被讨论，这是一个从投资回报率的角度来看至关重要的因素，高管们不可避免地会权衡。认识到其重要性，本节致力于理解影响成本的各种因素，同时讨论在不同架构层——即模型、数据、应用和基础设施层——上的成本优化策略。
- en: There are variations in costs and these costs also change over time for any
    service. That is the nature of any business, not only a technology-based solution
    such as ChatGPT. We won’t list exact pricing here as it will have already changed
    once this book has been published, if not sooner! Instead, we wanted to mention
    some of the categories to consider when pricing the solution. This varies by vendor,
    how large or small your enterprise solution is, and a dozen other factors.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 任何服务的成本都有变化，并且这些成本也会随时间而变化。这是任何商业的本质，不仅仅是像ChatGPT这样的基于技术的解决方案。我们不会在这里列出确切的价格，因为一旦这本书出版，价格可能已经改变，甚至更早！相反，我们想提到在定价解决方案时需要考虑的一些类别。这取决于供应商，您的企业解决方案的大小，以及许多其他因素。
- en: You must understand that there is not only the pricing of the GenAI/LLM models
    themselves to consider, each with its versions and types, but also how quickly
    you want those processed and also cost varies depending on the cost model – Pay-As-You-Go
    or PTU, as we described when we covered the TPMs and PTUs topic earlier this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须明白，需要考虑的不仅仅是GenAI/LLM模型的定价，每个模型都有其版本和类型，还需要考虑您希望这些处理有多快，以及成本根据成本模型的不同而变化——如我们之前在本章中讨论的TPMs和PTUs，即按量付费或按时间单位付费。
- en: Of course, there is the cost of any ancillary services to support your enterprise-ready
    GenAI deployment, and the costs of training and support, as described earlier
    in this section, as well as the cost of staff who design, deploy, manage, and
    operate the robust enterprise cloud solution.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有支持您的企业级GenAI部署的任何辅助服务的成本，以及之前在本节中描述的培训和支持成本，还有设计、部署、管理和运营强大企业云解决方案的员工成本。
- en: 'Below, we list cost considerations and some optimization best practices to
    help lower cost or reduce resources:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们列出成本考虑事项和一些优化最佳实践，以帮助降低成本或减少资源：
- en: Model and Data Layer
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型和数据层
- en: '**Model selection**: Choose a pre-trained model that closely aligns with your
    task requirements. This can reduce the need for extensive fine-tuning and data
    collection, saving time and resources. Use popular benchmarks discussed in [*Chapter
    3*](B21443_03.xhtml#_idTextAnchor052) to shortlist your models for a particular
    task. Consider small language models and open source models for low-impact, internal
    (non-client) facing applications and batch tasks to reduce costs where quality
    and performance is not of the highest importance.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型选择**：选择一个与您的任务需求紧密对齐的预训练模型。这可以减少对大量微调和数据收集的需求，节省时间和资源。使用在[*第三章*](B21443_03.xhtml#_idTextAnchor052)中讨论的流行基准来缩小特定任务的模型范围。对于低影响、内部（非客户）面向的应用程序和批量任务，考虑小型语言模型和开源模型以降低成本，在这些情况下，质量和性能不是最重要的。'
- en: '**Data efficiency**: Utilize data augmentation techniques to create more training
    data from your existing dataset. This can help you achieve better results with
    less data, reducing storage and processing costs. Textbook quality data can help
    you achieve more high performing models with less tokens. For example, Phi-2 a
    2.7B parameter model was created using textbook quality synthetic datasets. It
    outperforms models 25x its size on complex benchmarks.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据效率**：利用数据增强技术从现有的数据集中创建更多的训练数据。这可以帮助您用更少的数据实现更好的结果，从而降低存储和处理成本。教科书质量的数据可以帮助您用更少的标记创建更多高性能的模型。例如，使用教科书质量的合成数据集创建了2.7B参数的Phi-2模型。它在复杂的基准测试中优于其25倍大小的模型。'
- en: '**Early stopping**: Implement early stopping during training to prevent overfitting
    and reduce training time. This helps you find a good model without wasting resources
    on unnecessary iterations.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早期停止**: 在训练期间实施早期停止以防止过拟合并减少训练时间。这有助于您找到良好的模型，而无需在不必要的迭代上浪费资源。'
- en: '**Model optimization**: Prune or quantize your model to reduce its size and
    computational requirements. This can lead to faster training and inference, lowering
    cloud costs. Model quantization leads to reduced memory, faster computation, energy
    efficiency, network efficiency and hence leading to reduced costs.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型优化**: 通过剪枝或量化您的模型来减小其大小和计算需求。这可以导致更快的训练和推理，降低云成本。模型量化导致内存减少、计算更快、能源效率更高、网络效率更高，从而降低成本。'
- en: Application Layer
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用层
- en: '**API Parameters**: These are configurable settings or values used to customize
    the behavior of an API, allowing users to control aspects such as data processing,
    request format, and response content. Setting appropriate parameters ensures efficient
    utilization of resources and optimal interaction with the API.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API参数**: 这些是可以配置的设置或值，用于自定义API的行为，允许用户控制数据处理、请求格式和响应内容等方面。设置适当的参数确保资源的有效利用和与API的最优交互。'
- en: '**Token Size**: Always set the max_tokens parameter to control the token size
    per API call.'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记大小**: 总是设置max_tokens参数以控制每个API调用中的标记大小。'
- en: '**Batch requests**: Instead of individual requests consider sending batch requests
    to reduce the overall costs.'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量请求**: 考虑发送批量请求而不是单个请求，以降低整体成本。'
- en: '**Caching**: For applications where the same inputs might result in the same
    outputs frequently, implement caching mechanisms to save on compute costs by serving
    cached results instead of regenerating them.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存**: 对于可能频繁产生相同输出的应用程序，实现缓存机制可以通过提供缓存结果而不是重新生成它们来节省计算成本。'
- en: '**Prompt Guide**: Offer users guidance on crafting effective prompts with a
    sample prompt guide/collection. This approach ensures users can achieve their
    desired outcomes.with minimal iterations.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示指南**: 提供一个示例提示指南/集合，以指导用户制作有效的提示。这种方法确保用户可以在最少迭代次数内实现预期结果。'
- en: '**Context Window**: Despite the rise of context window lengths up to a million
    in LLMs, it’s crucial not to default to utilizing the full extent in every instance.
    Especially in RAG applications, strategically optimizing to use only a minimal
    number of tokens is key for cost efficiency.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文窗口**: 尽管在大型语言模型（LLM）中上下文窗口长度已上升至百万级别，但在每个实例中默认使用全部长度仍然至关重要。特别是在RAG应用中，战略性地优化以仅使用最小数量的标记对于成本效率至关重要。'
- en: Infrastructure Layer
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础设施层
- en: '**Cloud infrastructure**: Leverage cloud platforms that offer flexible pricing
    options and pay-as-you-go models. This allows you to scale your resources up or
    down based on your needs, avoiding unnecessary costs. Consider using managed services
    like Autoscaling and terminate compute instances when idle.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云基础设施**: 利用提供灵活定价选项和按需付费模式的云平台。这允许您根据需求调整资源的大小，避免不必要的成本。考虑使用自动扩展等托管服务，并在空闲时终止计算实例。'
- en: '**Spot VMs or Preemptimble VMs**: If not using PaaS services, then look for
    Spot or Low Priority VMs for model training or fine-tuning to benefit from lower
    pricing.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spot VM或可抢占VM**: 如果不使用PaaS服务，那么在模型训练或微调时寻找Spot或低优先级VM，以从较低的价格中受益。'
- en: '**Reserved Instances**: If you have predictable, steady-state workloads, purchasing
    reserved instances can offer significant savings over on-demand pricing in exchange
    for committing to a one-year or three-year term. This is beneficial for customer
    facing workloads where predictable performance is important. E.g Azure PTUs'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预留实例**: 如果您有可预测的、稳定的负载，购买预留实例可以在承诺一年或三年期限的情况下，与按需定价相比提供显著的节省。这对于需要可预测性能的客户面负载来说是有益的。例如，Azure
    PTUs。'
- en: '**Rate Limiting**: Rate limiting in Azure API Management is a policy to control
    the number of processed requests by a client within a specified time period, ensuring
    fair usage and preventing abuse of the API. This can help control costs too.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速率限制**: Azure API Management中的速率限制是一种策略，用于控制客户端在指定时间周期内处理的请求数量，确保公平使用并防止API滥用。这也有助于控制成本。'
- en: '**Monitoring and logging**: Continuously monitor your model’s performance and
    resource usage. This helps you identify areas for optimization and potential cost
    savings. You can build this telemetry using Azure API Management and Azure Cost
    Monitor.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和日志记录**：持续监控您模型的表现和资源使用情况。这有助于您识别优化区域和潜在的节省成本。您可以使用Azure API Management和Azure
    Cost Monitor构建这种遥测。'
- en: 'Note:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: We advise implementing a telemetry solution early to monitor your application’s
    token usage for prompts and completions. This allows for informed decisions between
    PTU and Pay-As-You-Go as your workload grows. Gradually scaling your solution
    to production through a ramp-up approach is recommended for cost-effective management.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议尽早实施遥测解决方案来监控您的应用程序的提示和完成时的令牌使用情况。这允许您在负载增长时在PTU和按量付费之间做出明智的决定。通过逐步扩大解决方案到生产环境，采用逐步增加的方法进行成本效益管理是推荐的。
- en: '**Data Transfer Costs/Egress Costs**: In a multi-cloud and/or a multi-region
    setup, monitoring egress usage and fees is crucial to managing total solution
    costs effectively. The traditional observability'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据传输成本/出口成本**：在多云和/或跨区域设置中，监控出口使用情况和费用对于有效管理总解决方案成本至关重要。传统的可观察性'
- en: '**Data Storage**: Store your training datasets or files generated from AI applications
    in lower cost object storage like Azure Blob, S3 or Google Cloud Storage when
    possible. Utilize compression techniques to reduce storage costs.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储**：在可能的情况下，将您的训练数据集或由AI应用程序生成的文件存储在成本更低的对象存储中，如Azure Blob、S3或Google Cloud
    Storage。利用压缩技术来降低存储成本。'
- en: Training
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: You have already started your journey of training for ChatGPT and OpenAI, especially
    if you have read this book thus far. There are many forms of learning and training
    that we already know about, but the key point here is that it is important to
    be knowledgeable or have staff/colleagues trained in not only the ChatGPT services
    themselves but other related services as well. We mentioned a few of these other
    services in the previous chapter, such as the APIM service, enterprise monitoring,
    instrumentation, logging, application and web development and management, and
    data science and analytics to name a few.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经开始了为ChatGPT和OpenAI进行训练的旅程，尤其是如果您已经阅读了这本书。我们已经了解了许多学习与训练的形式，但关键点是，了解ChatGPT服务本身以及其他相关服务是非常重要的。我们在上一章中提到了这些其他服务的一些，例如APIM服务、企业监控、仪表化、日志记录、应用和Web开发与管理，以及数据科学和分析等。
- en: Another aspect of training may include database management training, especially
    a NoSQL type of enterprise service such as Azure CosmosDB. Why? Typically, a large
    organization would want to save their prompt and completion history, for example,
    so that they can retrieve it later or search without having to resend the same
    prompts again. This does make for a highly efficient and optimized ChatGPT cloud
    service, with all the benefits a NoSQL database, such as CosmosDB, can provide
    – such as being highly performant, having lower costs, and being a globally scalable
    service. **Based on our experience, we have found that CosmosDB can be beneficial
    for Caching and Session Management of Conversational generative** **AI applications.**
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的另一个方面可能包括数据库管理培训，特别是像Azure CosmosDB这样的NoSQL类型的企业服务。为什么？通常，一个大型的组织会希望保存他们的提示和完成历史记录，例如，以便他们可以在以后检索它或进行搜索，而无需再次发送相同的提示。这确实为ChatGPT云服务提供了一个高度高效和优化的解决方案，以及NoSQL数据库（如CosmosDB）可以提供的所有好处——例如，高性能、低成本和全球可扩展的服务。**根据我们的经验，我们发现CosmosDB对于对话生成AI应用的缓存和会话管理是有益的**。
- en: Of course, no one person can run an enterprise solution, so you are not expected
    to know the intricate details and job tasks for every service – this is what an
    enterprise cloud team does… and does it as a team! However, identifying training
    requirements for the enterprise services you will run and identifying any gaps
    early in the service planning life cycle is highly recommended and a best practice.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，没有人能够独立运行一个企业解决方案，因此您不需要了解每个服务的复杂细节和任务——这正是企业云团队的工作……并且是以团队的形式完成的！然而，在服务规划生命周期的早期就确定您将运行的企业服务的培训需求，并识别任何差距，这是高度推荐的最佳实践。
- en: Support
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持
- en: Just like training is a critical part of designing and scaling a ChatGPT for
    cloud solutions, so is supporting this enterprise solution/service.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 就像训练是设计和扩展云解决方案中的ChatGPT的关键部分一样，支持这个企业解决方案/服务也同样重要。
- en: 'Many aspects of support need to be considered: internal technical support for
    the end users who may be using your enterprise-ready service and the internal
    support provided by various workload owners, including both primary and ancillary
    services, as described earlier.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑许多支持方面：为可能使用您的企业级服务的最终用户提供内部技术支持，以及各种工作负载所有者提供的内部支持，包括前面描述的主要和辅助服务。
- en: However, this is not only internal support, but also any external, third-party,
    and vendor cloud support you will need to consider. Both OpenAI and Azure provide
    many tiers of support, whether it is free-to-low-cost self-service forums, where
    communities support each other, or paid support by trained personnel who can quickly
    resolve an enterprise issue, and they have personnel trained in all aspects (components)
    of the service. These paid support services can have many tiers of support, depending
    on how quickly you want the solution to be resolved based on your internal SLAs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这不仅仅是内部支持，还包括您需要考虑的任何外部、第三方和供应商云支持。OpenAI和Azure都提供了许多支持层级，无论是免费至低成本的自助论坛，社区相互支持，还是由受过培训的人员提供的付费支持，他们可以快速解决企业问题，并且他们有接受过服务所有方面（组件）培训的人员。这些付费支持服务可以根据您基于内部SLA希望解决方案解决速度的快慢提供多个支持层级。
- en: When designing and scaling a ChatGPT for cloud solutions, ensure “support” is
    on your checklist of items for a successful, robust deployment. This category
    cannot be overlooked or skipped.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计和扩展云解决方案中的ChatGPT时，确保“支持”是您成功、稳健部署的项目清单中的项目。这个类别不能被忽视或跳过。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter on deploying GenAI in the cloud, we learned how to design and
    scale a robust, enterprise ready GenAI cloud solution. We covered what limits
    exist within each of the models and how to overcome these limits either by adding
    additional (Azure) OpenAI accounts and/or using an Azure APIM service.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在云中部署GenAI，如何设计和扩展一个强大、企业级就绪的GenAI云解决方案。我们涵盖了每个模型中存在的限制以及如何通过添加额外的（Azure）OpenAI账户和/或使用Azure
    APIM服务来克服这些限制。
- en: APIM, with its very important exponential interval retry setting, is yet another
    way to help organizations scale up to meet business and user requirements.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: APIM，凭借其非常重要的指数间隔重试设置，是帮助组织扩展以满足业务和用户需求的另一种方式。
- en: Reserved capacity, known as PTUs in Microsoft Azure, is another way an enterprise
    can scale up to meet business requirements. We described how additional PTUs can
    be added and scaled by increasing the number of PTUs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 预留容量，在Microsoft Azure中称为PTUs，是企业扩展以满足业务需求的另一种方式。我们描述了如何通过增加PTUs的数量来添加和扩展额外的PTUs。
- en: During our cloud scaling journey, we learned how to scale across multiple geographies,
    or multi-regions, to support broader scale globally, while also supporting our
    enterprise DR scenarios.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的云扩展之旅中，我们学会了如何在多个地理区域或多个区域进行扩展，以支持全球更广泛的规模，同时支持我们的企业灾难恢复场景。
- en: We now understand how to handle various response and error codes when making
    API calls against our generative AI models, and we also know about best practices
    such as always configuring error checking the size of the prompt against the model
    this prompt is intended for first for a more optimized experience.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在了解如何在调用我们的生成式AI模型时处理各种响应和错误代码，我们还了解了一些最佳实践，例如首先配置错误检查提示的大小，以针对该提示旨在使用的模型，以获得更优化的体验。
- en: Then, you learned about the scaling special sauce, an insightful technique that
    ensures both a large-scale and seamless experience by using a retry pattern known
    as retries with exponential backoff. With this technique, scaling at extremely
    large user and prompt counts can be achieved.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你了解了扩展的秘诀，这是一种洞察力强的技术，通过使用称为指数退避的重试模式，确保了大规模和无缝的体验。使用这种技术，可以实现极大规模的用户和提示计数下的扩展。
- en: As we wrapped up, we described how monitoring/instrumentation/observability
    plays a critical part in the overall solution by providing alerting notifications
    and deeper insights into the operational side of the service. Logging further
    supports the operational requirements for the enterprise, such as using logs for
    real-time analytics or historical data, so that it can be presented in reports.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束时，我们描述了监控/仪表化/可观察性如何通过提供警报通知和对服务操作方面的更深入了解，在整体解决方案中发挥关键作用。日志进一步支持企业的运营需求，例如使用日志进行实时分析或历史数据，以便可以在报告中展示。
- en: Finally, we covered categories that will require further investigation as you
    design a scalable and robust enterprise ChatGPT cloud solution – training, support,
    and costs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了在设计可扩展和稳健的企业ChatGPT云解决方案时需要进一步调查的类别——培训、支持和成本。
- en: 'In the next chapter, we will learn about another important aspect for enterprises
    that want to scale and deploy ChatGPT in the cloud: security. We will look at
    some of the critical security considerations or concerns for deploying ChatGPT
    for cloud solutions, as well as how to best address them for a continued robust,
    enterprise-ready cloud solution.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习对于想要在云端扩展和部署ChatGPT的企业来说的一个重要方面：安全性。我们将探讨部署ChatGPT用于云解决方案的一些关键安全考虑或担忧，以及如何最好地解决这些问题，以保持持续稳健、适用于企业的云解决方案。
- en: References
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Manage Azure OpenAI Service Quota: [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/quota?tabs=rest](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/quota?tabs=rest)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理Azure OpenAI服务配额：[https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/quota?tabs=rest](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/quota?tabs=rest)
- en: 'OpenAI rate limits in headers: [https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers](https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI头部中的速率限制：[https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers](https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers)
- en: 'What is Azure API Management?: [https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts](https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Azure API Management？：[https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts](https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts)
- en: 'Azure API Management retry policy: [https://learn.microsoft.com/en-us/azure/api-management/retry-policy](https://learn.microsoft.com/en-us/azure/api-management/retry-policy)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure API Management重试策略：[https://learn.microsoft.com/en-us/azure/api-management/retry-policy](https://learn.microsoft.com/en-us/azure/api-management/retry-policy)
- en: 'How to deploy an Azure API Management service instance to multiple Azure regions:
    [https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将Azure API Management服务实例部署到多个Azure区域：[https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region)
- en: 'Token size limits for each model in Azure OpenAI: [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI中每个模型的令牌大小限制：[https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models)
- en: 'Provisioned Throughput Units (PTU) getting started guide: [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-get-started](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-get-started)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置吞吐量单位（PTU）入门指南：[https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-get-started](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-get-started)
- en: 'Azure OpenAI monitoring and logging best practices guide: [https://techcommunity.microsoft.com/t5/fasttrack-for-azure/optimizing-azure-openai-a-guide-to-limits-quotas-and-best/ba-p/4076268](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/optimizing-azure-openai-a-guide-to-limits-quotas-and-best/ba-p/4076268)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI监控和日志最佳实践指南：[https://techcommunity.microsoft.com/t5/fasttrack-for-azure/optimizing-azure-openai-a-guide-to-limits-quotas-and-best/ba-p/4076268](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/optimizing-azure-openai-a-guide-to-limits-quotas-and-best/ba-p/4076268)
- en: 'Azure OpenAI pricing: [https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI定价：[https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)
- en: 'Part 4: Building Safe and Secure AI – Security and Ethical Considerations'
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4部分：构建安全可靠的AI——安全和伦理考虑
- en: This part will cover everything you need to know about creating AI applications
    that are not only safe and secure but also built with a responsible AI-first mindset.
    We’ll look into the security risks associated with generative AI, including the
    dangers of deepfakes, and discuss strategies to counter these issues, such as
    Red Teaming. We’ll introduce the principles of Responsible AI, highlight the emerging
    start-up ecosystem in this field, and examine the current global regulations surrounding
    AI. Additionally, we’ll explore how organizations can best prepare for these regulatory
    environments.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将涵盖您需要了解的关于创建既安全又可靠，并且以负责任的人工智能优先思维构建的AI应用的所有内容。我们将探讨与生成式AI相关的安全风险，包括深度伪造的危险，并讨论应对这些问题的策略，例如红队战术。我们将介绍负责任AI的原则，突出该领域的新兴初创生态系统，并审视围绕AI的当前全球法规。此外，我们还将探讨组织如何为这些监管环境做好最佳准备。
- en: 'This part contains the following chapters:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 8*](B21443_08.xhtml#_idTextAnchor163), *Security and Privacy Considerations
    for Generative AI: Building Safe and Secure LLMs*'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B21443_08.xhtml#_idTextAnchor163)，*生成式AI的安全和隐私考虑：构建安全可靠的LLMs*'
- en: '[*Chapter 9*](B21443_09.xhtml#_idTextAnchor184), *Responsible Development of
    AI Solutions: Building with Integrity and Care*'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B21443_09.xhtml#_idTextAnchor184)，*负责任地开发AI解决方案：以诚信和关怀构建*'
