- en: Agent Awareness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*You can run, but you can''t hide!*'
  prefs: []
  type: TYPE_NORMAL
- en: Oh, you're back? That's great, because it means that your eyes are capturing
    some light information that your brain is perceiving in an act that it is usually
    called reading. Everything we do, every decision we make, is based on what we
    perceive, and biologically we make short decision-making processes because time
    is crucial (e.g. you see a snake and your amygdala processes that information
    much faster and quicker than your visual cortex!).
  prefs: []
  type: TYPE_NORMAL
- en: With the same very concept, AI needs to base their decisions from facts by gathering
    information they need to perceive first. This chapter is all about perception,
    and how AI can get this information from the environment, so that it can be aware
    of its surroundings. We looked at EQS in the previous chapter, which gathers a
    lot of information about the surrounding environment and processes. Here, we will
    just limit ourselves to the simple act of perception. What the AI will do with
    that information is a topic for other chapters (and some we have already covered).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick rundown of the topics we will face in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Perception and awareness in existing video games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the Sensing System within Unreal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perception Component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Senses for Sight and Hear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perception Stimuli
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing sight in an agent (both in Blueprint and in C++)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing hearing in an agent (both in Blueprint and C++)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's start by looking at some examples of AI awareness in video games,
    and then we will see how we can set up a Perceiving system in Unreal.
  prefs: []
  type: TYPE_NORMAL
- en: Perception of AI in video games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's all a matter of perception, right? But when it comes to artificial intelligence—and
    AI in games, in particular—perception can make all the difference between winning
    and losing. In other words, *how* an AI character is able to perceive a player
    during gameplay can create a range of different experiences, thus creating environments
    full of tension and suspense while you turn every corner with slow, tentative
    footsteps.
  prefs: []
  type: TYPE_NORMAL
- en: Sound
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever tried to sneak past a guard in a game, trying not to make a noise
    or get detected? This is one of the most common ways that AI perceive a player
    and respond accordingly (often not in your favor!). However, one of the benefits
    of using sounds to influence an AI's perception of a player is that it gives the
    player the opportunity to initiate surprise attacks (e.g. Hitman, Assassin's Creed).
    For example, a player can sneak up on an enemy and stun or attack them from behind,
    therefore providing the player with an advantage. This can be particularly helpful
    when enemies are challenging to defeat, or a player is low in resources (e.g.
    ammunition, health packs/potions, etc).
  prefs: []
  type: TYPE_NORMAL
- en: Footsteps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like the preceding example suggests, one of the most common ways that
    AI can perceive characters via sound is through footsteps. No surprises here about
    how, but the proximity of detection here can depend on many factors. For example,
    some characters can walk while crouching to avoid detection, or simply by sneaking
    (e.g. *Abe''s Oddyssey*); other games allow some characters to be undetectable
    while moving around, unless visually spotted by an enemy (e.g. Natalia in *Resident
    Evil: Revelations 2*). Another key ingredient in using footsteps as a trigger
    for an AI''s perception is the type of ground material that a player is walking
    on. For example, a player walking through a forest, crunching on leaves and bark,
    is going to be a lot more obvious (and loud) than a player who is walking on sand.'
  prefs: []
  type: TYPE_NORMAL
- en: Knocking over objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While sneaking through a level or walking while crouching, or even in the prone
    position (e.g. *Battlefield*), will not trigger an enemy, chances are if you knock
    something over (e.g. a bottle, box, random item) it's going to alert them. In
    this case, environmental objects play an important role in an AI's ability to
    perceive a player's location by simply the player themselves fumbling around an
    environment. In some cases, certain objects are likely to attract more attention
    than others, depending on how much noise they make. Of course, as game designers,
    you have the power to determine this!
  prefs: []
  type: TYPE_NORMAL
- en: Position
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to sound, AI have the ability to see you based on your proximity to
    them. This one is a little more obvious and a bit harder to avoid when you are
    in plain sight of an enemy. Imagine that you're sneaking past an enemy and as
    soon as you come close enough to them, that's it, it's all over, you've been spotted!
    This is the unfortunate peril that many players face, but one that has many pay-offs,
    especially in terms of satisfaction to having outsmarted the enemy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine this concept a little further with some examples. To begin,
    we have games like *Assassin''s Creed*, *Hitman: Absolution*, and *Thief*, where
    the art of eluding your enemy through manoeuvring is paramount to the player''s
    success in completing a mission. Often, this requires that the play leverages
    the environmental surroundings such as NPCs, walls, haystacks, plants (trees,
    bushes), rooftops, and utilizing the element of surprise.'
  prefs: []
  type: TYPE_NORMAL
- en: Zone of proximity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In other cases, there is an explicit zone of proximity that a player can remain
    out of before they are detected. Often, in games, this is articulated by a light
    source such as a flashlight, forcing the player to dance between shadow and light
    to avoid detection. An excellent example of games that have adopted this approach
    are *Monaco: What''s Yours Is Mine* and *Metal Gear Solid*, where certain AI characters
    have a proximity of visibility via the use of torches or by simply facing you
    for an extended period of time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see an example of this in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3352ac11-2ea1-4ee9-9d47-71ffbc4ba88d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Screenshot from the game *Monaco: What''s Yours Is Mine*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here (in *Monaco: What''s Yours Is Mine*), you can see the radius of the flashlights,
    and as soon as a player enters it, they have a limited amount of time before they
    grab the attention of the guards.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since *Monaco: What''s Yours Is Mine* is entirely based on this mechanic, let''s
    look at some more screenshots to get a better feeling of how sight perception
    works in this game.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see how the perception changes when the
    player changes room:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42ef940a-7398-4ab6-981a-06388b9875ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Screenshot from the game *Monaco: What''s Yours Is Mine*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we have a close-up of the player''s perception:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58bf00d1-89d8-48fa-befb-e153919fb408.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Screenshot from the game *Monaco: What''s Yours Is Mine*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we have a close-up of a guard''s flashlight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6d333a7-f573-4b7d-965a-8f3b6ffa6ad2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Screenshot from the game *Monaco: What''s Yours Is Mine*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Changing game, In *Metal Gear Solid*, perception is used in a similar way with
    enemies (red dots) patrolling the environment around the player (white dot). In
    the following screenshot, you can see a camera (represented as a red dot in the
    *minimap*) with a yellow cone of view in the *minimap* (guards have a blue cone,
    instead):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89aa9ea2-407f-46cb-8e95-6444a386fbb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from the game *Metal Gear Solid*
  prefs: []
  type: TYPE_NORMAL
- en: The Metal Gear Solid game series is entirely based on perception, and it is
    worthwhile exploring more and learning about the game if you are interested in
    developing game AIs with this mechanic.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up, if you get too close to NPCs (e.g. within their range of visibility)
    you will be noticed, and they will try to interact with your character, whether
    it be good (beggars in *Assassin's Creed*) or bad (enemy attacks you), which unlocks
    many interesting mechanics based on perception.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with other enemies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An AI's perception about your position isn't necessarily related to when you
    enter their zone of visibility. In other cases (e.g. first-person shooters), this
    may happen when you start to shoot an enemy. This creates a ripple effect in that
    many AIs within your initial proximity will then target you (e.g. Metal Gear Solid,
    Army of Two, Battlefield, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: It isn't all about the "enemy"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many sporting games, AI has to be perceptive in order to respond accordingly,
    e.g. from preventing a goal, hitting a ball, or shooting hoops. AI within sports
    must be perceptive (and competitive) when it comes to playing *against* you. They
    need to know your location and the location of the ball (or any other object)
    so that they can respond (e.g. kicking the ball away from the goal posts).
  prefs: []
  type: TYPE_NORMAL
- en: Perceptive AI isn't just humanoid or animalistic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perceptive AI can also include machines, such as cars and other vehicles. Take
    the games *Grand Theft Auto*, *Driver*, and *The Getaway*, into account, these
    games require that a player navigates around a 3D world space at some point inside
    of a car. In some instances, there are NPCs inside, but for the most part, the
    cars themselves respond to your driving. This is also the case in more sport oriented
    games such as *Grand Turismo*, *Need for Speed*, and *Ridge Racer* (to name a
    few).
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Players
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, there are many ways in which AI can detect players. But one
    thing that a game designer must consider among all of this is how this will influence
    the game's experience; how will it drive gameplay? While the use of perceptive
    AI is quite a nice addition to any game, it also impacts how a game is played.
    For example, if you want to have gameplay that is heavily focused on skill, player
    dexterity, and more environmentally aware, then the perception of AI needs to
    be quite sensitive, with the player being a lot more vulnerable (e.g. Thief).
    But if, on the other hand, you want a fast-paced action game, you will need to
    have perceptive AI with the balance of allowing the player to respond accordingly.
    For example, they have a level playing field to fight against the AI.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Sensing System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coming back to Unreal, as you would expect, there is a subsystem of the AI Framework
    that implements AI Perception. Once again, you are free to implement your own
    system, especially if you have particular needs…
  prefs: []
  type: TYPE_NORMAL
- en: With ***Sensing and Perception***, we are collocating at a lower level than
    ***Decision-Making*** (like *Behavior Trees* and *EQS*). In fact, there is no
    decision to take, no place to select, but just a passage/flow of information.
  prefs: []
  type: TYPE_NORMAL
- en: If the Sensing System perceives something "interesting" (we will define what
    this means later), then it notifies the AI controller, which will decide what
    to do about the received stimuli (which is its perception, in Unreal terminology).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this chapter, we will focus on how to properly set up the Sensing
    System so that our AI can perceive, but we won't deal with what to do once we
    have received the stimuli (e.g. the player is in sight, so start chasing them).
    After all, if you already have the behavior ready (e.g. a *Behavior Tree* that
    chases the player; we will build such a tree later in this book), the logic behind
    the sensing is simple as "if the player is in sight (the AI controller received
    a stimuli from the Sensing system), then execute the Chasing Behavior tree".
  prefs: []
  type: TYPE_NORMAL
- en: 'In practical terms, the built-in sensing system of Unreal is based mainly on
    the use of two components: **AIPerceptionComponent** and **AIPerceptionStimuliSourceComponent**.
    The first is able to perceive stimuli, whereas the latter is able to produce one
    (but it is not the only way we can produce stimuli, as we will soon see).'
  prefs: []
  type: TYPE_NORMAL
- en: As odd as it might seem, the system believes that the AIPerceptionComponent
    is attached to the AI Controller (and not the Pawn/Character that they control).
    In fact, it's the AI Controller that will make a decision based on the stimuli
    received, not the mere Pawn. As a result, the AIPerceptionComponent needs to be
    attached directly to the AI Controller.
  prefs: []
  type: TYPE_NORMAL
- en: The AIPerceptionComponent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's break down how the **AIPerceptionComponent** works. We are going to do
    this both in Blueprint and C++.
  prefs: []
  type: TYPE_NORMAL
- en: AIPerceptionComponent in Blueprint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we open our Blueprint AI Controller, we are able to add the **AIPerceptionComponent**
    like we would any other component: from the Components tab, click on **Add Component**
    and select the **AIPerceptionComponent**, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2367df79-29df-45cf-a653-8361ab6d60d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When you select the component, you will see how it appears in the *Details*
    panel, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11de9f01-0203-4253-be9c-f3aaee0d4d45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It only has two parameters. One defines the dominant senses. In fact, **AIPerceptionComponent**
    can have more than one sense, and when it comes to retrieving the location of
    the target that''s been sensed, which one should the AI use? The **Dominant Sense**
    removes ambiguity by giving one sense priority over the others. The other parameter
    is an Array of senses. As you fill the Array with the different senses, you will
    be able to customize each one of them, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71c2ecc9-71b8-4232-b45b-2f316f5bdb20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Keep in mind that you can have more than one sense of each kind. Suppose that
    your enemy has two heads, facing a different direction: you might want to have
    two sight senses, one for each head. Of course, in this case, it requires a bit
    more setup to make them work correctly since you need to modify how the sight
    component works since, let''s say, the AI always watches from its forward vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each sense has its own properties and parameters. Let''s go through the two
    main ones: sight and hearing.'
  prefs: []
  type: TYPE_NORMAL
- en: Sense – Sight
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Sight sense works as you would expect, and it comes pretty ready out of
    the box (this might not be true for other senses, but sight and hearing are the
    most common). This is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c3fee81-8fd0-45b1-8358-ac6b1cf19ea2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s break down the main parameters that control sense of Sight:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sight Radius**: If a target (an object that can be seen) enters within this
    range, and it is not occluded, then the target is detected. In this sense, it
    is the "*Maximus sight distance to notice the target*".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lose Sight Radius**: If the target has already been seen, then the target
    will be still seen within this range, if not occluded. This value is greater than
    *Sight Radius*, meaning that the AI is able to perceive the target at a greater
    distance if it is already seen. In this sense, it is the "*Maximus sight distance
    to notice a target that has already been seen*".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PeripheralVisionHalfAngleDegrees**: As the name suggests, it specifies how
    far (in degrees) the AI can look. A value of 90 means (since this value is just
    half of the angle) that the AI is able to see everything that is in front of it
    up to 180 degrees. A value of 180 would mean that the AI can look in any direction;
    it has 360-degree vision. Also, it is important to note that this half angle is
    measured from the forward vector. The following diagram illustrates this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eedbf74b-20da-4d5c-9437-70784e032a1f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Auto Success Range From Last Seen**: By default, this is set to an invalid
    value (-1.0f), meaning that it isn''t used. This specifies a range from the last
    seen location of a target, and if it is within this range, then the target is
    always visible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other settings that are more general, and can be applied to many
    senses (including hearing, so they will not be repeated in the next section):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Detection By Affiliation**: *See the Different Teams* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Debug Color**: As the name suggests, it is the color with which this sense
    should be displayed in the visual debugger (see [Chapter 11](de51b2fe-fb19-4347-8de9-a31b2c2a6f2f.xhtml),
    *Debugging methods for AI – Logging*, for more information).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max Age**: It indicates the time (expressed in seconds) that a stimulus is
    recorded for. Imagine that a target exits from the vision of the AI; its last
    location is still recorded, and it is assigned an age (how old is that data).
    If the Age gets bigger than the Max Age, then the stimuli are erased. For instance,
    an AI is chasing the player, who escapes from his/her sight. Now, the AI should
    first check the last position where the player has been seen to try to bring him/her
    back into their sight. If it fails, or the position was recorded many minutes
    ago, that data is not relevant anymore, and it can be erased. In summary, this
    specifies the age limit after the stimuli that''s generated by this sense is forgotten.
    Moreover, a value of 0 means never.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sense – Hearing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Hearing sense has just one proper parameter, which is the Hearing Range.
    This sets the distance at which the AI is able to hear. The others are the general
    ones we already have seen for Sight (e.g. *Max Age*, *Debug Color*, and *Detection
    By Affiliation*). This is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c6deb86-c037-44d0-9ca6-308076ac2239.png)'
  prefs: []
  type: TYPE_IMG
- en: To make this book complete, it's worth mentioning that there is another option,
    called *LoSHearing*. To the best of my knowledge, and by looking at the Unreal
    Source code (version 4.20), this parameter doesn't seem to affect anything (except
    debugging). As a result, we leave it as not enabled.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, there are other options to control how sound is produced. Actually,
    the Hearing events need to be manually triggered with a special function/Blueprint
    node.
  prefs: []
  type: TYPE_NORMAL
- en: AIPerceptionComponent and Senses in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you skipped the previous sections, please read them first. In fact, all the
    concepts are the same, and in this section, I'm just going to show the use of
    the component in C++ without re-explaining all the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the `#include` statements for the classes we would like to use (I
    only included Sight and Hearing):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To add the component to a C++ AI Controller, you do so like any other component,
    with a variable in the `.h` file. So, to keep track of it, you can use the `inerith`
    variable from the base class, which is declared as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As a result, you are able to use this variable in any *AIController*, without
    declaring it in the header (`.h`) file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, with the `CreateDefaultSubobject()` function in the constructor in the
    `.cpp` file, we can create the component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, you will need extra variables, one for each of the senses you want
    to configure. For example, for the Sight and Hearing senses, you will need the
    following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To configure a Sense, you need to create it first, and you have access to all
    its properties and can set what you need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you need to bind the senses to the **AIPerceptionComponent**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In case you need to call back for the events, you can do so by bypassing the
    callback function (it has to have the same signature, not necessarily the same
    name):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the use of the **AIPerceptionComponent** and **Senses** in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Different Teams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the AI perception system that's built into Unreal goes, AIs and anything
    that can be detected can have a team. Some teams are against each other, whereas
    some are just neutral. As a result, when an AI comes to perceive something, that
    something can be Friendly (it is in the same team), Neutral, or an Enemy. For
    instance, if an AI is patrolling a camp, we can ignore Friendly and Neutral entities,
    and focus only on Enemies. By the way, the default settings are to perceive only
    enemies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way in which you can change which kind of entities an AI can perceive is
    through the *Detecting for Affiliation* settings of *Sense*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c50f213-fa1a-4343-80e6-aabd93154d5c.png)'
  prefs: []
  type: TYPE_IMG
- en: This provides three checkboxes where we can choose what we would like that AI
    to perceive.
  prefs: []
  type: TYPE_NORMAL
- en: There are 255 teams in total, and by default, every entity is within team 255
    (the only special team). Whoever is in team 255 is perceived as Neutral (even
    if both the entities are in the same team). Otherwise, if two entities are in
    the same team (different than 255), they "see" each other as Friendly. On the
    other hand, two entities in two different teams (different than 255) the "see"
    each other as Enemies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the question is, how can we change teams? Well, at the moment, this is
    only possible in C++. Moreover, we have talked about entities, but who can actually
    be in a team? Everything that implements the **IGenericTeamAgentInterface** can
    be part of a team. *AIControllers* already implements it. As a result, changing
    teams on an AI Controller is easy, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For other entities, once they implement the **IGenericTeamAgentInterface**,
    they can override the `GetGenericTeamId()` function, which provides a way for
    the AI to check in which team that entity is.
  prefs: []
  type: TYPE_NORMAL
- en: The AIStimuliSourceComponent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how an AI can perceive through a Sense, but how are the stimuli
    generated in the first place?
  prefs: []
  type: TYPE_NORMAL
- en: All Pawns are automatically detected
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of the Sight sense, by default, all the Pawns are already a stimuli
    source. In fact, later in this chapter, we will use the Player character, who
    will be detected by the AI without having an **AIStimuliSourceComponent**. In
    case you are interested in disabling this default behavior, you can do so by going
    into your project directory, and then going inside the **Config** folder. There,
    you will find a file named **DefaultGame.ini**, in which you can set a series
    of configuration variables. If you add the following two lines at the end of the
    file, Pawns will not produce Sight stimuli by default, and they will need the
    **AIStimuliSourceComponent** as well as everything else:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In our project, we are not going to add these lines, since we want the Pawns
    to be detected without using having to add more components.
  prefs: []
  type: TYPE_NORMAL
- en: AIStimuliSourceComponent in Blueprint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like any other component, it can be added to a Blueprint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57288860-895c-44b1-8f27-48c4ea2fea33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you select it, you will see in the *Details* panel that it has just two
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Auto Register as a Source**: As the name suggests, if checked, the source
    automatically registers inside the Perception system, and it will start proving
    stimuli from the start'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Register as Source for Senses**: This is an array of all the senses that
    this component provides stimuli for'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is not much more to say about this component. It is very simple to use,
    but important (your AI might not perceive any stimuli!). Thus, remember to add
    it to the non-Pawn entities when you want them to generate a stimulus (which can
    be as simple as being seen from the AI).
  prefs: []
  type: TYPE_NORMAL
- en: AIStimuliSourceComponent in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using this component in C++ is easy since you just create it, configure it,
    and it is ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the `#include` statement you need to use so that you have access to
    the class of the component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Like any other component, you need to add a variable in the `.h` file to keep
    track of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you need to generate it in the constructor with the `CreateDefaultSubobject()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you need to register a source Sense, as follows (in this case, *Sight*,
    but you can change `TSubClassOf<UAISense>()` to the Sense you need):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The **Auto Register as a Source** bool is protected and true by default.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on with the perception system – Sight AI Controller
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way we to learn about something is by using it. So, let's start by
    creating a simple perception system in which we print on the screen when something
    enters or leave the perception field of the AIs, along with the number of currently
    seen objects (including/excluding the one that just entered/exited).
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we will do this twice, once with Blueprint and another time with
    C++, so that we can get to know about both methods of creation.
  prefs: []
  type: TYPE_NORMAL
- en: A Blueprint perception system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, we need to create a new AI controller (unless you want to use
    the one we've already been using). In this example, I'm going to call it "SightAIController".
    Open up the Blueprint editor, add the AIPerception component, and feel free to
    rename it (if you like) to something like "SightPerceptionComponent".
  prefs: []
  type: TYPE_NORMAL
- en: 'Select this component. In the *Details* panel, we need to add this as a sense
    to *Sight*, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e782229-d2b9-4538-a23d-b97a187574e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can set the **Sight Radius** and the **Lose Sight Radius** to something
    reasonable, such as *600* and *700*, respectively, so that we have something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d351646-69df-40d5-8ca4-8a1b2d15f792.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can leave the angle untouched, but we need to change the **Detection By
    Affiliation**. In fact, it isn''t possible to change the Team from Blueprint,
    so the player will be in the same 255th team, which is neutral. Since we are just
    getting our hands dirty on how the system works, we can check all three checkboxes.
    Now, we should have something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bac0a4d6-a13b-40a3-a34c-b278ada0d7f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At the bottom of the component, we should have all the different events. In
    particular, we will need **On Target Perception** **Updated**, which is called
    every time a target enters or exits the perception field—exactly what we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48f8450c-3c7e-4028-87d7-4f57ba8edb47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the "**+**" sign to add the event in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b352ecc-ed31-4a25-99dc-c36c5365aa0e.png)'
  prefs: []
  type: TYPE_IMG
- en: This event will provide us with the Actor that caused the update and created
    the stimuli (it's worth remembering that a Perception component might have more
    than one perception at the time, and this variable tells you which stimuli caused
    the update). In our case, we have only Sight, so it can't be anything else. The
    next step is to understand how many targets we have insight and which one left
    or entered the field of view.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, drag the **SightPerceptionComponent** into the graph. From there, we can
    drag a pin to get all the "**Currently Perceived Actors**", which will give us
    back an array of Actors. Don''t forget to set the *Sense Class* to *Sight*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b2cf273-5140-456a-9672-7f11dcc3925d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By measuring the length of this array, we can get the number of currently perceived
    actors at the moment. Moreover, by checking whether the Actor that was passed
    from the event is in the currently "*seen Actors*" array, we can determine whether
    such an actor has left or entered the field of view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42ca93c2-6540-44c1-a5e9-9d44809b2b64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The last step is to format all of this information in a nice formatted string
    so that it can be shown on-screen. We will use the Append node to build the string,
    along with a select for the "*entered*" or "*left*" Actor. Finally, we will plug
    the end result into a *Print String*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/545cd541-2591-466c-9780-c691b041834e.png)'
  prefs: []
  type: TYPE_IMG
- en: The *Print String* is just for debugging purposes and it is not available when
    shipping games, but we are just testing and understanding how the perception system
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Also, I know that when the number of perceived actors is one, the string will
    produce "*1 objects*", which is incorrect, but correcting plurals (although possible,
    both with an if statement or in a more complex fashion to take care of language(s)
    structure(s)) is outside the scope of this book. This is why I am using this expression.
  prefs: []
  type: TYPE_NORMAL
- en: Save the AI controller and go back to the level. If you don't want to do the
    same in C++, skip the next section, and go directly to "*Test it all"*.
  prefs: []
  type: TYPE_NORMAL
- en: A C++ perception system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, if you are more on the C++ side, or want to experiment with how we can
    build the same AI Controller in C++, this is the section for you. We will follow
    the exact same steps (more or less), and instead of images, we will have code!
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by creating a new AIController class (if you don't remember how
    to, have a look at [Chapter 2](42076317-364e-4824-a8b7-cc2c418f021e.xhtml), *Moving
    the first steps in the AI world*). We will name it SightAIController and place
    it within the `AIControllers` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start editing the `SightAIController.h` file, in which we need to include
    some other `.h` files so that our compiler knows where the implementations of
    the class we need are. In fact, we will need access to the **AIPerception** and
    **AISense_Config** classes. So, at the top of your code file, you should have
    the following `#include` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in our class, we need to keep a reference to the `AIPerception` Component
    and an extra variable that will hold the configuration for the Sight sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, we need to add the `Constructor` function, as well as a callback
    for the `OnTargetPerceptionUpdate` event. In order to work, this last one has
    to be a `UFUNCTION()`, and needs to have an **Actor** and a **AIStimulus** as
    inputs. In this way, the reflection system will work as excepted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s move into our `.cpp` file. First, we need to create the `AIPerception`
    Component, as well as a Sight configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can configure the *Sight Sense* with the same parameters: **Sight
    Radius** to *600* and **Lose Sight Radius** to *700*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to check all the flags for the **DetectionByAffiliation** so
    that we detect our Player (since, at the moment, they both are in the 255th team;
    look at the *Exercise* section to learn how to improve this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we associate the Sight configuration with the `AIPerception` Component,
    and bind the `OnTargetPerceptionUpdate` function to the homonym event on the `AIPerceptionComponent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the *Constructor*, but we still need to implement the `OnTargetPerceptionUpdate()`
    function. First of all, we need to retrieve all the **Currently Perceived Actors**.
    This function requires an array of actors that it can fill, along with the implementation
    of the Sense to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, we will have our array filled up with the Perceived Actors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'By measuring the length of this array, we can get the number of currently perceived
    actors at the moment. Moreover, by checking if the Actor that was passed from
    the event (the parameter of the function) is in the currently "*seen Actors*" array,
    we can determine whether such an actor has left or entered the field of view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to pack this information into a formatted string, and then
    print it on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once again, I know that "1 objects" is incorrect, but correcting plurals (although
    possible) is outside the scope of this book; let's keep it simple.
  prefs: []
  type: TYPE_NORMAL
- en: Testing it all
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you should have an AI controller with the perception system implemented
    (whether it is in Blueprint or C++ – it doesn't matter, they should behave identically).
  prefs: []
  type: TYPE_NORMAL
- en: 'Create another `ThirdPersonCharacter` by *Alt* + *Dragging* the player into
    the level (if you want to use an AI that we created in the previous chapters,
    you can do so):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df7f7507-b26d-4568-a1a2-f47c14969dee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the *Details* panel, we make it be controlled by our AI controller, and
    not a player (this should be a process that''s easy to you by now):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c134131-d15e-4d83-9e7c-60f7cd77f2f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, if you are going with a C++ setup, choose the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50b3295c-6d69-4dc9-b9f4-63982f12d0a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Before pressing play, it would be nice to create some other objects that can
    be detected. We know that all the Pawns are detected (unless disabled), so let's
    try something that isn't a Pawn – maybe a moving platform. As a result, if we
    want to detect it, we need to use the **AIPerceptionStimuliSourceComponent**.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create the floating platform (which can be easily pushed by our
    character). If you are in the default level of the *ThirdPersonCharacter Example*,
    you can duplicate with *Alt + Drag* this big mesh, which is highlighted in the
    following screenshot (otherwise, if you are using a custom level, a cube that
    you can squash will work fine):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b681cee9-abf5-45c6-b6c5-75d0a6a19f0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So far, it''s way too big, so let''s scale it down to (1, 1, 0.5). Also, to
    be on the same page, you can move it to (-500, 310, 190). Finally, we need to
    change the mobility to Movable, since it needs to move:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cc272c4-56b5-4a87-8d07-fb1be8ddb556.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we want to be able to push such a platform, so we need to enable Physics
    simulation. To keep it pushable by our character, let''s give it a mass of *100
    Kg* (I know, it seems like a lot, but with little friction and with the fact that
    the platform floats, it''s the right amount). Moreover, we don''t want the platform
    to rotate, so we need to block all the three rotational axes inside **Constraints**.
    The same goes if we want the platform to float – if we lock the z-axis, the platform
    can only move along the *XY plane* with no rotation. This will ensure a nice,
    pushable platform. This is what the Physics part should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6752a91-6659-41af-a376-858798e8c904.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we need to add a **AIPerceptionStimuliSourceComponent**, from the
    **Add Component** green button near the name of the Actor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5a6ec6f-30c6-48a5-85bd-527d4e6bbeee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the component has been added, we can select it from the preceding menu.
    As a result, the *Details* panel will allow us to change **AIPerceptionStimuliSourceComponent**
    settings. In particular, we want to add the *Sight Sense*, and automatically register
    the component as a source. This is how we should set it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67575c9b-2175-4d6d-a9ff-f7be80327656.png)'
  prefs: []
  type: TYPE_IMG
- en: As an optional step, you can convert this into a blueprint so that you can reuse
    it, and maybe assign a more meaningful name. Also, you can duplicate it a few
    times if you want to have several objects be tracked by the *Sight Perception
    System*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can hit play and test what we have achieved so far. If you pass
    our *AI controlled Character*, you will get a notification on the top of the screen.
    We get the same output if we push a platform inside or out of the AI''s field
    of view. In the following screenshot, you can see the C++ implementation, but
    it works very similarly with the Blueprint one (just the color of the print changes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0baa2090-609d-4055-ab47-a299a33f8607.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, as anticipation, it is possible to see the AI field of view with the
    visual debugger, which we will explore in [Chapter 13](a8cbf52e-71e1-4f9d-a2bd-913a1e8bd8e1.xhtml),
    *Debugging Methods for AI - The Gameplay Debugger*. The following screenshot is
    a reference of the field of view of the AI Character we have created. For details
    on how to display it and understand what all this information means, hang on until [Chapter
    13](a8cbf52e-71e1-4f9d-a2bd-913a1e8bd8e1.xhtml), *Debugging Methods for AI - The
    Gameplay Debugger*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ffc59c-02df-4c6c-b515-6a9b19be5fc2.png)'
  prefs: []
  type: TYPE_IMG
- en: It's time to pat yourself on the back because it might seem like you've only
    done a little, but actually, you managed to learn about a complex system. Also,
    if you tried one way (Blueprint or C++), try the other one if you want to be able
    to master the system both in Blueprint and in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And that was a lot of information to perceive, wasn't it?
  prefs: []
  type: TYPE_NORMAL
- en: We started by understanding how the different pieces of the built-in perception
    system in Unreal works within the AI framework. From there, we explored how we
    can actually use those components (both in C++ and Blueprint), and learned how
    to properly configure them as well.
  prefs: []
  type: TYPE_NORMAL
- en: We concluded with a practical example of setting up a *Sight* perception system,
    and once again did so both in Blueprint and C++.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how we can simulate large *Crowds*.
  prefs: []
  type: TYPE_NORMAL
