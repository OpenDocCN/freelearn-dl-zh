<html><head></head><body>
  <div><h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-240" class="chapterTitle">RAG for Video Stock Production with Pinecone and OpenAI</h1>
    <p class="normal">Human creativity goes beyond the range of well-known patterns due to our unique ability to break habits and invent new ways of doing anything, anywhere. Conversely, Generative AI relies on our well-known established patterns across an increasing number of fields without really “creating” but rather replicating our habits. In this chapter, therefore, when we use the term “create” as a practical term, we only mean “generate.” Generative AI, with its efficiency in automating tasks, will continue its expansion until it finds ways of replicating any human task it can. We must, therefore, learn how these automated systems work to use them for the best in our projects. Think of this chapter as a journey into the architecture of RAG in the cutting-edge hybrid human and AI agent era we are living in. We will assume the role of a start-up aiming to build an AI-driven downloadable stock of online videos. To achieve this, we will establish a team of AI agents that will work together to create a stock of commented and labeled videos.</p>
    <p class="normal">Our journey begins with the Generator agent in <em class="italic">Pipeline 1: The Generator and the Commentator</em>. The Generator agent creates world simulations using Sora, an OpenAI text-to-video model. You’ll see how the <em class="italic">inVideo </em>AI application, powered by Sora, engages in “ideation,” transforming an idea into a video. The Commentator agent then splits the AI-generated videos into frames and generates technical comments with an OpenAI vision model. Next, in <em class="italic">Pipeline 2: The Vector Store Administrator, </em>we will continue our journey and build the Vector Store Administrator that manages Pinecone. The Vector Store Administrator will embed the technical video comments generated by the Commentator, upsert the vectorized comments, and query the Pinecone vector store to verify that the system is functional. Finally, we will build the Video Expert that processes user inputs, queries the vector store, and retrieves the relevant video frames. Finally, in <em class="italic">Pipeline 3: The Video Expert</em>, the Video Expert agent will augment user inputs with the raw output of the query and activate its expert OpenAI GPT-4o model, which will analyze the comment, detect imperfections, reformulate it more efficiently, and provide a label for the video.</p>
    <p class="normal">By the end of the chapter, you will know how to automatically generate a stock of short videos by automating the process of going from raw footage to videos with descriptions and labels. You’ll be able to offer a service where users can simply type a few words and obtain a video with a custom, real-time description and label.</p>
    <p class="normal">Summing that up, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">Designing Generative AI videos and comments</li>
      <li class="bulletList">Splitting videos into frames for OpenAI’s vision analysis models</li>
      <li class="bulletList">Embedding the videos and upserting the vectors to a Pinecone index</li>
      <li class="bulletList">Querying the vector store</li>
      <li class="bulletList">Improving and correcting the video comments with OpenAI GPT-4o</li>
      <li class="bulletList">Automatically labeling raw videos</li>
      <li class="bulletList">Displaying the full result of the raw video process with a commented and labeled video</li>
      <li class="bulletList">Evaluating outputs and implementing metric calculations</li>
    </ul>
    <p class="normal">Let’s begin by defining the architecture of RAG for video production.</p>
    <h1 id="_idParaDest-241" class="heading-1">The architecture of RAG for video production</h1>
    <p class="normal">Automating the process of real-world video generation, commenting, and labeling is extremely relevant in various industries, such as media, marketing, entertainment, and education. Businesses and creators are continuously seeking efficient ways to produce and manage content that can scale <a id="_idIndexMarker583"/>with growing demand. In this chapter, you will acquire practical skills that can be directly applied to meet these needs.</p>
    <p class="normal">The goal of our RAG video production use case in this chapter is to process AI-generated videos using AI agents to create a video stock of labeled videos to identify them. The system will also dynamically generate custom descriptions by pinpointing AI-generated technical comments on specific frames within the videos that fit the user input. <em class="italic">Figure 10.1</em> illustrates the AI-agent team that processes RAG for video production:</p>
    <p class="packt_figref"><img src="img/B31169_10_01.png" alt="A diagram of a video production process  Description automatically generated"/></p>
    <p class="packt_figref">Figure 10.1: From raw videos to labeled and commented videos</p>
    <p class="normal">We will implement AI agents for <a id="_idIndexMarker584"/>our RAG video production pipeline that will:</p>
    <ul>
      <li class="bulletList">Generate raw videos automatically and download them</li>
      <li class="bulletList">Split the videos into frames</li>
      <li class="bulletList">Analyze a sample of frames</li>
      <li class="bulletList">Activate an OpenAI LLM model to generate technical comments</li>
      <li class="bulletList">Save the technical comments with a unique index, the comment itself, the frame number analyzed, and the video file name</li>
      <li class="bulletList">Upsert the data in a Pinecone index vector store</li>
      <li class="bulletList">Query the Pinecone vector store with user inputs</li>
      <li class="bulletList">Retrieve the specific frame within a video that is most similar to its technical comment</li>
      <li class="bulletList">Augment the user input with the technical comment of the retrieved frame</li>
      <li class="bulletList">Ask the OpenAI LLM to analyze the logic of the technical comment that may contain contradictions and imperfections detected in the video and then produce a dynamic, well-tailored description of the video with the frame number and the video file name</li>
      <li class="bulletList">Display the selected video</li>
      <li class="bulletList">Evaluate the outputs and apply metric calculations</li>
    </ul>
    <p class="normal">We will thus go from raw videos to labeled videos with tailored descriptions based on the user input. For example, we will be<a id="_idIndexMarker585"/> able to ask precise questions such as the following:</p>
    <pre class="programlisting code"><code class="hljs-code">"Find a basketball player that is scoring with a dunk."
</code></pre>
    <p class="normal">This means that the system will be able to find a frame (image) within the initially unlabeled video, select the video, display it, and generate a tailored comment dynamically. To attain our goal, we will implement AI agents in three pipelines, as illustrated in the following figure:</p>
    <p class="packt_figref"><img src="img/B31169_10_02.png" alt="A diagram of a process  Description automatically generated"/></p>
    <p class="packt_figref">Figure 10.2: The RAG for Video Production Ecosystem with Generative AI agents</p>
    <p class="normal">Now, what you see in the figure above is:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline 1</strong>: The <strong class="keyWord">Generator</strong> and the <strong class="keyWord">Commentator</strong></li>
    </ul>
    <p class="normal-one">The <strong class="keyWord">Generator</strong> produces AI-generated videos with OpenAI Sora. The <strong class="keyWord">Commentator</strong> splits the videos into<a id="_idIndexMarker586"/> frames that are commented on by one of OpenAI’s vision models. The <strong class="keyWord">Commentator</strong> agent then saves the comments.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline 2</strong>: <strong class="keyWord">The Vector Store Administrator</strong></li>
    </ul>
    <p class="normal-one">This pipeline will embed and upsert the comments made by <em class="italic">Pipeline 1</em> to a Pinecone index.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline 3</strong>: <strong class="keyWord">The Video Expert</strong></li>
    </ul>
    <p class="normal-one">This pipeline will query the Pinecone vector store based on user input. The query will return the most similar frame within a video, augment the input with the technical comment, and ask OpenAI GPT-4o to find logic imperfections in the video, point them out, and then produce a tailored comment of the video for the user and a label. This section also contains evaluation functions (the Evaluator) and metric calculations.</p>
    <div><p class="normal">Time measurement functions are encapsulated in several of the key functions of the preceding ecosystem.</p>
    </div>
    <p class="normal">The RAG video production system we will build allows indefinite scaling by processing one video at a time, using only a CPU and little memory, while leveraging Pinecone’s storage capacity. This effectively demonstrates the concept of automated video production, but implementing this production system in a real-life project requires hard work. However, the technology is there, and the future of video production is undergoing a historical evolution. Let’s dive into the code, beginning with the environment.</p>
    <h1 id="_idParaDest-242" class="heading-1">The environment of the video production ecosystem</h1>
    <p class="normal">The <code class="inlineCode">Chapter10</code> directory<a id="_idIndexMarker587"/> on GitHub contains the environment for all four <a id="_idIndexMarker588"/>notebooks in this chapter:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Videos_dataset_visualization.ipynb</code></li>
      <li class="bulletList"><code class="inlineCode">Pipeline_1_The_Generator_and_the_Commentator.ipynb</code></li>
      <li class="bulletList"><code class="inlineCode">Pipeline_2_The_Vector_Store_Administrator.ipynb</code></li>
      <li class="bulletList"><code class="inlineCode">Pipeline_3_The_Video_Expert.ipynb</code></li>
    </ul>
    <p class="normal">Each notebook includes an <em class="italic">Installing the environment</em> section, including a set of the following sections that<a id="_idIndexMarker589"/> are identical across all notebooks:</p>
    <ul>
      <li class="bulletList"><em class="italic">Importing modules and libraries</em></li>
      <li class="bulletList"><em class="italic">GitHub</em></li>
      <li class="bulletList"><em class="italic">Video download and display functions</em></li>
      <li class="bulletList"><em class="italic">OpenAI</em></li>
      <li class="bulletList"><em class="italic">Pinecone</em></li>
    </ul>
    <p class="normal">This chapter aims to establish a common pre-production installation policy that will focus on the pipelines’ content once we dive into the RAG for video production code. This policy is limited to the scenario described in this chapter and will vary depending on the requirements of each real-life production environment.</p>
    <div><p class="normal">The notebooks in this chapter only require a CPU, limited memory, and limited disk space. As such, the whole process can be streamlined indefinitely one video at a time in an optimized, scalable environment.</p>
    </div>
    <p class="normal">Let’s begin by importing the modules and libraries we need for our project.</p>
    <h2 id="_idParaDest-243" class="heading-2">Importing modules and libraries</h2>
    <p class="normal">The goal is to prepare a pre-production<a id="_idIndexMarker590"/> global environment common to all the notebooks. As such, the modules and libraries are present in all four notebooks regardless of whether they are used or not in a specific program:</p>
    <pre class="programlisting code"><code class="hljs-code">from IPython.display import HTML # to display videos
import base64 # to encode videos as base64
from base64 import b64encode # to encode videos as base64
import os # to interact with the operating system
import subprocess # to run commands
import time # to measure execution time
import csv # to save comments
import uuid # to generate unique ids
import cv2 # to split videos
from PIL import Image # to display videos
import pandas as pd # to display comments
import numpy as np # to use Numerical Python
from io import BytesIO #to manage a binary stream of data in memory
</code></pre>
    <p class="normal">Each of the four notebooks <a id="_idIndexMarker591"/>contains these modules and libraries, as shown in the following table:</p>
    <table id="table001-2" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Code</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Comment</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">from IPython.display import HTML</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To display videos</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import base64</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To encode videos as <code class="inlineCode">base64</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">from base64 import b64encode</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To encode videos as <code class="inlineCode">base64</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import os</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To interact with the operating system</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import subprocess</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To run commands</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import time</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To measure execution time</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import csv</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To save comments</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import uuid</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To generate unique IDs</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import cv2</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To split videos (open source computer vision library)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">from PIL import Image</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To display videos</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import pandas as pd</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To display comments</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">import numpy as np</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">To use Numerical Python</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">from io import BytesIO</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">For a binary stream of data in memory</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 10.1: Modules and libraries for our video production system</p>
    <p class="normal">The <code class="inlineCode">Code</code> column contains the module or library name, while the <code class="inlineCode">Comment</code> column provides a brief description of<a id="_idIndexMarker592"/> their usage. Let’s move on to GitHub commands.</p>
    <h2 id="_idParaDest-244" class="heading-2">GitHub</h2>
    <p class="normal"><code class="inlineCode">download(directory, filename)</code> is present<a id="_idIndexMarker593"/> in all four notebooks. The main function of <code class="inlineCode">download(directory, filename)</code> is to<a id="_idIndexMarker594"/> download the files we need from the book’s GitHub repository:</p>
    <pre class="programlisting code"><code class="hljs-code">def download(directory, filename):
    # The base URL of the image files in the GitHub repository
    base_url = 'https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/'
    # Complete URL for the file
    file_url = f"{base_url}{directory}/{filename}"
    # Use curl to download the file
    try:
        # Prepare the curl command
        curl_command = f'curl -o {filename} {file_url}'
        # Execute the curl command
        subprocess.run(curl_command, check=True, shell=True)
        print(f"Downloaded '{filename}' successfully.")
    except subprocess.CalledProcessError:
        print(f"Failed to download '{filename}'. Check the URL, your internet connection, and if the token is correct and has appropriate permissions.")
</code></pre>
    <p class="normal">The preceding function takes two arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">directory</code>, which is the GitHub directory that the file we want to download is located in</li>
      <li class="bulletList"><code class="inlineCode">filename</code>, which is the name of the file we want to download</li>
    </ul>
    <h2 id="_idParaDest-245" class="heading-2">OpenAI</h2>
    <p class="normal">The OpenAI package is installed in all three<a id="_idIndexMarker595"/> pipeline <a id="_idIndexMarker596"/>notebooks but not in <code class="inlineCode">Video_dataset_visualization.ipynb</code>, which doesn’t require an LLM. You can retrieve the API key from a file or <a id="_idIndexMarker597"/>enter it manually (but it will be visible):</p>
    <pre class="programlisting code"><code class="hljs-code">#You can retrieve your API key from a file(1)
# or enter it manually(2)
#Comment this cell if you want to enter your key manually.
#(1)Retrieve the API Key from a file
#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)
from google.colab import drive
drive.mount('/content/drive')
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline()o
Nf.close()
</code></pre>
    <p class="normal">You will need to sign up at <code class="inlineCode">www.openai.com</code> before running the code and obtain an API key. The program<a id="_idIndexMarker598"/> installs the <code class="inlineCode">openai</code> package:</p>
    <pre class="programlisting code"><code class="hljs-code">try:
  import openai
except:
  #!pip install openai==1.45.0
  import openai
</code></pre>
    <p class="normal">Finally, we set an environment variable for the API key:</p>
    <pre class="programlisting code"><code class="hljs-code">#(2) Enter your manually by
# replacing API_KEY by your key.
#The OpenAI Key
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
</code></pre>
    <h2 id="_idParaDest-246" class="heading-2">Pinecone</h2>
    <p class="normal">The <em class="italic">Pinecone</em> section<a id="_idIndexMarker599"/> is only present in <code class="inlineCode">Pipeline_2_The_Vector_Store_Administrator.ipynb</code> and <code class="inlineCode">Pipeline_3_The_Video_Expert.ipynb</code> when the<a id="_idIndexMarker600"/> Pinecone vector store is required. The following command installs Pinecone, and then Pinecone is imported:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install pinecone-client==4.1.1
import pinecone
</code></pre>
    <p class="normal">The program then retrieves the key from a file (or you can enter it manually):</p>
    <pre class="programlisting code"><code class="hljs-code">f = open("drive/MyDrive/files/pinecone.txt", "r")
PINECONE_API_KEY=f.readline()
f.close()
</code></pre>
    <p class="normal">In production, you can<a id="_idIndexMarker601"/> set an environment variable or<a id="_idIndexMarker602"/> implement the method that best fits your project so that the API key is never visible.</p>
    <div><p class="normal">The <em class="italic">Evaluator</em> section of <code class="inlineCode">Pipeline_3_The_Video_Expert.ipynb</code> contains its own requirements and installations.</p>
    </div>
    <p class="normal">With that, we have defined the environment for all four notebooks, which contain the same sub-sections we just described in their respective <em class="italic">Installing the environment </em>sections. We can now fully focus on the processes involved in the video production programs. We will begin with the Generator and Commentator.</p>
    <h1 id="_idParaDest-247" class="heading-1">Pipeline 1: Generator and Commentator</h1>
    <p class="normal">A revolution is on its way in computer vision with automated video generation and analysis. We will introduce the Generator AI agent with Sora in <em class="italic">The AI-generated video dataset</em> section. We will explore how OpenAI Sora was used to generate the videos for this chapter with a text-to-video diffusion transformer. The technology itself is something we have expected and<a id="_idIndexMarker603"/> experienced to some extent in professional film-making environments. However, the novelty relies on the fact that the software has become mainstream in a few clicks, with inVideo, for example!</p>
    <p class="normal">In the <em class="italic">The Generator and the Commentator</em> section, we will extend the scope of the Generator to collecting and processing the AI-generated videos. The Generator splits the videos into frames and works with the Commentator, an OpenAI LLM, to produce comments on samples of video frames.</p>
    <p class="normal">The Generator’s task begins by producing the AI-generated video dataset.</p>
    <h2 id="_idParaDest-248" class="heading-2">The AI-generated video dataset</h2>
    <p class="normal">The first AI agent in this project is a text-to-video diffusion transformer model that generates a video dataset we <a id="_idIndexMarker604"/>will implement. The videos for this chapter were specifically generated by Sora, a text-to-video AI model released by OpenAI in February 2024. You can access Sora to view public AI-generated videos and create your own at <a href="https://ai.invideo.io/">https://ai.invideo.io/</a>. AI-generated videos also allow for<a id="_idIndexMarker605"/> free videos with flexible copyright terms that you can check out at <a href="https://invideo.io/terms-and-conditions/">https://invideo.io/terms-and-conditions/</a>.</p>
    <div><p class="normal">Once you have gone through this chapter, you can also create your own video dataset with any source of videos, such as smartphones, video stocks, and social media.</p>
    </div>
    <p class="normal">AI-generated videos enhance the speed of creating video datasets. Teams do not have to spend time finding videos that fit their needs. They can obtain a video quickly with a prompt that can be an idea expressed in a few words. AI-generated videos represent a huge leap into the future of AI applications. Sora’s potential applies to many industries, including filmmaking, education, and marketing. Its ability to generate nuanced video content from simple text prompts opens new avenues for creative and educational outputs.</p>
    <p class="normal">Although AI-generated videos (and, in particular, diffusion transformers) have changed the way we create world simulations, this represents a risk for jobs in many areas, such as filmmaking. The risk of deep fakes and misinformation is real. At a personal level, we must take ethical considerations into account when we implement Generative AI in a project, thus producing constructive, ethical, and realistic content.</p>
    <p class="normal">Let’s see how a diffusion transformer can produce realistic content.</p>
    <h3 id="_idParaDest-249" class="heading-3">How does a diffusion transformer work?</h3>
    <p class="normal">At the core of Sora, as described by Liu et al., 2024 (see the <em class="italic">References</em> section), is a diffusion transformer model that operates <a id="_idIndexMarker606"/>between an encoder and a decoder. It uses user text input to guide the content generation, associating it with patches from the encoder. The model iteratively refines these noisy latent representations, enhancing their clarity and coherence. Finally, the refined data is passed to the decoder to reconstruct high-fidelity video frames. The technology involved includes vision transformers such as CLIP and LLMs such as GPT-4, as well as other components OpenAI continually includes in its vision model releases.</p>
    <p class="normal">The encoder and decoder are integral components of the overall diffusion model, as illustrated in <em class="italic">Figure 10.3</em>. They both<a id="_idIndexMarker607"/> play a critical role in the workflow of the transformer diffusion model:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Encoder</strong>: The encoder’s primary function is to compress input data, such as images or videos, into a lower-dimensional latent space. The encoder thus transforms high-dimensional visual data into a compact representation while preserving crucial information. A lower-dimensional latent space obtained is a compressed representation of high-dimensional data, retaining essential features while reducing complexity. For example, a high-resolution image (1024x1024 pixels, 3 color channels) can be compressed by an encoder into a vector of 1000 values, capturing key details like shape and texture. This makes processing and manipulating images more efficient.</li>
      <li class="bulletList"><strong class="keyWord">Decoder</strong>: The decoder reconstructs the original data from the latent representation produced by the encoder. It performs the encoder’s reverse operation, transforming the low-dimensional latent space back into high-dimensional pixel space, thus generating the final output, such as images or videos.</li>
    </ul>
    <p class="packt_figref"><img src="img/B31169_10_03.png" alt="A diagram of a workflow  Description automatically generated"/></p>
    <p class="packt_figref">Figure 10.3: The encoding and decoding workflow of video diffusion models</p>
    <p class="normal">The process of a diffusion transformer model goes through five main steps, as you can observe in the previous figure:</p>
    <ol>
      <li class="numberedList" value="1">The visual encoder<a id="_idIndexMarker608"/> transforms datasets of images into a lower-dimensional latent space.</li>
      <li class="numberedList">The visual encoder splits the lower-dimensional latent space into patches that are like words in a sentence.</li>
      <li class="numberedList">The diffusion transformer associates user text input with its dictionary of patches.</li>
      <li class="numberedList">The diffusion transformer iteratively refines noisy image representations generated to produce coherent frames.</li>
      <li class="numberedList">The visual decoder reconstructs the refined latent representations into high-fidelity video frames that align with the user’s instructions.</li>
    </ol>
    <p class="normal">The video frames can then be played in a sequence. Every second of a video contains a set of frames. We will be deconstructing the AI-generated videos into frames and commenting on these frames <a id="_idIndexMarker609"/>later. But for now, we will analyze the video dataset produced by the diffusion transformer.</p>
    <h3 id="_idParaDest-250" class="heading-3">Analyzing the diffusion transformer model video dataset</h3>
    <p class="normal">Open the <code class="inlineCode">Videos_dataset_visualization.ipynb</code> notebook on GitHub. Hopefully, you have installed the environment<em class="italic"> </em>as described earlier in this chapter. We will move on to writing the download and <a id="_idIndexMarker610"/>display functions<a id="_idIndexMarker611"/> we need.</p>
    <h4 class="heading-4">Video download and display functions</h4>
    <p class="normal">The three main functions each use <code class="inlineCode">filename</code> (the name of the video file) as an argument. The three main functions <a id="_idIndexMarker612"/>download and display videos, and display frames in the videos.</p>
    <p class="normal"><code class="inlineCode">download_video</code> downloads one video at a time from the GitHub dataset, calling the <code class="inlineCode">download</code> function defined in the <em class="italic">GitHub</em> subsection of <em class="italic">The environment</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"># downloading file from GitHub
def download_video(filename):
  # Define your variables
  directory = "Chapter10/videos"
  filename = file_name
  download(directory, filename)
</code></pre>
    <p class="normal"><code class="inlineCode">display_video(file_name)</code> displays the video file downloaded by first encoding in <code class="inlineCode">base64</code>, a binary-to-text encoding scheme that represents binary data in ASCII string format. Then, the encoded video is displayed in HTML:</p>
    <pre class="programlisting code"><code class="hljs-code"># Open the file in binary mode
def display_video(file_name):
    with open(file_name, 'rb') as file:
      video_data = file.read()
  # Encode the video file as base64
  video_url = b64encode(video_data).decode()
  # Create an HTML string with the embedded video
  html = f'''
  &lt;video width="640" height="480" controls&gt;
    &lt;source src="img/mp4;base64,{video_url}" type="video/mp4"&gt;
  Your browser does not support the video tag.
  &lt;/video&gt;
  '''
  # Display the video
  HTML(html)
  # Return the HTML object
  return HTML(html)
</code></pre>
    <p class="normal"><code class="inlineCode">display_video_frame</code> takes <code class="inlineCode">file_name</code>, <code class="inlineCode">frame_number</code>, and <code class="inlineCode">size</code> (the image size to display) as arguments to display a frame in the video. The function first opens the video file and then extracts<a id="_idIndexMarker613"/> the frame number set by <code class="inlineCode">frame_number</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">def display_video_frame(file_name, frame_number, size):
    # Open the video file
    cap = cv2.VideoCapture(file_name)
    # Move to the frame_number
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
    # Read the frame
    success, frame = cap.read()
    if not success:
      return "Failed to grab frame"
</code></pre>
    <p class="normal">The function converts the file from the BGR (blue, green, and red) to the RGB (red, green, and blue) channel, converts it to PIL, an image array (such as one handled by OpenCV), and resizes it with the <code class="inlineCode">size</code> parameters:</p>
    <pre class="programlisting code"><code class="hljs-code"># Convert the color from BGR to RGB
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    # Convert to PIL image and resize
    img = Image.fromarray(frame)
    img = img.resize(size, Image.LANCZOS)  # Resize image to specified size
</code></pre>
    <p class="normal">Finally, the function encodes the image in string format with <code class="inlineCode">base64</code> and displays it in HTML:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Convert the PIL image to a base64 string to embed in HTML
    buffered = BytesIO()
    img.save(buffered, format="JPEG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    # Create an HTML string with the embedded image
    html_str = f'''
    &lt;img src="img/jpeg;base64,{img_str}" width="{size[0]}" height="{size[1]}"&gt;
    '''
    # Display the image
    display(HTML(html_str))
    # Return the HTML object for further use if needed
    return HTML(html_str)
</code></pre>
    <p class="normal">Once the environment is<a id="_idIndexMarker614"/> installed and the video processing functions are ready, we will display the introduction video.</p>
    <h4 class="heading-4">Introduction video (with audio)</h4>
    <p class="normal">The following cells download and display the introduction video using the functions we created in the previous section. A video file is<a id="_idIndexMarker615"/> selected and downloaded with the <code class="inlineCode">download_video</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"># select file
print("Collecting video")
file_name="AI_Professor_Introduces_New_Course.mp4"
#file_name = "AI_Professor_Introduces_New_Course.mp4" # Enter the name of the video file to process here
print(f"Video: {file_name}")
# Downloading video
print("Downloading video: downloading from GitHub")
download_video(file_name)
</code></pre>
    <p class="normal">The output confirms the selection and download status:</p>
    <pre class="programlisting con"><code class="hljs-con">Collecting video
Video: AI_Professor_Introduces_New_Course.mp4
Downloading video: downloading from GitHub
Downloaded 'AI_Professor_Introduces_New_Course.mp4' successfully.
</code></pre>
    <p class="normal">We can choose to display only a single frame of the video as a thumbnail with the <code class="inlineCode">display_video_frame</code> function by providing the file name, the frame number, and the image size to display. The program will first compute <code class="inlineCode">frame_count</code> (the number of frames in the video), <code class="inlineCode">frame_rate</code> (the number of frames per second), and <code class="inlineCode">video_duration</code> (the duration <a id="_idIndexMarker616"/>of the video). Then, it will make sure <code class="inlineCode">frame_number</code> (the frame we want to display) doesn’t exceed <code class="inlineCode">frame_count</code>. Finally, it displays the frame as a thumbnail:</p>
    <pre class="programlisting code"><code class="hljs-code">print("Displaying a frame of video: ",file_name)
video_capture = cv2.VideoCapture(file_name)
frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))
print(f'Total number of frames: {frame_count}')
frame_rate = video_capture.get(cv2.CAP_PROP_FPS)
print(f"Frame rate: {frame_rate}")
video_duration = frame_count / frame_rate
print(f"Video duration: {video_duration:.2f} seconds")
video_capture.release()
print(f'Total number of frames: {frame_count}')
frame_number=5
if frame_number &gt; frame_count and frame_count&gt;0:
  frame_number = 1
display_video_frame(file_name, frame_number, size=(135, 90));
</code></pre>
    <p class="normal">Here, <code class="inlineCode">frame_number</code> is set to <code class="inlineCode">5</code>, but you can choose another value. The output shows the information on the video and the thumbnail:</p>
    <pre class="programlisting con"><code class="hljs-con">Displaying a frame of video:  /content/AI_Professor_Introduces_New_Course.mp4
Total number of frames: 340
<img src="img/B31169_10_04.png" alt="A person taking a picture of a person sitting in a church  Description automatically generated"/>
</code></pre>
    <p class="normal">We can also display the full video if needed:</p>
    <pre class="programlisting code"><code class="hljs-code">#print("Displaying video: ",file_name)
display_video(file_name)
</code></pre>
    <p class="normal">The video will be displayed <a id="_idIndexMarker617"/>and can be played with the audio track:</p>
    <figure class="mediaobject"><img src="img/B31169_10_05.png" alt="A person sitting in a chair  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.4: AI-generated video</p>
    <p class="normal">Let’s describe and display AI-generated videos in the <code class="inlineCode">/videos</code> directory of this chapter’s GitHub directory. You can host this dataset in another location and scale it to the volume that meets your project’s specifications. The educational video dataset of this chapter is listed in <code class="inlineCode">lfiles</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">lfiles = [
    "jogging1.mp4",
    "jogging2.mp4",
    "skiing1.mp4",
    …
    "female_player_after_scoring.mp4",
    "football1.mp4",
    "football2.mp4",
    "hockey1.mp4"
]
</code></pre>
    <p class="normal">We can now move on<a id="_idIndexMarker618"/> and display any video we wish.</p>
    <h4 class="heading-4">Displaying thumbnails and videos in the AI-generated dataset</h4>
    <p class="normal">This section is a generalization of the <em class="italic">Introduction video (with audio)</em> section. This time, instead of downloading one video, it downloads all the videos and displays the thumbnails of all the videos. You <a id="_idIndexMarker619"/>can then select a video in the list and display it.</p>
    <p class="normal">The program first collects the video dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">for i in range(lf):
  file_name=lfiles[i]
  print("Collecting video",file_name)
  print("Downloading video",file_name)
  download_video(file_name)
</code></pre>
    <p class="normal">The output shows the file names of the downloaded videos:</p>
    <pre class="programlisting con"><code class="hljs-con">Collecting video jogging1.mp4
Downloading video jogging1.mp4
Downloaded 'jogging1.mp4' successfully.
Collecting video jogging2.mp4…
</code></pre>
    <p class="normal">The program calculates the number of videos in the list:</p>
    <pre class="programlisting code"><code class="hljs-code">lf=len(lfiles)
</code></pre>
    <p class="normal">The program goes through the list and displays the information for each video and displays its thumbnail:</p>
    <pre class="programlisting code"><code class="hljs-code">for i in range(lf):
  file_name=lfiles[i]
  video_capture.release()
  display_video_frame(file_name, frame_number=5, size=(100, 110))
</code></pre>
    <p class="normal">The information on the video and its thumbnail is displayed:</p>
    <pre class="programlisting con"><code class="hljs-con">Displaying a frame of video:  skiing1.mp4
Total number of frames: 58
Frame rate: 30.0
Video duration: 1.93 seconds
<img src="img/B31169_10_06.png" alt="A group of people skiing down a slope  Description automatically generated"/>
</code></pre>
    <p class="normal">You can select a video<a id="_idIndexMarker620"/> in the list and display it:</p>
    <pre class="programlisting code"><code class="hljs-code">file_name="football1.mp4" # Enter the name of the video file to process here
#print("Displaying video: ",file_name)
display_video(file_name)
</code></pre>
    <p class="normal">You can click on the video and watch it:</p>
    <figure class="mediaobject"><img src="img/B31169_10_07.png" alt="A person in a football uniform pointing at a football  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.5: Video of a football player</p>
    <p class="normal">We have explored how the AI-generated videos were produced and visualized the dataset. We are now ready to build the Generator and the Commentator.</p>
    <h2 id="_idParaDest-251" class="heading-2">The Generator and the Commentator</h2>
    <p class="normal">The dataset of AI-generated videos is ready. We will now build the Generator and the Commentator, which processes one video at a time, making scaling seamless. An indefinite number of videos can be processed one at a time, requiring only a CPU and limited disk space. The Generator and <a id="_idIndexMarker621"/>the Commentator work together, as shown in <em class="italic">Figure 10.8</em>. These AI agents will produce raw videos from text and then split them into frames that they will comment on:</p>
    <figure class="mediaobject"><img src="img/B31169_10_08.png" alt="A diagram of a sports game  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 10.6: The Generator and the Commentator work together to comment on video frames</p>
    <p class="normal">The Generator and the<a id="_idIndexMarker622"/> Commentator produce the commented frames required in four main steps that we will build in Python:</p>
    <ol>
      <li class="numberedList" value="1">The <strong class="keyWord">Generator</strong> generates the text-to-video inVideo video dataset based on the video production team’s text input. In this chapter, it is a dataset of sports videos.</li>
      <li class="numberedList">The <strong class="keyWord">Generator</strong> runs a scaled process by selecting one video at a time.</li>
      <li class="numberedList">The <strong class="keyWord">Generator</strong> splits the video into frames (images)</li>
      <li class="numberedList">The <strong class="keyWord">Commentator</strong> samples frames (images) and comments on them with an OpenAI LLM model. Each commented frame is saved with:<ul>
          <li class="bulletList level-2">Unique ID</li>
          <li class="bulletList level-2">Comment</li>
          <li class="bulletList level-2">Frame</li>
          <li class="bulletList level-2">Video file name</li>
        </ul>
      </li>
    </ol>
    <p class="normal">We will now build the Generator and the Commentator in Python, starting with the AI-generated videos. Open <code class="inlineCode">Pipeline_1_The_Generator_and_the_Commentator.ipynb</code> in the chapter’s GitHub directory. See the <em class="italic">The environment</em> section of this chapter for a description of the <em class="italic">Installing the environment</em> section of this notebook. The process of going from a video to<a id="_idIndexMarker623"/> comments on a sample of frames only takes three straightforward steps in Python:</p>
    <ol>
      <li class="numberedList" value="1">Displaying the video</li>
      <li class="numberedList">Splitting the video into frames</li>
      <li class="numberedList">Commenting on the frames</li>
    </ol>
    <p class="normal">We will define functions for each step and call them in the <code class="inlineCode">Pipeline-1 Controller</code> section of the program. The first step is to define a function to display a video.</p>
    <h3 id="_idParaDest-252" class="heading-3">Step 1. Displaying the video</h3>
    <p class="normal">The <code class="inlineCode">download</code> function is in the <em class="italic">GitHub</em> subsection of the <em class="italic">Installing the environment</em> section of this notebook. It will be called by <a id="_idIndexMarker624"/>the <em class="italic">Vector Store Administrator-Pipeline 1</em> in the <em class="italic">Administrator-Pipeline 1</em> section of this notebook on GitHub.</p>
    <p class="normal"><code class="inlineCode">display_video(file_name)</code> is the same as defined in the previous section, <em class="italic">The AI-generated video dataset</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"># Open the file in binary mode
def display_video(file_name):
  with open(file_name, 'rb') as file:
      video_data = file.read()
…
  # Return the HTML object
  return HTML(html)
</code></pre>
    <p class="normal">The downloaded video will now be split into frames.</p>
    <h3 id="_idParaDest-253" class="heading-3">Step 2. Splitting video into frames</h3>
    <p class="normal">The <code class="inlineCode">split_file(file_name)</code> function extracts <a id="_idIndexMarker625"/>frames from a video, as in the previous section, <em class="italic">The AI-generated video dataset</em>. However, in this case, we will expand the function to save frames as JPEG files:</p>
    <pre class="programlisting code"><code class="hljs-code">def split_file(file_name):
  video_path = file_name
  cap = cv2.VideoCapture(video_path)
  frame_number = 0
  while cap.isOpened():
      ret, frame = cap.read()
      if not ret:
          break
      cv2.imwrite(f"frame_{frame_number}.jpg", frame)
      frame_number += 1
      print(f"Frame {frame_number} saved.")
  cap.release()
</code></pre>
    <p class="normal">We have split the video into<a id="_idIndexMarker626"/> frames and saved them as JPEG images with their respective frame number, <code class="inlineCode">frame_number</code>. The Generator’s job finishes here and the Commentator now takes over.</p>
    <h3 id="_idParaDest-254" class="heading-3">Step 3. Commenting on the frames</h3>
    <p class="normal">The Generator has gone from<a id="_idIndexMarker627"/> text-to-video to splitting the video and saving the frames as JPEG frames. The Commentator now takes over to comment on the frames with three functions:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">generate_openai_comments(filename)</code> asks the GPT-4 series vision model to analyze a frame and produce a response that contains a comment describing the frame</li>
      <li class="bulletList"><code class="inlineCode">generate_comment(response_data)</code> extracts the comment from the response</li>
      <li class="bulletList"><code class="inlineCode">save_comment(comment, frame_number, file_name)</code> saves the comment</li>
    </ul>
    <p class="normal">We need to build the Commentator’s extraction function first:</p>
    <pre class="programlisting code"><code class="hljs-code">def generate_comment(response_data):
    """Extract relevant information from GPT-4 Vision response."""
    try:
        caption = response_data.choices[0].message.content
        return caption
    except (KeyError, AttributeError):
        print("Error extracting caption from response.")
        return "No caption available."
</code></pre>
    <p class="normal">We then write a function to save the extracted comment in a CSV file that bears the same name as the video file:</p>
    <pre class="programlisting code"><code class="hljs-code">def save_comment(comment, frame_number, file_name):
    """Save the comment to a text file formatted for seamless loading into a pandas DataFrame."""
    # Append .csv to the provided file name to create the complete file name
    path = f"{file_name}.csv"
    # Check if the file exists to determine if we need to write headers
    write_header = not os.path.exists(path)
    with open(path, 'a', newline='') as f:
        writer = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        if write_header:
            writer.writerow(['ID', 'FrameNumber', 'Comment', 'FileName'])  # Write the header if the file is being created
        # Generate a unique UUID for each comment
        unique_id = str(uuid.uuid4())
        # Write the data
        writer.writerow([unique_id, frame_number, comment, file_name])
</code></pre>
    <p class="normal">The goal is to save<a id="_idIndexMarker628"/> the comment in a format that can directly be upserted to Pinecone:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ID</code>: A unique string ID generated with <code class="inlineCode">str(uuid.uuid4())</code></li>
      <li class="bulletList"><code class="inlineCode">FrameNumber</code>: The frame number of the commented JPEG</li>
      <li class="bulletList"><code class="inlineCode">Comment</code>: The comment generated by the OpenAI vision model</li>
      <li class="bulletList"><code class="inlineCode">FileName</code>: The name of the video file</li>
    </ul>
    <p class="normal">The Commentator’s main function is to generate comments with the OpenAI vision model. However, in this program’s scenario, we will not save all the frames but a sample of the frames. The program first determines the number of frames to process:</p>
    <pre class="programlisting code"><code class="hljs-code">def generate_openai_comments(filename):
  video_folder = "/content"  # Folder containing your image frames
  total_frames = len([file for file in os.listdir(video_folder) if file.endswith('.jpg')]
</code></pre>
    <p class="normal">Then, a sample frequency is set that can be modified along with a counter:</p>
    <pre class="programlisting code"><code class="hljs-code">  nb=3      # sample frequency
  counter=0 # sample frequency counter
</code></pre>
    <p class="normal">The Commentator will then go through the sampled frames and request a comment:</p>
    <pre class="programlisting code"><code class="hljs-code">  for frame_number in range(total_frames):
      counter+=1 # sampler
      if counter==nb and counter&lt;total_frames:
        counter=0
        print(f"Analyzing frame {frame_number}...")
        image_path = os.path.join(video_folder, f"frame_{frame_number}.jpg")
        try:
            with open(image_path, "rb") as image_file:
                image_data = image_file.read()
                response = openai.ChatCompletion.create(
                    model="gpt-4-vision-preview",
</code></pre>
    <p class="normal">The message is very<a id="_idIndexMarker629"/> concise: <code class="inlineCode">"What is happening in this image?"</code> The message also includes the image of the frame:</p>
    <pre class="programlisting code"><code class="hljs-code">                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": "What is happening in this image?"},
                                {
                                    "type": "image",
                                    "image_url": f"data:image/jpeg;base64,{base64.b64encode(image_data).decode('utf-8')}"
                                },
                            ],
                       }
                    ],
                    max_tokens=150,
               )
</code></pre>
    <p class="normal">Once a response is returned, the <code class="inlineCode">generate_comment</code> and <code class="inlineCode">save_comment</code> functions are called to extract and save the comment, respectively:</p>
    <pre class="programlisting code"><code class="hljs-code">            comment = generate_comment(response)
            save_comment(comment, frame_number,file_name)
        except FileNotFoundError:
            print(f"Error: Frame {frame_number} not found.")
        except Exception as e:
            print(f"Unexpected error: {e}")
</code></pre>
    <p class="normal">The final function we require of the Commentator is to display the comments by loading the CSV file produced<a id="_idIndexMarker630"/> in a pandas DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code"># Read the video comments file into a pandas DataFrame
def display_comments(file_name):
  # Append .csv to the provided file name to create the complete file name
  path = f"{file_name}.csv"
  df = pd.read_csv(path)
  return df
</code></pre>
    <p class="normal">The function returns the DataFrame with the comments. An administrator controls <em class="italic">Pipeline 1</em>, the Generator, and the Commentator.</p>
    <h3 id="_idParaDest-255" class="heading-3">Pipeline 1 controller</h3>
    <p class="normal">The controller runs jobs for the preceding<a id="_idIndexMarker631"/> three steps of the Generator and the Commentator. It begins with <em class="italic">Step 1</em>, which includes selecting a video, downloading it, and displaying it. In an automated pipeline, these functions can be separated. For example, a script would iterate through a list of videos, automatically select each one, and encapsulate the controller <a id="_idIndexMarker632"/>functions. In this case, in a pre-production and educational context, we will collect, download, and display the videos one by one:</p>
    <pre class="programlisting code"><code class="hljs-code">session_time = time.time()  # Start timing before the request
# Step 1: Displaying the video
# select file
print("Step 1: Collecting video")
file_name = "skiing1.mp4" # Enter the name of the video file to process here
print(f"Video: {file_name}")
# Downloading video
print("Step 1:downloading from GitHub")
directory = "Chapter10/videos"
download(directory,file_name)
# Displaying video
print("Step 1:displaying video")
display_video(file_name)
</code></pre>
    <p class="normal">The controller then splits the video into frames and comments on the frames of the video:</p>
    <pre class="programlisting code"><code class="hljs-code"># Step 2.Splitting video
print("Step 2: Splitting the video into frames")
split_file(file_name)
</code></pre>
    <p class="normal">The controller activates the Generator to produce<a id="_idIndexMarker633"/> comments on frames of the video:</p>
    <pre class="programlisting code"><code class="hljs-code"># Step 3.Commenting on the video frames
print("Step 3: Commenting on the frames")
start_time = time.time()  # Start timing before the request
generate_openai_comments(file_name)
response_time = time.time() - session_time  # Measure response time
</code></pre>
    <p class="normal">The response time is measured as well. The controller then adds additional outputs to display the number of frames, the comments, the content generation time, and the total controller processing times:</p>
    <pre class="programlisting code"><code class="hljs-code"># number of frames
video_folder = "/content"  # Folder containing your image frames
total_frames = len([file for file in os.listdir(video_folder) if file.endswith('.jpg')])
print(total_frames)
# Display comments
print("Commenting video: displaying comments")
display_comments(file_name)
total_time = time.time() - start_time  # Start timing before the request
print(f"Response Time: {response_time:.2f} seconds")  # Print response time
print(f"Total Time: {total_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The controller has completed its task of producing content. However, depending on your project, you can introduce dynamic RAG for some or all the videos. If you need this functionality, you can apply the process described in <em class="italic">Chapter 5</em>, <em class="italic">Boosting RAG Performance with Expert Human Feedback</em>, to the Commentator’s outputs, including the cosine similarity quality control metrics, as we will in the <em class="italic">Pipeline 3: The Video Expert</em> section of this chapter.</p>
    <p class="normal">The controller can also<a id="_idIndexMarker634"/> save the comments and frames.</p>
    <h4 class="heading-4">Saving comments</h4>
    <p class="normal">To save the<a id="_idIndexMarker635"/> comments, set <code class="inlineCode">save=True</code>. To save the frames, set <code class="inlineCode">save_frames=True</code>. Set both values to <code class="inlineCode">False</code> if you just want to run the program and view the outputs, but, in our case, we will set them as <code class="inlineCode">True</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"># Ensure the file exists and double checking before saving the comments
save=True        # double checking before saving the comments
save_frames=True # double checking before saving the frames
</code></pre>
    <p class="normal">The comment is saved in CSV format in <code class="inlineCode">cpath</code> and contains the file name with the <code class="inlineCode">.csv</code> extension and in the location of your choice. In this case, the files are saved on Google Drive (make sure the path exists):</p>
    <pre class="programlisting code"><code class="hljs-code"># Save comments
if save==True:  # double checking before saving the comments
  # Append .csv to the provided file name to create the complete file name
  cpath = f"{file_name}.csv"
  if os.path.exists(cpath):
      # Use the Python variable 'path' correctly in the shell command
      !cp {cpath} /content/drive/MyDrive/files/comments/{cpath}
      print(f"File {cpath} copied successfully.")
  else:
      print(f"No such file: {cpath}")
</code></pre>
    <p class="normal">The output confirms that a file is saved:</p>
    <pre class="programlisting con"><code class="hljs-con">File alpinist1.mp4.csv copied successfully.
</code></pre>
    <p class="normal">The frames are saved in a root name direction, for which we remove the extension with <code class="inlineCode">root_name = root_name + extension.strip('.')</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"># Save frames
import shutil
if save_frames==True:
  # Extract the root name by removing the extension
  root_name, extension = os.path.splitext(file_name)
  # This removes the period from the extension
  root_name = root_name + extension.strip('.')
  # Path where you want to copy the jpg files
  target_directory = f'/content/drive/MyDrive/files/comments/{root_name}'
  # Ensure the directory exists
  os.makedirs(target_directory, exist_ok=True)
  # Assume your jpg files are in the current directory. Modify this as needed
  source_directory = os.getcwd()  # or specify a different directory
  # List all jpg files in the source directory
  for file in os.listdir(source_directory):
      if file.endswith('.jpg'):
        shutil.copy(os.path.join(source_directory, file), target_directory)
</code></pre>
    <p class="normal">The output is a<a id="_idIndexMarker636"/> directory with all the frames generated in it. We should delete the files if the controller runs in a loop over all the videos in a single session.</p>
    <h4 class="heading-4">Deleting files</h4>
    <p class="normal">To delete<a id="_idIndexMarker637"/> the files, just set <code class="inlineCode">delf=True</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">delf=False  # double checking before deleting the files in a session
if delf==True:
  !rm -f *.mp4 # video files
  !rm -f *.jpg # frames
  !rm -f *.csv # comments
</code></pre>
    <p class="normal">You can now process an unlimited number of videos one by one and scale to whatever size you wish, as long as you have disk space and a CPU!</p>
    <h1 id="_idParaDest-256" class="heading-1">Pipeline 2: The Vector Store Administrator</h1>
    <p class="normal">The Vector Store Administrator AI agent <a id="_idIndexMarker638"/>performs the tasks we implemented in <em class="italic">Chapter 6</em>, <em class="italic">Scaling RAG Bank Customer Data with Pinecone. </em>The novelty in this section relies on the fact that all the data we upsert for RAG is AI-generated. Let’s open <code class="inlineCode">Pipeline_2_The_Vector_Store_Administrator.ipynb</code> in the GitHub repository. We will build the Vector Store Administrator on top of the Generator and the Commentator AI agents in four steps, as illustrated in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_10_09.png" alt="A diagram of a video expert  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.7: Workflow of the Vector Store Administrator from processing to querying video frame comments</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Processing the video comments</strong>: The<a id="_idIndexMarker639"/> Vector Store Administrator will load and prepare the comments for chunking as in the <em class="italic">Pipeline 2: Scaling a Pinecone Index (vector store)</em> section of <em class="italic">Chapter 6</em>. Since we are processing one video at a time in a pipeline, the system deletes the files processed, which keeps disk space constant. You can enhance the functionality and scale this process indefinitely.</li>
      <li class="numberedList"><strong class="keyWord">Chunking and embedding the dataset</strong>: The column names <code class="inlineCode">('ID', 'FrameNumber', 'Comment', 'FileName')</code> of the dataset have already been prepared by the Commentator AI agent in <em class="italic">Pipeline 1</em>. The program chunks and embeds the dataset using the <a id="_idIndexMarker640"/>same functionality as in <em class="italic">Chapter 6</em> in the <em class="italic">Chunking and embedding the dataset</em> section.</li>
      <li class="numberedList"><strong class="keyWord">The Pinecone index</strong>: The Pinecone Index is created, and the data is upserted as in the <em class="italic">Creating the Pinecone Index</em> and <em class="italic">Upserting</em> sections of <em class="chapterRef">Chapter </em>6.</li>
      <li class="numberedList"><strong class="keyWord">Querying the vector store after upserting the dataset</strong>: This follows the same process as in <em class="chapterRef">Chapter 6</em>. However, in this case, the retrieval is hybrid, using both the Pinecone vector store and a separate file system to store videos and video frames.</li>
    </ol>
    <p class="normal">Go through <em class="italic">Steps 1</em> to <em class="italic">3</em> in the notebook to<a id="_idIndexMarker641"/> examine the Vector Store Administrator’s functions. After <em class="italic">Step 3</em>, the Pinecone index is ready for hybrid querying.</p>
    <h2 id="_idParaDest-257" class="heading-2">Querying the Pinecone index</h2>
    <p class="normal">In the notebook on GitHub, <em class="italic">Step 4: Querying the Pinecone index</em> implements functions to find a comment that matches user input and trace it to the frame of a video. This leads to the video source and frame, which can<a id="_idIndexMarker642"/> be displayed. We can display the videos and frames from the location we wish. This hybrid approach thus involves <a id="_idIndexMarker643"/>querying the Pinecone Index to retrieve information and also retrieve media files from another location.</p>
    <p class="normal">We saw that a vector store can contain images that are queried, as implemented in <em class="italic">Chapter 4</em>, <em class="italic">Multimodal Modular RAG for Drone Technology</em>. In this chapter, the video production use case videos and frame files are stored separately. In this case, it is in the GitHub repository. In production, the video and frame files can be retrieved from any storage system we need, which may or may not prove to be more cost-effective than storing data on Pinecone. The decision to store images in a vector store or a separate location will depend on the project’s needs.</p>
    <p class="normal">We begin by defining the number of top-k results we wish to process:</p>
    <pre class="programlisting code"><code class="hljs-code">k=1 # number of results
</code></pre>
    <p class="normal">We then design a rather difficult prompt:</p>
    <pre class="programlisting code"><code class="hljs-code">query_text = "Find a basketball player that is scoring with a dunk."
</code></pre>
    <p class="normal">Only a handful of frames in the whole video dataset contain an image of a basketball player jumping to score a slam dunk. Can our system find it? Let’s find out.</p>
    <p class="normal">We first embed our query to match the format of the data in the vector store:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
# Start timing before the request
start_time = time.time()
# Target vector
#query_text = "Find a basketball player."
query_embedding = get_embedding(query_text, model=embedding_model)
</code></pre>
    <p class="normal">Then we run a similarity vector <a id="_idIndexMarker644"/>search between the query and the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"># Perform the query using the embedding
query_results = index.query(vector=query_embedding, top_k=k, include_metadata=True)  # Request metadata
# Print the query results along with metadata
print("Query Results:")
for match in query_results['matches']:
    print(f"ID: {match['id']}, Score: {match['score']}")
    # Check if metadata is available
    if 'metadata' in match:
        metadata = match['metadata']
        text = metadata.get('text', "No text metadata available.")
        frame_number = metadata.get('frame_number', "No frame number available.")
        file_name = metadata.get('file_name', "No file name available.")
</code></pre>
    <p class="normal">Finally, we display the<a id="_idIndexMarker645"/> content of the response and the response time:</p>
    <pre class="programlisting code"><code class="hljs-code">        print(f"Text: {text}")
        print(f"Frame Number: {frame_number}")
        print(f"File Name: {file_name}")
    else:
        print("No metadata available.")
# Measure response time
response_time = time.time() - start_time
print(f"Querying response time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The output contains the ID of the comment retrieved and its score:</p>
    <pre class="programlisting con"><code class="hljs-con">Query Results:
ID: f104138b-0be8-4f4c-bf99-86d0eb34f7ee, Score: 0.866656184
</code></pre>
    <p class="normal">The output also contains the comment generated by the OpenAI LLM (the Commentator agent):</p>
    <pre class="programlisting con"><code class="hljs-con">Text: In this image, there is a person who appears to be in the process of executing a dunk in basketball. The individual is airborne, with one arm extended upwards towards the basketball hoop, holding a basketball in hand, preparing to slam it through the hoop. The word "dunk" is superimposed on the image, confirming the action taking place. The background shows clear skies and a modern building, suggesting this might be an outdoor basketball court in an urban setting. The player is wearing athletic wear and a pair of basketball shoes, suitable for the sport. The dynamic posture and the context indicate an athletic and powerful movement, typical of a basketball dunk.
</code></pre>
    <p class="normal">The final output contains the<a id="_idIndexMarker646"/> frame number that was commented, the video file of the <a id="_idIndexMarker647"/>frame, and the retrieval time:</p>
    <pre class="programlisting con"><code class="hljs-con">Frame Number: 191
File Name: basketball3.mp4
Querying response time: 0.57 seconds
</code></pre>
    <p class="normal">We can display the video by downloading it based on the file name:</p>
    <pre class="programlisting code"><code class="hljs-code">print(file_name)
# downloading file from GitHub
directory = "Chapter10/videos"
filename = file_name
download(directory,file_name)
</code></pre>
    <p class="normal">Then, use a standard Python function to display it:</p>
    <pre class="programlisting code"><code class="hljs-code"># Open the file in binary mode
def display_video(file_name):
  with open(file_name, 'rb') as file:
      video_data = file.read()
  # Encode the video file as base64
  video_url = b64encode(video_data).decode()
  # Create an HTML string with the embedded video
  html = f'''
  &lt;video width="640" height="480" controls&gt;
    &lt;source src="img/mp4;base64,{video_url}" type="video/mp4"&gt;
  Your browser does not support the video tag.
  &lt;/video&gt;
  '''
  # Display the video
  HTML(html)
  # Return the HTML object
  return HTML(html)
display_video(file_name)
</code></pre>
    <p class="normal">The video containing a <a id="_idIndexMarker648"/>basketball player performing a dunk is displayed:</p>
    <figure class="mediaobject"><img src="img/B31169_10_10.png" alt="A basketball hoop with net  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.8: Video output</p>
    <p class="normal">We can take this further <a id="_idIndexMarker649"/>with more precision by displaying the frame of the comment retrieved:</p>
    <pre class="programlisting code"><code class="hljs-code">file_name_root = file_name.split('.')[0]
…
from IPython.display import Image, display
# Specify the directory and file name
directory = '/content/'  # Adjust the directory if needed
file_path = os.path.join(directory, frame)
# Check if the file exists and verify its size
if os.path.exists(file_path):
    file_size = os.path.getsize(file_path)
    print(f"File '{frame}' exists. Size: {file_size} bytes.")
    # Define a logical size value in bytes, for example, 1000 bytes
    logical_size = 1000  # You can adjust this threshold as needed
    if file_size &gt; logical_size:
        print("The file size is greater than the logical value.")
        display(Image(filename=file_path))
    else:
        print("The file size is less than or equal to the logical value.")
else:
    print(f"File '{frame}' does not exist in the specified directory.")
</code></pre>
    <p class="normal">The output shows the exact frame that corresponds to the user input:</p>
    <figure class="mediaobject"><img src="img/B31169_10_11.png" alt="A person holding a ball  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.9: Video frame corresponding to our input</p>
    <div><p class="normal">Only the frames of <code class="inlineCode">basketball3.mp4</code> were saved in the GitHub repository for disk space reasons for this program. In production, all the frames you decide you need can be stored and retrieved.</p>
    </div>
    <p class="normal">The team of AI agents in <a id="_idIndexMarker650"/>this chapter worked together to generate videos (the Generator), comment on the video frames (the Commentator), upsert embedded comments in the vector store (the Vector Store Administrator), and prepare the retrieval process (the Vector Store Administrator). We also saw that the retrieval process already<a id="_idIndexMarker651"/> contained augmented input and output thanks to the OpenAI LLM (the Commentator) that generated natural language comments. The process that led to this point will definitely be applied in many domains: firefighting, medical imagery, marketing, and more.</p>
    <p class="normal">What more can we expect from this system? The Video Expert AI agent will answer that.</p>
    <h1 id="_idParaDest-258" class="heading-1">Pipeline 3: The Video Expert</h1>
    <p class="normal">The role of the OpenAI GPT-4o Video Expert is to analyze the comment made by the Commentator OpenAI LLM agent, point out the <a id="_idIndexMarker652"/>cognitive dissonances (things that don’t seem to fit together in the description), rewrite the comment, and provide a label. The workflow of the Video Expert, as illustrated in the following figure, also includes the code of the <em class="italic">Metrics calculations and display </em>section of <em class="italic">Chapter 7</em>, <em class="italic">Building Scalable Knowledge-Graph-Based RAG with Wikipedia API and LlamaIndex</em>.</p>
    <p class="normal">The Commentator’s role was only to describe what it saw. The Video Expert is there to make sure it makes sense and also label the videos so they can be classified in the dataset for further use.</p>
    <figure class="mediaobject"><img src="img/B31169_10_12.png" alt="A diagram of a video expert  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.10: Workflow of the Video Expert for automated dynamics descriptions and labeling</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">The Pinecone index</strong> will connect to the Pinecone index as described in the <em class="italic">Pipeline 2. The Vector Store Administrator</em> section of this chapter. This time, we will not upsert data but connect to the vector store.</li>
      <li class="numberedList"><strong class="keyWord">Define the RAG functions</strong> utilizing the straightforward functions we built in <em class="italic">Pipeline 1</em> and <em class="italic">Pipeline 2</em> of this chapter.</li>
      <li class="numberedList"><strong class="keyWord">Querying the vector store</strong> is nothing but querying the Pinecone Index as described in <em class="italic">Pipeline 2</em> of this chapter.</li>
      <li class="numberedList"><strong class="keyWord">Retrieval augmented generation</strong> finally determines the main role of Video Expert GPT-4o, which is to analyze and improve the vector store query responses. This final step will include evaluation and metric functions.</li>
    </ol>
    <p class="normal">There are as many strategies as projects to implement the video production use case we explored in this chapter, but the Video Expert plays an important role. Open <code class="inlineCode">Pipeline_3_The_Video_Expert.ipynb</code> on GitHub and go to the <em class="italic">Augmented Retrieval Generation</em> section in <em class="italic">Step 2: Defining the RAG functions</em>.</p>
    <p class="normal">The function makes an OpenAI GPT-4o <a id="_idIndexMarker653"/>call, like for the Commentator in <em class="italic">Pipeline 1</em>. However, this time, the role of the LLM is quite different:</p>
    <pre class="programlisting code"><code class="hljs-code">             "role": "system",
                "content": "You will be provided with comments of an image frame taken from a video. Analyze the text and 1. Point out the cognitive dissonances 2. Rewrite the comment in a logical engaging style. 3. Provide a label for this image such as Label: basketball, football, soccer or other label."
</code></pre>
    <p class="normal">The instructions for GPT-4o are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">You will be provided with comments of an image frame taken from a video</code>: This instructs the LLM to analyze the AI-generated comments. The Commentator had to remain neutral and describe the frame as it saw it. The role of the Video Expert agent is different: it has to analyze and enhance the comment.</li>
      <li class="bulletList"><code class="inlineCode">1. Point out the cognitive dissonances</code>: This instructs the model to find contradictions or discrepancies in the comment that can come from the way the AI-generated video was produced as well (lack of logic in the video).</li>
      <li class="bulletList"><code class="inlineCode">2. Rewrite the comment in a logical engaging style</code>: This instructs the Video Expert agent to rewrite the comment going from a technical comment to a description.</li>
      <li class="bulletList"><code class="inlineCode">3. Provide a label for this image such as Label: basketball, football, soccer or other label</code>: This instructs the model to provide a label for further use. On GitHub, <em class="italic">Step 3: Querying the Vector Store</em> reproduces the query and output described in <em class="italic">Pipeline 2</em> for a basketball player scoring with a dunk, with the corresponding video and frame. The output is:
        <pre class="programlisting con-one"><code class="hljs-con">ID=f104138b-0be8-4f4c-bf99-86d0eb34f7ee
score=0.866193652
text=In this image, there is a person who appears to be in the process of executing a dunk in basketball. The individual is airborne, with one arm extended upwards towards the basketball hoop, holding a basketball in hand, preparing to slam it through the hoop. The word "dunk" is superimposed on the image, confirming the action taking place. The background shows clear skies and a modern building, suggesting this might be an outdoor basketball court in an urban setting. The player is wearing athletic wear and a pair of basketball shoes, suitable for the sport. The dynamic posture and the context indicate an athletic and powerful movement, typical of a basketball dunk.
frame_number=191
file_name=basketball3.mp4
</code></pre>
      </li>
    </ul>
    <p class="normal">The comment provided seems<a id="_idIndexMarker654"/> acceptable. However, let’s see what GPT-4o thinks of it. The <em class="italic">Step 4: Retrieval Augmented Generation</em> section on GitHub takes the output and submits it as the user prompt to the Video Expert agent:</p>
    <pre class="programlisting code"><code class="hljs-code">prompt=text
</code></pre>
    <p class="normal">We then call the Video Expert agent to obtain its expertise:</p>
    <pre class="programlisting code"><code class="hljs-code">response_content = get_openai_response(prompt)
print(response_content)
</code></pre>
    <p class="normal">The output provides the Video Expert’s insights:</p>
    <pre class="programlisting con"><code class="hljs-con">1. Cognitive Dissonances:
   - The comment redundantly describes the action of dunking multiple times.
   - The mention of "the word 'dunk' is superimposed on the image" is unnecessary and breaks the flow of the description.
   - The background details about clear skies and a modern building are somewhat extraneous and could be more succinctly integrated.
2. Rewritten Comment:
   In this image, a basketball player is captured mid-air, executing a powerful dunk. With one arm extended towards the hoop and the basketball firmly in hand, the athlete's dynamic posture showcases the intensity of the moment. The outdoor court, set against a backdrop of clear skies and a modern building, highlights the urban setting. Dressed in athletic wear and basketball shoes, the player epitomizes the energy and skill of the sport.
3. Label: Basketball
</code></pre>
    <p class="normal">The response is well-structured and acceptable. The output may vary from one run to another due to the stochastic “creative” nature of Generative AI agents.</p>
    <p class="normal">The <em class="italic">Evaluator</em> section that follows <em class="italic">Step 4</em> runs ten examples using the same process as the basketball request we just made. Each <a id="_idIndexMarker655"/>example thus contains:</p>
    <ul>
      <li class="bulletList">A user prompt</li>
      <li class="bulletList">The comment returned by the vector store query</li>
      <li class="bulletList">The enhanced comment made by the GPT-4o model</li>
    </ul>
    <p class="normal">Each example also contains the same evaluation process as in <em class="italic">Chapter 7, Building Scalable Knowledge-Graph-Based RAG with Wikipedia API and LlamaIndex, </em>in the<em class="italic"> Examples for metrics</em> section. However, in this case, the human evaluator suggests content instead of a score (0 to 1). The human content becomes the ground truth, the expected output.</p>
    <p class="normal">Before beginning the evaluation, the program creates scores to keep track of the original response made by the query.</p>
    <p class="normal">The human evaluator rewrites the output provided by the Video Expert:</p>
    <pre class="programlisting code"><code class="hljs-code"># Human feedback flashcard comment
text1 = "This image shows soccer players on a field dribbling and passing the ball."
</code></pre>
    <p class="normal">The content rewritten by the Video Expert is extracted from the response:</p>
    <pre class="programlisting code"><code class="hljs-code"># Extract rewritten comment
text2 = extract_rewritten_comment(response_content)
</code></pre>
    <p class="normal">The human comment (ground truth, the reference output) and the LLM comments are displayed:</p>
    <pre class="programlisting code"><code class="hljs-code">print(f"Human Feedback Comment: {text1}")
print(f"Rewritten Comment: {text2}")
</code></pre>
    <p class="normal">Then, the cosine similarity score between the human and LLM comments is calculated and appended to <code class="inlineCode">scores</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">similarity_score3=calculate_cosine_similarity_with_embeddings(text1, text2)
print(f"Cosine Similarity Score with sentence transformer: {similarity_score3:.3f}")
scores.append(similarity_score3)
</code></pre>
    <p class="normal">The original score provided with the query is appended to the query’s retrieval score, <code class="inlineCode">rscores</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">rscores.append(score)
</code></pre>
    <p class="normal">The output displays the human<a id="_idIndexMarker656"/> feedback, the comment rewritten by GPT-4o (the Video Expert), and the similarity score:</p>
    <pre class="programlisting con"><code class="hljs-con">Human Feedback Comment: This image shows soccer players on a field dribbling and passing the ball.
Rewritten Comment: "A group of people are engaged in a casual game of soccer on a grassy field. One player is dribbling the ball while others are either defending or waiting for a pass. They are dressed in athletic attire, indicating this is a recreational game among friends or acquaintances. Interestingly, there is a superimposed text 'female' that seems unrelated to the activity shown in the image."
Cosine Similarity Score with sentence transformer: 0.621
</code></pre>
    <p class="normal">This program contains ten examples, but we can enter a corpus of as many examples as we wish to evaluate the system. The evaluation of each example applies the same choice of metrics as in <em class="chapterRef">Chapter 7</em><em class="italic">.</em> After the examples have been evaluated, the <em class="italic">Metrics calculations and display</em> section in the program also runs the metric calculations defined in the section of the same name in <em class="chapterRef">Chapter 7</em>.</p>
    <p class="normal">We can use all the metrics to analyze the performance of the system. The time measurements throughout the program also provide insights. The first metric, accuracy, is a good metric to start with. In this case, it shows that there is room for progress:</p>
    <pre class="programlisting con"><code class="hljs-con">Mean: 0.65
</code></pre>
    <p class="normal">Some requests and responses were challenging and required further work to improve the system:</p>
    <ul>
      <li class="bulletList">Checking the quality of the videos and their content</li>
      <li class="bulletList">Checking the comments and possibly modifying them with human feedback, as we did in <em class="chapterRef">Chapter 5</em>, <em class="italic">Boosting RAG Performance with Expert Human Feedback</em></li>
      <li class="bulletList">Fine-tuning a model with images and text as we did in <em class="chapterRef">Chapter 9</em>, <em class="italic">Empowering AI Models: Fine-Tuning RAG Data and Human Feedback</em></li>
      <li class="bulletList">Designing any other constructive idea that the video production team comes up with</li>
    </ul>
    <p class="normal">We can see that RAG-driven<a id="_idIndexMarker657"/> Generative AI systems in production are very effective. However, the road from design to production requires hard human effort! Though AI technology has made tremendous progress, it still requires humans to design, develop, and implement it in production.</p>
    <h1 id="_idParaDest-259" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we explored the hybrid era of human and AI agents, focusing on the creation of a streamlined process for generating, commenting, and labeling videos. By integrating cutting-edge Generative AI models, we demonstrated how to build an automated pipeline that transforms raw video inputs into structured, informative, and accessible video content.</p>
    <p class="normal">Our journey began with the <strong class="keyWord">Generator</strong> agent in <em class="italic">Pipeline 1</em>:<em class="italic"> The Generator and the Commentator</em>, which was tasked with creating video content from textual ideas. We can see that video generation processes will continue to expand through seamless integration ideation and descriptive augmentation generative agents. In <em class="italic">Pipeline 2</em>:<em class="italic"> The Vector Store Administrator</em>, we focused on organizing and embedding the generated comments and metadata into a searchable vector store. In this pipeline, we highlighted the optimization process of building a scalable video content library with minimal machine resources using only a CPU and no GPU. Finally, in <em class="italic">Pipeline 3</em>:<em class="italic"> The Video Expert</em>, we introduced the Expert AI agent, a video specialist designed to enhance and label the video content based on user inputs. We also implemented evaluation methods and metric calculations.</p>
    <p class="normal">By the end of this chapter, we had constructed a comprehensive, automated RAG-driven Generative AI system capable of generating, commenting on, and labeling videos with minimal human intervention. This journey demonstrated the power and potential of combining multiple AI agents and models to create an efficient pipeline for video content creation.</p>
    <p class="normal">The techniques and tools we explored can revolutionize various industries by automating repetitive tasks, enhancing content quality, and making information retrieval more efficient. This chapter not only provided a detailed technical roadmap but also underscored the transformative impact of AI in modern content creation and management. You are now all set to implement RAG-driven Generative AI in real-life projects.</p>
    <h1 id="_idParaDest-260" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with yes or no:</p>
    <ol>
      <li class="numberedList" value="1">Can AI now automatically comment and label videos?</li>
      <li class="numberedList">Does video processing involve splitting the video into frames?</li>
      <li class="numberedList">Can the programs in this chapter create a 200-minute movie?</li>
      <li class="numberedList">Do the programs in this chapter require a GPU?</li>
      <li class="numberedList">Are the embedded vectors of the video content stored on disk?</li>
      <li class="numberedList">Do the scripts involve querying a database for retrieving data?</li>
      <li class="numberedList">Is there functionality for displaying images in the scripts?</li>
      <li class="numberedList">Is it useful to have functions that specifically check file existence and size in any of the scripts?</li>
      <li class="numberedList">Is there a focus on multimodal data in these scripts?</li>
      <li class="numberedList">Do any of the scripts mention applications of AI in real-world scenarios?</li>
    </ol>
    <h1 id="_idParaDest-261" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Sora video generation model information and access:<ul>
          <li class="bulletList level-2"><strong class="keyWord">Sora</strong> | <strong class="keyWord">OpenAI</strong>: <a href="https://ai.invideo.io/">https://ai.invideo.io/</a></li>
          <li class="bulletList level-2"><a href="https://openai.com/index/video-generation-models-as-world-simulators/">https://openai.com/index/video-generation-models-as-world-simulators/</a></li>
        </ul>
      </li>
      <li class="bulletList"><em class="italic">Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</em> by Yixin Liu, Kai Zhang, Yuan Li, et al.: <a href="https://arxiv.org/pdf/2402.17177">https://arxiv.org/pdf/2402.17177</a></li>
    </ul>
    <h1 id="_idParaDest-262" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">OpenAI, ChatGPT: <a href="https://openai.com/chatgpt/">https://openai.com/chatgpt/</a></li>
      <li class="bulletList">OpenAI, Research: <a href="https://openai.com/research/">https://openai.com/research/</a></li>
      <li class="bulletList">Pinecone: <a href="https://docs.pinecone.io/home">https://docs.pinecone.io/home</a></li>
    </ul>
    <h1 id="_idParaDest-263" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>