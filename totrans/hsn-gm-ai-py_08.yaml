- en: Going Deep with DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to **deep learning** (**DL**) in order
    to handle newer, more challenging infinite **Markov decision process** (**MDP**)
    problems. We will cover some basics about DL that are relevant to **reinforcement
    learning** (**RL**), and then look at how we can solve a Q-learning. After that,
    we will look at how to build a Deep Q-learning or DQN agent in order to solve
    some Gym environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the topics we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: DL for RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PyTorch for DL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building neural networks with PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding DQN in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercising DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we introduce DL with respect to RL. Applying DL to **deep reinforcement
    learning** (**DRL**) is quite specific and is not covered in detail here.
  prefs: []
  type: TYPE_NORMAL
- en: DL for RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of the previous five chapters, we learned how to evaluate the
    value of state and actions for a given finite MDP. We learned how to solve various
    finite MDP problems using methods from MC, DP, Q-learning, and SARSA. Then we
    explored infinite MDP or continuous observation/action space problems, and we
    discovered this class of problems introduced computational limits that can only
    be overcome by introducing other methods, and this is where DL comes in.
  prefs: []
  type: TYPE_NORMAL
- en: DL is so popular and accessible now that we have decided to cover only a very
    broad overview of the topic in this book. Anyone serious about building DRL agents
    should look at studying DL further on their own.
  prefs: []
  type: TYPE_NORMAL
- en: For many, DL is about image classification, speech recognition, or that new
    cool thing called a **generative adversarial network** (**GAN**). Now, these are
    all great applications of DL, but, fundamentally, DL is about learning to minimize
    loss or errors. So when an image is shown to a network in order to learn the image,
    it is first split up and then fed into the network. Then the network spits out
    an answer. The correctness of the answer is determined, and any error is pushed
    back into the network as a way of learning. This method of pushing errors back
    through the network is called **backpropagation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basics of this whole system are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cc1fa9d-6928-4023-9c50-c0ce4829f2b0.png)'
  prefs: []
  type: TYPE_IMG
- en: DL oversimplified
  prefs: []
  type: TYPE_NORMAL
- en: The DL network learns by backpropagating the errors back through the network
    as corrections to each cell, called a neuron, and internal to the neuron are parameters
    called weights. Each weight controls the strength of a connection to that neuron.
    The strength of each connection or weight in the network is modified using an
    optimization method based on the gradient descent. **Gradient descent** (**GD**)
    is a method derived from calculus that allows us to calculate the effect each
    connection/weight has on the answer. Working back, we can, therefore, use GD to
    determine the amount of correction each weight needs.
  prefs: []
  type: TYPE_NORMAL
- en: The major downside to using backpropagation with GD is that the training or
    learning needs to happen very slowly. Thus, many, perhaps thousands or millions
    of, images need to be shown to the network in order for it to learn. This actually
    works well when we apply DL to RL since our trial-and-error learning methods also
    work iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind and other companies are currently working on other methods of learning,
    aside from backpropagation for DL networks. There has even been talk of doing
    one-shot learning. That is, being able to train a network on just a single image.
    Much like the way we humans can learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it is often said that DL can be interpreted in many ways. As RL practitioners
    our interest in DL will be its use as an equation solver. You see fundamentally
    that is all DL does: solve equations; whether they are equations for classifying
    images, performing speech translation, or for RL. Deep reinforcement learning
    is about applying DL in order to solve the learning equations we looked at in
    the previous chapters and more. In fact, the addition of DL also provides many
    further capabilities in learning that we will explore in the rest of this book.
    In the next section, we cover a broad overview of the common DL frameworks used
    for DRL.'
  prefs: []
  type: TYPE_NORMAL
- en: DL frameworks for DRL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of DL frameworks available for use, but only a few have been
    readily used in RL research or projects. Most of these frameworks share a number
    of similarities, so transferring knowledge from one to the other is relatively
    straightforward. The three most popular frameworks for RL in the past few years
    have been Keras, TensorFlow, and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of the strengths and weaknesses of each framework is shown in the
    table here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Framework** | **Keras** | **TensorFlow** | **PyTorch** |'
  prefs: []
  type: TYPE_TB
- en: '| **Accessibility** | Easiest to learn and use | Provides a high-level Keras
    interface and lower-level interface. | Medium to low-level interface |'
  prefs: []
  type: TYPE_TB
- en: '| **Scalability** | Scales well for smaller projects | Scales to any size project
    and supported output network models may be run on many different platforms. |
    Well suited to large projects requiring scale |'
  prefs: []
  type: TYPE_TB
- en: '| **Performance/Power** | Simple interface and limits customization | Powerful
    and great for performance | Excellent performance and provides the most control
    and additional interfaces for custom development |'
  prefs: []
  type: TYPE_TB
- en: '| **Popularity** | Popularity decreasing | Consistently popular framework and
    considered state of the art. | Popularity increasing especially for DRL applications
    |'
  prefs: []
  type: TYPE_TB
- en: If you review the table, the obvious choice for our DL framework will be PyTorch.
    PyTorch based on Torch is a relative newcomer but in just a few short years has
    gained incredible popularity as both a DL and DRL framework. Therefore, it will
    be our selected framework for this chapter. In the next section, we look at how
    to get started with PyTorch for DL.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorch for DL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch provides both a low- and medium-level interface to building DL networks/computational
    graphs. As much as we build DL systems as networks with neurons connected in layers,
    the actual implementation of a neural network is through a computational graph.
    Computational graphs reside at the heart of all DL frameworks, and TensorFlow
    is no exception. However, Keras abstracts away any concept of computational graphs
    from the user, which makes it easier to learn but does not provide flexibility
    like PyTorch. Before we begin building computational graphs with PyTorch though,
    let''s first install PyTorch in the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate your browser to [pytorch.org,](https://pytorch.org) and scroll down
    to the **Run this Command** section, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/dd08ecc6-e37c-460a-86f4-6ee4c5242cfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating a PyTorch installation command
  prefs: []
  type: TYPE_NORMAL
- en: Select the **Stable** version and then your specific **OS** (**Linux**, **Mac**,
    or **Windows**). Next select the **package** (**Conda**, **Pip**, **LibTorch**,
    or **Source**); our preference here is **Conda** for Anaconda, but if you have
    experience with others, use them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, choose the **language** (Python 2.7, Python 3.5, Python 3.7, or C++);
    for our purposes, we will use **Python 3.6**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next option **CUDA** (**9.2**, **10.0** or **None**) determines whether
    you have a **graphics processing unit** (**GPU**) that is capable of running **CUDA**.
    Currently, the only supported GPUs are built by NVIDIA. That's unlikely to change
    anytime soon. For our purposes, we will use **None**. **None** or CPU runs strictly
    on the CPU, which is slower, but will run across most devices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a 64-bit Python console under administrative rights. If you are using **Conda**,
    launch the window as an admin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new virtual environment with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the virtual environment using Python 3.6\. PyTorch currently runs
    on Windows 64-bit. This may differ according to the OS. Then activate the environment
    with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy and paste the `install` command that was generated previously into the
    window, and execute it. An example of the command for Windows running Anaconda
    is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This command should install PyTorch. If you saw any issues, such as errors claiming
    the libraries are not available for 32-bit, then make sure you are using a 64-bit
    version of Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding process will install PyTorch and all the required dependencies
    we will need for now. If you have issues installing the framework, check the online
    documentation or one of the many online help forums. In most cases, installation
    issues will be resolved by making sure you are using 64-bit and running as an
    administrator.
  prefs: []
  type: TYPE_NORMAL
- en: All the code examples for this book have been prepared and tested with Visual
    Studio Professional or Visual Studio Code, both with the Python tools installed.
    VS Code is a good solid editor that is free and cross-platform. It is a relative
    newcomer for Python development but benefits from Microsoft's years of experience
    building **integrated development environments** (**IDEs**).
  prefs: []
  type: TYPE_NORMAL
- en: With PyTorch installed, we can move onto working with a simple example that
    creates a computational DL graph in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Computational graphs with tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the core of all DL frameworks is the concept of a tensor or what we often
    think of as a multidimensional array or matrix. The computational graphs we construct
    will work on tensors using a variety of operations to linearly transform the inputs
    into final outputs. You can think of this as a kind of flow, and hence the reason
    TensorFlow has the name it does. In the following exercise, we are going to construct
    a two-layer DL network using a computation PyTorch graph and then train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: The concepts here assume an understanding of linear algebra and matrix multiplication
    and systems of linear equations. As such, it is recommended that any readers lacking
    in these skills or are in need of a quick refresher should do so. Of course, a
    quick refresher on calculus may also come in useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `Chapter_6_1.py` code example. The example was derived from a PyTorch
    quickstart manual, with some of the variable names altered to be more contextual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We start by importing the PyTorch library with `import torch`. Then we set
    our preferred data type `dtype` variable to `torch.float`. Then we initialize
    the device variable by using `torch.device` and passing in `cpu` to denote a CPU
    only. The option to enable the example to run with CUDA on a GPU was left in,
    but installing CUDA is left up to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up some variables to define how the data is processed and the
    architecture of the network. The `batch_size` parameter denotes how many items
    to train in an iteration. The `inputs` variable denotes the size of the input
    space into the network, whereas the `hidden` variable represents the number of
    hidden or middle-layer neurons in the network. The last `outputs` variable denotes
    the output space or the number of neurons in an output layer of a network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we set the inputs and outputs variables: `x` as the inputs, and
    `y` as the outputs, to be learned based on just a random sampling based on `batch_size`.
    The size of the `inputs` variable in this example is 1,000, so each element in
    the batch will have 1000 inputs for `x`. The outputs have a value of 10, so each
    sample of `y` will likewise have 10 items:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These two lines create our computational layers of a DL network defined by our
    previous `inputs`, `hidden`, and `outputs` parameters. The tensor contents of
    `layer1` and `layer2` at this point contain an initialized set of random weights
    the size of which is set by the number of inputs, hidden layers, and outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can visualize the size of these tensors by setting a breakpoint on the
    line after the layer setups and then running the file in debug mode *F5* on Visual
    Studio Code or Professional. When the breakpoint is hit, you can then use your
    mouse to hover over the variables to see information about the tensors as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c5d7d31e-0caf-4e29-81d1-dcb0ea66ad12.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the size of the layer weight tensors
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the first layer dimensions are 1000 x 100 and the second layer dimensions
    are 100 x 10\. Computationally, we transform the inputs by multiplying the weights
    of the first layer and then outputting the results to the second layer. Here,
    the second layer weights are multiplied by the output from the first layer. We
    will see how this functions shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we define a `learning_rate` parameter, or what we will clarify now as
    a hyperparameter. The learning rate is a multiplier by which we can scale the
    rate of learning and is not different than the learning rate alpha we previously
    explored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will often use the terms `weight` and `parameter` to mean the same in DL.
    As such, other parameters such as `learning_rate`, epochs, batch size, and so
    on will be described as hyperparameters. Learning to tune hyperparameters will
    be an ongoing journey in building DL examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get into the training loop, let''s run the sample and observe the
    output. Run the sample as you normally would, in debug mode or not. The output
    of this example is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/98f857ad-a2d4-4d7a-a3eb-a9efbf450b30.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of example Chapter_6_1.py
  prefs: []
  type: TYPE_NORMAL
- en: The output basically shows how the error loss decreases over training iterations.
    At iteration 99, we can see in the preceding example the error is around 635,
    but decreases down to almost zero by iteration 499\. While the inputs and outputs
    are all random, we can still see the network learns to identify a pattern in the
    data and thereby reduce errors. In the next section, we take a more detailed look
    at how this learning works.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network – computational graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to train a network or computational graph, we need to first feed it
    the input data, determine what the graph thinks is the answer, and then correct
    it iteratively using backpropagation. Let''s go back to the `Chapter_6_1.py` code
    example, and follow the next exercise to learn how training works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start at the beginning of the training loop that starts with the `for`
    loop, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So, 500 in this example denotes the total number of training iterations or epochs.
    In each iteration, we calculate the predicted output using the next three lines.
    This step is called the forward pass through the graph or network. Where the first
    line does the matrix multiplication of the `layer1` weights with the `x` inputs
    using `x.mm`. It then passes those output values through an activation function
    called **clamp**. Clamp sets limits on the output of the network, and in this
    case we use a clamp on 0\. This also happens to correspond with the rectified
    linear unit or ReLU function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use many different forms of activation functions in DL. The ReLU function
    is currently one of the more popular functions, but we will use others along the
    way throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: After the output is activated through the ReLU function, it is then matrix multiplied
    by the second layer weights, `layer2`. The output of this result is `y_pred`,
    a tensor containing the output predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From there we predict the loss or amount of error between what we want to actually
    predict in the `y` tensor and what our network just predicted as a tensor `y_pred`
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `loss` value or total error is calculated using a method called **mean squared
    error** or **MSE**. Keep in mind that since `y_pred` and `y` are tensors, the
    subtraction operation is done tensor-wide. That is, all values of the 10 predictions
    are subtracted from the predicted `y` value and then squared and summed. We use
    the same output technique here to print out the total loss for every 99 iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After computing the loss, we next need to compute the gradient of graph weights
    in order to determine how we push back and correct the errors in the graph. Calculating
    this gradient is outside the scope of this book, but the code is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We show the low-level code here to do GD against a simple network graph as
    an example of how the math works. Fortunately, automatic differentiation lets
    us for the most part ignore those finer, more painful details. The gradients calculated
    here now need to be applied back to the graph layer weights using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we are again using tensor subtraction to subtract the calculated
    gradients `grad_layer1` and `grad_layer2` scaled by the learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the sample again and you should see a similar output. It can be helpful
    to play with the `learning_rate` hyperparameter to see what effect this has on
    training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This previous example was a low-level look at how we can implement a computational
    graph that represents a two-layer neural network. While this example was meant
    to show you the inner details of how things work in practice, we will use the
    higher-level neural network subset of PyTorch to build graphs. We will see how
    to construct an example in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building neural networks with Torch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last section, we explored building computational graphs that resemble
    neural networks. This is a fairly common task as you may expect. So much so that
    PyTorch, as well as most DL frameworks, provides helper methods, classes, and
    functions to build DL graphs. Keras is essentially a wrapper around TensorFlow
    that does just that. Therefore, in this section, we are going to recreate the
    last exercise''s example using the neural network helper functions in PyTorch.
    Open the  `Chapter_6_2.py` code example and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code for the entire sample is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The code becomes greatly simplified, but not so much that it doesn''t allow
    us to control the internals of the DL graph itself. This is not something you
    may appreciate entirely until working with other DL frameworks. However, it is
    not the simplicity but the flexibility that is pushing PyTorch to be the number
    one framework in DL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a couple of big changes to the top section of the code, most notably
    with the setup of a model using `torch.nn.Sequential`. The setup of this model
    or graph is exactly the same as we did previously, except it describes each connection
    point more explicitly. We can see that the first layer is defined with `torch.nn.Linear`
    taking `inputs` and `hidden` as parameters. This gets connected to the activation
    function, again ReLU denoted by `torch.nn.ReLU`. After that, we create the final
    layer using `hidden` and `outputs` as the parameters. The `Sequential` term for
    the model denotes the whole graph is fully connected; the same as we looked at
    in the last example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After the model definition, we can also see our `loss_fn` loss function is
    more descriptive by using `torch.nn.MSELoss` as the function. This lets us know
    explicitly what the `loss` function is and how it is going to be reduced,  in
    this case, reducing the sum, denoted by `reduction=''sum''`, or the sum of average
    squared errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The start of the training loop remains the same but this time `y_pred` is taken
    from just inputting the entire `x` batch into the `model`. This operation is the
    same as the forward pass or where the network outputs the answer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we calculate `loss` as aTorch tensor, using the `loss_fn` function.
    The next piece of code is the same loss output code as we have seen before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we zero any gradients in the model—this is essentially a reset. Then
    we calculate the gradients in the loss tensor using the `backward` function. This
    is essentially that nasty bit of code we previously looked at, which has now been
    simplified to a single line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We finish off training the same way as before by adjusting the weights in the
    model using the calculated gradients of the `loss` tensor. While this section
    of code is more verbose than our last example, it explains better the actual learning
    process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the example just like you did previously, and you should see very similar
    output to what we saw in the `Chapter_6_1.py` example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Did you notice that the `learning_rate` variable in the second example was slightly
    lower?  The reason for this is because the neural network model differs slightly
    in a few areas, including the use of another weight for each neuron called a **bias**.
    If you want to learn more about the bias, be sure to pick up a good course on
    DL.
  prefs: []
  type: TYPE_NORMAL
- en: With a good basic understanding of how we can use PyTorch, we will now look
    and see how we can apply our knowledge to RL in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DQN in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep reinforcement learning became prominent because of the work of combining
    Q-learning with DL. The combination is known as deep Q-learning or **DQN** for
    **Deep Q Network**. This algorithm has powered some of the cutting edge examples
    of DRL, when Google DeepMind used it to make classic Atari games better than humans
    in 2012\. There are many implementations of this algorithm, and Google has even
    patented it. The current consensus is that Google patented such a base algorithm
    in order to thwart patent trolls striking at little guys or developers building
    commercial applications with DQN. It is unlikely that Google would exercise this
    legally or that it would have to since this algorithm is no longer considered
    state of the art.
  prefs: []
  type: TYPE_NORMAL
- en: Patent trolling is a practice whereby an often less-than-ethical company will
    patent any and all manner of inventions just for the sake of securing patents.
    In many cases, these inventions don't even originate with the company but their
    efficient patent process allows them to secure intellectual property cheaply.
    These trolls often work primarily in software, since software start-ups and other
    innovators in this area often ignore filing a patent. Google and other big software
    companies now go out of their way to file these patents, but in essence suggest
    they would never enforce such a patent. Of course, it could change its mind—just
    look at Java.
  prefs: []
  type: TYPE_NORMAL
- en: DQN is like the Hello World of DRL, and almost every text or course on this
    subject will have a version demonstrated. The version we are going to look at
    here follows the standard pattern but is broken down in a manner that showcases
    our previous learning on TD and the temporal credit assignment. This will provide
    a good comparison between using DL and not using DL. In the next sections, we
    learn how to set up and run a DQN model.
  prefs: []
  type: TYPE_NORMAL
- en: Refreshing the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A major cause of a new user''s frustration is often just setting up the examples.
    That is why we want to make sure that your environment has the proper components
    installed. If you recall earlier, you should have created a new virtual environment
    called `gameAI`. We are now going to install the other required modules we need
    for the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: You should be working with and in a new virtual environment now. As such, we
    will need to install the Gym and other required components again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that you have a recent version of a Windows C++ compiler installed.
    For a list of supported compilers, check this site: [https://wiki.python.org/moin/WindowsCompilers.](https://wiki.python.org/moin/WindowsCompilers)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First let us install the required libraries for Windows, recall these steps
    are only required for Windows installations with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After you have installed the prerequisites on Windows, you do the remainder
    of the installation with the command to install Gym. This is the same command
    you will use on Mac and Linux installations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next install `matplotlib` and `tqdm` with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Recall that those are the helper libraries we use to monitor training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After installing those packages, make sure your IDE is configured to point to
    the `gameAI` virtual environment. Your particular IDE will have instructions to
    do this. In the next section, we look to some assumptions that allow us to solve
    infinite MDPs with DL such as DQN.
  prefs: []
  type: TYPE_NORMAL
- en: Partially observable Markov decision process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen how we can tackle a continuous or infinite observation
    space by discretizing it into buckets. This works well but as we saw computationally,
    it does not scale well to massive problems of observation state space. By introducing
    DL, we can effectively increase our state space inputs, but not nearly in the
    amount we need. Instead, we need to introduce the concept of a **partially observable
    Markov decision process** (**POMDP**). That is, we can consider any problem that
    is an infinite MDP to be partially observable, meaning the agent or algorithm
    needs only observe the local or observed state in order to make actions. If you
    think about it, this is exactly the way you interact with your environment. Where
    you may consider your minute-to-minute activities as partially observable observations
    of the global infinite MDP or, in other words, the universe. Whereas your day-to-day
    actions and decisions occur at a higher observable state, you only ever have a
    partially observable view of the entire globally infinite MDP.
  prefs: []
  type: TYPE_NORMAL
- en: This concept of being able to switch from different partially observable views
    of the same infinite MDP is a center of much research. Currently, there are two
    main branches of DRL tackling this problem. They are **hierarchical reinforcement
    learning** (**HRL**), which attempts to describe a problem as a start to MDP hierarchies.
    The other branch is called **meta reinforcement learning** (**MRL**), and it takes
    a broader approach in an attempt to let the partially observable be learned in
    different time steps. By introducing time sequences here, we can also start to
    work with other forms of neural networks, called recurrent networks, that can
    learn time. We will revisit MRL in  [Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml),* From
    DRL to AGI*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we finally look at how to build a DQN with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can probably find a version of a DQN developed in every DL framework. The
    algorithm itself is an incredible achievement in learning, since it allows us
    now to learn continuous or infinite spaces/the infinite MDP. Open `Chapter_6_DQN.py`,
    and follow the next exercise to build the DQN sample:'
  prefs: []
  type: TYPE_NORMAL
- en: The source for this example was originally derived from this GitHub repository: [https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb).
    It has been modified significantly in order to match the previous samples in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this time, the samples have become too large to list in a single listing.
    Instead, we will go through section by section as we normally would. As usual,
    it is helpful if you follow along with the code in an editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'These are are usual imports, but it should be mentioned that `torch` needs
    to load first before the other imports like `gym` or `numpy`. We are going to
    jump down past the first `ReplayBuffer` function until later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows the typical setup for creating the RL environment
    and setting up our hyperparameters. Notice how we are generating`eps_by_episode`
    for the decaying `epsilon` using a lambda expression. This is a very Pythonic
    way of producing a decaying epsilon. The last couple lines of code plot the decaying
    epsilon in a chart and outputs something similar to the following graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a84f8452-fa58-41d6-82b5-b634e105b45a.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot showing decaying epsilon over training epochs
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see from the preceding plot that epsilon more or less stabilizes around
    2,000 iterations; this seems to suggest the agent should have learned enough by
    then. We will now scroll down to the next block of code not in a function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'These three lines of code set up the critical components—the model, which is
    of the `DQN` type, and a class we will get to shortly. The **optimizer**, in this
    case, is of the **Adam** type, defined by `optim.Adam`. The last line creates `ReplayBuffer`,
    another class we will get to shortly. We will again scroll down past all the code
    in functions and review the next section of main code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of this code should look familiar by now. Notice how we are now setting
    a new hyperparameter called `batch_size`. Basically, `batch_size` is the size
    of the number of items we push through a network at a single time. We prefer to
    do this in batches since it provides a better averaging mechanism. That means
    when we train the model, we will do so in batches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Once again, most of this code should feel quite familiar by now since it mirrors
    many of our previous examples. There are two highlighted sections of code that
    we will focus on. The first is the line that pushes the `state`, `action`, `reward`,
    `next_state`, and `done` functions onto`replay_buffer`. We have yet to look at
    the `replay` buffer, but just realize at this point all this information is getting
    stored for later. The other highlighted section has to do with the computation
    of loss using the `compute_td_loss` function. That function computes the loss
    using the TD error as we saw when covering TD and SARSA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we explore the additional functions and classes, run the sample so that
    you can see the output. As the sample runs, you will need to repeatedly close
    the plotting window after every 2,000 iterations. The following graphs show the
    output from several thousand training iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a12eb970-633b-40ea-b0ab-aaf5f9cadc73.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output after several thousand training iterations
  prefs: []
  type: TYPE_NORMAL
- en: The output in the graphs shows how the agent learns across episodes and is able
    to quickly maximize the reward as shown in the left plot. In comparison, the right
    plot shows a decreasing loss or total error in our agent's predictions. In fact,
    we can see around episode 8000 that the agent has indeed learned the problem and
    is able to consistently achieve the maximal reward. If you recall in [Chapter
    5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml), *Exploring SARSA*, we solved the
    CartPole environment, but just barely with discretized SARSA. Although, that still
    required almost 50,000 episodes of training. Now that we have observed this method
    is several times better than our previous attempts, in the next section we need
    to explore the details of how this works.
  prefs: []
  type: TYPE_NORMAL
- en: The replay buffer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fundamental to the DL methods is the need for us to feed batches of observed
    agent events into the neural network. Remember, we do this in batches so the algorithm
    is able to average across errors or loss better. This requirement is more a function
    of DL than anything to do with RL. As such, we want to store a previous number
    of the observed state, action, next state, reward, and returns from our agent,
    taking an action into a container called `ReplayBuffer`. We then randomly sample
    those events from the replay buffer and inject them into the neural network for
    training. Let''s see how the buffer is constructed again by reopening sample `Chapter_6_DQN.py`
    and following this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire code for the `ReplayBuffer` class is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Internally the `ReplayBuffer` uses a class called `deque`, which is a class
    that can store any manner of objects. In the `init` function, we create the queue
    of the required specified size. The class has three functions `push`, `sample`
    and `len`. The `len` function is fairly self-explanatory, but the other functions
    we should look at:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `push` function pushes the `state`, `action`, `reward`, `next_state`, and
    `done` observations on to the queue for later processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The other function, `sample`, is where the buffer randomly samples events from
    the queue and zips them up using `zip`. It will then return this batch of random
    events to be fed into the network for learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Find the line of code that sets the size of the replay buffer and change it
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Run the example again with the new buffer size, and observe the effect this
    has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now change the buffer size again with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Run the example again, and observe the output closely. Notice the changes in
    the training performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have effectively tried running the agent with 3 times, as well as 1/3 the
    buffer size. What you will find in this problem is that the smaller buffer size
    is more effective but perhaps not optimal. You can consider the buffer size as
    another one of those essential hyperparameters you will need to learn to set.
  prefs: []
  type: TYPE_NORMAL
- en: Replay buffers are a required component of our DL models, and we will see other
    similar classes in the future. In the next section, we will move on to building
    the DQN class.
  prefs: []
  type: TYPE_NORMAL
- en: The DQN class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, we saw how the DQN class was used to construct the neural network
    model that we will use to learn the TD loss function. Reopen exercise `Chapter_6_DQN.py`
    again to review the construction of the DQN class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire code for the DQN class is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `init` function initializes the network using the PyTorch `nn.Sequential` class to
    generate a fully connected network. We can see that the inputs into the first
    layer are set by `env.observation_space.shape[0``],` and the number of neurons
    is 128.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can see there are three layers in this network, with the first layer consisting
    of 128 neurons connected by ReLU to a middle layer with 128 neurons. This layer
    is connected to the output layer, with the number of outputs defined by `env.action_space.n`.
    What we can see from this is that the network will be learning which action to
    select.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `forward` function is just the forward pass or prediction by the network
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, the `act` function is quite similar to the other Q-learning samples
    we have built before. One thing we want to focus on is how the actual action is
    selected during non-exploration as the following code excerpt shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Calculating the `state` tensor in the first line with `autograd.Variable` is
    where the state is converted into a tensor so that it may be fed into the forward
    pass. It is the call to `self.forward` in the next line that calculates all the
    Q values, `q_value`, for that `state` tensor. We then use a greedy (max) selection
    strategy in the last line to choose the action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the network size from 128 neurons to 32, 64, or 256 to see the effect
    this has on training. The following code shows the proper way to configure the
    example to use 64 neurons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Run the example again with various size changes, and see the effect this has
    on training performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Good news, we consider the number of neurons and number of layers in a network
    to be the additional training hyperparameters we need to observe while tackling
    problems. As you may have already noticed, these new inputs can have a dramatic
    effect on the training performance and need to be selected carefully.
  prefs: []
  type: TYPE_NORMAL
- en: We almost have all the pieces together to understand the entire algorithm. In
    the next section, we will cover the last piece, determining loss and training
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating loss and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we can see how all this comes together to train the agent to learn
    a policy. Open up `Chapter_6_DQN.py` again, and follow the next exercise to see
    how loss is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The function that calculates the loss in terms of TD errors is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, we call `sample` from `replay_buffer` using `batch_size`
    as the input. This returns a randomly sampled set of events from a previous run.
    This returns `state`, `next_state`, `action`, `reward`, and `done`. These are
    then turned into tensors in the next five lines using the `autograd.Variable`
    function. This function is a helper for converting types into tensors of the appropriate
    type. Notice how the action is of the `long` type using `torch.LongTensor`, and
    the other variables are just floats.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next section of code calculates the Q values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Remember that when we call `model(state)` that is the equivalent of doing a
    forward pass or prediction on the network. This now becomes the same as sampling
    from the policy in our previous examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then go back to our previous defined Q Learning equation, and use that to
    determine what our best expected Q value should be, with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculating the `expected_q_value` value from earlier uses the Q Learning equation
    to determine what an expected value should be. Based on the expected value, we
    can determine the how much the network is in error and how much loss it needs
    to correct with the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This line converts the value to a tensor and then determines the loss using
    our old friend MSE. Our final step is to optimize or reduce the loss of the network,
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The code is quite similar to what we used before to optimize our neural network
    and computational graph examples. We first apply `zero_grad` to the optimizer
    in order to zero out any gradients as a reset. We then push the loss backward,
    and finally perform one step on the optimizer. That last part is new and has to
    do with the type of optimizer we are using.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We won't go heavily into the various optimizers you can use for DL until [Chapter
    6](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml), *Going Deeper with DDQN*. In most
    cases, we will use the Adam optimizer or some derivation of it, depending on the
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to run the code sample yet again in order to better observe all the
    details in training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hopefully by now, even with the inclusion of DL, these samples are starting
    to feel consistently familiar. In some ways, DL makes these algorithms much simpler
    than our previous examples. Fortunately, that is a good thing because our agents
    will need to get more complicated and robust as we evolve to harder and harder
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have only been able to view our agent training and see no actual
    update in performance. Since that is essential to our understanding of how an
    agent trains, in the next section we are going to add that to the last example.
  prefs: []
  type: TYPE_NORMAL
- en: Exercising DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have progressed through this book, we have spent time making sure we can
    see how our agents our progressing in their respective environments. In this section,
    we are aiming to add rendering to the agent environment during training using
    our last DQN example. Then we can see how the agent is actually performing and
    perhaps try out another couple of new environments along the way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding the ability to watch the agent play in the environment is not that difficult,
    and we can implement this as we have done with other examples. Open the `Chapter_6_DQN_wplay.py` code
    example, and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is almost identical to the DQN sample earlier, so we won''t need to
    review the whole code. However, we do want to introduce two new variables as hyperparameters;
    this will allow us to better control the network training and observer performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We will use `buffer_size` to denote the size of the buffer. This value will
    also come in handy when we determine whether our model has some amount of training.
    DQN will not start training the model until the replay buffer or what we often
    refer to as the experience buffer is full. Notice that we also added a new hyperparameter
    for neurons; this will allow us to quickly tweak the network as we need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next we will look at how the code to render the agent playing the game is injected
    into the training loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The highlighted lines represent the new code that will check whether the current
    episode is larger than `buffer_size.` If it is then we render the agent playing
    the game using the model/policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next we will look at the new `play_game` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This code is quite similar to other `play_game` functions we crafted previously.
    Notice the highlighted line showing where we predict the next action using the
    `model.act` function. Passed into this function is the state and our minimum value
    for epsilon, called `epsilon_final`. We set the minimum value here since we choose
    the agent performing minimal exploration and the actions are selected entirely
    from the policy/model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run this example, and you can watch the agent play the CartPole environment
    successfully as shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b159086f-f2cb-47c7-9212-348a85befab7.png)'
  prefs: []
  type: TYPE_IMG
- en: Example rendering of an agent playing CartPole successfully
  prefs: []
  type: TYPE_NORMAL
- en: With a new agent able to tackle the CartPole environment far easier, in the
    next section we will now look to throw our failed example from the last chapter,
    the LunarLander environment.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the LunarLander and beyond
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now with our solid example of DQN, we can move on to solve more difficult environments,
    like LunarLander. In this exercise, we set up the DQN agent to solve the LunarLander
    environment in order to compare our previous attempts with discretized SARSA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `Chapter_6_DQN_lunar.py` example, and note the change in the `env_id` environment
    ID and creation of the environment shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We also adjust a couple of the hyperparameters to account for the increased
    complexity of the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We increase `epsilon_decay` in order to encourage the agent to explore longer.
    Exploration is a trade-off we always need to balance with the environment. Note `buffer_size`
    is also increased to a value of 3,000 to account for the increase in environment
    complexity, again. As well, we also increase the size of the network to 192 neurons
    across the board.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may also need to increase the number of total training episodes from 10,000
    to a higher value. We will leave that decision up to you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From here, you can run the example and visualize the agent landing the lander
    as shown in the following screen image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4016a02c-fcf4-46ff-9226-eef8d82e3ceb.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent playing the LunarLander environment
  prefs: []
  type: TYPE_NORMAL
- en: That completes our exploration of DQN, and you are encouraged to follow the
    exercises in the next section to practice those new skills.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we progress through the book, I hope you can see the value of performing these
    additional hands-on exercises. Learning how to tune hyperparameters will be essential
    in building DRL models that can tackle difficult environments. Use the following
    exercises to reinforce your learning of the material:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the `batch_size`, `inputs`, `hidden`, and `outputs` hyperparameters from
    `Chapter_6_1.py` and see what effect these have on the output loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the number of training iterations in the `Chapter_6_1.py` example in conjunction
    with other hyperparameters in order to evaluate the impact this has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the `batch_size`, `inputs`, `hidden `, and `outputs`  hyperparameters from
    `Chapter_6_2.py`, and see what effect these have on the output loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the number of training iterations in the `Chapter_6_2.py` example in conjunction
    with other hyperparameters in order to evaluate the impact this has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters in the `Chapter_6_DQN.py` example to improve training
    performance on the CartPole environment. Create any additional hyperparameters
    you may need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters in the `Chapter_6_DQN_wplay.py` example to improve
    training performance on the CartPole environment. Create any additional hyperparameters
    you may need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters in the `Chapter_6_DQN_lunar.py` example to improve
    training performance on the LunarLander environment. Create any additional hyperparameters
    you may need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the `batch_size` hyperparameter to low values of 8 or 16 all the way up
    to 256, 512, and 1,024 to see what effect this has on any and all the DQN examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduce a main function that will take command-line arguments that will allow
    you to configure the various hyperparameters at runtime. You will likely need
    to use a helper library (`argparse`) to do this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the ability to render the training performance without blocking the training
    execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doing two or three of these exercises can greatly improve your grasp of this
    knowledge, and there really is no better way to learn than doing, just ask one
    of your agents. Alas, we have come to the end of this chapter, and in the next
    section we have the summary.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the Hello World of DRL, the DQN algorithm, and
    applying DL to RL. We first looked at why we need DL in order to tackle more complex
    continuous observation state environments like CartPole and LunarLander. Then
    we looked at the more common DL environments you may use for DL and the one we
    use, PyTorch. From there, we installed PyTorch and set up an example using computational
    graphs as a low-level neural network. Following that, we built a second example
    with the PyTorch neural network interface in order to see the difference between
    a raw computational graph and neural network.
  prefs: []
  type: TYPE_NORMAL
- en: With that knowledge, we then jumped in and explored DQN in detail. We looked
    at how DQN uses experience replay or a replay buffer to replay events when training
    the network/policy in DQN. As well, we looked at how the TD loss was calculated
    based on the difference between the predicted and expected value. We used our
    old friend the Q Learning equation in order to calculate the expected value and
    feed back the difference as a loss to the model. By doing so, we were able to
    train the model/policy so the agent could solve CartPole and the LunarLander environments,
    given sufficient iterations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we again extend our knowledge from this chapter and explore
    the next level of DQN, the Double DQN or DDQN. Along with this, we will explore
    advances in network image processing with CNN so that we can tackle even more
    complex environments, such as the classic Atari.
  prefs: []
  type: TYPE_NORMAL
