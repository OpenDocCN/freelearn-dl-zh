- en: Going Deep with DQN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与DQN
- en: In this chapter, you will be introduced to **deep learning** (**DL**) in order
    to handle newer, more challenging infinite **Markov decision process** (**MDP**)
    problems. We will cover some basics about DL that are relevant to **reinforcement
    learning** (**RL**), and then look at how we can solve a Q-learning. After that,
    we will look at how to build a Deep Q-learning or DQN agent in order to solve
    some Gym environments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解**深度学习**（**DL**），以便处理更新、更具挑战性的无限**马尔可夫决策过程**（**MDP**）问题。我们将涵盖一些与**强化学习**（**RL**）相关的DL基础知识，然后探讨如何解决Q学习问题。之后，我们将探讨如何构建深度Q学习或DQN代理来解决一些Gym环境。
- en: 'Here is a summary of the topics we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是本章我们将涵盖的主题摘要：
- en: DL for RL
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习在强化学习中的应用
- en: Using PyTorch for DL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch进行深度学习
- en: Building neural networks with PyTorch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch构建神经网络
- en: Understanding DQN in PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中理解DQN
- en: Exercising DQN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锻炼DQN
- en: In this chapter, we introduce DL with respect to RL. Applying DL to **deep reinforcement
    learning** (**DRL**) is quite specific and is not covered in detail here.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍深度学习与强化学习的关系。将深度学习应用于**深度强化学习**（**DRL**）非常具体，这里没有详细涵盖。
- en: DL for RL
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在强化学习中的应用
- en: Over the course of the previous five chapters, we learned how to evaluate the
    value of state and actions for a given finite MDP. We learned how to solve various
    finite MDP problems using methods from MC, DP, Q-learning, and SARSA. Then we
    explored infinite MDP or continuous observation/action space problems, and we
    discovered this class of problems introduced computational limits that can only
    be overcome by introducing other methods, and this is where DL comes in.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前五章中，我们学习了如何评估给定有限MDP的状态和动作的价值。我们学习了如何使用MC、DP、Q学习和SARSA等方法解决各种有限MDP问题。然后我们探讨了无限MDP或连续观察/动作空间问题，我们发现这类问题引入了计算限制，这些限制只能通过引入其他方法来克服，这就是深度学习介入的地方。
- en: DL is so popular and accessible now that we have decided to cover only a very
    broad overview of the topic in this book. Anyone serious about building DRL agents
    should look at studying DL further on their own.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习现在如此流行且易于获取，我们决定在这本书中只涵盖该主题的非常广泛的概述。任何认真想要构建DRL代理的人应该自己进一步学习深度学习。
- en: For many, DL is about image classification, speech recognition, or that new
    cool thing called a **generative adversarial network** (**GAN**). Now, these are
    all great applications of DL, but, fundamentally, DL is about learning to minimize
    loss or errors. So when an image is shown to a network in order to learn the image,
    it is first split up and then fed into the network. Then the network spits out
    an answer. The correctness of the answer is determined, and any error is pushed
    back into the network as a way of learning. This method of pushing errors back
    through the network is called **backpropagation**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多人来说，深度学习（DL）涉及图像分类、语音识别，或者那个被称为**生成对抗网络**（**GAN**）的新潮事物。现在，这些都是深度学习的伟大应用，但本质上，深度学习是关于学习如何最小化损失或错误。因此，当将图像展示给网络以学习图像时，它首先被分割，然后输入到网络中。然后网络输出一个答案。答案的正确性被确定，任何错误都会作为学习方式被推回到网络中。这种将错误推回网络的方法被称为**反向传播**。
- en: 'The basics of this whole system are shown here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 整个系统的基本原理如下所示：
- en: '![](img/1cc1fa9d-6928-4023-9c50-c0ce4829f2b0.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1cc1fa9d-6928-4023-9c50-c0ce4829f2b0.png)'
- en: DL oversimplified
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习过于简化
- en: The DL network learns by backpropagating the errors back through the network
    as corrections to each cell, called a neuron, and internal to the neuron are parameters
    called weights. Each weight controls the strength of a connection to that neuron.
    The strength of each connection or weight in the network is modified using an
    optimization method based on the gradient descent. **Gradient descent** (**GD**)
    is a method derived from calculus that allows us to calculate the effect each
    connection/weight has on the answer. Working back, we can, therefore, use GD to
    determine the amount of correction each weight needs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络通过反向传播错误，作为对每个称为神经元的单元的修正来学习，神经元内部有称为权重的参数。每个权重控制着与该神经元的连接强度。网络中每个连接或权重的强度使用基于梯度下降的优化方法进行修改。**梯度下降**（**GD**）是一种源自微积分的方法，它允许我们计算每个连接/权重对答案的影响。反向工作，因此我们可以使用GD来确定每个权重需要的修正量。
- en: The major downside to using backpropagation with GD is that the training or
    learning needs to happen very slowly. Thus, many, perhaps thousands or millions
    of, images need to be shown to the network in order for it to learn. This actually
    works well when we apply DL to RL since our trial-and-error learning methods also
    work iteratively.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GD（梯度下降）进行反向传播的主要缺点是训练或学习需要非常缓慢地进行。因此，可能需要向网络展示数千或数百万个图像，以便它能够学习。当我们将深度学习应用于强化学习时，这实际上效果很好，因为我们的试错学习方法也是迭代的。
- en: DeepMind and other companies are currently working on other methods of learning,
    aside from backpropagation for DL networks. There has even been talk of doing
    one-shot learning. That is, being able to train a network on just a single image.
    Much like the way we humans can learn.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind和其他公司目前正在研究除了用于深度学习网络的反向传播之外的其他学习方法。甚至有人谈论过进行一次学习。也就是说，只需用一个图像就能训练一个网络。这与我们人类学习的方式非常相似。
- en: 'Now, it is often said that DL can be interpreted in many ways. As RL practitioners
    our interest in DL will be its use as an equation solver. You see fundamentally
    that is all DL does: solve equations; whether they are equations for classifying
    images, performing speech translation, or for RL. Deep reinforcement learning
    is about applying DL in order to solve the learning equations we looked at in
    the previous chapters and more. In fact, the addition of DL also provides many
    further capabilities in learning that we will explore in the rest of this book.
    In the next section, we cover a broad overview of the common DL frameworks used
    for DRL.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，人们常说深度学习可以有多种解释。作为强化学习从业者，我们对深度学习的兴趣在于将其用作方程求解器。你本质上会发现，深度学习做的就是解方程；无论是用于图像分类、语音翻译还是用于强化学习的方程。深度强化学习是应用深度学习来解决我们在前几章中看到的以及更多的学习方程。实际上，深度学习的加入也为学习提供了许多其他能力，我们将在本书的其余部分进行探讨。在下一节中，我们将概述用于DRL的常见深度学习框架。
- en: DL frameworks for DRL
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DRL的深度学习框架
- en: There are a number of DL frameworks available for use, but only a few have been
    readily used in RL research or projects. Most of these frameworks share a number
    of similarities, so transferring knowledge from one to the other is relatively
    straightforward. The three most popular frameworks for RL in the past few years
    have been Keras, TensorFlow, and PyTorch.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有多个深度学习框架可供使用，但只有少数在强化学习研究或项目中得到了广泛应用。这些框架之间有许多相似之处，因此从一个框架转移到另一个框架相对简单。在过去几年中，用于强化学习的三个最受欢迎的框架是Keras、TensorFlow和PyTorch。
- en: 'A summary of the strengths and weaknesses of each framework is shown in the
    table here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下面这张表展示了每个框架的优缺点总结：
- en: '| **Framework** | **Keras** | **TensorFlow** | **PyTorch** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **框架** | **Keras** | **TensorFlow** | **PyTorch** |'
- en: '| **Accessibility** | Easiest to learn and use | Provides a high-level Keras
    interface and lower-level interface. | Medium to low-level interface |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **可访问性** | 最容易学习和使用 | 提供高级Keras接口和低级接口。 | 中级到低级接口 |'
- en: '| **Scalability** | Scales well for smaller projects | Scales to any size project
    and supported output network models may be run on many different platforms. |
    Well suited to large projects requiring scale |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性** | 适用于小型项目 | 可扩展到任何规模的项目，并且支持的输出网络模型可以在许多不同的平台上运行。 | 适用于需要扩展的大型项目
    |'
- en: '| **Performance/Power** | Simple interface and limits customization | Powerful
    and great for performance | Excellent performance and provides the most control
    and additional interfaces for custom development |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **性能/功率** | 简单的接口和有限的定制 | 强大且非常适合性能 | 优秀的性能，并提供最多的控制和额外的接口，用于定制开发 |'
- en: '| **Popularity** | Popularity decreasing | Consistently popular framework and
    considered state of the art. | Popularity increasing especially for DRL applications
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **流行度** | 流行度下降 | 持续流行的框架，被认为是行业最佳。 | 流行度上升，尤其是在DRL应用中 |'
- en: If you review the table, the obvious choice for our DL framework will be PyTorch.
    PyTorch based on Torch is a relative newcomer but in just a few short years has
    gained incredible popularity as both a DL and DRL framework. Therefore, it will
    be our selected framework for this chapter. In the next section, we look at how
    to get started with PyTorch for DL.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看这张表，我们选择深度学习框架的明显选择将是PyTorch。基于Torch的PyTorch是一个相对较新的框架，但在短短几年内，它作为深度学习和深度强化学习框架已经获得了巨大的流行度。因此，它将成为我们本章选择的框架。在下一节中，我们将探讨如何开始使用PyTorch进行深度学习。
- en: Using PyTorch for DL
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行深度学习
- en: 'PyTorch provides both a low- and medium-level interface to building DL networks/computational
    graphs. As much as we build DL systems as networks with neurons connected in layers,
    the actual implementation of a neural network is through a computational graph.
    Computational graphs reside at the heart of all DL frameworks, and TensorFlow
    is no exception. However, Keras abstracts away any concept of computational graphs
    from the user, which makes it easier to learn but does not provide flexibility
    like PyTorch. Before we begin building computational graphs with PyTorch though,
    let''s first install PyTorch in the next exercise:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了构建深度学习网络/计算图的低级和中级接口。尽管我们构建深度学习系统时将其视为具有层中连接的神经元的网络，但神经网络的实际实现是通过计算图来完成的。计算图位于所有深度学习框架的核心，TensorFlow
    也不例外。然而，Keras 从用户那里抽象掉了计算图的概念，这使得学习更容易，但并不像 PyTorch 那样提供灵活性。但在我们开始使用 PyTorch 构建计算图之前，让我们先在下一个练习中安装
    PyTorch：
- en: 'Navigate your browser to [pytorch.org,](https://pytorch.org) and scroll down
    to the **Run this Command** section, as shown in the following screenshot:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的浏览器导航到 [pytorch.org,](https://pytorch.org) 并向下滚动到 **运行此命令** 部分，如下面的截图所示：
- en: '![](img/dd08ecc6-e37c-460a-86f4-6ee4c5242cfe.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dd08ecc6-e37c-460a-86f4-6ee4c5242cfe.png)'
- en: Generating a PyTorch installation command
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成 PyTorch 安装命令
- en: Select the **Stable** version and then your specific **OS** (**Linux**, **Mac**,
    or **Windows**). Next select the **package** (**Conda**, **Pip**, **LibTorch**,
    or **Source**); our preference here is **Conda** for Anaconda, but if you have
    experience with others, use them.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **稳定** 版本，然后选择您的特定 **操作系统**（**Linux**、**Mac** 或 **Windows**）。接下来选择 **包**（**Conda**、**Pip**、**LibTorch**
    或 **源**）；我们在这里的首选是 **Conda** 用于 Anaconda，但如果您有其他经验，请使用它们。
- en: Next, choose the **language** (Python 2.7, Python 3.5, Python 3.7, or C++);
    for our purposes, we will use **Python 3.6**.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择 **语言**（Python 2.7、Python 3.5、Python 3.7 或 C++）；对于我们来说，我们将使用 **Python
    3.6**。
- en: The next option **CUDA** (**9.2**, **10.0** or **None**) determines whether
    you have a **graphics processing unit** (**GPU**) that is capable of running **CUDA**.
    Currently, the only supported GPUs are built by NVIDIA. That's unlikely to change
    anytime soon. For our purposes, we will use **None**. **None** or CPU runs strictly
    on the CPU, which is slower, but will run across most devices.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个选项 **CUDA**（**9.2**、**10.0** 或 **None**）确定您是否有能够运行 **CUDA** 的 **图形处理单元**（**GPU**）。目前，唯一受支持的
    GPU 是由 NVIDIA 制造的。这种情况不太可能在短期内改变。对于我们来说，我们将使用 **None**。**None** 或 CPU 仅在 CPU 上运行，这较慢，但可以在大多数设备上运行。
- en: Open a 64-bit Python console under administrative rights. If you are using **Conda**,
    launch the window as an admin.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在管理员权限下打开一个 64 位 Python 控制台。如果您使用 **Conda**，请以管理员身份启动窗口。
- en: 'Create a new virtual environment with the following commands:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建新的虚拟环境：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This creates the virtual environment using Python 3.6\. PyTorch currently runs
    on Windows 64-bit. This may differ according to the OS. Then activate the environment
    with the following commands:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将使用 Python 3.6 创建虚拟环境。PyTorch 目前在 Windows 64 位上运行。这可能会根据操作系统而有所不同。然后使用以下命令激活环境：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Copy and paste the `install` command that was generated previously into the
    window, and execute it. An example of the command for Windows running Anaconda
    is shown here:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将之前生成的 `install` 命令复制并粘贴到窗口中，并执行它。以下是在 Windows 上运行 Anaconda 的命令示例：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This command should install PyTorch. If you saw any issues, such as errors claiming
    the libraries are not available for 32-bit, then make sure you are using a 64-bit
    version of Python.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此命令应安装 PyTorch。如果您遇到任何问题，例如显示库对 32 位不可用的错误，请确保您正在使用 64 位版本的 Python。
- en: The preceding process will install PyTorch and all the required dependencies
    we will need for now. If you have issues installing the framework, check the online
    documentation or one of the many online help forums. In most cases, installation
    issues will be resolved by making sure you are using 64-bit and running as an
    administrator.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的过程将安装 PyTorch 以及我们现在需要的所有依赖项。如果您在安装框架时遇到问题，请检查在线文档或众多在线帮助论坛之一。在大多数情况下，通过确保您使用
    64 位并作为管理员运行，可以解决安装问题。
- en: All the code examples for this book have been prepared and tested with Visual
    Studio Professional or Visual Studio Code, both with the Python tools installed.
    VS Code is a good solid editor that is free and cross-platform. It is a relative
    newcomer for Python development but benefits from Microsoft's years of experience
    building **integrated development environments** (**IDEs**).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的所有代码示例都已准备并使用Visual Studio Professional或Visual Studio Code进行测试，两者都安装了Python工具。VS
    Code是一个免费且跨平台的优秀编辑器。它是Python开发的相对新手，但受益于微软多年来在构建**集成开发环境**（**IDEs**）方面的经验。
- en: With PyTorch installed, we can move onto working with a simple example that
    creates a computational DL graph in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了PyTorch之后，我们可以在下一节中继续使用一个简单的示例，该示例创建一个计算深度学习图。
- en: Computational graphs with tensors
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量计算图
- en: 'At the core of all DL frameworks is the concept of a tensor or what we often
    think of as a multidimensional array or matrix. The computational graphs we construct
    will work on tensors using a variety of operations to linearly transform the inputs
    into final outputs. You can think of this as a kind of flow, and hence the reason
    TensorFlow has the name it does. In the following exercise, we are going to construct
    a two-layer DL network using a computation PyTorch graph and then train the network:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所有深度学习框架的核心概念是张量，或者我们通常认为的多维数组或矩阵。我们构建的计算图将使用各种操作在张量上工作，将输入线性地转换成最终输出。您可以将其视为一种流程，这也是TensorFlow命名的原因。在接下来的练习中，我们将使用计算PyTorch图构建一个两层深度学习网络，并训练该网络：
- en: The concepts here assume an understanding of linear algebra and matrix multiplication
    and systems of linear equations. As such, it is recommended that any readers lacking
    in these skills or are in need of a quick refresher should do so. Of course, a
    quick refresher on calculus may also come in useful.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里所涉及的概念假设读者已经理解了线性代数、矩阵乘法和线性方程组。因此，建议任何缺乏这些技能或需要快速复习的读者进行复习。当然，快速复习微积分也可能很有用。
- en: 'Open the `Chapter_6_1.py` code example. The example was derived from a PyTorch
    quickstart manual, with some of the variable names altered to be more contextual:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_6_1.py`代码示例。该示例是从PyTorch快速入门手册中提取的，其中一些变量名被更改以更具上下文性：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We start by importing the PyTorch library with `import torch`. Then we set
    our preferred data type `dtype` variable to `torch.float`. Then we initialize
    the device variable by using `torch.device` and passing in `cpu` to denote a CPU
    only. The option to enable the example to run with CUDA on a GPU was left in,
    but installing CUDA is left up to you:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先使用`import torch`导入PyTorch库。然后设置我们首选的数据类型`dtype`变量为`torch.float`。接着通过使用`torch.device`并传入`cpu`来初始化设备变量，表示仅使用CPU。示例中保留了启用CUDA在GPU上运行的选项，但安装CUDA由您自行决定：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we set up some variables to define how the data is processed and the
    architecture of the network. The `batch_size` parameter denotes how many items
    to train in an iteration. The `inputs` variable denotes the size of the input
    space into the network, whereas the `hidden` variable represents the number of
    hidden or middle-layer neurons in the network. The last `outputs` variable denotes
    the output space or the number of neurons in an output layer of a network:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们设置一些变量来定义数据的处理方式和网络的架构。`batch_size`参数表示在一次迭代中要训练的项目数量。`inputs`变量表示输入空间的大小，而`hidden`变量代表网络中隐藏或中间层的神经元数量。最后的`outputs`变量表示输出空间或网络输出层的神经元数量：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After that, we set the inputs and outputs variables: `x` as the inputs, and
    `y` as the outputs, to be learned based on just a random sampling based on `batch_size`.
    The size of the `inputs` variable in this example is 1,000, so each element in
    the batch will have 1000 inputs for `x`. The outputs have a value of 10, so each
    sample of `y` will likewise have 10 items:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们设置了输入和输出变量：`x`作为输入，`y`作为输出，这些变量将基于`batch_size`的随机采样进行学习。在这个例子中，`inputs`变量的大小为1,000，因此批次的每个元素都将有1,000个输入用于`x`。输出有10个值，因此每个`y`的样本也将有10个项目：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These two lines create our computational layers of a DL network defined by our
    previous `inputs`, `hidden`, and `outputs` parameters. The tensor contents of
    `layer1` and `layer2` at this point contain an initialized set of random weights
    the size of which is set by the number of inputs, hidden layers, and outputs.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两条线创建了我们深度学习网络中的计算层，这些层是由我们之前的 `inputs`、`hidden` 和 `outputs` 参数定义的。此时，`layer1`
    和 `layer2` 的张量内容包含一个初始化的随机权重集，其大小由输入数量、隐藏层和输出数量设置。
- en: 'You can visualize the size of these tensors by setting a breakpoint on the
    line after the layer setups and then running the file in debug mode *F5* on Visual
    Studio Code or Professional. When the breakpoint is hit, you can then use your
    mouse to hover over the variables to see information about the tensors as shown
    in the following screenshot:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过在层设置之后的行设置断点，然后在 Visual Studio Code 或 Professional 中以调试模式 *F5* 运行文件来可视化这些张量的大小。当断点被触发时，您可以使用鼠标悬停在变量上以查看有关张量的信息，如下面的截图所示：
- en: '![](img/c5d7d31e-0caf-4e29-81d1-dcb0ea66ad12.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/c5d7d31e-0caf-4e29-81d1-dcb0ea66ad12.png)'
- en: Inspecting the size of the layer weight tensors
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 检查层权重张量的大小
- en: Notice how the first layer dimensions are 1000 x 100 and the second layer dimensions
    are 100 x 10\. Computationally, we transform the inputs by multiplying the weights
    of the first layer and then outputting the results to the second layer. Here,
    the second layer weights are multiplied by the output from the first layer. We
    will see how this functions shortly.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意第一层的维度是 1000 x 100，第二层的维度是 100 x 10。在计算上，我们通过乘以第一层的权重来转换输入，然后将结果输出到第二层。在这里，第二层的权重乘以第一层的输出。我们很快就会看到这是如何工作的。
- en: 'Next, we define a `learning_rate` parameter, or what we will clarify now as
    a hyperparameter. The learning rate is a multiplier by which we can scale the
    rate of learning and is not different than the learning rate alpha we previously
    explored:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个 `learning_rate` 参数，或者我们现在将明确地称之为超参数。学习率是一个乘数，我们可以用它来缩放学习的速率，并且与之前探索的学习率
    alpha 没有区别：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will often use the terms `weight` and `parameter` to mean the same in DL.
    As such, other parameters such as `learning_rate`, epochs, batch size, and so
    on will be described as hyperparameters. Learning to tune hyperparameters will
    be an ongoing journey in building DL examples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们经常使用 `weight` 和 `parameter` 这两个术语来表示相同的意思。因此，其他参数，如 `learning_rate`、epoch、批量大小等，将被描述为超参数。学习调整超参数将是构建深度学习示例的持续旅程。
- en: 'Before we get into the training loop, let''s run the sample and observe the
    output. Run the sample as you normally would, in debug mode or not. The output
    of this example is shown in the following screenshot:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们进入训练循环之前，让我们运行示例并观察输出。像平常一样运行示例，无论是以调试模式还是非调试模式。此示例的输出如下截图所示：
- en: '![](img/98f857ad-a2d4-4d7a-a3eb-a9efbf450b30.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/98f857ad-a2d4-4d7a-a3eb-a9efbf450b30.png)'
- en: Output of example Chapter_6_1.py
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 示例输出 Chapter_6_1.py
- en: The output basically shows how the error loss decreases over training iterations.
    At iteration 99, we can see in the preceding example the error is around 635,
    but decreases down to almost zero by iteration 499\. While the inputs and outputs
    are all random, we can still see the network learns to identify a pattern in the
    data and thereby reduce errors. In the next section, we take a more detailed look
    at how this learning works.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出基本上显示了错误损失如何在训练迭代中下降。在第 99 次迭代时，我们可以看到在前面的例子中错误大约为 635，但通过第 499 次迭代下降到几乎为零。尽管输入和输出都是随机的，我们仍然可以看到网络学会了在数据中识别模式，从而减少错误。在下一节中，我们将更详细地探讨这种学习是如何进行的。
- en: Training a neural network – computational graph
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络 – 计算图
- en: 'In order to train a network or computational graph, we need to first feed it
    the input data, determine what the graph thinks is the answer, and then correct
    it iteratively using backpropagation. Let''s go back to the `Chapter_6_1.py` code
    example, and follow the next exercise to learn how training works:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个网络或计算图，我们首先需要给它输入数据，确定图认为的答案，然后通过反向传播迭代地纠正它。让我们回到 `Chapter_6_1.py` 代码示例，并跟随下一个练习来学习训练是如何进行的：
- en: 'We will start at the beginning of the training loop that starts with the `for`
    loop, as shown in the following code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从以下代码中显示的 `for` 循环开始的训练循环的开始处开始。
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So, 500 in this example denotes the total number of training iterations or epochs.
    In each iteration, we calculate the predicted output using the next three lines.
    This step is called the forward pass through the graph or network. Where the first
    line does the matrix multiplication of the `layer1` weights with the `x` inputs
    using `x.mm`. It then passes those output values through an activation function
    called **clamp**. Clamp sets limits on the output of the network, and in this
    case we use a clamp on 0\. This also happens to correspond with the rectified
    linear unit or ReLU function.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，500表示总的训练迭代次数或周期数。在每个迭代中，我们使用接下来的三行计算预测输出。这一步被称为通过图或网络的正向传递。第一行使用`x.mm`对`layer1`权重与`x`输入进行矩阵乘法。然后，它将这些输出值通过一个名为**clamp**的激活函数。clamp为网络的输出设置限制，在这种情况下，我们使用clamp限制为0。这也恰好对应于修正线性单元或ReLU函数。
- en: We use many different forms of activation functions in DL. The ReLU function
    is currently one of the more popular functions, but we will use others along the
    way throughout this book.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们使用许多不同形式的激活函数。ReLU函数目前是较为流行的函数之一，但在这本书的整个过程中，我们还会使用其他函数。
- en: After the output is activated through the ReLU function, it is then matrix multiplied
    by the second layer weights, `layer2`. The output of this result is `y_pred`,
    a tensor containing the output predictions.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在ReLU函数激活输出之后，它随后与第二层权重`layer2`进行矩阵乘法。这个结果的输出是`y_pred`，它是一个包含输出预测的张量。
- en: 'From there we predict the loss or amount of error between what we want to actually
    predict in the `y` tensor and what our network just predicted as a tensor `y_pred`
    using the following code:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那里，我们使用以下代码预测我们想要实际预测的`y`张量与我们的网络刚刚预测的张量`y_pred`之间的损失或误差量：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `loss` value or total error is calculated using a method called **mean squared
    error** or **MSE**. Keep in mind that since `y_pred` and `y` are tensors, the
    subtraction operation is done tensor-wide. That is, all values of the 10 predictions
    are subtracted from the predicted `y` value and then squared and summed. We use
    the same output technique here to print out the total loss for every 99 iterations.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**均方误差**或**MSE**方法计算`loss`值或总误差。请注意，由于`y_pred`和`y`都是张量，减法操作是在张量范围内进行的。也就是说，所有10个预测值都从预测的`y`值中减去，然后平方并求和。我们在这里使用相同的输出技术来打印出每99次迭代的总损失。
- en: 'After computing the loss, we next need to compute the gradient of graph weights
    in order to determine how we push back and correct the errors in the graph. Calculating
    this gradient is outside the scope of this book, but the code is shown as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算损失之后，我们接下来需要计算图权重的梯度，以确定如何推动和纠正图中的错误。计算这个梯度超出了本书的范围，但代码如下所示：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We show the low-level code here to do GD against a simple network graph as
    an example of how the math works. Fortunately, automatic differentiation lets
    us for the most part ignore those finer, more painful details. The gradients calculated
    here now need to be applied back to the graph layer weights using the following
    code:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里展示低级代码，以GD（梯度下降）对抗一个简单的网络图为例，来说明数学是如何工作的。幸运的是，自动微分让我们大部分时间可以忽略那些更精细、更痛苦细节。这里计算出的梯度现在需要使用以下代码应用到图层的权重上：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Notice how we are again using tensor subtraction to subtract the calculated
    gradients `grad_layer1` and `grad_layer2` scaled by the learning rate.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们再次使用张量减法来减去按学习率缩放的已计算的梯度`grad_layer1`和`grad_layer2`。
- en: Run the sample again and you should see a similar output. It can be helpful
    to play with the `learning_rate` hyperparameter to see what effect this has on
    training.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行示例，你应该会看到类似的输出。尝试调整`learning_rate`超参数以查看它对训练有什么影响可能会有所帮助。
- en: This previous example was a low-level look at how we can implement a computational
    graph that represents a two-layer neural network. While this example was meant
    to show you the inner details of how things work in practice, we will use the
    higher-level neural network subset of PyTorch to build graphs. We will see how
    to construct an example in the next section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子是低级地查看我们如何实现一个表示两层神经网络的计算图。虽然这个例子旨在向您展示实际操作中事物是如何工作的内部细节，但我们将使用PyTorch的高级神经网络子集来构建图。我们将在下一节中看到如何构建一个示例。
- en: Building neural networks with Torch
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Torch构建神经网络
- en: 'In the last section, we explored building computational graphs that resemble
    neural networks. This is a fairly common task as you may expect. So much so that
    PyTorch, as well as most DL frameworks, provides helper methods, classes, and
    functions to build DL graphs. Keras is essentially a wrapper around TensorFlow
    that does just that. Therefore, in this section, we are going to recreate the
    last exercise''s example using the neural network helper functions in PyTorch.
    Open the  `Chapter_6_2.py` code example and follow the next exercise:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们探讨了构建类似于神经网络的计算图。这正如你所预期的那样是一个相当常见的任务。如此之多，以至于PyTorch以及大多数深度学习框架都提供了构建深度学习图的辅助方法、类和函数。Keras本质上是一个围绕TensorFlow的包装器，只做这件事。因此，在本节中，我们将使用PyTorch中的神经网络辅助函数重新创建上一节练习的例子。打开`Chapter_6_2.py`代码示例，并跟随下一个练习：
- en: 'The source code for the entire sample is as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整个示例的源代码如下：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The code becomes greatly simplified, but not so much that it doesn''t allow
    us to control the internals of the DL graph itself. This is not something you
    may appreciate entirely until working with other DL frameworks. However, it is
    not the simplicity but the flexibility that is pushing PyTorch to be the number
    one framework in DL:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码变得大大简化，但并没有到无法让我们控制深度学习图内部结构的地步。这可能是你直到与其他深度学习框架一起工作时才完全欣赏的东西。然而，推动PyTorch成为深度学习领域第一框架的不是简单性，而是灵活性：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'There are a couple of big changes to the top section of the code, most notably
    with the setup of a model using `torch.nn.Sequential`. The setup of this model
    or graph is exactly the same as we did previously, except it describes each connection
    point more explicitly. We can see that the first layer is defined with `torch.nn.Linear`
    taking `inputs` and `hidden` as parameters. This gets connected to the activation
    function, again ReLU denoted by `torch.nn.ReLU`. After that, we create the final
    layer using `hidden` and `outputs` as the parameters. The `Sequential` term for
    the model denotes the whole graph is fully connected; the same as we looked at
    in the last example:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码的上部有一些重大变化，最值得注意的是使用`torch.nn.Sequential`设置模型。这个模型或图的设置与我们之前做的是一样的，只是更明确地描述了每个连接点。我们可以看到第一层是用`torch.nn.Linear`定义的，它以`inputs`和`hidden`作为参数。这连接到激活函数，再次是ReLU，由`torch.nn.ReLU`表示。之后，我们使用`hidden`和`outputs`作为参数创建最终的层。模型的`Sequential`术语表示整个图是完全连接的；就像我们在上一个例子中看到的那样：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After the model definition, we can also see our `loss_fn` loss function is
    more descriptive by using `torch.nn.MSELoss` as the function. This lets us know
    explicitly what the `loss` function is and how it is going to be reduced,  in
    this case, reducing the sum, denoted by `reduction=''sum''`, or the sum of average
    squared errors:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型定义之后，我们还可以看到我们的`loss_fn`损失函数通过使用`torch.nn.MSELoss`作为函数变得更加描述性。这让我们明确知道`loss`函数是什么以及它将如何被减少，在这种情况下，减少总和，用`reduction='sum'`表示，或者平均平方误差的总和：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The start of the training loop remains the same but this time `y_pred` is taken
    from just inputting the entire `x` batch into the `model`. This operation is the
    same as the forward pass or where the network outputs the answer:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练循环的开始与之前相同，但这次`y_pred`是从将整个`x`批次输入到`model`中获得的。这个操作与正向传播相同，或者说是网络输出答案的地方：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After that, we calculate `loss` as aTorch tensor, using the `loss_fn` function.
    The next piece of code is the same loss output code as we have seen before:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们使用`loss_fn`函数计算`loss`作为Torch张量。接下来的代码片段与之前看到的相同损失输出代码：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we zero any gradients in the model—this is essentially a reset. Then
    we calculate the gradients in the loss tensor using the `backward` function. This
    is essentially that nasty bit of code we previously looked at, which has now been
    simplified to a single line:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在模型中清除任何梯度——这本质上是一个重置。然后我们使用`backward`函数计算损失张量中的梯度。这本质上是我们之前看到的那个讨厌的代码片段，现在已经被简化为单行：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We finish off training the same way as before by adjusting the weights in the
    model using the calculated gradients of the `loss` tensor. While this section
    of code is more verbose than our last example, it explains better the actual learning
    process.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过调整模型中的权重来结束训练，使用计算出的`loss`张量的梯度。虽然这段代码比我们之前的例子更冗长，但它更好地解释了实际的学习过程。
- en: Run the example just like you did previously, and you should see very similar
    output to what we saw in the `Chapter_6_1.py` example.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照你之前的方式运行示例，你应该会看到与`Chapter_6_1.py`示例中非常相似的结果。
- en: Did you notice that the `learning_rate` variable in the second example was slightly
    lower?  The reason for this is because the neural network model differs slightly
    in a few areas, including the use of another weight for each neuron called a **bias**.
    If you want to learn more about the bias, be sure to pick up a good course on
    DL.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您注意到第二个示例中的 `learning_rate` 变量略低吗？这是因为神经网络模型在几个方面略有不同，包括每个神经元使用另一个称为 **偏置**
    的权重。如果您想了解更多关于偏置的信息，请确保选择一门好的深度学习课程。
- en: With a good basic understanding of how we can use PyTorch, we will now look
    and see how we can apply our knowledge to RL in the next section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在对如何使用 PyTorch 有良好的基本理解之后，我们现在将探讨如何将我们的知识应用到下一节中的强化学习。
- en: Understanding DQN in PyTorch
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 PyTorch 中理解 DQN
- en: Deep reinforcement learning became prominent because of the work of combining
    Q-learning with DL. The combination is known as deep Q-learning or **DQN** for
    **Deep Q Network**. This algorithm has powered some of the cutting edge examples
    of DRL, when Google DeepMind used it to make classic Atari games better than humans
    in 2012\. There are many implementations of this algorithm, and Google has even
    patented it. The current consensus is that Google patented such a base algorithm
    in order to thwart patent trolls striking at little guys or developers building
    commercial applications with DQN. It is unlikely that Google would exercise this
    legally or that it would have to since this algorithm is no longer considered
    state of the art.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习之所以突出，是因为将 Q-learning 与深度学习相结合的工作。这种组合被称为深度 Q-learning 或 **DQN**（深度 Q
    网络）。该算法为一些 DRL 的尖端示例提供了动力，当谷歌 DeepMind 在 2012 年使用它使经典 Atari 游戏比人类表现得更好时。这个算法有许多实现，谷歌甚至为其申请了专利。目前的共识是，谷歌为了阻止专利勒索者攻击小公司或开发使用
    DQN 的商业应用程序而申请了这样一个基础算法。谷歌不太可能行使这项法律权利，或者由于这个算法不再被认为是尖端技术，因此不太可能需要这样做。
- en: Patent trolling is a practice whereby an often less-than-ethical company will
    patent any and all manner of inventions just for the sake of securing patents.
    In many cases, these inventions don't even originate with the company but their
    efficient patent process allows them to secure intellectual property cheaply.
    These trolls often work primarily in software, since software start-ups and other
    innovators in this area often ignore filing a patent. Google and other big software
    companies now go out of their way to file these patents, but in essence suggest
    they would never enforce such a patent. Of course, it could change its mind—just
    look at Java.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 专利勒索是一种做法，其中一家往往不太道德的公司会为任何和所有类型的发明申请专利，只是为了确保专利。在许多情况下，这些发明甚至不是由公司发明的，但它们高效的专利流程使它们能够以较低的成本获得知识产权。这些勒索者通常主要在软件领域工作，因为软件初创公司和该领域的其他创新者往往忽视申请专利。谷歌和其他大型软件公司现在会不遗余力地申请这些专利，但本质上暗示他们永远不会执行这些专利。当然，他们可能会改变主意——只需看看
    Java。
- en: DQN is like the Hello World of DRL, and almost every text or course on this
    subject will have a version demonstrated. The version we are going to look at
    here follows the standard pattern but is broken down in a manner that showcases
    our previous learning on TD and the temporal credit assignment. This will provide
    a good comparison between using DL and not using DL. In the next sections, we
    learn how to set up and run a DQN model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 就像是 DRL 的 Hello World，几乎每本关于这个主题的书籍或课程都会有一个演示版本。我们在这里将要查看的版本遵循标准模式，但以一种展示我们之前在
    TD 和时间信用分配方面的学习成果的方式分解。这将提供使用深度学习和不使用深度学习之间的良好比较。在接下来的章节中，我们将学习如何设置和运行一个 DQN 模型。
- en: Refreshing the environment
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 刷新环境
- en: 'A major cause of a new user''s frustration is often just setting up the examples.
    That is why we want to make sure that your environment has the proper components
    installed. If you recall earlier, you should have created a new virtual environment
    called `gameAI`. We are now going to install the other required modules we need
    for the next exercise:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 新用户感到沮丧的主要原因往往是仅仅设置示例。这就是为什么我们想要确保您的环境已经安装了适当的组件。如果您还记得，您应该已经创建了一个名为 `gameAI`
    的新虚拟环境。我们现在将安装其他我们为下一个练习所需的模块：
- en: You should be working with and in a new virtual environment now. As such, we
    will need to install the Gym and other required components again.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您现在应该在与新虚拟环境中工作。因此，我们将需要再次安装 Gym 和其他所需的组件。
- en: Make sure that you have a recent version of a Windows C++ compiler installed.
    For a list of supported compilers, check this site: [https://wiki.python.org/moin/WindowsCompilers.](https://wiki.python.org/moin/WindowsCompilers)
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您已安装了最新的Windows C++编译器。有关支持的编译器列表，请查看此网站：[https://wiki.python.org/moin/WindowsCompilers.](https://wiki.python.org/moin/WindowsCompilers)
- en: 'First let us install the required libraries for Windows, recall these steps
    are only required for Windows installations with the following commands:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们安装Windows所需的库，记住这些步骤仅适用于Windows安装，以下命令：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After you have installed the prerequisites on Windows, you do the remainder
    of the installation with the command to install Gym. This is the same command
    you will use on Mac and Linux installations:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Windows上安装了所有必备条件后，您可以使用安装Gym的命令来完成剩余的安装工作。这个命令在Mac和Linux的安装中也会用到：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next install `matplotlib` and `tqdm` with the following commands:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令安装`matplotlib`和`tqdm`：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Recall that those are the helper libraries we use to monitor training.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回想一下，那些是我们用来监控训练的辅助库。
- en: After installing those packages, make sure your IDE is configured to point to
    the `gameAI` virtual environment. Your particular IDE will have instructions to
    do this. In the next section, we look to some assumptions that allow us to solve
    infinite MDPs with DL such as DQN.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了这些包之后，请确保您的IDE配置为指向`gameAI`虚拟环境。您的特定IDE将提供如何做到这一点的说明。在下一节中，我们将探讨一些允许我们使用深度学习解决无限MDP，如DQN的假设。
- en: Partially observable Markov decision process
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部分可观察马尔可夫决策过程
- en: We have already seen how we can tackle a continuous or infinite observation
    space by discretizing it into buckets. This works well but as we saw computationally,
    it does not scale well to massive problems of observation state space. By introducing
    DL, we can effectively increase our state space inputs, but not nearly in the
    amount we need. Instead, we need to introduce the concept of a **partially observable
    Markov decision process** (**POMDP**). That is, we can consider any problem that
    is an infinite MDP to be partially observable, meaning the agent or algorithm
    needs only observe the local or observed state in order to make actions. If you
    think about it, this is exactly the way you interact with your environment. Where
    you may consider your minute-to-minute activities as partially observable observations
    of the global infinite MDP or, in other words, the universe. Whereas your day-to-day
    actions and decisions occur at a higher observable state, you only ever have a
    partially observable view of the entire globally infinite MDP.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，我们可以通过将其离散化成桶来处理连续或无限的观察空间。这工作得很好，但正如我们所看到的，它在处理观察状态空间的巨大问题时并不容易扩展。通过引入深度学习，我们可以有效地增加我们的状态空间输入，但远未达到我们需要的程度。相反，我们需要引入**部分可观察马尔可夫决策过程**（**POMDP**）的概念。也就是说，我们可以将任何无限MDP问题视为部分可观察的，这意味着智能体或算法只需要观察局部或观察到的状态来采取行动。如果你这么想，这正是你与环境互动的方式。你可能将你的日常活动视为对全局无限MDP的部分可观察观察，或者说，是宇宙。而你的日常行动和决策发生在更高的可观察状态，你只能对整个全局无限MDP有一个部分可观察的视角。
- en: This concept of being able to switch from different partially observable views
    of the same infinite MDP is a center of much research. Currently, there are two
    main branches of DRL tackling this problem. They are **hierarchical reinforcement
    learning** (**HRL**), which attempts to describe a problem as a start to MDP hierarchies.
    The other branch is called **meta reinforcement learning** (**MRL**), and it takes
    a broader approach in an attempt to let the partially observable be learned in
    different time steps. By introducing time sequences here, we can also start to
    work with other forms of neural networks, called recurrent networks, that can
    learn time. We will revisit MRL in  [Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml),* From
    DRL to AGI*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在不同的部分可观察视角之间切换同一无限MDP的概念是许多研究的核心。目前，有两个主要的DRL分支正在解决此问题。它们是**分层强化学习**（**HRL**），它试图将问题描述为MDP层次结构的起点。另一个分支被称为**元强化学习**（**MRL**），它采取更广泛的方法，试图在不同的时间步长中学习部分可观察性。通过引入时间序列，我们还可以开始使用其他形式的神经网络，称为循环神经网络，它们可以学习时间。我们将在第14章中重新探讨MRL，*从DRL到AGI*。
- en: In the next section, we finally look at how to build a DQN with PyTorch.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将最终探讨如何使用PyTorch构建DQN。
- en: Constructing DQN
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建DQN
- en: 'You can probably find a version of a DQN developed in every DL framework. The
    algorithm itself is an incredible achievement in learning, since it allows us
    now to learn continuous or infinite spaces/the infinite MDP. Open `Chapter_6_DQN.py`,
    and follow the next exercise to build the DQN sample:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能可以在每个深度学习框架中找到一个DQN的版本。这个算法本身在学习方面是一个了不起的成就，因为它现在允许我们学习连续或无限的空间/无限马尔可夫决策过程。打开`Chapter_6_DQN.py`，按照下一个练习来构建DQN示例：
- en: The source for this example was originally derived from this GitHub repository: [https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb).
    It has been modified significantly in order to match the previous samples in this
    book.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的源代码最初是从这个GitHub仓库中提取的：[https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb)。为了与本书中的先前示例相匹配，它已经进行了重大修改。
- en: 'At this time, the samples have become too large to list in a single listing.
    Instead, we will go through section by section as we normally would. As usual,
    it is helpful if you follow along with the code in an editor:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，样本已经变得太大，无法在一个列表中列出。相反，我们将像往常一样分部分进行。通常，如果你在编辑器中跟随代码，这会有所帮助：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'These are are usual imports, but it should be mentioned that `torch` needs
    to load first before the other imports like `gym` or `numpy`. We are going to
    jump down past the first `ReplayBuffer` function until later:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些是常见的导入，但应该指出的是，`torch`需要在导入`gym`或`numpy`等其他导入之前加载。我们将跳过第一个`ReplayBuffer`函数，直到稍后：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code shows the typical setup for creating the RL environment
    and setting up our hyperparameters. Notice how we are generating`eps_by_episode`
    for the decaying `epsilon` using a lambda expression. This is a very Pythonic
    way of producing a decaying epsilon. The last couple lines of code plot the decaying
    epsilon in a chart and outputs something similar to the following graph:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码显示了创建强化学习环境和设置超参数的典型设置。注意我们是如何使用lambda表达式生成`eps_by_episode`来生成衰减的`epsilon`的。这是一种非常Pythonic的方式来产生衰减的epsilon。代码的最后几行将衰减的epsilon绘制在图表中，并输出类似于以下图表的内容：
- en: '![](img/a84f8452-fa58-41d6-82b5-b634e105b45a.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a84f8452-fa58-41d6-82b5-b634e105b45a.png)'
- en: Plot showing decaying epsilon over training epochs
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 展示训练时期望epsilon衰减的图表
- en: 'You can see from the preceding plot that epsilon more or less stabilizes around
    2,000 iterations; this seems to suggest the agent should have learned enough by
    then. We will now scroll down to the next block of code not in a function:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以从前面的图中看到，epsilon在大约2,000次迭代时稳定在某个水平；这似乎表明代理那时应该已经学到了足够的知识。我们现在将向下滚动到函数外的下一块代码：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'These three lines of code set up the critical components—the model, which is
    of the `DQN` type, and a class we will get to shortly. The **optimizer**, in this
    case, is of the **Adam** type, defined by `optim.Adam`. The last line creates `ReplayBuffer`,
    another class we will get to shortly. We will again scroll down past all the code
    in functions and review the next section of main code:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这三行代码设置了关键组件——模型，它是`DQN`类型的，以及我们很快就会接触到的类。在这种情况下，**优化器**是`Adam`类型的，由`optim.Adam`定义。最后一行创建了`ReplayBuffer`，这是我们很快就会接触到的另一个类。我们再次向下滚动，跳过函数中的所有代码，并回顾主代码的下一部分：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Most of this code should look familiar by now. Notice how we are now setting
    a new hyperparameter called `batch_size`. Basically, `batch_size` is the size
    of the number of items we push through a network at a single time. We prefer to
    do this in batches since it provides a better averaging mechanism. That means
    when we train the model, we will do so in batches:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到现在为止，大部分代码应该看起来很熟悉。注意我们现在设置了一个新的超参数，称为`batch_size`。基本上，`batch_size`是我们一次通过网络推送的项目数量的大小。我们更喜欢批量处理，因为这提供了更好的平均机制。这意味着当我们训练模型时，我们将以批量的方式进行：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once again, most of this code should feel quite familiar by now since it mirrors
    many of our previous examples. There are two highlighted sections of code that
    we will focus on. The first is the line that pushes the `state`, `action`, `reward`,
    `next_state`, and `done` functions onto`replay_buffer`. We have yet to look at
    the `replay` buffer, but just realize at this point all this information is getting
    stored for later. The other highlighted section has to do with the computation
    of loss using the `compute_td_loss` function. That function computes the loss
    using the TD error as we saw when covering TD and SARSA.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次强调，现在大部分代码应该已经很熟悉了，因为它与我们之前的许多示例相似。我们将关注两个代码高亮部分。第一个是推送`state`、`action`、`reward`、`next_state`和`done`函数到`replay_buffer`的行。我们尚未查看`replay`缓冲区，但在此阶段，所有这些信息都将被存储以供后续使用。另一个高亮部分与使用`compute_td_loss`函数计算损失有关。该函数使用TD误差来计算损失，正如我们在介绍TD和SARSA时所见。
- en: 'Before we explore the additional functions and classes, run the sample so that
    you can see the output. As the sample runs, you will need to repeatedly close
    the plotting window after every 2,000 iterations. The following graphs show the
    output from several thousand training iterations:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们探索额外的函数和类之前，运行样本以便您可以看到输出。当样本运行时，您需要在每2,000次迭代后重复关闭绘图窗口。以下图表显示了数千次训练迭代的输出：
- en: '![](img/a12eb970-633b-40ea-b0ab-aaf5f9cadc73.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a12eb970-633b-40ea-b0ab-aaf5f9cadc73.png)'
- en: Example output after several thousand training iterations
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 几千次训练迭代后的示例输出
- en: The output in the graphs shows how the agent learns across episodes and is able
    to quickly maximize the reward as shown in the left plot. In comparison, the right
    plot shows a decreasing loss or total error in our agent's predictions. In fact,
    we can see around episode 8000 that the agent has indeed learned the problem and
    is able to consistently achieve the maximal reward. If you recall in [Chapter
    5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml), *Exploring SARSA*, we solved the
    CartPole environment, but just barely with discretized SARSA. Although, that still
    required almost 50,000 episodes of training. Now that we have observed this method
    is several times better than our previous attempts, in the next section we need
    to explore the details of how this works.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中的输出显示了智能体在各个回合中的学习情况，并且能够快速最大化奖励，如左图所示。相比之下，右图显示了智能体预测中的损失或总误差在下降。实际上，我们可以在第8000个回合看到，智能体确实已经学会了这个问题，并且能够持续获得最大奖励。如果你还记得[第5章](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml)，*探索SARSA*，我们解决了CartPole环境，但只是勉强通过离散化的SARSA解决了。尽管如此，这仍然需要大约50,000个回合的训练。现在我们已经观察到这种方法比我们之前的尝试好得多，在下一节中，我们需要探索这种方法是如何工作的细节。
- en: The replay buffer
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重放缓冲区
- en: 'Fundamental to the DL methods is the need for us to feed batches of observed
    agent events into the neural network. Remember, we do this in batches so the algorithm
    is able to average across errors or loss better. This requirement is more a function
    of DL than anything to do with RL. As such, we want to store a previous number
    of the observed state, action, next state, reward, and returns from our agent,
    taking an action into a container called `ReplayBuffer`. We then randomly sample
    those events from the replay buffer and inject them into the neural network for
    training. Let''s see how the buffer is constructed again by reopening sample `Chapter_6_DQN.py`
    and following this exercise:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习方法来说，基本的需求是我们需要将观察到的智能体事件批次输入到神经网络中。记住，我们这样做是为了让算法能够更好地平均错误或损失。这一需求更多的是深度学习的方法，而不是与强化学习有关。因此，我们想要存储之前观察到的状态、动作、下一个状态、奖励和从智能体采取动作返回的值，并将它们存储在一个称为`ReplayBuffer`的容器中。然后，我们从重放缓冲区中随机抽取这些事件，并将它们注入神经网络进行训练。让我们再次通过重新打开`Chapter_6_DQN.py`样本并遵循这个练习来查看缓冲区的构建方式：
- en: 'The entire code for the `ReplayBuffer` class is shown as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ReplayBuffer`类的整个代码如下所示：'
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Internally the `ReplayBuffer` uses a class called `deque`, which is a class
    that can store any manner of objects. In the `init` function, we create the queue
    of the required specified size. The class has three functions `push`, `sample`
    and `len`. The `len` function is fairly self-explanatory, but the other functions
    we should look at:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内部，`ReplayBuffer`使用一个名为`deque`的类，这是一个可以存储任何类型对象的类。在`init`函数中，我们创建了一个所需指定大小的队列。该类有三个函数`push`、`sample`和`len`。`len`函数相当直观，但我们应该查看的其他函数：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `push` function pushes the `state`, `action`, `reward`, `next_state`, and
    `done` observations on to the queue for later processing:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`push`函数将`state`、`action`、`reward`、`next_state`和`done`观察结果推送到队列中以便稍后处理：'
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The other function, `sample`, is where the buffer randomly samples events from
    the queue and zips them up using `zip`. It will then return this batch of random
    events to be fed into the network for learning.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个函数`sample`是缓冲区从队列中随机采样事件并使用`zip`将它们组合起来。然后，它将返回这个随机事件批次以供网络学习。
- en: 'Find the line of code that sets the size of the replay buffer and change it
    to the following:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到设置重放缓冲区大小的代码行，并将其更改为以下内容：
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Run the example again with the new buffer size, and observe the effect this
    has on training.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行示例，并观察新的缓冲区大小对训练的影响。
- en: 'Now change the buffer size again with the following code:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在再次使用以下代码更改缓冲区大小：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Run the example again, and observe the output closely. Notice the changes in
    the training performance.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行示例，并仔细观察输出。注意训练性能的变化。
- en: We have effectively tried running the agent with 3 times, as well as 1/3 the
    buffer size. What you will find in this problem is that the smaller buffer size
    is more effective but perhaps not optimal. You can consider the buffer size as
    another one of those essential hyperparameters you will need to learn to set.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上尝试了3次运行代理，以及缓冲区大小的1/3。你会在这个问题中发现，较小的缓冲区大小更有效，但可能不是最优的。你可以将缓冲区大小视为你需要学习的另一个基本超参数。
- en: Replay buffers are a required component of our DL models, and we will see other
    similar classes in the future. In the next section, we will move on to building
    the DQN class.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 重放缓冲区是我们深度学习模型的一个必要组件，我们将在未来看到其他类似的类。在下一节中，我们将继续构建DQN类。
- en: The DQN class
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN类
- en: 'Previously, we saw how the DQN class was used to construct the neural network
    model that we will use to learn the TD loss function. Reopen exercise `Chapter_6_DQN.py`
    again to review the construction of the DQN class:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看到了如何使用DQN类构建我们将用于学习TD损失函数的神经网络模型。再次打开`Chapter_6_DQN.py`练习以回顾DQN类的构建：
- en: 'The entire code for the DQN class is as follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DQN类的整个代码如下：
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `init` function initializes the network using the PyTorch `nn.Sequential` class to
    generate a fully connected network. We can see that the inputs into the first
    layer are set by `env.observation_space.shape[0``],` and the number of neurons
    is 128.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`init`函数使用PyTorch的`nn.Sequential`类初始化网络，以生成一个全连接网络。我们可以看到第一层的输入由`env.observation_space.shape[0]`设置，神经元数量为128。'
- en: We can see there are three layers in this network, with the first layer consisting
    of 128 neurons connected by ReLU to a middle layer with 128 neurons. This layer
    is connected to the output layer, with the number of outputs defined by `env.action_space.n`.
    What we can see from this is that the network will be learning which action to
    select.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到这个网络有三个层，第一层由128个神经元组成，通过ReLU连接到中间层，中间层也有128个神经元。这一层连接到输出层，输出数量由`env.action_space.n`定义。我们可以看到，网络将学习选择哪个动作。
- en: The `forward` function is just the forward pass or prediction by the network
    model.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`forward`函数只是网络模型的正向传递或预测。'
- en: 'Finally, the `act` function is quite similar to the other Q-learning samples
    we have built before. One thing we want to focus on is how the actual action is
    selected during non-exploration as the following code excerpt shows:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，`act`函数与其他我们之前构建的Q学习样本非常相似。我们想要关注的一点是在非探索期间如何选择实际动作，如下面的代码片段所示：
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Calculating the `state` tensor in the first line with `autograd.Variable` is
    where the state is converted into a tensor so that it may be fed into the forward
    pass. It is the call to `self.forward` in the next line that calculates all the
    Q values, `q_value`, for that `state` tensor. We then use a greedy (max) selection
    strategy in the last line to choose the action.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一行使用`autograd.Variable`计算`state`张量是将状态转换为张量以便它可以被输入到前向传递中。这是在下一行调用`self.forward`来计算该`state`张量的所有Q值`q_value`的地方。然后我们在最后一行使用贪婪（最大）选择策略来选择动作。
- en: 'Change the network size from 128 neurons to 32, 64, or 256 to see the effect
    this has on training. The following code shows the proper way to configure the
    example to use 64 neurons:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将网络大小从128个神经元更改为32、64或256，以观察这对训练的影响。以下代码显示了配置示例以使用64个神经元的正确方式：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Run the example again with various size changes, and see the effect this has
    on training performance.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行示例，并观察不同大小变化对训练性能的影响。
- en: Good news, we consider the number of neurons and number of layers in a network
    to be the additional training hyperparameters we need to observe while tackling
    problems. As you may have already noticed, these new inputs can have a dramatic
    effect on the training performance and need to be selected carefully.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息，我们认为网络中神经元和层数的数量是我们解决问题时需要观察的额外训练超参数。正如你可能已经注意到的，这些新输入可以对训练性能产生重大影响，需要仔细选择。
- en: We almost have all the pieces together to understand the entire algorithm. In
    the next section, we will cover the last piece, determining loss and training
    the network.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎已经拼凑出整个算法的所有部分，以便理解。在下一节中，我们将介绍最后一部分，确定损失和训练网络。
- en: Calculating loss and training
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失和训练
- en: 'Finally, we can see how all this comes together to train the agent to learn
    a policy. Open up `Chapter_6_DQN.py` again, and follow the next exercise to see
    how loss is calculated:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以看到所有这些是如何结合在一起来训练智能体学习策略的。再次打开`Chapter_6_DQN.py`，并跟随下一个练习来了解损失是如何计算的：
- en: 'The function that calculates the loss in terms of TD errors is shown as follows:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算基于TD误差的损失的函数如下所示：
- en: '[PRE35]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the first line, we call `sample` from `replay_buffer` using `batch_size`
    as the input. This returns a randomly sampled set of events from a previous run.
    This returns `state`, `next_state`, `action`, `reward`, and `done`. These are
    then turned into tensors in the next five lines using the `autograd.Variable`
    function. This function is a helper for converting types into tensors of the appropriate
    type. Notice how the action is of the `long` type using `torch.LongTensor`, and
    the other variables are just floats.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一行中，我们使用`batch_size`作为输入从`replay_buffer`调用`sample`。这返回了从之前运行中随机采样的一组事件。这返回了`state`、`next_state`、`action`、`reward`和`done`。然后，在接下来的五行中，使用`autograd.Variable`函数将这些转换为张量。这个函数是一个辅助函数，用于将类型转换为适当类型的张量。注意，动作是`long`类型，使用`torch.LongTensor`，而其他变量只是浮点数。
- en: 'The next section of code calculates the Q values:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一节代码计算Q值：
- en: '[PRE36]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Remember that when we call `model(state)` that is the equivalent of doing a
    forward pass or prediction on the network. This now becomes the same as sampling
    from the policy in our previous examples.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住，当我们调用`model(state)`时，这相当于在网络上进行前向传递或预测。现在这和我们在之前的例子中从策略中采样是相同的。
- en: 'We then go back to our previous defined Q Learning equation, and use that to
    determine what our best expected Q value should be, with the following code:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们回到之前定义的Q学习方程，并使用它来确定最佳期望Q值应该是什么，以下代码所示：
- en: '[PRE37]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Calculating the `expected_q_value` value from earlier uses the Q Learning equation
    to determine what an expected value should be. Based on the expected value, we
    can determine the how much the network is in error and how much loss it needs
    to correct with the following line:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从之前计算`expected_q_value`值使用Q学习方程来确定期望值应该是什么。基于期望值，我们可以确定网络误差的大小以及它需要通过以下行来纠正的损失：
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This line converts the value to a tensor and then determines the loss using
    our old friend MSE. Our final step is to optimize or reduce the loss of the network,
    using the following code:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这行代码将值转换为张量，然后使用我们老朋友均方误差（MSE）来确定损失。我们的最后一步是使用以下代码来优化或减少网络的损失：
- en: '[PRE39]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The code is quite similar to what we used before to optimize our neural network
    and computational graph examples. We first apply `zero_grad` to the optimizer
    in order to zero out any gradients as a reset. We then push the loss backward,
    and finally perform one step on the optimizer. That last part is new and has to
    do with the type of optimizer we are using.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码与我们之前用来优化我们的神经网络和计算图示例的代码非常相似。我们首先对优化器应用`zero_grad`，以便将任何梯度归零作为重置。然后我们将损失反向传播，最后在优化器上执行一步。最后这部分是新的，与我们使用的优化器类型有关。
- en: We won't go heavily into the various optimizers you can use for DL until [Chapter
    6](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml), *Going Deeper with DDQN*. In most
    cases, we will use the Adam optimizer or some derivation of it, depending on the
    environment.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨你可以在深度学习中使用的各种优化器，直到[第6章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)，*深入探索DDQN*。在大多数情况下，我们将使用Adam优化器或其某种变体，这取决于环境。
- en: Feel free to run the code sample yet again in order to better observe all the
    details in training.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随意再次运行代码示例，以便更好地观察训练中的所有细节。
- en: Hopefully by now, even with the inclusion of DL, these samples are starting
    to feel consistently familiar. In some ways, DL makes these algorithms much simpler
    than our previous examples. Fortunately, that is a good thing because our agents
    will need to get more complicated and robust as we evolve to harder and harder
    environments.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到现在为止，即使包括了深度学习（DL），这些示例也开始感觉越来越熟悉。在某种程度上，DL 使得这些算法比我们之前的例子简单得多。幸运的是，这是一个好事，因为随着我们向更难的环境进化，我们的智能体将需要变得更加复杂和健壮。
- en: Up until now, we have only been able to view our agent training and see no actual
    update in performance. Since that is essential to our understanding of how an
    agent trains, in the next section we are going to add that to the last example.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只能查看智能体的训练过程，看不到性能的实际更新。由于这是我们对智能体训练理解的关键，在下一节中，我们将将其添加到上一个示例中。
- en: Exercising DQN
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习 DQN
- en: As we have progressed through this book, we have spent time making sure we can
    see how our agents our progressing in their respective environments. In this section,
    we are aiming to add rendering to the agent environment during training using
    our last DQN example. Then we can see how the agent is actually performing and
    perhaps try out another couple of new environments along the way.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们通过这本书的进展，我们花时间确保我们可以看到我们的智能体在各自环境中的进展。在本节中，我们的目标是使用我们最后的 DQN 示例在训练期间为智能体环境添加渲染。然后我们可以看到智能体实际的表现，也许还可以在途中尝试几个新的环境。
- en: 'Adding the ability to watch the agent play in the environment is not that difficult,
    and we can implement this as we have done with other examples. Open the `Chapter_6_DQN_wplay.py` code
    example, and follow the next exercise:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 添加观看智能体在环境中玩的能力并不困难，我们可以像其他示例一样实现它。打开 `Chapter_6_DQN_wplay.py` 代码示例，并遵循下一个练习：
- en: 'The code is almost identical to the DQN sample earlier, so we won''t need to
    review the whole code. However, we do want to introduce two new variables as hyperparameters;
    this will allow us to better control the network training and observer performance:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码几乎与之前的 DQN 示例完全相同，所以我们不需要审查整个代码。然而，我们确实想介绍两个新的变量作为超参数；这将使我们能够更好地控制网络训练和观察性能：
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We will use `buffer_size` to denote the size of the buffer. This value will
    also come in handy when we determine whether our model has some amount of training.
    DQN will not start training the model until the replay buffer or what we often
    refer to as the experience buffer is full. Notice that we also added a new hyperparameter
    for neurons; this will allow us to quickly tweak the network as we need.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 `buffer_size` 来表示缓冲区的大小。这个值在我们确定我们的模型是否有一些训练量时也会很有用。DQN 不会开始训练模型，直到重放缓冲区或我们通常所说的经验缓冲区填满。注意我们还添加了一个新的神经元超参数；这将允许我们根据需要快速调整网络。
- en: 'Next we will look at how the code to render the agent playing the game is injected
    into the training loop:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看将渲染智能体玩游戏代码注入到训练循环中的方法：
- en: '[PRE41]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The highlighted lines represent the new code that will check whether the current
    episode is larger than `buffer_size.` If it is then we render the agent playing
    the game using the model/policy.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高亮行表示新的代码，该代码将检查当前剧集是否大于 `buffer_size`。如果是，则使用模型/策略渲染智能体玩游戏。
- en: 'Next we will look at the new `play_game` function, as follows:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看新的 `play_game` 函数，如下所示：
- en: '[PRE42]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This code is quite similar to other `play_game` functions we crafted previously.
    Notice the highlighted line showing where we predict the next action using the
    `model.act` function. Passed into this function is the state and our minimum value
    for epsilon, called `epsilon_final`. We set the minimum value here since we choose
    the agent performing minimal exploration and the actions are selected entirely
    from the policy/model.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码与我们之前编写的其他 `play_game` 函数非常相似。注意高亮行显示了如何使用 `model.act` 函数预测下一个动作。传递给这个函数的是状态和我们的最小
    epsilon 值，称为 `epsilon_final`。我们在这里设置最小值，因为我们选择执行最小探索的智能体，并且动作完全从策略/模型中选择。
- en: 'Run this example, and you can watch the agent play the CartPole environment
    successfully as shown in the following diagram:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行这个示例，你可以看到智能体成功地在以下图中玩 CartPole 环境：
- en: '![](img/b159086f-f2cb-47c7-9212-348a85befab7.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b159086f-f2cb-47c7-9212-348a85befab7.png)'
- en: Example rendering of an agent playing CartPole successfully
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体成功玩 CartPole 的示例渲染
- en: With a new agent able to tackle the CartPole environment far easier, in the
    next section we will now look to throw our failed example from the last chapter,
    the LunarLander environment.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在有了能够轻松应对CartPole环境的智能体之后，在下一节中，我们将现在来看一下上一章中失败的例子，即LunarLander环境。
- en: Revisiting the LunarLander and beyond
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾LunarLander及其他
- en: 'Now with our solid example of DQN, we can move on to solve more difficult environments,
    like LunarLander. In this exercise, we set up the DQN agent to solve the LunarLander
    environment in order to compare our previous attempts with discretized SARSA:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了DQN的稳固示例，我们可以继续解决更困难的环境，如LunarLander。在这个练习中，我们设置了DQN智能体来解决LunarLander环境，以便比较我们之前的尝试与离散化SARSA：
- en: 'Open the `Chapter_6_DQN_lunar.py` example, and note the change in the `env_id` environment
    ID and creation of the environment shown as follows:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_6_DQN_lunar.py`示例，注意`env_id`环境ID和创建的环境如下所示：
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We also adjust a couple of the hyperparameters to account for the increased
    complexity of the environment:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还调整了一些超参数，以应对环境复杂性的增加：
- en: '[PRE44]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We increase `epsilon_decay` in order to encourage the agent to explore longer.
    Exploration is a trade-off we always need to balance with the environment. Note `buffer_size`
    is also increased to a value of 3,000 to account for the increase in environment
    complexity, again. As well, we also increase the size of the network to 192 neurons
    across the board.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们增加`epsilon_decay`的值，以鼓励智能体进行更长时间的探索。探索是我们始终需要与环境平衡的权衡。注意，`buffer_size`也增加到3,000，以应对环境复杂性的增加。此外，我们还把网络的神经元数量统一增加到192。
- en: You may also need to increase the number of total training episodes from 10,000
    to a higher value. We will leave that decision up to you.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可能还需要将总训练轮数从10,000增加到更高的值。我们将把这个决定留给你。
- en: 'From here, you can run the example and visualize the agent landing the lander
    as shown in the following screen image:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里，你可以运行示例并可视化智能体着陆的情况，如下面的屏幕截图所示：
- en: '![](img/4016a02c-fcf4-46ff-9226-eef8d82e3ceb.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4016a02c-fcf4-46ff-9226-eef8d82e3ceb.png)'
- en: Agent playing the LunarLander environment
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体在LunarLander环境中进行游戏
- en: That completes our exploration of DQN, and you are encouraged to follow the
    exercises in the next section to practice those new skills.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们对DQN的探索，并鼓励你在下一节中跟随练习来练习这些新技能。
- en: Exercises
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'As we progress through the book, I hope you can see the value of performing these
    additional hands-on exercises. Learning how to tune hyperparameters will be essential
    in building DRL models that can tackle difficult environments. Use the following
    exercises to reinforce your learning of the material:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们通过本书的进展，我希望你能看到进行这些额外动手练习的价值。学习如何调整超参数对于构建能够应对困难环境的DRL模型至关重要。使用以下练习来巩固你对材料的理解：
- en: Modify the `batch_size`, `inputs`, `hidden`, and `outputs` hyperparameters from
    `Chapter_6_1.py` and see what effect these have on the output loss.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`Chapter_6_1.py`中修改`batch_size`、`inputs`、`hidden`和`outputs`超参数，并观察这些参数对输出损失的影响。
- en: Alter the number of training iterations in the `Chapter_6_1.py` example in conjunction
    with other hyperparameters in order to evaluate the impact this has on training.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Chapter_6_1.py`示例中，结合其他超参数调整训练迭代次数，以评估这对训练的影响。
- en: Modify the `batch_size`, `inputs`, `hidden `, and `outputs`  hyperparameters from
    `Chapter_6_2.py`, and see what effect these have on the output loss.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`Chapter_6_2.py`中修改`batch_size`、`inputs`、`hidden`和`outputs`超参数，并观察这些参数对输出损失的影响。
- en: Alter the number of training iterations in the `Chapter_6_2.py` example in conjunction
    with other hyperparameters in order to evaluate the impact this has on training.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Chapter_6_2.py`示例中，结合其他超参数调整训练迭代次数，以评估这对训练的影响。
- en: Tune the hyperparameters in the `Chapter_6_DQN.py` example to improve training
    performance on the CartPole environment. Create any additional hyperparameters
    you may need.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Chapter_6_DQN.py`示例中调整超参数，以改善CartPole环境上的训练性能。创建你可能需要的任何额外超参数。
- en: Tune the hyperparameters in the `Chapter_6_DQN_wplay.py` example to improve
    training performance on the CartPole environment. Create any additional hyperparameters
    you may need.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Chapter_6_DQN_wplay.py`示例中调整超参数，以改善CartPole环境上的训练性能。创建你可能需要的任何额外超参数。
- en: Tune the hyperparameters in the `Chapter_6_DQN_lunar.py` example to improve
    training performance on the LunarLander environment. Create any additional hyperparameters
    you may need.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Chapter_6_DQN_lunar.py`示例中调整超参数，以改善LunarLander环境上的训练性能。创建你可能需要的任何附加超参数。
- en: Tune the `batch_size` hyperparameter to low values of 8 or 16 all the way up
    to 256, 512, and 1,024 to see what effect this has on any and all the DQN examples.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`batch_size`超参数调整到低值，如8或16，一直调整到256、512和1,024，以观察这将对所有DQN示例产生什么影响。
- en: Introduce a main function that will take command-line arguments that will allow
    you to configure the various hyperparameters at runtime. You will likely need
    to use a helper library (`argparse`) to do this.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍一个主要功能，该功能将接受命令行参数，允许你在运行时配置各种超参数。你可能需要使用辅助库（`argparse`）来完成此操作。
- en: Add the ability to render the training performance without blocking the training
    execution.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加渲染训练性能的能力，而不会阻塞训练执行。
- en: Doing two or three of these exercises can greatly improve your grasp of this
    knowledge, and there really is no better way to learn than doing, just ask one
    of your agents. Alas, we have come to the end of this chapter, and in the next
    section we have the summary.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 做两到三个这样的练习可以极大地提高你对这些知识的掌握，而且实际上没有比实践更好的学习方法了，只需问问你的智能体。唉，我们已经到达了本章的结尾，在下一节中我们将有总结。
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at the Hello World of DRL, the DQN algorithm, and
    applying DL to RL. We first looked at why we need DL in order to tackle more complex
    continuous observation state environments like CartPole and LunarLander. Then
    we looked at the more common DL environments you may use for DL and the one we
    use, PyTorch. From there, we installed PyTorch and set up an example using computational
    graphs as a low-level neural network. Following that, we built a second example
    with the PyTorch neural network interface in order to see the difference between
    a raw computational graph and neural network.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了DRL的Hello World，即DQN算法，以及将深度学习应用于强化学习。我们首先探讨了为什么我们需要深度学习来处理更复杂的连续观察状态环境，如CartPole和LunarLander。然后，我们探讨了你可能用于深度学习的更常见的深度学习环境，以及我们使用的PyTorch。从那里，我们安装了PyTorch，并使用计算图作为低级神经网络设置了一个示例。随后，我们使用PyTorch神经网络接口构建了第二个示例，以便比较原始计算图和神经网络之间的差异。
- en: With that knowledge, we then jumped in and explored DQN in detail. We looked
    at how DQN uses experience replay or a replay buffer to replay events when training
    the network/policy in DQN. As well, we looked at how the TD loss was calculated
    based on the difference between the predicted and expected value. We used our
    old friend the Q Learning equation in order to calculate the expected value and
    feed back the difference as a loss to the model. By doing so, we were able to
    train the model/policy so the agent could solve CartPole and the LunarLander environments,
    given sufficient iterations.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得这些知识后，我们便深入探讨了DQN。我们研究了DQN如何使用经验回放或回放缓冲区在训练网络/策略时重放事件。此外，我们还研究了基于预测值和期望值之间差异的TD损失是如何计算的。我们使用我们熟悉的朋友Q学习方程来计算期望值，并将差异作为损失反馈给模型。通过这样做，我们能够训练模型/策略，使得智能体能够在足够多的迭代次数下解决CartPole和LunarLander环境。
- en: In the next chapter, we again extend our knowledge from this chapter and explore
    the next level of DQN, the Double DQN or DDQN. Along with this, we will explore
    advances in network image processing with CNN so that we can tackle even more
    complex environments, such as the classic Atari.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将再次扩展本章的知识，并探索DQN的下一个层次，即Double DQN或DDQN。同时，我们将探索网络图像处理方面的进展，使用CNN，以便我们可以处理更复杂的环境，例如经典的Atari。
