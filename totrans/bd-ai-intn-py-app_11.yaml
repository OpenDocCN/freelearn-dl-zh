- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Common Failures of Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have just built your **generative AI** (**GenAI**) application, then
    you may be so fascinated by what it can do that you lose sight of answer quality
    and accuracy. Discovering how often GenAI is incorrect is a challenge in itself.
  prefs: []
  type: TYPE_NORMAL
- en: Many tend to believe that when a computer gives an answer, it gives an accurate
    answer—usually, more accurate than a human being. For example, most people feel
    relieved that machines, and not just people, fly airplanes today. Airplanes may
    be much safer now compared to 15 years ago because of this advancement, but when
    it comes to GenAI, the results are not nearly as accurate as the onboard systems
    of a flight craft.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter takes a detailed look at the top five challenges with GenAI applications
    and why they occur. Understanding these challenges is crucial for developers to
    devise effective solutions. By the end of this chapter, you will have a good understanding
    of these challenges, how they influence your outcomes, how they relate to each
    other, and why this particular set of technologies, despite these challenges,
    is still highly valuable to users.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sycophancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data leakage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the examples in this chapter can be demonstrated by simply repeating
    the prompt or example in ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the greatest challenges of working with GenAI, and perhaps the most well-known,
    is **hallucination**. Hallucination in GenAI refers to the phenomenon where the
    AI model generates content that sounds plausible but is factually incorrect, nonsensical,
    or not grounded in the provided input data. This issue is particularly prevalent
    in **natural language processing** (**NLP**) models, such as those used for text
    generation, but can also occur in other generative models such as image generation
    and LLMs such as GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: In the worst case, both the developers and their users do not know whether the
    answer given by GenAI is correct, partially correct, mostly incorrect, or a complete
    fabrication.
  prefs: []
  type: TYPE_NORMAL
- en: Causes of hallucinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much of the data that organizations capture is either **redundant, obsolete,
    trivial** (**ROT**), or altogether unclassified. As a portion, *good* data forms
    a small fraction of the data lakes, warehouses, and databases that most companies
    have. Whenever beginning your GenAI application journey, one of the first things
    you’re likely to notice is that much of the data you’d like to use to train your
    GenAI application is poor quality. Shortly thereafter, you’ll learn that hallucinations
    are caused by **poor-quality** **training data**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Engineers can best think of this as a **garbage in, garbage out** problem.
    When training data has errors, inconsistencies, irrelevancy, outdated information,
    biased information, and other issues, the model will learn to replicate those
    problems. The accuracy of an AI model is heavily dependent on the quality of training
    data, and the following data issues are more likely to cause output problems and
    hallucinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inaccurate data**: Errors in the input will propagate and compound in the
    system, so it is critical to know that any automated or real-time data streaming
    to your GenAI application has accurate information. For example, if you’re ingesting
    sensor data to predict when equipment will fail but receive inaccurate sensor
    readings, then your GenAI application may not predict the failure correctly or
    in a timely way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incomplete data**: Training on incomplete datasets can cause the model to
    generate plausible but incorrect content to fill perceived gaps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outdated or obsolete data**: At its heart, obsolete data is often simply
    no longer accurate, providing AI with false information. Relevant data updates
    ensure that your GenAI application continues to provide your users with accurate
    outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Irrelevant data**: It can be tempting to stuff your GenAI application with
    as much data as possible so that it can use that information for analysis; however,
    this is a way to increase costs without improving accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misleading or misrepresentative data**: If a machine learning model is trained
    on images that are poorly labeled or unrepresentative of real-world scenarios,
    it will struggle to correctly identify or classify images when deployed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duplicated data**: This also includes poorly integrated datasets. Redundant
    data can give AI the impression that something is more important than it is because
    it’s repeated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model architecture and objectives**: Models such as GPT-4 are trained to
    predict the next word in a sequence based on context, and not necessarily to verify
    facts. This objective can lead to the model generating fluent text that is not
    factually accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these causes slightly different issues and, in combination, can make
    your GenAI application incapable of producing satisfactory results. Therefore,
    your training data must be accurate, comprehensive, and representative of the
    diverse conditions the model will encounter in real-world applications. Much of
    GenAI is continuously self-learning, so maintaining data quality is an ongoing
    issue, not a **first-deploying-to-production** issue.
  prefs: []
  type: TYPE_NORMAL
- en: Generative models focus on producing outputs that are coherent and contextually
    relevant, which sometimes comes at the expense of factual correctness. These models
    are also excellent at recognizing and replicating patterns in data. However, this
    can result in outputs that follow learned patterns even when those patterns do
    not align with factual reality. This is the **correlation, not** **causation**
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: Also, models are trained on static datasets and lack real-time access to updated
    information, which can lead to outdated or incorrect outputs. For instance, GPT
    and its ilk are trained on data scraped from the web several months (or even years!)
    ago. Products, insights, and world news from yesterday are not available. When
    asking questions about recent events, in the best case, the user receives an answer
    such as `I do not have this information`. In the worst case, the GenAI application
    simply hallucinates a response. Generative models may not fully understand the
    context or possess the real-world knowledge required to validate the correctness
    of generated information.
  prefs: []
  type: TYPE_NORMAL
- en: Implications of hallucinations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides just “being wrong” and “making up answers,” hallucinations can have
    other unexpected implications. Misinformation can easily be propagated to thousands
    of people, some of whom may find it difficult to turn around later. For instance,
    if today, ChatGPT (a popular GenAI model) started telling every person who asked
    that a popular open-source project has a critical vulnerability, then the news
    would spread like wildfire, making damage control difficult. It would reach many
    more people than the statement put out on the company blog about how the information
    wasn’t true. Many users trust the AI’s output without verification.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations undermine the reliability of AI systems, particularly in fields
    such as healthcare, legal, or financial services, where accuracy is paramount.
    Moreover, consistent hallucinations can erode user trust in AI applications, leading
    to reduced adoption and skepticism regarding AI capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect information can lead to ethical dilemmas and potential legal liabilities,
    especially if the AI’s output influences critical decisions or public opinion.
    As GenAI is added into all sorts of applications, it becomes more and more difficult
    to both opt out of (for the users) and discern whether the answers are legitimate.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth saying also that receiving an answer that is not a hallucination
    is far different from receiving the best answer.
  prefs: []
  type: TYPE_NORMAL
- en: Sycophancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A sycophant is a person who does whatever they can to win your approval, even
    at the cost of their ethics or knowledge of what is true. AI models demonstrate
    this behavior often enough for AI researchers and developers to use the same term—**sycophancy**—to
    describe how models respond to human feedback and prompting in deceptive or problematic
    ways. Human feedback is commonly utilized to fine-tune AI assistants. But human
    feedback may also encourage model responses that match user beliefs over truthful
    ones, a trait known as sycophancy. Sycophancy exists in multiple ways, such as
    mirroring feedback, easily being swayed, and changing correct answers if the user
    pushes back. If users share their beliefs and views on a topic, AI assistants
    will provide answers that align with the user’s beliefs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sycophancy can be observed and described on multiple levels, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedback sycophancy**: When users express likes or dislikes about a text,
    AI assistants may provide more positive or negative feedback accordingly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swaying easily**: After answering a question correctly, AI assistants may
    change their answer when users challenge them, even if the original answer was
    correct'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Belief conformity**: When users share their views on a topic, AI assistants
    tend to provide answers that align with those beliefs, leading to decreased accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In testing, researchers Mrinank Sharma et al. demonstrated sycophantic answers
    generated by Claude ([https://arxiv.org/abs/2310.13548](https://arxiv.org/abs/2310.13548)),
    as shown in *Figure 11**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Example responses demonstrating sycophancy'
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that repeated testing of the same and similar questions in
    ChatGPT did not yield consistent results.
  prefs: []
  type: TYPE_NORMAL
- en: Causes of sycophancy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The exact causes of sycophancy are not well understood. This phenomenon exists
    in many LLMs because these models have been instructed to take in contextual and
    parametric information to inform their responses. GenAI applications have a *learning*
    feature where the more they interact with users, the more they learn about syntax,
    context, and providing sufficient answers. As they do so, the applications exhibit
    what can only be described as *people-pleasing behaviors*, causing them to deviate
    from a purely factual relaying of information.
  prefs: []
  type: TYPE_NORMAL
- en: In the above research, it was found that sycophancy is a side effect of RLHF-like
    alignment training. **Reinforcement learning from human feedback** (**RLHF**)
    is a technique that is used to train LLMs to align the agent or machine with human
    preferences. This is particularly important in areas such as language models.
    To illustrate this, let’s look at some examples of what this means and why it
    matters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: When you greet a coworker, you might say “Hello, sir/madam,” “Hello,” “Good
    morning,” “Good day,” “Hi,” “What’s up,” “Greetings,” or many other potential
    salutations. Hypothetically, all are appropriate, but there are human preferences
    as to which is more suitable.
  prefs: []
  type: TYPE_NORMAL
- en: To further understand this, let’s begin with cultural preference. In some cultures,
    it would be shocking indeed if you did not include the coworker’s name, as in
    “Good morning, Mr. Smith.” Yet in other cultures, to address someone in this manner
    would seem exceedingly strange. The human preference on which greeting is preferred
    has some basis, part of which is cultural, part of which is situational and contextual
    (is Mr. Smith the president? Is he your 20-year-old new hire?), and part of which
    is purely you, the individual.
  prefs: []
  type: TYPE_NORMAL
- en: Engineers decided that when people interact with GenAI, they prefer that their
    conversations and interactions feel human. To do that, the machines must consider
    cultural, situational, behavioral, and, to some extent, individual preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Training models have access to vast amounts of information, both contextual
    (passages of text from websites, books, research, etc.) and parametric (embeddings
    of nearest-neighbor words). They will use any cultural, contextual, or behavioral
    clues that the user provides to help inform their answer. That is, how the user
    phrases the question influences the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT confirms this. When asked how it arrives at an answer, it states the
    following clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is possible to request GenAI to disregard your previous interactions, personal
    preferences, syntax, and/or any data it has concluded about you before it creates
    answers to your questions, but, of course, this would require the user to know
    that this is happening in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Implications of sycophancy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As helpful as this functionality is, it has real-world implications for the
    outputs of GenAI applications. In the same research paper cited earlier in this
    chapter ([https://arxiv.org/abs/2310.13548](https://arxiv.org/abs/2310.13548)),
    it was determined that the consequences of sycophancy, while machine in origin,
    can result in incorrect deference to user opinion, propagation of user-created
    errors, and biased responses. Therefore, instead of helping create a more factual
    and consistent understanding of the world, GenAI perpetuates and perhaps accelerates
    the spread of misinformation.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers at Google DeepMind found that the problem grew worse as the model
    became bigger ([https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/](https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/)).
    LLMs with more parametric inputs had a greater tendency to agree with objectively
    false statements than smaller ones. This tendency held true even for mathematical
    equations, that is, questions where there is only one correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are constantly learning, evolving, and being improved by their creators.
    In the future, perhaps LLMs will weigh the objective truth of a statement higher
    than the opinion or preferences of the user, but as of 2023, that is yet to happen.
    Ongoing research and testing will make them ever more adept at balancing user
    expectations, user opinions, and facts. Still, as of the time this book was written,
    sycophancy remains a primary concern with GenAI applications, particularly where
    the outputs consider opinions and user preferences before generating their response.
    Further testing using synthetic data and retraining models has reduced the tendency
    of sycophancy by up to 10%, which is still not 100% ([https://arxiv.org/abs/2308.03958](https://arxiv.org/abs/2308.03958)).
    This means that the tendency persists, even with fairly substantial modifications
    to the fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data leakage**, in the context of GenAI, refers to situations where information
    from outside the desired training dataset is used to create the model, leading
    to overly optimistic performance metrics and potentially flawed or misleading
    predictions. This can happen at various stages of model development, from data
    collection to model evaluation, and can significantly compromise the validity
    of the AI system. There are multiple types of datasets with different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Training datasets, which are used to train the LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning datasets, which can be used to improve LLM responses and reduce
    hallucinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation datasets, which can be useful in evaluating the accuracy of responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causes of data leakage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The causes of data leakage are straightforward and easily avoided, as long
    as the developers of these applications are aware of these causes. First, let’s
    understand at a high level what leads to data leakage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inappropriate dataset overlap**: Each dataset should be used at the appropriate
    training and evaluation stage. When this is not true, you have data leakage. For
    example, when the training dataset overlaps with the evaluation dataset, GenAI
    applications will, of course, perform better during testing because they already
    know the exact answers. In this scenario, your stock price predictor application
    would have had duplicated historical data points present in its training and evaluation
    datasets; therefore, its performance when testing its outputs will be unrealistically
    high because it has already seen the answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Future information**: Each dataset should only include information that would
    be available at the time of prediction. For instance, you would not include real
    or hypothetical information in your training dataset from a period in the future,
    or data that the model would not typically have access to in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data normalization and transformation efforts**: When transformations or
    feature-engineering steps inadvertently introduce data from outside the training
    set, it is possible for information to leak from evaluation datasets into the
    training process. For GenAI, you want training data that is as close to *real
    life* as possible, both in terms of user interaction and whatever context the
    application will be operating within, so that your application has truly representative
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To illustrate these causes, let’s use a hypothetical GenAI application that
    predicts stock prices upon request using historical data. In this scenario, it
    is May 2024, and your application is in the final testing phases. Before pushing
    to production, you’d like to determine how accurate its predictions are. You begin
    by checking your application’s response to the following user request.
  prefs: []
  type: TYPE_NORMAL
- en: '**User request**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Output answer**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The **training** data fed to the model should **not** include any data points
    from May 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **evaluation** dataset should include all prices from May 2024 and could
    include the actual calculated value of the average stock price. This is because
    you would like to compare the model’s estimate to the actual value, and then give
    it a score for accuracy, then plot that month over month, in order to see whether
    the application consistently makes low or high estimates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re trying for accuracy with your May 2024 estimate, but you’ve already
    fed it the May 2024 data in the training phase, this would be considered inappropriate
    dataset overlap. Let’s look at another example.
  prefs: []
  type: TYPE_NORMAL
- en: '**User request**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Output answer**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You would not provide a training dataset that already includes an annual average
    because that information is not yet available. While you could include a year-to-date
    average in the training dataset, you should not include an annual average with
    synthetic or generated forward-looking data. If you created an estimated annual
    stock price and included that in the training data, then you would be using future
    information. Now, let’s consider a final example.
  prefs: []
  type: TYPE_NORMAL
- en: '**User request**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Output answer**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the user query is worded differently here as compared to the first
    example, though it leads to the same answer. LLMs are quite skilled at inferring
    user intention. Remember that users asking even fairly simple questions will phrase
    them in many different ways (*estimate*, *predict*, *forecast*, *imagine*, *guess*,
    and *projection* are all words they might use).
  prefs: []
  type: TYPE_NORMAL
- en: For your training dataset, you might include a prompt-and-answer pairing in
    the style of **frequently asked questions** (**FAQs**) for your entire support
    database. However, resist the urge to correct aspects such as wording and spelling.
    While you want to be aware of “garbage in, garbage out” problems, you do not want
    to shield your GenAI application so much that it won’t know how to respond when
    your users inevitably input garbage. This is particularly relevant for GenAI chatbots.
    Users have so many ways of asking a question. Those questions are presented usually
    without proper syntax, terminology, or contextual awareness, and their knowledge
    may also be outdated. Data normalization and transformation efforts should not
    normalize and cleanse your training data so much so that it becomes less useful.
  prefs: []
  type: TYPE_NORMAL
- en: Implications of data leakage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implications of data leakage vary widely, depending on whether you’ve leaked
    a teardrop or a waterfall. If there is data leakage, then the results of your
    GenAI evaluation and testing prior to production will be wrong and misrepresentative
    of your application’s actual performance, leading to overly optimistic tests or
    misleading conclusions. In all data overlap cases, the most obvious consequence
    of overlapping the training and test datasets is that the model may learn to simply
    *memorize* the training data and perform poorly on any new data from which it
    must make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: This can give application developers and testers a false sense of confidence
    in the model’s performance. Later, when real-world data is offered and users are
    asking questions in production, the application will perform markedly worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Avoiding data leakage is simple, and it begins with splitting your datasets
    into distinct entities, then doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that training, validation, and test datasets are strictly separated.
    Use techniques such as time-based splitting for time-series data to prevent future
    information from leaking into the training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use tools to ensure that data transformations are only applied to the training
    set during model training and applied to the test set independently during evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineer features in a way that prevents future data from being used. Avoid
    using future values or aggregated future statistics as part of your training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returning to the stock price prediction application, you would ideally want
    the data for your training and test sets to be based on time, ensuring that stock
    prices in the training set occur chronologically before those in the test set.
    Then, your application would only have features that were used in the historical
    stock data available up to the point of the stock price being predicted, marking
    a clear delineation between authentic prior stock prices and predicted future
    stock prices. Next, to validate your application, use time-based cross-validation
    to ensure that model performance is evaluated on data that simulates real-world
    prediction scenarios or the scenarios your application would allow.
  prefs: []
  type: TYPE_NORMAL
- en: By rigorously managing how data is handled throughout the model development
    process, you can minimize the risk of data leakage and ensure that your GenAI
    model provides reliable and valid predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With so many distinct, complex, and potentially expensive moving parts, it is
    critical for engineers to know the costs of their GenAI application and how to
    contain these costs. While you will learn more about cost optimization strategies
    in [*Chapter 12*](B22495_12.xhtml#_idTextAnchor253), *Correcting and Optimizing
    Your Generative AI Application*, this section will serve as an introduction to
    understanding the financial costs of GenAI applications, which are in some ways
    different from web development applications.
  prefs: []
  type: TYPE_NORMAL
- en: Types of costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using GenAI, costs can arise from several different areas. These costs
    can be broadly categorized into computational, storage, data acquisition, development,
    and maintenance costs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training costs**: Training GenAI models requires significant computational
    resources. This is especially true for large models such as GPT-4\. These resources
    often include **graphics processing units** (**GPUs**) or **tensor processing
    units** (**TPUs**), which are optimized for parallel processing tasks. The infrastructure
    to support these setups consumes a lot of electricity and requires cooling systems
    to maintain operational temperatures. Most engineers may not be in the position
    to pay these costs and, instead, will utilize models from vendors, such as OpenAI,
    Anthropic, Google, Meta, or others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference, or real-time computation**: Generating responses or outputs from
    a trained model, which is called **inference**, also incurs computational costs,
    especially for models that need to provide real-time answers. Bigger models cost
    more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage costs**: Storing large datasets required for training GenAI models
    incurs costs. This includes raw data, preprocessed data, user interaction data,
    observability data, and the models themselves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data collection**: Acquiring high-quality datasets can be expensive. This
    can include purchasing data from third-party providers or generating proprietary
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data labeling and cleaning**: Preprocessing data to ensure it is suitable
    for training involves costs. This can include paying for human annotators to label
    data or developing algorithms to clean and prepare the data as either training
    or evaluation datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software development**: Writing and maintaining the code base for training
    and deploying GenAI applications requires skilled engineers and data analysts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experimentation and testing**: Developing GenAI often involves extensive
    experimentation and fine-tuning, which requires time and resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data updates**: Training and evaluation datasets require periodic updates
    to maintain their accuracy and relevance, which involves additional computational
    and human resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and support**: The continuous monitoring of AI systems to ensure
    they are performing correctly and handling issues as they arise involves operational
    costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance and security**: Ensuring data privacy and security and complying
    with regulations (such as GDPR) involves additional costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not an exhaustive list. Therefore, estimating your expected costs is
    complex and a non-trivial endeavor. But let’s hone in on the most important cost
    driver, which is text, and therefore tokens. Next, you will learn how to estimate
    and control costs here.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs process text using **tokens**, which are common sequences of characters
    found in a set of text. Tokens are the currency of the GenAI application. Each
    user input and output is a *token*, and both the question and response token count
    can be controlled. The cost per token is tiny. GenAI vendors look to make their
    money *per transaction*, which can add up quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand this concept with an example. The statement `Hello how are
    you` is 5 tokens. A helpful rule of thumb is that one token generally corresponds
    to ~4 characters of text for common English text. This translates to roughly ¾
    of a word (so, 100 tokens ~= 75 words). The example of `Hello how are you` has
    18 characters including spaces, therefore 18/4 = 4.5, ergo 5 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Each input and output for the GenAI application is reduced down to this simple
    unit of measurement.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT and other LLMs have a token limit, thus capping how much text the user
    can enter as their prompt and limiting the output response. These limits, however,
    are generous. For most use cases, it is unlikely the average consumer would hit
    these limits.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, GPT-4 has a token limit of 32,768 per interaction and an estimated
    word count of 25,000 words, whereas Claude 3 (the LLM hosted by Anthropic) has
    a token limit of 100,000+ as of the time of writing. For a simple customer service
    chatbot, it is very unlikely that you would hit this limit, but it *is* possible.
    Let’s look at two examples to explain how this might be true.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: A customer asks a GenAI chatbot a simple question.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inquiry**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Response**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The initial inquiry is 36 characters and 7 tokens. The response is 162 tokens,
    or 741 characters. If you were to have interactions limited to 300 tokens, you
    would still be well under the limit that you have allowed for your users.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 2**: A server experiences an out-of-memory error, and the GenAI automatically
    analyzes the stack traces and logs an analysis for a human to review later.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inquiry**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Response**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This analyzer is useful to a diagnostics engineer, as the analyzer quickly reviews
    a stack trace, summarizes its findings, and generates recommendations for solving
    the issue.
  prefs: []
  type: TYPE_NORMAL
- en: However, the prompt is 275 tokens (1,240 characters), and the response is 248
    tokens (1,205 characters). As this is a sample stack trace, the real-life implementation
    would possibly be more detailed with real information. So, if the control setting
    was still 300 tokens for input or output, you would be quite close to this limit
    already.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the input and output token limits for your application is critically
    important. While you want to control costs, you also do not want to fundamentally
    limit functionality. If the token limit is too low, the LLM may not be able to
    generate the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: Performance issues in generative AI applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most obvious failures of GenAI are performance- and reliability-related
    issues. Since you’ve learned about accuracy in [*Chapter 10*](B22495_10.xhtml#_idTextAnchor214),
    *Refining the Semantic Data Model to Improve Accuracy*, performance in this chapter’s
    context means slowness. If a user asks your AI application a question and there
    is either no response, a metered response, or a partial response, it is typically
    much more apparent than if the response was hallucinated or sycophantic.
  prefs: []
  type: TYPE_NORMAL
- en: Several factors can contribute to the slowness of a GenAI application. Some
    of the most common causes of performance issues in GenAI are computational load,
    network latency, model serving strategies, and high **input/output** (**I/O**)
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: There can be many more causes, of course. The rest of this section will explain
    some of these performance killers in detail and their impact on your application
    and users.
  prefs: []
  type: TYPE_NORMAL
- en: Computational load
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you already know, LLMs require significant computational power. The time
    required to generate responses to queries increases with the complexity and the
    size of the model. Poorly formed requests significantly increase the computational
    load for a GenAI application. Let’s look at a few examples of this so that you’re
    able to understand how this failure mode can happen.
  prefs: []
  type: TYPE_NORMAL
- en: Extensive data processing and calculations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Requests that require processing large datasets or performing extensive calculations
    can be computationally demanding, as happens in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: '**User request**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Fetching 20,000 random stock prices sounds simple, but the user does not specify
    a timeframe. For what period should the model evaluate the last 20,000 stock prices?
    Over the last month? Last year at random? The sorting of those values is computationally
    expensive and adds further processing to the returned list.
  prefs: []
  type: TYPE_NORMAL
- en: High-complexity requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Complex requests that involve evaluating a large amount of data, summarizing,
    and then returning many results are also taxing. Often, this involves chaining
    multiple LLM calls through advanced prompting techniques, such as the ReACT pattern
    and function calling.
  prefs: []
  type: TYPE_NORMAL
- en: The **reasoning and acting** (**ReACT**) pattern is an advanced prompting technique
    used in GenAI models to handle complex tasks that require multiple steps of reasoning
    and interaction. This pattern involves a sequence where the model reasons about
    the task, generates intermediate actions, and then produces the final output.
    The ReACT pattern helps the model break down complex requests into manageable
    steps, improving accuracy and coherence in the final response.
  prefs: []
  type: TYPE_NORMAL
- en: '**Function calling** in the context of LLMs involves instructing the model
    to execute specific functions or actions as part of its response generation process.
    This can be particularly useful for tasks that require structured outputs, calculations,
    data retrieval, or interactions with external systems. As an example, the developer
    specifies functions within the prompt that the model can call to perform specific
    tasks. These functions are predefined and can handle various operations, such
    as querying databases, performing calculations, or fetching external data.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a high-complexity request to illustrate this.
  prefs: []
  type: TYPE_NORMAL
- en: '**User request**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this scenario, the GenAI must first create the list of every US president,
    then seek information about each one, and then create a detailed summary of their
    policies and events during their terms in office. It must also retrieve content
    related to which things the presidents prioritized, identify consensus on what
    pieces of content were the top priorities, compile and summarize all that information,
    and then output it to the user. This is extensive knowledge retrieval, analysis,
    and text generation. Most likely, this information would require multiple LLM
    queries, and more queries equate to more spend.
  prefs: []
  type: TYPE_NORMAL
- en: These examples illustrate how certain types of user requests can significantly
    increase the computational load for GenAI applications. Let’s now see how model
    serving strategies can impact GenAI performance.
  prefs: []
  type: TYPE_NORMAL
- en: Model serving strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generating responses for every request individually can be inefficient, depending
    on volume. If the application is not designed to handle multiple requests concurrently,
    it will become slower the more users you have. If the application relies on cloud-based
    services, network latency can affect performance. Slow internet connections or
    high latency between the client and the server can cause delays. Frequent or complex
    API calls to external services can add to the response time, especially if those
    services are experiencing a high load or are geographically distant.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to the stock predictor application for an example.
  prefs: []
  type: TYPE_NORMAL
- en: Because your GenAI application receives some news coverage, your website experiences
    a surge in traffic, and the number of customers interacting with the application
    increases dramatically. But, since your application handles each request individually
    and cannot process multiple requests concurrently, the response time for each
    user increases as the system becomes overwhelmed. Users experience slower response
    times, leading to frustration.
  prefs: []
  type: TYPE_NORMAL
- en: The news coverage was from an influencer in Sydney, Australia, so the surge
    in users is from Asia. Your servers are in the US East region, and network latency
    due to the geographical distance between the server and the clients causes delays.
    Customers with slow internet connections experience even longer wait times, further
    degrading the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Your application frequently calls external APIs to fetch real-time data for
    stock prices and financial market news. If these external services are experiencing
    high load, the API calls take longer to complete.
  prefs: []
  type: TYPE_NORMAL
- en: High I/O operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Poor data-handling practices, such as reading large datasets inefficiently
    or not using appropriate data structures, can slow down performance. Frequent
    read/write operations to disk can be a bottleneck, as can poorly optimized database
    interactions and malformed queries. The example stock price predictor application
    frequently reads large historical stock price datasets to make predictions. Let’s
    walk through some potential issues with data handling that result in high I/O
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: The application reads large datasets inefficiently, such as loading the entire
    dataset into memory even when only a subset is needed, which consumes excessive
    memory and processing power, slowing down performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application saves intermediate prediction results and logs to disk after
    every prediction cycle. Frequent read/write operations to disk form a bottleneck,
    which significantly increases the time it takes to complete each prediction cycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application queries a database to fetch recent financial news and other
    relevant data before making predictions. However, a lack of indexes means that
    query results are slowly delivered. This increases response times, making the
    application slow to respond to user requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming you have a large dataset, you’ll want to avoid these practices as they
    will affect user experience and increase costs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have navigated through these GenAI challenges, you can appreciate
    some of the complexities and nuances that accompany these powerful technologies.
    The issues of hallucinations, sycophancy, data leakage, cost, and performance
    present formidable obstacles that demand a critical eye and innovative solutions.
    Each challenge offers a unique perspective on the limitations and potential pitfalls
    inherent in GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite these hurdles, GenAI remains unequivocally valuable. It continues to
    transform industries, enhance productivity, and open new avenues for creativity
    and innovation. By understanding and addressing these challenges, developers can
    harness the full potential of GenAI, delivering robust, reliable, and responsible
    applications. At the same time, it’s also important to note that applications
    can be useful even when they are not always correct. To take ChatGPT as an example:
    it has greatly improved the productivity of millions of users already, even though
    its deficiencies are well-known (and some not so easily worked around). Your GenAI
    application could be just as useful and popular but with similar caveats.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you’ll look at ways to optimize your GenAI application,
    improving its outputs and performance for a better user experience as well as
    combatting some of the issues discussed here.
  prefs: []
  type: TYPE_NORMAL
