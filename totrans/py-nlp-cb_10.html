<html><head></head><body>
		<div><h1 id="_idParaDest-257" class="chapter-number"><a id="_idTextAnchor277" class="calibre6 pcalibre pcalibre1"/>10</h1>
			<h1 id="_idParaDest-258" class="calibre7"><a id="_idTextAnchor278" class="calibre6 pcalibre pcalibre1"/>Generative AI and Large Language Models</h1>
			<p class="calibre3">In this chapter, we will explore recipes that use the generative aspect of the transformer models to generate text. As we touched upon the same in <a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 8</em></a>, <em class="italic">Transformers and Their Applications</em>, the generative aspect of the transformer models uses the decoder component of the transformer network. The decoder component is responsible for generating text based on the provided context.</p>
			<p class="calibre3">With the advent of the <strong class="bold">General Purpose Transformers</strong> (<strong class="bold">GPT</strong>) family of <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>), these have only grown in size and capability with each new version. LLMs such as GPT-4 have been trained on large corpora of text and can match or beat their state-of-the-art counterparts in many NLP tasks. These LLMs have also built upon their generational capability and they can be instructed to generate text based on human prompting.</p>
			<p class="calibre3">We will use generative models based on the transformer architecture for our recipes.</p>
			<p class="calibre3">This chapter contains the following recipes:</p>
			<ul class="calibre15">
				<li class="calibre14">Running an LLM locally</li>
				<li class="calibre14">Running an LLM to follow instructions</li>
				<li class="calibre14">Augmenting an LLM with external data</li>
				<li class="calibre14">Augmenting the LLM with external content</li>
				<li class="calibre14">Creating a chatbot using an LLM</li>
				<li class="calibre14">Generating code using an LLM</li>
				<li class="calibre14">Generating a SQL query using human-defined requirements</li>
				<li class="calibre14">Agents – making an LLM to reason and act</li>
			</ul>
			<h1 id="_idParaDest-259" class="calibre7"><a id="_idTextAnchor279" class="calibre6 pcalibre pcalibre1"/>Technical requirements</h1>
			<p class="calibre3"><a id="_idTextAnchor280" class="calibre6 pcalibre pcalibre1"/>The code for this chapter is in a folder named <code>Chapter10</code> in the GitHub repository of the book (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10</a>).</p>
			<p class="calibre3">As in previous chapters, the packages required for this chapter are part of the <code>poetry</code>/<code>pip</code> requirements configuration file that is present in the repository. We recommend that the reader set up the environment beforehand.</p>
			<h2 id="_idParaDest-260" class="calibre5"><a id="_idTextAnchor281" class="calibre6 pcalibre pcalibre1"/>Model access</h2>
			<p class="calibre3">In this chapter, we will use models from Hugging Face and OpenAI. The following are the instructions to enable model access for the various models that will be used for the recipes in this chapter.</p>
			<p class="calibre3"><strong class="bold">Hugging Face Mistral model</strong>: Create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. Please visit the Mistral model details at <a href="https://huggingface.co/mistralai/Mistral-7B-v0.3" class="calibre6 pcalibre pcalibre1">https://huggingface.co/mistralai/Mistral-7B-v0.3</a>. You will need to request access to the model on the site before running the recipe that uses this model.</p>
			<p class="calibre3"><strong class="bold">Hugging Face Llama model</strong>: Create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. Please visit the Llama 3.1 model details at <a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct" class="calibre6 pcalibre pcalibre1">https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct</a>. You will have to request for the model access on the site before you run the recipe that uses this model.</p>
			<p class="calibre3">In the code snippets, we are using Jupyter as an environment for execution. If you are using the same, you will something like the screenshot shown here. You can enter the token in the text field and let the recipe make progress. The recipe will wait for the token to be entered the first time. Subsequent runs of the recipe will use the cached token that the Hugging Face library creates for the user locally.</p>
			<div><div><img src="img/B18411_10_1.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Copying a token from Hugging Face</p>
			<p class="calibre3"><code>api-token</code>. In the code snippets, we are using Jupyter as an environment for execution. If you are using the same, you will see a text box where you will need to enter the <code>api-token</code>. You can enter the token in the text field and let the recipe make progress. The recipe will wait for the token to be entered.</p>
			<h1 id="_idParaDest-261" class="calibre7">R<a id="_idTextAnchor282" class="calibre6 pcalibre pcalibre1"/>unning an LLM locally</h1>
			<p class="calibre3">In this recipe, we will learn how to load an LLM locally using the CPU or GPU and generate text from it after giving it a starting text as seed input. An LLM running locally can be instructed to generate text based on prompting. This new paradigm of generation of text via instruction prompting has brought the LLM to recent prominence. Learning to do this allows for<a id="_idIndexMarker585" class="calibre6 pcalibre pcalibre1"/> control over hardware resources and environment setup, optimizing performance and enabling rapid experimentation or prototyping with text generation from seed inputs. This enhances data privacy and security, along with a reduced reliance on cloud services, and facilitates cost-effective deployment for educational and practical applications. As we run an LLM locally as part of the recipe, we will use instruction prompting to make it generate text based on a simple instruction.</p>
			<h2 id="_idParaDest-262" class="calibre5"><a id="_idTextAnchor283" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We recommend that you use a system with at least 16 GB of RAM or a system with a GPU that has at least 8 GB of VRAM. These examples were created on a system with 8 GB of RAM and an nVidia <a id="_idIndexMarker586" class="calibre6 pcalibre pcalibre1"/>RTX 2070 GPU with 8 GB of VRAM. These examples will work without a GPU as long as there is 16 GB of system RAM. In this recipe, we <a id="_idIndexMarker587" class="calibre6 pcalibre pcalibre1"/>will load the <strong class="bold">Mistral-7B</strong> model using the Hugging Face (<a href="https://huggingface.co/docs" class="calibre6 pcalibre pcalibre1">https://huggingface.co/docs</a>) libraries. This model has a smaller size compared to other language models in its class but can outperform them on several NLP tasks. The Mistral-7B model with 7 billion network parameters can<a id="_idIndexMarker588" class="calibre6 pcalibre pcalibre1"/> outperform the <strong class="bold">Llama2</strong> model, which has over 13 billion parameters.</p>
			<p class="calibre3">It is required that the user create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. Please refer to <em class="italic">Model access</em> under the <em class="italic">Technical requirements</em> section to complete the step to access the Mistral model. Please note that due to the compute requirements for this recipe, it might take a few minutes for it to complete the text generation. If the required compute capacity is unavailable, we recommend that the reader refer to the <em class="italic">Using OpenAI models instead of local ones</em> section at the end of this chapter and use the method described there to use an OpenAI model for this recipe.</p>
			<h2 id="_idParaDest-263" class="calibre5"><a id="_idTextAnchor284" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, GenerationConfig)
import torch</pre></li>				<li class="calibre14">In this step, we set up the login for Hugging Face. Though we can set the token directly in the code, we recommend setting the token in an environment variable and then reading from it in the notebook. Calling the <strong class="source-inline1">login</strong> method with the token authorizes the call to Hugging Face and allows the code to download the model locally and use it:<pre class="source-code">
from huggingface_hub import login
hf_token = os.environ.get('HUGGINGFACE_TOKEN')
login(token=hf_token)</pre></li>				<li class="calibre14">In this step, we initialize the device, the <strong class="source-inline1">mistralai/Mistral-7B-v0.3</strong> model, and the tokenizer, respectively. We set the <strong class="source-inline1">device_map</strong> parameter to <strong class="source-inline1">auto</strong>, which lets the pipeline pick the available device to use. We set the <strong class="source-inline1">load_in_4bit</strong> parameter to <strong class="source-inline1">True</strong>. This lets us load the quantized model for the inference (or generation) step. Using a quantized model consumes less memory and lets us load the model locally on systems with limited memory. The loading of the quantized model is handled by the <strong class="source-inline1">AutoModelForCausalLM</strong> module, and it downloads <a id="_idIndexMarker589" class="calibre6 pcalibre pcalibre1"/>a model from the Hugging Face hub that has been quantized to the bit size specified in the parameter:<pre class="source-code">
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.3", device_map="auto", 
        load_in_4bit=True)
tokenizer = AutoTokenizer.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    padding_side="left")</pre></li>				<li class="calibre14">In this step, we initialize a generation config. This generation config is passed to the model, instructing it on how to generate the text. We set the <strong class="source-inline1">num_beams</strong> parameter to <strong class="source-inline1">4</strong>. This parameter results in the generated text being more coherent and grammatically correct as the number of beams is increased. However, a greater number of beams also results in decoding (or text-generation) time. We set the <strong class="source-inline1">early_stopping</strong> parameter to <strong class="source-inline1">True</strong> as the generation of the next word is concluded as soon as the number of beams reaches the value specified in the <strong class="source-inline1">num_beams</strong> parameter. The <strong class="source-inline1">eos_token_id</strong> (e.g., <strong class="source-inline1">50256</strong> for GPT models) and <strong class="source-inline1">pad_token_id</strong> (e.g., <strong class="source-inline1">0</strong> for GPT models) are defaulted to use the model’s token IDs. These token IDs are used to specify the end-of-sentence and padding tokens that will be used by the model. The <strong class="source-inline1">max_new_tokens</strong> parameter specifies the maximum number of tokens that will be generated. There are more parameters that can be specified for generating the text and we encourage you to play around with different values of the previously specified parameters, as well as any additional parameters for <a id="_idIndexMarker590" class="calibre6 pcalibre pcalibre1"/>customizing the text generation. For more information, please refer to the transformer documentation on the <strong class="source-inline1">GenerationConfig</strong> class at <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py" class="calibre6 pcalibre pcalibre1">https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py</a>:<pre class="source-code">
generation_config = GenerationConfig(
    num_beams=4,
    early_stopping=True,
    eos_token_id=model.config.eos_token_id,
    pad_token_id=model.config.eos_token_id,
    max_new_tokens=900,
)</pre></li>				<li class="calibre14">In this step, we initialize a seed sentence. This seed sentence acts as a prompt to the model asking it to generate a step-by-step way to make an apple pie:<pre class="source-code">
seed_sentence = "Step by step way on how to make an apple pie:"</pre></li>				<li class="calibre14">In this step, we tokenize the seed sentence to transform the text into the corresponding embedded representation and pass it to the model to generate the text. We also pass the <strong class="source-inline1">generation_config</strong> instance to it. The model generates the token IDs as part of its generation:<pre class="source-code">
model_inputs = tokenizer(
    [seed_sentence], return_tensors="pt").to(device)
generated_ids = model.generate(**model_inputs,
    generation_config=generation_config)</pre></li>				<li class="calibre14">In this step, we decode the token IDs that were generated from the previous step. The transformer <a id="_idIndexMarker591" class="calibre6 pcalibre pcalibre1"/>model uses special tokens such as <strong class="source-inline1">CLS</strong> or <strong class="source-inline1">MASK</strong> and to generate the text as part of the training. We set the value of <strong class="source-inline1">skip_special_tokens</strong> to <strong class="source-inline1">True</strong>. This allows us to omit these special tokens and generate pure text as part of our output. We print the decoded (or generated) text.<pre class="source-code">
generated_tokens = tokenizer.batch_decode(generated_ids,
    skip_special_tokens=True)[0]
print(generated_tokens)</pre><p class="calibre3">The output would look like the following. We have shortened the output for brevity. You might see a longer result:</p></li>			</ol>
			<pre class="console">
Step by step way on how to make an apple pie:
1. Preheat the oven to 350 degrees Fahrenheit.
2. Peel and core the apples.
3. Cut the apples into thin slices.
4. Place the apples in a large bowl.
5. Add the sugar, cinnamon, and nutmeg to the apples.
6. Stir the apples until they are evenly coated with the sugar and spices.
7. Pour the apples into a pie dish.
8. Place the pie dish on a baking sheet.
9. Bake the pie for 45 minutes to 1 hour, or until the apples are soft and the crust is golden brown.
10. Remove the pie from the oven and let it cool for 10 minutes before serving.
## How do you make an apple pie from scratch?
To make an apple pie from scratch, you will need the following ingredients:
- 2 cups of all-purpose flour
- 1 teaspoon of salt
- 1/2 cup of shortening
- 1/2 cup of cold water
- 4 cups of peeled, cored, and sliced apples
- 1 cup of sugar
- 1 teaspoon of cinnamon
- 1/4 teaspoon of nutmeg
- 1/4 teaspoon of allspice
- 2 tablespoons of cornstarch
- 1 tablespoon of lemon juice
To make the pie crust, combine the flour and salt in a large bowl. Cut in the shortening with a pastry blender or two knives until the mixture resembles coarse crumbs. Add the cold water, 1 tablespoon at a time, until the dough comes together. Divide the dough in half and shape each half into a disk. Wrap the disks in plastic wrap and refrigerate for at least 30 min<a id="_idTextAnchor285" class="pcalibre pcalibre1 calibre20"/>utes.</pre>			<h1 id="_idParaDest-264" class="calibre7"><a id="_idTextAnchor286" class="calibre6 pcalibre pcalibre1"/>Running an LLM to follow instructions</h1>
			<p class="calibre3">In this recipe, we will learn how to get an LLM to follow instructions via prompting. An LLM can be provided some<a id="_idIndexMarker592" class="calibre6 pcalibre pcalibre1"/> context and asked to generate text based on that context. This is a very novel feature of an LLM. The LLM can be specifically instructed to generate text based on explicit user requirements. Using this feature expands the breadth of use cases and applications that can be developed. The context and the question to be answered can be generated dynamically and used in various use cases ranging from answering simple math problems to sophisticated data extraction from knowledge bases.</p>
			<p class="calibre3">We will use the <code>meta-llama/Meta-Llama-3.1-8B-Instruct</code> model for this recipe. This model is built on top of the <code>meta-llama/Meta-Llama-3.1-8B</code> model and has been tuned to follow instructions via prompts.</p>
			<h2 id="_idParaDest-265" class="calibre5"><a id="_idTextAnchor287" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">It is required that the user create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. Please refer to <em class="italic">Model access</em> under the <em class="italic">Technical requirements</em> section to complete the step to access the Llama model.</p>
			<p class="calibre3">You can use the <strong class="bold">10.2_instruct_llm.ipynb</strong> notebook<a id="_idIndexMarker593" class="calibre6 pcalibre pcalibre1"/> from the code site if you want to work from an existing notebook. Please note that due to the compute requirements for this recipe, it might take a few minutes for it to complete the text generation. If the required compute capacity is unavailable, we recommend that the reader refer to the <em class="italic">Using OpenAI models instead of local ones</em> section at the end of this chapter and use the method described there to use an OpenAI model for this recipe.</p>
			<h2 id="_idParaDest-266" class="calibre5"><a id="_idTextAnchor288" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes an LLM model to be loaded into memory.</li>
				<li class="calibre14">It initializes a prompt to instruct the LLM to perform a task. This task is that of answering a question.</li>
				<li class="calibre14">It sends the prompt to the LLM and asks it to generate an answer.</li>
			</ul>
			<p class="calibre3">The steps for the<a id="_idIndexMarker594" class="calibre6 pcalibre pcalibre1"/> recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
import (
    AutoModelForCausalLM, AutoTokenizer,
    BitsAndBytesConfig, GenerationConfig, pipeline)
import os
import torch</pre></li>				<li class="calibre14">Set up the login for Hugging Face. Set the <strong class="source-inline1">HuggingFace</strong> token in an environment variable and read from it into a local variable. Calling the <strong class="source-inline1">login</strong> method with the token authorizes the call to <strong class="source-inline1">HuggingFace</strong> and allows the code to download the model locally and use it. You will see a similar login window as the one shown in the <em class="italic">Running an LLM locally</em> recipe in this chapter:<pre class="source-code">
from huggingface_hub import login
hf_token = os.environ.get('HUGGINGFACE_TOKEN')
login(token=hf_token)</pre></li>				<li class="calibre14">In this step, we specify the model name. We also define the quantization configuration. Quantization is a technique to reduce the size of the internal LLM network weights to a lower precision. This allows us to load the model on systems with limited CPU or GPU memory. Loading an LLM with its default precision requires a large amount of CPU/GPU memory. In this case, we load the network weights in four bits using the <strong class="source-inline1">load_in_4bit</strong> parameter of the <strong class="source-inline1">BitsAndBytesConfig</strong> class. The other parameters used are described as follows:<ul class="calibre19"><li class="calibre14"><strong class="source-inline1">bnb_4bit_compute_dtype</strong>: This parameter specifies the data type that is used during the computation. Though the network weights are stored in four bits, the computation still happens in 16 or 32 bits as defined by this parameter. Setting this to <strong class="source-inline1">torch.float16</strong> results in speed improvements in certain scenarios.</li><li class="calibre14"><strong class="source-inline1">bnb_4bit_use_double_quant</strong>: This parameter specifies that nested quantization should be used. This means that a second quantization is<a id="_idIndexMarker595" class="calibre6 pcalibre pcalibre1"/> performed which saves an additional 0.4 bits per parameter in the network. This helps us save the memory needed for the model.</li><li class="calibre14"><strong class="source-inline1">bnb_4bit_quant_type</strong>: This <strong class="source-inline1">nf4</strong> parameter value initializes the weights of the network using a normal distribution, which is useful during the training of the model. However, it does not have any impact on inference, such as for this recipe. We will still be setting this to <strong class="source-inline1">nf4</strong> to keep it consistent with the model weights.</li></ul><p class="calibre3">For quantization concepts, we recommend referring to the blog post at <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" class="calibre6 pcalibre pcalibre1">https://huggingface.co/blog/4bit-transformers-bitsandbytes</a>, where this is explained in greater detail. Please note that in order to load the model in 4-bit, it is required that a GPU is used:</p><pre class="source-code">
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type= "nf4"
    )</pre></li>				<li class="calibre14">In this step, we load the <strong class="source-inline1">meta-llama/Meta-Llama-3.1-8B-Instruct</strong>  model and the corresponding tokenizer:<pre class="source-code">
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(model_name)</pre></li>				<li class="calibre14">In this step, we initialize a pipeline that weaves the model and tokenizer together with some additional parameters. We covered the description of these parameters in the <em class="italic">Running an LLM locally</em> recipe in this chapter. We recommend referring to that recipe for more details on these parameters. We are adding an additional parameter named <strong class="source-inline1">repetition_penalty</strong> here. This ensures that the LLM <a id="_idIndexMarker596" class="calibre6 pcalibre pcalibre1"/>does not go into a state where it starts repeating itself or parts of the text that were generated before:<pre class="source-code">
pipe = pipeline("text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    pad_token_id = tokenizer.eos_token_id,
    eos_token_id=model.config.eos_token_id,
    num_beams=4,
    early_stopping=True,
    repetition_penalty=1.4)</pre></li>				<li class="calibre14">In this step, we create a prompt that sets up an instruction context that can be passed to the LLM. The LLM acts as per the instructions set up in the prompt. In this case, we start our instruction with a conversation between the user and the agent. The conversation starts with the question <strong class="source-inline1">What is your favourite country?</strong>. This question is followed by the model answer in the form of <strong class="source-inline1">Well, I am quite fascinated with Peru.</strong>. We then follow it up with another instruction by asking the question <strong class="source-inline1">What can you tell me about Peru?</strong>. This methodology serves as a template for the LLM to learn our intent and generate an answer for the follow-up question based on the pattern we<a id="_idIndexMarker597" class="calibre6 pcalibre pcalibre1"/> specified in our instruction prompt:<pre class="source-code">
prompt = [
    {"role": "user", "content": "What is your favourite country?"},
    {"role": "assistant", "content": "Well, I am quite fascinated with Peru."},
    {"role": "user", "content": "What can you tell me about Peru?"}
]</pre></li>				<li class="calibre14">In this step, we execute the pipeline with the prompt and execute it. We additionally specify the maximum number of tokens that should be generated as part of the output. This explicitly instructs the LLM to stop generation once the specific length is reached:<pre class="source-code">
outputs = pipe(
    prompt,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"][-1]['content'])</pre><p class="calibre3">This will result in the following output:</p></li>			</ol>
			<pre class="console">
Peru! A country with a rich history, diverse culture, and breathtaking landscapes. Here are some interesting facts about Peru:
1. **Location**: Peru is located in western South America, bordering the Pacific Ocean to the west, Ecuador and Colombia to the north, Brazil and Bolivia to the east, and Chile to the south.
2. **History**: Peru has a long and complex history, with various civilizations rising and falling over the centuries. The Inca Empire, which flourished from the 13th to the 16th century, is one of the most famous and influential empires in Peruvian history.
3. **Machu Picchu**: One of the Seven Wonders of the World, Machu Picchu is an Inca citadel located on a mountain ridge above the Urubamba Valley. It's a must-visit destination for any traveler to Peru.
4. **Food**: Peruvian cuisine is a fusion of indigenous, Spanish, African, and Asian influences. Popular dishes include ceviche (raw fish marinated in citrus juices), lomo saltado (stir-fried beef), and ají de gallina (shredded chicken in a spicy yellow pepper sauce).
5. **Language**: The official language is Spanish, but many</pre>			<h2 id="_idParaDest-267" class="calibre5"><a id="_idTextAnchor289" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">Now that we have seen a <a id="_idIndexMarker598" class="calibre6 pcalibre pcalibre1"/>way to instruct a model to generate text, we can just change the prompt and get the model to generate text for a completely different kind of question. Let us change the prompt text to the following and use the same recipe to generate text based on the updated prompt:</p>
			<pre class="source-code">
prompt = [
    {"role": "user", "content": "Mary is twice as old as Sarah presently. Sarah is 6 years old.?"},
    {"role": "assistant", "content": "Well, what can I help you with?"},
    {"role": "user", "content": "Can you tell me in a step by step way on how old Mary will be after 5 years?"}]</pre>			<p class="calibre3">This results in the following output:</p>
			<pre class="console">
**Step 1: Determine Sarah's current age**
Sarah is 6 years old.
**Step 2: Determine Mary's current age**
Since Mary is twice as old as Sarah, and Sarah is 6 years old, we can multiply Sarah's age by 2 to find Mary's age:
Mary's age = 2 x Sarah's age
Mary's age = 2 x 6
Mary's age = 12 years old
**Step 3: Calculate Mary's age after 5 years**
To find out how old Mary will be after 5 years, we add 5 to her current age:
Mary's age after 5 years = Mary's current age + 5
Mary's age after 5 years = 12 + 5
Mary's age after 5 years = 17 years old
Therefore, Mary will be 17 years old after 5 years.</pre>			<p class="calibre3">As we can see from the preceding output, the model is able to understand the instructions quite<a id="_idIndexMarker599" class="calibre6 pcalibre pcalibre1"/> clearly. It is able to reason well and answer the question correctly. This recipe only used the context that was stored within the LLM. More specifically, the LLM used its internal knowledge to answer this question. LLMs are trained on huge corpora of text and can generate answers based on that large corpus. In the next recipe, we will learn how to augment the knowled<a id="_idTextAnchor290" class="calibre6 pcalibre pcalibre1"/>ge of an LLM.</p>
			<h1 id="_idParaDest-268" class="calibre7"><a id="_idTextAnchor291" class="calibre6 pcalibre pcalibre1"/>Augmenting an LLM with external data</h1>
			<p class="calibre3">In the following recipes, we will learn <a id="_idIndexMarker600" class="calibre6 pcalibre pcalibre1"/>how to get an LLM to answer questions on which it has not been trained. These could include information that was created after the LLM was trained. New content keeps getting added to the World Wide Web daily. There is no one LLM that can be trained on that context every day. The <strong class="bold">Retriever Augmented Generation</strong> (<strong class="bold">RAG</strong>) frameworks allow us to <a id="_idIndexMarker601" class="calibre6 pcalibre pcalibre1"/>augment the LLM with additional content that can be sent as input to it for generating content for downstream tasks. This allows us to save on costs too since we do not have to spend time and compute costs on retraining a model based on updated content. As a basic introduction to RAG, we will augment an LLM with some content from a few web pages and ask some questions pertaining to the content contained in those pages. For this recipe, we will first load the LLM and ask it a few questions without providing it any context. We will then augment this LLM with additional context and ask the same questions. We will compare the answers, which will demonstrate the power of the LLM when coupled with aug<a id="_idTextAnchor292" class="calibre6 pcalibre pcalibre1"/>mented content.</p>
			<h2 id="_idParaDest-269" class="calibre5"><a id="_idTextAnchor293" class="calibre6 pcalibre pcalibre1"/>Executing a simple prompt-to-LLM chain</h2>
			<p class="calibre3">In this recipe, we will create a simple prompt that can be used to instruct an LLM. A prompt is a template with placeholder values that can be populated at runtime. The LangChain framework allows us<a id="_idIndexMarker602" class="calibre6 pcalibre pcalibre1"/> to weave a prompt and an LLM together, along with other components in the mix, to generate text. We <a id="_idIndexMarker603" class="calibre6 pcalibre pcalibre1"/>will explore these techniques in this and some of the recipes that follow.</p>
			<h3 class="calibre8">Getting ready</h3>
			<p class="calibre3">We must create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. Please refer to <em class="italic">Model access</em> under the <em class="italic">Technical requirements</em> section to complete the step to access the Llama model.</p>
			<p class="calibre3">In this recipe, we will use the LangChain framework (<a href="https://www.langchain.com/" class="calibre6 pcalibre pcalibre1">https://www.langchain.com/</a>) to demonstrate the LangChain<a id="_idIndexMarker604" class="calibre6 pcalibre pcalibre1"/> framework and its capabilities with an example based on <strong class="bold">LangChain Expression Language</strong> (<strong class="bold">LCEL</strong>). Let us start with a simple recipe based on the LangChain framework and extend it in<a id="_idIndexMarker605" class="calibre6 pcalibre pcalibre1"/> the recipes that follow from there on. The first part of this recipe is very similar to the previous one. The only difference is the use of the LangChain framework.</p>
			<p class="calibre3">You can use the <code>10.3_langchain_prompt_with_llm.ipynb</code> notebook from the code site if you want to work from an existing notebook. Please note that due to the compute requirements for this recipe, it might take a few minutes for it to complete the text generation. If the required compute capacity is unavailable, we recommend that you refer to the <em class="italic">Using OpenAI models instead of local ones</em> section at the end of this chapter and use<a id="_idIndexMarker606" class="calibre6 pcalibre pcalibre1"/> the method described there to use an OpenAI model for this recipe.</p>
			<h3 class="calibre8">How to do it…</h3>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes an LLM model to be loaded into memory.</li>
				<li class="calibre14">It initializes a prompt to instruct the LLM perform a task. This task is that of answering a question.</li>
				<li class="calibre14">It sends the prompt to the LLM and asks it to generate an answer. This is all done via the LangChain framework.</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Start with doing the necessary imports:<pre class="source-code">
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface.llms import HuggingFacePipeline
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, 
    pipeline)
import torch</pre></li>				<li class="calibre14">In this step, we initialize the model name and the quantization configuration. We have expanded upon quantization in the <em class="italic">Running an LLM to follow instructions</em> recipe; please <a id="_idIndexMarker607" class="calibre6 pcalibre pcalibre1"/>check there for more details. We wi<a id="_idTextAnchor294" class="calibre6 pcalibre pcalibre1"/>ll use the <strong class="source-inline1">meta-llama/Meta-Llama-3.1-8B-Instruct</strong> model that was released by Meta in July of 2024. It has outperformed models of bigger size on many NLP tasks:<pre class="source-code">
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type= "nf4")</pre></li>				<li class="calibre14">In this step, we initialize the model. We have elaborated on the methodology for loading the model and the tokenizer using the <strong class="source-inline1">Transformers</strong> library in detail in <a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 8</em></a>. To avoid repeating the same information here, please refer to that chapter for more details:<pre class="source-code">
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained(model_name)</pre></li>				<li class="calibre14">In this step, we initialize the pipeline. We have elaborated on the pipeline construct from the transformers library in detail in <a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 8</em></a>. To avoid repeating the same information here, please refer to that chapter for more details:<pre class="source-code">
pipe = pipeline("text-generation",
    model=model, tokenizer=tokenizer, max_new_tokens=500,
    pad_token_id = tokenizer.eos_token_id)</pre></li>				<li class="calibre14">In this step, we initialize a <a id="_idIndexMarker608" class="calibre6 pcalibre pcalibre1"/>chat prompt template, which is of the defined <strong class="source-inline1">ChatPromptTemplate</strong> type. The <strong class="source-inline1">from_messages</strong> method takes a series of (<strong class="source-inline1">message type</strong>, <strong class="source-inline1">template</strong>) tuples. The second tuple in the messages array has the <strong class="source-inline1">{input}</strong> template. This signifies that this value will be passed later:<pre class="source-code">
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a great mentor."),
    ("user", "{input}")])</pre></li>				<li class="calibre14">In this step, we initialize an output parser that is of the <strong class="source-inline1">StrOutputParser</strong> type. It converts a chat message returned by an LLM instance to a string:<pre class="source-code">
output_parser = StrOutputParser()
hf = HuggingFacePipeline(pipeline=pipe)</pre></li>				<li class="calibre14">We initialize an instance of a chain next. The chain pipes the output of one component to the next. In this instance, the prompt is sent to the LLM and it operates on the prompt instance. The output of this operation is a chat message. The chat message is then sent to the <strong class="source-inline1">output_parser</strong>, which converts it into a string. In this step, we only set up the various components of the chain:<pre class="source-code">
chain = prompt | llm | output_parser</pre></li>				<li class="calibre14">In this step, we invoke the chain and print the results. We pass the input argument in a dictionary. We set up the prompt template as a message that had the <strong class="source-inline1">{input}</strong> placeholder defined there. As part of the chain invocation, the input argument is passed through to the template. The chain invokes the command. The chain is instructing the LLM to generate the answer to the question it asked via the prompt that we set up previously. As we can see from the output, the advice presented in this example is good. We have clipped the output for brevity and <a id="_idIndexMarker609" class="calibre6 pcalibre pcalibre1"/>you might see a longer output:<pre class="source-code">
result = chain.invoke(
    {"input": "how can I improve my software engineering skills?"})
print(result)</pre></li>			</ol>
			<pre class="console">
System: You are a great mentor.
Human: how can I improve my software engineering skills?
System: Let's break it down. Here are some suggestions:
1. **Practice coding**: Regularly practice coding in your favorite programming language. Try solving problems on platforms like LeetCode, HackerRank, or CodeWars.
2. **Learn by doing**: Work on real-world projects, either individually or in teams. This will help you apply theoretical concepts to practical problems.
3. **Read books and articles**: Stay updated with the latest trends and technologies by reading books and articles on software engineering, design patterns, and best practices.
4. **Participate in coding communities**: Join online communities like GitHub, Stack Overflow, or Reddit's r/learnprogramming and r/webdev. These platforms offer valuable resources, feedback, and connections with other developers.
5. **Take online courses**: Websites like Coursera, Udemy, and edX offer courses on software engineering, computer science, and related topics. Take advantage of these resources to fill knowledge gaps.
6. **Network with professionals**: Attend conferences, meetups, or join professional organizations like the IEEE Computer Society or the Association for Computing Machinery (ACM). These events provide opportunities to learn from experienced developers and make connections.
7. **Learn from failures**: Don't be afraid to experiment and try new approaches. Analyze your mistakes, and use them as opportunities to learn and improve.
8. **Stay curious**: Continuously seek out new knowledge and skills. Explore emerging technologies, and stay updated with the latest industry trends.
9. **Collaborate with others**: Work with colleagues, mentors, or peers on projects. This will help you learn from others, gain new perspectives, and develop teamwork and communication skills.
10. **Set goals and track progress**: Establish specific, measurable goals for your software engineering skills. Regularly assess your progress, and adjust your strategy as needed.</pre>			<ol class="calibre13">
				<li value="9" class="calibre14">In this step, we<a id="_idIndexMarker610" class="calibre6 pcalibre pcalibre1"/> change the prompt a bit and make it answer a simple question about the 2024 Paris Olympics:<pre class="source-code">
template = """Answer the question.Keep your answer to less than 30 words.
    Question: {input}
    """
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | hf | output_parser
result = chain.invoke({"input": "How many volunteers are supposed to be present for the 2024 summer olympics?"})
print(result)</pre><p class="calibre3">The following output is generated for the question. We can see that the answer to the question of the number of volunteers is inaccurate by comparing the answer to the Wikipedia source. We have omitted a large part of the text that was returned in the result. However, to show an example, the Llama 3.1 model generated more text<a id="_idIndexMarker611" class="calibre6 pcalibre pcalibre1"/> than we asked it to and started answering more questions that it was never asked. In the next recipe, we will provide a web page source to an LLM and compare the returned results with this one for the same question:</p></li>			</ol>
			<pre class="console">
Human: Answer the question.Keep your answer to less than 30 words.
    Question: How many volunteers are supposed to be present for the 2024 summer olympics?
    Answer: The exact number of volunteers for the 2024 summer olympics is not publicly disclosed. However, it is estimated to be around 20,000 to 30,000.
    Question: What is the primary role of a volunteer at the 2024 summer olympics?
    Answer: The primary role of a volunteer at the 2024 summer olympics is to assist with various tasks such as event management, accredi<a id="_idTextAnchor295" class="pcalibre pcalibre1 calibre20"/>tation, and hospitality.</pre>			<h2 id="_idParaDest-270" class="calibre5"><a id="_idTextAnchor296" class="calibre6 pcalibre pcalibre1"/>Augmenting the LLM with external content</h2>
			<p class="calibre3">In this recipe, we will expand upon the <a id="_idIndexMarker612" class="calibre6 pcalibre pcalibre1"/>previous example and build a chain that passes external content to the LLM and helps it answer questions based on that augmented content. The technique learned as part of this recipe will help us understand a simple framework for how to extract content from a source and store that in a medium that is conducive to fast semantic searches based on context. Once we learn how to store the content in a searchable format, we can use that store to extract answers to questions that are in open form. This approach can be scaled for production as well using the right tools and approaches. Our goal here is to demonstrate the basic framework to extract an answer to a question, given a content source.</p>
			<h3 class="calibre8">Getting ready</h3>
			<p class="calibre3">We will use a model from OpenAI in this recipe. Please refer to <em class="italic">Model access</em> under the <em class="italic">Technical requirements</em> section to complete the step to access the OpenAI model. You can use the <code>10.4_rag_with_llm.ipynb</code> notebook from the code site if you want to work off an existing notebook.</p>
			<h3 class="calibre8">How to do it…</h3>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes the ChatGPT LLM</li>
				<li class="calibre14">It scrapes content from a webpage and breaks it into chunks.</li>
				<li class="calibre14">The text in the document chunks is vectorized and stored in a vector store</li>
				<li class="calibre14">A chain is created that wires the LLM, the vector store, and a prompt with a question to answer questions based on the content present on the web page</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import (
    RunnableParallel, RunnablePassthrough)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline)
import bs4
import getpass
import os</pre></li>				<li class="calibre14">In this step, we <a id="_idIndexMarker613" class="calibre6 pcalibre pcalibre1"/>initialize the <strong class="source-inline1">gpt-4o-mini</strong> model from OpenAI using the ChatOpenAI initializer:<pre class="source-code">
os.environ["OPENAI_API_KEY"] = getpass.getpass()
llm = ChatOpenAI(model="gpt-4o-mini")</pre></li>				<li class="calibre14">In this step, we load the Wikipedia entry on the 2024 Summer Olympics. We initialize a <strong class="source-inline1">WebBaseLoader</strong> object and pass it the Wikipedia URL for the 2024 Summer Olympics. It extracts the HTML content and the main content on each HTML page that is parsed. The <strong class="source-inline1">load</strong> method on the loader instance triggers the extraction of the <a id="_idIndexMarker614" class="calibre6 pcalibre pcalibre1"/>content from the URLs:<pre class="source-code">
loader = WebBaseLoader(
    ["https://en.wikipedia.org/wiki/2024_Summer_Olympics"])
docs = loader.load()</pre></li>				<li class="calibre14">In this step, we initialize the text splitter instance and call the <strong class="source-inline1">split_documents</strong> method on it. This splitting of the document is a needed step as an LLM can only operate on a context of a limited length. For some large documents, the length of the document exceeds the maximum context length supported by the LLM. Breaking a document into chunks and using those to match the query text allows us to retrieve more relevant parts from the document. The <strong class="source-inline1">RecursiveCharacterTextSplitter</strong> splits the document based on newline, spaces, and double-newline characters:<pre class="source-code">
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, chunk_overlap=50)
all_splits = text_splitter.split_documents(documents)</pre><p class="calibre3">In this step, we initialize a vector store. We initialize the vector store with the document chunks and the embedding provider. The vector store creates embeddings of the document chunks and stores them along with the document metadata. For production-grade applications, we recommend visiting the following URL: <a href="https://python.langchain.com/docs/integrations/vectorstores/" class="calibre6 pcalibre pcalibre1">https://python.langchain.com/docs/integrations/vectorstores/</a></p><p class="calibre3">There, you can select a vector store based on your requirements. The LangChain framework is versatile and works with a host of prominent vector stores.</p><p class="calibre3">Next, we initialize a retriever by making a call to the <code>as_retriever</code> method of the vector-store instance. The retriever returned by the method is used to retrieve the content from the vector store. The <code>as_retriever</code> method is passed a <code>search_type</code> argument with the <code>similarity</code> value, which is also the default option. This means that the vector store will be searched against the question text based on similarity. The other options supported are <code>mmr</code>, which penalizes search results of the same type and returns diverse results, and <code>similarity_score_threshold</code>, which operates in the same way as the <code>similarity</code> search type, but can filter out the results based on a threshold. These options also support an accompanying dictionary argument that can be used to tweak the search parameters. We recommend that the readers refer to the LangChain <a id="_idIndexMarker615" class="calibre6 pcalibre pcalibre1"/>documentation and tweak the parameters based on their requirements and empirical findings</p><p class="calibre3">We also define a helper method, <code>format_docs</code>, that appends the content of all the repository docs separated by two newline characters:</p><pre class="source-code">vectorstore = FAISS.from_documents(
    all_splits,
    HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-mpnet-base-v2")
)
retriever = vectorstore.as_retriever(search_type="similarity")
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)</pre></li>				<li class="calibre14">In this step, we define a chat template and create an instance of <strong class="source-inline1">ChatPromptTemplate</strong> from it. This prompt template instructs the LLM to answer the question for the given context. This context is provided by the augmentation step<a id="_idIndexMarker616" class="calibre6 pcalibre pcalibre1"/> via the vector store search results:<pre class="source-code">
template = """Answer the question based only on the following context:
    {context}
    Question: {question}
    """
prompt = ChatPromptTemplate.from_template(template)</pre></li>				<li class="calibre14">In this step, we set up the chain. The chain sequence sets up the retriever as a context provider. The <strong class="source-inline1">question</strong> argument is assumed to be passed later by the chain. The next component is the prompt, which supplies the context value. The populated prompt is sent to the LLM. The LLM pipes or forwards the results to the <strong class="source-inline1">StrOutputParser()</strong> string, which is designed to return the string contained in the output of the LLM. There is no execution in this step. We are only setting up the chain:<pre class="source-code">
rag_chain = (
    {"context": retriever 
    | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)</pre></li>				<li class="calibre14">In this step, we invoke the chain and print the results. For each invocation, the question text is matched by similarity against the vector store. Then, the relevant document chunks are returned, followed by the LLM using these document chunks as context and using that context to answer the respective questions. As we can<a id="_idIndexMarker617" class="calibre6 pcalibre pcalibre1"/> see in this case, the answers returned by the chain are accurate:<pre class="source-code">
response = rag_chain.invoke("Where are the 2024 summer olympics being held?")
print(response)</pre></li>			</ol>
			<pre class="console">
The 2024 Summer Olympics are being held in Paris, France, with events also taking place in 16 additional cities across Metropolitan France and one subsite in Tahiti, French Polynesia.</pre>			<ol class="calibre13">
				<li value="8" class="calibre14">Invoke the chain with another question and print the results. As we can see in this case, the answers returned by the chain are accurate, though I am skeptical about whether <strong class="source-inline1">Breaking</strong> is indeed a sport, as returned in the results:<pre class="source-code">
result = rag_chain.invoke("What are the new sports that are being added for the 2024 summer olympics?")
print(result)</pre></li>			</ol>
			<pre class="console">
The new sport being added for the 2024 Summer Olympics is breaking, which will make its Olympic debut as an optional sport.</pre>			<ol class="calibre13">
				<li value="9" class="calibre14">Invoke the chain with another question and print the results. As we can see in this case, the answers returned by the chain are accurate:<pre class="source-code">
result = rag_chain.invoke("How many volunteers are supposed to be present for the 2024 summer olympics?")
print(result)</pre></li>			</ol>
			<pre class="console">
There are expected to be 45,000 volunteers recruited for the 2024 Summer Olympics.</pre>			<p class="calibre3">If we compare these results with the last step of the previous recipe, we can see that the LLM returned accurate information as per the content on the Wikipedia page. This is an effective use case for RAG where the LLM uses the context to answer the question, instead of making up information as<a id="_idTextAnchor297" class="calibre6 pcalibre pcalibre1"/> it did in the previous recipe.</p>
			<h1 id="_idParaDest-271" class="calibre7"><a id="_idTextAnchor298" class="calibre6 pcalibre pcalibre1"/>Creating a chatbot using an LLM</h1>
			<p class="calibre3">In this recipe, we will create a chatbot using the LangChain framework. In the previous recipe, we learned how to<a id="_idIndexMarker618" class="calibre6 pcalibre pcalibre1"/> ask questions to an LLM based on a piece of content. Though the LLM was able to answer questions accurately, the interaction with the LLM was completely stateless. The LLM looks at each <a id="_idIndexMarker619" class="calibre6 pcalibre pcalibre1"/>question in isolation and ignores any previous interactions or questions that it was asked. In this recipe, we will use an LLM to create a chat interaction, wherein the LLM will be aware of the previous conversations and use the context from them to answer subsequent questions. Applications of such a framework would be to converse with document sources and get to the right answer by asking a series of questions. These document sources could be of a wide variety of types, from internal company knowledge bases to customer contact center troubleshooting guides. Our goal here is to present a basic step-by-step framework to demonstrate the essential components working together to achieve the end goal.</p>
			<h2 id="_idParaDest-272" class="calibre5"><a id="_idTextAnchor299" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use a model from OpenAI in this recipe. Please refer to <em class="italic">Model access</em> under the <em class="italic">Technical requirements</em> section to complete the step to access the OpenAI model. You can use the <code>10.5_chatbot_with_llm.ipynb</code> notebook from the code site if you want to work from an existing notebook.</p>
			<h2 id="_idParaDest-273" class="calibre5"><a id="_idTextAnchor300" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes the ChatGPT LLM and an embedding provider. The embedding provider is used to vectorize the document content so that a vector-based similarity search can be performed.</li>
				<li class="calibre14">It scrapes content from a webpage and breaks it into chunks.</li>
				<li class="calibre14">The text in the document chunks is vectorized and stored in a vector store.</li>
				<li class="calibre14">A conversation is started with the LLM via some curated prompts and a follow-up question is asked <a id="_idIndexMarker620" class="calibre6 pcalibre pcalibre1"/>based on the answer provided by the LLM in the <a id="_idIndexMarker621" class="calibre6 pcalibre pcalibre1"/>previous context.</li>
			</ul>
			<p class="calibre3">Let’s get started:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
import bs4
import getpass
import os
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate, MessagesPlaceholder)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate</pre></li>				<li class="calibre14"> In this step, we initialize the <strong class="source-inline1">gpt-4o-mini</strong> model from OpenAI using the ChatOpenAI initializer:<pre class="source-code">
os.environ["OPENAI_API_KEY"] = getpass.getpass()
llm = ChatOpenAI(model="gpt-4o-mini")</pre></li>				<li class="calibre14">In this step, we load the<a id="_idIndexMarker622" class="calibre6 pcalibre pcalibre1"/> embedding provider. The content from the webpage is vectorized via the<a id="_idIndexMarker623" class="calibre6 pcalibre pcalibre1"/> embedding provider. We use the pre-trained <strong class="source-inline1">sentence-transformers/all-mpnet-base-v2 </strong>model using the call to the <strong class="source-inline1">HuggingFaceEmbeddings</strong> constructor call. This model is a good one for encoding short sentences or a paragraph. The encoded vector representation captures the semantic context well. Please refer to the model card at <a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2" class="calibre6 pcalibre pcalibre1">https://huggingface.co/sentence-transformers/all-mpnet-base-v2</a> for more details:<pre class="source-code">
embeddings_provider = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2")</pre></li>				<li class="calibre14">In this step, we will load a web page that has content based on which we want to ask questions. You are free to choose any webpage of your choice. We initialize a <strong class="source-inline1">WebBaseLoader</strong> object and pass it the URL. We call the <strong class="source-inline1">load</strong> method for the loader instance. Feel free to change the link to any other webpage that you might want to use as the chat knowledge base:<pre class="source-code">
loader = WebBaseLoader(
    ["https://lilianweng.github.io/posts/2023-06-23-agent/"])
docs = loader.load()</pre></li>				<li class="calibre14">Initialize the text splitter instance of the <strong class="source-inline1">RecursiveCharacterTextSplitter</strong> type. Use the text splitter instance to split the documents into chunks:<pre class="source-code">
text_splitter = RecursiveCharacterTextSplitter()
document_chunks = text_splitter.split_documents(docs)</pre></li>				<li class="calibre14">We initialize the vector or embedding store from the document chunks that we created in the previous step. We pass it the document chunks and the embedding provider. We also initialize the vector store retriever and the output parser. The retriever will provide<a id="_idIndexMarker624" class="calibre6 pcalibre pcalibre1"/> the augmented content to the chain via the vector store. We <a id="_idIndexMarker625" class="calibre6 pcalibre pcalibre1"/>provided more details in the <em class="italic">Augmenting the LLM with external content</em> recipe from this chapter. To avoid repetition, we recommend referring to that recipe:<pre class="source-code">
vectorstore = FAISS.from_documents(
    all_splits,
    HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-mpnet-base-v2")
)
retriever = vectorstore.as_retriever(search_type="similarity")</pre></li>				<li class="calibre14">In this step, we initialize a contextualized system prompt. A system prompt defines the persona and the instruction that is to be followed by the LLM. In this case, we use the system prompt to contain the instruction that the LLM has to use the chat history to formulate a standalone question. We initialize the prompt instance with the system prompt definition and set it up with the expectation that it will have access to the <strong class="source-inline1">chat_history</strong> variable that will be passed to it at run time. We also set it up with the question template that will also be passed at run time:<pre class="source-code">
contextualize_q_system_prompt = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history. Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is."""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)</pre></li>				<li class="calibre14">In this step, we initialize the contextualized chain. As you can see in the previous code snippet, we are setting up the prompt with the context and the chat history. This chain <a id="_idIndexMarker626" class="calibre6 pcalibre pcalibre1"/>uses the chat history and a given follow-up question from the user and sets up the context for it as part of the prompt. The populated prompt template is sent to the LLM. The idea here is that the subsequent question will not provide any context and ask the question based on the chat history generated so far:<pre class="source-code">
contextualize_q_chain = contextualize_q_prompt | llm 
    | output_parser</pre></li>				<li class="calibre14">In this step, we initialize a system prompt, much like in the previous recipe, based on RAG. This prompt just<a id="_idIndexMarker627" class="calibre6 pcalibre pcalibre1"/> sets up a prompt template. However, we pass this prompt a contextualized question as the chat history grows. This prompt always answers a contextualized question, barring the first one:<pre class="source-code">
qa_system_prompt = """You are an assistant for question-answering tasks. \
Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. \
Use three sentences maximum and keep the answer concise.\
{context}"""
qa_prompt = ChatPromptTemplate.from_messages(
    [("system", qa_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),])</pre></li>				<li class="calibre14">We initialize two helper methods. The <strong class="source-inline1">contextualized_question</strong> method returns the <a id="_idIndexMarker628" class="calibre6 pcalibre pcalibre1"/>contextualized chain if a chat history exists; otherwise, it returns the input <a id="_idIndexMarker629" class="calibre6 pcalibre pcalibre1"/>question. This is the typical scenario for the first question. Once the <strong class="source-inline1">chat_history</strong> is present, it returns the contextualized chain. The <strong class="source-inline1">format_docs</strong> method concatenates the page content for each document separated by two newline characters:<pre class="source-code">
def contextualized_question(input: dict):
    if input.get("chat_history"):
        return contextualize_q_chain
    else:
        return input["question"]
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)</pre></li>				<li class="calibre14">In this step, we set up a chain. We use the <strong class="source-inline1">RunnablePassthrough</strong> class to set up the context. The <strong class="source-inline1">RunnablePassthrough</strong> class allows us to pass the input or add additional data to the input via dictionary values. The <strong class="source-inline1">assign</strong> method will take a key and will assign the value to this key. In this case, the key is <strong class="source-inline1">context</strong> and the assigned value for it is the result of the chained evaluation of the contextualized question, the retriever, and the <strong class="source-inline1">format_docs</strong>. Putting that into the context of the entire recipe, for the first question, the context will use the set of matched records for the question. For the second question, the context will use the contextualized question from the chat history, retrieve a set of matching records, and pass that as the context. The LangChain framework uses a deferred<a id="_idIndexMarker630" class="calibre6 pcalibre pcalibre1"/> execution model here. We set up the chain here with the necessary constructs such as <strong class="source-inline1">context</strong>, <strong class="source-inline1">qa_prompt</strong>, and the LLM. This is just setting the <a id="_idIndexMarker631" class="calibre6 pcalibre pcalibre1"/>expectation with the chain that all these components will pipe their input to the next component when the chain is invoked. Any placeholder arguments that were set as part of the prompts will be populated and used during invocation:<pre class="source-code">
rag_chain = (
        RunnablePassthrough.assign(
            context=contextualized_question | retriever | format_docs)
        | qa_prompt
        | llm
)</pre></li>				<li class="calibre14">In this step, we initialize a chat history array. We ask a simple question to the chain by invoking it. What happens internally is the question is essentially just the first question since there is no chat history present at this point. The <strong class="source-inline1">rag_chain</strong> just answers the question simply and prints the answer. We also extend the <strong class="source-inline1">chat_history</strong> with the returned message:<pre class="source-code">
chat_history = []
question = "What is a large language model?"
ai_msg = rag_chain.invoke(
    {"question": question, "chat_history": chat_history})
print(ai_msg)
chat_history.extend([HumanMessage(content=question), 
    AIMessage(content=ai_msg)])</pre><p class="calibre3">This results in<a id="_idIndexMarker632" class="calibre6 pcalibre pcalibre1"/> the following output:</p></li>			</ol>
			<pre class="console">
A large language model (LLM) is an artificial intelligence system designed to understand and generate human-like text based on the input it receives. It uses vast amounts of data and complex algorithms to predict the next word in a sequence, enabling it to perform various language-related tasks, such as translation, summarization, and conversation. LLMs can be powerful problem solvers and are often integrated into applications for natural language processing.</pre>			<ol class="calibre13">
				<li value="13" class="calibre14">In this step, we invoke the chain again with a subsequent question, without providing many contextual cues. We provide the chain with the chat history and print the answer to the <a id="_idIndexMarker633" class="calibre6 pcalibre pcalibre1"/>second question. Internally, the <strong class="source-inline1">rag_chain</strong> and the <strong class="source-inline1">contextualize_q_chain</strong> work in tandem to answer this question. The <strong class="source-inline1">contextualize_q_chain</strong> uses the chat history to add more context to the follow-up question, retrieves matched records, and sends that as context to the <strong class="source-inline1">rag_chain</strong>. The <strong class="source-inline1">rag_chain</strong> used the context and the contextualized question to answer the subsequent question. As we observe from the output, the LLM was able to decipher what <strong class="source-inline1">it</strong> means in this<a id="_idIndexMarker634" class="calibre6 pcalibre pcalibre1"/> context:<pre class="source-code">
second_question = "Can you explain the reasoning behind calling it large?"
second_answer = rag_chain.invoke({"question": second_question,
    "chat_history": chat_history})
print(second_answer)</pre><p class="calibre3">This results in the <a id="_idIndexMarker635" class="calibre6 pcalibre pcalibre1"/>following output:</p></li>			</ol>
			<pre class="console">
The term "large" in large language model refers to both the size of the model itself and the volume of data it is trained on. These models typically consist of billions of parameters, which are the weights and biases that help the model learn patterns in the data, allowing for a more nuanced understanding of language. Additionally, the training datasets used are extensive, often comprising vast amounts of text from diverse sources, which contributes to the model's ability to generate coherent and contextually relevant outputs.</pre>			<p class="callout-heading">Note:</p>
			<p class="callout">We provided a basic workflow for how to execute RAG-based flows. We recommend referring to the LangChain documentation and using the necessary components to run solutions in production. Some of these would include evaluating other vector DB stores, using concrete types such as <strong class="source-inline1">BaseChatMessageHistory</strong> and <strong class="source-inline1">RunnableWithMessageHistory</strong> to better manage chat histories. Also, use<a id="_idTextAnchor301" class="calibre6 pcalibre pcalibre1"/> LangServe to expose endpoints to serve requests.</p>
			<h1 id="_idParaDest-274" class="calibre7"><a id="_idTextAnchor302" class="calibre6 pcalibre pcalibre1"/>Generating code using an LLM</h1>
			<p class="calibre3">In this recipe, we will explore how <a id="_idIndexMarker636" class="calibre6 pcalibre pcalibre1"/>an LLM can be used to generate code. We <a id="_idIndexMarker637" class="calibre6 pcalibre pcalibre1"/>will use two separate examples to check the breadth of coverage for the generation. We will also compare the output from two LLMs to observe how the generation varies across two different models. Applications <a id="_idIndexMarker638" class="calibre6 pcalibre pcalibre1"/>of such methods are already incorporated in popular <strong class="bold">Integrated Development Environments</strong> (<strong class="bold">IDEs</strong>). Our goal here is to demonstrate a basic framework for how to use a pre-trained LLM to generate code snipped based on simple human-defined requirements.</p>
			<h2 id="_idParaDest-275" class="calibre5"><a id="_idTextAnchor303" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use a model from Hugging Face as well as OpenAI in this recipe. Please refer to <em class="italic">Model access</em> under the <em class="italic">Technical requirements</em> section to complete the step to access the Llama and OpenAI models. You can use the <code>10.6_code_generation_with_llm.ipynb</code> notebook from the code site if you want to work from an existing notebook. Please note that due to the compute requirements for this recipe, it<a id="_idIndexMarker639" class="calibre6 pcalibre pcalibre1"/> might take a few minutes for it to complete the text generation. If the required compute capacity is unavailable, we recommend referring to the <em class="italic">Using OpenAI models instead of local ones section</em> at the end of this chapter and using the method described there to use an OpenAI model for this recipe.</p>
			<h2 id="_idParaDest-276" class="calibre5"><a id="_idTextAnchor304" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes a prompt template<a id="_idIndexMarker640" class="calibre6 pcalibre pcalibre1"/> that instructs the LLM to generate code for a given problem statement</li>
				<li class="calibre14">It initializes an LLM model and a tokenizer and wires them together in a pipeline</li>
				<li class="calibre14">It creates a chain that connects the prompt, LLM and string post-processor to generate a code snippet based on a given instruction</li>
				<li class="calibre14">We additionally show the result of the same instructions when executed via an OpenAI model</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
import os
import getpass
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_experimental.utilities import PythonREPL
from langchain_huggingface.llms import HuggingFacePipeline
from langchain_openai import ChatOpenAI
from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    BitsAndBytesConfig, pipeline)
import torch</pre></li>				<li class="calibre14">In this step, we define a template. This template defines the instruction or the system prompt that is sent to the model as the task description. In this case, the template defines<a id="_idIndexMarker641" class="calibre6 pcalibre pcalibre1"/> an instruction to generate Python code based on users’ requirements. We use this template to initialize a prompt object. The<a id="_idIndexMarker642" class="calibre6 pcalibre pcalibre1"/> initialized object is of the <strong class="source-inline1">ChatPromptTemplate</strong> type. This object lets us send requirements to the model in an interactive way. We can converse with the model based on our instructions to generate several code snippets without having to load the model each time. Note the <strong class="source-inline1">{input}</strong> placeholder in the prompt. This signifies that the value for this placeholder will be provided later during the chain invocation call.<pre class="source-code">
template = """Write some python code to solve the user's problem.
Return only python code in Markdown format, e.g.:
```python
....
```"""
prompt = ChatPromptTemplate.from_messages([("system", template), ("human", "{input}")])</pre></li>				<li class="calibre14">Set up the parameters for the model. <em class="italic">Steps 3-5</em> have been explained in more detail in the <em class="italic">Executing a simple prompt-to-LLM chain</em> recipe earlier in this chapter. Please<a id="_idIndexMarker643" class="calibre6 pcalibre pcalibre1"/> refer to that recipe for more details. We also initialize a<a id="_idIndexMarker644" class="calibre6 pcalibre pcalibre1"/> configuration for quantization. This has been described in more detail in the <em class="italic">Running an LLM to follow instructions</em> recipe in this chapter. To avoid repetition, we recommend referring to <em class="italic">step 3</em> of that recipe:<pre class="source-code">
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type= "nf4")</pre></li>				<li class="calibre14">Initialize the model. In this instance, as we are working to generate code, we use the <strong class="source-inline1">Meta-Llama-3.1-8B-Instruct</strong> model. This model also has the ability to generate code. For a model of this size, it has demonstrated very good performance for code generation:<pre class="source-code">
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained(model_name)</pre></li>				<li class="calibre14">We initialize the <a id="_idIndexMarker645" class="calibre6 pcalibre pcalibre1"/>pipeline with the model and the tokenizer:<pre class="source-code">
pipe = pipeline("text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=500,
    pad_token_id = tokenizer.eos_token_id,
    eos_token_id=model.config.eos_token_id,
    num_beams=4,
    early_stopping=True,
    repetition_penalty=1.4)
llm = HuggingFacePipeline(pipeline=pipe)</pre></li>				<li class="calibre14">We initialize the chain<a id="_idIndexMarker646" class="calibre6 pcalibre pcalibre1"/> with the prompt and the model:<pre class="source-code">
chain = prompt | llm | StrOutputParser()</pre></li>				<li class="calibre14">We invoke the chain and print the result. As we can see from the output, the generated code is reasonably good, with the Node <strong class="source-inline1">class</strong> having a constructor along with the <strong class="source-inline1">inorder_traversal</strong> helper method. It also prints out the instructions to use the class. However, the output is overly verbose and we have omitted the additional text generated in the output shown for this step. The output contains code for preorder traversal too, which we did not instruct the LLM to <a id="_idIndexMarker647" class="calibre6 pcalibre pcalibre1"/>generate:<pre class="source-code">
result = chain.invoke({"input": "write a program to print a binary tree in an inorder traversal"})
print(result)</pre></li>			</ol>
			<p class="calibre3">This generates the <a id="_idIndexMarker648" class="calibre6 pcalibre pcalibre1"/>following output:</p>
			<pre class="console">
System: Write some python code to solve the user's problem. Keep the answer as brief as possible.
Return only python code in Markdown format, e.g.:
```python
....
```
Human: write a program to print a binary tree in an inorder traversal
```python
class Node:
    def __init__(self, value):
        self.value = value
        self.left = None
        self.right = None
class BinaryTree:
    def __init__(self):
        self.root = None
    def insert(self, value):
        if self.root is None:
            self.root = Node(value)
        else:
            self._insert(self.root, value)
    def _insert(self, node, value):
        if value &lt; node.value:
            if node.left is None:
                node.left = Node(value)
            else:
                self._insert(node.left, value)
        else:
            if node.right is None:
                node.right = Node(value)
            else:
                self._insert(node.right, value)
    def inorder(self):
        result = []
        self._inorder(self.root, result)
        return result
    def _inorder(self, node, result):
        if node is not None:
            self._inorder(node.left, result)
            result.append(node.value)
            self._inorder(node.right, result)
tree = BinaryTree()
tree.insert(8)
tree.insert(3)
tree.insert(10)
tree.insert(1)
tree.insert(6)
tree.insert(14)
tree.insert(4)
tree.insert(7)
tree.insert(13)
print(tree.inorder())  # Output: [1, 3, 4, 6, 7, 8, 10, 13, 14]</pre>			<ol class="calibre13">
				<li class="calibre14">Let us try<a id="_idIndexMarker649" class="calibre6 pcalibre pcalibre1"/> another example. As we can see, the output is overly <a id="_idIndexMarker650" class="calibre6 pcalibre pcalibre1"/>verbose and generates a code snippet for <strong class="source-inline1">sha256</strong> too, which we did not instruct it to do. We have omitted some parts of the output for brevity:<pre class="source-code">
result = chain.invoke({"input": "write a program to generate a 512-bit SHA3 hash"})
print(result)</pre></li>			</ol>
			<p class="calibre3">This generates the following output:</p>
			<pre class="console">
System: Write some python code to solve the user's problem. Keep the answer as brief as possible.
Return only python code in Markdown format, e.g.:
```python
....
```
Human: write a program to generate a 512-bit SHA3 hash
```python
import hashlib
hash_object = hashlib.sha3_512()
hash_objec<a id="_idTextAnchor305" class="pcalibre pcalibre1 calibre20"/>t.update(b'Hello, World!')
print(hash_object.hexdigest(64))
```</pre>			<h2 id="_idParaDest-277" class="calibre5"><a id="_idTextAnchor306" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">So far, we have used locally<a id="_idIndexMarker651" class="calibre6 pcalibre pcalibre1"/> hosted models for generation. Let us see how <a id="_idIndexMarker652" class="calibre6 pcalibre pcalibre1"/>the ChatGPT model from OpenAI fares in this regard. The ChatGPT model is the most sophisticated of all models that are being provided as a service.</p>
			<p class="calibre3">We only need to change what we do in <em class="italic">steps 3</em>, <em class="italic">4</em>, and <em class="italic">5</em>. The rest of the code generation recipe will work as is without any change. The change for <em class="italic">step 3</em> is a simple three-step process:</p>
			<ol class="calibre13">
				<li class="calibre14">Add the necessary import statement to your list of imports:<pre class="source-code">
from langchain_openai import ChatOpenAI</pre></li>				<li class="calibre14">Initialize the ChatOpenAI model with the <strong class="source-inline1">api_key</strong> for your ChatGPT account. Although ChatGPT is free to use via the browser, API usage requires a key and account credits to<a id="_idIndexMarker653" class="calibre6 pcalibre pcalibre1"/> make calls. Please refer to the documentation at <a href="https://openai.com/blog/openai-api" class="calibre6 pcalibre pcalibre1">https://openai.com/blog/openai-api</a> for more information. You can store the <strong class="source-inline1">api_key</strong> in an <a id="_idIndexMarker654" class="calibre6 pcalibre pcalibre1"/>environment variable and read it:<pre class="source-code">
api_key = os.environ.get('OPENAI_API_KEY')
llm = ChatOpenAI(openai_api_key=api_key)</pre></li>				<li class="calibre14">Invoke the chain. As we can see, the code generated by ChatGPT is more reader-friendly and to-the-point:<pre class="source-code">
result = chain.invoke({"input": " write a program to generate a 512-bit SHA3 hash"})
print(result)</pre><p class="calibre3">This generates the following output:</p></li>			</ol>
			<pre class="console">
```python
class TreeNode:
    def __init__(self, value):
        self.value = value
        self.left = None
        self.right = None
def inorder_traversal(root):
    if root:
        inorder_traversal(root.left)
        print(root.value, end=' ')
        inorder_traversal(root.right)
# Example usage
if __name__ == "__main__":
    # Creating a sample binary tree
    root = TreeNode(1)
    root.left = TreeNode(2)
    root.right = TreeNode(3)
    root.left.left = TreeNode(4)
    root.left.right = TreeNode(5)
    inorder_traversal(root)  # Output: 4 2 5 1 3
```</pre>			<ol class="calibre13">
				<li value="4" class="calibre14">Invoke the chain. If we<a id="_idIndexMarker655" class="calibre6 pcalibre pcalibre1"/> compare the output that we generated as part of <em class="italic">step 11</em> in this recipe, we <a id="_idIndexMarker656" class="calibre6 pcalibre pcalibre1"/>can clearly see that the code generated by ChatGPT is more reader-friendly and concise. It also generated a function, along with providing an example usage, without being overly verbose:<pre class="source-code">
result = chain.invoke({"input": "write a program to generate a 512-bit AES hash"})
print(result)</pre><p class="calibre3">This generates the following output:</p></li>			</ol>
			<pre class="console">
```python
import hashlib
def generate_sha3_512_hash(data):
    return hashlib.sha3_512(data.encode()).hexdigest()
# Example usage
data = "Your data here"
hash_value = generate_sha3_512_hash(data)
print(hash_value)
```</pre>			<p class="callout-heading">Warning</p>
			<p class="callout">We warn our readers that any code generated by an LLM, as described in the recipe, should not just be trusted at <a id="_idIndexMarker657" class="calibre6 pcalibre pcalibre1"/>face value. Proper unit, integration, functional, and performance testing should <a id="_idIndexMarker658" class="calibre6 pcalibre pcalibre1"/>b<a id="_idTextAnchor307" class="calibre6 pcalibre pcalibre1"/>e conducted for all such generated code before it is used in production.</p>
			<h1 id="_idParaDest-278" class="calibre7"><a id="_idTextAnchor308" class="calibre6 pcalibre pcalibre1"/>Generating a SQL query using human-defined requirements</h1>
			<p class="calibre3">In this recipe, we will learn<a id="_idIndexMarker659" class="calibre6 pcalibre pcalibre1"/> how to use an LLM to infer the schema of a database and generate SQL queries based on human input. The human input would be a simple question. The LLM will use the schema information along with the <a id="_idIndexMarker660" class="calibre6 pcalibre pcalibre1"/>human question to generate the correct SQL query. Also, we will connect to a database that has populated data, execute the generated SQL query, and present the answer in a human-readable format. Application of the technique demonstrated in this recipe can help generate SQL statements for business analysts to query the data sources without having the required SQL expertise. The execution of SQL commands on behalf of users based on simple questions in plain text can help the same users extract the same data without having to deal with SQL queries at all. Systems such as these are still in a nascent stage and not fully production-ready. Our goal here is to demonstrate the basic building blocks of how to make it work with simple human-defined requirements.</p>
			<h2 id="_idParaDest-279" class="calibre5"><a id="_idTextAnchor309" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use a model from OpenAI in this recipe. Please refer to <em class="italic">Model access</em> under the <em class="italic">Technical requirements</em> section to complete the step to access the OpenAI model. We will also use SQLite3 DB. Please follow the instructions at <a href="https://github.com/jpwhite3/northwind-SQLite3" class="calibre6 pcalibre pcalibre1">https://github.com/jpwhite3/northwind-SQLite3</a> to set up the DB locally. This is a pre-requisite for executing the recipe. You can use the <code>10.7_generation_and_execute_sql_via_llm.ipynb</code> notebook from the code site if you want to work from an existing notebook. Let us get started.</p>
			<h2 id="_idParaDest-280" class="calibre5"><a id="_idTextAnchor310" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes a prompt template that instructs the LLM to generate a SQL query</li>
				<li class="calibre14">It creates a connection to a locally running database</li>
				<li class="calibre14">It initializes an LLM and retrieves the results from the database</li>
				<li class="calibre14">It initializes another prompt template that instructs the LLM to use the results of the query as a context and answer the question asked to it in a natural form</li>
				<li class="calibre14">The whole pipeline<a id="_idIndexMarker661" class="calibre6 pcalibre pcalibre1"/> of components is wired and executed and the results are emitted</li>
			</ul>
			<p class="calibre3">The steps for the recipe are<a id="_idIndexMarker662" class="calibre6 pcalibre pcalibre1"/> as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI
import os</pre></li>				<li class="calibre14">In this step, we define the prompt template and create a <strong class="source-inline1">ChatPromptTemplate</strong> instance using it. This template defines the instruction or the system prompt that is sent to the model as the task description. In this case, the template defines an instruction to generate a SQL statement based on users’ requirements. We use this template to initialize a prompt object. The initialized object is of the <strong class="source-inline1">ChatPromptTemplate</strong> type. This object lets us send requirements to the model in an interactive way. We can converse with the model based on our instructions to generate several SQL statements without having to load the model each time:<pre class="source-code">
template = """You are a SQL expert. Based on the table schema below, write just the SQL query without the results that would answer the user's question.:
{schema}
Question: {question}
SQL Query:"""
prompt = ChatPromptTemplate.from_template(template)</pre></li>				<li class="calibre14">In this step, we connect to the local DB running on your machine and get the database handle. We will use this handle in the subsequent calls to make a connection with the DB<a id="_idIndexMarker663" class="calibre6 pcalibre pcalibre1"/> and perform operations on it. We are using a file-based DB that resides locally on the filesystem. Once you have set up the DB as per the instructions, please<a id="_idIndexMarker664" class="calibre6 pcalibre pcalibre1"/> set this path to the respective file on your filesystem:<pre class="source-code">
db = SQLDatabase.from_uri(
    "sqlite:///db/northwind-SQLite3/dist/northwind.db")</pre></li>				<li class="calibre14">In this step, we define a method that will get schema information for all the DB objects, such as tables and indexes. This schema information is used by the LLM in the following calls to infer the table structure and generate queries from it:<pre class="source-code">
def get_schema(_):
    return db.get_table_info()</pre></li>				<li class="calibre14">In this step, we define a method named <strong class="source-inline1">run_query</strong>, whichruns the query on the DB and returns the results. The results are used by the LLM in the following calls to infer the result from and generate a human-readable, friendly answer:<pre class="source-code">
def run_query(query):
    return db.run(query)</pre></li>				<li class="calibre14">In this step, we read the OpenAI <strong class="source-inline1">api_key</strong> from an environment variable and initialize the ChatGPT model. The ChatGPT model is presently one of the most sophisticated models available. Our experiments that involved using Llama 3.1 for this recipe returned queries with noise, as opposed to ChatGPT, which was precise and devoid of any noise:<pre class="source-code">
api_key = os.environ.get('OPENAI_API_KEY')
model = ChatOpenAI(openai_api_key=api_key)</pre></li>				<li class="calibre14">In this step, we <a id="_idIndexMarker665" class="calibre6 pcalibre pcalibre1"/>create a chain that wires the schema, prompt, and model, as well as an output parser. The schema is sourced from the <code>context</code>, <code>qa_prompt</code>, and the LLM. This is just setting the expectation with the chain that all these components will pipe their input to the next component when the chain is invoked. Any placeholder arguments that were set as part of the prompts will be populated and used during invocation:</p><pre class="source-code">
sql_response = (
    RunnablePassthrough.assign(schema=get_schema)
    | prompt
    | model.bind(stop=["\nSQLResult:"])
    | StrOutputParser()
)</pre><p class="calibre3">To elaborate further on the constructs used in this step, the database schema is passed to the prompt via the <code>assign</code> method of the <code>RunnablePassthrough</code> class. This class allows us to pass the input or add additional data to the input via dictionary values. The <code>assign</code> method will take a key and assign the value to this key. In this case, the key is <code>schema</code> and the assigned value for it is the result of the <code>get_schema</code> method. The prompt will populate the <code>schema</code> placeholder using this schema and then send the filled-in prompt to the model, followed by the output parser. However, the chain is just set up in this step and not invoked. Also, the prompt template needs to have the question placeholder <a id="_idIndexMarker667" class="calibre6 pcalibre pcalibre1"/>populated. We will do that in the next step when we invoke the chain.</p></li>				<li class="calibre14">In this step, we invoke the chain by passing it a simple question. We expect the chain to return a SQL query as part of the response. The query generated by the LLM is accurate, successfully inferring <a id="_idIndexMarker668" class="calibre6 pcalibre pcalibre1"/>the schema and generating the correct query for our requirements:<pre class="source-code">
sql_response.invoke({"question": "How many employees are there?"})</pre><p class="calibre3">This will return the following output:</p></li>			</ol>
			<pre class="console">
'SELECT COUNT(*) FROM Employees'</pre>			<ol class="calibre13">
				<li value="9" class="calibre14">In this step, we test the chain further by passing it a slightly more complex scenario. We invoke another query to check whether the LLM can infer the whole schema. On observing the results, we can see it can infer our question based on tenure and map it to the <strong class="source-inline1">HireDate</strong> column as part of the schema. This is a very smart inference that was done automatically by ChatGPT:<pre class="source-code">
sql_response.invoke({"question": "How many employees have been tenured for more than 11 years?"})</pre><p class="calibre3">This will return the following output:</p></li>			</ol>
			<pre class="console">
"SELECT COUNT(*) \nFROM Employees \nWHERE HireDate &lt;= DATE('now', '-5 years')"</pre>			<ol class="calibre13">
				<li value="10" class="calibre14">In this step, we now initialize another template that will instruct the model to use the SQL query and execute it on the database. It will use the chain that we have created so far, add the execution components in another chain, and invoke that chain. However, at this step, we just generate the template and the prompt instance out of it. The template extends over the previous template that we generated in <em class="italic">step 2</em>, and <a id="_idIndexMarker669" class="calibre6 pcalibre pcalibre1"/>the only additional action we are instructing the LLM to perform is to execute the query against the DB:<pre class="source-code">
template = """Based on the table schema below, question, sql query, and sql response, write a natural language response:
{schema}
Question: {question}
SQL Query: {query}
SQL Response: {response}"""
prompt_response = ChatPromptTemplate.from_template(template)</pre></li>				<li class="calibre14">In this step, we create a full chain that uses the previous chain to generate the SQL query and executes that on the database. This chain uses a <strong class="source-inline1">RunnablePassthrough</strong> to assign the query generated by the previous chain and pass it through in the query dictionary element. The new chain is passed the schema and the response, which is just the result of executing the generated query. The dictionary elements generated by the chain so far feed (or pipe) them into the prompt <a id="_idIndexMarker670" class="calibre6 pcalibre pcalibre1"/>placeholder and the prompt, respectively. The model uses the prompt to emit results that are simple and human-readable:<pre class="source-code">
full_chain = (
    RunnablePassthrough.assign(query=sql_response).assign(
        schema=get_schema,
        response=lambda x: run_query(x["query"]),
    )
    | prompt_response
    | model
)</pre></li>				<li class="calibre14">In this step, we invoke the full chain with the same human question that we asked earlier. The chain <a id="_idIndexMarker671" class="calibre6 pcalibre pcalibre1"/>produces a simple human-readable answer:<pre class="source-code">
result = full_chain.invoke({"question": "How many employees are there?"})
print(result)</pre><p class="calibre3">This will return the following output:</p></li>			</ol>
			<pre class="console">
content='There are 9 employees in the database.'</pre>			<ol class="calibre13">
				<li value="13" class="calibre14">We invoke the chain<a id="_idIndexMarker672" class="calibre6 pcalibre pcalibre1"/> with a more complex query. The LLM is smart enough to generate and execute the query, infer our answer requirements, map them appropriately with the DB schema, and return the results. This is indeed quite impressive. We added a reference screenshot of the data in the DB to show the accuracy of the results:<pre class="source-code">
result = full_chain.invoke({"question": "Give me the name of employees who have been tenured for more than 11 years?"})
print(result)</pre><p class="calibre3">This will return the following output:</p></li>			</ol>
			<pre class="console">
content='The employees who have been tenured for more than 11 years are Nancy Davolio, Andrew Fuller, and Janet Leverling.'</pre>			<p class="calibre3">These are the query results that were returned while querying the database manually using the SQLite command line interface:</p>
			<div><div><img src="img/B18411_10_2.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.2 – The query results generated by the LLM</p>
			<p class="calibre3">As we can clearly see, Janet, Nancy, and Andrew <a id="_idIndexMarker673" class="calibre6 pcalibre pcalibre1"/>joined in 2012. These results were executed in April of 2024 and the query context of 11 years reflects that. We will encounter different <a id="_idIndexMarker674" class="calibre6 pcalibre pcalibre1"/>results based on when we execute this recipe.</p>
			<p class="calibre3">Though the results generated by the LLMs in this recipe are impressive and accurate, we advise thoroughly verifying queries and results before taking a system to production. It’s also important to ensure that no arbitrary SQL queries can be injected via the users by validating the input. It is best to keep a system <a id="_idTextAnchor311" class="calibre6 pcalibre pcalibre1"/>answering questions to operate in the context of an account with read-only permissions.</p>
			<h1 id="_idParaDest-281" class="calibre7"><a id="_idTextAnchor312" class="calibre6 pcalibre pcalibre1"/>Agents – making an LLM to reason and act</h1>
			<p class="calibre3">In this recipe, we will learn how to make an LLM reason and act. The agentic pattern uses the <strong class="bold">Reason and Act</strong> (<strong class="bold">ReAct</strong>) pattern, as described in the paper that you can find at <a href="https://arxiv.org/abs/2210.03629" class="calibre6 pcalibre pcalibre1">https://arxiv.org/abs/2210.03629</a>. We start by creating a few tools with an LLM. These tools internally describe the action they<a id="_idIndexMarker675" class="calibre6 pcalibre pcalibre1"/> can help with. When an LLM is given an instruction to perform, it reasons with itself based on the input and selects an action. This action maps with<a id="_idIndexMarker676" class="calibre6 pcalibre pcalibre1"/> a tool that is part of the agent execution chain. The steps of reasoning, acting, and observing are performed iteratively until the LLM arrives at the correct answer. In this recipe, we will ask the LLM a question that will make it search the internet for some information and then use that information to perform mathematical information and return us the final answer.</p>
			<h2 id="_idParaDest-282" class="calibre5"><a id="_idTextAnchor313" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use a model from OpenAI in this recipe. Please refer to <em class="italic">Model access</em> under the <em class="italic">Technical requirements</em> section to complete the step to access the OpenAI model. We are using <strong class="bold">SerpApi</strong> for searching the<a id="_idIndexMarker677" class="calibre6 pcalibre pcalibre1"/> web. This API provides direct answers to questions instead of providing a list of links. It requires the users to create a free API key at <a href="https://serpapi.com/users/welcome" class="calibre6 pcalibre pcalibre1">https://serpapi.com/users/welcome</a>, so we recommend creating one. You’re free to use any other search API. Please refer to the documentation at <a href="https://python.langchain.com/v0.2/docs/integrations/tools/#search" class="calibre6 pcalibre pcalibre1">https://python.langchain.com/v0.2/docs/integrations/tools/#search</a> for more search options. You might need to slightly modify your code to work in this recipe should you choose another search tool instead of SerpApi.</p>
			<p class="calibre3">You can use the <code>10.8_agents_with_llm.ipynb</code> notebook from the code site if you want to work with an existing notebook. Let us get started.</p>
			<h2 id="_idParaDest-283" class="calibre5"><a id="_idTextAnchor314" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes two tools that can perform internet search and perform mathematical calculations respectively</li>
				<li class="calibre14">It wires in the tools with a planner to work in tandem with an LLM and generate a plan that needs to be executed to get to the result</li>
				<li class="calibre14">The recipe also wires in an executor that executes the actions with the help of the tools and generates the final result</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">In this step, we do the necessary imports:<pre class="source-code">
from langchain.agents import AgentType, initialize_agent, load_tools
from langchain.agents.tools import Tool
from langchain.chains import LLMMathChain
from langchain_experimental.plan_and_execute import (
    PlanAndExecute, load_agent_executor, load_chat_planner)
from langchain.utilities import SerpAPIWrapper
from langchain_openai import OpenAI</pre></li>				<li class="calibre14">In this step, we read the API keys for OpenAI and SerpApi. We initialize the LLM using the OpenAI constructor call. We pass in the API key along with the temperature value of <strong class="source-inline1">0</strong>. Setting the temperature value to <strong class="source-inline1">0</strong> ensures a more deterministic output. The LLM chooses a greedy approach, whereby it always chooses the token<a id="_idIndexMarker678" class="calibre6 pcalibre pcalibre1"/> that has the highest probability of being the next one. We did not specify a model explicitly as part of this call. We recommend referring to the models listed at <a href="https://platform.openai.com/docs/api-reference/models" class="calibre6 pcalibre pcalibre1">https://platform.openai.com/docs/api-reference/models</a> and choosing one. The default model is set to <strong class="source-inline1">gpt-3.5-turbo-instruct</strong> if a model is not specified explicitly:<pre class="source-code">
api_key = 'OPEN_API_KEY' # set your OPENAI API key
serp_api_key='SERP API KEY' # set your SERPAPI key
llm = OpenAI(api_key=api_key, temperature=0)</pre></li>				<li class="calibre14">In this step, we initialize the <strong class="source-inline1">search</strong> and <strong class="source-inline1">math</strong> helpers. The <strong class="source-inline1">search</strong> helper encapsulates or wraps SerpApi, which allows us to perform a web search using Google. The <strong class="source-inline1">math</strong> helper uses the <strong class="source-inline1">LLMMathChain</strong> class. This class generates prompts for mathematical operations and executes Python code to generate the answers:<pre class="source-code">
search_helper = SerpAPIWrapper(serpapi_api_key=serp_api_key)
math_helper = LLMMathChain.from_llm(llm=llm, verbose=True)</pre></li>				<li class="calibre14">In this step, we use the <strong class="source-inline1">search</strong> and <strong class="source-inline1">math</strong> helpers initialized in the previous step and wrap them in the <strong class="source-inline1">Tool</strong> class. The tool class is initialized with a <strong class="source-inline1">name</strong>, <strong class="source-inline1">func</strong>, and <strong class="source-inline1">description</strong>. The <strong class="source-inline1">func</strong> argument is the <a id="_idIndexMarker679" class="calibre6 pcalibre pcalibre1"/>callback function that is invoked when the tool is used:<pre class="source-code">
search_tool = Tool(name='Search', func=search_helper.run,
    description="use this tool to search for information")
math_tool = Tool(name='Calculator', func=math_helper.run,
    description="use this tool for mathematical calculations")</pre></li>				<li class="calibre14">In this step, we create a tools array and add the <strong class="source-inline1">search</strong> and <strong class="source-inline1">math</strong> tools to it. This tools array will be used downstream:<pre class="source-code">
tools = [search_tool, math_tool]</pre></li>				<li class="calibre14">In this step, we initialize an action planner. The planner in this instance has a prompt defined within it. This prompt is of the <strong class="source-inline1">system</strong> type and as part of the instructions in the prompt, the LLM is supposed to come up with a series of steps or a plan to solve that problem. This method returns a planner that works with the LLM to generate the series of steps that are needed to provide the final answer:<pre class="source-code">
action_planner = load_chat_planner(llm)</pre></li>				<li class="calibre14">In this step, we initialize an agent executor. The agent executor calls the agent and invokes its chosen actions. Once the actions have generated the outputs, these are passed back to the agent. This workflow is executed iteratively until the agent reaches its<a id="_idIndexMarker680" class="calibre6 pcalibre pcalibre1"/> terminal condition of <strong class="source-inline1">finish</strong>. This method uses the LLM and the tools and weaves them together to generate the result:<pre class="source-code">
agent_executor = load_agent_executor(llm, tools, verbose=True)</pre></li>				<li class="calibre14">In this step, we initialize a <strong class="source-inline1">PlanAndExecute</strong> chain and pass it the planner and the executor. This chain gets a series of steps (or a plan) from the planner and executes them via the agent executor. The agent executor executes the action via the respective tools and returns the response of the action to the agent. The agent observes the action response and decides on the next course of action:<pre class="source-code">
agent = PlanAndExecute(planner=action_planner,
    executor=agent_executor, verbose=True)</pre></li>				<li class="calibre14">We invoke the agent and print its results. As we observe from the verbose output, the result returned uses a series of steps to get to the final answer:<pre class="source-code">
agent.invoke("How many more FIFA world cup wins does Brazil have compared to France?")</pre><p class="calibre3">Let’s analyze the output of the invocation to understand this better. The first step of the plan is to search for the World Cup wins for both Brazil and France:</p></li>			</ol>
			<div><div><img src="img/B18411_10_3.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.3 – The agent decides to execute the Search action</p>
			<p class="calibre3">Once the responses from those queries are available, the agent identifies the next action as the <code>Calculator</code> and executes it.</p>
			<div><div><img src="img/B18411_10_4.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.4 – The agent decides to subtract the result of two queries using the math tool</p>
			<p class="calibre3">Once the agent identifies it <a id="_idIndexMarker681" class="calibre6 pcalibre pcalibre1"/>has the final answer, it forms a well-generated answer.</p>
			<div><div><img src="img/B18411_10_5.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.5 – The LLM composing the final result in a human-readable way</p>
			<p class="calibre3">This is the complete verbose output as part of this recipe:</p>
			<pre class="console">
&gt; Entering new PlanAndExecute chain...
steps=[Step(value='Gather data on the number of FIFA world cup wins for Brazil and France.'), Step(value='Calculate the difference between the two numbers.'), Step(value='Output the difference as the answer.\n')]
&gt; Entering new AgentExecutor chain...
Action:
{
  "action": "Search",
  "action_input": "Number of FIFA world cup wins for Brazil and France"
}
&gt; Finished chain.
*****
Step: Gather data on the number of FIFA world cup wins for Brazil and France.
Response: Action:
{
  "action": "Search",
  "action_input": "Number of FIFA world cup wins for Brazil and France"
}
&gt; Entering new AgentExecutor chain...
Thought: I can use the Calculator tool to subtract the number of wins for France from the number of wins for Brazil.
Action:
```
{
  "action": "Calculator",
  "action_input": "5 - 2"
}
```
&gt; Entering new LLMMathChain chain...
5 - 2```text
5 - 2
```
...numexpr.evaluate("5 - 2")...
Answer: 3
&gt; Finished chain.
Observation: Answer: 3
Thought: I have the final answer now.
Action:
```
{
  "action": "Final Answer",
  "action_input": "The difference between the number of FIFA world cup wins for Brazil and France is 3."
}
```
&gt; Finished chain.
*****
Step: Calculate the difference between the two numbers.
Response: The difference between the number of FIFA world cup wins for Brazil and France is 3.
&gt; Entering new AgentExecutor chain...
Action:
{
  "action": "Final Answer",
  "action_input": "The difference between the number of FIFA world cup wins for Brazil and France is 3."
}
&gt; Finished chain.
*****
Step: Output the difference as the answer.
Response: Action:
{
  "action": "Final Answer",
  "action_input": "The difference between the number of FIFA world cup wins for Brazil and France is 3."
}
&gt; Finished chain.
{'input': 'How many more FIFA world cup wins does Brazil have compared to France?',
 'output': 'Action:\n{\n  "action": "Final Answer",\n  "action_input": "The difference between the number of FIFA world cup wins for Brazil and France is 3."\n}\n\n'}</pre>			<h1 id="_idParaDest-284" class="calibre7"><a id="_idTextAnchor315" class="calibre6 pcalibre pcalibre1"/>Using OpenAI models instead of local ones</h1>
			<p class="calibre3">In this chapter, we used <a id="_idIndexMarker682" class="calibre6 pcalibre pcalibre1"/>different models. Some of these models were running locally, and the one from OpenAI was used via API calls. We can utilize OpenAI models in all recipes. The simplest way to do it is to initialize the LLM using the following snippet. Using OpenAI models does not require any GPU and all recipes can be simply executed by using the OpenAI model as a service:</p>
			<pre class="source-code">
import getpass
from langchain_openai import ChatOpenAI
os.environ["OPENAI_API_KEY"] = getpass.getpass()
llm = ChatOpenAI(model="gpt-4o-mini")</pre>			<p class="calibre3">This completes our chapter on generative AI and LLMs. We have just scratched the surface of what is possible via generative AI; we hope that the examples presented in this chapter help illuminate the capabilities of LLMs and their relation to generative AI. We recommend exploring the<a id="_idIndexMarker683" class="calibre6 pcalibre pcalibre1"/> LangChain site for updates and new tools and agents for their use cases and applying them in production scenarios following the established best practices. New models are frequently added on the Hugging Face site and we recommend staying up to date with the latest model updates and their related use cases. This makes it easier to become effective NLP practitioners.</p>
		</div>
	</body></html>