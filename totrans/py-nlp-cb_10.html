<html><head></head><body>
		<div id="_idContainer048" class="calibre2">
			<h1 id="_idParaDest-257" class="chapter-number"><a id="_idTextAnchor277" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">10</span></h1>
			<h1 id="_idParaDest-258" class="calibre7"><a id="_idTextAnchor278" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Generative AI and Large Language Models</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we will explore recipes that use the generative aspect of the transformer models to generate text. </span><span class="kobospan" id="kobo.3.2">As we touched upon the same in </span><a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.4.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.5.1">, </span><em class="italic"><span class="kobospan" id="kobo.6.1">Transformers and Their Applications</span></em><span class="kobospan" id="kobo.7.1">, the generative aspect of the transformer models uses the decoder component of the transformer network. </span><span class="kobospan" id="kobo.7.2">The decoder component is responsible for generating text based on the </span><span><span class="kobospan" id="kobo.8.1">provided context.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.9.1">With the advent of the </span><strong class="bold"><span class="kobospan" id="kobo.10.1">General Purpose Transformers</span></strong><span class="kobospan" id="kobo.11.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.12.1">GPT</span></strong><span class="kobospan" id="kobo.13.1">) family of </span><strong class="bold"><span class="kobospan" id="kobo.14.1">Large Language Models</span></strong><span class="kobospan" id="kobo.15.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.16.1">LLMs</span></strong><span class="kobospan" id="kobo.17.1">), these have only grown in size and capability with each new version. </span><span class="kobospan" id="kobo.17.2">LLMs such as GPT-4 have been trained on large corpora of text and can match or beat their state-of-the-art counterparts in many NLP tasks. </span><span class="kobospan" id="kobo.17.3">These LLMs have also built upon their generational capability and they can be instructed to generate text based on </span><span><span class="kobospan" id="kobo.18.1">human prompting.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.19.1">We will use generative models based on the transformer architecture for </span><span><span class="kobospan" id="kobo.20.1">our recipes.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.21.1">This chapter contains the </span><span><span class="kobospan" id="kobo.22.1">following recipes:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.23.1">Running an </span><span><span class="kobospan" id="kobo.24.1">LLM locally</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.25.1">Running an LLM to </span><span><span class="kobospan" id="kobo.26.1">follow instructions</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.27.1">Augmenting an LLM with </span><span><span class="kobospan" id="kobo.28.1">external data</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.29.1">Augmenting the LLM with </span><span><span class="kobospan" id="kobo.30.1">external content</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.31.1">Creating a chatbot using </span><span><span class="kobospan" id="kobo.32.1">an LLM</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.33.1">Generating code using </span><span><span class="kobospan" id="kobo.34.1">an LLM</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.35.1">Generating a SQL query using </span><span><span class="kobospan" id="kobo.36.1">human-defined requirements</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.37.1">Agents – making an LLM to reason </span><span><span class="kobospan" id="kobo.38.1">and act</span></span></li>
			</ul>
			<h1 id="_idParaDest-259" class="calibre7"><a id="_idTextAnchor279" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.39.1">Technical requirements</span></h1>
			<p class="calibre3"><a id="_idTextAnchor280" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.40.1">The code for this chapter is in a folder named </span><strong class="source-inline"><span class="kobospan" id="kobo.41.1">Chapter10</span></strong><span class="kobospan" id="kobo.42.1"> in the GitHub repository of the </span><span><span class="kobospan" id="kobo.43.1">book (</span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.44.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10</span></span></a><span><span class="kobospan" id="kobo.45.1">).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.46.1">As in previous chapters, the packages required for this chapter are part of the </span><strong class="source-inline"><span class="kobospan" id="kobo.47.1">poetry</span></strong><span class="kobospan" id="kobo.48.1">/</span><strong class="source-inline"><span class="kobospan" id="kobo.49.1">pip</span></strong><span class="kobospan" id="kobo.50.1"> requirements configuration file that is present in the repository. </span><span class="kobospan" id="kobo.50.2">We recommend that the reader set up the </span><span><span class="kobospan" id="kobo.51.1">environment beforehand.</span></span></p>
			<h2 id="_idParaDest-260" class="calibre5"><a id="_idTextAnchor281" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.52.1">Model access</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.53.1">In this chapter, we will use models from Hugging Face and OpenAI. </span><span class="kobospan" id="kobo.53.2">The following are the instructions to enable model access for the various models that will be used for the recipes in </span><span><span class="kobospan" id="kobo.54.1">this chapter.</span></span></p>
			<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.55.1">Hugging Face Mistral model</span></strong><span class="kobospan" id="kobo.56.1">: Create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. </span><span class="kobospan" id="kobo.56.2">Please visit the Mistral model details at </span><a href="https://huggingface.co/mistralai/Mistral-7B-v0.3" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.57.1">https://huggingface.co/mistralai/Mistral-7B-v0.3</span></a><span class="kobospan" id="kobo.58.1">. </span><span class="kobospan" id="kobo.58.2">You will need to request access to the model on the site before running the recipe that uses </span><span><span class="kobospan" id="kobo.59.1">this model.</span></span></p>
			<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.60.1">Hugging Face Llama model</span></strong><span class="kobospan" id="kobo.61.1">: Create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. </span><span class="kobospan" id="kobo.61.2">Please visit the Llama 3.1 model details at </span><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.62.1">https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct</span></a><span class="kobospan" id="kobo.63.1">. </span><span class="kobospan" id="kobo.63.2">You will have to request for the model access on the site before you run the recipe that uses </span><span><span class="kobospan" id="kobo.64.1">this model.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.65.1">In the code snippets, we are using Jupyter as an environment for execution. </span><span class="kobospan" id="kobo.65.2">If you are using the same, you will something like the screenshot shown here. </span><span class="kobospan" id="kobo.65.3">You can enter the token in the text field and let the recipe make progress. </span><span class="kobospan" id="kobo.65.4">The recipe will wait for the token to be entered the first time. </span><span class="kobospan" id="kobo.65.5">Subsequent runs of the recipe will use the cached token that the Hugging Face library creates for the </span><span><span class="kobospan" id="kobo.66.1">user locally.</span></span></p>
			<div class="calibre2">
				<div id="_idContainer043" class="img---figure">
					<span class="kobospan" id="kobo.67.1"><img src="image/B18411_10_1.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.68.1">Figure 10.1 – Copying a token from Hugging Face</span></p>
			<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.69.1">OpenAI model</span></strong><span class="kobospan" id="kobo.70.1">: For the recipes that use the model from OpenAI, we recommend that the user create an account to generate an API token. </span><span class="kobospan" id="kobo.70.2">Please refer to the documentation at </span><a href="https://openai.com/blog/openai-api" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.71.1">https://openai.com/blog/openai-api</span></a><span class="kobospan" id="kobo.72.1"> for more information. </span><span class="kobospan" id="kobo.72.2">Access to the OpenAI models requires the </span><strong class="source-inline"><span class="kobospan" id="kobo.73.1">api-token</span></strong><span class="kobospan" id="kobo.74.1">. </span><span class="kobospan" id="kobo.74.2">In the code snippets, we are using Jupyter as an environment for execution. </span><span class="kobospan" id="kobo.74.3">If you are using the same, you will see a text box where you will need to enter the </span><strong class="source-inline"><span class="kobospan" id="kobo.75.1">api-token</span></strong><span class="kobospan" id="kobo.76.1">. </span><span class="kobospan" id="kobo.76.2">You can enter the token in the text field and let the recipe make progress. </span><span class="kobospan" id="kobo.76.3">The recipe will wait for the token to </span><span><span class="kobospan" id="kobo.77.1">be entered.</span></span></p>
			<h1 id="_idParaDest-261" class="calibre7"><span class="kobospan" id="kobo.78.1">R</span><a id="_idTextAnchor282" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.79.1">unning an LLM locally</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.80.1">In this recipe, we will learn how to load an LLM locally using the CPU or GPU and generate text from it after giving it a starting text as seed input. </span><span class="kobospan" id="kobo.80.2">An LLM running locally can be instructed to generate text based on prompting. </span><span class="kobospan" id="kobo.80.3">This new paradigm of generation of text via instruction prompting has brought the LLM to recent prominence. </span><span class="kobospan" id="kobo.80.4">Learning to do this allows for</span><a id="_idIndexMarker585" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.81.1"> control over hardware resources and environment setup, optimizing performance and enabling rapid experimentation or prototyping with text generation from seed inputs. </span><span class="kobospan" id="kobo.81.2">This enhances data privacy and security, along with a reduced reliance on cloud services, and facilitates cost-effective deployment for educational and practical applications. </span><span class="kobospan" id="kobo.81.3">As we run an LLM locally as part of the recipe, we will use instruction prompting to make it generate text based on a </span><span><span class="kobospan" id="kobo.82.1">simple instruction.</span></span></p>
			<h2 id="_idParaDest-262" class="calibre5"><a id="_idTextAnchor283" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.83.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.84.1">We recommend that you use a system with at least 16 GB of RAM or a system with a GPU that has at least 8 GB of VRAM. </span><span class="kobospan" id="kobo.84.2">These examples were created on a system with 8 GB of RAM and an nVidia </span><a id="_idIndexMarker586" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.85.1">RTX 2070 GPU with 8 GB of VRAM. </span><span class="kobospan" id="kobo.85.2">These examples will work without a GPU as long as there is 16 GB of system RAM. </span><span class="kobospan" id="kobo.85.3">In this recipe, we </span><a id="_idIndexMarker587" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.86.1">will load the </span><strong class="bold"><span class="kobospan" id="kobo.87.1">Mistral-7B</span></strong><span class="kobospan" id="kobo.88.1"> model using the Hugging Face (</span><a href="https://huggingface.co/docs" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.89.1">https://huggingface.co/docs</span></a><span class="kobospan" id="kobo.90.1">) libraries. </span><span class="kobospan" id="kobo.90.2">This model has a smaller size compared to other language models in its class but can outperform them on several NLP tasks. </span><span class="kobospan" id="kobo.90.3">The Mistral-7B model with 7 billion network parameters can</span><a id="_idIndexMarker588" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.91.1"> outperform the </span><strong class="bold"><span class="kobospan" id="kobo.92.1">Llama2</span></strong><span class="kobospan" id="kobo.93.1"> model, which has over 13 </span><span><span class="kobospan" id="kobo.94.1">billion parameters.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.95.1">It is required that the user create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. </span><span class="kobospan" id="kobo.95.2">Please refer to </span><em class="italic"><span class="kobospan" id="kobo.96.1">Model access</span></em><span class="kobospan" id="kobo.97.1"> under the </span><em class="italic"><span class="kobospan" id="kobo.98.1">Technical requirements</span></em><span class="kobospan" id="kobo.99.1"> section to complete the step to access the Mistral model. </span><span class="kobospan" id="kobo.99.2">Please note that due to the compute requirements for this recipe, it might take a few minutes for it to complete the text generation. </span><span class="kobospan" id="kobo.99.3">If the required compute capacity is unavailable, we recommend that the reader refer to the </span><em class="italic"><span class="kobospan" id="kobo.100.1">Using OpenAI models instead of local ones</span></em><span class="kobospan" id="kobo.101.1"> section at the end of this chapter and use the method described there to use an OpenAI model for </span><span><span class="kobospan" id="kobo.102.1">this recipe.</span></span></p>
			<h2 id="_idParaDest-263" class="calibre5"><a id="_idTextAnchor284" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.103.1">How to do it…</span></h2>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.104.1">Do the </span><span><span class="kobospan" id="kobo.105.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.106.1">
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, GenerationConfig)
import torch</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.107.1">In this step, we set up the login for Hugging Face. </span><span class="kobospan" id="kobo.107.2">Though we can set the token directly in the code, we recommend setting the token in an environment variable and then reading from it in the notebook. </span><span class="kobospan" id="kobo.107.3">Calling the </span><strong class="source-inline1"><span class="kobospan" id="kobo.108.1">login</span></strong><span class="kobospan" id="kobo.109.1"> method with the token authorizes the call to Hugging Face and allows the code to download the model locally and </span><span><span class="kobospan" id="kobo.110.1">use it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.111.1">
from huggingface_hub import login
hf_token = os.environ.get('HUGGINGFACE_TOKEN')
login(token=hf_token)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.112.1">In this step, we initialize the device, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.113.1">mistralai/Mistral-7B-v0.3</span></strong><span class="kobospan" id="kobo.114.1"> model, and the tokenizer, respectively. </span><span class="kobospan" id="kobo.114.2">We set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.115.1">device_map</span></strong><span class="kobospan" id="kobo.116.1"> parameter to </span><strong class="source-inline1"><span class="kobospan" id="kobo.117.1">auto</span></strong><span class="kobospan" id="kobo.118.1">, which lets the pipeline pick the available device to use. </span><span class="kobospan" id="kobo.118.2">We set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.119.1">load_in_4bit</span></strong><span class="kobospan" id="kobo.120.1"> parameter to </span><strong class="source-inline1"><span class="kobospan" id="kobo.121.1">True</span></strong><span class="kobospan" id="kobo.122.1">. </span><span class="kobospan" id="kobo.122.2">This lets us load the quantized model for the inference (or generation) step. </span><span class="kobospan" id="kobo.122.3">Using a quantized model consumes less memory and lets us load the model locally on systems with limited memory. </span><span class="kobospan" id="kobo.122.4">The loading of the quantized model is handled by the </span><strong class="source-inline1"><span class="kobospan" id="kobo.123.1">AutoModelForCausalLM</span></strong><span class="kobospan" id="kobo.124.1"> module, and it downloads </span><a id="_idIndexMarker589" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.125.1">a model from the Hugging Face hub that has been quantized to the bit size specified in </span><span><span class="kobospan" id="kobo.126.1">the parameter:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.127.1">
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.3", device_map="auto", 
        load_in_4bit=True)
tokenizer = AutoTokenizer.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    padding_side="left")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.128.1">In this step, we initialize a generation config. </span><span class="kobospan" id="kobo.128.2">This generation config is passed to the model, instructing it on how to generate the text. </span><span class="kobospan" id="kobo.128.3">We set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.129.1">num_beams</span></strong><span class="kobospan" id="kobo.130.1"> parameter to </span><strong class="source-inline1"><span class="kobospan" id="kobo.131.1">4</span></strong><span class="kobospan" id="kobo.132.1">. </span><span class="kobospan" id="kobo.132.2">This parameter results in the generated text being more coherent and grammatically correct as the number of beams is increased. </span><span class="kobospan" id="kobo.132.3">However, a greater number of beams also results in decoding (or text-generation) time. </span><span class="kobospan" id="kobo.132.4">We set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.133.1">early_stopping</span></strong><span class="kobospan" id="kobo.134.1"> parameter to </span><strong class="source-inline1"><span class="kobospan" id="kobo.135.1">True</span></strong><span class="kobospan" id="kobo.136.1"> as the generation of the next word is concluded as soon as the number of beams reaches the value specified in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.137.1">num_beams</span></strong><span class="kobospan" id="kobo.138.1"> parameter. </span><span class="kobospan" id="kobo.138.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.139.1">eos_token_id</span></strong><span class="kobospan" id="kobo.140.1"> (e.g., </span><strong class="source-inline1"><span class="kobospan" id="kobo.141.1">50256</span></strong><span class="kobospan" id="kobo.142.1"> for GPT models) and </span><strong class="source-inline1"><span class="kobospan" id="kobo.143.1">pad_token_id</span></strong><span class="kobospan" id="kobo.144.1"> (e.g., </span><strong class="source-inline1"><span class="kobospan" id="kobo.145.1">0</span></strong><span class="kobospan" id="kobo.146.1"> for GPT models) are defaulted to use the model’s token IDs. </span><span class="kobospan" id="kobo.146.2">These token IDs are used to specify the end-of-sentence and padding tokens that will be used by the model. </span><span class="kobospan" id="kobo.146.3">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.147.1">max_new_tokens</span></strong><span class="kobospan" id="kobo.148.1"> parameter specifies the maximum number of tokens that will be generated. </span><span class="kobospan" id="kobo.148.2">There are more parameters that can be specified for generating the text and we encourage you to play around with different values of the previously specified parameters, as well as any additional parameters for </span><a id="_idIndexMarker590" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.149.1">customizing the text generation. </span><span class="kobospan" id="kobo.149.2">For more information, please refer to the transformer documentation on the </span><strong class="source-inline1"><span class="kobospan" id="kobo.150.1">GenerationConfig</span></strong><span class="kobospan" id="kobo.151.1"> class </span><span><span class="kobospan" id="kobo.152.1">at </span></span><a href="https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.153.1">https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py</span></span></a><span><span class="kobospan" id="kobo.154.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.155.1">
generation_config = GenerationConfig(
    num_beams=4,
    early_stopping=True,
    eos_token_id=model.config.eos_token_id,
    pad_token_id=model.config.eos_token_id,
    max_new_tokens=900,
)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.156.1">In this step, we initialize a seed sentence. </span><span class="kobospan" id="kobo.156.2">This seed sentence acts as a prompt to the model asking it to generate a step-by-step way to make an </span><span><span class="kobospan" id="kobo.157.1">apple pie:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.158.1">
seed_sentence = "Step by step way on how to make an apple pie:"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.159.1">In this step, we tokenize the seed sentence to transform the text into the corresponding embedded representation and pass it to the model to generate the text. </span><span class="kobospan" id="kobo.159.2">We also pass the </span><strong class="source-inline1"><span class="kobospan" id="kobo.160.1">generation_config</span></strong><span class="kobospan" id="kobo.161.1"> instance to it. </span><span class="kobospan" id="kobo.161.2">The model generates the token IDs as part of </span><span><span class="kobospan" id="kobo.162.1">its generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.163.1">
model_inputs = tokenizer(
    [seed_sentence], return_tensors="pt").to(device)
generated_ids = model.generate(**model_inputs,
    generation_config=generation_config)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.164.1">In this step, we decode the token IDs that were generated from the previous step. </span><span class="kobospan" id="kobo.164.2">The transformer </span><a id="_idIndexMarker591" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.165.1">model uses special tokens such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.166.1">CLS</span></strong><span class="kobospan" id="kobo.167.1"> or </span><strong class="source-inline1"><span class="kobospan" id="kobo.168.1">MASK</span></strong><span class="kobospan" id="kobo.169.1"> and to generate the text as part of the training. </span><span class="kobospan" id="kobo.169.2">We set the value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.170.1">skip_special_tokens</span></strong><span class="kobospan" id="kobo.171.1"> to </span><strong class="source-inline1"><span class="kobospan" id="kobo.172.1">True</span></strong><span class="kobospan" id="kobo.173.1">. </span><span class="kobospan" id="kobo.173.2">This allows us to omit these special tokens and generate pure text as part of our output. </span><span class="kobospan" id="kobo.173.3">We print the decoded (or </span><span><span class="kobospan" id="kobo.174.1">generated) text.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.175.1">
generated_tokens = tokenizer.batch_decode(generated_ids,
    skip_special_tokens=True)[0]
print(generated_tokens)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.176.1">The output would look like the following. </span><span class="kobospan" id="kobo.176.2">We have shortened the output for brevity. </span><span class="kobospan" id="kobo.176.3">You might see a </span><span><span class="kobospan" id="kobo.177.1">longer result:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.178.1">
Step by step way on how to make an apple pie:
1. </span><span class="kobospan1" id="kobo.178.2">Preheat the oven to 350 degrees Fahrenheit.
</span><span class="kobospan1" id="kobo.178.3">2. </span><span class="kobospan1" id="kobo.178.4">Peel and core the apples.
</span><span class="kobospan1" id="kobo.178.5">3. </span><span class="kobospan1" id="kobo.178.6">Cut the apples into thin slices.
</span><span class="kobospan1" id="kobo.178.7">4. </span><span class="kobospan1" id="kobo.178.8">Place the apples in a large bowl.
</span><span class="kobospan1" id="kobo.178.9">5. </span><span class="kobospan1" id="kobo.178.10">Add the sugar, cinnamon, and nutmeg to the apples.
</span><span class="kobospan1" id="kobo.178.11">6. </span><span class="kobospan1" id="kobo.178.12">Stir the apples until they are evenly coated with the sugar and spices.
</span><span class="kobospan1" id="kobo.178.13">7. </span><span class="kobospan1" id="kobo.178.14">Pour the apples into a pie dish.
</span><span class="kobospan1" id="kobo.178.15">8. </span><span class="kobospan1" id="kobo.178.16">Place the pie dish on a baking sheet.
</span><span class="kobospan1" id="kobo.178.17">9. </span><span class="kobospan1" id="kobo.178.18">Bake the pie for 45 minutes to 1 hour, or until the apples are soft and the crust is golden brown.
</span><span class="kobospan1" id="kobo.178.19">10. </span><span class="kobospan1" id="kobo.178.20">Remove the pie from the oven and let it cool for 10 minutes before serving.
</span><span class="kobospan1" id="kobo.178.21">## How do you make an apple pie from scratch?
</span><span class="kobospan1" id="kobo.178.22">To make an apple pie from scratch, you will need the following ingredients:
- 2 cups of all-purpose flour
- 1 teaspoon of salt
- 1/2 cup of shortening
- 1/2 cup of cold water
- 4 cups of peeled, cored, and sliced apples
- 1 cup of sugar
- 1 teaspoon of cinnamon
- 1/4 teaspoon of nutmeg
- 1/4 teaspoon of allspice
- 2 tablespoons of cornstarch
- 1 tablespoon of lemon juice
To make the pie crust, combine the flour and salt in a large bowl. </span><span class="kobospan1" id="kobo.178.23">Cut in the shortening with a pastry blender or two knives until the mixture resembles coarse crumbs. </span><span class="kobospan1" id="kobo.178.24">Add the cold water, 1 tablespoon at a time, until the dough comes together. </span><span class="kobospan1" id="kobo.178.25">Divide the dough in half and shape each half into a disk. </span><span class="kobospan1" id="kobo.178.26">Wrap the disks in plastic wrap and refrigerate for at least 30 min</span><a id="_idTextAnchor285" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.179.1">utes.</span></pre>			<h1 id="_idParaDest-264" class="calibre7"><a id="_idTextAnchor286" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.180.1">Running an LLM to follow instructions</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.181.1">In this recipe, we will learn how to get an LLM to follow instructions via prompting. </span><span class="kobospan" id="kobo.181.2">An LLM can be provided some</span><a id="_idIndexMarker592" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.182.1"> context and asked to generate text based on that context. </span><span class="kobospan" id="kobo.182.2">This is a very novel feature of an LLM. </span><span class="kobospan" id="kobo.182.3">The LLM can be specifically instructed to generate text based on explicit user requirements. </span><span class="kobospan" id="kobo.182.4">Using this feature expands the breadth of use cases and applications that can be developed. </span><span class="kobospan" id="kobo.182.5">The context and the question to be answered can be generated dynamically and used in various use cases ranging from answering simple math problems to sophisticated data extraction from </span><span><span class="kobospan" id="kobo.183.1">knowledge bases.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.184.1">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.185.1">meta-llama/Meta-Llama-3.1-8B-Instruct</span></strong><span class="kobospan" id="kobo.186.1"> model for this recipe. </span><span class="kobospan" id="kobo.186.2">This model is built on top of the </span><strong class="source-inline"><span class="kobospan" id="kobo.187.1">meta-llama/Meta-Llama-3.1-8B</span></strong><span class="kobospan" id="kobo.188.1"> model and has been tuned to follow instructions </span><span><span class="kobospan" id="kobo.189.1">via prompts.</span></span></p>
			<h2 id="_idParaDest-265" class="calibre5"><a id="_idTextAnchor287" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.190.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.191.1">It is required that the user create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. </span><span class="kobospan" id="kobo.191.2">Please refer to </span><em class="italic"><span class="kobospan" id="kobo.192.1">Model access</span></em><span class="kobospan" id="kobo.193.1"> under the </span><em class="italic"><span class="kobospan" id="kobo.194.1">Technical requirements</span></em><span class="kobospan" id="kobo.195.1"> section to complete the step to access the </span><span><span class="kobospan" id="kobo.196.1">Llama model.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.197.1">You can use the </span><strong class="bold"><span class="kobospan" id="kobo.198.1">10.2_instruct_llm.ipynb</span></strong><span class="kobospan" id="kobo.199.1"> notebook</span><a id="_idIndexMarker593" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.200.1"> from the code site if you want to work from an existing notebook. </span><span class="kobospan" id="kobo.200.2">Please note that due to the compute requirements for this recipe, it might take a few minutes for it to complete the text generation. </span><span class="kobospan" id="kobo.200.3">If the required compute capacity is unavailable, we recommend that the reader refer to the </span><em class="italic"><span class="kobospan" id="kobo.201.1">Using OpenAI models instead of local ones</span></em><span class="kobospan" id="kobo.202.1"> section at the end of this chapter and use the method described there to use an OpenAI model for </span><span><span class="kobospan" id="kobo.203.1">this recipe.</span></span></p>
			<h2 id="_idParaDest-266" class="calibre5"><a id="_idTextAnchor288" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.204.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.205.1">The recipe does the </span><span><span class="kobospan" id="kobo.206.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.207.1">It initializes an LLM model to be loaded </span><span><span class="kobospan" id="kobo.208.1">into memory.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.209.1">It initializes a prompt to instruct the LLM to perform a task. </span><span class="kobospan" id="kobo.209.2">This task is that of answering </span><span><span class="kobospan" id="kobo.210.1">a question.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.211.1">It sends the prompt to the LLM and asks it to generate </span><span><span class="kobospan" id="kobo.212.1">an answer.</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.213.1">The steps for the</span><a id="_idIndexMarker594" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.214.1"> recipe are </span><span><span class="kobospan" id="kobo.215.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.216.1">Do the </span><span><span class="kobospan" id="kobo.217.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.218.1">
import (
    AutoModelForCausalLM, AutoTokenizer,
    BitsAndBytesConfig, GenerationConfig, pipeline)
import os
import torch</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.219.1">Set up the login for Hugging Face. </span><span class="kobospan" id="kobo.219.2">Set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.220.1">HuggingFace</span></strong><span class="kobospan" id="kobo.221.1"> token in an environment variable and read from it into a local variable. </span><span class="kobospan" id="kobo.221.2">Calling the </span><strong class="source-inline1"><span class="kobospan" id="kobo.222.1">login</span></strong><span class="kobospan" id="kobo.223.1"> method with the token authorizes the call to </span><strong class="source-inline1"><span class="kobospan" id="kobo.224.1">HuggingFace</span></strong><span class="kobospan" id="kobo.225.1"> and allows the code to download the model locally and use it. </span><span class="kobospan" id="kobo.225.2">You will see a similar login window as the one shown in the </span><em class="italic"><span class="kobospan" id="kobo.226.1">Running an LLM locally</span></em><span class="kobospan" id="kobo.227.1"> recipe in </span><span><span class="kobospan" id="kobo.228.1">this chapter:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.229.1">
from huggingface_hub import login
hf_token = os.environ.get('HUGGINGFACE_TOKEN')
login(token=hf_token)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.230.1">In this step, we specify the model name. </span><span class="kobospan" id="kobo.230.2">We also define the quantization configuration. </span><span class="kobospan" id="kobo.230.3">Quantization is a technique to reduce the size of the internal LLM network weights to a lower precision. </span><span class="kobospan" id="kobo.230.4">This allows us to load the model on systems with limited CPU or GPU memory. </span><span class="kobospan" id="kobo.230.5">Loading an LLM with its default precision requires a large amount of CPU/GPU memory. </span><span class="kobospan" id="kobo.230.6">In this case, we load the network weights in four bits using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.231.1">load_in_4bit</span></strong><span class="kobospan" id="kobo.232.1"> parameter of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.233.1">BitsAndBytesConfig</span></strong><span class="kobospan" id="kobo.234.1"> class. </span><span class="kobospan" id="kobo.234.2">The other parameters used are described </span><span><span class="kobospan" id="kobo.235.1">as follows:</span></span><ul class="calibre19"><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.236.1">bnb_4bit_compute_dtype</span></strong><span class="kobospan" id="kobo.237.1">: This parameter specifies the data type that is used during the computation. </span><span class="kobospan" id="kobo.237.2">Though the network weights are stored in four bits, the computation still happens in 16 or 32 bits as defined by this parameter. </span><span class="kobospan" id="kobo.237.3">Setting this to </span><strong class="source-inline1"><span class="kobospan" id="kobo.238.1">torch.float16</span></strong><span class="kobospan" id="kobo.239.1"> results in speed improvements in </span><span><span class="kobospan" id="kobo.240.1">certain scenarios.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.241.1">bnb_4bit_use_double_quant</span></strong><span class="kobospan" id="kobo.242.1">: This parameter specifies that nested quantization should be used. </span><span class="kobospan" id="kobo.242.2">This means that a second quantization is</span><a id="_idIndexMarker595" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.243.1"> performed which saves an additional 0.4 bits per parameter in the network. </span><span class="kobospan" id="kobo.243.2">This helps us save the memory needed for </span><span><span class="kobospan" id="kobo.244.1">the model.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.245.1">bnb_4bit_quant_type</span></strong><span class="kobospan" id="kobo.246.1">: This </span><strong class="source-inline1"><span class="kobospan" id="kobo.247.1">nf4</span></strong><span class="kobospan" id="kobo.248.1"> parameter value initializes the weights of the network using a normal distribution, which is useful during the training of the model. </span><span class="kobospan" id="kobo.248.2">However, it does not have any impact on inference, such as for this recipe. </span><span class="kobospan" id="kobo.248.3">We will still be setting this to </span><strong class="source-inline1"><span class="kobospan" id="kobo.249.1">nf4</span></strong><span class="kobospan" id="kobo.250.1"> to keep it consistent with the </span><span><span class="kobospan" id="kobo.251.1">model weights.</span></span></li></ul><p class="calibre3"><span class="kobospan" id="kobo.252.1">For quantization concepts, we recommend referring to the blog post at </span><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.253.1">https://huggingface.co/blog/4bit-transformers-bitsandbytes</span></a><span class="kobospan" id="kobo.254.1">, where this is explained in greater detail. </span><span class="kobospan" id="kobo.254.2">Please note that in order to load the model in 4-bit, it is required that a GPU </span><span><span class="kobospan" id="kobo.255.1">is used:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.256.1">
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type= "nf4"
    )</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.257.1">In this step, we load the </span><strong class="source-inline1"><span class="kobospan" id="kobo.258.1">meta-llama/Meta-Llama-3.1-8B-Instruct</span></strong><span class="kobospan" id="kobo.259.1">  model and the </span><span><span class="kobospan" id="kobo.260.1">corresponding tokenizer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.261.1">
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(model_name)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.262.1">In this step, we initialize a pipeline that weaves the model and tokenizer together with some additional parameters. </span><span class="kobospan" id="kobo.262.2">We covered the description of these parameters in the </span><em class="italic"><span class="kobospan" id="kobo.263.1">Running an LLM locally</span></em><span class="kobospan" id="kobo.264.1"> recipe in this chapter. </span><span class="kobospan" id="kobo.264.2">We recommend referring to that recipe for more details on these parameters. </span><span class="kobospan" id="kobo.264.3">We are adding an additional parameter named </span><strong class="source-inline1"><span class="kobospan" id="kobo.265.1">repetition_penalty</span></strong><span class="kobospan" id="kobo.266.1"> here. </span><span class="kobospan" id="kobo.266.2">This ensures that the LLM </span><a id="_idIndexMarker596" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.267.1">does not go into a state where it starts repeating itself or parts of the text that were </span><span><span class="kobospan" id="kobo.268.1">generated before:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.269.1">
pipe = pipeline("text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    pad_token_id = tokenizer.eos_token_id,
    eos_token_id=model.config.eos_token_id,
    num_beams=4,
    early_stopping=True,
    repetition_penalty=1.4)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.270.1">In this step, we create a prompt that sets up an instruction context that can be passed to the LLM. </span><span class="kobospan" id="kobo.270.2">The LLM acts as per the instructions set up in the prompt. </span><span class="kobospan" id="kobo.270.3">In this case, we start our instruction with a conversation between the user and the agent. </span><span class="kobospan" id="kobo.270.4">The conversation starts with the question </span><strong class="source-inline1"><span class="kobospan" id="kobo.271.1">What is your favourite country?</span></strong><span class="kobospan" id="kobo.272.1">. </span><span class="kobospan" id="kobo.272.2">This question is followed by the model answer in the form of </span><strong class="source-inline1"><span class="kobospan" id="kobo.273.1">Well, I am quite fascinated with Peru.</span></strong><span class="kobospan" id="kobo.274.1">. </span><span class="kobospan" id="kobo.274.2">We then follow it up with another instruction by asking the question </span><strong class="source-inline1"><span class="kobospan" id="kobo.275.1">What can you tell me about Peru?</span></strong><span class="kobospan" id="kobo.276.1">. </span><span class="kobospan" id="kobo.276.2">This methodology serves as a template for the LLM to learn our intent and generate an answer for the follow-up question based on the pattern we</span><a id="_idIndexMarker597" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.277.1"> specified in our </span><span><span class="kobospan" id="kobo.278.1">instruction prompt:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.279.1">
prompt = [
    {"role": "user", "content": "What is your favourite country?"},
    {"role": "assistant", "content": "Well, I am quite fascinated with Peru."},
    {"role": "user", "content": "What can you tell me about Peru?"}
]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.280.1">In this step, we execute the pipeline with the prompt and execute it. </span><span class="kobospan" id="kobo.280.2">We additionally specify the maximum number of tokens that should be generated as part of the output. </span><span class="kobospan" id="kobo.280.3">This explicitly instructs the LLM to stop generation once the specific length </span><span><span class="kobospan" id="kobo.281.1">is reached:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.282.1">
outputs = pipe(
    prompt,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"][-1]['content'])</span></pre><p class="calibre3"><span class="kobospan" id="kobo.283.1">This will result in the </span><span><span class="kobospan" id="kobo.284.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.285.1">
Peru! </span><span class="kobospan1" id="kobo.285.2">A country with a rich history, diverse culture, and breathtaking landscapes. </span><span class="kobospan1" id="kobo.285.3">Here are some interesting facts about Peru:
1. </span><span class="kobospan1" id="kobo.285.4">**Location**: Peru is located in western South America, bordering the Pacific Ocean to the west, Ecuador and Colombia to the north, Brazil and Bolivia to the east, and Chile to the south.
</span><span class="kobospan1" id="kobo.285.5">2. </span><span class="kobospan1" id="kobo.285.6">**History**: Peru has a long and complex history, with various civilizations rising and falling over the centuries. </span><span class="kobospan1" id="kobo.285.7">The Inca Empire, which flourished from the 13th to the 16th century, is one of the most famous and influential empires in Peruvian history.
</span><span class="kobospan1" id="kobo.285.8">3. </span><span class="kobospan1" id="kobo.285.9">**Machu Picchu**: One of the Seven Wonders of the World, Machu Picchu is an Inca citadel located on a mountain ridge above the Urubamba Valley. </span><span class="kobospan1" id="kobo.285.10">It's a must-visit destination for any traveler to Peru.
</span><span class="kobospan1" id="kobo.285.11">4. </span><span class="kobospan1" id="kobo.285.12">**Food**: Peruvian cuisine is a fusion of indigenous, Spanish, African, and Asian influences. </span><span class="kobospan1" id="kobo.285.13">Popular dishes include ceviche (raw fish marinated in citrus juices), lomo saltado (stir-fried beef), and ají de gallina (shredded chicken in a spicy yellow pepper sauce).
</span><span class="kobospan1" id="kobo.285.14">5. </span><span class="kobospan1" id="kobo.285.15">**Language**: The official language is Spanish, but many</span></pre>			<h2 id="_idParaDest-267" class="calibre5"><a id="_idTextAnchor289" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.286.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.287.1">Now that we have seen a </span><a id="_idIndexMarker598" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.288.1">way to instruct a model to generate text, we can just change the prompt and get the model to generate text for a completely different kind of question. </span><span class="kobospan" id="kobo.288.2">Let us change the prompt text to the following and use the same recipe to generate text based on the </span><span><span class="kobospan" id="kobo.289.1">updated prompt:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.290.1">
prompt = [
    {"role": "user", "content": "Mary is twice as old as Sarah presently. </span><span class="kobospan1" id="kobo.290.2">Sarah is 6 years old.?"},
    {"role": "assistant", "content": "Well, what can I help you with?"},
    {"role": "user", "content": "Can you tell me in a step by step way on how old Mary will be after 5 years?"}]</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.291.1">This results in the </span><span><span class="kobospan" id="kobo.292.1">following output:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.293.1">
**Step 1: Determine Sarah's current age**
Sarah is 6 years old.
</span><span class="kobospan1" id="kobo.293.2">**Step 2: Determine Mary's current age**
Since Mary is twice as old as Sarah, and Sarah is 6 years old, we can multiply Sarah's age by 2 to find Mary's age:
Mary's age = 2 x Sarah's age
Mary's age = 2 x 6
Mary's age = 12 years old
**Step 3: Calculate Mary's age after 5 years**
To find out how old Mary will be after 5 years, we add 5 to her current age:
Mary's age after 5 years = Mary's current age + 5
Mary's age after 5 years = 12 + 5
Mary's age after 5 years = 17 years old
Therefore, Mary will be 17 years old after 5 years.</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.294.1">As we can see from the preceding output, the model is able to understand the instructions quite</span><a id="_idIndexMarker599" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.295.1"> clearly. </span><span class="kobospan" id="kobo.295.2">It is able to reason well and answer the question correctly. </span><span class="kobospan" id="kobo.295.3">This recipe only used the context that was stored within the LLM. </span><span class="kobospan" id="kobo.295.4">More specifically, the LLM used its internal knowledge to answer this question. </span><span class="kobospan" id="kobo.295.5">LLMs are trained on huge corpora of text and can generate answers based on that large corpus. </span><span class="kobospan" id="kobo.295.6">In the next recipe, we will learn how to augment the knowled</span><a id="_idTextAnchor290" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.296.1">ge of </span><span><span class="kobospan" id="kobo.297.1">an LLM.</span></span></p>
			<h1 id="_idParaDest-268" class="calibre7"><a id="_idTextAnchor291" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.298.1">Augmenting an LLM with external data</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.299.1">In the following recipes, we will learn </span><a id="_idIndexMarker600" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.300.1">how to get an LLM to answer questions on which it has not been trained. </span><span class="kobospan" id="kobo.300.2">These could include information that was created after the LLM was trained. </span><span class="kobospan" id="kobo.300.3">New content keeps getting added to the World Wide Web daily. </span><span class="kobospan" id="kobo.300.4">There is no one LLM that can be trained on that context every day. </span><span class="kobospan" id="kobo.300.5">The </span><strong class="bold"><span class="kobospan" id="kobo.301.1">Retriever Augmented Generation</span></strong><span class="kobospan" id="kobo.302.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.303.1">RAG</span></strong><span class="kobospan" id="kobo.304.1">) frameworks allow us to </span><a id="_idIndexMarker601" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.305.1">augment the LLM with additional content that can be sent as input to it for generating content for downstream tasks. </span><span class="kobospan" id="kobo.305.2">This allows us to save on costs too since we do not have to spend time and compute costs on retraining a model based on updated content. </span><span class="kobospan" id="kobo.305.3">As a basic introduction to RAG, we will augment an LLM with some content from a few web pages and ask some questions pertaining to the content contained in those pages. </span><span class="kobospan" id="kobo.305.4">For this recipe, we will first load the LLM and ask it a few questions without providing it any context. </span><span class="kobospan" id="kobo.305.5">We will then augment this LLM with additional context and ask the same questions. </span><span class="kobospan" id="kobo.305.6">We will compare the answers, which will demonstrate the power of the LLM when coupled with </span><span><span class="kobospan" id="kobo.306.1">aug</span><a id="_idTextAnchor292" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.307.1">mented content.</span></span></p>
			<h2 id="_idParaDest-269" class="calibre5"><a id="_idTextAnchor293" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.308.1">Executing a simple prompt-to-LLM chain</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.309.1">In this recipe, we will create a simple prompt that can be used to instruct an LLM. </span><span class="kobospan" id="kobo.309.2">A prompt is a template with placeholder values that can be populated at runtime. </span><span class="kobospan" id="kobo.309.3">The LangChain framework allows us</span><a id="_idIndexMarker602" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.310.1"> to weave a prompt and an LLM together, along with other components in the mix, to generate text. </span><span class="kobospan" id="kobo.310.2">We </span><a id="_idIndexMarker603" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.311.1">will explore these techniques in this and some of the recipes </span><span><span class="kobospan" id="kobo.312.1">that follow.</span></span></p>
			<h3 class="calibre8"><span class="kobospan" id="kobo.313.1">Getting ready</span></h3>
			<p class="calibre3"><span class="kobospan" id="kobo.314.1">We must create the necessary credentials on the Hugging Face site to ensure that the model is available to be used or downloaded via the code. </span><span class="kobospan" id="kobo.314.2">Please refer to </span><em class="italic"><span class="kobospan" id="kobo.315.1">Model access</span></em><span class="kobospan" id="kobo.316.1"> under the </span><em class="italic"><span class="kobospan" id="kobo.317.1">Technical requirements</span></em><span class="kobospan" id="kobo.318.1"> section to complete the step to access the </span><span><span class="kobospan" id="kobo.319.1">Llama model.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.320.1">In this recipe, we will use the LangChain framework (</span><a href="https://www.langchain.com/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.321.1">https://www.langchain.com/</span></a><span class="kobospan" id="kobo.322.1">) to demonstrate the LangChain</span><a id="_idIndexMarker604" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.323.1"> framework and its capabilities with an example based on </span><strong class="bold"><span class="kobospan" id="kobo.324.1">LangChain Expression Language</span></strong><span class="kobospan" id="kobo.325.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.326.1">LCEL</span></strong><span class="kobospan" id="kobo.327.1">). </span><span class="kobospan" id="kobo.327.2">Let us start with a simple recipe based on the LangChain framework and extend it in</span><a id="_idIndexMarker605" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.328.1"> the recipes that follow from there on. </span><span class="kobospan" id="kobo.328.2">The first part of this recipe is very similar to the previous one. </span><span class="kobospan" id="kobo.328.3">The only difference is the use of the </span><span><span class="kobospan" id="kobo.329.1">LangChain framework.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.330.1">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.331.1">10.3_langchain_prompt_with_llm.ipynb</span></strong><span class="kobospan" id="kobo.332.1"> notebook from the code site if you want to work from an existing notebook. </span><span class="kobospan" id="kobo.332.2">Please note that due to the compute requirements for this recipe, it might take a few minutes for it to complete the text generation. </span><span class="kobospan" id="kobo.332.3">If the required compute capacity is unavailable, we recommend that you refer to the </span><em class="italic"><span class="kobospan" id="kobo.333.1">Using OpenAI models instead of local ones</span></em><span class="kobospan" id="kobo.334.1"> section at the end of this chapter and use</span><a id="_idIndexMarker606" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.335.1"> the method described there to use an OpenAI model for </span><span><span class="kobospan" id="kobo.336.1">this recipe.</span></span></p>
			<h3 class="calibre8"><span class="kobospan" id="kobo.337.1">How to do it…</span></h3>
			<p class="calibre3"><span class="kobospan" id="kobo.338.1">The recipe does the </span><span><span class="kobospan" id="kobo.339.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.340.1">It initializes an LLM model to be loaded </span><span><span class="kobospan" id="kobo.341.1">into memory.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.342.1">It initializes a prompt to instruct the LLM perform a task. </span><span class="kobospan" id="kobo.342.2">This task is that of answering </span><span><span class="kobospan" id="kobo.343.1">a question.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.344.1">It sends the prompt to the LLM and asks it to generate an answer. </span><span class="kobospan" id="kobo.344.2">This is all done via the </span><span><span class="kobospan" id="kobo.345.1">LangChain framework.</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.346.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.347.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.348.1">Start with doing the </span><span><span class="kobospan" id="kobo.349.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.350.1">
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface.llms import HuggingFacePipeline
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, 
    pipeline)
import torch</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.351.1">In this step, we initialize the model name and the quantization configuration. </span><span class="kobospan" id="kobo.351.2">We have expanded upon quantization in the </span><em class="italic"><span class="kobospan" id="kobo.352.1">Running an LLM to follow instructions</span></em><span class="kobospan" id="kobo.353.1"> recipe; please </span><a id="_idIndexMarker607" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.354.1">check there for more details. </span><span class="kobospan" id="kobo.354.2">We wi</span><a id="_idTextAnchor294" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.355.1">ll use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.356.1">meta-llama/Meta-Llama-3.1-8B-Instruct</span></strong><span class="kobospan" id="kobo.357.1"> model that was released by Meta in July of 2024. </span><span class="kobospan" id="kobo.357.2">It has outperformed models of bigger size on many </span><span><span class="kobospan" id="kobo.358.1">NLP tasks:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.359.1">
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type= "nf4")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.360.1">In this step, we initialize the model. </span><span class="kobospan" id="kobo.360.2">We have elaborated on the methodology for loading the model and the tokenizer using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.361.1">Transformers</span></strong><span class="kobospan" id="kobo.362.1"> library in detail in </span><a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.363.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.364.1">. </span><span class="kobospan" id="kobo.364.2">To avoid repeating the same information here, please refer to that chapter for </span><span><span class="kobospan" id="kobo.365.1">more details:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.366.1">
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained(model_name)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.367.1">In this step, we initialize the pipeline. </span><span class="kobospan" id="kobo.367.2">We have elaborated on the pipeline construct from the transformers library in detail in </span><a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.368.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.369.1">. </span><span class="kobospan" id="kobo.369.2">To avoid repeating the same information here, please refer to that chapter for </span><span><span class="kobospan" id="kobo.370.1">more details:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.371.1">
pipe = pipeline("text-generation",
    model=model, tokenizer=tokenizer, max_new_tokens=500,
    pad_token_id = tokenizer.eos_token_id)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.372.1">In this step, we initialize a </span><a id="_idIndexMarker608" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.373.1">chat prompt template, which is of the defined </span><strong class="source-inline1"><span class="kobospan" id="kobo.374.1">ChatPromptTemplate</span></strong><span class="kobospan" id="kobo.375.1"> type. </span><span class="kobospan" id="kobo.375.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.376.1">from_messages</span></strong><span class="kobospan" id="kobo.377.1"> method takes a series of (</span><strong class="source-inline1"><span class="kobospan" id="kobo.378.1">message type</span></strong><span class="kobospan" id="kobo.379.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.380.1">template</span></strong><span class="kobospan" id="kobo.381.1">) tuples. </span><span class="kobospan" id="kobo.381.2">The second tuple in the messages array has the </span><strong class="source-inline1"><span class="kobospan" id="kobo.382.1">{input}</span></strong><span class="kobospan" id="kobo.383.1"> template. </span><span class="kobospan" id="kobo.383.2">This signifies that this value will be </span><span><span class="kobospan" id="kobo.384.1">passed later:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.385.1">
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a great mentor."),
    ("user", "{input}")])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.386.1">In this step, we initialize an output parser that is of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.387.1">StrOutputParser</span></strong><span class="kobospan" id="kobo.388.1"> type. </span><span class="kobospan" id="kobo.388.2">It converts a chat message returned by an LLM instance to </span><span><span class="kobospan" id="kobo.389.1">a string:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.390.1">
output_parser = StrOutputParser()
hf = HuggingFacePipeline(pipeline=pipe)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.391.1">We initialize an instance of a chain next. </span><span class="kobospan" id="kobo.391.2">The chain pipes the output of one component to the next. </span><span class="kobospan" id="kobo.391.3">In this instance, the prompt is sent to the LLM and it operates on the prompt instance. </span><span class="kobospan" id="kobo.391.4">The output of this operation is a chat message. </span><span class="kobospan" id="kobo.391.5">The chat message is then sent to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.392.1">output_parser</span></strong><span class="kobospan" id="kobo.393.1">, which converts it into a string. </span><span class="kobospan" id="kobo.393.2">In this step, we only set up the various components of </span><span><span class="kobospan" id="kobo.394.1">the chain:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.395.1">
chain = prompt | llm | output_parser</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.396.1">In this step, we invoke the chain and print the results. </span><span class="kobospan" id="kobo.396.2">We pass the input argument in a dictionary. </span><span class="kobospan" id="kobo.396.3">We set up the prompt template as a message that had the </span><strong class="source-inline1"><span class="kobospan" id="kobo.397.1">{input}</span></strong><span class="kobospan" id="kobo.398.1"> placeholder defined there. </span><span class="kobospan" id="kobo.398.2">As part of the chain invocation, the input argument is passed through to the template. </span><span class="kobospan" id="kobo.398.3">The chain invokes the command. </span><span class="kobospan" id="kobo.398.4">The chain is instructing the LLM to generate the answer to the question it asked via the prompt that we set up previously. </span><span class="kobospan" id="kobo.398.5">As we can see from the output, the advice presented in this example is good. </span><span class="kobospan" id="kobo.398.6">We have clipped the output for brevity and </span><a id="_idIndexMarker609" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.399.1">you might see a </span><span><span class="kobospan" id="kobo.400.1">longer output:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.401.1">
result = chain.invoke(
    {"input": "how can I improve my software engineering skills?"})
print(result)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.402.1">
System: You are a great mentor.
</span><span class="kobospan1" id="kobo.402.2">Human: how can I improve my software engineering skills?
</span><span class="kobospan1" id="kobo.402.3">System: Let's break it down. </span><span class="kobospan1" id="kobo.402.4">Here are some suggestions:
1. </span><span class="kobospan1" id="kobo.402.5">**Practice coding**: Regularly practice coding in your favorite programming language. </span><span class="kobospan1" id="kobo.402.6">Try solving problems on platforms like LeetCode, HackerRank, or CodeWars.
</span><span class="kobospan1" id="kobo.402.7">2. </span><span class="kobospan1" id="kobo.402.8">**Learn by doing**: Work on real-world projects, either individually or in teams. </span><span class="kobospan1" id="kobo.402.9">This will help you apply theoretical concepts to practical problems.
</span><span class="kobospan1" id="kobo.402.10">3. </span><span class="kobospan1" id="kobo.402.11">**Read books and articles**: Stay updated with the latest trends and technologies by reading books and articles on software engineering, design patterns, and best practices.
</span><span class="kobospan1" id="kobo.402.12">4. </span><span class="kobospan1" id="kobo.402.13">**Participate in coding communities**: Join online communities like GitHub, Stack Overflow, or Reddit's r/learnprogramming and r/webdev. </span><span class="kobospan1" id="kobo.402.14">These platforms offer valuable resources, feedback, and connections with other developers.
</span><span class="kobospan1" id="kobo.402.15">5. </span><span class="kobospan1" id="kobo.402.16">**Take online courses**: Websites like Coursera, Udemy, and edX offer courses on software engineering, computer science, and related topics. </span><span class="kobospan1" id="kobo.402.17">Take advantage of these resources to fill knowledge gaps.
</span><span class="kobospan1" id="kobo.402.18">6. </span><span class="kobospan1" id="kobo.402.19">**Network with professionals**: Attend conferences, meetups, or join professional organizations like the IEEE Computer Society or the Association for Computing Machinery (ACM). </span><span class="kobospan1" id="kobo.402.20">These events provide opportunities to learn from experienced developers and make connections.
</span><span class="kobospan1" id="kobo.402.21">7. </span><span class="kobospan1" id="kobo.402.22">**Learn from failures**: Don't be afraid to experiment and try new approaches. </span><span class="kobospan1" id="kobo.402.23">Analyze your mistakes, and use them as opportunities to learn and improve.
</span><span class="kobospan1" id="kobo.402.24">8. </span><span class="kobospan1" id="kobo.402.25">**Stay curious**: Continuously seek out new knowledge and skills. </span><span class="kobospan1" id="kobo.402.26">Explore emerging technologies, and stay updated with the latest industry trends.
</span><span class="kobospan1" id="kobo.402.27">9. </span><span class="kobospan1" id="kobo.402.28">**Collaborate with others**: Work with colleagues, mentors, or peers on projects. </span><span class="kobospan1" id="kobo.402.29">This will help you learn from others, gain new perspectives, and develop teamwork and communication skills.
</span><span class="kobospan1" id="kobo.402.30">10. </span><span class="kobospan1" id="kobo.402.31">**Set goals and track progress**: Establish specific, measurable goals for your software engineering skills. </span><span class="kobospan1" id="kobo.402.32">Regularly assess your progress, and adjust your strategy as needed.</span></pre>			<ol class="calibre13">
				<li value="9" class="calibre14"><span class="kobospan" id="kobo.403.1">In this step, we</span><a id="_idIndexMarker610" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.404.1"> change the prompt a bit and make it answer a simple question about the 2024 </span><span><span class="kobospan" id="kobo.405.1">Paris Olympics:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.406.1">
template = """Answer the question.Keep your answer to less than 30 words.
</span><span class="kobospan1" id="kobo.406.2">    Question: {input}
    """
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | hf | output_parser
result = chain.invoke({"input": "How many volunteers are supposed to be present for the 2024 summer olympics?"})
print(result)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.407.1">The following output is generated for the question. </span><span class="kobospan" id="kobo.407.2">We can see that the answer to the question of the number of volunteers is inaccurate by comparing the answer to the Wikipedia source. </span><span class="kobospan" id="kobo.407.3">We have omitted a large part of the text that was returned in the result. </span><span class="kobospan" id="kobo.407.4">However, to show an example, the Llama 3.1 model generated more text</span><a id="_idIndexMarker611" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.408.1"> than we asked it to and started answering more questions that it was never asked. </span><span class="kobospan" id="kobo.408.2">In the next recipe, we will provide a web page source to an LLM and compare the returned results with this one for the </span><span><span class="kobospan" id="kobo.409.1">same question:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.410.1">
Human: Answer the question.Keep your answer to less than 30 words.
</span><span class="kobospan1" id="kobo.410.2">    Question: How many volunteers are supposed to be present for the 2024 summer olympics?
</span><span class="kobospan1" id="kobo.410.3">    Answer: The exact number of volunteers for the 2024 summer olympics is not publicly disclosed. </span><span class="kobospan1" id="kobo.410.4">However, it is estimated to be around 20,000 to 30,000.
</span><span class="kobospan1" id="kobo.410.5">    Question: What is the primary role of a volunteer at the 2024 summer olympics?
</span><span class="kobospan1" id="kobo.410.6">    Answer: The primary role of a volunteer at the 2024 summer olympics is to assist with various tasks such as event management, accredi</span><a id="_idTextAnchor295" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.411.1">tation, and hospitality.</span></pre>			<h2 id="_idParaDest-270" class="calibre5"><a id="_idTextAnchor296" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.412.1">Augmenting the LLM with external content</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.413.1">In this recipe, we will expand upon the </span><a id="_idIndexMarker612" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.414.1">previous example and build a chain that passes external content to the LLM and helps it answer questions based on that augmented content. </span><span class="kobospan" id="kobo.414.2">The technique learned as part of this recipe will help us understand a simple framework for how to extract content from a source and store that in a medium that is conducive to fast semantic searches based on context. </span><span class="kobospan" id="kobo.414.3">Once we learn how to store the content in a searchable format, we can use that store to extract answers to questions that are in open form. </span><span class="kobospan" id="kobo.414.4">This approach can be scaled for production as well using the right tools and approaches. </span><span class="kobospan" id="kobo.414.5">Our goal here is to demonstrate the basic framework to extract an answer to a question, given a </span><span><span class="kobospan" id="kobo.415.1">content source.</span></span></p>
			<h3 class="calibre8"><span class="kobospan" id="kobo.416.1">Getting ready</span></h3>
			<p class="calibre3"><span class="kobospan" id="kobo.417.1">We will use a model from OpenAI in this recipe. </span><span class="kobospan" id="kobo.417.2">Please refer to </span><em class="italic"><span class="kobospan" id="kobo.418.1">Model access</span></em><span class="kobospan" id="kobo.419.1"> under the </span><em class="italic"><span class="kobospan" id="kobo.420.1">Technical requirements</span></em><span class="kobospan" id="kobo.421.1"> section to complete the step to access the OpenAI model. </span><span class="kobospan" id="kobo.421.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.422.1">10.4_rag_with_llm.ipynb</span></strong><span class="kobospan" id="kobo.423.1"> notebook from the code site if you want to work off an </span><span><span class="kobospan" id="kobo.424.1">existing notebook.</span></span></p>
			<h3 class="calibre8"><span class="kobospan" id="kobo.425.1">How to do it…</span></h3>
			<p class="calibre3"><span class="kobospan" id="kobo.426.1">The recipe does the </span><span><span class="kobospan" id="kobo.427.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.428.1">It initializes the </span><span><span class="kobospan" id="kobo.429.1">ChatGPT LLM</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.430.1">It scrapes content from a webpage and breaks it </span><span><span class="kobospan" id="kobo.431.1">into chunks.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.432.1">The text in the document chunks is vectorized and stored in a </span><span><span class="kobospan" id="kobo.433.1">vector store</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.434.1">A chain is created that wires the LLM, the vector store, and a prompt with a question to answer questions based on the content present on the </span><span><span class="kobospan" id="kobo.435.1">web page</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.436.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.437.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.438.1">Do the </span><span><span class="kobospan" id="kobo.439.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.440.1">
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import (
    RunnableParallel, RunnablePassthrough)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline)
import bs4
import getpass
import os</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.441.1">In this step, we </span><a id="_idIndexMarker613" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.442.1">initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.443.1">gpt-4o-mini</span></strong><span class="kobospan" id="kobo.444.1"> model from OpenAI using the </span><span><span class="kobospan" id="kobo.445.1">ChatOpenAI initializer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.446.1">
os.environ["OPENAI_API_KEY"] = getpass.getpass()
llm = ChatOpenAI(model="gpt-4o-mini")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.447.1">In this step, we load the Wikipedia entry on the 2024 Summer Olympics. </span><span class="kobospan" id="kobo.447.2">We initialize a </span><strong class="source-inline1"><span class="kobospan" id="kobo.448.1">WebBaseLoader</span></strong><span class="kobospan" id="kobo.449.1"> object and pass it the Wikipedia URL for the 2024 Summer Olympics. </span><span class="kobospan" id="kobo.449.2">It extracts the HTML content and the main content on each HTML page that is parsed. </span><span class="kobospan" id="kobo.449.3">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.450.1">load</span></strong><span class="kobospan" id="kobo.451.1"> method on the loader instance triggers the extraction of the </span><a id="_idIndexMarker614" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.452.1">content from </span><span><span class="kobospan" id="kobo.453.1">the URLs:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.454.1">
loader = WebBaseLoader(
    ["https://en.wikipedia.org/wiki/2024_Summer_Olympics"])
docs = loader.load()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.455.1">In this step, we initialize the text splitter instance and call the </span><strong class="source-inline1"><span class="kobospan" id="kobo.456.1">split_documents</span></strong><span class="kobospan" id="kobo.457.1"> method on it. </span><span class="kobospan" id="kobo.457.2">This splitting of the document is a needed step as an LLM can only operate on a context of a limited length. </span><span class="kobospan" id="kobo.457.3">For some large documents, the length of the document exceeds the maximum context length supported by the LLM. </span><span class="kobospan" id="kobo.457.4">Breaking a document into chunks and using those to match the query text allows us to retrieve more relevant parts from the document. </span><span class="kobospan" id="kobo.457.5">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.458.1">RecursiveCharacterTextSplitter</span></strong><span class="kobospan" id="kobo.459.1"> splits the document based on newline, spaces, and </span><span><span class="kobospan" id="kobo.460.1">double-newline characters:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.461.1">
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, chunk_overlap=50)
all_splits = text_splitter.split_documents(documents)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.462.1">In this step, we initialize a vector store. </span><span class="kobospan" id="kobo.462.2">We initialize the vector store with the document chunks and the embedding provider. </span><span class="kobospan" id="kobo.462.3">The vector store creates embeddings of the document chunks and stores them along with the document metadata. </span><span class="kobospan" id="kobo.462.4">For production-grade applications, we recommend visiting the following </span><span><span class="kobospan" id="kobo.463.1">URL: </span></span><a href="https://python.langchain.com/docs/integrations/vectorstores/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.464.1">https://python.langchain.com/docs/integrations/vectorstores/</span></span></a></p><p class="calibre3"><span class="kobospan" id="kobo.465.1">There, you can select a vector store based on your requirements. </span><span class="kobospan" id="kobo.465.2">The LangChain framework is versatile and works with a host of prominent </span><span><span class="kobospan" id="kobo.466.1">vector stores.</span></span></p><p class="calibre3"><span class="kobospan" id="kobo.467.1">Next, we initialize a retriever by making a call to the </span><strong class="source-inline"><span class="kobospan" id="kobo.468.1">as_retriever</span></strong><span class="kobospan" id="kobo.469.1"> method of the vector-store instance. </span><span class="kobospan" id="kobo.469.2">The retriever returned by the method is used to retrieve the content from the vector store. </span><span class="kobospan" id="kobo.469.3">The </span><strong class="source-inline"><span class="kobospan" id="kobo.470.1">as_retriever</span></strong><span class="kobospan" id="kobo.471.1"> method is passed a </span><strong class="source-inline"><span class="kobospan" id="kobo.472.1">search_type</span></strong><span class="kobospan" id="kobo.473.1"> argument with the </span><strong class="source-inline"><span class="kobospan" id="kobo.474.1">similarity</span></strong><span class="kobospan" id="kobo.475.1"> value, which is also the default option. </span><span class="kobospan" id="kobo.475.2">This means that the vector store will be searched against the question text based on similarity. </span><span class="kobospan" id="kobo.475.3">The other options supported are </span><strong class="source-inline"><span class="kobospan" id="kobo.476.1">mmr</span></strong><span class="kobospan" id="kobo.477.1">, which penalizes search results of the same type and returns diverse results, and </span><strong class="source-inline"><span class="kobospan" id="kobo.478.1">similarity_score_threshold</span></strong><span class="kobospan" id="kobo.479.1">, which operates in the same way as the </span><strong class="source-inline"><span class="kobospan" id="kobo.480.1">similarity</span></strong><span class="kobospan" id="kobo.481.1"> search type, but can filter out the results based on a threshold. </span><span class="kobospan" id="kobo.481.2">These options also support an accompanying dictionary argument that can be used to tweak the search parameters. </span><span class="kobospan" id="kobo.481.3">We recommend that the readers refer to the LangChain </span><a id="_idIndexMarker615" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.482.1">documentation and tweak the parameters based on their requirements and </span><span><span class="kobospan" id="kobo.483.1">empirical findings</span></span></p><p class="calibre3"><span class="kobospan" id="kobo.484.1">We also define a helper method, </span><strong class="source-inline"><span class="kobospan" id="kobo.485.1">format_docs</span></strong><span class="kobospan" id="kobo.486.1">, that appends the content of all the repository docs separated by two </span><span><span class="kobospan" id="kobo.487.1">newline characters:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.488.1">vectorstore = FAISS.from_documents(
    all_splits,
    HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-mpnet-base-v2")
)
retriever = vectorstore.as_retriever(search_type="similarity")
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.489.1">In this step, we define a chat template and create an instance of </span><strong class="source-inline1"><span class="kobospan" id="kobo.490.1">ChatPromptTemplate</span></strong><span class="kobospan" id="kobo.491.1"> from it. </span><span class="kobospan" id="kobo.491.2">This prompt template instructs the LLM to answer the question for the given context. </span><span class="kobospan" id="kobo.491.3">This context is provided by the augmentation step</span><a id="_idIndexMarker616" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.492.1"> via the vector store </span><span><span class="kobospan" id="kobo.493.1">search results:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.494.1">
template = """Answer the question based only on the following context:
    {context}
    Question: {question}
    """
prompt = ChatPromptTemplate.from_template(template)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.495.1">In this step, we set up the chain. </span><span class="kobospan" id="kobo.495.2">The chain sequence sets up the retriever as a context provider. </span><span class="kobospan" id="kobo.495.3">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.496.1">question</span></strong><span class="kobospan" id="kobo.497.1"> argument is assumed to be passed later by the chain. </span><span class="kobospan" id="kobo.497.2">The next component is the prompt, which supplies the context value. </span><span class="kobospan" id="kobo.497.3">The populated prompt is sent to the LLM. </span><span class="kobospan" id="kobo.497.4">The LLM pipes or forwards the results to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.498.1">StrOutputParser()</span></strong><span class="kobospan" id="kobo.499.1"> string, which is designed to return the string contained in the output of the LLM. </span><span class="kobospan" id="kobo.499.2">There is no execution in this step. </span><span class="kobospan" id="kobo.499.3">We are only setting up </span><span><span class="kobospan" id="kobo.500.1">the chain:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.501.1">
rag_chain = (
    {"context": retriever 
    | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.502.1">In this step, we invoke the chain and print the results. </span><span class="kobospan" id="kobo.502.2">For each invocation, the question text is matched by similarity against the vector store. </span><span class="kobospan" id="kobo.502.3">Then, the relevant document chunks are returned, followed by the LLM using these document chunks as context and using that context to answer the respective questions. </span><span class="kobospan" id="kobo.502.4">As we can</span><a id="_idIndexMarker617" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.503.1"> see in this case, the answers returned by the chain </span><span><span class="kobospan" id="kobo.504.1">are accurate:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.505.1">
response = rag_chain.invoke("Where are the 2024 summer olympics being held?")
print(response)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.506.1">
The 2024 Summer Olympics are being held in Paris, France, with events also taking place in 16 additional cities across Metropolitan France and one subsite in Tahiti, French Polynesia.</span></pre>			<ol class="calibre13">
				<li value="8" class="calibre14"><span class="kobospan" id="kobo.507.1">Invoke the chain with another question and print the results. </span><span class="kobospan" id="kobo.507.2">As we can see in this case, the answers returned by the chain are accurate, though I am skeptical about whether </span><strong class="source-inline1"><span class="kobospan" id="kobo.508.1">Breaking</span></strong><span class="kobospan" id="kobo.509.1"> is indeed a sport, as returned in </span><span><span class="kobospan" id="kobo.510.1">the results:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.511.1">
result = rag_chain.invoke("What are the new sports that are being added for the 2024 summer olympics?")
print(result)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.512.1">
The new sport being added for the 2024 Summer Olympics is breaking, which will make its Olympic debut as an optional sport.</span></pre>			<ol class="calibre13">
				<li value="9" class="calibre14"><span class="kobospan" id="kobo.513.1">Invoke the chain with another question and print the results. </span><span class="kobospan" id="kobo.513.2">As we can see in this case, the answers returned by the chain </span><span><span class="kobospan" id="kobo.514.1">are accurate:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.515.1">
result = rag_chain.invoke("How many volunteers are supposed to be present for the 2024 summer olympics?")
print(result)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.516.1">
There are expected to be 45,000 volunteers recruited for the 2024 Summer Olympics.</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.517.1">If we compare these results with the last step of the previous recipe, we can see that the LLM returned accurate information as per the content on the Wikipedia page. </span><span class="kobospan" id="kobo.517.2">This is an effective use case for RAG where the LLM uses the context to answer the question, instead of making up information as</span><a id="_idTextAnchor297" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.518.1"> it did in the </span><span><span class="kobospan" id="kobo.519.1">previous recipe.</span></span></p>
			<h1 id="_idParaDest-271" class="calibre7"><a id="_idTextAnchor298" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.520.1">Creating a chatbot using an LLM</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.521.1">In this recipe, we will create a chatbot using the LangChain framework. </span><span class="kobospan" id="kobo.521.2">In the previous recipe, we learned how to</span><a id="_idIndexMarker618" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.522.1"> ask questions to an LLM based on a piece of content. </span><span class="kobospan" id="kobo.522.2">Though the LLM was able to answer questions accurately, the interaction with the LLM was completely stateless. </span><span class="kobospan" id="kobo.522.3">The LLM looks at each </span><a id="_idIndexMarker619" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.523.1">question in isolation and ignores any previous interactions or questions that it was asked. </span><span class="kobospan" id="kobo.523.2">In this recipe, we will use an LLM to create a chat interaction, wherein the LLM will be aware of the previous conversations and use the context from them to answer subsequent questions. </span><span class="kobospan" id="kobo.523.3">Applications of such a framework would be to converse with document sources and get to the right answer by asking a series of questions. </span><span class="kobospan" id="kobo.523.4">These document sources could be of a wide variety of types, from internal company knowledge bases to customer contact center troubleshooting guides. </span><span class="kobospan" id="kobo.523.5">Our goal here is to present a basic step-by-step framework to demonstrate the essential components working together to achieve the </span><span><span class="kobospan" id="kobo.524.1">end goal.</span></span></p>
			<h2 id="_idParaDest-272" class="calibre5"><a id="_idTextAnchor299" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.525.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.526.1">We will use a model from OpenAI in this recipe. </span><span class="kobospan" id="kobo.526.2">Please refer to </span><em class="italic"><span class="kobospan" id="kobo.527.1">Model access</span></em><span class="kobospan" id="kobo.528.1"> under the </span><em class="italic"><span class="kobospan" id="kobo.529.1">Technical requirements</span></em><span class="kobospan" id="kobo.530.1"> section to complete the step to access the OpenAI model. </span><span class="kobospan" id="kobo.530.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.531.1">10.5_chatbot_with_llm.ipynb</span></strong><span class="kobospan" id="kobo.532.1"> notebook from the code site if you want to work from an </span><span><span class="kobospan" id="kobo.533.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-273" class="calibre5"><a id="_idTextAnchor300" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.534.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.535.1">The recipe does the </span><span><span class="kobospan" id="kobo.536.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.537.1">It initializes the ChatGPT LLM and an embedding provider. </span><span class="kobospan" id="kobo.537.2">The embedding provider is used to vectorize the document content so that a vector-based similarity search can </span><span><span class="kobospan" id="kobo.538.1">be performed.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.539.1">It scrapes content from a webpage and breaks it </span><span><span class="kobospan" id="kobo.540.1">into chunks.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.541.1">The text in the document chunks is vectorized and stored in a </span><span><span class="kobospan" id="kobo.542.1">vector store.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.543.1">A conversation is started with the LLM via some curated prompts and a follow-up question is asked </span><a id="_idIndexMarker620" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.544.1">based on the answer provided by the LLM in the </span><a id="_idIndexMarker621" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.545.1">previous context.</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.546.1">Let’s </span><span><span class="kobospan" id="kobo.547.1">get started:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.548.1">Do the </span><span><span class="kobospan" id="kobo.549.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.550.1">
import bs4
import getpass
import os
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate, MessagesPlaceholder)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.551.1"> In this step, we initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.552.1">gpt-4o-mini</span></strong><span class="kobospan" id="kobo.553.1"> model from OpenAI using the </span><span><span class="kobospan" id="kobo.554.1">ChatOpenAI initializer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.555.1">
os.environ["OPENAI_API_KEY"] = getpass.getpass()
llm = ChatOpenAI(model="gpt-4o-mini")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.556.1">In this step, we load the</span><a id="_idIndexMarker622" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.557.1"> embedding provider. </span><span class="kobospan" id="kobo.557.2">The content from the webpage is vectorized via the</span><a id="_idIndexMarker623" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.558.1"> embedding provider. </span><span class="kobospan" id="kobo.558.2">We use the pre-trained </span><strong class="source-inline1"><span class="kobospan" id="kobo.559.1">sentence-transformers/all-mpnet-base-v2 </span></strong><span class="kobospan" id="kobo.560.1">model using the call to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.561.1">HuggingFaceEmbeddings</span></strong><span class="kobospan" id="kobo.562.1"> constructor call. </span><span class="kobospan" id="kobo.562.2">This model is a good one for encoding short sentences or a paragraph. </span><span class="kobospan" id="kobo.562.3">The encoded vector representation captures the semantic context well. </span><span class="kobospan" id="kobo.562.4">Please refer to the model card at </span><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.563.1">https://huggingface.co/sentence-transformers/all-mpnet-base-v2</span></a><span class="kobospan" id="kobo.564.1"> for </span><span><span class="kobospan" id="kobo.565.1">more details:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.566.1">
embeddings_provider = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.567.1">In this step, we will load a web page that has content based on which we want to ask questions. </span><span class="kobospan" id="kobo.567.2">You are free to choose any webpage of your choice. </span><span class="kobospan" id="kobo.567.3">We initialize a </span><strong class="source-inline1"><span class="kobospan" id="kobo.568.1">WebBaseLoader</span></strong><span class="kobospan" id="kobo.569.1"> object and pass it the URL. </span><span class="kobospan" id="kobo.569.2">We call the </span><strong class="source-inline1"><span class="kobospan" id="kobo.570.1">load</span></strong><span class="kobospan" id="kobo.571.1"> method for the loader instance. </span><span class="kobospan" id="kobo.571.2">Feel free to change the link to any other webpage that you might want to use as the chat </span><span><span class="kobospan" id="kobo.572.1">knowledge base:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.573.1">
loader = WebBaseLoader(
    ["https://lilianweng.github.io/posts/2023-06-23-agent/"])
docs = loader.load()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.574.1">Initialize the text splitter instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.575.1">RecursiveCharacterTextSplitter</span></strong><span class="kobospan" id="kobo.576.1"> type. </span><span class="kobospan" id="kobo.576.2">Use the text splitter instance to split the documents </span><span><span class="kobospan" id="kobo.577.1">into chunks:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.578.1">
text_splitter = RecursiveCharacterTextSplitter()
document_chunks = text_splitter.split_documents(docs)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.579.1">We initialize the vector or embedding store from the document chunks that we created in the previous step. </span><span class="kobospan" id="kobo.579.2">We pass it the document chunks and the embedding provider. </span><span class="kobospan" id="kobo.579.3">We also initialize the vector store retriever and the output parser. </span><span class="kobospan" id="kobo.579.4">The retriever will provide</span><a id="_idIndexMarker624" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.580.1"> the augmented content to the chain via the vector store. </span><span class="kobospan" id="kobo.580.2">We </span><a id="_idIndexMarker625" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.581.1">provided more details in the </span><em class="italic"><span class="kobospan" id="kobo.582.1">Augmenting the LLM with external content</span></em><span class="kobospan" id="kobo.583.1"> recipe from this chapter. </span><span class="kobospan" id="kobo.583.2">To avoid repetition, we recommend referring to </span><span><span class="kobospan" id="kobo.584.1">that recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.585.1">
vectorstore = FAISS.from_documents(
    all_splits,
    HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-mpnet-base-v2")
)
retriever = vectorstore.as_retriever(search_type="similarity")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.586.1">In this step, we initialize a contextualized system prompt. </span><span class="kobospan" id="kobo.586.2">A system prompt defines the persona and the instruction that is to be followed by the LLM. </span><span class="kobospan" id="kobo.586.3">In this case, we use the system prompt to contain the instruction that the LLM has to use the chat history to formulate a standalone question. </span><span class="kobospan" id="kobo.586.4">We initialize the prompt instance with the system prompt definition and set it up with the expectation that it will have access to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.587.1">chat_history</span></strong><span class="kobospan" id="kobo.588.1"> variable that will be passed to it at run time. </span><span class="kobospan" id="kobo.588.2">We also set it up with the question template that will also be passed at </span><span><span class="kobospan" id="kobo.589.1">run time:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.590.1">
contextualize_q_system_prompt = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history. </span><span class="kobospan1" id="kobo.590.2">Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is."""
</span><span class="kobospan1" id="kobo.590.3">contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.591.1">In this step, we initialize the contextualized chain. </span><span class="kobospan" id="kobo.591.2">As you can see in the previous code snippet, we are setting up the prompt with the context and the chat history. </span><span class="kobospan" id="kobo.591.3">This chain </span><a id="_idIndexMarker626" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.592.1">uses the chat history and a given follow-up question from the user and sets up the context for it as part of the prompt. </span><span class="kobospan" id="kobo.592.2">The populated prompt template is sent to the LLM. </span><span class="kobospan" id="kobo.592.3">The idea here is that the subsequent question will not provide any context and ask the question based on the chat history generated </span><span><span class="kobospan" id="kobo.593.1">so far:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.594.1">
contextualize_q_chain = contextualize_q_prompt | llm 
    | output_parser</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.595.1">In this step, we initialize a system prompt, much like in the previous recipe, based on RAG. </span><span class="kobospan" id="kobo.595.2">This prompt just</span><a id="_idIndexMarker627" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.596.1"> sets up a prompt template. </span><span class="kobospan" id="kobo.596.2">However, we pass this prompt a contextualized question as the chat history grows. </span><span class="kobospan" id="kobo.596.3">This prompt always answers a contextualized question, barring the </span><span><span class="kobospan" id="kobo.597.1">first one:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.598.1">
qa_system_prompt = """You are an assistant for question-answering tasks. </span><span class="kobospan1" id="kobo.598.2">\
Use the following pieces of retrieved context to answer the question. </span><span class="kobospan1" id="kobo.598.3">\
If you don't know the answer, just say that you don't know. </span><span class="kobospan1" id="kobo.598.4">\
Use three sentences maximum and keep the answer concise.\
{context}"""
qa_prompt = ChatPromptTemplate.from_messages(
    [("system", qa_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.599.1">We initialize two helper methods. </span><span class="kobospan" id="kobo.599.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.600.1">contextualized_question</span></strong><span class="kobospan" id="kobo.601.1"> method returns the </span><a id="_idIndexMarker628" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.602.1">contextualized chain if a chat history exists; otherwise, it returns the input </span><a id="_idIndexMarker629" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.603.1">question. </span><span class="kobospan" id="kobo.603.2">This is the typical scenario for the first question. </span><span class="kobospan" id="kobo.603.3">Once the </span><strong class="source-inline1"><span class="kobospan" id="kobo.604.1">chat_history</span></strong><span class="kobospan" id="kobo.605.1"> is present, it returns the contextualized chain. </span><span class="kobospan" id="kobo.605.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.606.1">format_docs</span></strong><span class="kobospan" id="kobo.607.1"> method concatenates the page content for each document separated by two </span><span><span class="kobospan" id="kobo.608.1">newline characters:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.609.1">
def contextualized_question(input: dict):
    if input.get("chat_history"):
        return contextualize_q_chain
    else:
        return input["question"]
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.610.1">In this step, we set up a chain. </span><span class="kobospan" id="kobo.610.2">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.611.1">RunnablePassthrough</span></strong><span class="kobospan" id="kobo.612.1"> class to set up the context. </span><span class="kobospan" id="kobo.612.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.613.1">RunnablePassthrough</span></strong><span class="kobospan" id="kobo.614.1"> class allows us to pass the input or add additional data to the input via dictionary values. </span><span class="kobospan" id="kobo.614.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.615.1">assign</span></strong><span class="kobospan" id="kobo.616.1"> method will take a key and will assign the value to this key. </span><span class="kobospan" id="kobo.616.2">In this case, the key is </span><strong class="source-inline1"><span class="kobospan" id="kobo.617.1">context</span></strong><span class="kobospan" id="kobo.618.1"> and the assigned value for it is the result of the chained evaluation of the contextualized question, the retriever, and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.619.1">format_docs</span></strong><span class="kobospan" id="kobo.620.1">. </span><span class="kobospan" id="kobo.620.2">Putting that into the context of the entire recipe, for the first question, the context will use the set of matched records for the question. </span><span class="kobospan" id="kobo.620.3">For the second question, the context will use the contextualized question from the chat history, retrieve a set of matching records, and pass that as the context. </span><span class="kobospan" id="kobo.620.4">The LangChain framework uses a deferred</span><a id="_idIndexMarker630" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.621.1"> execution model here. </span><span class="kobospan" id="kobo.621.2">We set up the chain here with the necessary constructs such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.622.1">context</span></strong><span class="kobospan" id="kobo.623.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.624.1">qa_prompt</span></strong><span class="kobospan" id="kobo.625.1">, and the LLM. </span><span class="kobospan" id="kobo.625.2">This is just setting the </span><a id="_idIndexMarker631" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.626.1">expectation with the chain that all these components will pipe their input to the next component when the chain is invoked. </span><span class="kobospan" id="kobo.626.2">Any placeholder arguments that were set as part of the prompts will be populated and used </span><span><span class="kobospan" id="kobo.627.1">during invocation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.628.1">
rag_chain = (
        RunnablePassthrough.assign(
            context=contextualized_question | retriever | format_docs)
        | qa_prompt
        | llm
)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.629.1">In this step, we initialize a chat history array. </span><span class="kobospan" id="kobo.629.2">We ask a simple question to the chain by invoking it. </span><span class="kobospan" id="kobo.629.3">What happens internally is the question is essentially just the first question since there is no chat history present at this point. </span><span class="kobospan" id="kobo.629.4">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.630.1">rag_chain</span></strong><span class="kobospan" id="kobo.631.1"> just answers the question simply and prints the answer. </span><span class="kobospan" id="kobo.631.2">We also extend the </span><strong class="source-inline1"><span class="kobospan" id="kobo.632.1">chat_history</span></strong><span class="kobospan" id="kobo.633.1"> with the </span><span><span class="kobospan" id="kobo.634.1">returned message:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.635.1">
chat_history = []
question = "What is a large language model?"
</span><span class="kobospan1" id="kobo.635.2">ai_msg = rag_chain.invoke(
    {"question": question, "chat_history": chat_history})
print(ai_msg)
chat_history.extend([HumanMessage(content=question), 
    AIMessage(content=ai_msg)])</span></pre><p class="calibre3"><span class="kobospan" id="kobo.636.1">This results in</span><a id="_idIndexMarker632" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.637.1"> the </span><span><span class="kobospan" id="kobo.638.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.639.1">
A large language model (LLM) is an artificial intelligence system designed to understand and generate human-like text based on the input it receives. </span><span class="kobospan1" id="kobo.639.2">It uses vast amounts of data and complex algorithms to predict the next word in a sequence, enabling it to perform various language-related tasks, such as translation, summarization, and conversation. </span><span class="kobospan1" id="kobo.639.3">LLMs can be powerful problem solvers and are often integrated into applications for natural language processing.</span></pre>			<ol class="calibre13">
				<li value="13" class="calibre14"><span class="kobospan" id="kobo.640.1">In this step, we invoke the chain again with a subsequent question, without providing many contextual cues. </span><span class="kobospan" id="kobo.640.2">We provide the chain with the chat history and print the answer to the </span><a id="_idIndexMarker633" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.641.1">second question. </span><span class="kobospan" id="kobo.641.2">Internally, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.642.1">rag_chain</span></strong><span class="kobospan" id="kobo.643.1"> and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.644.1">contextualize_q_chain</span></strong><span class="kobospan" id="kobo.645.1"> work in tandem to answer this question. </span><span class="kobospan" id="kobo.645.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.646.1">contextualize_q_chain</span></strong><span class="kobospan" id="kobo.647.1"> uses the chat history to add more context to the follow-up question, retrieves matched records, and sends that as context to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.648.1">rag_chain</span></strong><span class="kobospan" id="kobo.649.1">. </span><span class="kobospan" id="kobo.649.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.650.1">rag_chain</span></strong><span class="kobospan" id="kobo.651.1"> used the context and the contextualized question to answer the subsequent question. </span><span class="kobospan" id="kobo.651.2">As we observe from the output, the LLM was able to decipher what </span><strong class="source-inline1"><span class="kobospan" id="kobo.652.1">it</span></strong><span class="kobospan" id="kobo.653.1"> means in </span><span><span class="kobospan" id="kobo.654.1">this</span></span><span><a id="_idIndexMarker634" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.655.1"> context:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.656.1">
second_question = "Can you explain the reasoning behind calling it large?"
</span><span class="kobospan1" id="kobo.656.2">second_answer = rag_chain.invoke({"question": second_question,
    "chat_history": chat_history})
print(second_answer)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.657.1">This results in the </span><a id="_idIndexMarker635" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.658.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.659.1">
The term "large" in large language model refers to both the size of the model itself and the volume of data it is trained on. </span><span class="kobospan1" id="kobo.659.2">These models typically consist of billions of parameters, which are the weights and biases that help the model learn patterns in the data, allowing for a more nuanced understanding of language. </span><span class="kobospan1" id="kobo.659.3">Additionally, the training datasets used are extensive, often comprising vast amounts of text from diverse sources, which contributes to the model's ability to generate coherent and contextually relevant outputs.</span></pre>			<p class="callout-heading"><span class="kobospan" id="kobo.660.1">Note:</span></p>
			<p class="callout"><span class="kobospan" id="kobo.661.1">We provided a basic workflow for how to execute RAG-based flows. </span><span class="kobospan" id="kobo.661.2">We recommend referring to the LangChain documentation and using the necessary components to run solutions in production. </span><span class="kobospan" id="kobo.661.3">Some of these would include evaluating other vector DB stores, using concrete types such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.662.1">BaseChatMessageHistory</span></strong><span class="kobospan" id="kobo.663.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.664.1">RunnableWithMessageHistory</span></strong><span class="kobospan" id="kobo.665.1"> to better manage chat histories. </span><span class="kobospan" id="kobo.665.2">Also, use</span><a id="_idTextAnchor301" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.666.1"> LangServe to expose endpoints to </span><span><span class="kobospan" id="kobo.667.1">serve requests.</span></span></p>
			<h1 id="_idParaDest-274" class="calibre7"><a id="_idTextAnchor302" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.668.1">Generating code using an LLM</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.669.1">In this recipe, we will explore how </span><a id="_idIndexMarker636" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.670.1">an LLM can be used to generate code. </span><span class="kobospan" id="kobo.670.2">We </span><a id="_idIndexMarker637" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.671.1">will use two separate examples to check the breadth of coverage for the generation. </span><span class="kobospan" id="kobo.671.2">We will also compare the output from two LLMs to observe how the generation varies across two different models. </span><span class="kobospan" id="kobo.671.3">Applications </span><a id="_idIndexMarker638" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.672.1">of such methods are already incorporated in popular </span><strong class="bold"><span class="kobospan" id="kobo.673.1">Integrated Development Environments</span></strong><span class="kobospan" id="kobo.674.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.675.1">IDEs</span></strong><span class="kobospan" id="kobo.676.1">). </span><span class="kobospan" id="kobo.676.2">Our goal here is to demonstrate a basic framework for how to use a pre-trained LLM to generate code snipped based on simple </span><span><span class="kobospan" id="kobo.677.1">human-defined requirements.</span></span></p>
			<h2 id="_idParaDest-275" class="calibre5"><a id="_idTextAnchor303" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.678.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.679.1">We will use a model from Hugging Face as well as OpenAI in this recipe. </span><span class="kobospan" id="kobo.679.2">Please refer to </span><em class="italic"><span class="kobospan" id="kobo.680.1">Model access</span></em><span class="kobospan" id="kobo.681.1"> under the </span><em class="italic"><span class="kobospan" id="kobo.682.1">Technical requirements</span></em><span class="kobospan" id="kobo.683.1"> section to complete the step to access the Llama and OpenAI models. </span><span class="kobospan" id="kobo.683.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.684.1">10.6_code_generation_with_llm.ipynb</span></strong><span class="kobospan" id="kobo.685.1"> notebook from the code site if you want to work from an existing notebook. </span><span class="kobospan" id="kobo.685.2">Please note that due to the compute requirements for this recipe, it</span><a id="_idIndexMarker639" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.686.1"> might take a few minutes for it to complete the text generation. </span><span class="kobospan" id="kobo.686.2">If the required compute capacity is unavailable, we recommend referring to the </span><em class="italic"><span class="kobospan" id="kobo.687.1">Using OpenAI models instead of local ones section</span></em><span class="kobospan" id="kobo.688.1"> at the end of this chapter and using the method described there to use an OpenAI model for </span><span><span class="kobospan" id="kobo.689.1">this recipe.</span></span></p>
			<h2 id="_idParaDest-276" class="calibre5"><a id="_idTextAnchor304" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.690.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.691.1">The recipe does the </span><span><span class="kobospan" id="kobo.692.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.693.1">It initializes a prompt template</span><a id="_idIndexMarker640" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.694.1"> that instructs the LLM to generate code for a given </span><span><span class="kobospan" id="kobo.695.1">problem statement</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.696.1">It initializes an LLM model and a tokenizer and wires them together in </span><span><span class="kobospan" id="kobo.697.1">a pipeline</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.698.1">It creates a chain that connects the prompt, LLM and string post-processor to generate a code snippet based on a </span><span><span class="kobospan" id="kobo.699.1">given instruction</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.700.1">We additionally show the result of the same instructions when executed via an </span><span><span class="kobospan" id="kobo.701.1">OpenAI model</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.702.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.703.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.704.1">Do the </span><span><span class="kobospan" id="kobo.705.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.706.1">
import os
import getpass
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_experimental.utilities import PythonREPL
from langchain_huggingface.llms import HuggingFacePipeline
from langchain_openai import ChatOpenAI
from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    BitsAndBytesConfig, pipeline)
import torch</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.707.1">In this step, we define a template. </span><span class="kobospan" id="kobo.707.2">This template defines the instruction or the system prompt that is sent to the model as the task description. </span><span class="kobospan" id="kobo.707.3">In this case, the template defines</span><a id="_idIndexMarker641" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.708.1"> an instruction to generate Python code based on users’ requirements. </span><span class="kobospan" id="kobo.708.2">We use this template to initialize a prompt object. </span><span class="kobospan" id="kobo.708.3">The</span><a id="_idIndexMarker642" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.709.1"> initialized object is of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.710.1">ChatPromptTemplate</span></strong><span class="kobospan" id="kobo.711.1"> type. </span><span class="kobospan" id="kobo.711.2">This object lets us send requirements to the model in an interactive way. </span><span class="kobospan" id="kobo.711.3">We can converse with the model based on our instructions to generate several code snippets without having to load the model each time. </span><span class="kobospan" id="kobo.711.4">Note the </span><strong class="source-inline1"><span class="kobospan" id="kobo.712.1">{input}</span></strong><span class="kobospan" id="kobo.713.1"> placeholder in the prompt. </span><span class="kobospan" id="kobo.713.2">This signifies that the value for this placeholder will be provided later during the chain </span><span><span class="kobospan" id="kobo.714.1">invocation call.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.715.1">
template = """Write some python code to solve the user's problem.
</span><span class="kobospan1" id="kobo.715.2">Return only python code in Markdown format, e.g.:
```python
....
</span><span class="kobospan1" id="kobo.715.3">```"""
prompt = ChatPromptTemplate.from_messages([("system", template), ("human", "{input}")])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.716.1">Set up the parameters for the model. </span><em class="italic"><span class="kobospan" id="kobo.717.1">Steps 3-5</span></em><span class="kobospan" id="kobo.718.1"> have been explained in more detail in the </span><em class="italic"><span class="kobospan" id="kobo.719.1">Executing a simple prompt-to-LLM chain</span></em><span class="kobospan" id="kobo.720.1"> recipe earlier in this chapter. </span><span class="kobospan" id="kobo.720.2">Please</span><a id="_idIndexMarker643" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.721.1"> refer to that recipe for more details. </span><span class="kobospan" id="kobo.721.2">We also initialize a</span><a id="_idIndexMarker644" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.722.1"> configuration for quantization. </span><span class="kobospan" id="kobo.722.2">This has been described in more detail in the </span><em class="italic"><span class="kobospan" id="kobo.723.1">Running an LLM to follow instructions</span></em><span class="kobospan" id="kobo.724.1"> recipe in this chapter. </span><span class="kobospan" id="kobo.724.2">To avoid repetition, we recommend referring to </span><em class="italic"><span class="kobospan" id="kobo.725.1">step 3</span></em><span class="kobospan" id="kobo.726.1"> of </span><span><span class="kobospan" id="kobo.727.1">that recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.728.1">
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type= "nf4")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.729.1">Initialize the model. </span><span class="kobospan" id="kobo.729.2">In this instance, as we are working to generate code, we use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.730.1">Meta-Llama-3.1-8B-Instruct</span></strong><span class="kobospan" id="kobo.731.1"> model. </span><span class="kobospan" id="kobo.731.2">This model also has the ability to generate code. </span><span class="kobospan" id="kobo.731.3">For a model of this size, it has demonstrated very good performance for </span><span><span class="kobospan" id="kobo.732.1">code generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.733.1">
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained(model_name)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.734.1">We initialize the </span><a id="_idIndexMarker645" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.735.1">pipeline with the model and </span><span><span class="kobospan" id="kobo.736.1">the tokenizer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.737.1">
pipe = pipeline("text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=500,
    pad_token_id = tokenizer.eos_token_id,
    eos_token_id=model.config.eos_token_id,
    num_beams=4,
    early_stopping=True,
    repetition_penalty=1.4)
llm = HuggingFacePipeline(pipeline=pipe)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.738.1">We initialize the chain</span><a id="_idIndexMarker646" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.739.1"> with the prompt and </span><span><span class="kobospan" id="kobo.740.1">the model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.741.1">
chain = prompt | llm | StrOutputParser()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.742.1">We invoke the chain and print the result. </span><span class="kobospan" id="kobo.742.2">As we can see from the output, the generated code is reasonably good, with the Node </span><strong class="source-inline1"><span class="kobospan" id="kobo.743.1">class</span></strong><span class="kobospan" id="kobo.744.1"> having a constructor along with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.745.1">inorder_traversal</span></strong><span class="kobospan" id="kobo.746.1"> helper method. </span><span class="kobospan" id="kobo.746.2">It also prints out the instructions to use the class. </span><span class="kobospan" id="kobo.746.3">However, the output is overly verbose and we have omitted the additional text generated in the output shown for this step. </span><span class="kobospan" id="kobo.746.4">The output contains code for preorder traversal too, which we did not instruct the LLM </span><span><span class="kobospan" id="kobo.747.1">to </span></span><span><a id="_idIndexMarker647" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.748.1">generate:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.749.1">
result = chain.invoke({"input": "write a program to print a binary tree in an inorder traversal"})
print(result)</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.750.1">This generates the </span><a id="_idIndexMarker648" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.751.1">following output:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.752.1">
System: Write some python code to solve the user's problem. </span><span class="kobospan1" id="kobo.752.2">Keep the answer as brief as possible.
</span><span class="kobospan1" id="kobo.752.3">Return only python code in Markdown format, e.g.:
```python
....
</span><span class="kobospan1" id="kobo.752.4">```
Human: write a program to print a binary tree in an inorder traversal
```python
class Node:
    def __init__(self, value):
        self.value = value
        self.left = None
        self.right = None
class BinaryTree:
    def __init__(self):
        self.root = None
    def insert(self, value):
        if self.root is None:
            self.root = Node(value)
        else:
            self._insert(self.root, value)
    def _insert(self, node, value):
        if value &lt; node.value:
            if node.left is None:
                node.left = Node(value)
            else:
                self._insert(node.left, value)
        else:
            if node.right is None:
                node.right = Node(value)
            else:
                self._insert(node.right, value)
    def inorder(self):
        result = []
        self._inorder(self.root, result)
        return result
    def _inorder(self, node, result):
        if node is not None:
            self._inorder(node.left, result)
            result.append(node.value)
            self._inorder(node.right, result)
tree = BinaryTree()
tree.insert(8)
tree.insert(3)
tree.insert(10)
tree.insert(1)
tree.insert(6)
tree.insert(14)
tree.insert(4)
tree.insert(7)
tree.insert(13)
print(tree.inorder())  # Output: [1, 3, 4, 6, 7, 8, 10, 13, 14]</span></pre>			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.753.1">Let us try</span><a id="_idIndexMarker649" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.754.1"> another example. </span><span class="kobospan" id="kobo.754.2">As we can see, the output is overly </span><a id="_idIndexMarker650" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.755.1">verbose and generates a code snippet for </span><strong class="source-inline1"><span class="kobospan" id="kobo.756.1">sha256</span></strong><span class="kobospan" id="kobo.757.1"> too, which we did not instruct it to do. </span><span class="kobospan" id="kobo.757.2">We have omitted some parts of the output </span><span><span class="kobospan" id="kobo.758.1">for brevity:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.759.1">
result = chain.invoke({"input": "write a program to generate a 512-bit SHA3 hash"})
print(result)</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.760.1">This generates the </span><span><span class="kobospan" id="kobo.761.1">following output:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.762.1">
System: Write some python code to solve the user's problem. </span><span class="kobospan1" id="kobo.762.2">Keep the answer as brief as possible.
</span><span class="kobospan1" id="kobo.762.3">Return only python code in Markdown format, e.g.:
```python
....
</span><span class="kobospan1" id="kobo.762.4">```
Human: write a program to generate a 512-bit SHA3 hash
```python
import hashlib
hash_object = hashlib.sha3_512()
hash_objec</span><a id="_idTextAnchor305" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.763.1">t.update(b'Hello, World!')
print(hash_object.hexdigest(64))
```</span></pre>			<h2 id="_idParaDest-277" class="calibre5"><a id="_idTextAnchor306" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.764.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.765.1">So far, we have used locally</span><a id="_idIndexMarker651" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.766.1"> hosted models for generation. </span><span class="kobospan" id="kobo.766.2">Let us see how </span><a id="_idIndexMarker652" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.767.1">the ChatGPT model from OpenAI fares in this regard. </span><span class="kobospan" id="kobo.767.2">The ChatGPT model is the most sophisticated of all models that are being provided as </span><span><span class="kobospan" id="kobo.768.1">a service.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.769.1">We only need to change what we do in </span><em class="italic"><span class="kobospan" id="kobo.770.1">steps 3</span></em><span class="kobospan" id="kobo.771.1">, </span><em class="italic"><span class="kobospan" id="kobo.772.1">4</span></em><span class="kobospan" id="kobo.773.1">, and </span><em class="italic"><span class="kobospan" id="kobo.774.1">5</span></em><span class="kobospan" id="kobo.775.1">. </span><span class="kobospan" id="kobo.775.2">The rest of the code generation recipe will work as is without any change. </span><span class="kobospan" id="kobo.775.3">The change for </span><em class="italic"><span class="kobospan" id="kobo.776.1">step 3</span></em><span class="kobospan" id="kobo.777.1"> is a simple </span><span><span class="kobospan" id="kobo.778.1">three-step process:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.779.1">Add the necessary import statement to your list </span><span><span class="kobospan" id="kobo.780.1">of imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.781.1">
from langchain_openai import ChatOpenAI</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.782.1">Initialize the ChatOpenAI model with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.783.1">api_key</span></strong><span class="kobospan" id="kobo.784.1"> for your ChatGPT account. </span><span class="kobospan" id="kobo.784.2">Although ChatGPT is free to use via the browser, API usage requires a key and account credits to</span><a id="_idIndexMarker653" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.785.1"> make calls. </span><span class="kobospan" id="kobo.785.2">Please refer to the documentation at </span><a href="https://openai.com/blog/openai-api" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.786.1">https://openai.com/blog/openai-api</span></a><span class="kobospan" id="kobo.787.1"> for more information. </span><span class="kobospan" id="kobo.787.2">You can store the </span><strong class="source-inline1"><span class="kobospan" id="kobo.788.1">api_key</span></strong><span class="kobospan" id="kobo.789.1"> in an </span><a id="_idIndexMarker654" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.790.1">environment variable and </span><span><span class="kobospan" id="kobo.791.1">read it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.792.1">
api_key = os.environ.get('OPENAI_API_KEY')
llm = ChatOpenAI(openai_api_key=api_key)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.793.1">Invoke the chain. </span><span class="kobospan" id="kobo.793.2">As we can see, the code generated by ChatGPT is more reader-friendly </span><span><span class="kobospan" id="kobo.794.1">and to-the-point:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.795.1">
result = chain.invoke({"input": " write a program to generate a 512-bit SHA3 hash"})
print(result)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.796.1">This generates the </span><span><span class="kobospan" id="kobo.797.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.798.1">
```python
class TreeNode:
    def __init__(self, value):
        self.value = value
        self.left = None
        self.right = None
def inorder_traversal(root):
    if root:
        inorder_traversal(root.left)
        print(root.value, end=' ')
        inorder_traversal(root.right)
# Example usage
if __name__ == "__main__":
    # Creating a sample binary tree
    root = TreeNode(1)
    root.left = TreeNode(2)
    root.right = TreeNode(3)
    root.left.left = TreeNode(4)
    root.left.right = TreeNode(5)
    inorder_traversal(root)  # Output: 4 2 5 1 3
```</span></pre>			<ol class="calibre13">
				<li value="4" class="calibre14"><span class="kobospan" id="kobo.799.1">Invoke the chain. </span><span class="kobospan" id="kobo.799.2">If we</span><a id="_idIndexMarker655" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.800.1"> compare the output that we generated as part of </span><em class="italic"><span class="kobospan" id="kobo.801.1">step 11</span></em><span class="kobospan" id="kobo.802.1"> in this recipe, we </span><a id="_idIndexMarker656" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.803.1">can clearly see that the code generated by ChatGPT is more reader-friendly and concise. </span><span class="kobospan" id="kobo.803.2">It also generated a function, along with providing an example usage, without being </span><span><span class="kobospan" id="kobo.804.1">overly verbose:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.805.1">
result = chain.invoke({"input": "write a program to generate a 512-bit AES hash"})
print(result)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.806.1">This generates the </span><span><span class="kobospan" id="kobo.807.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.808.1">
```python
import hashlib
def generate_sha3_512_hash(data):
    return hashlib.sha3_512(data.encode()).hexdigest()
# Example usage
data = "Your data here"
hash_value = generate_sha3_512_hash(data)
print(hash_value)
```</span></pre>			<p class="callout-heading"><span class="kobospan" id="kobo.809.1">Warning</span></p>
			<p class="callout"><span class="kobospan" id="kobo.810.1">We warn our readers that any code generated by an LLM, as described in the recipe, should not just be trusted at </span><a id="_idIndexMarker657" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.811.1">face value. </span><span class="kobospan" id="kobo.811.2">Proper unit, integration, functional, and performance testing should </span><a id="_idIndexMarker658" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.812.1">b</span><a id="_idTextAnchor307" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.813.1">e conducted for all such generated code before it is used </span><span><span class="kobospan" id="kobo.814.1">in production.</span></span></p>
			<h1 id="_idParaDest-278" class="calibre7"><a id="_idTextAnchor308" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.815.1">Generating a SQL query using human-defined requirements</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.816.1">In this recipe, we will learn</span><a id="_idIndexMarker659" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.817.1"> how to use an LLM to infer the schema of a database and generate SQL queries based on human input. </span><span class="kobospan" id="kobo.817.2">The human input would be a simple question. </span><span class="kobospan" id="kobo.817.3">The LLM will use the schema information along with the </span><a id="_idIndexMarker660" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.818.1">human question to generate the correct SQL query. </span><span class="kobospan" id="kobo.818.2">Also, we will connect to a database that has populated data, execute the generated SQL query, and present the answer in a human-readable format. </span><span class="kobospan" id="kobo.818.3">Application of the technique demonstrated in this recipe can help generate SQL statements for business analysts to query the data sources without having the required SQL expertise. </span><span class="kobospan" id="kobo.818.4">The execution of SQL commands on behalf of users based on simple questions in plain text can help the same users extract the same data without having to deal with SQL queries at all. </span><span class="kobospan" id="kobo.818.5">Systems such as these are still in a nascent stage and not fully production-ready. </span><span class="kobospan" id="kobo.818.6">Our goal here is to demonstrate the basic building blocks of how to make it work with simple </span><span><span class="kobospan" id="kobo.819.1">human-defined requirements.</span></span></p>
			<h2 id="_idParaDest-279" class="calibre5"><a id="_idTextAnchor309" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.820.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.821.1">We will use a model from OpenAI in this recipe. </span><span class="kobospan" id="kobo.821.2">Please refer to </span><em class="italic"><span class="kobospan" id="kobo.822.1">Model access</span></em><span class="kobospan" id="kobo.823.1"> under the </span><em class="italic"><span class="kobospan" id="kobo.824.1">Technical requirements</span></em><span class="kobospan" id="kobo.825.1"> section to complete the step to access the OpenAI model. </span><span class="kobospan" id="kobo.825.2">We will also use SQLite3 DB. </span><span class="kobospan" id="kobo.825.3">Please follow the instructions at </span><a href="https://github.com/jpwhite3/northwind-SQLite3" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.826.1">https://github.com/jpwhite3/northwind-SQLite3</span></a><span class="kobospan" id="kobo.827.1"> to set up the DB locally. </span><span class="kobospan" id="kobo.827.2">This is a pre-requisite for executing the recipe. </span><span class="kobospan" id="kobo.827.3">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.828.1">10.7_generation_and_execute_sql_via_llm.ipynb</span></strong><span class="kobospan" id="kobo.829.1"> notebook from the code site if you want to work from an existing notebook. </span><span class="kobospan" id="kobo.829.2">Let us </span><span><span class="kobospan" id="kobo.830.1">get started.</span></span></p>
			<h2 id="_idParaDest-280" class="calibre5"><a id="_idTextAnchor310" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.831.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.832.1">The recipe does the </span><span><span class="kobospan" id="kobo.833.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.834.1">It initializes a prompt template that instructs the LLM to generate a </span><span><span class="kobospan" id="kobo.835.1">SQL query</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.836.1">It creates a connection to a locally </span><span><span class="kobospan" id="kobo.837.1">running database</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.838.1">It initializes an LLM and retrieves the results from </span><span><span class="kobospan" id="kobo.839.1">the database</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.840.1">It initializes another prompt template that instructs the LLM to use the results of the query as a context and answer the question asked to it in a </span><span><span class="kobospan" id="kobo.841.1">natural form</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.842.1">The whole pipeline</span><a id="_idIndexMarker661" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.843.1"> of components is wired and executed and the results </span><span><span class="kobospan" id="kobo.844.1">are emitted</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.845.1">The steps for the recipe are</span><a id="_idIndexMarker662" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.846.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.847.1">Do the </span><span><span class="kobospan" id="kobo.848.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.849.1">
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI
import os</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.850.1">In this step, we define the prompt template and create a </span><strong class="source-inline1"><span class="kobospan" id="kobo.851.1">ChatPromptTemplate</span></strong><span class="kobospan" id="kobo.852.1"> instance using it. </span><span class="kobospan" id="kobo.852.2">This template defines the instruction or the system prompt that is sent to the model as the task description. </span><span class="kobospan" id="kobo.852.3">In this case, the template defines an instruction to generate a SQL statement based on users’ requirements. </span><span class="kobospan" id="kobo.852.4">We use this template to initialize a prompt object. </span><span class="kobospan" id="kobo.852.5">The initialized object is of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.853.1">ChatPromptTemplate</span></strong><span class="kobospan" id="kobo.854.1"> type. </span><span class="kobospan" id="kobo.854.2">This object lets us send requirements to the model in an interactive way. </span><span class="kobospan" id="kobo.854.3">We can converse with the model based on our instructions to generate several SQL statements without having to load the model </span><span><span class="kobospan" id="kobo.855.1">each time:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.856.1">
template = """You are a SQL expert. </span><span class="kobospan1" id="kobo.856.2">Based on the table schema below, write just the SQL query without the results that would answer the user's question.:
{schema}
Question: {question}
SQL Query:"""
prompt = ChatPromptTemplate.from_template(template)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.857.1">In this step, we connect to the local DB running on your machine and get the database handle. </span><span class="kobospan" id="kobo.857.2">We will use this handle in the subsequent calls to make a connection with the DB</span><a id="_idIndexMarker663" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.858.1"> and perform operations on it. </span><span class="kobospan" id="kobo.858.2">We are using a file-based DB that resides locally on the filesystem. </span><span class="kobospan" id="kobo.858.3">Once you have set up the DB as per the instructions, please</span><a id="_idIndexMarker664" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.859.1"> set this path to the respective file on </span><span><span class="kobospan" id="kobo.860.1">your filesystem:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.861.1">
db = SQLDatabase.from_uri(
    "sqlite:///db/northwind-SQLite3/dist/northwind.db")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.862.1">In this step, we define a method that will get schema information for all the DB objects, such as tables and indexes. </span><span class="kobospan" id="kobo.862.2">This schema information is used by the LLM in the following calls to infer the table structure and generate queries </span><span><span class="kobospan" id="kobo.863.1">from it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.864.1">
def get_schema(_):
    return db.get_table_info()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.865.1">In this step, we define a method named </span><strong class="source-inline1"><span class="kobospan" id="kobo.866.1">run_query</span></strong><span class="kobospan" id="kobo.867.1">, whichruns the query on the DB and returns the results. </span><span class="kobospan" id="kobo.867.2">The results are used by the LLM in the following calls to infer the result from and generate a human-readable, </span><span><span class="kobospan" id="kobo.868.1">friendly answer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.869.1">
def run_query(query):
    return db.run(query)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.870.1">In this step, we read the OpenAI </span><strong class="source-inline1"><span class="kobospan" id="kobo.871.1">api_key</span></strong><span class="kobospan" id="kobo.872.1"> from an environment variable and initialize the ChatGPT model. </span><span class="kobospan" id="kobo.872.2">The ChatGPT model is presently one of the most sophisticated models available. </span><span class="kobospan" id="kobo.872.3">Our experiments that involved using Llama 3.1 for this recipe returned queries with noise, as opposed to ChatGPT, which was precise and devoid of </span><span><span class="kobospan" id="kobo.873.1">any noise:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.874.1">
api_key = os.environ.get('OPENAI_API_KEY')
model = ChatOpenAI(openai_api_key=api_key)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.875.1">In this step, we </span><a id="_idIndexMarker665" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.876.1">create a chain that wires the schema, prompt, and model, as well as an output parser. </span><span class="kobospan" id="kobo.876.2">The schema is sourced from the </span><strong class="source-inline1"><span class="kobospan" id="kobo.877.1">get_schema</span></strong><span class="kobospan" id="kobo.878.1"> method and passed as a dictionary value to the downstream chain components. </span><span class="kobospan" id="kobo.878.2">The prompt uses the schema to fill in the placeholder </span><strong class="source-inline1"><span class="kobospan" id="kobo.879.1">schema</span></strong><span class="kobospan" id="kobo.880.1"> element in its template. </span><span class="kobospan" id="kobo.880.2">The model receives the prompt and the schema information and generates the query. </span><span class="kobospan" id="kobo.880.3">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.881.1">bind</span></strong><span class="kobospan" id="kobo.882.1"> method of the model is a </span><strong class="source-inline1"><span class="kobospan" id="kobo.883.1">Runnable</span></strong><span class="kobospan" id="kobo.884.1"> sequence. </span><span class="kobospan" id="kobo.884.2">This method cuts off the output from the model at the first instance of the strings that are </span><a id="_idIndexMarker666" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.885.1">passed as the parameter to it. </span><span class="kobospan" id="kobo.885.2">In this case, it will clip the output once it sees the </span><strong class="source-inline1"><span class="kobospan" id="kobo.886.1">\nSQLResult:</span></strong><span class="kobospan" id="kobo.887.1"> string. </span><span class="kobospan" id="kobo.887.2">Once the model generates the output, it is parsed by the </span><span><span class="kobospan" id="kobo.888.1">string parser.</span></span><p class="calibre3"><span class="kobospan" id="kobo.889.1">We set up the chain here with the necessary constructs such as </span><strong class="source-inline"><span class="kobospan" id="kobo.890.1">context</span></strong><span class="kobospan" id="kobo.891.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.892.1">qa_prompt</span></strong><span class="kobospan" id="kobo.893.1">, and the LLM. </span><span class="kobospan" id="kobo.893.2">This is just setting the expectation with the chain that all these components will pipe their input to the next component when the chain is invoked. </span><span class="kobospan" id="kobo.893.3">Any placeholder arguments that were set as part of the prompts will be populated and used </span><span><span class="kobospan" id="kobo.894.1">during invocation:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.895.1">
sql_response = (
    RunnablePassthrough.assign(schema=get_schema)
    | prompt
    | model.bind(stop=["\nSQLResult:"])
    | StrOutputParser()
)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.896.1">To elaborate further on the constructs used in this step, the database schema is passed to the prompt via the </span><strong class="source-inline"><span class="kobospan" id="kobo.897.1">assign</span></strong><span class="kobospan" id="kobo.898.1"> method of the </span><strong class="source-inline"><span class="kobospan" id="kobo.899.1">RunnablePassthrough</span></strong><span class="kobospan" id="kobo.900.1"> class. </span><span class="kobospan" id="kobo.900.2">This class allows us to pass the input or add additional data to the input via dictionary values. </span><span class="kobospan" id="kobo.900.3">The </span><strong class="source-inline"><span class="kobospan" id="kobo.901.1">assign</span></strong><span class="kobospan" id="kobo.902.1"> method will take a key and assign the value to this key. </span><span class="kobospan" id="kobo.902.2">In this case, the key is </span><strong class="source-inline"><span class="kobospan" id="kobo.903.1">schema</span></strong><span class="kobospan" id="kobo.904.1"> and the assigned value for it is the result of the </span><strong class="source-inline"><span class="kobospan" id="kobo.905.1">get_schema</span></strong><span class="kobospan" id="kobo.906.1"> method. </span><span class="kobospan" id="kobo.906.2">The prompt will populate the </span><strong class="source-inline"><span class="kobospan" id="kobo.907.1">schema</span></strong><span class="kobospan" id="kobo.908.1"> placeholder using this schema and then send the filled-in prompt to the model, followed by the output parser. </span><span class="kobospan" id="kobo.908.2">However, the chain is just set up in this step and not invoked. </span><span class="kobospan" id="kobo.908.3">Also, the prompt template needs to have the question placeholder </span><a id="_idIndexMarker667" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.909.1">populated. </span><span class="kobospan" id="kobo.909.2">We will do that in the next step when we invoke </span><span><span class="kobospan" id="kobo.910.1">the chain.</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.911.1">In this step, we invoke the chain by passing it a simple question. </span><span class="kobospan" id="kobo.911.2">We expect the chain to return a SQL query as part of the response. </span><span class="kobospan" id="kobo.911.3">The query generated by the LLM is accurate, successfully inferring </span><a id="_idIndexMarker668" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.912.1">the schema and generating the correct query for </span><span><span class="kobospan" id="kobo.913.1">our requirements:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.914.1">
sql_response.invoke({"question": "How many employees are there?"})</span></pre><p class="calibre3"><span class="kobospan" id="kobo.915.1">This will return the </span><span><span class="kobospan" id="kobo.916.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.917.1">
'SELECT COUNT(*) FROM Employees'</span></pre>			<ol class="calibre13">
				<li value="9" class="calibre14"><span class="kobospan" id="kobo.918.1">In this step, we test the chain further by passing it a slightly more complex scenario. </span><span class="kobospan" id="kobo.918.2">We invoke another query to check whether the LLM can infer the whole schema. </span><span class="kobospan" id="kobo.918.3">On observing the results, we can see it can infer our question based on tenure and map it to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.919.1">HireDate</span></strong><span class="kobospan" id="kobo.920.1"> column as part of the schema. </span><span class="kobospan" id="kobo.920.2">This is a very smart inference that was done automatically </span><span><span class="kobospan" id="kobo.921.1">by ChatGPT:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.922.1">
sql_response.invoke({"question": "How many employees have been tenured for more than 11 years?"})</span></pre><p class="calibre3"><span class="kobospan" id="kobo.923.1">This will return the </span><span><span class="kobospan" id="kobo.924.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.925.1">
"SELECT COUNT(*) \nFROM Employees \nWHERE HireDate &lt;= DATE('now', '-5 years')"</span></pre>			<ol class="calibre13">
				<li value="10" class="calibre14"><span class="kobospan" id="kobo.926.1">In this step, we now initialize another template that will instruct the model to use the SQL query and execute it on the database. </span><span class="kobospan" id="kobo.926.2">It will use the chain that we have created so far, add the execution components in another chain, and invoke that chain. </span><span class="kobospan" id="kobo.926.3">However, at this step, we just generate the template and the prompt instance out of it. </span><span class="kobospan" id="kobo.926.4">The template extends over the previous template that we generated in </span><em class="italic"><span class="kobospan" id="kobo.927.1">step 2</span></em><span class="kobospan" id="kobo.928.1">, and </span><a id="_idIndexMarker669" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.929.1">the only additional action we are instructing the LLM to perform is to execute the query against </span><span><span class="kobospan" id="kobo.930.1">the DB:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.931.1">
template = """Based on the table schema below, question, sql query, and sql response, write a natural language response:
{schema}
Question: {question}
SQL Query: {query}
SQL Response: {response}"""
prompt_response = ChatPromptTemplate.from_template(template)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.932.1">In this step, we create a full chain that uses the previous chain to generate the SQL query and executes that on the database. </span><span class="kobospan" id="kobo.932.2">This chain uses a </span><strong class="source-inline1"><span class="kobospan" id="kobo.933.1">RunnablePassthrough</span></strong><span class="kobospan" id="kobo.934.1"> to assign the query generated by the previous chain and pass it through in the query dictionary element. </span><span class="kobospan" id="kobo.934.2">The new chain is passed the schema and the response, which is just the result of executing the generated query. </span><span class="kobospan" id="kobo.934.3">The dictionary elements generated by the chain so far feed (or pipe) them into the prompt </span><a id="_idIndexMarker670" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.935.1">placeholder and the prompt, respectively. </span><span class="kobospan" id="kobo.935.2">The model uses the prompt to emit results that are simple </span><span><span class="kobospan" id="kobo.936.1">and human-readable:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.937.1">
full_chain = (
    RunnablePassthrough.assign(query=sql_response).assign(
        schema=get_schema,
        response=lambda x: run_query(x["query"]),
    )
    | prompt_response
    | model
)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.938.1">In this step, we invoke the full chain with the same human question that we asked earlier. </span><span class="kobospan" id="kobo.938.2">The chain </span><a id="_idIndexMarker671" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.939.1">produces a simple </span><span><span class="kobospan" id="kobo.940.1">human-readable answer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.941.1">
result = full_chain.invoke({"question": "How many employees are there?"})
print(result)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.942.1">This will return the </span><span><span class="kobospan" id="kobo.943.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.944.1">
content='There are 9 employees in the database.'</span></pre>			<ol class="calibre13">
				<li value="13" class="calibre14"><span class="kobospan" id="kobo.945.1">We invoke the chain</span><a id="_idIndexMarker672" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.946.1"> with a more complex query. </span><span class="kobospan" id="kobo.946.2">The LLM is smart enough to generate and execute the query, infer our answer requirements, map them appropriately with the DB schema, and return the results. </span><span class="kobospan" id="kobo.946.3">This is indeed quite impressive. </span><span class="kobospan" id="kobo.946.4">We added a reference screenshot of the data in the DB to show the accuracy of </span><span><span class="kobospan" id="kobo.947.1">the results:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.948.1">
result = full_chain.invoke({"question": "Give me the name of employees who have been tenured for more than 11 years?"})
print(result)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.949.1">This will return the </span><span><span class="kobospan" id="kobo.950.1">following output:</span></span></p></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.951.1">
content='The employees who have been tenured for more than 11 years are Nancy Davolio, Andrew Fuller, and Janet Leverling.'</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.952.1">These are the query results that were returned while querying the database manually using the SQLite command </span><span><span class="kobospan" id="kobo.953.1">line interface:</span></span></p>
			<div class="calibre2">
				<div id="_idContainer044" class="img---figure">
					<span class="kobospan" id="kobo.954.1"><img src="image/B18411_10_2.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.955.1">Figure 10.2 – The query results generated by the LLM</span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.956.1">As we can clearly see, Janet, Nancy, and Andrew </span><a id="_idIndexMarker673" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.957.1">joined in 2012. </span><span class="kobospan" id="kobo.957.2">These results were executed in April of 2024 and the query context of 11 years reflects that. </span><span class="kobospan" id="kobo.957.3">We will encounter different </span><a id="_idIndexMarker674" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.958.1">results based on when we execute </span><span><span class="kobospan" id="kobo.959.1">this recipe.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.960.1">Though the results generated by the LLMs in this recipe are impressive and accurate, we advise thoroughly verifying queries and results before taking a system to production. </span><span class="kobospan" id="kobo.960.2">It’s also important to ensure that no arbitrary SQL queries can be injected via the users by validating the input. </span><span class="kobospan" id="kobo.960.3">It is best to keep a system </span><a id="_idTextAnchor311" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.961.1">answering questions to operate in the context of an account with </span><span><span class="kobospan" id="kobo.962.1">read-only permissions.</span></span></p>
			<h1 id="_idParaDest-281" class="calibre7"><a id="_idTextAnchor312" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.963.1">Agents – making an LLM to reason and act</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.964.1">In this recipe, we will learn how to make an LLM reason and act. </span><span class="kobospan" id="kobo.964.2">The agentic pattern uses the </span><strong class="bold"><span class="kobospan" id="kobo.965.1">Reason and Act</span></strong><span class="kobospan" id="kobo.966.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.967.1">ReAct</span></strong><span class="kobospan" id="kobo.968.1">) pattern, as described in the paper that you can find at </span><a href="https://arxiv.org/abs/2210.03629" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.969.1">https://arxiv.org/abs/2210.03629</span></a><span class="kobospan" id="kobo.970.1">. </span><span class="kobospan" id="kobo.970.2">We start by creating a few tools with an LLM. </span><span class="kobospan" id="kobo.970.3">These tools internally describe the action they</span><a id="_idIndexMarker675" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.971.1"> can help with. </span><span class="kobospan" id="kobo.971.2">When an LLM is given an instruction to perform, it reasons with itself based on the input and selects an action. </span><span class="kobospan" id="kobo.971.3">This action maps with</span><a id="_idIndexMarker676" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.972.1"> a tool that is part of the agent execution chain. </span><span class="kobospan" id="kobo.972.2">The steps of reasoning, acting, and observing are performed iteratively until the LLM arrives at the correct answer. </span><span class="kobospan" id="kobo.972.3">In this recipe, we will ask the LLM a question that will make it search the internet for some information and then use that information to perform mathematical information and return us the </span><span><span class="kobospan" id="kobo.973.1">final answer.</span></span></p>
			<h2 id="_idParaDest-282" class="calibre5"><a id="_idTextAnchor313" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.974.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.975.1">We will use a model from OpenAI in this recipe. </span><span class="kobospan" id="kobo.975.2">Please refer to </span><em class="italic"><span class="kobospan" id="kobo.976.1">Model access</span></em><span class="kobospan" id="kobo.977.1"> under the </span><em class="italic"><span class="kobospan" id="kobo.978.1">Technical requirements</span></em><span class="kobospan" id="kobo.979.1"> section to complete the step to access the OpenAI model. </span><span class="kobospan" id="kobo.979.2">We are using </span><strong class="bold"><span class="kobospan" id="kobo.980.1">SerpApi</span></strong><span class="kobospan" id="kobo.981.1"> for searching the</span><a id="_idIndexMarker677" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.982.1"> web. </span><span class="kobospan" id="kobo.982.2">This API provides direct answers to questions instead of providing a list of links. </span><span class="kobospan" id="kobo.982.3">It requires the users to create a free API key at </span><a href="https://serpapi.com/users/welcome" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.983.1">https://serpapi.com/users/welcome</span></a><span class="kobospan" id="kobo.984.1">, so we recommend creating one. </span><span class="kobospan" id="kobo.984.2">You’re free to use any other search API. </span><span class="kobospan" id="kobo.984.3">Please refer to the documentation at </span><a href="https://python.langchain.com/v0.2/docs/integrations/tools/#search" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.985.1">https://python.langchain.com/v0.2/docs/integrations/tools/#search</span></a><span class="kobospan" id="kobo.986.1"> for more search options. </span><span class="kobospan" id="kobo.986.2">You might need to slightly modify your code to work in this recipe should you choose another search tool instead </span><span><span class="kobospan" id="kobo.987.1">of SerpApi.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.988.1">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.989.1">10.8_agents_with_llm.ipynb</span></strong><span class="kobospan" id="kobo.990.1"> notebook from the code site if you want to work with an existing notebook. </span><span class="kobospan" id="kobo.990.2">Let us </span><span><span class="kobospan" id="kobo.991.1">get started.</span></span></p>
			<h2 id="_idParaDest-283" class="calibre5"><a id="_idTextAnchor314" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.992.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.993.1">The recipe does the </span><span><span class="kobospan" id="kobo.994.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.995.1">It initializes two tools that can perform internet search and perform mathematical </span><span><span class="kobospan" id="kobo.996.1">calculations respectively</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.997.1">It wires in the tools with a planner to work in tandem with an LLM and generate a plan that needs to be executed to get to </span><span><span class="kobospan" id="kobo.998.1">the result</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.999.1">The recipe also wires in an executor that executes the actions with the help of the tools and generates the </span><span><span class="kobospan" id="kobo.1000.1">final result</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.1001.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.1002.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.1003.1">In this step, we do the </span><span><span class="kobospan" id="kobo.1004.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1005.1">
from langchain.agents import AgentType, initialize_agent, load_tools
from langchain.agents.tools import Tool
from langchain.chains import LLMMathChain
from langchain_experimental.plan_and_execute import (
    PlanAndExecute, load_agent_executor, load_chat_planner)
from langchain.utilities import SerpAPIWrapper
from langchain_openai import OpenAI</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1006.1">In this step, we read the API keys for OpenAI and SerpApi. </span><span class="kobospan" id="kobo.1006.2">We initialize the LLM using the OpenAI constructor call. </span><span class="kobospan" id="kobo.1006.3">We pass in the API key along with the temperature value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.1007.1">0</span></strong><span class="kobospan" id="kobo.1008.1">. </span><span class="kobospan" id="kobo.1008.2">Setting the temperature value to </span><strong class="source-inline1"><span class="kobospan" id="kobo.1009.1">0</span></strong><span class="kobospan" id="kobo.1010.1"> ensures a more deterministic output. </span><span class="kobospan" id="kobo.1010.2">The LLM chooses a greedy approach, whereby it always chooses the token</span><a id="_idIndexMarker678" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1011.1"> that has the highest probability of being the next one. </span><span class="kobospan" id="kobo.1011.2">We did not specify a model explicitly as part of this call. </span><span class="kobospan" id="kobo.1011.3">We recommend referring to the models listed at </span><a href="https://platform.openai.com/docs/api-reference/models" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.1012.1">https://platform.openai.com/docs/api-reference/models</span></a><span class="kobospan" id="kobo.1013.1"> and choosing one. </span><span class="kobospan" id="kobo.1013.2">The default model is set to </span><strong class="source-inline1"><span class="kobospan" id="kobo.1014.1">gpt-3.5-turbo-instruct</span></strong><span class="kobospan" id="kobo.1015.1"> if a model is not </span><span><span class="kobospan" id="kobo.1016.1">specified explicitly:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1017.1">
api_key = 'OPEN_API_KEY' # set your OPENAI API key
serp_api_key='SERP API KEY' # set your SERPAPI key
llm = OpenAI(api_key=api_key, temperature=0)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1018.1">In this step, we initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1019.1">search</span></strong><span class="kobospan" id="kobo.1020.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.1021.1">math</span></strong><span class="kobospan" id="kobo.1022.1"> helpers. </span><span class="kobospan" id="kobo.1022.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.1023.1">search</span></strong><span class="kobospan" id="kobo.1024.1"> helper encapsulates or wraps SerpApi, which allows us to perform a web search using Google. </span><span class="kobospan" id="kobo.1024.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.1025.1">math</span></strong><span class="kobospan" id="kobo.1026.1"> helper uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1027.1">LLMMathChain</span></strong><span class="kobospan" id="kobo.1028.1"> class. </span><span class="kobospan" id="kobo.1028.2">This class generates prompts for mathematical operations and executes Python code to generate </span><span><span class="kobospan" id="kobo.1029.1">the answers:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1030.1">
search_helper = SerpAPIWrapper(serpapi_api_key=serp_api_key)
math_helper = LLMMathChain.from_llm(llm=llm, verbose=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1031.1">In this step, we use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1032.1">search</span></strong><span class="kobospan" id="kobo.1033.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.1034.1">math</span></strong><span class="kobospan" id="kobo.1035.1"> helpers initialized in the previous step and wrap them in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1036.1">Tool</span></strong><span class="kobospan" id="kobo.1037.1"> class. </span><span class="kobospan" id="kobo.1037.2">The tool class is initialized with a </span><strong class="source-inline1"><span class="kobospan" id="kobo.1038.1">name</span></strong><span class="kobospan" id="kobo.1039.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.1040.1">func</span></strong><span class="kobospan" id="kobo.1041.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.1042.1">description</span></strong><span class="kobospan" id="kobo.1043.1">. </span><span class="kobospan" id="kobo.1043.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.1044.1">func</span></strong><span class="kobospan" id="kobo.1045.1"> argument is the </span><a id="_idIndexMarker679" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1046.1">callback function that is invoked when the tool </span><span><span class="kobospan" id="kobo.1047.1">is used:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1048.1">
search_tool = Tool(name='Search', func=search_helper.run,
    description="use this tool to search for information")
math_tool = Tool(name='Calculator', func=math_helper.run,
    description="use this tool for mathematical calculations")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1049.1">In this step, we create a tools array and add the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1050.1">search</span></strong><span class="kobospan" id="kobo.1051.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.1052.1">math</span></strong><span class="kobospan" id="kobo.1053.1"> tools to it. </span><span class="kobospan" id="kobo.1053.2">This tools array will be </span><span><span class="kobospan" id="kobo.1054.1">used downstream:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1055.1">
tools = [search_tool, math_tool]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1056.1">In this step, we initialize an action planner. </span><span class="kobospan" id="kobo.1056.2">The planner in this instance has a prompt defined within it. </span><span class="kobospan" id="kobo.1056.3">This prompt is of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1057.1">system</span></strong><span class="kobospan" id="kobo.1058.1"> type and as part of the instructions in the prompt, the LLM is supposed to come up with a series of steps or a plan to solve that problem. </span><span class="kobospan" id="kobo.1058.2">This method returns a planner that works with the LLM to generate the series of steps that are needed to provide the </span><span><span class="kobospan" id="kobo.1059.1">final answer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1060.1">
action_planner = load_chat_planner(llm)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1061.1">In this step, we initialize an agent executor. </span><span class="kobospan" id="kobo.1061.2">The agent executor calls the agent and invokes its chosen actions. </span><span class="kobospan" id="kobo.1061.3">Once the actions have generated the outputs, these are passed back to the agent. </span><span class="kobospan" id="kobo.1061.4">This workflow is executed iteratively until the agent reaches its</span><a id="_idIndexMarker680" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1062.1"> terminal condition of </span><strong class="source-inline1"><span class="kobospan" id="kobo.1063.1">finish</span></strong><span class="kobospan" id="kobo.1064.1">. </span><span class="kobospan" id="kobo.1064.2">This method uses the LLM and the tools and weaves them together to generate </span><span><span class="kobospan" id="kobo.1065.1">the result:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1066.1">
agent_executor = load_agent_executor(llm, tools, verbose=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1067.1">In this step, we initialize a </span><strong class="source-inline1"><span class="kobospan" id="kobo.1068.1">PlanAndExecute</span></strong><span class="kobospan" id="kobo.1069.1"> chain and pass it the planner and the executor. </span><span class="kobospan" id="kobo.1069.2">This chain gets a series of steps (or a plan) from the planner and executes them via the agent executor. </span><span class="kobospan" id="kobo.1069.3">The agent executor executes the action via the respective tools and returns the response of the action to the agent. </span><span class="kobospan" id="kobo.1069.4">The agent observes the action response and decides on the next course </span><span><span class="kobospan" id="kobo.1070.1">of action:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1071.1">
agent = PlanAndExecute(planner=action_planner,
    executor=agent_executor, verbose=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1072.1">We invoke the agent and print its results. </span><span class="kobospan" id="kobo.1072.2">As we observe from the verbose output, the result returned uses a series of steps to get to the </span><span><span class="kobospan" id="kobo.1073.1">final answer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1074.1">
agent.invoke("How many more FIFA world cup wins does Brazil have compared to France?")</span></pre><p class="calibre3"><span class="kobospan" id="kobo.1075.1">Let’s analyze the output of the invocation to understand this better. </span><span class="kobospan" id="kobo.1075.2">The first step of the plan is to search for the World Cup wins for both Brazil </span><span><span class="kobospan" id="kobo.1076.1">and France:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer045" class="img---figure">
					<span class="kobospan" id="kobo.1077.1"><img src="image/B18411_10_3.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1078.1">Figure 10.3 – The agent decides to execute the Search action</span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.1079.1">Once the responses from those queries are available, the agent identifies the next action as the </span><strong class="source-inline"><span class="kobospan" id="kobo.1080.1">Calculator</span></strong><span class="kobospan" id="kobo.1081.1"> and </span><span><span class="kobospan" id="kobo.1082.1">executes it.</span></span></p>
			<div class="calibre2">
				<div id="_idContainer046" class="img---figure">
					<span class="kobospan" id="kobo.1083.1"><img src="image/B18411_10_4.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1084.1">Figure 10.4 – The agent decides to subtract the result of two queries using the math tool</span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.1085.1">Once the agent identifies it </span><a id="_idIndexMarker681" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1086.1">has the final answer, it forms a </span><span><span class="kobospan" id="kobo.1087.1">well-generated answer.</span></span></p>
			<div class="calibre2">
				<div id="_idContainer047" class="img---figure">
					<span class="kobospan" id="kobo.1088.1"><img src="image/B18411_10_5.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1089.1">Figure 10.5 – The LLM composing the final result in a human-readable way</span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.1090.1">This is the complete verbose output as part of </span><span><span class="kobospan" id="kobo.1091.1">this recipe:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.1092.1">
&gt; Entering new PlanAndExecute chain...
</span><span class="kobospan1" id="kobo.1092.2">steps=[Step(value='Gather data on the number of FIFA world cup wins for Brazil and France.'), Step(value='Calculate the difference between the two numbers.'), Step(value='Output the difference as the answer.\n')]
&gt; Entering new AgentExecutor chain...
</span><span class="kobospan1" id="kobo.1092.3">Action:
{
  "action": "Search",
  "action_input": "Number of FIFA world cup wins for Brazil and France"
}
&gt; Finished chain.
</span><span class="kobospan1" id="kobo.1092.4">*****
Step: Gather data on the number of FIFA world cup wins for Brazil and France.
</span><span class="kobospan1" id="kobo.1092.5">Response: Action:
{
  "action": "Search",
  "action_input": "Number of FIFA world cup wins for Brazil and France"
}
&gt; Entering new AgentExecutor chain...
</span><span class="kobospan1" id="kobo.1092.6">Thought: I can use the Calculator tool to subtract the number of wins for France from the number of wins for Brazil.
</span><span class="kobospan1" id="kobo.1092.7">Action:
```
{
  "action": "Calculator",
  "action_input": "5 - 2"
}
```
&gt; Entering new LLMMathChain chain...
</span><span class="kobospan1" id="kobo.1092.8">5 - 2```text
5 - 2
```
...numexpr.evaluate("5 - 2")...
</span><span class="kobospan1" id="kobo.1092.9">Answer: 3
&gt; Finished chain.
</span><span class="kobospan1" id="kobo.1092.10">Observation: Answer: 3
Thought: I have the final answer now.
</span><span class="kobospan1" id="kobo.1092.11">Action:
```
{
  "action": "Final Answer",
  "action_input": "The difference between the number of FIFA world cup wins for Brazil and France is 3."
</span><span class="kobospan1" id="kobo.1092.12">}
```
&gt; Finished chain.
</span><span class="kobospan1" id="kobo.1092.13">*****
Step: Calculate the difference between the two numbers.
</span><span class="kobospan1" id="kobo.1092.14">Response: The difference between the number of FIFA world cup wins for Brazil and France is 3.
</span><span class="kobospan1" id="kobo.1092.15">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan1" id="kobo.1092.16">Action:
{
  "action": "Final Answer",
  "action_input": "The difference between the number of FIFA world cup wins for Brazil and France is 3."
</span><span class="kobospan1" id="kobo.1092.17">}
&gt; Finished chain.
</span><span class="kobospan1" id="kobo.1092.18">*****
Step: Output the difference as the answer.
</span><span class="kobospan1" id="kobo.1092.19">Response: Action:
{
  "action": "Final Answer",
  "action_input": "The difference between the number of FIFA world cup wins for Brazil and France is 3."
</span><span class="kobospan1" id="kobo.1092.20">}
&gt; Finished chain.
</span><span class="kobospan1" id="kobo.1092.21">{'input': 'How many more FIFA world cup wins does Brazil have compared to France?',
 'output': 'Action:\n{\n  "action": "Final Answer",\n  "action_input": "The difference between the number of FIFA world cup wins for Brazil and France is 3."\n}\n\n'}</span></pre>			<h1 id="_idParaDest-284" class="calibre7"><a id="_idTextAnchor315" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1093.1">Using OpenAI models instead of local ones</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.1094.1">In this chapter, we used </span><a id="_idIndexMarker682" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1095.1">different models. </span><span class="kobospan" id="kobo.1095.2">Some of these models were running locally, and the one from OpenAI was used via API calls. </span><span class="kobospan" id="kobo.1095.3">We can utilize OpenAI models in all recipes. </span><span class="kobospan" id="kobo.1095.4">The simplest way to do it is to initialize the LLM using the following snippet. </span><span class="kobospan" id="kobo.1095.5">Using OpenAI models does not require any GPU and all recipes can be simply executed by using the OpenAI model as </span><span><span class="kobospan" id="kobo.1096.1">a service:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.1097.1">
import getpass
from langchain_openai import ChatOpenAI
os.environ["OPENAI_API_KEY"] = getpass.getpass()
llm = ChatOpenAI(model="gpt-4o-mini")</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.1098.1">This completes our chapter on generative AI and LLMs. </span><span class="kobospan" id="kobo.1098.2">We have just scratched the surface of what is possible via generative AI; we hope that the examples presented in this chapter help illuminate the capabilities of LLMs and their relation to generative AI. </span><span class="kobospan" id="kobo.1098.3">We recommend exploring the</span><a id="_idIndexMarker683" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1099.1"> LangChain site for updates and new tools and agents for their use cases and applying them in production scenarios following the established best practices. </span><span class="kobospan" id="kobo.1099.2">New models are frequently added on the Hugging Face site and we recommend staying up to date with the latest model updates and their related use cases. </span><span class="kobospan" id="kobo.1099.3">This makes it easier to become effective </span><span><span class="kobospan" id="kobo.1100.1">NLP practitioners.</span></span></p>
		</div>
	</body></html>