["```py\nsystem_template = \"\"\"\nYou are a helpful assistant who answers all the user's questions politely.\n\"\"\"\nprompt_template = \"\"\"\nAnswer the user's question using only the provided context. If you cannot answer using the context, respond with \"I don't know.\"\nContext: {context}\nUser question: {user_question}\n\"\"\"\nuser_question = \"<your_question>\"\nretrieved_context = retrieve(user_question)\nprompt = f\"{system_template}\\n\"\nprompt += prompt_template.format(context=retrieved_context, user_question=user_question)\nanswer  = llm(prompt) \n```", "```py\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nsentences = [\n\"The dog sits outside waiting for a treat.\",\n\"I am going swimming.\",\n\"The dog is swimming.\"\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# Output: [3, 384]\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# Output:\n# tensor([[ 1.0000, -0.0389, 0.2692],\n# [-0.0389, 1.0000, 0.3837],\n# [ 0.2692, 0.3837, 1.0000]])\n#\n# similarities[0, 0] = The similarity between the first sentence and itself.\n# similarities[0, 1] = The similarity between the first and second sentence.\n# similarities[2, 1] = The similarity between the third and second sentence. \nhttps://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_embeddings.py.\n```", "```py\nfrom io import BytesIO\nimport requests\nfrom PIL import Image\nfrom sentence_transformers import SentenceTransformer\nresponse = requests.get(\n\"https://github.com/PacktPublishing/LLM-Engineering/blob/main/images/crazy_cat.jpg?raw=true\"\n)\nimage = Image.open(BytesIO(response.content))\nmodel = SentenceTransformer(\"clip-ViT-B-32\")\nimg_emb = model.encode(image)\ntext_emb = model.encode(\n[\"A crazy cat smiling.\",\n\"A white and brown cat with a yellow bandana.\",\n\"A man eating in the garden.\"]\n)\nprint(text_emb.shape) # noqa\n# Output: (3, 512)\nsimilarity_scores = model.similarity(img_emb, text_emb)\nprint(similarity_scores) # noqa\n# Output: tensor([[0.3068, 0.3300, 0.1719]]) \n```", "```py\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR(\"hkunlp/instructor-base\")\nsentence = \"RAG Fundamentals First\"\ninstruction = \"Represent the title of an article about AI:\"\nembeddings = model.encode([[instruction, sentence]])\nprint(embeddings.shape) # noqa\n# Output: (1, 768) \n```", "```py\npython3 -m venv instructor_venv && source instructor_venv/bin/activate \n```", "```py\npip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1 \n```", "```py\nfrom pydantic import BaseSettings\nclass Settings(BaseSettings):\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n    … # Some other settings…\n    # RAG\n    TEXT_EMBEDDING_MODEL_ID: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    RERANKING_CROSS_ENCODER_MODEL_ID: str = \"cross-encoder/ms-marco-MiniLM-L-4-v2\"\n    RAG_MODEL_DEVICE: str = \"cpu\"\n    # QdrantDB Vector DB\n    USE_QDRANT_CLOUD: bool = False\n    QDRANT_DATABASE_HOST: str = \"localhost\"\n    QDRANT_DATABASE_PORT: int = 6333\n    QDRANT_CLOUD_URL: str = \"str\"\n    QDRANT_APIKEY: str | None = None\n    … # More settings…\nsettings = Settings() \n```", "```py\nfrom zenml import pipeline\nfrom llm_engineering.interfaces.orchestrator.steps import feature_engineering as fe_steps\n@pipeline\ndef feature_engineering(author_full_names: list[str]) -> None:\n    raw_documents = fe_steps.query_data_warehouse(author_full_names)\n    cleaned_documents = fe_steps.clean_documents(raw_documents)\n     last_step_1 = fe_steps.load_to_vector_db(cleaned_documents)\n    embedded_documents = fe_steps.chunk_and_embed(cleaned_documents)\n    last_step_2 = fe_steps.load_to_vector_db(embedded_documents)\n    return [last_step_1.invocation_id, last_step_2.invocation_id] \n```", "```py\nparameters:\n  author_full_names:\n    - Alex Vesa\n    - Maxime Labonne\n    - Paul Iusztin \n```", "```py\nfeature_engineering.with_options(config_path=\"…/feature_engineering.yaml\")() \n```", "```py\npython -m tools.run --no-cache --run-feature-engineering \n```", "```py\npoetry poe run-feature-engineering-pipeline \n```", "```py\n… # other imports\nfrom zenml import get_step_context, step\n@step\ndef query_data_warehouse(\n    author_full_names: list[str],\n) -> Annotated[list, \"raw_documents\"]:\n    documents = []\n    authors = []\n    for author_full_name in author_full_names:\n        logger.info(f\"Querying data warehouse for user: {author_full_name}\")\n        first_name, last_name = utils.split_user_full_name(author_full_name)\n        logger.info(f\"First name: {first_name}, Last name: {last_name}\")\n        user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)\n        authors.append(user)\n        results = fetch_all_data(user)\n        user_documents = [doc for query_result in results.values() for doc in query_result]\n        documents.extend(user_documents)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"raw_documents\", metadata=_get_metadata(documents))\n    return documents \n```", "```py\ndef fetch_all_data(user: UserDocument) -> dict[str, list[NoSQLBaseDocument]]:\n    user_id = str(user.id)\n    with ThreadPoolExecutor() as executor:\n        future_to_query = {\n            executor.submit(__fetch_articles, user_id): \"articles\",\n            executor.submit(__fetch_posts, user_id): \"posts\",\n            executor.submit(__fetch_repositories, user_id): \"repositories\",\n        }\n        results = {}\n        for future in as_completed(future_to_query):\n            query_name = future_to_query[future]\n            try:\n                results[query_name] = future.result()\n            except Exception:\n                logger.exception(f\"'{query_name}' request failed.\")\n                results[query_name] = []\n    return results \n```", "```py\ndef _get_metadata(documents: list[Document]) -> dict:\n    metadata = {\n        \"num_documents\": len(documents),\n    }\n    for document in documents:\n        collection = document.get_collection_name()\n        if collection not in metadata:\n            metadata[collection] = {}\n        if \"authors\" not in metadata[collection]:\n            metadata[collection][\"authors\"] = list()\n        metadata[collection][\"num_documents\"] = metadata[collection].get(\"num_documents\", 0) + 1\n        metadata[collection][\"authors\"].append(document.author_full_name)\n    for value in metadata.values():\n        if isinstance(value, dict) and \"authors\" in value:\n            value[\"authors\"] = list(set(value[\"authors\"]))\n    return metadata \n```", "```py\n@step\ndef clean_documents(\n    documents: Annotated[list, \"raw_documents\"],\n) -> Annotated[list, \"cleaned_documents\"]:\n    cleaned_documents = []\n    for document in documents:\n        cleaned_document = CleaningDispatcher.dispatch(document)\n        cleaned_documents.append(cleaned_document)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"cleaned_documents\", metadata=_get_metadata(cleaned_documents))\n    return cleaned_documents \n```", "```py\n@step\ndef chunk_and_embed(\n    cleaned_documents: Annotated[list, \"cleaned_documents\"],\n) -> Annotated[list, \"embedded_documents\"]:\n    metadata = {\"chunking\": {}, \"embedding\": {}, \"num_documents\": len(cleaned_documents)}\n    embedded_chunks = []\n    for document in cleaned_documents:\n        chunks = ChunkingDispatcher.dispatch(document)\n        metadata[\"chunking\"] = _add_chunks_metadata(chunks, metadata[\"chunking\"])\n        for batched_chunks in utils.misc.batch(chunks, 10):\n            batched_embedded_chunks = EmbeddingDispatcher.dispatch(batched_chunks)\n            embedded_chunks.extend(batched_embedded_chunks)\n    metadata[\"embedding\"] = _add_embeddings_metadata(embedded_chunks, metadata[\"embedding\"])\n    metadata[\"num_chunks\"] = len(embedded_chunks)\n    metadata[\"num_embedded_chunks\"] = len(embedded_chunks)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"embedded_documents\", metadata=metadata)\n    return embedded_chunks \n```", "```py\n@step\ndef load_to_vector_db(\n    documents: Annotated[list, \"documents\"],\n) -> None:\n    logger.info(f\"Loading {len(documents)} documents into the vector database.\")\n    grouped_documents = VectorBaseDocument.group_by_class(documents)\n    for document_class, documents in grouped_documents.items():\n        logger.info(f\"Loading documents into {document_class.get_collection_name()}\")\n        for documents_batch in utils.misc.batch(documents, size=4):\n            try:\n                document_class.bulk_insert(documents_batch)\n            except Exception:\n                return False\n    return True \n```", "```py\nclass CleanedDocument(VectorBaseDocument, ABC):\n    content: str\n    platform: str\n    author_id: UUID4\n    author_full_name: str\nclass CleanedPostDocument(CleanedDocument):\n    image: Optional[str] = None\n    class Config:\n        name = \"cleaned_posts\"\n        category = DataCategory.POSTS\n        use_vector_index = False\nclass CleanedArticleDocument(CleanedDocument):\n    link: str\n    class Config:\n        name = \"cleaned_articles\"\n        category = DataCategory.ARTICLES\n        use_vector_index = False\nclass CleanedRepositoryDocument(CleanedDocument):\n    name: str\n    link: str\n    class Config:\n        name = \"cleaned_repositories\"\n        category = DataCategory.REPOSITORIES\n        use_vector_index = False \n```", "```py\nclass Chunk(VectorBaseDocument, ABC):\n    content: str\n    platform: str\n    document_id: UUID4\n    author_id: UUID4\n    author_full_name: str\n    metadata: dict = Field(default_factory=dict)\n… # PostChunk, ArticleChunk, RepositoryChunk\nclass EmbeddedChunk(VectorBaseDocument, ABC):\n    content: str\n    embedding: list[float] | None\n    platform: str\n    document_id: UUID4\n    author_id: UUID4\n    author_full_name: str\n    metadata: dict = Field(default_factory=dict)\n… # EmbeddedPostChunk, EmbeddedArticleChunk, EmbeddedRepositoryChunk \n```", "```py\nclass DataCategory(StrEnum):\n    POSTS = \"posts\"\n    ARTICLES = \"articles\"\n    REPOSITORIES = \"repositories\" \n```", "```py\nfrom pydantic import UUID4, BaseModel\nfrom typing import Generic\nfrom llm_engineering.infrastructure.db.qdrant import connection\nT = TypeVar(\"T\", bound=\"VectorBaseDocument\")\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    id: UUID4 = Field(default_factory=uuid.uuid4)\n    @classmethod\n    def from_record(cls: Type[T], point: Record) -> T:\n        _id = UUID(point.id, version=4)\n        payload = point.payload or {}\n        attributes = {\n            \"id\": _id,\n            **payload,\n        }\n        if cls._has_class_attribute(\"embedding\"):\n            payload[\"embedding\"] = point.vector or None\n        return cls(**attributes)\n    def to_point(self: T, **kwargs) -> PointStruct:\n        exclude_unset = kwargs.pop(\"exclude_unset\", False)\n        by_alias = kwargs.pop(\"by_alias\", True)\n        payload = self.dict(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)\n        _id = str(payload.pop(\"id\"))\n        vector = payload.pop(\"embedding\", {})\n        if vector and isinstance(vector, np.ndarray):\n            vector = vector.tolist()\n        return PointStruct(id=_id, vector=vector, payload=payload) \n```", "```py\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) -> bool:\n        try:\n            cls._bulk_insert(documents)\n        except exceptions.UnexpectedResponse:\n            logger.info(\n                f\"Collection '{cls.get_collection_name()}' does not exist. Trying to create the collection and reinsert the documents.\"\n            )\n            cls.create_collection()\n            try:\n                cls._bulk_insert(documents)\n            except exceptions.UnexpectedResponse:\n                logger.error(f\"Failed to insert documents in '{cls.get_collection_name()}'.\")\n                return False\n        return True\n    @classmethod\n    def _bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) -> None:\n        points = [doc.to_point() for doc in documents]\n        connection.upsert(collection_name=cls.get_collection_name(), points=points) \n```", "```py\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def get_collection_name(cls: Type[T]) -> str:\n        if not hasattr(cls, \"Config\") or not hasattr(cls.Config, \"name\"):\n            raise ImproperlyConfigured(\n                \"The class should define a Config class with\" \"the 'name' property that reflects the collection's name.\"\n            )\n        return cls.Config.name \n```", "```py\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> tuple[list[T], UUID | None]:\n        try:\n            documents, next_offset = cls._bulk_find(limit=limit, **kwargs)\n        except exceptions.UnexpectedResponse:\n            logger.error(f\"Failed to search documents in '{cls.get_collection_name()}'.\")\n            documents, next_offset = [], None\n        return documents, next_offset\n    @classmethod\n    def _bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> tuple[list[T], UUID | None]:\n        collection_name = cls.get_collection_name()\n        offset = kwargs.pop(\"offset\", None)\n        offset = str(offset) if offset else None\n        records, next_offset = connection.scroll(\n            collection_name=collection_name,\n            limit=limit,\n            with_payload=kwargs.pop(\"with_payload\", True),\n            with_vectors=kwargs.pop(\"with_vectors\", False),\n            offset=offset,\n            **kwargs,\n        )\n        documents = [cls.from_record(record) for record in records]\n        if next_offset is not None:\n            next_offset = UUID(next_offset, version=4)\n        return documents, next_offset \n```", "```py\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def search(cls: Type[T], query_vector: list, limit: int = 10, **kwargs) -> list[T]:\n        try:\n            documents = cls._search(query_vector=query_vector, limit=limit, **kwargs)\n        except exceptions.UnexpectedResponse:\n            logger.error(f\"Failed to search documents in '{cls.get_collection_name()}'.\")\n            documents = []\n        return documents\n    @classmethod\n    def _search(cls: Type[T], query_vector: list, limit: int = 10, **kwargs) -> list[T]:\n        collection_name = cls.get_collection_name()\n        records = connection.search(\n            collection_name=collection_name,\n            query_vector=query_vector,\n            limit=limit,\n            with_payload=kwargs.pop(\"with_payload\", True),\n            with_vectors=kwargs.pop(\"with_vectors\", False),\n            **kwargs,\n        )\n        documents = [cls.from_record(record) for record in records]\n        return documents \n```", "```py\nclass CleaningDispatcher:\n    cleaning_factory = CleaningHandlerFactory()\n    @classmethod\n    def dispatch(cls, data_model: NoSQLBaseDocument) -> VectorBaseDocument:\n        data_category = DataCategory(data_model.get_collection_name())\n        handler = cls.cleaning_factory.create_handler(data_category)\n        clean_model = handler.clean(data_model)\n        logger.info(\n            \"Data cleaned successfully.\",\n            data_category=data_category,\n            cleaned_content_len=len(clean_model.content),\n        )\n        return clean_model \n```", "```py\nclass CleaningHandlerFactory:\n    @staticmethod\n    def create_handler(data_category: DataCategory) -> CleaningDataHandler:\n        if data_category == DataCategory.POSTS:\n            return PostCleaningHandler()\n        elif data_category == DataCategory.ARTICLES:\n            return ArticleCleaningHandler()\n        elif data_category == DataCategory.REPOSITORIES:\n            return RepositoryCleaningHandler()\n        else:\n            raise ValueError(\"Unsupported data type\") \n```", "```py\n… # Other imports.\nfrom typing import Generic, TypeVar\nDocumentT = TypeVar(\"DocumentT\", bound=Document)\nCleanedDocumentT = TypeVar(\"CleanedDocumentT\", bound=CleanedDocument)\nclass CleaningDataHandler(ABC, Generic[DocumentT, CleanedDocumentT]):\n    @abstractmethod\n    def clean(self, data_model: DocumentT) -> CleanedDocumentT:\n        pass \n```", "```py\nclass PostCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: PostDocument) -> CleanedPostDocument:\n        return CleanedPostDocument(\n            id=data_model.id,\n            content=clean_text(\" #### \".join(data_model.content.values())),\n            … # Copy the rest of the parameters from the data_model object.\n        )\nclass ArticleCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: ArticleDocument) -> CleanedArticleDocument:\n        valid_content = [content for content in data_model.content.values() if content]\n        return CleanedArticleDocument(\n            id=data_model.id,\n            content=clean_text(\" #### \".join(valid_content)),\n            platform=data_model.platform,\n            link=data_model.link,\n            author_id=data_model.author_id,\n            author_full_name=data_model.author_full_name,\n        )\nclass RepositoryCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: RepositoryDocument) -> CleanedRepositoryDocument:\n        return CleanedRepositoryDocument(\n            id=data_model.id,\n            content=clean_text(\" #### \".join(data_model.content.values())),\n            … # Copy the rest of the parameters from the data_model object.\n        ) \n```", "```py\n… # Other imports.\nfrom typing import Generic, TypeVar\nCleanedDocumentT = TypeVar(\"CleanedDocumentT\", bound=CleanedDocument)\nChunkT = TypeVar(\"ChunkT\", bound=Chunk)\n class ChunkingDataHandler(ABC, Generic[CleanedDocumentT, ChunkT]):\n    @property\n    def metadata(self) -> dict:\n        return {\n            \"chunk_size\": 500,\n            \"chunk_overlap\": 50,\n        }\n    @abstractmethod\n    def chunk(self, data_model: CleanedDocumentT) -> list[ChunkT]:\n        pass \n```", "```py\nclass ArticleChunkingHandler(ChunkingDataHandler):\n    @property\n    def metadata(self) -> dict:\n        return {\n            \"min_length\": 1000,\n            \"max_length\": 1000,\n        }\n    def chunk(self, data_model: CleanedArticleDocument) -> list[ArticleChunk]:\n        data_models_list = []\n        cleaned_content = data_model.content\n        chunks = chunk_article(\n            cleaned_content, min_length=self.metadata[\"min_length\"], max_length=self.metadata[\"max_length\"]\n        )\n        for chunk in chunks:\n            chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n            model = ArticleChunk(\n                id=UUID(chunk_id, version=4),\n                content=chunk,\n                platform=data_model.platform,\n                link=data_model.link,\n                document_id=data_model.id,\n                author_id=data_model.author_id,\n                author_full_name=data_model.author_full_name,\n                metadata=self.metadata,\n            )\n            data_models_list.append(model)\n        return data_models_list \n```", "```py\ndef chunk_article(text: str, min_length: int, max_length: int) -> list[str]:\n    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", text)\n    extracts = []\n    current_chunk = \"\"\n    for sentence in sentences:\n        sentence = sentence.strip()\n        if not sentence:\n            continue\n        if len(current_chunk) + len(sentence) <= max_length:\n            current_chunk += sentence + \" \"\n        else:\n            if len(current_chunk) >= min_length:\n                extracts.append(current_chunk.strip())\n            current_chunk = sentence + \" \"\n    if len(current_chunk) >= min_length:\n        extracts.append(current_chunk.strip())\n    return extracts \n```", "```py\n    … # Other imports.\n    from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n    from llm_engineering.application.networks import EmbeddingModelSingleton\n    def chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> list[str]:\n        character_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=0)\n        text_split_by_characters = character_splitter.split_text(text)\n        token_splitter = SentenceTransformersTokenTextSplitter(\n            chunk_overlap=chunk_overlap,\n            tokens_per_chunk=embedding_model.max_input_length,\n            model_name=embedding_model.model_id,\n        )\n        chunks_by_tokens = []\n        for section in text_split_by_characters:\n            chunks_by_tokens.extend(token_splitter.split_text(section))\n        return chunks_by_tokens \n    ```", "```py\n… # Other imports.\nfrom typing import Generic, TypeVar, cast\nfrom llm_engineering.application.networks import EmbeddingModelSingleton\nChunkT = TypeVar(\"ChunkT\", bound=Chunk)\nEmbeddedChunkT = TypeVar(\"EmbeddedChunkT\", bound=EmbeddedChunk)\nembedding_model = EmbeddingModelSingleton()\nclass EmbeddingDataHandler(ABC, Generic[ChunkT, EmbeddedChunkT]):\n    \"\"\"\n    Abstract class for all embedding data handlers.\n    All data transformations logic for the embedding step is done here\n    \"\"\"\n    def embed(self, data_model: ChunkT) -> EmbeddedChunkT:\n        return self.embed_batch([data_model])[0]\n    def embed_batch(self, data_model: list[ChunkT]) -> list[EmbeddedChunkT]:\n        embedding_model_input = [data_model.content for data_model in data_model]\n        embeddings = embedding_model(embedding_model_input, to_list=True)\n        embedded_chunk = [\n            self.map_model(data_model, cast(list[float], embedding))\n            for data_model, embedding in zip(data_model, embeddings, strict=False)\n        ]\n        return embedded_chunk\n    @abstractmethod\n    def map_model(self, data_model: ChunkT, embedding: list[float]) -> EmbeddedChunkT:\n        pass \n```", "```py\nclass ArticleEmbeddingHandler(EmbeddingDataHandler):\n    def map_model(self, data_model: ArticleChunk, embedding: list[float]) -> EmbeddedArticleChunk:\n        return EmbeddedArticleChunk(\n            id=data_model.id,\n            content=data_model.content,\n            embedding=embedding,\n            platform=data_model.platform,\n            link=data_model.link,\n            document_id=data_model.document_id,\n            author_id=data_model.author_id,\n            author_full_name=data_model.author_full_name,\n            metadata={\n                \"embedding_model_id\": embedding_model.model_id,\n                \"embedding_size\": embedding_model.embedding_size,\n                \"max_input_length\": embedding_model.max_input_length,\n            },\n        ) \n```", "```py\nfrom sentence_transformers.SentenceTransformer import SentenceTransformer\nfrom llm_engineering.settings import settings\nfrom .base import SingletonMeta\nclass EmbeddingModelSingleton(metaclass=SingletonMeta):\n    def __init__(\n        self,\n        model_id: str = settings.TEXT_EMBEDDING_MODEL_ID,\n        device: str = settings.RAG_MODEL_DEVICE,\n        cache_dir: Optional[Path] = None,\n    ) -> None:\n        self._model_id = model_id\n        self._device = device\n        self._model = SentenceTransformer(\n            self._model_id,\n            device=self._device,\n            cache_folder=str(cache_dir) if cache_dir else None,\n        )\n        self._model.eval()\n    @property\n    def model_id(self) -> str:\n        return self._model_id\n    @cached_property\n    def embedding_size(self) -> int:\n        dummy_embedding = self._model.encode(\"\")\n        return dummy_embedding.shape[0]\n    @property\n    def max_input_length(self) -> int:\n        return self._model.max_seq_length\n    @property\n    def tokenizer(self) -> AutoTokenizer:\n        return self._model.tokenizer\n    def __call__(\n        self, input_text: str | list[str], to_list: bool = True\n    ) -> NDArray[np.float32] | list[float] | list[list[float]]:\n        try:\n            embeddings = self._model.encode(input_text)\n        except Exception:\n            logger.error(f\"Error generating embeddings for {self._model_id=} and {input_text=}\")\n            return [] if to_list else np.array([])\n        if to_list:\n            embeddings = embeddings.tolist()\n        return embeddings \n```"]