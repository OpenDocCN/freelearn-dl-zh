- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Addressing Ethical Considerations and Charting a Path Toward Trustworthy Generative
    AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As generative AI advances, it will extend beyond basic language tasks, integrating
    into daily life and impacting almost every sector. The inevitability of its widespread
    adoption highlights the need to address its ethical implications. The promise
    of this technology to revolutionize industries, enhance creativity, and solve
    complex problems must be coupled with the responsibility to navigate its ethical
    landscape diligently. This chapter will explore these ethical considerations,
    dissect the intricacies of biases entangled in these models, and look at strategies
    for cultivating trust in general-purpose AI systems. Through thorough examination
    and reflection, we can begin to outline a path toward responsible use, helping
    to ensure that advancements in generative AI are leveraged for the greater good
    while minimizing harm.
  prefs: []
  type: TYPE_NORMAL
- en: To ground our discussion, we will first identify some ethical norms and universal
    values relevant to generative AI. While this chapter cannot be exhaustive, it
    aims to introduce key ethical considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical norms and values in the context of generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ethical norms and values guiding the development and deployment of generative
    AI are rooted in transparency, equity, accountability, privacy, consent, security,
    and inclusivity. These principles can serve as a foundation for developing and
    adopting systems aligned with societal values and supporting the greater good.
    Let’s explore these in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transparency** involves clearly explaining the methodologies, data sources,
    and processes behind large language model (LLM) construction. This practice builds
    trust by enabling stakeholders to understand the technology’s reliability and
    limits. For example, a company could publish a detailed report on the types of
    data trained on their LLM and the steps taken to ensure data privacy and bias
    mitigation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equity** in the context of LLMs ensures fair treatment and outcomes for all
    users by actively preventing biases in models. This requires thorough analysis
    and correction of training data and continuous monitoring of exchanges to reduce
    discrimination. One measure a firm might apply is a routine review of LLM performance
    across various demographic groups to identify and address unintended biases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accountability** establishes that developers and users of LLMs are responsible
    for model outputs and impacts. It includes transparent and accessible mechanisms
    for reporting and addressing negative consequences or ethical violations. In practice,
    this could manifest as the establishment of an independent review board that oversees
    AI projects and intervenes in cases of ethical misconduct.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy and consent**, in principle, involves ensuring that individual privacy
    and consent are respected and preserved during the use of personal data as input
    to LLMs. In practice, developers should avoid using personal data for training
    without explicit permission and implement strong data protection measures. For
    example, a developer might use data anonymization or privacy-preserving techniques
    to train models, ensuring that personal identifiers and sensitive information
    are removed before data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security** involves protecting LLM-integrated systems and their data from
    unauthorized access and cyber threats. In practice, setting up LLM-specific red
    teams (or teams that test defenses by simulating attacks) can help safeguard AI
    systems against potential breaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inclusivity** involves the deliberate effort to include diverse voices and
    perspectives in the development process of LLMs, ensuring the technology is accessible
    and beneficial to a broad spectrum of users. In practice, it is vital to collaborate
    with socio-technical subject-matter experts who can guide appropriate actions
    to promote and preserve inclusion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This set of principles is not comprehensive but may help to form a conceptual
    foundation for ethical LLM development and adoption with the universal goal of
    advancing the technology in ways that avoid harm.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, various leading authorities have published guidance regarding
    responsible AI, inclusive of ethical implications. These include the US Department
    of Commerce’s **National Institute of Standards and Technology** (**NIST**), Stanford
    University’s **Institute for Human-Centered Artificial Intelligence** (**HAI**),
    and the **Distributed AI Research Institute** (**DAIR**), to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating and minimizing bias in generative LLMs and generative image models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias in generative AI models, including both LLMs and generative image models,
    is a complex issue that requires careful investigation and mitigation strategies.
    Bias can manifest as unintended stereotypes, inaccuracies, and exclusions in the
    generated outputs, often stemming from biased datasets and model architectures.
    Recognizing and addressing these biases is crucial to creating equitable and trustworthy
    AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, algorithmic or model bias refers to systematic errors that lead
    to preferential treatment or unfair outcomes for certain groups. In generative
    AI, this can appear as gender, racial, or socioeconomic biases in outputs, often
    mirroring societal stereotypes. For example, an LLM may produce content that reinforces
    these biases, reflecting the historical and societal biases present in its training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Let us again revisit our hypothetical fashion retailer, StyleSprint. Consider
    a situation where StyleSprint experimented with using a multimodal generative
    LLM model to generate promotional images and captions for its latest sneaker line.
    It finds that the model predominantly generates sneakers in urban, graffiti-laden
    backgrounds, unintentionally drawing an association that relies on stereotypes.
    Moreover, the team begins noticing that the captions are also laden with language
    that perpetuates stereotypes. This realization prompts a reevaluation of the imagery
    and text, first with an investigation of how the problem surfaced.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating bias involves various techniques, from analyzing the diversity
    and representativeness of training datasets to implementing testing protocols
    that specifically look for biased outputs across different demographics and scenarios.
    Statistical analysis can reveal disparities in model outcomes, while comparative
    studies and user feedback can help identify biases in the generated content.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, let us assume that StyleSprint was using an LLM-provider without
    the ability to influence its training data or development process. To mitigate
    the risk of bias, the team might employ the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Post-processing adjustments to diversify the imagery, ensuring a broader representation
    of backgrounds that resonate with its customer base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The institution of a manual review process, enlisting team members to scrutinize
    and curate AI-generated images and captions before publishing (i.e., “human-in-the-loop”),
    ensuring that every piece of content aligns with the brand’s commitment to diversity
    and inclusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As is true for other kinds of evaluation of generative AI, evaluating bias demands
    both quantitative and qualitative methods. Statistical analysis can uncover performance
    disparities across groups, and comparative studies can detect biases in outputs.
    Gathering feedback from diverse users aids the understanding of real-world bias
    impacts, while independent audits and research are essential for identifying issues
    that internal evaluations may miss.
  prefs: []
  type: TYPE_NORMAL
- en: With a better understanding of how we might investigate and evaluate model outcomes
    for societal bias, we can explore technical methods for guiding model outcomes
    toward reliability, equity, and general trustworthiness to curb biased or inequitable
    outcomes during inference.
  prefs: []
  type: TYPE_NORMAL
- en: Constrained generation and eliciting trustworthy outcomes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, it is possible to constrain model generation and guide outcomes
    toward factuality and equitable outcomes. As discussed, guiding models toward
    trustworthy outcomes can be done through continued training and fine-tuning, or
    during inference. For example, methodologies such as **reinforcement learning
    from human feedback** (**RLHF**) and **direct preference optimization** (**DPO**)
    increasingly refine model outputs to align model outcomes with human judgment.
    Additionally, as discussed in [*Chapter 7*](B21773_07.xhtml#_idTextAnchor225),
    various grounding techniques help to ensure that model outputs reflect verified
    data, continuously guiding the model toward responsible and accurate content generation.
  prefs: []
  type: TYPE_NORMAL
- en: Constrained generation with fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refinement strategies such as RLHF integrate human judgments into the model
    training process, steering the AI toward behavior that aligns with ethical and
    truthful standards. By incorporating human feedback loops, RLHF ensures that the
    AI’s outputs meet technical accuracy and societal norms.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, DPO refines model outputs based on explicit human preferences, providing
    precise control to ensure outcomes adhere to ethical standards and human values.
    This technique exemplifies the shift toward more ethically aligned content generation
    by directly incorporating human values into the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: Constrained generation through prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discovered in [*Chapter 7*](B21773_07.xhtml#_idTextAnchor225), we can
    guide model responses by grounding the LLM with factual information. This can
    be achieved directly using the context window or retrieval approach (e.g., Retrieval
    Augmented Generation (RAG)). Just as we can apply these methods to induce factual
    responses, we can apply the same technique to guide the model toward equitable
    and inclusive outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider an online news outlet looking to use an LLM to review
    article content for grammar and readability. The model does an excellent job of
    reviewing and revising its drafts. However, during peer review, it realizes some
    of the language is culturally insensitive or lacks inclusivity. As discussed,
    qualitative evaluation and human oversight are critical to ensuring that model
    output aligns with human judgment. Notwithstanding, the writing team can guide
    the model toward alignment with company values using a set of general guidelines
    for inclusive and debiased language. For example, it could ground the model with
    excerpts from its internal policy documents or content from its unconscious bias
    training guides.
  prefs: []
  type: TYPE_NORMAL
- en: Employing methodologies such as RLHF and DPO, alongside grounding techniques,
    ensures that LLMs generate content that is not only factual but also ethically
    aligned, demonstrating the potential of generative AI to adhere to high standards
    of truthfulness and inclusivity. Although we cannot underestimate or deemphasize
    the importance of human judgment in shaping model outputs, we can apply practical
    supplemental methods such as grounding to reduce the likelihood of harmful or
    biased model outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll explore the risks and ethical dilemmas posed by attempts
    to circumvent the constraints we have just discussed, highlighting the ongoing
    challenge of balancing the rapid adoption of generative LLMs with appropriate
    safeguards against misuse.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding jailbreaking and harmful behaviors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of generative LLMs, the term **jailbreaking** describes techniques
    and strategies that intend to manipulate models to override any ethical safeguards
    or content restrictions, thereby enabling the generation of restricted or harmful
    content. Jailbreaking exploits models through sophisticated adversarial prompting
    that can induce unexpected or harmful responses. For example, an attacker might
    try to instruct an LLM to explain how to generate explicit content or express
    discriminatory views. Understanding this susceptibility is crucial for developers
    and stakeholders to safeguard applied generative AI against misuse and minimize
    potential harm.
  prefs: []
  type: TYPE_NORMAL
- en: These jailbreaking attacks exploit the fact that LLMs are trained to interpret
    and respond to instructions. Despite sophisticated efforts to defend against misuse,
    attackers can take advantage of the complex and expansive knowledge embedded in
    LLMs to find gaps in their safety precautions. In particular, models that have
    been trained on uncurated datasets are the most susceptible, as the universe of
    possible outputs that the models sample from can include harmful and toxic content.
    Moreover, LLMs are multilingual and can accept various encodings as input. For
    example, an encoding such as **base64**, which can be used to translate plain
    text into binary format, could be applied to obfuscate a harmful instruction.
    In this case, safety filters may perform inconsistently, failing to detect some
    languages or alternative inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite this inherent weakness in LLMs, developers and practitioners can take
    several practical steps to mitigate jailbreaking risks. Remember, these cannot
    be exhaustive as new adversarial techniques are often uncovered:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing and safety filtering**: Implement robust content filtering
    to detect and block unsafe semantic patterns across languages and input types.
    For example, a firm might apply machine learning techniques to analyze prompts
    for adversarial patterns and block suspicious inputs before passing them to the
    LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Postprocessing and output screening**: Apply a specialized classifier or
    other sophisticated technique to screen LLM outputs for inappropriate content
    before returning them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety-focused fine-tuning**: Provide additional safety-focused fine-tuning
    to the LLM to reinforce and expand its safety knowledge. Focus on known jailbreaking
    tactics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and iterating**: Actively monitor for jailbreaking or policy violation
    attempts in production, analyze them to identify gaps, and continually update
    defense measures to stay ahead of creative attackers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While eliminating all possible jailbreaking attempts is infeasible, a multi-layered
    defense and operational best practices can significantly mitigate the risk.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will apply a real-time defense mechanism for jailbreaking,
    all while reducing the likelihood of biased and harmful output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Practice project: Minimizing harmful behaviors with filtering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this project, we will use response filtering to try to minimize misuse and
    curb unwanted LLM output. Again, we’ll consider our hypothetical business, StyleSprint.
    After successfully using an LLM to generate product descriptions and fine-tuning
    it to answer FAQs, StyleSprint now wants to attempt to use a general-purpose LLM
    (without fine-tuning) to refine its website search. However, giving its customers
    direct access to the LLM poses the risk of misuse. Bad actors may attempt to use
    the LLM search to produce harmful content with the intention of harming StyleSprint’s
    reputation. To prevent this behavior, we can revisit our RAG implementation from
    [*Chapter 7*](B21773_07.xhtml#_idTextAnchor225), applying a filter that evaluates
    whether queries deviate from the appropriate use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reusing our previous implementation from the last chapter (found in the GitHub
    repository: [https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)),
    which applied RAG to answer specific product-related questions, we can evaluate
    how the model would respond to questions outside the desired scope. Recall that
    RAG is simply a vector search engine combined with an LLM to produce coherent
    and more precise responses, contextualized by a specific data source. We will
    directly reuse that implementation and the same product data for simplicity, but
    this time, we’ll input a completely unrelated query instead of asking about products:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the model did not attempt to constrain its answer to the contents
    of the search index. It returned an answer based on its vast training. This is
    precisely the behavior we want to avoid. Imagine that a bad actor induced the
    model to produce explicit content or some other unwanted output. Moreover, consider
    a sophisticated attacker that could induce the model to leak training data or
    expose sensitive information accidentally memorized during training procedures
    (Carlini et al., 2018; Hu et al., 2022). In either case, StyleSprint could face
    material risk and exposure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent this, we can leverage a filter to constrain the output to provide
    answers relevant to a given question explicitly. The implementation is already
    built into the LlamaIndex RAG interface. It is a feature they call Structured
    Answer Filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: With structured_answer_filtering set to True, our refine module is able to filter
    out any input nodes that are not relevant to the question being asked. This is
    particularly useful for RAG-based Q&A systems that involve retrieving chunks of
    text from external vector store for a given user query. (LlamaIndex)
  prefs: []
  type: TYPE_NORMAL
- en: In short, this functionality gives us fine-grained control to restrict the context
    we provide to the LLM for synthesis, ensuring that only the most relevant results
    are included. Filtering out irrelevant content before synthesizing responses ensures
    that only information related to the user’s question is used. This approach helps
    avoid answers that are off-topic or outside the intended subject matter. We can
    quickly reimplement our RAG approach, applying minor changes that enable the feature.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This functionality is most reliable when using an LLM that can support function
    calling.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how this functionality can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this approach, the model returns a response to the standard question
    but no response to the irrelevant question. In fact, we can take this further
    and compound this filtering with additional instructions in the prompt template.
    For example, if we revise `response_synthesizer`, we can promote a stricter response
    from the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This time, the model responded explicitly, `I cannot answer`. Using a prompt
    template, StyleSprint could return a message it deems appropriate in response
    to inputs unrelated to the search index and, as a side effect, ignore queries
    that do not adhere to its policies. Although not entirely a perfect solution,
    combining RAG with more strict answer filtering can help deter or defend against
    harmful instructions or adversarial prompting. Additionally, as explored in [*Chapter
    7*](B21773_07.xhtml#_idTextAnchor225), we can apply RAG-specific evaluation techniques
    such as RAGAS to measure factual consistency and answer relevancy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we recognized the increasing prominence of generative AI and
    explored the ethical considerations that should steer its progress. We outlined
    key concepts such as transparency, fairness, accountability, respect for privacy,
    informed consent, security, and inclusivity, which are essential to the responsible
    development and use of these technologies.
  prefs: []
  type: TYPE_NORMAL
- en: We reviewed strategies to attempt to counter these biases, including human-aligned
    training techniques and practical application-level measures against susceptibilities
    such as jailbreaking. In sum, we explored a multidimensional and human-centered
    approach to generative AI adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Having completed our foundational exploration of generative AI, we can now reflect
    on our journey. We began by laying the groundwork, examining foundational generative
    architectures such as generative adversarial networks (GANs), diffusion models,
    and transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapters 2* and *3* guided us through the evolution of language models, with
    a particular focus on autoregressive transformers. We explored how these models
    have significantly advanced the capabilities of generative AI, pushing the boundaries
    of machine understanding and the generation of human-like language.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B21773_04.xhtml#_idTextAnchor123) provided us with practical
    experience in production-ready environments. In [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180),
    we explored the fine-tuning of LLMs for specific tasks, a technique that enhances
    their performance and adaptability to specific applications. [*Chapter 6*](B21773_06.xhtml#_idTextAnchor211)
    focused on the concept of domain adaptation, demonstrating how tailoring AI models
    to understand domain-specific nuances can greatly improve their utility in specialized
    fields such as finance, law, and healthcare.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapters 7* and *8* centered on prompt engineering and constrained generation,
    addressing techniques to ensure that AI-generated content remains trustworthy
    and aligned with ethical guidelines.'
  prefs: []
  type: TYPE_NORMAL
- en: This book has aimed to provide a solid foundation in generative AI, preparing
    professionals across disciplines and sectors with the necessary theoretical knowledge
    and practical skills to effectively engage with this transformative technology.
    The potential of generative AI is significant, and with our deeper understanding
    of its technologies, coupled with a thoughtful approach to ethical and societal
    considerations, we are ready to responsibly leverage its advantages.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y., Lyu, W.,
    Zhang, Y., Li, X., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Kailkhura, B., Xiong,
    C., Xiao, C., Li, C., Xing, E., . . . Zhao, Y. (2024). *TrustLLM: Trustworthiness
    in Large Language Models*. *ArXiv*. /abs/2401.05561'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., & Song, D. (2018). *The secret
    sharer: Evaluating and testing unintended memorization in neural networks*. In
    arXiv [cs.LG]. [http://arxiv.org/abs/1802.08232](http://arxiv.org/abs/1802.08232)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu, H., Salcic, Z., Sun, L., Dobbie, G., Yu, P. S., & Zhang, X. (2022). *Membership
    inference attacks on machine learning: A survey. ACM Computing Surveys*, 54(11s),
    1–37\. [https://doi.org/10.1145/3523273](https://doi.org/10.1145/3523273)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LlamaIndex. (n.d.). *Response synthesizers. In LlamaIndex Documentation (stable
    version)*. Retrieved March 12, 2024\. [https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
