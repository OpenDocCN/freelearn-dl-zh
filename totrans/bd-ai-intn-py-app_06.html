<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-99"><a id="_idTextAnchor137"/><a id="_idTextAnchor138"/>6</h1>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor139"/>AI/ML Application Design</h1>
			<p>As the landscape of intelligent applications evolves, their architectural design becomes pivotal for efficiency, scalability, operability, and security. This chapter provides a guide on key topics to consider as you embark on creating robust and responsive AI/ML applications.</p>
			<p>The chapter begins with <strong class="bold">data modeling</strong>, examining how to organize data in a way that maximizes effectiveness for three different consumers: humans, applications, and AI models. You will learn about <strong class="bold">data storage</strong>, considering the impact of different data types and determining the best storage technology. You will estimate storage needs and determine the best MongoDB Atlas cluster configuration for your example application.</p>
			<p>As you learn about <strong class="bold">data flow</strong>, you will explore the detailed movement of data through ingestion, processing, and output to maintain integrity and velocity. This chapter also addresses <strong class="bold">data lifecycle management</strong>, including updates, aging, and retention, ensuring that data remains relevant and compliant.</p>
			<p>Security concerns are stretched further for AI/ML applications due to the risks of exposing data or logic to AI models. This chapter discusses security measures and <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) to protect sensitive data and logic integrity. You will also learn the best principles for data storage, flow, modeling, and security, providing practical advice to avoid common pitfalls.</p>
			<p>Throughout the chapter, you will use a fictitious news application called <strong class="bold">MongoDB Developer News</strong> (<strong class="bold">MDN</strong>), which will be like Medium.com, equipping you to create intelligent applications by using a practical example.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>Data modeling</li>
				<li>Data storage</li>
				<li>Data flow</li>
				<li>Freshness and retention</li>
				<li>Security and RBAC</li>
				<li>Best practices</li>
			</ul>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor140"/>Technical requirements</h1>
			<p>The following are the prerequisites to follow along with the code in this chapter:</p>
			<ul>
				<li>A MongoDB Atlas cluster <code>M0</code> tier (free) should be sufficient</li>
				<li>An OpenAI account and API key with access to the <code>text-embedding-3-large</code> model</li>
				<li>A Python 3 working environment</li>
				<li>Installed Python libraries for MongoDB, LangChain, and OpenAI</li>
				<li>Atlas Search indexes and Vector Search indexes created on the MongoDB Atlas cluster</li>
			</ul>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor141"/>Data modeling</h1>
			<p>This section delves into the diverse types of data required by AI/ML systems, including structured, unstructured, and semi-structured data, and how these are applied to MDN’s news articles. The following are short descriptions of each to set a basic understanding:</p>
			<ul>
				<li><strong class="bold">Structured data</strong> conforms to a predefined schema and is traditionally stored in relational databases for transactional information. It powers systems of engagement and intelligence.</li>
				<li><strong class="bold">Unstructured data</strong> includes binary assets, such as PDFs, images, videos, and others. Object stores such as Amazon S3 allow storing these under a flexible directory structure at a lower cost.</li>
				<li><strong class="bold">Semi-structured data</strong>, such as JSON documents, allow each document to define its schema, accommodating both common and unique data points, or even the absence of some data.</li>
			</ul>
			<p>MDN will store news articles, subscriber profiles, billing information, and more. For simplicity, in this chapter, you will focus on the data about each news article and related binary content (which would be images). <em class="italic">Figure 6</em><em class="italic">.1</em> describes the data model of the <code>articles</code> collection.</p>
			<div><div><img alt="" role="presentation" src="img/B22495_06_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1: Schema for the articles collection</p>
			<p>The articles collection represents a news article with metadata, including creation details, tags, and contributors. All documents feature a title, summary, body content in HTML and plain text, and associated media elements such as images.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor142"/>Enriching data with embeddings</h2>
			<p>To complete the MDN data model, you need to consider data that will also be represented and stored via embeddings. <strong class="bold">Text embeddings</strong> for article titles and summaries will enable semantic search, while <strong class="bold">image embeddings</strong> will help find similar artwork used across articles. <em class="italic">Table 6.1</em> describes the data fields, embedding models to use, and their vector sizes.</p>
			<table class="No-Table-Style" id="table001-4">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Type</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Field(s)</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Embedding model</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Vector size</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Text</p>
						</td>
						<td class="No-Table-Style">
							<p><code>title</code></p>
						</td>
						<td class="No-Table-Style" rowspan="2">
							<p>OpenAI <code>text-embedding-3-large</code></p>
						</td>
						<td class="No-Table-Style" rowspan="2">
							<p>1,024</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Text</p>
						</td>
						<td class="No-Table-Style">
							<p><code>summary</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Image</p>
						</td>
						<td class="No-Table-Style">
							<p><code>contents</code></p>
						</td>
						<td class="No-Table-Style">
							<p>OpenAI CLIP</p>
						</td>
						<td class="No-Table-Style">
							<p>768</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1: Embeddings for the articles collection</p>
			<p>Each article has a title and summary. Instead of embedding them separately, you will concatenate them and create one text embedding for simplicity. Ideally, for images, you would store the embedding with each content object in the <code>contents</code> array. However, support for fields inside arrays of objects for vector indexes is not available today in MongoDB Atlas and leads to the <strong class="bold">anti-pattern of bloated documents</strong>. The best practice is to store image embeddings in a separate collection and use the <strong class="bold">extended reference schema design pattern</strong>. You can learn more about indexing arrays with MongoDB, bloated documents, and the extended reference pattern from the links given in the <em class="italic">Further Reading</em> chapter of this book. <em class="italic">Figure 6</em><em class="italic">.2</em> shows the updated data model.</p>
			<div><div><img alt="" role="presentation" src="img/B22495_06_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2: Schema for articles with embeddings</p>
			<p><em class="italic">Table 6.2</em> shows the corresponding vector indexes.</p>
			<table class="No-Table-Style _idGenTablePara-1" id="table002-2">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>articles</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>article_content_embeddings</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>semantic_embedding_vix</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>content_embedding_vix</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<pre class="source-code">
{</pre>							<pre class="source-code">
  "fields": [</pre>							<pre class="source-code">
	{</pre>							<pre class="source-code">
  	"numDimensions": 1024,</pre>							<pre class="source-code">
  	"path": "semantic_embedding",</pre>							<pre class="source-code">
  	"similarity": "cosine",</pre>							<pre class="source-code">
  	"type": "vector"</pre>							<pre class="source-code">
	}</pre>							<pre class="source-code">
  ]</pre>							<pre class="source-code">
}</pre>						</td>
						<td class="No-Table-Style">
							<pre class="source-code">
{</pre>							<pre class="source-code">
  "fields": [</pre>							<pre class="source-code">
	{</pre>							<pre class="source-code">
  	"numDimensions": 768,</pre>							<pre class="source-code">
  	"path": "content_embedding",</pre>							<pre class="source-code">
  	"similarity": "cosine",</pre>							<pre class="source-code">
  	"type": "vector"</pre>							<pre class="source-code">
	}</pre>							<pre class="source-code">
  ]</pre>							<pre class="source-code">
}</pre>						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.2: Vector search index definitions</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor143"/>Considering search use cases</h2>
			<p>Before finalizing the data model, let’s consider search use cases for articles, and adapt the model once more. Here are some broader search use cases:</p>
			<ul>
				<li><code>title</code> and <code>summary</code> fields for text search, and the <code>brand</code> and <code>subscription_type</code> fields for filtering.</li>
				<li><code>tags</code> field. You will also need a vector search index to cover the <code>title + </code><code>summary</code> embedding.</li>
				<li><code>_id</code>, <code>brand</code>, and <code>subscription_type</code> fields from the <code>articles</code> collection into the <code>article_content_embeddings</code> collection. Since there is already an <code>_id</code> field in this collection, you can create a composite primary key that includes the <code>_id</code> of the article and the <code>_id</code> of the content. <em class="italic">Figure 6</em><em class="italic">.3</em> shows the updated data model.</li>
			</ul>
			<div><div><img alt="" role="presentation" src="img/B22495_06_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3: Updated schema for articles with embeddings</p>
			<p><em class="italic">Table 6.3</em> shows updated vector indexes.</p>
			<table class="No-Table-Style _idGenTablePara-1" id="table003">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>articles</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>article_content_embeddings</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>semantic_embedding_vix</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>content_embedding_vix</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<pre class="source-code">
{</pre>							<pre class="source-code">
  "fields": [</pre>							<pre class="source-code">
    {</pre>							<pre class="source-code">
      "numDimensions": 1024,</pre>							<pre class="source-code">
      "path": "semantic_embedding",</pre>							<pre class="source-code">
      "similarity": "cosine",</pre>							<pre class="source-code">
      "type": "vector"</pre>							<pre class="source-code">
    },</pre>							<pre class="source-code">
    {</pre>							<pre class="source-code">
      "path": "brand",</pre>							<pre class="source-code">
      "type": "filter"</pre>							<pre class="source-code">
    },</pre>							<pre class="source-code">
    {</pre>							<pre class="source-code">
      "path": "subscription_type",</pre>							<pre class="source-code">
      "type": "filter"</pre>							<pre class="source-code">
    }</pre>							<pre class="source-code">
  ]</pre>							<pre class="source-code">
}</pre>						</td>
						<td class="No-Table-Style">
							<pre class="source-code">
{</pre>							<pre class="source-code">
  "fields": [</pre>							<pre class="source-code">
    {</pre>							<pre class="source-code">
      "numDimensions": 768,</pre>							<pre class="source-code">
      "path": "content_embedding",</pre>							<pre class="source-code">
      "similarity": "cosine",</pre>							<pre class="source-code">
      "type": "vector"</pre>							<pre class="source-code">
    },</pre>							<pre class="source-code">
    {</pre>							<pre class="source-code">
      "path": "brand",</pre>							<pre class="source-code">
      "type": "filter"</pre>							<pre class="source-code">
    },</pre>							<pre class="source-code">
    {</pre>							<pre class="source-code">
      "path": "subscription_type",</pre>							<pre class="source-code">
      "type": "filter"</pre>							<pre class="source-code">
    },</pre>							<pre class="source-code">
    {</pre>							<pre class="source-code">
      "path": "_id.article_id",</pre>							<pre class="source-code">
      "type": "filter"</pre>							<pre class="source-code">
    }</pre>							<pre class="source-code">
  ]</pre>							<pre class="source-code">
}</pre>						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.3: Updated vector search index definitions</p>
			<p><em class="italic">Table 6.4</em> shows the new text search index.</p>
			<table class="No-Table-Style _idGenTablePara-1" id="table004">
				<colgroup>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>articles</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>lexical_six</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<pre class="source-code">
{</pre>							<pre class="source-code">
  "mappings": {</pre>							<pre class="source-code">
    "dynamic": false,</pre>							<pre class="source-code">
    "fields": {</pre>							<pre class="source-code">
      "brand": {</pre>							<pre class="source-code">
        "normalizer": "lowercase",</pre>							<pre class="source-code">
        "type": "token"</pre>							<pre class="source-code">
      },</pre>							<pre class="source-code">
      "subscription_type": {</pre>							<pre class="source-code">
        "normalizer": "lowercase",</pre>							<pre class="source-code">
        "type": "token"</pre>							<pre class="source-code">
      },</pre>							<pre class="source-code">
      "summary": {</pre>							<pre class="source-code">
        "type": "string"</pre>							<pre class="source-code">
      },</pre>							<pre class="source-code">
      "tags": {</pre>							<pre class="source-code">
        "normalizer": "lowercase",</pre>							<pre class="source-code">
        "type": "token"</pre>							<pre class="source-code">
      },</pre>							<pre class="source-code">
      "title": {</pre>							<pre class="source-code">
        "type": "string"</pre>							<pre class="source-code">
      }</pre>							<pre class="source-code">
    }</pre>							<pre class="source-code">
  }</pre>							<pre class="source-code">
}</pre>						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.4: Text search index definition</p>
			<p>You learned about writing vector search queries in <a href="B22495_04.xhtml#_idTextAnchor061"><em class="italic">Chapter 4</em></a><em class="italic">, Embedding Models</em>. To learn more about hybrid search queries, you can refer to the tutorial at <a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/">https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/</a>.</p>
			<p>Now that you understand your data model and the indexes required, you need to consider the number of articles MDN will bear (including the sizes of embeddings and indexes), peak daily times, and more to determine the overall storage and database cluster requirements.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor144"/>Data storage</h1>
			<p>In this section, you will perform sizing, which is an educated estimate, for storage requirements. You will consider not just volume size and speed, but also several other aspects of the database cluster that are needed for harnessing the data of your application while following expected data access patterns.</p>
			<p>MDN plans to publish 100 articles daily. Keeping the articles from the last 5 years, the number of articles would total 182,500. With 48 million subscribers and 24 million daily active users, peak access occurs for 30 minutes daily across three major time zones, as shown in <em class="italic">Figure 6</em><em class="italic">.4</em>.</p>
			<div><div><img alt="" role="presentation" src="img/B22495_06_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4: MDN subscriber time zones and peak times</p>
			<p>First, you will estimate the total data size. Each article has one 1,024-dimension embedding for semantic search and five 768-dimension embeddings for image search, totaling 40 KB uncompressed (dimensions use the double type). With the <code>title</code>, <code>summary</code>, <code>body</code> (with and without markup), and other fields, the average article size will be about 300 KB uncompressed.</p>
			<p>Five years of articles will require about 100 GB uncompressed. With MongoDB’s <strong class="bold">WiredTiger </strong>compression (Snappy, zlib, and zstd are also available as compression options), this reduces to about 50 GB on disk. The defined vector indexes add about 3.6 GB. Images and binary assets will be stored in Amazon S3. For simplicity, you will not estimate the size of search and traditional indexes. You can safely say that MDN will need 80 to 100 GB on disk in MongoDB Atlas, which is very manageable by today’s cloud computing standards.</p>
			<p>Now, you will determine the most suitable MongoDB Atlas cluster configuration.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor145"/>Determining the type of database cluster</h2>
			<p>MongoDB Atlas provides two main cluster types:</p>
			<ul>
				<li><strong class="bold">Replica sets</strong> have a primary node for writes and secondary nodes for high availability, which can also be used for reads. These sets scale vertically and can also scale horizontally for reads by adding more nodes in the same or different cloud regions.</li>
				<li><strong class="bold">Sharded clusters</strong> consist of multiple shards, each being a part of the overall dataset, and each being a replica set. They scale vertically and horizontally for both reads and writes. Shards can be placed in different cloud regions to enhance data locality and compliance.</li>
			</ul>
			<p>So, how can you determine whether a replica set is sufficient or a sharded cluster is needed? Key factors include the size of the dataset or the throughput of applications that can challenge the capacity of a single server. For example, high query rates can exhaust the server’s CPU capacity and working set sizes larger than the system’s RAM can stress the I/O capacity of disk drives. MDN publishes 100 articles per day, so sharding is not necessary for this reason.</p>
			<p>Other reasons for sharding include data governance and compliance and <strong class="bold">recovery point objective</strong> (<strong class="bold">RPO</strong>) and <strong class="bold">recovery time objective</strong> (<strong class="bold">RTO</strong>) policies, which are key metrics in disaster recovery and business continuity planning. None of these are applicable to MDN.</p>
			<p>Considering the small number of writes per second and manageable data size, it makes sense to use a replica set. You will now need to determine the amount of RAM and IOPS needed; both are key components for fast response times.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor146"/>Determining IOPS</h2>
			<p>MDN is a low-write, high-read use case. With only 100 articles added per day, there is minimal pressure on the storage system for writes. <em class="italic">Table 6.5</em> shows the storage and IOPS options provided by MongoDB Atlas.</p>
			<table class="No-Table-Style _idGenTablePara-1" id="table005">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Storage types</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Lowest IOPS/storage</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Highest IOPS/storage</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Standard IOPS</p>
						</td>
						<td class="No-Table-Style">
							<p>3,000 IOPS/10 GB</p>
						</td>
						<td class="No-Table-Style">
							<p>12,288 IOPS/4 TB
16,000 IOPS/14 TB*</p>
							<p>*Extended storage enabled</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Provisioned IOPS</p>
						</td>
						<td class="No-Table-Style">
							<p>100 IOPS/10 GB</p>
						</td>
						<td class="No-Table-Style">
							<p>64,000 IOPS/4 TB</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>NVMe</p>
						</td>
						<td class="No-Table-Style">
							<p>100,125 100% random read IOPS</p>
							<p>35,000 write IOPS 380 GB</p>
						</td>
						<td class="No-Table-Style">
							<p>3,300,000 100% random read IOPS</p>
							<p>1,400,000 write IOPS 4,000 GB</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.5: MongoDB Atlas storage types on AWS</p>
			<p>As shown in <em class="italic">Figure 6</em><em class="italic">.4</em>, there will be a 30-minutes peak period, during which 24 million users are expected to be active daily. So, you need to provision 6,000 IOPS, as shown in <em class="italic">Table 6.6</em>. This is based on subscriber distribution, memory versus disk reads, and each article requiring 3 IOPS for disk reads (150 KB compressed ÷ 64 KB I/O size of Amazon EBS).</p>
			<table class="No-Table-Style _idGenTablePara-1" id="table006">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Region</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Allocation</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">DAU</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">20% reads </strong><strong class="bold">from disk</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Disk reads/sec during </strong><strong class="bold">peak time</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">IOPS required</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>AMER</code>^</p>
						</td>
						<td class="No-Table-Style">
							<p>40%</p>
						</td>
						<td class="No-Table-Style">
							<p>9,600,000</p>
						</td>
						<td class="No-Table-Style">
							<p>1,920,000</p>
						</td>
						<td class="No-Table-Style">
							<p>1,067</p>
						</td>
						<td class="No-Table-Style">
							<p>3,200^</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>EMEA</code>^</p>
						</td>
						<td class="No-Table-Style">
							<p>20%</p>
						</td>
						<td class="No-Table-Style">
							<p>4,800,000</p>
						</td>
						<td class="No-Table-Style">
							<p>960,000</p>
						</td>
						<td class="No-Table-Style">
							<p>533</p>
						</td>
						<td class="No-Table-Style">
							<p>1,600^</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>APAC</code></p>
						</td>
						<td class="No-Table-Style">
							<p>25%</p>
						</td>
						<td class="No-Table-Style">
							<p>6,000,000</p>
						</td>
						<td class="No-Table-Style">
							<p>1,200,000</p>
						</td>
						<td class="No-Table-Style">
							<p>667</p>
						</td>
						<td class="No-Table-Style">
							<p>2,000</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>LATAM</code>^</p>
						</td>
						<td class="No-Table-Style">
							<p>15%</p>
						</td>
						<td class="No-Table-Style">
							<p>3,600,000</p>
						</td>
						<td class="No-Table-Style">
							<p>720,000</p>
						</td>
						<td class="No-Table-Style">
							<p>400</p>
						</td>
						<td class="No-Table-Style">
							<p>1,200^</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>^ Zones overlapping at peak time</p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>Peak IOPS</p>
						</td>
						<td class="No-Table-Style">
							<p>6,000</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.6: MDN global subscriber distribution</p>
			<p>The minimum standard IOPS on any Atlas cluster on AWS is 3,000. To achieve 6,000 IOPS you would need to use an Atlas <code>M50</code> tier with 2TB disk, which feels over-provisioned and would not provide low latency to all readers if deployed in a single cloud region. To address this, MDN will deploy the application stack in major geographies, enabling regional provisioning, workload distribution, and local reads for an optimal customer experience.</p>
			<p>With MongoDB Atlas, you can place vector search nodes across regions. The <code>S40</code> tier offers 26,875 read IOPS, which is sufficient for this example, and a 2-node minimum per region, which ensures high availability.</p>
			<p>While Vector Search nodes will handle lexical, semantic, and image search, the full JSON document must be fetched from a MongoDB data node after matching. To fully support local reads, we must provision read-only nodes in the same regions and meet IOPS requirements. We can do this with the Atlas <code>M40</code> tier. Having determined the IOPS needed, you now need to estimate RAM.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor147"/>Determining RAM</h2>
			<p>For data nodes, the Atlas <code>M40</code> tier provides 16 GB of RAM. The MongoDB WiredTiger storage engine reserves 50% of (RAM - 1 GB) for its cache. With documents averaging 300 KB in size, the cache can hold approximately 28,000 documents. Keep in mind that traditional index sizes might slightly reduce this number. Given the addition of 100 new articles daily, the cache on an <code>M40</code> tier can accommodate data for about 280 days, or roughly 9 months, which is more than sufficient for this example.</p>
			<p>The Search <code>S40</code> tier offers 16 GB of RAM, 2 vCPUs, and 100 GB of storage. The HNSW graph, or the vector index, must fit in the memory.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You learned about <strong class="bold">HNSW</strong> or <strong class="bold">hierarchical navigable small worlds</strong> in <a href="B22495_05.xhtml#_idTextAnchor115"><em class="italic">Chapter 5</em></a>, <em class="italic">Vector Databases</em>.</p>
			<p>One article uses 1 x 1,024 vector + 5 x 768 vectors = 19.5 KB. With 3.5 GB needed for 182,500 articles, 16 GB of RAM is more than sufficient for vector search and leaves room for the lexical search index. The <code>S30</code> tier, which offers 4 GB of RAM, 1 vCPU, and 50 GB storage, is less costly, but note that more CPUs allows more concurrent searches.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor148"/>Final cluster configuration</h2>
			<p>You have now determined the cluster configuration for MDN. <em class="italic">Table 6.7</em> describes the MDN global cloud architecture, detailing the distribution of Atlas nodes across different regions. The <code>AMER</code> region, identified as the primary region, uses <code>M40</code> tier nodes and <code>S30</code> vector search nodes to serve writes and searches for the Americas, while the <code>EMEA</code>, <code>APAC</code>, and <code>LATAM</code> regions use <code>M40</code> read-only nodes and <code>S30</code> vector search nodes to serve local searches only for their respective region. Each region will need a deployment of the MDN application stack, as pictured in the global map in <em class="italic">Table 6.7</em>.</p>
			<table class="No-Table-Style _idGenTablePara-1" id="table007">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Region</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Atlas base </strong><strong class="bold">tier nodes</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Atlas </strong><strong class="bold">read-only nodes</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Atlas Vector </strong><strong class="bold">Search nodes</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>AMER</code></p>
							<p>(primary region)</p>
						</td>
						<td class="No-Table-Style">
							<p><code>M40</code></p>
							<p>(three included)</p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><code>S30</code> x2</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>EMEA</code></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><code>M40</code> x2</p>
						</td>
						<td class="No-Table-Style">
							<p><code>S30</code> x2</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>APAC</code></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><code>M40</code> x2</p>
						</td>
						<td class="No-Table-Style">
							<p><code>S30</code> x2</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>LATAM</code></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><code>M40</code> x2</p>
						</td>
						<td class="No-Table-Style">
							<p><code>S30</code> x2</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" colspan="4">
							<p><strong class="bold">MDN global </strong><strong class="bold">cloud architecture</strong></p>
							<div><div><img alt="" role="presentation" src="img/B22495_Table_6.7.jpg"/>
								</div>
							</div>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.7: MongoDB Atlas cluster configuration for MDN</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor149"/>Performance and availability versus cost</h2>
			<p>Notice that additional read-only nodes were not provisioned in the <code>AMER</code> region, using the two secondary nodes as read-only instead. This saves costs due to MDN’s low write profile, despite potential resource competition. Provisioning only one <code>M40</code> read-only node in other regions saves more costs but increases latency during maintenance windows, as reads will be rerouted.</p>
			<p>To protect against a complete <code>AMER</code> outage while adhering to best practices, consider provisioning five nodes across three regions and deploying the application stack in the two regions with two electable nodes each.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor150"/>Data flow</h1>
			<p><strong class="bold">Data flow</strong> involves the movement of data through a system, affecting the accuracy, relevance, and speed of the results delivered to consumers, which, in turn, influences their engagement. This section explores design considerations for handling data sources, processing data, prompting LLMs, and embedding models to enrich data using MDN as an example. <em class="italic">Figure 6</em><em class="italic">.5</em> illustrates this flow.</p>
			<div><div><img alt="" role="presentation" src="img/B22495_06_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5: Typical data flow in an AI/ML application</p>
			<p>Let's us begin with the design for handling data sources. Data can be ingested into MongoDB Atlas either statically (at rest) from files as it is, or dynamically (in motion), allowing for continuous updates, data transformation, and logic execution.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor151"/>Handling static data sources</h2>
			<p>The simplest way to import static data is to use <code>mongoimport</code>, which supports JSON, CSV, and TSV formats. It is ideal for initial loads or bulk updates as it can handle large datasets. Moreover, increasing the number of insertion workers to match the host’s vCPUs can boost import speed.</p>
			<p><code>mongoimport</code> can also be used dynamically to update externally sourced data. You can build invocation commands at runtime and execute them as out-of-process tasks. Some video game companies use this method to update player profiles with purchase data from mobile app stores.</p>
			<p>Using MDN as an example, users can provide their GitHub ID when subscribing. With GitHub’s API, you can create a list of the programming languages used in the repositories that users own or have contributed to. A scheduled job can fetch this data periodically. The list of languages can then imported and merged into their profiles to recommend articles later. <em class="italic">Table 6.8</em> demonstrates how you can do this.</p>
			<table class="No-Table-Style _idGenTablePara-1" id="table008">
				<colgroup>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>github-20240719.json</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<pre class="source-code">
{ "github_id" : "user1", "languages" : ["python", "csharp"], …}</pre>							<pre class="source-code">
{ "github_id" : "user2", "languages" : ["python", "cpp"], …}…</pre>						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>mdn.subscribers</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<pre class="source-code">
{ "_id" : ObjectId("669…ab8"), "github_id" : "user1", … }</pre>							<pre class="source-code">
{ "_id" : ObjectId("669…ab9"), "github_id" : "user2", … }…</pre>						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>The <code>mongoimport</code> invocation to merge data matching on the <code>github_id</code> field</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<pre class="source-code">
mongoimport --uri=&lt;connection string to Atlas cluster&gt;</pre>							<pre class="source-code">
--db=mdn --collection=subscribers --mode=merge</pre>							<pre class="source-code">
--file=github-20240719.json --upsertFields=github_id</pre>							<pre class="source-code">
--numInsertionWorkers=4</pre>						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>mdn.subscribers</code> after merge</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<pre class="source-code">
{ "_id" : ObjectId("669…ab8"), "github_id" : "user1", "languages" : ["python", "csharp"], … }</pre>							<pre class="source-code">
{ "_id" : ObjectId("669…ab9"), "github_id" : "user2", "languages" : ["python", "cpp"], … }…</pre>						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.8: Example of using mongoimport to merge data</p>
			<p>While <code>mongoimport</code> is a versatile tool for various data import needs, it does not support continuous synchronization, logic execution, or data transformations. You will now explore some methods that do support these functions.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor152"/>Storing operational data enriched with vector embeddings</h2>
			<p>When original representations are stored or updated, their corresponding vector embeddings must be refreshed to accurately reflect the content. This can be done in the following ways:</p>
			<ul>
				<li><strong class="bold">Synchronously</strong>: Obtains the updated vector embedding before the database operation, writing both data and embedding together. This method is suitable for fast, simple embedding models or when the model is locally hosted. However, it may fail if the response times of the embedding model vary.</li>
				<li><strong class="bold">Asynchronously</strong>: Ensures immediate consistency of primary data and allows for prompting the embedding model afterward. While this offers scalability and handles unpredictable models, it introduces latency during which embeddings are temporarily outdated.</li>
			</ul>
			<p>You can keep embeddings up to date asynchronously in MongoDB using the following four methods:</p>
			<ul>
				<li><strong class="bold">Kafka connector</strong>: You can facilitate data flow from Apache Kafka into MongoDB collections through the Kafka connector. It is a Confluent-verified connector and allows data to flow from Apache Kafka topics into MongoDB as a <strong class="bold">data sink</strong> and publishes changes from MongoDB to Kafka topics as a <strong class="bold">data source</strong>. To keep embeddings up to date, you would use the sink connector and develop a post-processor in Java. You can learn more about sink post-processors here: <a href="https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/fundamentals/post-processors/#sink-connector-post-processors">https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/fundamentals/post-processors/#sink-connector-post-processors</a>.</li>
				<li><strong class="bold">Atlas Stream Processing</strong>: This method handles complex data streams with the same query API as MongoDB Atlas databases. It enables continuous aggregation and includes schema validation for message integrity and timely issue detection. Processed data can be written to Atlas collections, and they are integrated into Atlas projects and independent of Atlas clusters. Atlas Stream Processing logic is programmed in JavaScript using MongoDB aggregation syntax. For an example of using Atlas Stream Processing to handle embedding data, see <a href="https://www.mongodb.com/solutions/solutions-library/rag-applications">https://www.mongodb.com/solutions/solutions-library/rag-applications</a>.</li>
				<li><strong class="bold">Atlas Triggers</strong>: Atlas Triggers execute application and database logic by responding to events or following predefined schedules. Each Trigger listens for specific event types and is linked to an Atlas Function. When a matching event occurs, the Trigger fires and passes the event object to the linked Function. Triggers can respond to various events, such as specific operations in a collection, authentication events such as user creation or deletion, and scheduled times. They are fully managed instances of change streams but limited to JavaScript. For an example of using Atlas Triggers to keep embeddings up to date, see <a href="https://www.mongodb.com/developer/products/atlas/semantic-search-mongodb-atlas-vector-search/">https://www.mongodb.com/developer/products/atlas/semantic-search-mongodb-atlas-vector-search/</a>.</li>
				<li><strong class="bold">Change streams</strong>: This method provides real-time access to data changes. Applications can subscribe to changes in a collection, database, or entire deployment and react immediately, with events processed in order and being resumable? Using the aggregation framework, change streams allow filtering and transforming notifications. They can be used with any programming language supported by an official MongoDB driver. However, they are not fully managed, requiring a running host to be maintained alongside the main application.</li>
			</ul>
			<p>Given that this book is written for Python developers, you will learn how to use a change stream written in Python. <em class="italic">Table 6.9</em> shows a Python 3 change stream using LangChain and OpenAI to embed the title and summary of an MDN article. It is triggered for new articles or changes to the title or summary following the data model from <em class="italic">Figure 6</em><em class="italic">.3</em> and the vector index from <em class="italic">Table 6.3</em>.</p>
			<pre class="source-code">
import os
from langchain_openai import OpenAIEmbeddings
from pymongo import MongoClient
from pymongo.errors import PyMongoError
# Set the OpenAI API key as an environment variable
os.environ["OPENAI_API_KEY"] = "YOUR-OPENAI-API-KEY"
# Define the MongoDB Atlas connection string
ATLAS_CONNECTION_STRING = "YOUR-MONGODB_ATLAS-CONNSTRING"
# Create a MongoClient instance to connect to MongoDB Atlas
client = MongoClient(
    ATLAS_CONNECTION_STRING, tls=True, tlsAllowInvalidCertificates=True
)
# Select the 'articles' collection from the 'mdn' database
coll = client["mdn"]["articles"]
# Instantiate the OpenAIEmbeddings model with specified parameters
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-large", dimensions=1024, disallowed_special=()
)
# Define a function to handle changes detected in the MongoDB collection
def handle_changes(change):
    # Extract the document ID from the change event
    doc_id = change["documentKey"]["_id"]
    # Create a filter to identify the document in the collection
    doc_filter = {
        "_id": doc_id
    }
    # Combine the title and summary of the document into a single text string
    text = [change["fullDocument"]["title"] + " " + change["fullDocument"]["summary"]]
    # Generate embeddings for the text
    embeddings = embedding_model.embed_documents(text)
    # Create an update document to set the 'semantic_embedding' field with the generated embeddings
    set_fields = {
        "$set": {
            "semantic_embedding": embeddings[0]
        }
    }
    # Update the document in the collection with the new embeddings
    coll.update_one(doc_filter, set_fields)
    print(f"Updated embeddings for document {doc_id}")
# Start monitoring the MongoDB collection for changes
try:
    # Define a stream filter to match insert and update operations affecting the title or summary fields
    stream_filter = [
        {
            "$match": {
                "$or": [
                    {"operationType": "insert"},
                    {
                        "$and": [
                            {"operationType": "update"},
                            {
                                "$or": [
                                    {
                                        "updateDescription.updatedFields.title": {
                                            "$exists": True
                                        }
                                    },
                                    {
                                        "updateDescription.updatedFields.summary": {
                                            "$exists": True
                                        }
                                    },
                                ]
                            },
                        ]
                    },
                ]
            }
        }
    ]
    # Open a change stream to watch for changes in the collection
    with coll.watch(stream_filter, full_document="updateLookup") as stream:
        print("Listening for changes...")
        for change in stream:
            print(f"Change detected: {change}. Processing")
            handle_changes(change)
except PyMongoError as e:
    # Print an error message if a PyMongoError occurs
    print(f"An error occurred: {e}")
finally:
    # Close the MongoDB client connection
    client.close()</pre>			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.9: Change stream written in Python to set or update embeddings</p>
			<p>Now that you have learned how to handle the data flow for setting or updating embeddings, you will learn about data freshness and retention, which are essential for delivering relevant and timely content.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor153"/>Freshness and retention</h1>
			<p>Fresh data and effective retention strategies ensure that your content is relevant and delivered on time. <strong class="bold">Freshness</strong> keeps users engaged with the latest articles, comments, and recommendations. <strong class="bold">Retention strategies</strong> manage the data lifecycle, preserving valuable historical data for analytics while purging obsolete data. This section explores methods for ensuring up-to-date content and efficient data flow.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor154"/>Real-time updates</h2>
			<p>The primary concern is to ingest and update new data in real time, making it available across all cloud regions. For the news site, this means new articles and their vector embeddings should be promptly persisted and replicated for global access.</p>
			<p>To achieve this with a distributed data model and application, use an ACID transaction to ensure that the article and its content embeddings are written together as a single unit. For an example of creating MongoDB transactions in Python, see <a href="https://learn.mongodb.com/learn/course/mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions-in-python-applications/learn?page=2">https://learn.mongodb.com/learn/course/mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions-in-python-applications/learn?page=2</a>.</p>
			<p>Next, balance data reliability, consistency, and performance in a distributed setup using MongoDB’s tunable consistency with <code>writeConcern</code>, <code>readConcern</code>, and <code>readPreference</code>. These modifiers help to ensure data integrity and quick access. The following is an explanation of these modifiers, but for a deeper understanding, you can visit <a href="https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/">https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/</a>:</p>
			<ul>
				<li><code>writeConcern:majority</code> ensures data consistency and durability by acknowledging write operations only after data is written to the majority of replica set members, reducing the risk of data loss during failures. It is the default write concern.</li>
				<li><code>readConcern:majority</code> provides read consistency by ensuring that read operations return the most recent data acknowledged by the majority of the replica set members, providing a consistent view of the data across the application.</li>
				<li><code>readPreference:nearest</code> optimizes latency by directing read operations to the replica set member with the lowest network latency. For MDN, this minimizes response times by allowing each regional application deployment to read from the nearest MongoDB data and vector nodes, and balancing consistency and performance.</li>
			</ul>
			<p>Now that you have learned how to ensure data availability and speed, the next focus is on data lifecycle management, a key aspect of data freshness and retention.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor155"/>Data lifecycle</h2>
			<p><strong class="bold">Data lifecycle</strong> refers to the various stages data goes through from creation to deletion, and how it may traverse and change systems or storage formats, including when data is archived or deleted. As latest content is added, older content may become less relevant.</p>
			<p>For example, older articles can be moved to an archive database or cold storage, reducing storage costs and optimizing active database performance. However, moving data to cold storage may reduce search capabilities compared to the operational database. Here are three approaches for handling the data lifecycle, along with their trade-offs:</p>
			<ul>
				<li><strong class="bold">All data in the operational cluster</strong>: Keeping all data in the operational cluster is the most performant but costly approach, suitable for scenarios where most data is frequently accessed, such as global online games, authentication providers, or financial platforms. MongoDB Atlas supports this with sharded clusters and global clusters. Global clusters allocate <em class="italic">data zones</em> to cloud regions for capacity management and data locality.</li>
				<li><strong class="bold">Active and historic operational data clusters</strong>: This involves using high-performance hardware for recent data and less capable hardware for older data, balancing functionality, and cost savings. With MongoDB Atlas, data can be moved from active to historic cluster(s) using Cluster-to-Cluster Sync and TTL indexes. Other platforms such as Apache Kafka, Confluent, and Striim also support this method.</li>
				<li><strong class="bold">Active data cluster and historical storage</strong>: Full historical data can be offloaded to cold storage while retaining key fields in the operational cluster, allowing for full or limited query and search capabilities. For MDN, this ensures that users can find historical articles through lexical semantic searches, with full articles stored in cold storage and accessed when needed. With MongoDB Atlas, this can be achieved using Online Archive and Data Federation. <strong class="bold">Online Archive</strong> automatically moves data from the cluster to lower-cost cloud storage based on the set expiration. <strong class="bold">Data Federation</strong> allows transparent querying of both clusters and the archive, regardless of the source.</li>
			</ul>
			<p>This section covered data lifecycle management, emphasizing how data is managed from creation to archival. You learned about three strategies: maintaining all data in the operational cluster for maximum performance, separating active and historical data to balance cost and performance, and offloading historical data to cold storage while retaining some search functionality. Now, you will learn about upgrading embedding models.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor156"/>Adopting new embedding models</h2>
			<p>OpenAI superseded the <code>text-search-davinci-*-001</code> model with <code>text-embedding-ada-002</code> on December 15, 2022, and subsequently with <code>text-embedding-small/large</code> on January 25, 2024. It is likely that by the time you read this book, these models will be replaced too.</p>
			<p>As you learned in the <a href="B22495_04.xhtml#_idTextAnchor061"><em class="italic">Chapter 4</em></a>, <em class="italic">Embedding Models</em>, embeddings from one model are not compatible with another. Re-embedding previously indexed data may be necessary as newer models are adopted. This is a resource-intensive activity that requires design considerations upfront.</p>
			<p>You will need to choose an approach toward adopting new embedding models. You can either continue using the existing vector fields and perform lengthy all-or-nothing upgrades, double-embed for a period, or implement a gradual upgrade. Let's explore these three approaches:</p>
			<ul>
				<li><strong class="bold">Use existing vector fields</strong>: This approach keeps the application code intact but requires downtime to re-embed data and replace vector indexes. This approach is suitable if the re-embedding and re-indexing time fits within your allowable downtime windows.</li>
				<li><strong class="bold">Double-embed temporarily</strong>: This approach double embeds fields for new or modified data using the old and new model. It uses a background job to add new embeddings for data that is not modified. When all data has double embeddings, the application will be updated and deployed to use the new embeddings. Once stable, the deprecated vectors and indexes can be removed with another background job. Ensure sufficient disk space and memory for when two sets of vectors coexist. This approach is suitable if the downtime windows are small and only accommodate application deployment times.</li>
				<li><code>filter</code> type of MongoDB’s vector indexes (shown in <em class="italic">Table 6.3</em>), you can introduce a new field to distinguish between documents with old and new vectors and implement the union. Eventually, old vectors and indexes can be dropped, and you can remove unneeded logic. This approach is suitable if no downtime is allowed.</li>
			</ul>
			<p>By addressing these three main concerns—data ingestion and real-time updates, managing the data lifecycle and aging, and upgrading embedding models—your application can ensure that its data remains fresh and relevant, providing an optimal platform and striving for the best user experience. Now, you will learn about security and its considerations for AI-intensive applications.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor157"/>Security and RBAC</h1>
			<p><strong class="bold">Security measures</strong> protect data from unauthorized access and breaches, while RBAC ensures appropriate access levels based on roles. Here are key security and RBAC strategies to protect data integrity and privacy:</p>
			<ul>
				<li><strong class="bold">Data encryption and secure storage</strong>: Encrypting data at rest and in transit is crucial for securing an application. Encryption at rest protects data from unauthorized access, while encryption in transit secures data as it moves between users and the application. MongoDB Atlas offers built-in integration with <strong class="bold">AWS Key Management Service</strong> (<strong class="bold">AWS KMS</strong>) for encryption at rest and TLS/SSL out of the box for data in transit.</li>
				<li><strong class="bold">Access controls and user authentication</strong>: RBAC manages permissions, ensuring that users access only necessary data and functionalities. In the case of MDN, separate roles, such as editors and readers, require various levels of access. Different database users on MongoDB can be set up with distinct levels of permissions following the principle of least privilege. For example, only the application identity used by the microservice that embeds data would have write permissions to the collections where embeddings are stored, while the application identity used by human actors would only have read permissions.</li>
				<li><strong class="bold">Monitoring and auditing</strong>: Continuous monitoring and auditing detect and respond to security incidents in real time. Monitoring tools and audit logs track user activities and identify unusual access patterns. MongoDB Atlas offers advanced monitoring and alerting capabilities, allowing administrators to set up alerts for suspicious activities. Regularly reviewing audit logs ensures compliance with security policies and provides insights for improving security.</li>
				<li><strong class="bold">Data backup and recovery</strong>: Maintain data integrity and availability with regular backups to minimize downtime and loss during security breaches or incidents. MongoDB Atlas offers automated backup solutions with snapshots, ensuring quick recovery. If encryption at rest is enabled (for example, AWS KMS), embeddings and operational data are encrypted under the same key in both volumes and backups.</li>
			</ul>
			<p>While there are many security-related concerns, the ones just covered should suffice to start building AI applications. Ensuring security is a continuous effort that organizations must adopt and enforce to maintain compliance, foster user trust, and safeguard application integrity.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor158"/>Best practices for AI/ML application design</h1>
			<p>This section covers best practices for the five concerns covered in this chapter—data modeling, data storage, data flow, data freshness and retention, and security and RBAC. These guidelines will help ensure that your application is efficient, scalable, and secure, providing a solid foundation for building reliable and high-performing AI apps. Here are the top two best practices for each aspect of your AI/ML application design.</p>
			<ol>
				<li><strong class="bold">Data modeling</strong>: The following techniques ensure efficiency and performance for handling embeddings:<ul><li><strong class="bold">Embeddings in separate collections</strong>: Store embeddings in a separate collection to avoid bloated documents, especially when multiple embeddings and nested indexing limitations are involved. Duplicate fields to ensure efficient filtering and maintain performant searches.</li><li><strong class="bold">Hybrid search</strong>: Combine semantic and lexical searches using reciprocal rank fusion. This hybrid approach boosts search functionality by leveraging the strengths of both.</li></ul></li>
				<li><strong class="bold">Data storage</strong>: To optimize database cluster sizing, implement the following best practices:<ul><li><strong class="bold">Sufficient IOPS and RAM based on peak usage</strong>: Calculate required IOPS based on peak access times and application read/write patterns. Ensure data and search nodes have enough RAM to handle the caching and indexing needs of the most requested data.</li><li><strong class="bold">Local reads</strong>: Deploying nodes across regions helps minimize read latency and enhances the user experience. Ensure that each region has all the nodes required to fully serve data locally.</li></ul></li>
				<li><strong class="bold">Data flow</strong>: Consider the following strategies for harnessing data flow effectively:<ul><li><strong class="bold">Asynchronous embedding updates</strong>: Ensure primary data consistency by updating vector embeddings asynchronously. This method accommodates scalability and unpredictable model response times, although it introduces temporary latency.</li><li><strong class="bold">Dynamic data handling</strong>: Leverage technologies such as change streams, Atlas Triggers, Kafka, and Atlas Stream Processing to handle continuous updates, transformations, and logic execution.</li></ul></li>
				<li><strong class="bold">Data freshness and retention</strong>: The following best practices can ensure that your application is relevant and prompt:<ul><li><strong class="bold">Up-to-date embedding models</strong>: Embeddings from one model are not compatible with another. Plan for model upgrades during downtime if possible, or consider gradual upgrades, which are architecturally complex but require no downtime. Leverage MongoDB’s flexible data model to transition between embeddings.</li><li><strong class="bold">Data tiering</strong>: Implement a data aging strategy by moving older data to an archive cluster or cold storage while keeping recent data in high-performance clusters. Use broader MongoDB Atlas features such as Online Archive, Data Federation, and more for effective data tiering.</li></ul></li>
				<li><strong class="bold">Security and RBAC</strong>: Following are the best practices for ensuring the security of your data:<ul><li><strong class="bold">RBAC</strong>: Assign role-based permissions and follow the <strong class="bold">principle of least privilege</strong> (<strong class="bold">PoLP</strong>), ensuring that users and entities access only necessary data and actions. For instance, code embedding data should have write access only to embedding collections.</li><li><strong class="bold">Encryption and storage</strong>: Turn on encryption at rest and integrate with KMS to ensure that all data volumes and backups are encrypted with your own key.</li></ul></li>
			</ol>
			<p>Implementing these best practices boosts the efficiency, scalability, and security of your AI/ML applications. Though just a starting point, these guidelines lay a solid foundation for building reliable, high-performing systems. With these best practices, you can navigate the complexities of modern AI and prepare your applications for long-term success and adaptability in a rapidly evolving tech landscape.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor159"/>Summary</h1>
			<p>This chapter covered critical architectural considerations for developing intelligent applications. You learned about data modeling and how to evolve your model to fulfill use cases, address technical limitations, and consider patterns and anti-patterns. This approach ensures that data is not only useful but also accessible and optimally utilized across various components of your AI/ML system.</p>
			<p>Data storage was another key aspect of this chapter, focusing on the selection of appropriate storage technologies based on different data types and the specific needs of the application. It highlighted the importance of accurately estimating storage requirements and other aspects of choosing the right MongoDB Atlas cluster configuration. The fictitious example of the MDN application served as a practical case study, illustrating how to apply these principles in a real-world scenario.</p>
			<p>The chapter also explored the flow of data through ingestion, processing, and output to ensure data integrity and maintain the velocity of data operations. This chapter also addressed data lifecycle management, including the importance of data freshness and retention. You learned strategies for managing updates and changing embedding models used by your application.</p>
			<p>Security is a paramount concern in AI/ML applications, and you learned brief but important points about protecting the integrity of data and application logic. Concluding with a compilation of best practices, this chapter summarized key principles from data modeling, storage, flow, and security, offering practical advice to avoid common pitfalls and enhance the development of robust AI/ML applications.</p>
			<p>In the next chapter, you will explore different AI/ML frameworks, Python libraries, and publicly available APIs and other tools.</p>
		</div>
	

		<div><div></div>
		</div>
		<div><h1 id="_idParaDest-121"><a id="_idTextAnchor160"/>Part 2</h1>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor161"/>Building Your Python Application: Frameworks, Libraries, APIs, and Vector Search</h1>
			<p>This following set of chapters will equip you with the necessary tools for AI development through detailed instructions and examples on enhancing developer and user experience with Python and retrieval-augmented generation.</p>
			<p>This part of the book includes the following chapters:</p>
			<ul>
				<li><a href="B22495_07.xhtml#_idTextAnchor162"><em class="italic">Chapter 7</em></a>, <em class="italic">Useful Frameworks, Libraries, and APIs</em></li>
				<li><a href="B22495_08.xhtml#_idTextAnchor180"><em class="italic">Chapter 8</em></a>, <em class="italic">Implementing Vector Search in AI Applications</em></li>
			</ul>
		</div>
	</body></html>