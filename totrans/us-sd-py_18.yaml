- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications – Object Editing and Style Transferring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Stable Diffusion** (**SD**) is not only capable of generating a variety of
    images but it can also be utilized for image editing and style transfer from one
    image to another. In this chapter, we will explore solutions for image editing
    and style transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Along the way, we will also introduce the tools that enable us to achieve these
    goals: **CLIPSeg**, which is used to detect the content of an image; **Rembg**,
    which is a tool that flawlessly removes the background of an image; and **IP-Adapter**,
    which is used to transfer the style from one image to another.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Editing images using Stable Diffusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object and style transferring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start.
  prefs: []
  type: TYPE_NORMAL
- en: Editing images using Stable Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do you recall the background swap example we discussed in [*Chapter 1*](B21263_01.xhtml#_idTextAnchor015)?
    In this section, we will introduce a solution that can assist you in editing the
    content of an image.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can edit anything, we need to identify the boundary of the object
    we want to edit. In our case, to obtain the background mask, we will use the CLIPSeg
    [1] model. **CLIPSeg**, which stands for **CLIP-based Image Segmentation**, is
    a model trained to segment images based on text prompts or reference images. Unlike
    traditional segmentation models that require a large amount of labeled data, CLIPSeg
    can achieve impressive results with little to no training data.
  prefs: []
  type: TYPE_NORMAL
- en: CLIPSeg builds upon the success of CLIP, the same model used by SD. CLIP is
    a powerful pre-trained model that learns to connect text and images. The CLIPSeg
    model adds a small decoder module on top of CLIP, allowing it to translate the
    learned relationships into pixel-level segmentation. This means we can provide
    CLIPSeg with a simple description such as “the background of this picture,” and
    CLIPSeg will return the mask of the targeted objects.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how we can use CLIPSeg to accomplish some tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing image background content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first load up the CLIPSeg processor and model, then provide both the
    prompt and image to the model to generate the mask data, and finally, use the
    SD inpainting pipeline to redraw the background. Let’s do it step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the CLIPSeg model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code will load up the `CLIPSegProcessor` processor and `CLIPSegForImageSegmentation`
    model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `processor` will be used to preprocess both the prompt and images input.
    The `model` will be the one responsible for model inference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Generate the grayscale mask.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By default, the CLIPSeg model will return logits of its result. By applying
    the `torch.sigmoid()` function, we can then have the grayscale mask of the target
    object in the image. The grayscale mask can then enable us to generate the binary
    mask, which will be used in the SD inpainting pipeline:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will generate a grayscale mask image that highlights the
    background, as shown in *Figure 18**.1*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 18.1: Background grayscale mask](img/B21263_18_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.1: Background grayscale mask'
  prefs: []
  type: TYPE_NORMAL
- en: This mask is still not the one we want; we need a binary mask. Why do we need
    a binary mask? Because SD v1.5 inpainting works better with a binary mask than
    a grayscale mask. You may also add the grayscale mask to the SD pipeline to see
    the result; there’s nothing to lose by trying different combinations and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Generate a binary mask.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use the following code to convert a grayscale mask into a 0-1 binary
    mask image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let me explain the key elements we presented in the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bw_thresh`: This defines the threshold of treating a pixel as black or white.
    In the preceding code, any grayscale pixel value higher than 100 will be treated
    as a white highlight.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_pil.convert("L")`: This converts the `mask_pil` image into grayscale
    mode. Grayscale images have only one channel, representing pixel intensity values
    from 0 (black) to 255 (white).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.point(bw_fn, mode="1")`: This applies the `bw_fn` thresholding function to
    each pixel of the grayscale image. The `mode="1"` argument ensures that the output
    image is a 1-bit binary image (black and white only).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will see the result shown in *Figure 18**.2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 18.2: Background binary mask](img/B21263_18_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.2: Background binary mask'
  prefs: []
  type: TYPE_NORMAL
- en: 'Redraw the background using the SD inpainting model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we use the SD v1.4 model as the inpainting model because
    it generates better results than the SD v1.5 model. If you execute it, you will
    see the exact result we presented in [*Chapter 1*](B21263_01.xhtml#_idTextAnchor015).
    The background is now no longer a vast planetary universe but blue sky and mountains.
  prefs: []
  type: TYPE_NORMAL
- en: The same technique can be used for many other purposes, such as editing clothing
    in a photo and adding items to a photo.
  prefs: []
  type: TYPE_NORMAL
- en: Removing the image background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many times, we want to just remove the background of an image. With the binary
    mask in hand, removing the background isn’t hard at all. We can do it using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a breakdown of what each line does:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from PIL import Image, ImageOps`: This line imports the `Image` and `ImageOps`
    modules from PIL. The `Image` module provides a class with the same name that
    is used to represent a PIL image. The `ImageOps` module contains a number of “ready-made”
    image-processing operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_image = Image.new("RGBA", source_image.size, (255,255,255,255))`: This
    line creates a new image with the same size as `source_image`. The new image will
    be in RGBA mode, meaning it includes channels for red, green, blue, and alpha
    (transparency). The initial color of all pixels in the image is set to white `(255,255,255)`
    with full opacity `(255)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inverse_bw_mask_pil = ImageOps.invert(bw_mask_pil)`: This line inverts the
    colors of the `bw_mask_pil` image using the `invert` function from ImageOps. If
    `bw_mask_pil` is a black and white image, the result will be a negative of the
    original image, that is, black becomes white and white becomes black.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r = Image.composite(source_image ,output_image, inverse_bw_mask_pil)`: This
    line creates a new image by blending `source_image` and `output_image` based on
    the `inverse_bw_mask_pil` mask image. Where the mask image is white (or shades
    of gray), the corresponding pixels from `source_image` are used, and where the
    mask image is black, the corresponding pixels from `output_image` are used. The
    result is assigned to `r`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simply four lines of code enable the replacement of the background with pure
    white, as shown in *Figure 18**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.3: Remove background using CLIPSeg](img/B21263_18_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.3: Remove background using CLIPSeg'
  prefs: []
  type: TYPE_NORMAL
- en: 'But, we will see jagged edges; this is not good and can’t be perfectly solved
    using CLIPSeg. If you are going to feed this image into the diffusion pipeline
    again, SD will help fix the jagged edges problem by using another image-to-image
    pipeline. Based on the nature of the diffusion model, the background edges will
    be either blurred or rerendered with other pixels. To remove the background neatly,
    we will need other tools to help, for example, the Rembg project [2]. Its usage
    is also simple:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove the background with two lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And we see the background is completely removed, as shown in *Figure 18**.4*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 18.4: Remove background using Rembg](img/B21263_18_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.4: Remove background using Rembg'
  prefs: []
  type: TYPE_NORMAL
- en: 'To set the background as white, use three more lines of code, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We can find the background is completely replaced with a white background. An
    object with a pure white background can be useful in some cases; for instance,
    we are going to use the object as a guidance embedding. No, you did not read that
    wrong; we can use the image as the input prompt. Let’s explore this in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Object and style transferring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we introduced the theory behind SD in *Chapters 4* and *5*, we learned
    that only text embedding is involved in the UNet diffusion process. Even if we
    provide an initial image as the starting point, the initial image is simply used
    as the starting noise or concatenated with initial noises. It does not have any
    influence on the steps of the diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: That is until the IP-Adapter project [3] came about. IP-Adapter is a tool that
    lets you use an existing image as a reference for text prompts. In other words,
    we can take the image as another piece of prompt work together with text guidance
    to generate an image. Unlike Textual Inversion, which usually works well for certain
    concepts or styles, IP-Adapter works with any images.
  prefs: []
  type: TYPE_NORMAL
- en: With the help of IP-Adapter, we can magically transfer an object from one image
    to a completely different one.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s start using IP-Adapter to transfer an object from one image to another
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Loading up a Stable Diffusion pipeline with IP-Adapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using IP-Adapter in Diffusers is simple enough, you don’t need to install any
    additional packages or manually download any model files:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the image encoder. It is this dedicated image encoder that plays a key
    role in turning the image into a guidance prompt embedding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load a vanilla SD pipeline but with one additional `image_encoder` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will use the image encoder model from `models/image_encoder` even when loading
    an SDXL pipeline, rather than `sdxl_models/image_encoder`; otherwise, an error
    message will be thrown. You can also replace the SD v1.5 base model with any other
    community-shared models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply IP-Adapter to the UNet pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you are using an SDXL pipeline, replace `models` with `sdxl_models`, and
    replace `ip-adapter_sd15.bin` with `ip-adapter_sdxl.bin`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: That is all; now we can use the pipeline just like any other pipeline. Diffusers
    will help you download the model files automatically if no IP-Adapter models exist.
    In the next section, we are going to use the IP-Adapter model to transfer a style
    from one image to another.
  prefs: []
  type: TYPE_NORMAL
- en: Transferring style
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we are going to write code to transfer the famous *Girl with
    a Pearl Earring* by Johannes Vermeer (see *Figure 18**.5*) to the *astronaut riding
    a* *horse* image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.5: Girl with a Pearl Earring by Johannes Vermeer](img/B21263_18_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.5: Girl with a Pearl Earring by Johannes Vermeer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, let’s kick off the pipeline to transfer style:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we used the original astronaut image – `source_image`
    – as the base, and the oil painting image as the IP-Adapter image prompt – `ip_image`
    (we want its style). Amazingly, we get the result shown in *Figure 18**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.6: Astronaut riding a horse with a new style](img/B21263_18_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.6: Astronaut riding a horse with a new style'
  prefs: []
  type: TYPE_NORMAL
- en: The style and feel of the *Girl with a Pearl Earring* image have successfully
    been applied to another image.
  prefs: []
  type: TYPE_NORMAL
- en: IP-Adapter’s potential is huge. We can even transfer the clothing and face from
    one image to another. More usage samples can be found in the original IP-Adapter
    repository [3] and the Diffusers PR page [5].
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the focus was on using SD for image editing and style transferring.
    The chapter introduced tools such as CLIPSeg for image content detection, Rembg
    for background removal, and IP-Adapter for transferring styles between images.
  prefs: []
  type: TYPE_NORMAL
- en: The first section covered image editing, specifically replacing or removing
    the background. CLIPSeg is used to generate a mask of the background, which is
    then converted to a binary mask. The background is either replaced using SD or
    removed, with the latter option showing jagged edges. Rembg was introduced as
    a solution for smoother background removal.
  prefs: []
  type: TYPE_NORMAL
- en: The second section explored object and style transferring using IP-Adapter.
    The process involves loading an image encoder, incorporating it into an SD pipeline,
    and applying IP-Adapter to the UNet of the pipeline. The chapter concluded with
    an example of transferring the style of Vermeer’s *Girl with a Pearl Earring*
    onto an image of an astronaut riding a horse.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to explore solutions to save and read the
    parameters and prompt information to and from the generated image files.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CLIPSeg GitHub repository: [https://github.com/timojl/clipseg](https://github.com/timojl/clipseg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Timo Lüddecke and Alexander S. Ecker, *Image Segmentation Using Text and Image*
    *Prompts*: [https://arxiv.org/abs/2112.10003](https://arxiv.org/abs/2112.10003)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'IP-Adapter GitHub repository: [https://github.com/tencent-ailab/IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rembg, a tool to remove image backgrounds: [https://github.com/danielgatis/rembg](https://github.com/danielgatis/rembg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'IP-Adapters original samples: [https://github.com/huggingface/diffusers/pull/5713](https://github.com/huggingface/diffusers/pull/5713)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
