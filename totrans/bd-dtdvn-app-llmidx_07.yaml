- en: <title>Querying Our Data, Part 1 – Context Retrieval</title>
  prefs: []
  type: TYPE_NORMAL
- en: Querying Our Data, Part 1 – Context Retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The focus of this chapter will be on understanding the querying capabilities
    of LlamaIndex in an RAG workflow. We’ll be covering the overall working of the
    querying system, mostly focusing on the retrieval capabilities of the framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main sections that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about query mechanics – an overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the basic retrievers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building more advanced retrieval mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing efficiency with asynchronous retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with metadata filters, tools, and selectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming queries and generating sub-queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the concepts of dense and sparse retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <title>Technical requirements</title>
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you will need to install the `Rank-BM25` package in your environment.
    You can find it at https://pypi.org/project/rank-bm25/ .
  prefs: []
  type: TYPE_NORMAL
- en: 'Two additional integration packages are required to run the sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '*OpenAI Question* *Generator* : https://pypi.org/project/llama-index-question-gen-openai/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BM25* *Retriever* : https://pypi.org/project/llama-index-retrievers-bm25/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the code samples for this chapter can be found in the ch6 subfolder of
    this book’s GitHub repository: https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <title>Learning about query mechanics – an overview</title>
  prefs: []
  type: TYPE_NORMAL
- en: Learning about query mechanics – an overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will finally begin to reap the fruits of our work so far.
    Document ingestion, parsing and segmenting, metadata extraction, and index building
    were all just preparatory steps for what we are about to discuss: **querying**
    . At the heart of any RAG workflow is the idea of being able to bring relevant
    context into the prompt we use in the LLM query. So far, we have been concerned
    with constructing and organizing this context, but now, it is time to use it and
    extract the best possible answers from our interactions with LLMs. In the following
    sections, we will discuss various techniques that LlamaIndex provides us for the
    query part. As usual, we will start with the simplest query methods – called *naive*
    methods in jargon – and then discuss more advanced query variants.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to understand the typical steps in the query process: **retrieval**
    , **postprocessing** , and **response synthesis** .'
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 3* *,* *Kickstarting Your Journey with LlamaIndex* , in the *Indexes*
    section, we discussed the simplest way to go through the three steps – using `QueryEngine`
    but built very simply by running `index.as_query_engine()` . This is very simple
    but not necessarily always effective as this *naive* way of querying an index
    is just the tip of the iceberg. We will now explore the three mechanisms individually
    and understand how they work and the customizable options they offer.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll focus on **retrievers** .
  prefs: []
  type: TYPE_NORMAL
- en: <title>Understanding the basic retrievers</title>
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basic retrievers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core import SummaryIndex, SimpleDirectoryReader documents
    = SimpleDirectoryReader("files").load_data() summary_index = SummaryIndex.from_documents(documents)
    retriever = summary_index.as_retriever(     retriever_mode=''embedding'' ) result
    = retriever.retrieve("Tell me about ancient Rome") print(result[0].text) from
    llama_index.core import SummaryIndex, SimpleDirectoryReader from llama_index.core.retrievers
    import SummaryIndexEmbeddingRetriever documents = SimpleDirectoryReader("files").load_data()
    summary_index = SummaryIndex.from_documents(documents) retriever = SummaryIndexEmbeddingRetriever(
        index=summary_index ) result = retriever.retrieve("Tell me about ancient Rome")
    print(result[0].text) VectorStoreIndex.as_retriever() SummaryIndex.as_retriever(retriever_mode
    = ''default'') SummaryIndex.as_retriever(retriever_mode=''embedding'') SummaryIndex.as_retriever(retriever_mode=''llm'')
    DocumentSummaryIndex.as_retriever(retriever_mode=''llm'') DocumentSummaryIndex.as_retriever(
        retriever_mode=''embedding'' ) TreeIndex.as_retriever(retriever_mode=''select_leaf'').
    TreeIndex.as_retriever(     retriever_mode=''select_leaf_embedding'' ) TreeIndex.as_retriever(retriever_mode=''all_leaf'')
    TreeIndex.as_retriever(retriever_mode=''root'') KeywordTableIndex.as_retriever(retriever_mode=''default'')
    KeywordTableIndex.as_retriever(retriever_mode=''simple'') KeywordTableIndex.as_retriever(retriever_mode=''rake'')
    KnowledgeGraphIndex.as_retriever(retriever_mode=''keyword'') KnowledgeGraphIndex.as_retriever(
        retriever_mode=''embedding'' ) KnowledgeGraphIndex.as_retriever(retriever_mode=''hybrid'')
    import asyncio from llama_index.core import KeywordTableIndex from llama_index.core
    import SimpleDirectoryReader async def retrieve(retriever, query, label):     response
    = await retriever.aretrieve(query)     print(f"{label} retrieved {str(len(response))}
    nodes") async def main():     reader = SimpleDirectoryReader(''files'')     documents
    = reader.load_data()     index = KeywordTableIndex.from_documents(documents)     retriever1
    = index.as_retriever( retriever_mode=''default'' )     retriever2 = index.as_retriever(
            retriever_mode=''simple'' )     query = "Where is the Colosseum?"     await
    asyncio.gather(         retrieve(retriever1, query, ''<llm>''),         retrieve(retriever2,
    query, ''<simple>'')     ) asyncio.run(main())'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval mechanisms** are a central element in any RAG system. Although
    they work in different ways, all types of retrievers are based on the same principle:
    they browse an index and select the relevant nodes to build the necessary context.
    Each index type offers several retrieval modes, each providing different features
    and customization options. Regardless of the retriever type, the result that will
    be returned is in the form of a `NodeWithScore` object – a structure that combines
    a node with an associated score. The score can be useful further in the RAG flow
    because it allows us to sort the returned nodes according to their relevance.
    However, keep in mind that while all retrievers return `NodeWithScore` , not all
    of them associate a specific node score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, LlamaIndex offers multiple alternatives to accomplish a task, so
    a retriever can be constructed in several ways. The simplest path is direct construction
    from an `Index` object. Assuming that we have already dealt with document ingestion,
    the following code builds an index and then builds a retriever based on the structure
    of the index:'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, the generated retriever is of the `SummaryIndexRetriever`
    type. This is the default retriever for this index.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option is direct instantiation, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll go through a list of retrieval options that are available
    for each index type. Next to each retriever type, I’ve specified how it can be
    instantiated from the corresponding index. I warn you now that a lot of information
    has been condensed in the next section. However, it is useful information that
    you can bookmark and come back to later when you start building real applications
    with the LlamaIndex framework.
  prefs: []
  type: TYPE_NORMAL
- en: So, here’s the list of retrievers for each type of index.
  prefs: []
  type: TYPE_NORMAL
- en: The VectorStoreIndex retrievers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have two retriever options available for this index. Let’s have a look at
    how they work and how to customize them for different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: VectorIndexRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The default retriever that’s used by `VectorStoreIndex` is `VectorIndexRetriever`
    . It can easily be constructed using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, since `VectorStoreIndex` is one of the most sophisticated and widely
    used indexes, this retriever is also complex.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6* *.1* exemplifies its operating mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Node retrieval using VectorIndexRetriever
  prefs: []
  type: TYPE_NORMAL
- en: 'This retriever operates by converting queries into vectors and then performing
    *similarity-based* searches in the vector space. Several parameters can be customized
    for different use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '`similarity_top_k` : This defines the number of *top (k)* results returned
    by the retriever. This determines how many of the most similar results are returned
    for each query. For example, if we want a broader search, we can change the default
    value, which is `2` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector_store_query_mode` : This sets the query mode of the vector store. Different
    variants of external vector stores, such as *Pinecone* ( https://www.pinecone.io/
    ), *OpenSearch* ( https://opensearch.org/ ), and others, support different query
    modes. This is the mechanism by which we can make best use of their search capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filters` : Remember that in *Chapter 3* , in the *Nodes* section, we saw how
    to add metadata to our nodes? Well, we can use this metadata to narrow down the
    search scope of the retriever. We will see a practical example of this in this
    chapter, where we will use metadata filters to implement a simple system for filtering
    nodes returned by an index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha` : This one is useful when using a hybrid search mode (a combination
    of sparse and **dense search** ). We will discuss the difference between sparse
    and dense search in more detail later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparse_top_k` : The number of top results for the **sparse search** . This
    is relevant in hybrid search modes. The previous mention applies here also.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_ids` : Similar to metadata filters, but slightly coarser, `doc_ids` can
    be used to restrict the search to a specific subset of documents. For example,
    suppose the organization uses a common knowledge base that is shared by all departments.
    At the same time, however, the organization has a clear naming convention for
    documents. If the department’s name or code is found in the document name, we
    could use this parameter to limit a user’s query to documents in their department
    only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node_ids` : This parameter is similar to `doc_ids` but refers to node IDs
    within the index. This can give us even more granular control over the information
    that’s returned by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector_store_kwargs` : This parameter can pass additional arguments that are
    specific to each vector store so that they can be sent at query time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a secure design principle, security should be implemented as early as possible
    in the life cycle of an application. This is also true for an RAG application.
    For example, if we want to better control access to information, we should filter
    the information that’s processed by the application as early as possible. In an
    RAG flow, which means from the moment it is retrieved – if not earlier. There
    are ways to filter the information later in the query engine – for example, in
    post-processing or even in response synthesis – but it is much easier not to introduce
    risks in the first place by introducing information into the flow that is outside
    the user’s security context. There is also a cost issue. Since much of the processing
    in an RAG flow is based on LLM ingestion, the less information we process, the
    lower the cost.
  prefs: []
  type: TYPE_NORMAL
- en: VectorIndexAutoRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the parameters we discussed earlier regarding `VectorIndexRetriever` are
    very useful when we know exactly what we are looking for and understand the structure
    of the data very well. Unfortunately, in some situations, we will be dealing with
    complex structures or ambiguities in the indexed data.
  prefs: []
  type: TYPE_NORMAL
- en: '`VectorIndexAutoRetriever` is a more advanced form of retriever that can use
    an LLM to automatically set query parameters in a vector store based on a natural
    language description of the content and supporting metadata. This is particularly
    useful when users are unfamiliar with the structure of the data or do not know
    how to formulate an effective query. In these situations, this retriever can transform
    vague or unclear queries into more structured queries and better leverage the
    capabilities of the vector store, thus increasing the chances of finding relevant
    results. Since a detailed discussion of this mechanism would take several pages
    and I am probably digressing too much from the main topic, if you want to learn
    more about how it works, I suggest that you consult the official documentation
    at https://docs.llamaindex.ai/en/stable/examples/vector_stores/elasticsearch_auto_retriever.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: The SummaryIndex retrievers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are three retriever options available for this index. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: SummaryIndexRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This retriever can be built using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the default retriever for `SummaryIndex` . As seen in *Figure 6* *.2*
    , it has a very simple approach – it returns all nodes in the index without applying
    any filtering or sorting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Retrieving nodes using SummaryIndexRetriever
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when we want to get a complete view of the data in the index,
    without having to filter or sort the results. No relevance score is returned for
    the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: SummaryIndexEmbeddingRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can build this one with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This retriever relies on embeddings to retrieve nodes from `SummaryIndex` .
    While `SummaryIndex` itself stores nodes in plain text, this retriever uses an
    embedding model to convert these plain text nodes into embeddings when a query
    is made. Have a look at *Figure 6* *.3* to get a better view of its operating
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Inner workings of SummaryIndexEmbeddingRetriever
  prefs: []
  type: TYPE_NORMAL
- en: The embeddings are created dynamically as needed for retrieval, rather than
    being stored persistently with the index. The `similarity_top_k` parameter determines
    the number of nodes to return, based on their similarity to the query. This retriever
    is useful for finding the most relevant nodes concerning a given query by using
    similarity computation.
  prefs: []
  type: TYPE_NORMAL
- en: For each selected node, the retriever calculates a similarity score – based
    on embeddings – which is then returned alongside the node as `NodeWithScore` .
    This score is a reflection of the extent to which each node corresponds to the
    query.
  prefs: []
  type: TYPE_NORMAL
- en: SummaryIndexLLMRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This retriever can be built using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As its name suggests, this retriever uses an LLM to retrieve nodes from `SummaryIndex`
    . It uses a prompt to select the most relevant nodes. Check out *Figure 6* *.4*
    for an overview of its approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – SummaryIndexLLMRetriever in action
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wish, we can override the default prompt using the `choice_select_prompt`
    parameter. Queries are processed in batches; the size of each batch is determined
    by the `choice_batch_size` parameter. Optionally, we can also provide the `format_node_batch_fn`
    and `parse_choice_select_answer_fn` functions as parameters. These are used to
    format the batch of nodes and parse the LLM responses. The `parse_choice_select_answer_fn`
    function is also responsible for calculating node-specific relevance scores. The
    scores are determined by parsing the LLM responses. These scores are then associated
    with the corresponding nodes and returned as `NodeWithScore` . If we don’t want
    to use the default LLM, that’s not a problem: the retriever accepts `service_context`
    as a parameter. In Chapter 3 , we saw how to customize the default LLM using `ServiceContext`
    .'
  prefs: []
  type: TYPE_NORMAL
- en: This type of retriever is useful in complex search systems where LLMs can provide
    contextual and detailed answers to queries.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll talk about retrievers for `DocumentSummaryIndex` .
  prefs: []
  type: TYPE_NORMAL
- en: The DocumentSummaryIndex retrievers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this index, we only have two retrieval options. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: DocumentSummaryIndexLLMRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can build this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This retriever uses an LLM to select relevant summaries from an index of document
    summaries. You can get a better understanding of how it works by looking at *Figure
    6* *.5* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – How DocumentSummaryIndexLLMRetriever works
  prefs: []
  type: TYPE_NORMAL
- en: This retriever processes queries in batches, with each batch containing a specified
    number of nodes to send to the LLM for evaluation. The `choice_batch_size` parameter
    can be used to specify the size of a batch. The retriever can use a custom prompt
    provided via the `choice_select_prompt` parameter to determine the relevance of
    the abstracts to the query. Results are sorted by relevance and returned according
    to the number specified by `choice_top_k` . The `format_node_batch_fn` and `parse_choice_select_answer_fn`
    functions can also be specified as parameters. The first function, `format_node_batch_fn`
    , prepares the information from nodes in a format suitable for the LLM. This may
    include combining text from multiple nodes, structuring the information in a particular
    way, or adding contextual elements to help the LLM understand and evaluate the
    content. The second function, `parse_choice_select_answer_fn` , can, for example,
    determine which nodes are most relevant to the query and extract relevance scores
    or other metrics associated with each node. By analyzing the LLM response, this
    function allows the retriever to decide which nodes are most relevant to the user’s
    query. To summarize, `DocumentSummaryIndexLLMRetriever` is useful for retrieving
    useful data from a large number of documents using the natural language processing
    power of LLMs. As a useful side note, it is good to know that this retriever also
    returns the relevance score that is associated with each of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Additional observation
  prefs: []
  type: TYPE_NORMAL
- en: During my experimentation with this type of retriever, I noticed that the relevance
    scores that are assigned to each node by the LLM were consistently high, often
    reaching the maximum value of 10 (tested using GPT3.5-Turbo). For applications
    where nuanced differentiation between degrees of relevance is crucial, it might
    be beneficial to adjust the prompt or apply post-processing to the LLM’s responses
    to achieve a more balanced and nuanced distribution of relevance scores. This
    issue also underscores the importance of tailoring LLM prompts and response handling
    to suit the specific needs and contexts of different applications. We’ll talk
    more about prompt customization in *Chapter 10* .
  prefs: []
  type: TYPE_NORMAL
- en: DocumentSummaryIndexEmbeddingRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To build this retriever, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This retriever relies on embeddings to retrieve summary nodes from the index.
    *Figure 6* *.6* exemplifies its operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – DocumentSummaryIndexEmbeddingRetriever
  prefs: []
  type: TYPE_NORMAL
- en: It computes the embeddings for the query and then finds the summaries with the
    highest similarity to the query. For this method to work, the index should have
    been built with the `embed_summaries` parameters set to `True` . The `similarity_top_k`
    parameter specifies the number of summary nodes to return based on similarity.
    The retriever does not return a relevance score associated with each node.
  prefs: []
  type: TYPE_NORMAL
- en: It is effective for finding the most relevant summaries relative to a given
    query, using similarity calculation techniques based on embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The TreeIndex retrievers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a more complex index type that constructs a tree graph of nodes, as
    we saw in *Chapter 5* *, Indexing with LlamaIndex* , in the *Other index types
    in* *LlamaIndex* section.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '`TreeIndex` , by its very nature, is designed to reflect hierarchical relationships
    within data, making it a great tool for scenarios where data is naturally organized
    in a tree-like structure, such as filesystems, organizational charts, or product
    categories. That being said, the LlamaIndex implementation of this structure is
    a tree of summaries about the data. Regardless of any existing structure in the
    initial document, this index builds a parallel hierarchical structure by chunking
    it down and creating summaries at each level of the tree. Because of the recursive
    nature of `TreeSelectLeafRetriever` and `TreeSelectLeafEmbeddingRetriever` , navigating
    this structure at query time could be more computationally expensive than with
    other types of indexes. This recursive process adds computational overhead, especially
    for deep trees or large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: That being said, we have several ways to query `TreeIndex` .
  prefs: []
  type: TYPE_NORMAL
- en: TreeSelectLeafRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can construct this retriever like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also the default retriever that’s used by `TreeIndex` . Its purpose
    is to recursively navigate the index structure and identify the leaf nodes that
    are most relevant to the query being formulated. This can be seen in *Figure 6*
    *.7* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – TreeSelectLeafRetriever configured with a child_branch_factor argument
    value of 1
  prefs: []
  type: TYPE_NORMAL
- en: 'The `child_branch_factor` argument specifies the number of child nodes to be
    considered at each level of the tree. Setting a higher value can result in a more
    exhaustive search and increase the chance of finding the most relevant nodes.
    However, this has the disadvantage of increasing the computational cost and processing
    time. If no value is specified, the retriever defaults to a value of `1` . Another
    very useful parameter is `Verbose` , which, when set to `True` , causes the detailed
    selection process to be displayed. This is a very good way to understand how the
    retriever works or troubleshoot possible execution problems. The nodes that are
    returned by this retriever do not contain an associated relevance score. As this
    retriever uses an LLM for node selection, several parameters can be used to customize
    the prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`query_template` : This is a prompt template that we can use to customize queries
    for the LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_qa_template` : This is another template that’s used for text-based Q&A
    queries. It is used to get specific answers from text nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`refine_template` : This template is used to refine or enhance the initial
    answers that are obtained from the LLM. It can be used to add additional context
    or clarify answers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_template_multiple` : An alternative prompt template that allows queries
    to be formulated for multiple nodes simultaneously. It is useful when using a
    `child_branch_factor` argument that’s higher than 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll talk about `TreeSelectEmbeddingRetriever` .
  prefs: []
  type: TYPE_NORMAL
- en: TreeSelectLeafEmbeddingRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This particular kind of retriever can be built using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: As its name suggests, this retriever navigates the index by using the similarity
    of the embeddings between the query and the node text to select the relevant nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This process is recursive, navigating all levels of the tree. It works almost
    identically to `TreeSelectLeafRetriever` , with the only difference being that
    it uses embeddings for node selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters we discussed earlier are also valid here, but there is an additional
    parameter: `embed_model` . This can be used to specify a preferred embedding model.
    As with the previous retriever, the nodes that are returned by this retriever
    do not contain an associated relevance score.'
  prefs: []
  type: TYPE_NORMAL
- en: TreeAllLeafRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s the fastest way to construct this retriever:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find an explanatory diagram in *Figure 6* *.8* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Retrieving all nodes by using TreeAllLeafRetriever
  prefs: []
  type: TYPE_NORMAL
- en: This retriever is useful for its ability to analyze a large amount of data,
    ensuring that no potentially relevant information is missed in the response generation
    process. In a similar way to `SummaryIndexRetriever` , this retriever extracts
    all nodes from the index and sorts them, regardless of their position in the hierarchy.
    This is akin to a bulk retrieval but without it returning any relevance score.
  prefs: []
  type: TYPE_NORMAL
- en: TreeRootRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can build this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike `TreeAllLeafRetriever` , this retriever focuses on retrieving responses
    directly from the root nodes of the tree. It assumes that the index tree already
    stores the response. Unlike other methods that might parse information down the
    tree to extract relevant nodes, `TreeRootRetriever` relies on the fact that the
    answer is already at the root level. *Figure 6* *.9* provides a visual explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Retrieving from the root of the tree
  prefs: []
  type: TYPE_NORMAL
- en: It is effective in cases where essential information is aggregated or synthesized
    at the top level of the data structure, such as data summaries, general conclusions,
    or answers to frequently asked questions. This retriever also does not return
    relevance scores associated with nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Practical use case
  prefs: []
  type: TYPE_NORMAL
- en: A practical example would be a **clinical decision support system** ( **CDSS**
    ) in the medical field. Imagine such a system having a `TreeIndex` retriever in
    which each root node represents a specific medical question and the corresponding
    answers or clinical advice are pre-computed and stored in these root nodes. For
    example, the root nodes may store a pre-computed answer such as *Common symptoms
    of COVID-19 include fever, dry cough, tiredness, and so on* . In this scenario,
    when a doctor or patient interrogates the system with the *Symptoms of a COVID-19
    infection* query, this retriever will look at the appropriate root node and return
    the pre-computed answer without any additional processing or having to traverse
    the tree to find information.
  prefs: []
  type: TYPE_NORMAL
- en: The KeywordTableIndex retrievers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The retrieval process from `KeywordTableIndex` starts by extracting the relevant
    keywords from the query given to the retriever. Extraction can be done in several
    ways, depending on the retriever being used. Once the keywords have been extracted,
    the retriever counts their frequency in the different indexed. All retrievers
    that are available for this index operate as described in *Figure 6* *.10* . The
    only difference is the method that’s used to extract the keywords:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – KeywordTableIndex
  prefs: []
  type: TYPE_NORMAL
- en: The nodes are sorted by the number of matching keywords, usually in descending
    order of relevance, and returned as a `NodeWithScore` response.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that queries against this type of index do not return a relevance
    score associated with the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at the available retrievers for this Index.
  prefs: []
  type: TYPE_NORMAL
- en: KeywordTableGPTRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can build this type of retriever with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses an LLM query to identify relevant keywords in a query and then returns
    the nodes associated with those keywords.
  prefs: []
  type: TYPE_NORMAL
- en: KeywordTableSimpleRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This retriever can be built as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: This is a simpler method that does not use the LLM and is faster. However, it
    may be less efficient at identifying complex or contextual keywords. It uses a
    regular expression-based keyword extractor.
  prefs: []
  type: TYPE_NORMAL
- en: KeywordTableRAKERetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To define this, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previous retriever, this one uses the *RAKE method* to efficiently
    extract relevant keywords. We discussed the RAKE method in *Chapter 5* , *Indexing
    with LlamaIndex* , i *n the A simple usage model for* *KeywordTableIndex* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also several common arguments that we can use to set up the retrievers
    of `KeywordTableIndex` :'
  prefs: []
  type: TYPE_NORMAL
- en: '`query_keyword_extract_template` : This is used to change the default prompt
    that’s used to extract keywords from the text of a query. This can only be applied
    to the default mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_keywords_per_query` : This specifies the maximum number of keywords that
    can be extracted from a single query. This parameter is important to control query
    complexity and to avoid overloading the system with too many keywords.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_chunks_per_query` : This specifies the maximum number of chunks that can
    be retrieved in a query. This parameter helps limit the amount of data that can
    be processed in a single query, optimizing system performance and efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we’ll talk about how to retrieve data from knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: The KnowledgeGraphIndex retrievers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, this type of Index constructs a graph
    made up of *triplets* . Each **triplet** consists of a subject, a predicate, and
    an object. The **subject** is the entity or concept about which a statement is
    being made. The **predicate** is the relationship or verb that links the subject
    to the object, describing how the two are related, and the object is the entity
    or concept that is linked to the subject by the predicate. At the core of this
    index, there are two retrievers, `KGTableRetriever` and `KnowledgeGraphRAGRetriever`
    , both of which extract relevant nodes from a knowledge graph based on queries.
  prefs: []
  type: TYPE_NORMAL
- en: '`KGTableRetriever` is the default retriever for `KnowledgeGraphIndex` and can
    be configured in three retrieval modes: using keywords only, using embeddings
    only, or a combination of both – in hybrid mode. All modes operate as described
    in *Figure 6* *.11* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – The inner workings of KGTableRetriever
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how they work under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The retriever can be built in this mode using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: When configured in keyword mode, the retriever uses keywords extracted from
    the query to find relevant nodes containing those keywords.
  prefs: []
  type: TYPE_NORMAL
- en: Keywords are evaluated in case-sensitive mode. This means that on a hypothetical
    index, a query of the form *where is the Colosseum?* will return a correct result,
    while *where is the colosseum?* will return no nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can set it to this mode using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: In this mode, the retriever turns the query into an embedding and the system
    finds nodes in the graph whose vector representation is similar to the embedding
    of the query, even if the same keywords are not used.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This mode can be configured using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: In hybrid mode, the retriever uses both the keywords extracted from the query
    and the embeddings to find a set of relevant Nodes. It combines the results from
    both the keyword-based and embedding-based retrieval steps and removes any duplicated
    results. This approach combines the precision of keyword-based search with the
    semantic understanding of the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several customizable parameters for this type of retriever. For example,
    `query_keyword_extract_template` , `refine_template` , and `text_qa_template`
    can be used to change the default prompt for keyword extraction, the default prompt
    for query refinement, and the default prompt for text queries and answers, respectively.
    Here are some other useful parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_keywords_per_query` : This limits the number of keywords to avoid overloading
    the search process. The default value is `10` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_chunks_per_query` : This determines how many text fragments can be parsed
    in a single query. The default is `10` and any change must take into account the
    performance impact and limitations of the LLM used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_text` : The default value is `True` . This argument indicates whether
    the text of the source document should be used in queries in each relevant triplet.
    This can enrich the query with additional context but inevitably increases the
    computational cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`similarity_top_k` : When the retriever is configured in embedding or hybrid
    mode, this parameter specifies the number of similar embeddings to be considered
    in the retrieval process. The default value is `2` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`graph_store_query_depth` : This parameter controls how deep into the graph
    structure to search for relevant information. The default value is `2` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_global_node_triplets` : When set to `True` , the retriever will not limit
    itself to keywords extracted directly from the user query; instead, it will search
    for other keywords or entities in the text fragments that have already been identified
    as relevant to the initial keywords. This process helps bring an additional layer
    of knowledge to the query. By exploring the relationships and connections between
    different nodes in the graph, the retriever can access richer and more contextual
    information than would be possible by limiting itself to the original keywords.
    However, this approach is more costly in terms of computing resources and search
    time as it involves analyzing a greater number of nodes and relationships in the
    graph. For this reason, the option is disabled by default – that is, it’s set
    to `False` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_knowledge_sequence` : This parameter provides a balance between the quality
    and quantity of information presented. For example, if a query can theoretically
    generate 100 sequences of relevant knowledge, but `max_knowledge_sequence` is
    set to 30, only the most relevant 30 sequences will be presented as answers. This
    is also the default. Setting a limit ensures that the answer does not become too
    long or difficult to interpret, while still retaining enough information to be
    useful'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although they return `NodeWithScore` objects, the knowledge graph retrievers
    do not provide any score for the actual nodes. Instead, they simply return a default
    value of `1000` for each retrieved node.
  prefs: []
  type: TYPE_NORMAL
- en: If the retrievers do not find any nodes in the index based on the configured
    mode and search parameters, they will first try to identify nodes based on the
    provided keywords only. If they do not find any relevant nodes, they will return
    a single placeholder node with the text *No relationships found* and a score of
    1.
  prefs: []
  type: TYPE_NORMAL
- en: KnowledgeGraphRAGRetriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This additional retriever is a bit more special in that it operates by identifying
    key entities within a query and leveraging these to navigate the knowledge graph.
    It utilizes functions and templates for entity extraction ( `extraction entity_extract_fn`
    and `entity_extract_template` ) and synonym expansion ( `synonym_expand_fn` and
    `synonym_expand_template` ) to enrich the query with a broader context of related
    terms and concepts. The retriever traverses the graph to a specified depth – `graph_traversal_depth`
    – based on these entities and their synonyms, constructing a knowledge sequence
    relevant to the query.
  prefs: []
  type: TYPE_NORMAL
- en: This retriever can operate in various modes and can be configured by setting
    `retriever_mode` , allowing for flexibility in its approach to finding relevant
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like `KGTableRetriever` , this retriever has three operating modes: `keyword`
    , `embedding` , and `keyword_embedding` .'
  prefs: []
  type: TYPE_NORMAL
- en: A note regarding retrieval modes
  prefs: []
  type: TYPE_NORMAL
- en: As of January 2024, in LlamaIndex v0.9.25, only the keyword retrieval mode was
    implemented.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the retriever features the `with_nl2graphquery` option, which,
    when enabled, combines **Natural Language to Graph Query** ( **NL2GraphQuery**
    ) capabilities, enhancing its ability to interpret and respond to complex queries.
    NL2GraphQuery is a process that converts natural language queries into graph-based
    query languages. This is achieved via a combination of entity extraction, synonym
    expansion, and graph traversal techniques. This parameter is set to `False` by
    default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some other parameters that we may wish to customize:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_knowledge_sequence` : Sets a limit on the number of knowledge sequences
    included in the response, balancing detail with clarity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_entities` : Specifies the maximum number of entities to extract from the
    query, defaulting to `5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_synonyms` : Determines the maximum number of synonyms to expand for each
    entity, with a default value of `5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synonym_expand_policy` : Controls the policy for synonym expansion, either
    *union* or *intersection* , with *union* as the default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`entity_extract_policy` : Sets the policy for entity extraction, also either
    *union* or *intersection* , defaulting to *union*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` : As usual, this is used to enable or disable the printing of debug
    information, aiding in the understanding of the Retriever’s operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`graph_traversal_depth` : Determines the depth of the traversal within the
    knowledge graph. By default, this is set to `2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick note
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s something important to highlight for all retrievers that use LLMs and
    accept parameters for customization prompts: All of these parameters are of the
    `BasePromptTemplate` type. We will talk more about the structure of this class
    and how to use it in *Chapter 10* , *Prompt Engineering Guidelines and* *Best
    Practices* .'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve covered the differences between each type of retriever. Now,
    let’s see what they all have in common.
  prefs: []
  type: TYPE_NORMAL
- en: Common characteristics shared by all retrievers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All retrievers accept either a query directly or a `QueryBundle` object as a
    parameter. `QueryBundle` is a universal mechanism that can be used for more advanced
    use cases, such as searching based on embeddings or searching for images and/or
    text in a multimodal scenario.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, all retrievers accept the `callback_manager` argument. We will
    discuss this mechanism in more detail in *Chapter 10* , *Prompt Engineering Guidelines
    and* *Best Practices* .
  prefs: []
  type: TYPE_NORMAL
- en: These are the basic building blocks for the retrieval logic of our RAG applications.
    If we want a generic and easy-to-build solution, we can use them directly. However,
    for more complex cases, there are several advanced retrieval modules in LlamaIndex
    that either combine the functionality of the basic retrievers or add new features
    to the mix. We will discuss some of them later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, some retrievers use either embedding models or LLM queries
    to identify the most relevant nodes. However, at their core, all of the retriever
    types listed here are subclasses of `BaseRetriever` . This means that they all
    inherit the main `retrieve()` method, as well as `aretrieve()` , for asynchronous
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss the asynchronous operation next.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient use of retrieval mechanisms – asynchronous operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the sake of simplicity, all the code examples we have discussed so far have
    used **synchronous methods** . Although the synchronous – or **serialized** –
    mode of operation is linear, easy to understand, and predictable, in modern applications,
    performance and low latency are very important to provide a great user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that LlamaIndex already offers – in most cases – **asynchronous
    execution** alternatives. Here’s a simple example of asynchronous execution for
    two Retrievers defined over `KeywordTableIndex` :'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code executes the two retrievals in parallel. Of course, being
    a trivial example with a very small dataset, the performance benefits of **asynchronous
    operation** will not be significant in this case.
  prefs: []
  type: TYPE_NORMAL
- en: However, in the context of a commercial application that frequently calls retrievers
    and operates numerous complex queries over many indexed nodes, the benefits will
    be substantial. Asynchronous operation improves performance, uses resources more
    efficiently, reduces latency, and generally provides a more natural user experience
    by reducing waiting times.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to talk about the more advanced retrieval methods.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Building more advanced retrieval mechanisms</title>
  prefs: []
  type: TYPE_NORMAL
- en: Building more advanced retrieval mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core.vector_stores.types import (     FilterOperator, FilterCondition)
    filters = MetadataFilters(     filters=[         MetadataFilter(             key="department",
                value="Procurement"         ),         MetadataFilter(             key="security_classification",
                value=<user_clearance_level>,             operator=FilterOperator.LTE
            ),     ],     condition=FilterCondition.AND ) from llama_index.core.selectors.llm_selectors
    import LLMSingleSelector options = [     "option 1: this is good for summarization
    questions",     "option 2: this is useful for precise definitions",     "option
    3: this is useful for comparing concepts", ] selector = LLMSingleSelector.from_defaults()
    decision = selector.select(     options,     query="What''s the definition of
    space?" ).selections[0] print(decision.index+1) print(decision.reason) from llama_index.core.selectors
    import PydanticMultiSelector from llama_index.core.retrievers import RouterRetriever
    from llama_index.core.tools import RetrieverTool from llama_index.core import
    (     VectorStoreIndex, SummaryIndex, SimpleDirectoryReader) documents = SimpleDirectoryReader("files").load_data()
    vector_index = VectorStoreIndex.from_documents([documents[0]]) summary_index =
    SummaryIndex.from_documents([documents[1]]) vector_retriever = vector_index.as_retriever()
    summary_retriever = summary_index.as_retriever() vector_tool = RetrieverTool.from_defaults(
        retriever=vector_retriever,     description="Use this for answering questions
    about Ancient Rome" ) summary_tool = RetrieverTool.from_defaults(     retriever=summary_retriever,
        description="Use this for answering questions about dogs" ) retriever = RouterRetriever(
        selector=PydanticMultiSelector.from_defaults(),     retriever_tools=[         vector_tool,
            summary_tool     ] ) response = retriever.retrieve(     "What can you
    tell me about the Ancient Rome?" ) for r in response:     print(r.text) retriever.retrieve("What
    can you tell me about the Ancient Rome?") retriever.retrieve("Tell me all you
    know about dogs") retriever.retrieve("Tell me about dogs in Ancient Rome") from
    llama_index.core.indices.query.query_transform.base import DecomposeQueryTransform
    decompose = DecomposeQueryTransform() query_bundle = decompose.run(     "Tell
    me about buildings in ancient Rome" ) print(query_bundle.query_str) What were
    some famous buildings in ancient Rome? from llama_index.question_gen.openai import
    OpenAIQuestionGenerator from llama_index.core.tools import RetrieverTool, ToolMetadata
    from llama_index.core import (     VectorStoreIndex, SummaryIndex,     SimpleDirectoryReader,
    QueryBundle) documents = SimpleDirectoryReader("files").load_data() vector_index
    = VectorStoreIndex.from_documents(     [documents[0]] ) summary_index = SummaryIndex.from_documents([documents[1]])
    vector_tool_metadata = ToolMetadata(     name="Vector Tool",     description="Use
    this for answering questions about Ancient Rome" ) summary_tool_metadata = ToolMetadata(
        name="Summary Tool",     description="Use this for answering questions about
    dogs" ) vector_tool = RetrieverTool(     retriever=vector_index.as_retriever(),
        metadata=vector_tool_metadata ) summary_tool = RetrieverTool(     retriever=summary_index.as_retriever(),
        metadata=summary_tool_metadata ) question_generator = OpenAIQuestionGenerator.from_defaults()
    query_bundle = QueryBundle(     query_str="Tell me about dogs and Ancient Rome")
    sub_questions = question_generator.generate(     tools=[vector_tool.metadata,
    summary_tool.metadata],     query=query_bundle ) for sub_question in sub_questions:
        print(f"{sub_question.tool_name}: {sub_question.sub_question}") Summary Tool:
    What are the different breeds of dog? Summary Tool: What was the role of dogs
    in ancient Rome? Vector Tool: What were the most important events in Ancient Rome?
    Vector Tool: What were the most famous buildings in ancient Rome?'
  prefs: []
  type: TYPE_NORMAL
- en: Now we understand the basic components offered by LlamaIndex, we can build increasingly
    sophisticated solutions. On one hand, the retrievers we have discussed already
    provide efficient solutions for knowledge base querying and context enhancement
    in an RAG flow. On the other hand, we’ll see that there are many more advanced
    retrieval methods that either use specific techniques or ingeniously combine the
    retrievers already discussed.
  prefs: []
  type: TYPE_NORMAL
- en: The naive retrieval method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LlamaIndex provides fast query methods by default. As we have seen, in just
    a few lines of code, we can ingest documents, create nodes and, for example, build
    a `VectorStoreIndex` retriever, which we can then just as easily query to return
    the most relevant parts using a retriever that uses similarity measurement techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The method is very simple and easy to implement. However, it is not an ideal
    method in all situations. More often than not, the **naive method** , as it is
    usually called, produces mediocre rather than **state-of-the-art** ( **SOTA**
    ) solutions.
  prefs: []
  type: TYPE_NORMAL
- en: To use an analogy…
  prefs: []
  type: TYPE_NORMAL
- en: It’s pretty much like using a hammer for all kinds of repairs in a house. The
    hammer is an essential and easy-to-use tool, but it is not always the best solution
    for every problem. Similarly, using a simplified method of questioning may be
    effective for basic situations but will not be as effective for more complex situations
    or specific needs that require a greater degree of finesse and adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: In these more complex cases, it is necessary to explore more advanced and tailored
    solutions, which may involve adapting the retrieval algorithms or combining them
    in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: Also, for large datasets, naive methods can be inefficient, either returning
    too many irrelevant results or missing important information. They can also underperform
    in terms of response time and resource consumption.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, in a real-world situation, data can vary significantly in terms
    of quality, structure, and format. Simple methods are not always able to manage
    this diversity and extract valuable information.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the specific information we are looking for is scattered in
    small chunks that are randomly distributed throughout the document, the results
    will be below expectations. In the next few sections, we’ll discuss some more
    advanced retrieval methods that can provide much better results in various specific
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing metadata filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A very simple but also effective retrieval mechanism is filtering the retrieved
    nodes by **metadata** . We’ll tackle a practical problem that’s usually encountered
    in an organization and for which the retrieval functions in LlamaIndex can provide
    a solution.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how to implement a retrieval system that filters the returned nodes
    according to the user’s department. Similar to the concept of polymorphism in
    object-oriented programming, it often happens that the same concept has different
    definitions, depending on the area of use.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the user is looking for the definition of an incident in an
    organizational knowledge base. However, the term *incident* may have a different
    definition for those who deal with information security than for those who deal
    with IT service operations. Let’s have a look at how we can implement a form of
    polymorphism in a retrieval mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must take care of the necessary imports and define a mapping of users
    from llama_index.core.vector_stores.types import MetadataFilter, MetadataFilters
    from llama_index.core import VectorStoreIndex from llama_index.core.schema import
    TextNode user_departments = {"Alice": "Security", "Bob": "IT"} to departments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we must define two nodes that both store the definition of the concept
    of incident. The difference is in nodes = [     TextNode(         text=(             "An
    incident is an accidental or malicious event that has the potential to cause unwanted
    effects on the security of our IT assets."),         metadata={"department": "Security"},
        ),     TextNode(         text=("An incident is an unexpected interruption
    or             degradation of an IT service."),         metadata={"department":
    "IT"},     ) ] Next, we must define the function that''s responsible for filtering
    and retrieval: def show_report(index, user, query):     user_department = user_departments[user]
        filters = MetadataFilters(         filters=[             MetadataFilter(key="department",
                    value=user_department)         ]     )     retriever = index.as_retriever(filters=filters)
        response = retriever.retrieve(query)     print(f"Response for {user}: {response[0].node.text}")
    the metadata, which specifies the department where the definition applies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, if we run the same index = VectorStoreIndex(nodes) query = "What is an
    incident?" show_report(index, "Alice", query) show_report(index, "Bob", query)
    Response for Alice: An incident is an accidental or malicious event that has the
    potential to cause unwanted effects on the security of our IT assets. Response
    for Bob: An incident is an unexpected interruption or degradation of an IT service.
    query in the context of each user, we will get different answers, depending on
    the department each user belongs to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: See how simple that was? The same mechanism can be used, for example, to control
    access to information and define security rules.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a knowledge base system shared by several clients on a multi-tenancy
    model, we can restrict access by implementing `MetadataFilters` .
  prefs: []
  type: TYPE_NORMAL
- en: 'The code you saw earlier only does simple filtering: it restricts the search
    to nodes for which the value of the `department` key is equal to the user’s department.
    But there are also more complex filtering variants that use operators based on
    the `FilterOperator` class. Unfortunately, the default vector store in LlamaIndex
    only supports the `EQ` (equal) operator – that is, it can only apply filters where
    the value of a key is equal to a certain parameter. If we use a more sophisticated
    version of vector store (such as Pinecone or ChromaDB), we can use the full range
    of operators available in `FilterOperator` , as listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Symbolic Operator** | **Programming Equivalent** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EQ | == | Equal (default) |'
  prefs: []
  type: TYPE_TB
- en: '| GT | > | Greater than |'
  prefs: []
  type: TYPE_TB
- en: '| LT | < | Less than |'
  prefs: []
  type: TYPE_TB
- en: '| NE | != | Not equal to |'
  prefs: []
  type: TYPE_TB
- en: '| GTE | >= | Greater than or equal to |'
  prefs: []
  type: TYPE_TB
- en: '| LTE | <= | Less than or equal to |'
  prefs: []
  type: TYPE_TB
- en: '| IN | in | In array |'
  prefs: []
  type: TYPE_TB
- en: '| NIN | nin | Not in array |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – A complete list of operators available for FilterOperator
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example where we use filter operators and filter aggregation conditions
    to implement more complex scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we implemented a very simple access control mechanism
    based on clearance level and security classification. Only nodes that belong to
    a particular department and have a classification level less than or equal to
    the user’s access level will be returned. We’ll talk about another method next.
  prefs: []
  type: TYPE_NORMAL
- en: Using selectors for more advanced decision logic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an advanced user interaction system, the user may employ a wide variety of
    queries. For example, they may ask a very specific question, looking for a precise
    definition. At other times, the user may be looking for more general information
    or may be asking the system to summarize or compare two documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these complex situations, which retriever should be used? It becomes clear
    that the best implementation is based on the combined strength of many retrieval
    systems. But this implicitly means that the RAG application must have an internal
    selection mechanism to choose the most appropriate retriever according to the
    query. This brings us to the topic of this section: the use of **selectors** .'
  prefs: []
  type: TYPE_NORMAL
- en: 'In LlamaIndex, they come in five different flavors: `LLMSingleSelector` , `LLMMultiSelector`
    , `EmbeddingSingleSelector` , `PydanticSingleSelector` , and `PydanticMultiSelector`
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'The way they work is slightly different. As the name suggests, some rely on
    the decision capabilities of an LLM, others select a particular option from a
    list of options based on a similarity calculation, and others use Pydantic objects
    to return a selection. Some return a single option from a list; others may return
    multiple selections from a list of options. In the end, however, their result
    is more or less the same: they help us implement advanced conditional logic in
    the applications we develop.'
  prefs: []
  type: TYPE_NORMAL
- en: That is because they can evaluate complex conditions and decide which logic
    branch the application should follow – just like an *IF* decision block, but able
    to handle more complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram can help us better understand the role a selector plays
    in the logic of an RAG application. *Figure 6* *.12* , provides a visual representation
    of how `LLMSingleSelector` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Visualizing LLMSingleSelector
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a very simple implementation of a selector that uses an LLM to return
    a single option from a list of predefined options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first part of the code, we defined the options as a list of strings
    to be sent to the LLM via the `.` `select()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: The `.select()` method takes the defined options and the user query as arguments.
    Under the hood, the selector uses a specially constructed prompt to ask the LLM
    to choose the best option from the list based on the query.
  prefs: []
  type: TYPE_NORMAL
- en: As a response, the selector returns a `SingleSelection` object containing the
    number of the selected option and a justification for the selection made. As you
    can see, the selector is not something specific to retrievers. We haven’t even
    defined a retriever in this example.
  prefs: []
  type: TYPE_NORMAL
- en: This is because I wanted to show that the mechanism is generic and can be used
    for absolutely any conditional logic we want to implement in the application.
    The returned option number could help us to choose from a list of parsers, indexes,
    retrievers, and so on. In this simple version, the selector simply chooses from
    a list of strings defining the available options. However, there is a more advanced
    form of selection that involves the use of the `ToolMetadata` class. But to understand
    this concept, we first need to clarify what a **tool** is.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An essential element in any **agentic functionality** , where the application
    decides which method to use depending on the context, is a generic container.
    It may contain different functionalities that can be called by the application
    at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a rich collection of tools already developed and available in LlamaHub:
    https://llamahub.ai/?tab=tools . They can perform various specific functions,
    from composing and sending emails to querying various APIs or interacting with
    the computer’s filesystem. We will talk much more about the use of tools in implementing
    **agents** in *Chapter 8* , *Building Chatbots and Agents with LlamaIndex* , where
    we will build our PITS chatbot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, I want to show you how we can encapsulate a retriever in a tool container,
    and then use selectors to implement an adaptive retrieval mechanism. We will focus
    on the `RetrieverTool` class, which takes two important arguments: a retriever
    and a textual description of the retriever. Based on the description, the selector
    decides, for example, whether to use one retriever or another for a particular
    query. We define a `RouterRetriever` object on top of each retriever we build.
    This `RouterRetriever` is a complex decision mechanism that uses the selector
    to decide which retriever to use depending on the situation. The most important
    arguments to give it are the selector and the options to choose from – in the
    form of `RetrieverTool` objects. Let’s see how we can implement this in code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we took the two sample files from the `files` subfolder. The first file
    contains information about ancient Rome and the second is a generic text about
    dogs. Then, we created an index for each file and from each index, we created
    a retriever. Now, we must define the tools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have wrapped each retriever into `RetrieverTool` and added
    a clear description for the selector to use. Next, we must build `RouterRetriever`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s all we need to do. From this point on, every time we query this dynamic
    retriever, the selector will determine which individual retriever to use to return
    the context. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will use `vector_tool` for retrieval. Now, take a look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will call `summary_tool` . Because we used `PydanticMultiSelector` , we
    can also handle situations where both retrievers should be used, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike `PydanticSingleSelector` , `PydanticMultiSelector` can simultaneously
    select multiple options from the selector list, covering multiple use cases. Similarly,
    we can also define more complex routers at the query engine level by using `RouterQueryEngine`
    . We will discuss this in more detail in *Chapter 7* .
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to cover a few other advanced forms of retrievers.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming and rewriting queries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we saw how we can use selectors and the router concept
    to let the application decide which retriever to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another very powerful tool that our RAG application can use is the `QueryTransform`
    construct. This allows us to rewrite and modify a query before using it to interrogate
    the index, as shown in *Figure 6* *.13* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – QueryTransform improving the retrieval process
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine a scenario where we might need the functionality provided by `QueryTransform`
    .
  prefs: []
  type: TYPE_NORMAL
- en: Practical example
  prefs: []
  type: TYPE_NORMAL
- en: '**A chatbot designed to provide technical support for complex software** :
    Users often describe their problems in vague or non-technical terms. `QueryTransform`
    can interpret these descriptions, break them down into more specific sub-queries,
    or enrich them with technical terms that better match the documentation. For example,
    a query of the form *My computer keeps freezing* could be transformed into a more
    specific query, such as *Troubleshooting steps for operating* *system freezes*
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several variations of `QueryTransform` that we can use. Each has
    its specific role in augmenting the information retrieval process. Let’s look
    at each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IdentityQueryTransform` : This is a basic transform that does not modify the
    query. It returns the query as it was received, without any transformation. It’s
    useful for maintaining default or basic behavior where no specific transformations
    are required'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HyDEQueryTransform` : **Hypothetical Document Embeddings** ( **HyDE** ) transforms
    the query into a hypothetical document generated by an LLM. The idea is to generate
    hypothetical query answers and use them as embedding strings. This can help improve
    the relevance of the results. This method filters out inaccurate details while
    grounding the generated response in the actual content. You can read more about
    the benefits of using this technique here: *Gao, Luyu; Ma, Xueguang; Lin, Jimmy;
    Callan, Jamie (2022). “Precise Zero-Shot Dense Retrieval without Relevance Labels”.
    arXiv:2212.10496v1 [* *cs.IR].* https://arxiv.org/abs/2212.10496'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecomposeQueryTransform` : This type of transform decomposes a complex query
    into a simpler and more focused subquery. This can be useful to make queries easier
    for the index to process and increase the chances of finding relevant nodes, especially
    if the index structure is not optimized for complex or ambiguous queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ImageOutputQueryTransform` : This method adds instructions for formatting
    results as images, such as generating HTML *<img>* tags. It is useful for cases
    where query results are expected to be displayed as images or when the output
    is just an intermediate step in more complex logic and has to be further processed
    in a particular format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StepDecomposeQueryTransform` : This is similar to `DecomposeQueryTransform`
    but it adds an extra layer by taking previous reasoning or context into account
    when decomposing the query. This can help to continually refine the query based
    on feedback or previous results, thus improving retrieval accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these transformations improves a system’s ability to process and respond
    to queries in a more efficient way that is better tailored to the user’s specific
    needs or the nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at a practical example to better understand how they work:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we run the code, `DecomposeQueryTransform` takes in our original – and
    otherwise very ambiguous – query. It then uses a specially designed prompt to
    generate a more precise query using the LLM. In our example, the output should
    look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: You can immediately see that the new query is much clearer and greatly increases
    the chances of the retriever generating a correct context from the index.
  prefs: []
  type: TYPE_NORMAL
- en: Creating more specific sub-queries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another useful approach to augmenting a query is to generate sub-queries. Sometimes,
    an ambiguous or very complex question becomes much clearer when it is split into
    several specific questions. LlamaIndex comes to our rescue this time too. `OpenAIQuestionGenerator`
    is a mechanism that’s designed exactly for this operation. Here is the code we
    used as an example earlier when we talked about selectors and routers. This time,
    we will adapt it a bit to demonstrate how `OpenAIQuestionGenerator` works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, the code is identical to the earlier example. We read the two files
    from the `files` subfolder and then create an index for each document:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each index, we define a name and a description in a `ToolMetadata` structure.
    This information will be used by `OpenAIQuestionGenerator` to *understand* what
    role each retriever has and what type of questions it might answer. Next, we will
    define the two retrievers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now follows the generation of sub-queries. First, we initialize an `OpenAIQuestionGenerator`
    object with the default settings. Then, we build a `QueryBundle` object that will
    contain the original query received from the user. This `QueryBundle` will be
    sent as an argument to the question generator:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the subquery generator takes two arguments – a list of tools
    at its disposal, and the original query from which it can build more specific
    queries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, the generated questions might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`OpenAIQuestionGenerator` took the initial query and, using the LLM, returned
    a list of more specific questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer that’s returned in the `sub_questions` variable is a list of `SubQuestion`
    items - a simple class with two attributes: `tool_name` and `sub_question` . We
    can now iterate through all the items in the list and get the tools and questions
    we are looking for.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, using more specific queries, as in the preceding example, is likely
    to generate more context with the retriever and therefore likely to get a better-quality
    answer from `QueryEngine` .
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to `OpenAIQuestionGenerator` , it is good to know that there
    is also `LLMQuestionGenerator` , which, as its name suggests, allows us to use
    any LLM. Another difference between the two is that `LLMQuestionGenerator` uses
    a special parser to structure the output, unlike `OpenAIQuestionGenerator` , which
    relies on the generation of Pydantic objects.
  prefs: []
  type: TYPE_NORMAL
- en: The same collection of question generators also includes `GuidanceQuestionGenerator`
    . This mechanism uses an LLM to create helper questions to guide the query engine.
    It can be extremely useful when you’re dealing with complex queries that need
    to be broken down and processed in a particular order.
  prefs: []
  type: TYPE_NORMAL
- en: Once these sub-queries have been generated, they can be used in a specially
    constructed query engine. We will discuss this step in more detail in *Chapter
    7* , *Querying Our Data, Part 2 – Postprocessing and Response Synthesis* , when
    we talk about `SubQuestionQueryEngine` .
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll talk about two important concepts related to information retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Understanding the concepts of dense and sparse retrieval</title>
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concepts of dense and sparse retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'pip install rank-bm25 pip install llama-index-retrievers-bm25 from llama_index.retrievers.bm25
    import BM25Retriever from llama_index.core.node_parser import SentenceSplitter
    from llama_index.core import SimpleDirectoryReader reader = SimpleDirectoryReader(''files'')
    documents = reader.load_data() splitter = SentenceSplitter.from_defaults(     chunk_size=60,
        chunk_overlap=0,     include_metadata=False ) nodes = splitter.get_nodes_from_documents(
        documents ) retriever = BM25Retriever.from_defaults(     nodes=nodes,     similarity_top_k=2
    ) response = retriever.retrieve("Who built the Colosseum? ") for node_with_score
    in response:     print(''Text:''+node_with_score.node.text)     print(''Score:
    ''+str(node_with_score.score))'
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, retrieval methods are a critical component of RAG systems.
    They enable the identification and ranking of relevant content for queries, which
    is the first step in generating useful answers from an LLM. During your journey
    into RAG application development, you’re likely to encounter two dominant retrieval
    paradigms – **dense retrieval** and **sparse retrieval** . Because it is important
    to understand these concepts, this section will focus on their characteristics,
    trade-offs, and the benefits of combining them.
  prefs: []
  type: TYPE_NORMAL
- en: Dense retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dense retrieval method relies on embedding vectors to represent text in
    a continuous, high-dimensional space. Using embedding models, texts are **encoded**
    into fixed-length numerical vectors that are intended to capture semantic meaning.
    Queries are also encoded so that the similarity between them and the node vectors
    can be measured using geometric operations. In dense retrieval, nodes are embedded
    in vectors and stored in a specialized index such as `VectorStoreIndex` .
  prefs: []
  type: TYPE_NORMAL
- en: We call them **dense** because these vectors are typically densely populated
    with non-zero values, representing rich and nuanced semantic information in a
    compact form. During retrieval, incoming queries are dynamically embedded and
    used to retrieve the top-k nodes using similarity search algorithms, such as those
    discussed in *Chapter 5* .
  prefs: []
  type: TYPE_NORMAL
- en: This approach has several advantages, particularly in terms of semantic understanding,
    speed, and scalability. Nodes that convey similar meanings tend to cluster closer
    together. Also, the words themselves do not have to match perfectly. Synonyms
    and polysemous words don’t affect precision as much.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized indexing solutions, such as those provided by a Pinecone vector
    database ( https://www.pinecone.io/product/ ), also allow lightning-fast similarity
    searches over millions of vectors. Latencies range from milliseconds to less than
    a second and scaling is easily achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, however, several drawbacks associated with dense search:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational cost** : Embedding and indexing large volumes of data can be
    computationally expensive and time-consuming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A trade-off between precision and recall** : Dense retrieval systems can
    sometimes favor recall over precision or vice versa, depending on how the embedding
    model is tuned. Finding the right balance between retrieving all relevant documents
    and not retrieving too many irrelevant documents can be difficult.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Difficulty in dealing with long documents** : Dense models that generate
    fixed-length vectors can sometimes struggle with very long content, where important
    information can be diluted or lost in the embedding process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logical reasoning gaps** : While these methods are excellent at capturing
    semantic similarity, they typically lack logical reasoning capabilities. This
    means that they can identify documents that are semantically similar to the query
    but may struggle to understand context or logical relationships that require reasoning
    beyond this pattern matching. As a result, they may retrieve documents that are
    superficially related to the query but not truly relevant to the user’s intent,
    especially in cases where the query requires an understanding of complex relationships
    or nuanced reasoning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependence on model quality** : The effectiveness of a dense retrieval system
    is highly dependent on the quality of the underlying embedding model. Poorly trained
    models can result in suboptimal retrieval performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we’ll talk about sparse retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sparse retrieval methods associate documents with keywords. These methods are
    based on exact keyword matching or overlaps between the query and the documents.
  prefs: []
  type: TYPE_NORMAL
- en: The general process involves indexing documents by analyzing them for important
    terms. These keywords are then recorded in inverted indexes, which are data structures
    used to quickly retrieve documents containing a given keyword.
  prefs: []
  type: TYPE_NORMAL
- en: During the retrieval phase, queries are searched against these inverted indexes
    to find documents that share keywords with the query. Documents are ranked based
    on the number of common terms identified between the query and each indexed document.
    One of the most common techniques used in sparse retrieval is the **Term Frequency
    – Inverse Document Frequency** ( **TF-IDF** ) method.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF in sparse retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TF-IDF is a numerical statistic that reflects how important a word is to each
    document in a collection of documents. This method transforms text into a numerical
    representation that captures the significance of words in documents, taking into
    account both their frequency in individual documents and across the entire collection
    of documents.
  prefs: []
  type: TYPE_NORMAL
- en: '**Term Frequency** ( **TF** ) measures how often a term occurs in a document,
    normalized by the total number of terms in the document. It’s calculated by dividing
    the number of times a particular term – that is, a word – appears in a document
    by the total number of words in that document. This indicates the importance of
    the term within the specific document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverse Document Frequency** ( **IDF** ) assesses the importance of the term
    across the collection. It is calculated by taking the logarithm of the ratio of
    the total number of documents to the number of documents containing the term.
    This helps to downplay the importance of terms that occur very frequently in many
    documents. Common terms such as *the* or *is* appear in many documents and are
    less informative, so they have lower IDF scores. Unique terms have higher IDF
    scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **TF-IDF score** , which is obtained by multiplying the TF by the IDF,
    represents the importance of each term in a document, adjusted for its commonness
    across the collection. In sparse retrieval, each document is represented as a
    vector in a high-dimensional space, where each dimension corresponds to a unique
    term and the value is the TF-IDF score: https://en.wikipedia.org/wiki/Tf%E2%80%93idf
    .'
  prefs: []
  type: TYPE_NORMAL
- en: We call it *sparse* because, in this high-dimensional vector space, most dimensions
    (terms) will have a value of zero for any given document, indicating that most
    terms in the collection do not appear in that document. If we were to visualize
    these vectors, this would result in a *sparse* representation, with many zeros,
    as most documents contain only a small subset of the total vocabulary of the collection.
  prefs: []
  type: TYPE_NORMAL
- en: During retrieval, a query is also converted into its TF-IDF vector representation.
    The relevance of each document to the query is calculated using measures such
    as cosine similarity, and the documents are ranked accordingly. The top-ranked
    documents with the highest similarity scores to the query are then returned as
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse retrieval methods such as TF-IDF are particularly effective for tasks
    where exact term matching is important. However, they may not capture the semantic
    meaning of the text or the context in which terms are used, which can be addressed
    by more advanced retrieval techniques such as dense retrieval methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you’ve probably guessed, they have several advantages compared to dense
    retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficient handling of large datasets** : Sparse retrieval methods, such as
    TF-IDF, are generally more efficient at handling large datasets. The inverted
    index structure allows fast search and retrieval of documents based on keyword
    matching, making it suitable for large collections of text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High precision** : Sparse methods often provide high accuracy in scenarios
    where the exact matching of terms is critical. They excel at retrieving documents
    that contain specific keywords present in the user’s query, which is beneficial
    in applications where keyword specificity is essential'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplicity and interpretability** : Sparse retrieval methods are conceptually
    simpler and more interpretable than dense methods. The fact that they rely on
    explicit keyword frequencies makes it easier to understand why certain documents
    are retrieved in response to a query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Less resource intensive** : Unlike dense retrieval, sparse methods do not
    require complex neural network models to generate embeddings. This makes them
    less resource-intensive in terms of computing power and memory requirements. This
    means they’re easier to deploy and maintain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Less dependence on model variability** : Because sparse retrieval doesn’t
    depend on the nuances of machine learning models to the same extent as dense retrieval,
    it’s generally more robust to variations in model quality. Performance is more
    predictable and consistent across different datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparse methods also have their limitations. Some of the most important are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of semantic understanding** : Sparse methods may not capture the semantic
    relationships between words. They may miss documents that are contextually relevant
    but do not share exact keyword matches with the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vulnerability to synonymy and polysemy** : These methods struggle with synonymy
    – different words with similar meanings – and polysemy – words with multiple meanings
    – leading to potential misses or irrelevant retrievals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failure to capture context and nuance** : Sparse retrieval does not effectively
    capture the broader context or nuances in language that can be critical to understanding
    the true intent behind a query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing sparse retrieval in LlamaIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a core level, constructs such as `KeywordTableIndex` can already be considered
    a basic form of sparse retrieval. After all, they share most of the principles
    and methods described above. However, there are even more advanced sparse retrieval
    capabilities available in LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: A perfect example is `BM25Retriever` , which implements the **Best Matching
    25** ( **BM25** ) retrieval algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: BM25, a refinement of the TF-IDF method, is a more sophisticated algorithm that’s
    used for sparse retrieval. Unlike TF-IDF, BM25 takes into account both term frequency
    and document length, providing a more nuanced approach to document relevance scoring.
    With this retriever, nodes are ranked based on their BM25 scores relative to the
    query. The top-k nodes with the highest scores are returned as query results,
    providing users with the most relevant results.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example of how we can use `BM25Retriever` .
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this particular retriever, you’ll need to install the required Python
    package and the corresponding LlamaIndex integration package by running the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the `rank-bm25` package, you can test it with this sample
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re using the two initial sample files containing data about ancient Rome
    and different breeds of dogs. In this example, I’ve used `SentenceSplitter` ,
    configured with a relatively small chunk size. That is because the sample file
    is small in size and I wanted to produce more granular nodes structured as sentences
    to better exemplify the workings of `BM25Retriever` . Next, let’s implement the
    retriever:'
  prefs: []
  type: TYPE_NORMAL
- en: After chunking the two documents, we use the retriever to apply the BM25 algorithm
    and retrieve the two most relevant chunks relative to our query about the Colosseum.
  prefs: []
  type: TYPE_NORMAL
- en: You can further experiment with this sample and try to adjust the `similarity_top_k`
    parameter, the query, or the chunking strategy to better understand how this retriever
    works.
  prefs: []
  type: TYPE_NORMAL
- en: When should we use sparse retrieval instead of dense retrieval?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s consider a practical example of when sparse retrieval might give better
    results than dense retrieval in an RAG application.
  prefs: []
  type: TYPE_NORMAL
- en: A practical use case for sparse retrieval
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we’ve built a system for retrieving legal documents. In this scenario,
    user queries would likely include precise legal terms, citations, or specific
    phrases found in legal texts. Let’s assume a user inputs a query such as, “ *Article
    45 of the GDPR regarding personal data transfers on the basis of an adequacy decision.*
    ” This query contains specific phrases, such as “Article 45” and “GDPR,” which
    are likely to be found in relevant legal documents exactly in this form.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse search is likely to provide very accurate results for such a query. It
    will accurately locate documents that contain the specific article from the GDPR,
    reducing noise and irrelevant retrievals. Given that legal documents often have
    a structured format, with different sections and articles, sparse retrieval methods
    can efficiently parse through this structured data and retrieve nodes based on
    direct references found in the query.
  prefs: []
  type: TYPE_NORMAL
- en: Because dense retrieval methods tend to prioritize general meaning over exact
    term matching, they may produce less accurate results in such a specialized, keyword-specific
    query.
  prefs: []
  type: TYPE_NORMAL
- en: Unless trained specifically on legal texts, an embedding model used for dense
    retrieval might struggle to accurately interpret and match the complex legal jargon
    and specific citation styles used in legal queries.
  prefs: []
  type: TYPE_NORMAL
- en: When would dense retrieval be a better choice?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here’s another practical example.
  prefs: []
  type: TYPE_NORMAL
- en: A typical use case where dense retrieval would most likely produce better results
    would be a customer support chatbot designed to understand and respond to a wide
    range of customer queries. Let’s say the chatbot is tasked with assisting users
    with various issues related to technical products, such as hardware troubleshooting,
    software features, usage tips, and general inquiries about products and services.
  prefs: []
  type: TYPE_NORMAL
- en: A user might ask a question such as “ *My laptop battery is draining really
    quickly, even when I’m not using it much. What can I do about it?* ” Because dense
    search excels at understanding the semantic context of queries, in this case,
    it could understand the broader meaning behind phrases such as “battery drains
    really fast” and relate them to similar problems, even if the exact phrase isn’t
    in the knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse methods, on the other hand, may not perform well if the query doesn’t
    contain specific keywords that are present in the support documents. In our example,
    the user might describe a problem using different terms to those used in the technical
    manuals or FAQs.
  prefs: []
  type: TYPE_NORMAL
- en: Can we combine the two methods in a single retriever?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The short answer is yes. You’ve probably already guessed that I’m building a
    case along these lines. By combining them, we’d get the best of both worlds in
    terms of benefits and features. A few pages ago, we talked about using selectors
    and routers to implement more complex query behavior in our RAG application.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll leave it up to you to adapt the methods I’ve demonstrated there and implement
    a hybrid system that uses both dense and sparse retrieval methods. If you feel
    the need for an additional example, you can have a look at this one, which uses
    the Pinecone vector database to implement hybrid search: https://docs.llamaindex.ai/en/stable/examples/vector_stores/PineconeIndexDemo-Hybrid.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with empty results from the retrieval process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, our retrievers may come up empty-handed, without finding any indexed
    content matching the current query. This typically means that there are no relevant
    nodes in the index for that particular query.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such cases, the retriever may return an empty result set, indicating that
    no matching nodes were found. Depending on the type of index used, this situation
    can arise if the query keywords are very specific or rare, and none of the nodes
    in the index contain those exact keywords, or, in the case of embedding-based
    indexes, the similarity search that was performed during the search did not find
    any matching nodes with the current parameters used. To handle this scenario,
    we can consider various approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fallback mechanisms** : The search system can have fallback strategies in
    place, such as performing a more general search by adjusting the retriever’s parameters
    or suggesting alternative query terms to the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query expansion** : The query can be automatically expanded to include synonyms,
    related terms, or broader concepts to increase the chances of finding relevant
    nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevance scoring** : Even if no exact keyword matches are found, the search
    system can employ relevance scoring algorithms to identify nodes that are semantically
    similar to the query or contain partial matches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering other advanced retrieval methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the basic concepts just discussed, several other advanced retrieval
    methods are worth familiarizing yourself with. There is a special section in the
    official documentation where these methods are explained: https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/advanced_retrieval.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: There, you will learn more about special techniques, such as *Small-to-Big*
    *retrieval* , *recursive retrieval* , *retrieval from embedded tables* , *multi-modal
    retrieval* , *auto-merging retrieval* , and others.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed explanation of each retrieval strategy would go far beyond what I
    intend to cover in this book, but that doesn’t mean they aren’t important. After
    all, there is no point in ingesting and indexing the original documents if we
    cannot effectively extract the context we need in RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Practical advice
  prefs: []
  type: TYPE_NORMAL
- en: Always read the latest version of the official documentation before starting
    a major project. Things move so fast, and new methods and techniques emerge so
    quickly, that it is a shame to waste time reinventing the wheel. As an anecdote,
    I can tell you from personal experience that I have spent hours *inventing* something
    very similar to the *small-to-big* method, only to discover a few days later that
    it was already a tested and documented technique.
  prefs: []
  type: TYPE_NORMAL
- en: That’s enough information for one chapter. We’ll skip the PITS coding practice
    now as we’ll let more information accumulate in the next chapter before implementing
    additional features in our personal tutoring project.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Summary</title>
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various querying strategies and architectures within
    LlamaIndex with a deep focus on retrievers. Retrievers provide essential capabilities
    for extracting relevant information from indexes to generate useful responses
    in RAG systems. Throughout this chapter, we looked at basic retriever types such
    as `VectorIndexRetriever` and `SummaryIndexRetriever` . We also gained an understanding
    of advanced concepts such as asynchronous retrieval, metadata filters, tools,
    selectors, and query transformations. These allow us to build more sophisticated
    retrieval logic.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we covered fundamental paradigms such as dense retrieval and sparse
    retrieval and discussed their strengths and weaknesses. Implementations in LlamaIndex
    such as BM25Retriever were also introduced.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this chapter provided an overview of retrieval capabilities in LlamaIndex,
    laying the foundation for building high-performance and contextually-aware RAG
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re now equipped with the necessary knowledge to effectively retrieve information
    from indexes. In the next chapter, we’ll build on this knowledge by addressing
    the other important components of a query engine: post-processors and response
    synthesizers.'
  prefs: []
  type: TYPE_NORMAL
