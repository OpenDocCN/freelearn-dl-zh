- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Utilizing spaCy with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Transformers** are the latest hot topic in NLP. The goal of this chapter
    is to learn how to use transformers to improve the performance of trainable components
    in spaCy.'
  prefs: []
  type: TYPE_NORMAL
- en: First, you will learn about transformers and transfer learning. Next, you’ll
    learn more about spaCy trainable components and how to train a component, introducing
    spaCy’s **config.cfg** files and spaCy’s CLI. Then, you will learn about the architectural
    details of the commonly used Transformer architecture – **Bidirectional Encoder**
    **Representations from Transformers** ( **BERT** ) and its successor, RoBERTa.
    Finally, you’ll train the **TextCategorizer** component to classify texts using
    a transformer layer to improve accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to prepare data for training and
    fine-tune your own spaCy components. Because of the way spaCy is designed; while
    doing that, you’ll be following software engineering best practices. You’ll also
    have a solid basis of how transformers work, which will be useful when we work
    with **large** **language models** ( **LLMs** ) in [*Chapter 7*](B22441_07.xhtml#_idTextAnchor102)
    . You’ll be able to build state-of-the-art NLP pipelines with just a few lines
    of code with the power of Transformer models and transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Model training and transfer learning with spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying text with spaCy pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with spaCy **config.cfg** files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing training data to fine-tune models with spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Hugging Face’s Transformer for downstream tasks with spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset and the chapter code can be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  prefs: []
  type: TYPE_NORMAL
- en: We will use the **pandas** library to manipulate the datasets and also install
    the **spacy-transformers** library to work with the **transformer** spaCy component.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers and transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A milestone in NLP happened in 2017 with the release of the research paper
    *Attention Is All You Need* , by Vaswani et al. ( [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
    ), which introduced a brand-new machine learning idea and architecture – **transformers**
    . Transformers in NLP is a fresh idea that aims to solve sequential modeling tasks
    and targets some problems introduced by **Long Short-Term Memory** ( **LSTM**
    ) architecture. Here’s how the paper explains how transformers work:'
  prefs: []
  type: TYPE_NORMAL
- en: “ *The Transformer is the first transduction model relying entirely on self-attention
    to compute representations of its input and output without using sequence aligned
    RNNs* *or convolution.* ”
  prefs: []
  type: TYPE_NORMAL
- en: '**Transduction** in this context means transforming input to output by transforming
    input words and sentences into vectors. Typically, a transformer is trained on
    a huge corpus. Then, in our downstream tasks, we use these vectors as they carry
    information regarding the word semantics, sentence structure, and sentence semantics.'
  prefs: []
  type: TYPE_NORMAL
- en: Before transformers, the cool kid in the NLP world was the **word vector** technique.
    A word vector is basically a dense number representation of a word. What’s surprising
    about these vectors is that semantically similar words have similar word vectors.
    Word vectors such as **GloVe** and **FastText** vectors are already trained on
    the Wikipedia corpus and can be used for semantic similarity calculations. The
    **Token.similarity()** , **Span.similarity()** , and **Doc.similarity()** methods
    all use word vectors to predict how similar these containers are. This is a simple
    example usage of **transfer learning** , where we are using the knowledge from
    the texts (the knowledge extracted from words in the word vectors training) to
    solve a new problem (the similarity problem). Transformers are more powerful because
    they are designed to understand language in context, something that word vectors
    can’t do. We’ll learn more about this in the *BERT* section.
  prefs: []
  type: TYPE_NORMAL
- en: '*Transformer* is the name of the model architecture, but Hugging Face Transformers
    is also the name of the Hugging Face set of APIs and tools to easily download
    and train state-of-the-art pretrained models. Hugging Face Transformers offer
    thousands of pre-trained models to perform NLP tasks, such as text classification,
    text summarization, question answering, machine translation, and natural language
    generation in more than 100 languages. The goal is to make state-of-the-art NLP
    accessible to everyone.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to use the transformers models to apply a form
    of transfer learning to improve the accuracy of a downstream task – in our case,
    the text classification task. We’ll do that using spaCy’s **transformer** component
    from the **spacy-transformers** package in conjunction with the **textcat** component
    to increase the accuracy of the pipeline. With spaCy, there is also the option
    to use predictions directly from an existing Hugging Face model. To do that, you
    can use the wrappers from the **spacy-huggingface-pipelines** package. We will
    see how to do that in [*Chapter 11*](B22441_11.xhtml#_idTextAnchor143) .
  prefs: []
  type: TYPE_NORMAL
- en: Alright, now that you know what transformers are, let’s go ahead and learn more
    about the machine learning concepts behind the technique.
  prefs: []
  type: TYPE_NORMAL
- en: From LSTMs to Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before transformers, LSTM neural network cells were the go-to solution for modeling
    text. LSTMs are a variation of a **recurrent neural network** ( **RNN** ) cell.
    RNN is a special neural network architecture that can process sequential data
    in steps. In usual neural networks, we assume that all the inputs and outputs
    are independent of each other. The problem is that this way of modeling is not
    true for text data. Every word’s presence depends on the neighbor’s words. For
    example, during a machine translation task, we predict a word by considering all
    the words we predicted before. RNNs address this situation by capturing information
    about the past sequence elements and holding them in memory (called **hidden state**
    ).
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs were created to fix some computational problems of RNNs. RNNs have the
    problem of forgetting some data back in the sequence, as well as some numerical
    stability issues due to chain multiplications called vanishing and exploding gradients.
    An LSTM cell is slightly more complicated than an RNN cell, but the logic of computation
    is the same: we feed one input word at each time step and LSTM outputs a value
    at each time step.'
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are better than vanilla RNNs, but they have some shortcomings too. The
    LSTM architecture sometimes has difficulties with learning long text. Statistical
    dependencies in a long text can be difficult to represent by an LSTM because,
    as the time steps pass, the LSTM can forget some of the words that were processed
    at earlier time steps. Also, the nature of LSTMs is sequential. We process one
    word at each time step. This means parallelizing the learning process is impossible;
    we must process it sequentially. Not allowing parallelization creates a performance
    bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers address these problems by not using recurrent layers at all. The
    Transformer architecture consists of two parts – an input encoder (called the
    **Encoder** ) block on the left, and the output decoder (called the **Decoder**
    ) block on the right. The following diagram is taken from the original paper,
    *Attention Is All You Need* ( [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
    ), and exhibits the transformer architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Transformer architecture from the paper “Attention is All You
    Need”](img/B22441_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Transformer architecture from the paper “Attention is All You Need”
  prefs: []
  type: TYPE_NORMAL
- en: The preceding architecture is used for a machine translation task; hence, the
    input is a sequence of words from the source language, and the output is a sequence
    of words in the target language. The encoder generates a vector representation
    of the input words and passes them to the decoder (the word vector transfer is
    represented by the arrow from the encoder block in the direction of the decoder
    block). The decoder takes these input word vectors, transforms the output words
    into word vectors, and finally, generates the probability of each output word
    (labeled in *Figure 6* *.1* as **Output Probabilities** ).
  prefs: []
  type: TYPE_NORMAL
- en: The innovation transformers bring lies in the **Multi-Head Attention** block.
    This block creates a dense representation for each word by using a self-attention
    mechanism. The self-attention mechanism relates each word in the input sentence
    to the other words in the input sentence. The word embedding of each word is calculated
    by taking a weighted average of the other words’ embeddings. This way, the importance
    of each word in the input sentence is calculated, so the architecture focuses
    its attention on each input word in turn.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram illustrates the mechanism of self-attention in a Transformer
    model. It shows how the input words on the left-hand side attend to the input
    word “it” on the right-hand side. The color gradient in the diagram represents
    the level of relevance each word has with respect to “it.” Words with darker,
    more intense colors, such as “The animal,” have higher relevance, while words
    with lighter shades, like “didn't” or “too tired,” have lower relevance.
  prefs: []
  type: TYPE_NORMAL
- en: This visualization demonstrates that the Transformer can precisely determine
    that the pronoun “it” refers to “The animal” in this sentence. Such a capability
    allows Transformers to resolve complex semantic dependencies and relationships
    within a sentence, showcasing their powerful ability to understand context and
    meaning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Illustration of the self-attention mechanism](img/B22441_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Illustration of the self-attention mechanism
  prefs: []
  type: TYPE_NORMAL
- en: Later on in the chapter, we are going to learn about a famous transformer model
    called **BERT** , so don’t worry if all this content seems too abstract now. We
    will learn how to use transformers through a text classification use case, but
    before using transformers, we need to see how to tackle the text classification
    problem with spaCy. Let’s do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification with spaCy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: spaCy models are very successful for general NLP purposes, such as understanding
    a sentence’s syntax, splitting a paragraph into sentences, and extracting entities.
    However, sometimes, we work on very specific domains that spaCy pre-trained models
    didn’t learn how to handle.
  prefs: []
  type: TYPE_NORMAL
- en: For example, X (formerly Twitter) text contains many non-regular words, such
    as hashtags, emoticons, and mentions. Also, X sentences are usually just phrases,
    not full sentences. Here, it’s entirely reasonable that spaCy’s POS tagger performs
    in a substandard manner as the POS tagger is trained on full, grammatically correct
    English sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is the medical domain. It contains many entities, such as drug,
    disease, and chemical compound names. These entities are not expected to be recognized
    by spaCy’s NER model because it has no disease or drug entity labels. NER does
    not know anything about the medical domain at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be working with the *Amazon Fine Food Reviews* dataset
    ( [https://www.kaggle.com/snap/amazon-fine-food-reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews)
    ). This dataset contains customer reviews about fine food sold on Amazon ( *J.
    McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating
    dimensions with review text. RecSys, 2013* , [https://dl.acm.org/doi/abs/10.1145/2507157.2507163](https://dl.acm.org/doi/abs/10.1145/2507157.2507163)
    ). Reviews include user and product information, user ratings, and text. We want
    to classify these reviews as positive or negative. As this is a specific domain
    problem, spaCy doesn’t know how to classify it (yet). To teach the pipeline how
    to do that, we will use **TextCategorizer** , a trainable component to classify
    text. We’ll do that in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Training the TextCategorizer component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**TextCategorizer** is an optional and trainable pipeline component to predict
    categories over a whole document. To train it, we need to provide examples and
    their class labels. *Figure 6* *.3* shows exactly where the **TextCategorizer**
    component lies in the NLP pipeline; this component comes after the essential components.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – TextCategorizer in the NLP pipeline](img/B22441_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – TextCategorizer in the NLP pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'A neural network architecture lies behind spaCy’s **TextCategorizer** component,
    which provides us with user-friendly and end-to-end approaches to training the
    classifier. This means we don’t have to deal directly with the neural network
    architecture. **TextCategorizer** is available in two flavors: single-label classifier
    ( **textcat** ) and multilabel classifier ( **textcat_multilabel** ). As the name
    suggests, a multilabel classifier can predict more than one class. A single-label
    classifier predicts only one class for each example and classes are mutually exclusive.
    The predictions of the component are saved in **doc.cats** as a dictionary, where
    the key is the name of the category, and the value is a score between 0 and 1
    (inclusive).'
  prefs: []
  type: TYPE_NORMAL
- en: To understand how to use the **TextCategorizer** component, it is helpful to
    learn how to train a deep model in general. Let’s do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training a deep learning model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train a neural network, we need to configure the model parameters and provide
    the training examples. Each prediction of the neural network is a sum of its weight
    values; hence, the training procedure adjusts the weights of the neural network
    with our examples. If you want to learn more about how neural networks function,
    you can read the excellent guide at [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: In the training procedure, we’ll go over the training set several times and
    show each example several times. Each iteration is called an **epoch** . At each
    iteration, we also shuffle the training data to prevent the model from learning
    patterns specific to the order of the examples. This shuffling of training data
    helps to ensure that the model generalizes well to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In each epoch, the training code updates the weights of the neural network through
    incremental updates. These incremental updates are usually applied by dividing
    the data of each epoch into **mini-batches** . A **loss** is calculated by comparing
    the actual label with the neural network’s current output. **Optimizers** are
    functions that update the neural network weights subject to that loss. **Gradient
    descent** is the name of the algorithm used to find the direction and the rate
    to update the network parameters. The optimizers work by iteratively updating
    the model parameters in the direction of the gradient that reduces the loss. That
    is the training process in a nutshell.
  prefs: []
  type: TYPE_NORMAL
- en: If you ever had to train a deep learning model using PyTorch or TensorFlow,
    you’re likely familiar with the often-challenging nature of the process. spaCy
    uses Thinc, a lightweight deep learning library with a functional programming
    API for composing models. With Thinc, we can switch between PyTorch, TensorFlow,
    and MXNet models without changing the code (and without having to code with these
    libraries directly).
  prefs: []
  type: TYPE_NORMAL
- en: The Thinc conceptual model is a little different from the other neural network
    libraries. To train spaCy models, we’ll have to learn about Thinc’s configuration
    system. We will do that in the next sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the training process, we need to gather and prepare the data, define
    the optimizer to update the weights for each mini-batch, split the data into mini-batches,
    and shuffle each mini-batch for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We haven’t touched on the phase of gathering and preparing the data yet. spaCy’s
    **Example** container holds the information for one training instance. It stores
    two **Doc** objects: one for holding the reference label and one for holding the
    predictions of the pipeline. Let''s learn how to build the **Example** objects
    from our training data in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for spaCy trainable components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create a dataset for training, we need to construct Example objects. Example
    objects can be created using the **Example.from_dict()** method with a Doc reference
    and a dictionary of gold-standard annotations. For the **TextCategorizer** component,
    the annotation name for **Example** should be a **cat** dictionary of label/value
    pairs indicating how relevant the category is for the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each review of the dataset we’ll be working on can be either positive or negative.
    Here is an example of a review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The **Example.from_dict()** method takes **Doc** as the first parameter and
    **Dict[str, Any]** as the second parameter. For our classification use case, the
    **Doc** will be the review text and **Dict[str, Any]** will be **cat** dictionary
    with the labels and the correct classification. Let''s build **Example** for the
    previous review:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s load a blank English pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create a **Doc** to wrap the review text and create the **cats**
    dictionary with the correct labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s create an **Example** object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this chapter, we are only fine-tuning the **TextCategorizer** component,
    but with spaCy, you can also train other trainable components such as **Tagger**
    or **DependencyParser** . The process of creating the **Example** objects is the
    same; the only thing that differs is the type of annotation for each. Here are
    some examples of different annotations (you can find the full list at [https://spacy.io/api/data-formats#dict-input](https://spacy.io/api/data-formats#dict-input)
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '**text** : Raw text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cats** : Dictionary of **label** / **value** pairs indicating how relevant
    a certain text category is for the text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tags** : List of fine-grained POS tags'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**deps** : List of string values indicating the dependency relation of a token
    to its head'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **amazon_food_reviews.csv** file has a sample of 4,000 rows of the original
    *Amazon Fine Food Reviews* dataset. We will take 80% of these rows for training
    and use the other 20% for testing. Let’s create the array with all the training
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s download the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s load and split 80% of the dataset for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s create the train examples and store them in a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that you know how to create the examples to feed the training data, we can
    go ahead and write the training script.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the training script
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The recommended way of training the models is using the **spacy train** command
    with spaCy’s CLI; we *shouldn’t* be writing our own training scripts. In this
    section, we will write our own training script for learning purposes. We’ll learn
    how to properly train the models using the CLI in the next sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review the steps to train a deep learning model. At each epoch, we shuffle
    the training data and *update the weights of the neural network* through i *ncremental
    updates* using *optimizer functions* . spaCy offers methods to create all of these
    steps of the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to train the **TextCategorizer** component, so the first step is
    to create it and add it to the pipeline. Since this is a trainable component,
    we need to initialize it providing the **Examples** . We also need to provide
    the current **nlp** object. Here is the code to create and initialize the component,
    using the list we’ve created in the previous code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We pass the whole **TRAIN_EXAMPLES** list as a **lambda** function. The next
    step is to define the optimizer to update the model weights. spaCy’s **Language**
    class has a **resume_training()** method that creates and returns an optimizer.
    By default, it returns the **Adam** optimizer, and we will stick to it here.
  prefs: []
  type: TYPE_NORMAL
- en: We’re ready to define the training loop. For each epoch, we go over training
    examples one by one and update the weights of **textcat** . We go over the data
    for 40 epochs. spaCy’s **util.minibatch** function iterates over batches of items.
    The **size** parameter defines the batch size. I have a GPU with enough memory
    so I'm dividing the data into groups of 200 rows.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you’re running the code and run into a “GPU out of memory” error, you may
    try to decrease the **size** parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the loop through the training data in place, the next step is to finally
    compute the difference between the model prediction and the correct labels and
    update the weights accordingly. The **update** method of the **Language** class
    handles that. We will provide the data and a dictionary to update the loss so
    we can keep track of it and the optimizer we’ve created previously. The following
    code defines the full training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the pipeline and the component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the training loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And there you have it, your first trained spaCy component. If everything is
    okay, the model is learning and the losses should be decreasing. At every 10 epochs,
    we print the losses to check if this is happening. Let’s predict the categories
    of some unseen reviews to have a quick glance at how the model is behaving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 6* *.4* shows the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Categories of the review examples](img/B22441_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Categories of the review examples
  prefs: []
  type: TYPE_NORMAL
- en: The model is right for the first two examples, but the last one is clearly a
    negative review and the model classifies it to be positive. We can see that the
    review has some very objective indicators of a bad review such as the passage
    **this is the worst** . Maybe if we add more information about the context of
    words like transformers do, we can increase the performance of the model. Let’s
    try this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Hugging Face transformers in spaCy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to use spaCy’s **transformer** component from
    **spacy-transformers** in conjunction with the **textcat** component to increase
    the accuracy of the pipeline. This time, we will create the pipeline using spaCy’s
    **config.cfg** system, which is the recommended way to train the spaCy components.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first get to know the **Transformer** component.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Transformer** component is provided by the **spacy-transformers** package.
    With the **Transformer** component, we can use transformer models to improve the
    accuracy of our tasks. The component supports all models that are available via
    the Hugging Face **transformers** library. In this chapter, we are going to use
    the RoBERTa model. We'll learn more about this model in the next sections of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer** adds a **Doc._.trf_data** attribute to the **Doc** objects.
    These transformer tokens can be shared with the other pipeline components. In
    this chapter, we are going to use the tokens of the RoBERTa model as part of the
    **TextCategorizer** component. But first, let’s use the RoBERTa model without
    **TextCategorizer** to see how it works. The **Transformers** component allows
    us to use a lot of different architectures. To use the **roberta-base** model
    from Hugging Face, we need to use the **spacy-transformers.TransformerModel.v3**
    architecture. This is how we do it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries and load a blank model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the architecture we want to use with the **Transformer** component.
    The **Transformer** component accepts a **model** config to set the Thinc model
    wrapping the transformer. We set the architecture to **spacy-transformers.TransformerModel.v3**
    and the model to **roberta-base** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the component to the pipeline, initialize it, and print the vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is a **FullTransformerBatch** object that holds a batch of input
    and output objects for the transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Cool, now we need to use this model output together with the **TextCategorizer**
    component. We'll do that using the **config.cfg** file, so first we need to learn
    how to work with this configuration system.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy’s configuration system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'spaCy v3.0 introduces configuration files. These files are used to include
    all settings and hyperparameters for training pipelines. Under the hood, the training
    config uses the configuration system provided by the Thinc library. As the spaCy
    documentation points out, some of the main advantages and features of spaCy’s
    training config are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured sections** : The config is grouped into sections, and nested sections
    are defined using the **.** notation. For example, **[components.textcat]** defines
    the settings for the pipeline’s **TextCategorizer** component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpolation** : If you have hyperparameters or other settings used by multiple
    components, define them once and reference them as variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reproducibility with no hidden defaults** : The config file is the “single
    source of truth” and includes all settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated checks and validation** : When you load a config, spaCy checks
    whether the settings are complete and whether all values have the correct types.
    This lets you catch potential mistakes early. In your custom architectures, you
    can use Python type hints to tell the config which types of data to expect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The config is divided into sections and subsections, indicated by the square
    brackets and dot notation. For example, **[components]** is a section, and **[components.textcat]**
    is a subsection. The main top-level sections of a config file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**paths** : Paths to data and other assets. Reused across the config as variables
    (e.g., **${paths.train}** ) and can be overwritten on the CLI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**system** : Settings related to system and hardware. Reused across the config
    as variables (e.g., **${system.seed}** ) and can be overwritten on the CLI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nlp** : Definition of the **nlp** object, its tokenizer, and processing pipeline
    component names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**components** : Definitions of the pipeline components and their models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**training** : Settings and controls for the training and evaluation process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pretraining** : Optional settings and controls for the language model pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initialize** : Data resources and arguments passed to components when **nlp.initialize
    ()** is called before training (but not at runtime).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now know how to train a deep learning model and how to define the configuration
    for this training using Thinc as part of the spaCy training process. This configuration
    system is very handy for maintaining and reproducing NLP pipelines, and they are
    not only for training but also for building the pipelines when we don’t need to
    train the components. The spaCy config system really shines when combined with
    the spaCy CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Training the TextCategorizer with a config file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will use spaCy’s CLI to fine-tune the classification pipeline.
    As usual, the first thing you need to do to train the model is prepare the data.
    The recommended way of preparing data for training with spaCy is using the **DocBin**
    container instead of creating the **Example** objects as we did earlier. **DocBin**
    container packs a collection of **Doc** objects for binary serialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the training data with **DocBin** , we will create the **Doc** objects
    using the text of the reviews and add the **doc.cats** attribute accordingly.
    The process is pretty straightforward as we just need to use the **DocBin.add()**
    method to add a **Doc** annotation for serialization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load and split the data as we did previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we create a **DocBin** object and, inside the **for** loop, we create
    the **Doc** objects and add them to **DocBin** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we save the **DocBin** object to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will also need to create a **dev** test set (it will be used in the training),
    so let’s create a function to convert the datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we have all the data prepared in the recommended way to train the models.
    We will use these **.spacy** files in a minute; let’s first learn how to use the
    spaCy CLI.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy’s CLI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With spaCy’s CLI, you can perform spaCy operations using the command line. Using
    the command line is important because we can create and automate the execution
    of the pipelines, making sure that every time we run the pipeline, it will follow
    the same steps.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy’s CLI provides commands for training pipelines, converting data, debugging
    your config files, evaluating the models, and so on. You can type **python -m
    spacy --help** to see the list of all CLI commands.
  prefs: []
  type: TYPE_NORMAL
- en: The **spacy train** command trains or updates a spaCy pipeline. It requires
    data in spaCy’s binary format but you can also convert data from other formats
    using the **spacy convert** command. The config file should include all settings
    and hyperparameters used during training. We can override settings using command-line
    options. For instance, **--training.batch_size 128** overrides the value of **"batch_size"**
    in the **"[** **training]"** block.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the **spacy init config** CLI command to create the configuration
    file. The information in the configuration file includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The paths for the converted datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A seed number and the GPU config
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create the **nlp** object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build the components we will use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do the training itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For training, it is important to *make all the settings explicit* . We don´t
    want hidden defaults because they can make the pipelines hard to reproduce. That’s
    part of the design of the configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a training configuration that doesn’t use the **Transformer**
    component to see the proper way of training a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a configuration file named **config_without_transformer.cfg**
    using an English model and with a **TextCategorizer** component, with all the
    other settings defined by default.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the file, in the **paths** section, we should point to the **train**
    and **dev** data paths. Then, in the **system** section, we set the random seed.
    spaCy uses CuPy for GPU support. CuPy provides a NumPy-compatible interface for
    GPU arrays. The **gpu_allocator** parameter sets the library for CuPy to route
    GPU memory allocation to and the values can be either **pytorch** or **tensorflow**
    . This avoids the memory problems when using CuPy together with one of these libraries,
    but since it's now our case, here we can leave it set to **null** .
  prefs: []
  type: TYPE_NORMAL
- en: In the **nlp** section, we specify the model we’ll use and define the components
    of the pipeline, which is just **textcat** for now. In the **components** section,
    we need to specify how to initialize the component, so we set the **factory =
    "textcat"** parameter in the **component.textcat** subsection. **textcat** is
    the name of the registered function to create the **TextCategorizer** component.
    You can see all the available config parameters at [https://spacy.io/api/data-formats#config](https://spacy.io/api/data-formats#config)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'With the config all set, we can run the **spacy train** command. The result
    of this run is a new pipeline, so you need to specify a path to save it. Here
    is the full command to run the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This command trains the pipeline using the configuration file we’ve created
    and points to the **train.spacy** and **dev.spacy** data. *Figure 6* *.5* shows
    the training output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Training output](img/B22441_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Training output
  prefs: []
  type: TYPE_NORMAL
- en: '**E** indicates the epoch, and you can also see the loss and the score for
    each optimization step. The best model is saved at **pipeline_without_transformer/model-last**
    . Let’s load it and check the results of the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 6* *.6* shows the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Categories of the review examples using the new pipeline](img/B22441_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Categories of the review examples using the new pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Now, the model is incorrect for the first two examples and correct for the third
    one. Let’s see whether we can improve that using the **transformers** component.
    Before doing that, now is a good time to learn the internals of one of the most
    influential transformer models, **BERT** . Then, we’ll learn more about its successor,
    **RoBERTa** , the model we’ll use in this chapter’s classification use case.
  prefs: []
  type: TYPE_NORMAL
- en: BERT and RoBERTa
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll explore the most influential and commonly used Transformer
    model, BERT. BERT is introduced in Google’s 2018 research paper; you can read
    it here: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'What does BERT do exactly? To understand what BERT outputs, let’s dissect the
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: BERT is essentially a trained transformer encoder stack. Input into BERT is
    a sentence, and the output is a sequence of word vectors. The difference between
    BERT and previous word vector techniques is that BERT’s word vectors are contextual,
    which means that a vector is assigned to a word based on the input sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word vectors such as GloVe are context-free, meaning that the word vector for
    a word is always the same independent of the sentence it is used in. The following
    diagram explains this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Word vector for the word “bank”](img/B22441_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Word vector for the word “bank”
  prefs: []
  type: TYPE_NORMAL
- en: Here, even though the word *bank* has two completely different meanings in these
    two sentences, the word vectors are the same. Each word has only one vector and
    vectors are saved to a file following training.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the contrary, BERT word vectors are dynamic. BERT can generate different
    word vectors for the same word depending on the input sentence. The following
    diagram shows the word vectors generated by BERT:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Two distinct word vectors generated by BERT for the same word,
    “bank,” in two different contexts](img/B22441_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Two distinct word vectors generated by BERT for the same word,
    “bank,” in two different contexts
  prefs: []
  type: TYPE_NORMAL
- en: How does BERT generate these word vectors? In the next section, we’ll explore
    the details of the BERT architecture.
  prefs: []
  type: TYPE_NORMAL
- en: BERT architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BERT is a transformer encoder stack, which means that several encoder layers
    are stacked on top of each other. The first layer initializes the word vectors
    randomly, and then each encoder layer transforms the output of the previous encoder
    layer. The paper introduces two model sizes for BERT: BERT Base and BERT Large.
    The following diagram shows the BERT architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – BERT Base and Large architectures, having 12 and 24 encoder
    layers, respectively](img/B22441_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – BERT Base and Large architectures, having 12 and 24 encoder layers,
    respectively
  prefs: []
  type: TYPE_NORMAL
- en: Both BERT models have a huge number of encoder layers. BERT Base has 12 encoder
    layers and BERT Large has 24 encoder layers. The dimensions of the resulting word
    vectors are different too; BERT Base generates word vectors of size 768 and BERT
    Large generates word vectors of size 1024.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram exhibits a high-level overview of BERT inputs and outputs
    (ignore the CLS token for now; you’ll learn about it in the *BERT input* *format*
    section):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – BERT model input word and output word vectors](img/B22441_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – BERT model input word and output word vectors
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see a high-level overview of BERT inputs and
    outputs. BERT input must be in a special format and include some special tokens,
    such as CLS in *Figure 6* *.10* . In the next section, you’ll learn about the
    details of the BERT input format.
  prefs: []
  type: TYPE_NORMAL
- en: BERT input format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand how BERT generates the output vectors, we need to know the BERT
    input data format. BERT input format can represent a single sentence, as well
    as a pair of sentences. For tasks such as question answering and semantic similarity,
    we input two sentences to the model in a single sequence of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT works with a class of special tokens and a special tokenization algorithm
    called **WordPiece** . The main special tokens are **[CLS]** , **[SEP]** , and
    **[PAD]** :'
  prefs: []
  type: TYPE_NORMAL
- en: The first special token of BERT is [ **CLS** ]. The first token of every input
    sequence has to be [ **CLS** ]. We use this token in classification tasks as an
    aggregate of the input sentence. We ignore this token in non-classification tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ **SEP** ] means a sentence separator. If the input is a single sentence,
    we place this token at the end of the sentence. If the input is two sentences,
    then we use this token to separate two sentences. Hence, for a single sentence,
    the input looks like [ **CLS** ] sentence [ **SEP** ], and for two sentences,
    the input looks like [ **CLS** ] sentence1 [ **SEP** ] sentence2 [ **SEP** ].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ **PAD** ] is a special token meaning padding. BERT receives sentences of
    a fixed length; hence, we pad the short sentences before feeding them to BERT.
    The maximum length of tokens we can feed to BERT is 512.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT tokenizes the words using WordPiece tokenization. A “word piece” is literally
    a piece of a word. The WordPiece algorithm breaks words down into several sub-words.
    The idea is to break down complex/long tokens into simpler tokens. For example,
    the word **playing** is tokenized as **play** and **##ing** . A **##** character
    is placed before every word piece to indicate that this token is not a word from
    the language’s vocabulary but that it’s a word piece.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some more examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: By doing that, we represent the language vocabulary more compactly, grouping
    common sub-words. WordPiece tokenization creates wonders on rare/unseen words,
    as these words are broken down into their sub-words.
  prefs: []
  type: TYPE_NORMAL
- en: After tokenizing the input sentence and adding the special tokens, each token
    is converted to its ID. After that, we feed the sequence of token IDs to BERT.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, this is how we transform a sentence into a BERT input format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Transforming an input sentence into BERT input format](img/B22441_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Transforming an input sentence into BERT input format
  prefs: []
  type: TYPE_NORMAL
- en: This tokenization process is crucial for transformer models because it allows
    the model to handle out-of-vocabulary words and helps in generalization. For example,
    the model can learn that the suffix *ness* in words such as *happiness* and *sadness*
    has a specific meaning and can use this knowledge for new words that also have
    this suffix.
  prefs: []
  type: TYPE_NORMAL
- en: BERT is trained on a large unlabeled Wiki corpus and a huge book corpus. As
    stated in Google Research’s BERT GitHub repository, [https://github.com/google-research/bert](https://github.com/google-research/bert)
    , they trained a large model (12-layer to 24-layer Transformer) on a large corpus
    (Wikipedia + BookCorpus) for a long time (1M update steps).
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT is trained with two training methods: **masked language modeling** ( **MLM**
    ) and **next sentence prediction** ( **NSP** ). **Language modeling** is the task
    of predicting the next token given the sequence of previous tokens. For example,
    given the sequence of the words *Yesterday I visited a* , a language model can
    predict the next token as one of the tokens such as *church* , *hospital* , *school*
    , and so on. **Masked language modeling** is a kind of language modeling where
    we mask a percentage of the tokens randomly by replacing them with a **[MASK]**
    token. We expect MLM to predict the masked words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The masked language model data preparation in BERT is implemented as follows.
    First, 15 of the input tokens are chosen at random. Then, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: 80% of the tokens chosen are replaced with **[MASK]**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10% of the tokens chosen are replaced with another token from the vocabulary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining 10% are left unchanged
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A training example sentence for MLM looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: NSP is the task of predicting the next sentence given an input sentence. In
    this approach, we feed two sentences to BERT and expect BERT to predict the order
    of the sentences, more specifically, whether the second sentence is the sentence
    that comes after the first sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make an example input to NSP. We’ll feed two sentences separated by the
    **[SEP]** token as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the second sentence can follow the first sentence; hence, the
    predicted label is **IsNext** . How about this example?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This example pair of sentences generates the **NotNext** label, as they are
    not contextually or semantically related.
  prefs: []
  type: TYPE_NORMAL
- en: Both these training techniques allow the model to learn complex concepts about
    the language. Transformers are the basis of LLMs. LLMs are revolutionizing the
    NLP world, mostly because of their capacity for understanding context.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the BERT architecture, the details of the input format, and
    training data preparation, you have a solid basis to understand how LLMs work.
    Getting back to our classification use case, we’ll work with a successor of BERT,
    a model called RoBERTa. Let’s learn about RoBERTa in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The RoBERTa model was proposed at [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)
    . It builds on BERT and the key difference between them is in the data preparation
    and training.
  prefs: []
  type: TYPE_NORMAL
- en: BERT does the token masking once during data preprocessing, which results in
    the same mask for each training instance in every epoch. RoBERTa uses *dynamic
    masking* , where they generate the masking patterns every time we feed a sequence
    to the model. They also removed the NSP because they found that it matches or
    slightly improves downstream task performance.
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa also uses larger batch sizes than BERT and a larger vocabulary size,
    from a vocabulary size of 30K to a vocabulary containing 50K sub-word units. The
    paper is a very good read to understand design decisions that impact transformer
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how BERT and RoBERTa work, it’s time to finally use RoBERTa
    in our text classification pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Training the TextCategorizer with a transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To work with the **transformer** component for downstream tasks, we need to
    tell spaCy how to connect the component output with the other pipeline components.
    We’ll do that with the **spacy-transformers.TransformerModel.v3** and the **spacy-transformers.TransformerListener.v1**
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: In spaCy, we have different model architectures, which are functions that wire
    up Thinc **Model** instances. Both **TransformerModel** and the **TransformerListener**
    model are Transformer layers.
  prefs: []
  type: TYPE_NORMAL
- en: The **spacy-transformers.TransformerModel.v3** layer loads and wraps the transformer
    models from the Hugging Face Transformers library. It works with any transformer
    that has pretrained weights and has a PyTorch implementation. The **spacy-transformers.TransformerListener.v1**
    layer takes a list of **Doc** objects as input and uses the **TransformerModel**
    layer to produce a list of two-dimensional arrays as output.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the spaCy layer concept, it is time to revisit the **TextCategorizer**
    component. In spaCy, the **TextCategorizer** component has different model architecture
    layers. Typically, each architecture accepts sublayers as arguments. By default,
    the **TextCategorizer** component uses the **spacy.TextCatEnsemble.v2** layer,
    which is a stacked ensemble of a linear bag-of-words model and a neural network
    model. We used this layer when we trained the pipeline using the configuration
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To finish our journey of this chapter, we will change the neural network layer
    of **TextCatEnsemble** from the default **spacy.Tok2Vec.v2** layer and use the
    RoBERTa transformer model. We will do this by creating a new configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This command created a **config_transformer.cfg** file optimized for accuracy
    and for training on a GPU. *Figure 6* *.12* shows the output of the command.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Creating the new training configuration that uses RoBERTa](img/B22441_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Creating the new training configuration that uses RoBERTa
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can train the pipeline pointing to this configuration file, train the
    model, and make the predictions, just as we did in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This time, we do the training using a GPU, so we set the **gpu_id** parameter.
    *Figure 6* *.13* shows the results using this new trained model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Categories of the review examples using the pipeline with transformer](img/B22441_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Categories of the review examples using the pipeline with transformer
  prefs: []
  type: TYPE_NORMAL
- en: Now, the model classifies reviews correctly. Nice, isn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s fair to say that this chapter is one of the most important chapters of
    the book. Here, we learned about transfer learning and transformers, and how to
    use the spaCy configuration system to train the **TextCategorizer** component.
  prefs: []
  type: TYPE_NORMAL
- en: With the knowledge of how to prepare the data to train spaCy components and
    the knowledge of how to use the configuration files to define the training settings,
    you are now able to fine-tune any spaCy trainable component. That´s a huge step,
    congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned about language models. In the next chapter, you
    will learn how to use LLMs, which are the most powerful NLP models today.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Customizing and Integrating NLP Workflows'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This final section focuses on creating tailored NLP solutions and integrating
    spaCy with other tools and platforms. You’ll explore how to leverage LLMs, train
    custom models, and integrate spaCy projects with web applications to build end-to-end
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B22441_07.xhtml#_idTextAnchor102) , *Enhancing NLP Tasks Using
    LLMs with spacy-llm*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B22441_08.xhtml#_idTextAnchor109) , *Training an NER Component
    with Your Own Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B22441_09.xhtml#_idTextAnchor123) , *Creating End-to-End spaCy
    Workflows with Weasel*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B22441_10.xhtml#_idTextAnchor134) , *Training an Entity Linker
    Model with spaCy*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B22441_11.xhtml#_idTextAnchor143) , *Integrating spaCy with
    Third-Party Libraries*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
