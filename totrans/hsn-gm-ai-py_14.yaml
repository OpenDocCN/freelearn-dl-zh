- en: DRL Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working through and exploring the code in this book is meant to be a learning
    exercise in how **Reinforcement Learning** (**RL**) algorithms work but also how
    difficult it can be to get them to work. It is because of this difficulty that
    so many open source RL frameworks seem to pop up every day. In this chapter, we
    will explore a couple of the more popular frameworks. We will start with why you
    would want to use a framework and then move on to exploring the more popular frameworks
    such as Dopamine, Keras-RL, TF-Agents, and RL Lib.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick summary of the main topics we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Google Dopamine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing with Keras-RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring RL Lib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using TF agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use a combination of notebook environments on Google Colab and virtual
    environments depending on the complexity of the examples in this chapter. Jupyter
    Notebooks, which Colab is based on, is an excellent way to demonstrate code. It
    is generally not the preferred way to develop code and this is the reason we avoided
    this method until now.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at why you would want to choose a framework.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have surmised by now, writing your own RL algorithms and functions
    on top of a deep learning framework, such as PyTorch, is not trivial. It is also
    important to remember that the algorithms in this book go back about 30 years
    over the development of RL. That means that any serious new advances in RL take
    substantial effort and time—yes, for both development and especially training.
    Unless you have the time, resources, and incentive for developing your own framework,
    then it is highly recommended to graduate using a mature framework. However, there
    is an ever-increasing number of new and comparable frameworks out there, so you
    may find that you are unable to choose just one. Until one of these frameworks
    achieves true AGI, then you may also need separate frameworks for different environments
    or even different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, **AGI** stands for **Artificial General Intelligence** and it really
    is the goal of any RL framework to be AGI. An AGI framework can be trained on
    any environment. An advanced AGI framework may be able to transfer learning across
    tasks. Transfer learning is where an agent can learn one task and then use those
    learnings to accomplish another similar task.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to look at the current most popular frameworks that have the most
    promise, in later sections. It is important to compare the various current frameworks
    to see whether one may be a better choice for you and your team. Therefore, we
    will look at a comparison of the various RL frameworks currently available in
    the following list.
  prefs: []
  type: TYPE_NORMAL
- en: 'This list features the current most popular frameworks ordered by current popularity
    (by Google), but this list is expected to change with time:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI Gym and Baselines**: OpenAI Gym is the framework we have used for
    most of the environments we have explored in this book. This library also has
    a companion called Baselines that provides several agents for, you guessed it,
    baselining the Gym environments. Baselines is also a very popular and good RL
    framework but we have omitted it here in favor of looking at other libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Dopamine**: This is a relatively new framework that has gained popularity
    very quickly. This is likely due, in part, to its implementation of the RainbowDQN
    agent. The framework is well developed but has been described as being clunky
    and not very modular. We showcase it here because it is popular and you will likely
    want a closer look at it anyway.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML-Agents**: We have more or less already covered a whole chapter on this
    framework, so we won''t need to explore it here. Unity has developed a very solid
    but not very modular framework. The implementation currently only supports PG
    methods such as PPO and Soft Actor-Critic. ML-Agents on its own, however, it can
    be a great and recommended way to demonstrate RL to development teams or even
    introduce concepts to clients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RL Lib with the ray-project**: This has strange origins in that it started
    as a parallelization project for Python and evolved into a training platform for
    RL. As such, it tends to favor training regimes that use asynchronous agents such
    as A3C, and it is well suited to complex environments. Not to mention, this project
    is based on PyTorch so it will be worth a look.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keras-RL**: Keras itself is another deep learning framework that is very
    popular on its own. The deep learning library itself is quite concise and easy
    to use—perhaps in some ways, too easy. However, it can be an excellent way to
    prototype an RL concept or environment and deserves a closer look by us.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TRFL**: This library, not unlike Keras-RL, is an extension of the TensorFlow
    framework to incorporate RL. TensorFlow is another low-level deep learning framework.
    As such, the code to build any working agent also needs to be quite a low level
    and using this library likely won''t be for you, especially if you enjoy PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tensorforce**: This is another library focused on extending TensorFlow for
    RL. The benefit of using a TF-based solution is cross-compatibility and even the
    ability to port your code to web or mobile. However, building low-level computational
    graphs is not for everyone and does require a higher level of mathematics than
    we covered in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizon**: This framework is from Facebook and is developed on top of PyTorch.
    Unfortunately, the benefits of this framework fall short in several areas including
    not having a `pip` installer. It also lacks tight integration with Gym environments
    so, unless you work at Facebook, you will likely want to avoid this framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coach**: This is one of those sleeper frameworks that could build a substantial
    following of its own someday. There are many useful and powerful features to Coach,
    including a dedicated dashboard and direct support for Kubernetes. This framework
    also currently boasts the largest implementation of RL algorithms and will give
    you plenty of room to explore. Coach is a framework worth exploring on your own.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MAgent**: This project is similar to RLLib (Ray) in that it specializes in
    training multiple agents asynchronously or in various configurations. It is developed
    on top of TensorFlow and uses its own grid-world designed environments for what
    is coined as real-life simulations. This is a very specialized framework for developers
    or real-life RL solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TF-Agents**: This is another RL implementation from Google developed on top
    of TensorFlow. As such, it is a more low-level framework but is more robust and
    capable than the other TF frameworks mentioned here. This framework appears to
    be a strong contender for more serious research and/or production implementations
    and worth a further look from readers looking to do such work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLM-Lab**: This is another PyTorch-based framework that is actually based
    on top of Ray (RLLib), although it is designed more for pure research. As such,
    it lacks a `pip` installer and assumes the user is pulling code directly from
    a repository. It is likely best to leave this framework to the researchers for
    now.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeeR**: This is another library that is integrated with Keras and is intended
    to be more accessible. The library is well kept and the documentation is clear.
    However, this framework is intended for those learning RL and if you made it this
    far, you likely already need something more advanced and robust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Garage**: This is another TF-based framework that has some excellent functionality
    but lacks documentation and any good installation procedures, which makes this
    another good research framework but may be better avoided by those interested
    in developing working agents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Surreal**: This framework is designed more for robotics applications and,
    as such, is more closed. Robotics RL with environments such as Mujoco have been
    shown to be commercially viable. As such, this branch of RL is seeing the impact
    of those trying to take their share. This means that this framework is currently
    free but not open source and the free part is likely to change soon. Still, if
    you are specializing in robotics applications, this may be worth a serious look.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RLgraph**: This is perhaps another sleeper project to keep your eye on. This
    library is currently absorbing a ton of commits and changing quickly. It is also
    built with both PyTorch and TensorFlow mappings. We will spend some time looking
    at using this framework in a later section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple RL**: This is perhaps as simple as you can get with an RL framework.
    The project is intended to be very accessible and examples with multiple agents
    can be developed in less than eight lines of code. It can actually be as simple
    as the following block of code taken from the example documentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With so many frameworks to choose from, we only have time to go over the most
    popular frameworks in this chapter. While frameworks become popular because they
    are well written and tend to work well in a wide variety of environments, until
    we reach AGI, you may still need to explore various frameworks to find an algorithm/agent
    that works for you and your problem.
  prefs: []
  type: TYPE_NORMAL
- en: To see how this has evolved over time, we can use Google Trends to perform a
    search comparison analysis. Doing this can often give us an indication of how
    popular a particular framework is trending in search terms. More search terms
    means more interest in the framework, which in turn, leads to more development
    and better software.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Google Trends plot shows a comparison of the top five listed
    frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/212f143f-e9d7-466a-b94a-e2f6c6d214f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Google trends comparison of RL frameworks
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the plot the trending increase for RL Lib and Google Dopamine.
    It is also interesting to note that the primary interest in RL development is
    the current greatest in the US and Japan, with Japan taking a special interest
    in ML-Agents.
  prefs: []
  type: TYPE_NORMAL
- en: ML-Agents' popularity lends itself to several factors, one of which being the
    VP of AI and ML at Unity, Dr. Danny Lange. Dr. Lange lived in Japan for several
    years and is fluent in Japanese and this has likely contributed to this specific
    popularity.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to note the absence of China in this area, at least for these
    types of frameworks. China's interest in RL is currently very specific to planning
    applications popularized by the defeat of the game of Go by an RL agent. That
    RL agent was developed using an algorithm called Monte Carlo Tree Search, which
    is intended to do a full exploration of complex but finite state spaces. We started
    looking at finite state spaces but took a turn to explore continuous or infinite
    state spaces. These types of agents also transition well to general games and
    robotics, which is not a major interest by the Chinese. Therefore, it remains
    to be seen how or what interest China shows in this area but that will most likely
    influence this space as well when that happens.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at our first framework and one that we may find
    the most familiar, Google Dopamine.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Google Dopamine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dopamine was developed at Google as a platform to showcase the company's latest
    advances in DRL. Of course, there are also other groups at Google doing the same
    thing, so it is perhaps a testament to how varied these platforms still are and
    need to be. In the next exercise, we will use Google Colab to build an example
    that uses Dopamine on the cloud to train an agent.
  prefs: []
  type: TYPE_NORMAL
- en: To access all of the features on Colab, you will likely need to create a Google
    account with payment authorized. This likely means entering a credit or debit
    card. The plus here is that Google provides $300 US in credits to use the GCP
    platform, of which Colab is one small part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your browser to [colab.research.google.com](http://colab.research.google.com) and
    follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: We will first start by creating a new **Python 3 Notebook**. Be sure to choose
    this by the prompt dialog or through the Colab **File** menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This notebook is based on a variation by the Dopamine authors and the original
    may be found in the following link: [https://github.com/google/dopamine/blob/master/dopamine/colab/agents.ipynb](https://github.com/google/dopamine/blob/master/dopamine/colab/agents.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to install several packages to support the training. On a Colab
    notebook, we can pass any command to the underlying shell by prefixing it with
    `!`. Enter the following code in a cell and then run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we do some imports and set up some global strings in a new cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `@param` function denotes the value as a parameter and this provides a
    helpful textbox on the interface for changing this parameter later. This is a
    cool notebook feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we run the preceding command and code in another new cell. This loads
    the data we will use to run the agent on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Create a new cell and enter the preceding code and run it. This creates a random
    DQN agent for more or less blindly exploring an environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we want to train the agent by creating a new cell and entering the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This can take a while, so if you have authorized payment enabled, you can run
    this example on a GPU instance by just changing the notebook type. You can do
    this by selecting from the menu **Runtime** | **Change runtime type**. A dialog
    will pop up; change the runtime type and close the dialog, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/10cffd3e-db75-433f-adb6-ec4a4fef591f.png)'
  prefs: []
  type: TYPE_IMG
- en: Changing the runtime type on Colab
  prefs: []
  type: TYPE_NORMAL
- en: After you change the runtime type, you will need to run the whole notebook again.
    To do this, select from the menu **Runtime | Run all** to run all of the cells
    again. You will still need to wait a while for the training to run; it is, after
    all, running an Atari environment but that is the point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent we just built is using a random DQN agent running on the classic Atari
    game, *Asterix*. Dopamine is a powerful framework that is easy to use as we have
    just seen. You can find much more about the library from the source itself, including
    how to output the results from the last example exercise.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will move away from Colab and explore another framework,
    Keras-RL with regular Python.
  prefs: []
  type: TYPE_NORMAL
- en: Playing with Keras-RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is a very popular deep learning framework on its own and it is heavily
    used by newcomers looking to learn about the basics of constructing networks.
    The framework is considered very high-level and abstracts most of the inner details
    of constructing networks. It only goes to assume that an RL framework built with
    Keras would attempt to do the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: This example is dependent on the version of Keras and TensorFlow and may not
    work correctly unless the two can work together. If you encounter trouble, try
    installing a different version of TensorFlow and try again.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this example, we will start by doing the installation and all of the
    setup in this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Keras, you should create a new virtual environment using Python
    3.6 and use `pip` to install it along with the `keras-rl` framework. The commands
    to do all of this on Anaconda are shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After all of the packages are installed, open the sample code file, `Chapter_12_Keras-RL.py`,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We haven't covered any Keras code but hopefully, the simple nature of the code
    makes it fairly self-explanatory. If anything, the code should feel quite familiar,
    although missing a training loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice in the proceeding code block how the model is constructed using something
    called a `Sequential` class. The class is a container for network layers, which
    we then add next interspersed with appropriate activation functions. Note at the
    end of the network, how the last layer uses a linear activation function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will take a closer look at the construction of memory, policy, and
    agent itself. See the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The interesting thing to note here is how we construct the network model outside
    the agent and feed it as an input to the agent along with the memory and policy.
    This is very powerful and provides for some interesting extensibility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the end of the file, we can find the training code. The training function
    called `fit` is used to iteratively train the agent. All of the code to do this
    is encapsulated in the `fit` function, as the following code shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The last section of code saves the model and then runs a test on the agent
    with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the code as you normally would and watch the visual training output and
    testing as shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/31a25484-5cd9-43cc-880e-88369ad378b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_12_Keras-RL.py
  prefs: []
  type: TYPE_NORMAL
- en: Keras-RL is a light powerful framework for testing concepts or other ideas quickly. The
    performance of Keras itself is not as powerful as TensorFlow or PyTorch, so any
    serious development should be done using one of those platforms. In the next section,
    we will look at another RL platform based on PyTorch called RLLib.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring RL Lib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL Lib is based on the Ray project, which is essentially a Python job-based
    system. RL Lib is more like ML-Agents, where it exposes functionality using config
    files although, in the case of ML-Agents, the structure is completely run on their
    platform. Ray is very powerful but requires a detailed understanding of the configuration
    parameters and setup. As such, the exercise we show here is just to demonstrate
    the power and flexibility of Ray but you are directed to the full online documentation
    for further discovery on your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your browser to [colab.research.google.com](http://colab.research.google.com) and
    follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The great thing about using Colab is it can be quite easy to run and set up.
    Create a new Python 3 notebook and enter the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These commands install the framework on the Colab instance. After this is installed,
    you need to restart the runtime by selecting from the menu: **Runtime | Restart
    runtime**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the runtime restarts, create a new cell and enter the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: That block of code imports the framework and the tune class for hyperparameter
    tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new cell and enter the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Believe it or not, that's it. That is the remainder of the code to build a DQN
    agent to run and train on the `CartPole` environment. Not to mention the `tune`
    class is set to tune the learning rate hyperparameter, `lr` (`alpha`), using the
    `tune.grid_search` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the last cell and observe the output. The output is extremely comprehensive
    and an example is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ab7b843c-19da-41af-85b0-f6cf8a7dd1ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Training RLLib on Google Colab
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding screenshot, this is a very powerful framework
    designed to optimize hyperparameter tuning and it provides plenty of options to
    do so. It also allows for multiagent training in various configurations. This
    framework is a must-study for anyone doing serious work or research in RL. In
    the next section, we will look at the last framework, TF-Agents.
  prefs: []
  type: TYPE_NORMAL
- en: Using TF-Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last framework we are going to look at is TF-Agents, a relatively new but
    up-and-coming tool, again, from Google. It seems Google's approach to building
    RL frameworks is a bit like RL itself. They are trying multiple trial and error
    attempts/actions to get the best reward—not entirely a bad idea for Google, and
    considering the resources they are throwing at RL, it may not unexpected to see
    more RL libraries come out.
  prefs: []
  type: TYPE_NORMAL
- en: TF-Agents, while newer, is typically seen as more robust and mature. It is a
    framework designed for notebooks and that makes it perfect for trying out various
    configurations, hyperparameters, or environments. The framework is developed on
    TensorFlow 2.0 and works beautifully on Google Colab. It will likely become the de-facto
    platform to teach basic RL concepts and demo RL in the future.
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of notebook examples to show how to use TF-Agents at the TF-Agents
    Colab repository ([https://github.com/tensorflow/agents/tree/master/tf_agents/colabs](https://github.com/tensorflow/agents/tree/master/tf_agents/colabs)). The
    whole repository is a great resource but this section itself can be especially
    useful for those of us that want to see working code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your browser up to the TF-Agents Colab page at the preceding link and
    follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we are going to modify the training environment for one of
    the samples. That should give us enough of an overview of what the code looks
    like and how to make changes yourself later on. Locate `1_dqn_tutorial.ipynb`
    and click on it to open the page. Note that `.ipynb` stands for **I-Python Notebook**;
    I-Python is a server platform for hosting notebooks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the link at the top that says **Run in Google Colab**. This will open
    the notebook in Colab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the menu, select **Runtime** | **Change runtime type** **to GPU** and then
    click **Save**. We are going to convert this example to use the Lunar Lander from
    Cart Pole. As we know, this will take more compute cycles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we will want to modify the initial `pip install` commands to import
    the full `gym` package by updating the commands in the first cell to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will want to locate the two cells that refer to the **CartPole** environment.
    We want to change all mentions of **CartPole** to **LunarLander**, as shown in
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm this example uses is a simple DQN model. As we know from experience,
    we can''t just run the same hyperparameters for `LunarLander`; therefore, we will
    change them to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`num_iterations`: 500000 from 20000'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial_collect_steps`: 20000 from 1000'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collect_steps_per_iteration`: 5 from 1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replay_buffer_max_length`: 250000 from 100000'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: 32 from 64'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: 1e-35 from 1e-3'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_interval`: 2000 from 200'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_eval_episodes`: 15 from 10'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_interval`: 500 from 1000'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s move on to adjusting the network size. Locate the following line of
    code and change it as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to change other parameters as you like. If you have done your homework,
    working with this example should be very straightforward. One of the great things
    about TF-Agents and Google Colab, in general, is how interactive sample and the
    training output is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This book was almost entirely written with Google Colab notebooks. However,
    as good as they are, notebooks still lack a few good elements needed for larger
    samples. They also make it difficult for several reasons to use later in other
    examples. Therefore a preference was given to keep the samples in Python files.
  prefs: []
  type: TYPE_NORMAL
- en: From the menu, select **Runtime** | **Run all**, to run the sample and then
    wait patiently for the output. This may take a while so grab a beverage and relax
    for a while.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the page, you will be able to see several other algorithm forms that we
    have covered and that we did not get time to cover in this book. The following
    is a list of the agent types TF-Agents currently supports and a brief description
    about each:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DQN**: This is the standard deep Q-learning network agent we have already
    looked at plenty of times. There isn''t a DDQN agent so it looks like you may
    need to just put two DQN agents together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**REINFORCE**: This is the first policy gradient method we looked at.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DDPG**: This is a PG method, more specifically, the deep deterministic policy
    gradient method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TD3**: This is best described as a clipped double Q-learning model that uses
    Actor-Critic to better describe the advantages in discrete action spaces. Typically,
    PG methods can perform poorly in discrete action spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PPO**: This is our old friend proximal policy optimization, another PG method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SAC**: This is based on soft Actor-Critic—an off-policy maximum entropy deep
    reinforcement learning with a stochastic actor. The basic reasoning here is the
    agent maximizes expected rewards by being as random as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-Agents is a nice stable platform that allows you to build up intuitive samples
    that you can train in the cloud very easily. This will likely make it a very popular
    framework for building proof-of-concept models for a variety of problems. In the
    next section, we will wrap up this chapter with the usual additional exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The exercises in this section are a bit wider in scope in this chapter in hopes
    you look through several frameworks on your own:'
  prefs: []
  type: TYPE_NORMAL
- en: Take some time and look at one of the frameworks listed earlier but not reviewed
    in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use SimpleRL to solve a grid-world MDP that is different than the one in the
    example. Be sure to take the time to tune hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Google Dopamine to train an agent to play the LunarLander environment. The
    best choice is likely RainbowDQN or a variation of that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Keras-RL to train an agent to play the Lunar Lander environment; make sure
    to spend time tuning hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use RL Lib to train an agent to play the Lunar Lander environment; make sure
    to spend time tuning hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the Keras-RL example and modify the network structure. Change the number
    of neurons and layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Modify the RL Lib example and change some of the hyperparameters such as the `num`
    workers and the number of GPUs, as shown in the following `tune` code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Modify the RLLib example and use a different agent type. You will likely have
    to check the documentation for RLLib to see what other agents are supported.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use TD3 from TF-Agents to train an agent to complete the Lunar Lander environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use SAC from TF-Agents and use it train the Lunar Lander environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to perform these exercises with Google Colab or in your favorite IDE.
    If you do use an IDE, you may need to take extra care to install some dependencies.
    In the next and last section of this chapter, we will finish up with the summary.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was a short but intense chapter in which we spent time looking at various
    third-party DRL frameworks. Fortunately, all of these frameworks are all still
    free and open source, and let's hope they stay that way. We started by looking
    at the many growing frameworks and some pros and cons. Then, we looked at what
    are currently the most popular or promising libraries. Starting with Google Dopamine,
    which showcases RainbowDQN, we looked at how to run a quick sample of Google Colab.
    After that, Keras-RL was next, and we introduced ourselves to the Keras framework
    as well as how to use the Keras-RL library. Moving on to RLLib, we looked at the
    powerful automation of the DRL framework that has many capabilities. Finally,
    we finished up this chapter with another entry from Google, TF-Agents, where we
    ran a complete DQN agent using TF-Agents on a Google Colab notebook.
  prefs: []
  type: TYPE_NORMAL
- en: We have spent plenty of time learning about and using RL and DRL algorithms.
    So much so, we should be fairly comfortable with training and looking to move
    on to more challenging environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will move on to training agents in more complex environments
    such as the real world. However, instead of the real world, we are going to use
    the next best thing: 3D worlds.'
  prefs: []
  type: TYPE_NORMAL
