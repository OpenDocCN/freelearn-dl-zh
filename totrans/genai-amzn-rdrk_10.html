<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-179"><a id="_idTextAnchor192"/>10</h1>
<h1 id="_idParaDest-180"><a id="_idTextAnchor193"/>Developing Intelligent Agents with Amazon Bedrock</h1>
<p>In this chapter, readers will explore the concept of agents in <strong class="bold">generative artificial intelligence</strong> (<strong class="bold">GenAI</strong>), diving<a id="_idIndexMarker822"/> into their importance, benefits, and the various tools and types available.</p>
<p>Readers will learn to build knowledge bases and develop agents specifically designed for Amazon Bedrock, gaining insights into configurations, testing, and deployment strategies. Additionally, the chapter will showcase real-world industrial use cases, highlighting the practical applications of Agents in conjunction with Amazon Bedrock.</p>
<p>Through hands-on examples, readers will acquire the skills to describe the role of agents, integrate LangChain Agents with Amazon Bedrock, understand Agents’ configurations for Amazon Bedrock, and build, test, and deploy agents tailored to this service. Furthermore, they will explore diverse industrial use cases, demonstrating the versatility and potential of Agents in enhancing the capabilities of Amazon Bedrock.</p>
<p>Specifically, readers will gain a clear understanding of the following:</p>
<ul>
<li>What are Agents?</li>
<li>GenAI agent personas, roles, and use-case scenarios</li>
<li>Amazon Bedrock integration with LangChain Agents</li>
<li>Agents for Amazon Bedrock</li>
<li>Deploying an Agent for Amazon Bedrock</li>
</ul>
<h1 id="_idParaDest-181"><a id="_idTextAnchor194"/>Technical requirements</h1>
<p>This chapter requires you to have access to an AWS account. If you don’t have one already, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create an AWS account.</p>
<p>Secondly, you will need to install and configure AWS CLI (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>) after you create an account, which will be needed to access Amazon Bedrock FMs from your local machine. Since the majority chunk of code cells we will be executing is based in Python, setting up an AWS Python SDK (Boto3) (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a>) would be beneficial at this point. You can carry out the Python setup in the following manner: install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or leverage Amazon SageMaker.</p>
<p class="callout-heading">Note</p>
<p class="callout">There will be a charge associated with the invocation and customization of Amazon Bedrock FMs. Please refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-182"><a id="_idTextAnchor195"/>What are Agents?</h1>
<p>In the dynamic landscape <a id="_idIndexMarker823"/>of AI, a new category of sophisticated systems has emerged, poised to significantly impact our relationship with technology. These entities, known as <strong class="bold">GenAI agents</strong>, represent a formidable advancement characterized by their autonomy, adaptability, and cognitive abilities.</p>
<p>GenAI agents are software constructs engineered to comprehend and execute complex tasks with minimal human oversight. They demonstrate a remarkable capacity to assess objectives, devise strategic plans, and execute actions to achieve goals, all while continuously learning and refining their approaches based on feedback.</p>
<p>What distinguishes GenAI agents from previous iterations is their multifaceted nature, combining reactivity, proactivity, learning capabilities, and social adeptness. This unique amalgamation enables them to promptly respond to changing environments, anticipate future needs, evolve through ongoing learning processes, and seamlessly collaborate with humans and fellow agents.</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor196"/>Features of agents</h2>
<p>Here are some of the <a id="_idIndexMarker824"/>key features of Agents:</p>
<ul>
<li><strong class="bold">Prompt response agility</strong>: GenAI Agents excel in swift response to prompts, ensuring agility in adapting to dynamic circumstances. For instance, in manufacturing settings, these agents can efficiently recalibrate processes and reallocate resources to maintain operational efficiency amid fluctuating demand or unexpected disruptions.</li>
<li><strong class="bold">Chain-of-thought (CoT)/Reasoning and Acting (ReAct)-style action</strong>: GenAI Agents take action based on CoT/ReAct-style prompts, ensuring prompt and effective responses to stimuli. If you recall, ReAct prompting is a method designed to prompt LLMs, aiming to enhance the accuracy of their responses compared to other methods such as CoT. For instance, in customer service, these agents can generate responses to inquiries based on predefined conversational cues.</li>
<li><strong class="bold">Prediction utility</strong>: Though GenAI Agents do not autonomously anticipate or predict future<a id="_idIndexMarker825"/> events, they act based on predefined parameters and inputs. In financial forecasting, these agents utilize historical data and predefined algorithms to generate predictions rather than autonomously forecasting future trends.</li>
<li><strong class="bold">Continuous learning</strong>: GenAI Agents possess the capability for continuous learning and self-improvement. Similar to human counterparts, these agents refine their strategies and decision-making processes based on outcomes, continuously evolving and optimizing performance. For instance, in online recommendation systems, these agents continuously learn from user interactions to improve personalized recommendations.</li>
<li><strong class="bold">Collaboration</strong>: GenAI Agents demonstrate adeptness in seamless collaboration with humans and other agents. Through effective communication and coordination, they enhance productivity and efficiency in interconnected environments. In autonomous vehicle systems, these agents collaborate with other vehicles and infrastructure to ensure safe and efficient navigation on roads.</li>
</ul>
<h2 id="_idParaDest-184"><a id="_idTextAnchor197"/>Practical applications of Agents – unleashing the potential</h2>
<p>GenAI Agents offer a wide<a id="_idIndexMarker826"/> array of applications across various industries, showcasing their versatility and potential impact. In finance, these agents are being experimented with to explore the potential to analyze market trends, detect investment opportunities, and execute trades, enhancing decision-making processes and optimizing investment strategies. For instance, they can utilize historical data and predictive algorithms to identify patterns and forecast market movements, aiding financial professionals in making informed investment decisions.</p>
<p>In logistics and <strong class="bold">supply chain management</strong> (<strong class="bold">SCM</strong>), GenAI Agents can play a crucial role in optimizing resource allocation, streamlining distribution networks, and minimizing operational costs. By leveraging real-time data and predictive analytics, these agents can optimize inventory management, route planning, and warehouse operations, improving efficiency and reducing lead times. For example, they can dynamically adjust shipping routes based on traffic conditions and demand fluctuations to ensure timely delivery of goods.</p>
<p>Furthermore, GenAI Agents have the potential to transform education by personalizing learning experiences for students. They can adapt curricula and teaching methods to individual student needs and learning styles, facilitating personalized learning pathways and improving student outcomes. For instance, these agents can analyze student performance data, identify areas for improvement, and recommend tailored learning resources and activities to address specific learning needs.</p>
<p>While GenAI Agents hold significant promise and potential across various industries, it’s essential to acknowledge that the widespread adoption and scaling of these technologies may still be in its early stages. While there are certainly organizations and research initiatives actively exploring and implementing GenAI solutions, achieving widespread adoption and scalability often requires overcoming various technical, regulatory, and ethical challenges.</p>
<p>In many cases, the practical implementation of GenAI Agents may involve pilot projects, <strong class="bold">proofs of concept</strong> (<strong class="bold">POCs</strong>), or targeted deployments rather than large-scale implementations across entire industries. Additionally, the effectiveness and impact of these technologies may vary depending on factors such as data quality, model performance, and the specific use case in question.</p>
<p>Therefore, it’s crucial to approach discussions about the applications of GenAI Agents with a critical lens, considering the current state of technology, existing limitations, and potential challenges<a id="_idIndexMarker827"/> associated with widespread adoption.</p>
<p>Now, let us take a step further into exploring diverse use case scenarios relevant to GenAI agents.</p>
<h1 id="_idParaDest-185"><a id="_idTextAnchor198"/>GenAI agent personas, roles, and use-case scenarios</h1>
<p>GenAI agents are heavily <a id="_idIndexMarker828"/>leveraged <a id="_idIndexMarker829"/>in enterprises<a id="_idIndexMarker830"/> as orchestrators. In the<a id="_idIndexMarker831"/> context of <a id="_idIndexMarker832"/>GenAI, <strong class="bold">orchestration</strong> refers to the coordinated management and deployment of these agents in various roles and use cases.</p>
<p>There are a variety of roles that GenAI agents can play as part of this orchestration. Some potential use cases of GenAI agents and the roles they could play include the following:</p>
<ul>
<li><strong class="bold">Virtual </strong><strong class="bold">assistant roles</strong>:<ul><li><em class="italic">Personal assistant</em>: Agents can act as personal assistants, helping individuals manage their schedules, set reminders, and handle tasks such as booking appointments, making reservations, or tracking deliveries.</li><li><em class="italic">Information lookup</em>: They can also serve as information lookup tools, providing quick access to knowledge across various domains, such as current events, sports, entertainment, or general reference queries.</li><li><em class="italic">Personalized recommenders</em>: By learning individual preferences and patterns, these agents can offer personalized recommendations and suggestions tailored to each user’s needs and interests.</li></ul></li>
<li><strong class="bold">Customer service </strong><strong class="bold">chatbot roles</strong>:<ul><li><em class="italic">Customer support agent</em>: Agents can handle customer inquiries and support requests 24/7, providing instant assistance and reducing wait times for human agents.</li><li><em class="italic">Product advisor</em>: They can guide customers through troubleshooting processes, provide product information, and offer personalized recommendations based on customer preferences and purchase history.</li><li>Chatbots can also handle routine tasks such as order tracking, refund requests, and account management, freeing up human agents for more complex issues, such as troubleshooting technical issues, providing personalized recommendations, resolving disputes, and offering specialized expertise.</li></ul></li>
<li><strong class="bold">Tutoring and </strong><strong class="bold">education roles</strong>:<ul><li><em class="italic">Tutors</em>: GenAI agents can serve as virtual tutors, providing personalized learning experiences adapted to each student’s pace, level, and learning style. They can explain complex concepts in engaging ways, provide practice exercises, and offer feedback and guidance to reinforce learning.</li><li><em class="italic">Domain experts</em>: Agents can also act as domain experts, offering in-depth knowledge and insights across various academic subjects or professional fields.</li></ul></li>
<li><strong class="bold">Content </strong><strong class="bold">creation roles</strong>:<ul><li><em class="italic">Writer</em>: Agents <a id="_idIndexMarker833"/>can <a id="_idIndexMarker834"/>assist<a id="_idIndexMarker835"/> writers, journalists, and content creators by generating initial drafts, outlines, or summaries based on provided prompts or ideas.</li><li><em class="italic">Storyteller</em>: They can help with ideation, story development, and creative writing tasks, offering suggestions and expanding on concepts.</li><li><em class="italic">Content generator</em>: Agents can also generate marketing copy, product descriptions, or social media content, helping businesses and creators save time and effort.</li></ul></li>
<li><strong class="bold">Creative </strong><strong class="bold">design roles</strong>:<ul><li><em class="italic">Designer</em>: Agents can serve as creative collaborators, generating design ideas, artwork, logos, and visual concepts based on provided prompts or specifications.</li><li><em class="italic">Artist</em>: They can offer inspiration and fresh perspectives, helping designers explore new directions and overcome creative blocks.</li><li><em class="italic">Creative collaborator</em>: Agents can also assist in the iterative design process, generating variations and refinements based on feedback and direction from human designers.</li></ul></li>
<li><strong class="bold">Gaming and </strong><strong class="bold">entertainment roles</strong>:<ul><li><em class="italic">Game character</em>: In gaming, agents can play the role of <strong class="bold">non-player characters</strong> (<strong class="bold">NPCs</strong>), providing<a id="_idIndexMarker836"/> engaging interactions, dialogue, and storylines that adapt to player actions and choices.</li><li><em class="italic">Interactive companion</em>: As interactive companions, agents can engage in conversations, tell stories, provide entertainment, and even offer emotional support or<a id="_idIndexMarker837"/> companionship<a id="_idIndexMarker838"/> in virtual <a id="_idIndexMarker839"/>environments.</li></ul></li>
<li><strong class="bold">Healthcare roles</strong>:<ul><li><em class="italic">Medical assistant</em>: Agents can act as medical assistants, providing patients with information about conditions, treatments, and healthy lifestyle recommendations.</li><li><em class="italic">Patient educator</em>: They can assist in patient education, explaining complex medical concepts in easy-to-understand language and addressing common concerns or questions.</li><li>Agents can also support healthcare professionals by summarizing patient records, generating medical reports, or assisting in research and data analysis tasks.</li></ul></li>
<li><strong class="bold">Software </strong><strong class="bold">development roles</strong>:<ul><li><em class="italic">Code assistant</em>: GenAI agents can serve as code assistants, helping developers write and optimize code by generating code snippets, suggesting improvements, or explaining programming concepts.</li><li><em class="italic">Documentation generator</em>: They can assist in generating documentation, automatically creating descriptions and explanations for code segments, making it easier to maintain and collaborate on projects.</li><li>Agents can also help with code refactoring, identifying areas for optimization, and suggesting more efficient or secure coding practices.</li></ul></li>
<li><strong class="bold">Research and </strong><strong class="bold">analysis roles</strong>:<ul><li><em class="italic">Research assistant</em>: Agents can act as research assistants, gathering and synthesizing information from various sources, generating summaries, and identifying key insights or trends.</li><li><em class="italic">Data analyst/data engineer</em>: They can assist in data analysis tasks, such as exploring<a id="_idIndexMarker840"/> datasets, identifying<a id="_idIndexMarker841"/> patterns, and <a id="_idIndexMarker842"/>generating visualizations or reports.</li><li><em class="italic">ML engineer/data scientist</em>: Agents can also support researchers by generating hypotheses, suggesting experimental designs, or offering ideas for further exploration based on existing knowledge and data.</li></ul></li>
<li><strong class="bold">Language </strong><strong class="bold">learning roles</strong>:<ul><li><em class="italic">Conversational partner</em>: GenAI agents can serve as conversational partners, allowing language learners to practice speaking and comprehension in a safe and encouraging environment.</li><li><em class="italic">Language tutors</em>: As language tutors, agents can provide feedback on pronunciation, grammar, and vocabulary, tailoring lessons and exercises to individual learners’ needs and progress.</li><li>They can also generate engaging language learning materials, such as dialogues, stories, or exercises, adapting the content and difficulty level based on the learner’s proficiency.</li></ul></li>
</ul>
<p>In all these cases, the agent’s role is to augment human capabilities, provide personalized <a id="_idIndexMarker843"/>assistance, and<a id="_idIndexMarker844"/> enhance<a id="_idIndexMarker845"/> efficiency and productivity in various domains.</p>
<p>Now, let us take a step further to explore Agents and their utility functionality in the context of Amazon Bedrock.</p>
<h1 id="_idParaDest-186"><a id="_idTextAnchor199"/>Amazon Bedrock integration with LangChain Agents</h1>
<p>LangChain, a<a id="_idIndexMarker846"/> powerful framework for <a id="_idIndexMarker847"/>developing applications with LLMs, provides a robust and flexible agent system that enables developers to build sophisticated agents capable of tackling a wide range of challenges.</p>
<p>An agent in LangChain is a high-level abstraction that combines an LLM, a set of tools, and other components in order to coordinate the execution of actions. The agent leverages the <a id="_idIndexMarker848"/>LLM’s <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) capabilities to interpret user inputs, determine the appropriate actions to take, and orchestrate the utilization of available tools to accomplish the desired task.</p>
<p>The core components of a LangChain agent include the following:</p>
<ul>
<li><code>WebSearchTool</code>: Performs web searches using a search engine such as Google.</li><li><code>WikipediaSearchTool</code>: Searches and retrieves information from <em class="italic">Wikipedia</em>.</li><li><code>PythonCallbackTool</code>: Executes Python functions as tools.</li><li><code>CSVTool</code>: Interacts with CSV files, allowing operations such as reading, writing, and querying data.</li></ul></li>
<li><strong class="bold">LLM</strong>: The LLM serves as the reasoning engine behind the agent. It is responsible for understanding the user’s intent, determining the appropriate actions to take, and interpreting the results of tool executions.</li>
<li><code>SQLDatabaseToolkit</code> contains tools for interacting with SQL databases, including querying, creating tables, and inserting data.</li>
<li><code>ConversationBufferMemory</code> stores the conversation history, allowing the agent to refer to previous inputs and outputs during the conversation.</li>
</ul>
<p>LangChain provides a diverse set of built-in tools and agent classes to cater to various use cases. Some of the available tools include the following:</p>
<ul>
<li><code>serpapi</code>: A search engine tool for querying web search results</li>
<li><code>google-search</code>: A wrapper around Google Search for conducting web searches</li>
<li><code>llm-math</code>: A tool for answering math-related questions using an LLM</li>
<li><code>open-meteo-api</code>: A tool for retrieving weather information from the Open-Meteo API</li>
<li><code>news-api</code>: A tool for fetching information about current news headlines</li>
<li><code>tmdb-api</code>: A tool for querying information<a id="_idIndexMarker851"/> from <strong class="bold">The Movie </strong><strong class="bold">Database</strong> (<strong class="bold">TMDB</strong>)</li>
<li><code>wolfram-alpha</code>: A tool for querying the Wolfram|Alpha computational knowledge engine</li>
<li><code>requests</code>: A tool for fetching content from specific URLs</li>
<li><code>terminal</code>: A tool for executing terminal commands</li>
<li><code>pal-math</code>: A language model specialized in solving complex word math problems</li>
<li><code>pal-colored-objects</code>: A language model for reasoning about positions and color attributes of objects</li>
</ul>
<p>As for agent <a id="_idIndexMarker852"/>classes, LangChain<a id="_idIndexMarker853"/> provides several options, each with its own strategy for action selection and problem-solving. Some examples include the following:</p>
<ul>
<li><code>zero-shot-react-description</code>: An agent that relies on the LLM’s zero-shot capabilities to select actions based on the tool descriptions</li>
<li><code>conversational-react-description</code>: An agent that engages in a conversational approach, asking for clarification when needed, and selecting actions based on the tool descriptions</li>
<li><code>react-docstore</code>: An agent that leverages a document store to retrieve relevant information and select actions based on the tool descriptions</li>
</ul>
<p>Details on each of the aforementioned tools and agent classes are highlighted here: <a href="https://api.python.langchain.com/en/latest/_modules/langchain/agents/agent_types.html">https://api.python.langchain.com/en/latest/_modules/langchain/agents/agent_types.html</a></p>
<p>Now that we have an understanding of LangChain Agents, let us dive into the integration of LangChain Agents with Amazon Bedrock, exemplified through a practical use case. We will import Bedrock via the LangChain package and leverage two LangChain tools, namely <code>YoutubeSearchTool</code> and <code>WikipediaTool</code>, enabling the LangChain Agent to leverage their capabilities. Additionally, we will be utilizing the Anthropic <a id="_idIndexMarker854"/>Claude <a id="_idIndexMarker855"/>model through Bedrock in this straightforward application.</p>
<p class="callout-heading">Note</p>
<p class="callout">Ensure you have the correct permissions to invoke Amazon Bedrock, as explained in the earlier chapters. Further, please ensure that the latest version of LangChain packages and libraries are installed as per the code. In case the packages are not installed, run the following command in your Jupyter notebook (note that <code>! Or %</code> will not be needed if you’re running Python code from a Python terminal):</p>
<p class="callout"><code>%pip </code><code>install &lt;library_name&gt;</code></p>
<pre class="source-code">
# Install the respective packages for YoutubeSearchTool and Wikipedia Tool in your Jupyter Notebook
%pip install --upgrade --quiet wikipedia youtube_search langchain
#Install LangChain modules
%pip install -U langchain-community
%pip install -U langchain-aws langchainhub
# Import langchain libaries for Tools, Agents and Amazon Bedrock
from langchain.agents import AgentType
from langchain.agents import initialize_agent, Tool
from langchain_aws import BedrockLLM
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.agents import AgentExecutor, create_react_agent
from langchain import hub
# Import respective packages for Wikipedia and Youtube tools
from langchain_community.tools import YouTubeSearchTool
from langchain_community.utilities import WikipediaAPIWrapper
# Using anthropic model with langchain
llm = BedrockLLM(model_id="anthropic.claude-v2")
# Define Tools below
wikipedia_wrapper = WikipediaAPIWrapper()
# Wikipedia Tool
wikipedia_tool = Tool(
        name="Wikipedia",
        func=wikipedia_wrapper.run,
        description="Useful tool for finding information on the Internet related to world events, issues, etc. Worth using for general topics. Use precise questions.",)
youtube_wrapper = YouTubeSearchTool()
#Youtube Tool
youtube_tool = Tool(name= "Youtube", func = youtube_wrapper.run, description = "Useful tool for searching youtube videos on and sharing the youtube links to the user. Use precise questions.")
#Create a memory instance
conversational_memory = ConversationBufferMemory()
prompt = hub.pull("hwchase17/react")
memory = conversational_memory
model = llm
tools = [wikipedia_tool,youtube_tool]
# Create an agent
agent = create_react_agent(model, tools, prompt = prompt)
# Create an agent executor
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
#Run the agent executor
response = agent_executor.invoke({"input": "Memristor?"})
print(response)</pre>
<p>The generated output looks like this:</p>
<pre class="console">
{'input': 'Memristor?', 'output': 'A memristor is a non-linear two-terminal electrical component that relates electric charge and magnetic flux linkage. It was first theorized by Leon Chua in 1971 as the fourth fundamental circuit element alongside the resistor, capacitor and inductor. Unlike those components, memristors exhibit memory - their resistance depends on the history of current that has previously flowed through them. Memristors have applications in memory, computing, and neuromorphic/brain-inspired computing due to their ability to store and process information. They are an active area of research in nanoelectronics.'}</pre>
<p>The output may look like what’s shown in <em class="italic">Figure 10</em><em class="italic">.1</em> and <em class="italic">Figure 10</em><em class="italic">.2</em>. You will notice the <a id="_idIndexMarker856"/>agent <a id="_idIndexMarker857"/>performing reasoning using question, thought, action, and chaining:  </p>
<div><div><img alt="Figure 10.1 – ﻿AgentExecutor chain output" src="img/B22045_10_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – AgentExecutor chain output</p>
<div><div><img alt="Figure 10.2 – Generated chain using LangChain agents with Bedrock" src="img/B22045_10_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Generated chain using LangChain agents with Bedrock</p>
<p>As shown in this section, when we are exploring technical topics such as memristors without any other context, the <em class="italic">Wikipedia</em> tool agent is invoked, providing comprehensive and detailed information, followed by the YouTube tool to provide additional information on the subject matter. In case the user writes <code>Elon Musk video on Neuralink</code> in the preceding conversation with the agent, the YouTube tool gets invoked and links are showcased to said user. Users are encouraged to try out different<a id="_idIndexMarker858"/> questions<a id="_idIndexMarker859"/> and test the agent.</p>
<p>Here is a sample output response for <code>response = agent_executor.invoke({"input": "Elon Musk video </code><code>on Neuralink"})</code>:</p>
<pre class="console">
&gt; Entering new AgentExecutor chain...
 Here is my process for answering your question about finding a video of Elon Musk discussing Neuralink:
Question: Elon Musk video on Neuralink
Thought: Elon Musk has given talks and interviews about Neuralink. YouTube would be a good place to search for videos of him discussing it.
Action: Youtube
Action Input: "elon musk neuralink"
['https://www.youtube.com/watch?v=tN1lVwTHCMw&amp;pp=ygUVZWxvbiBtdXNrIG5ldXJhbGluayIK', 'https://www.youtube.com/watch?v=k0I9Z-ARbjo&amp;pp=ygUVZWxvbiBtdXNrIG5ldXJhbGluayIK'] Here is my thought process for answering your question:
Question: Elon Musk video on Neuralink
Thought: Elon Musk has discussed Neuralink, his brain-machine interface company, in various interviews and presentations. YouTube would be a good place to search for videos of him talking about Neuralink.
Action: Youtube
Action Input: "elon musk neuralink"
['https://www.youtube.com/watch?v=tN1lVwTHCMw&amp;pp=ygUVZWxvbiBtdXNrIG5ldXJhbGluayIK', 'https://www.youtube.com/watch?v=k0I9Z-ARbjo&amp;pp=ygUVZWxvbiBtdXNrIG5ldXJhbGluayIK'] Here is my thought process and answer:
Question: Elon Musk video on Neuralink
Thought: Elon Musk has given presentations and interviews about Neuralink, the brain-computer interface company he founded. YouTube would be a good place to search for videos of him discussing Neuralink and its technology.
...
https://www.youtube.com/watch?v=tN1lVwTHCMw
https://www.youtube.com/watch?v=k0I9Z-ARbjo
&gt; Finished chain.</pre>
<p>As demonstrated, utilizing these tools enables us to access in-depth knowledge on the subject matter. It’s worth noting that LangChain offers the capability to create custom tools as well, further expanding the capabilities of agents. This flexibility is highlighted in the documentation at <a href="https://python.langchain.com/docs/modules/agents/tools/custom_tools">https://python.langchain.com/docs/modules/agents/tools/custom_tools</a>, where you can find guidance on crafting tailored tools to suit your specific needs.</p>
<p>As shown earlier, since we are searching for a technical topic for memristors, we get detailed information using the <em class="italic">Wikipedia</em> tool agent. You can create custom tools as well with agents, as shown here: <a href="https://python.langchain.com/docs/modules/agents/tools/custom_tools">https://python.langchain.com/docs/modules/agents/tools/custom_tools</a>.</p>
<p>You can check out how to build GenAI agents with Amazon Bedrock and LangChain, coupled with Amazon Kendra, Amazon DynamoDB, and Amazon Lex, on the following blog: <a href="https://aws.amazon.com/blogs/machine-learning/build-generative-ai-agents-with-amazon-bedrock-amazon-dynamodb-amazon-kendra-amazon-lex-and-langchain/">https://aws.amazon.com/blogs/machine-learning/build-generative-ai-agents-with-amazon-bedrock-amazon-dynamodb-amazon-kendra-amazon-lex-and-langchain/</a>.</p>
<p>By effectively integrating Amazon Bedrock with LangChain Agents, organizations can unlock the full potential of LLMs, enabling the development of intelligent and context-aware applications that drive innovation, automate complex workflows, and deliver exceptional <a id="_idIndexMarker860"/>user <a id="_idIndexMarker861"/>experiences.</p>
<p>Now, let’s jump into building Amazon Bedrock Agents for leveraging an end-to-end GenAI application.</p>
<h1 id="_idParaDest-187"><a id="_idTextAnchor200"/>Agents for Amazon Bedrock</h1>
<p>One of the <a id="_idIndexMarker862"/>powerful capabilities offered by Amazon Bedrock is<a id="_idIndexMarker863"/> the ability to build and configure autonomous agents within your applications. These agents act as intelligent assistants, helping end users complete tasks based on organizational data and user input. Agents orchestrate interactions between FMs (LLMs), data sources, software applications, and user conversations. They can automatically call APIs to take actions and invoke knowledge bases to supplement information for these actions. By integrating agents, developers can save weeks of development effort and accelerate the delivery of GenAI applications.</p>
<p>Agents on Amazon Bedrock are designed to automate tasks for customers and provide intelligent responses to their questions. For example, you could create an agent that assists customers in processing insurance claims or making travel reservations. The beauty of agents is that you don’t have to worry about provisioning capacity, managing infrastructure, or writing custom code from scratch. Amazon Bedrock handles the complexities of prompt engineering, memory management, monitoring, encryption, user permissions, and API invocation.</p>
<p>Agents on Amazon Bedrock perform the following key tasks:</p>
<ul>
<li><strong class="bold">Extend FMs</strong>: Agents leverage LLMs to understand user requests and break down complex tasks into smaller, manageable steps.</li>
<li><strong class="bold">Collect additional information</strong>: Through natural conversation, agents can gather additional information from users to fulfill their requests effectively.</li>
<li><strong class="bold">Take actions</strong>: Agents can make API calls to your company’s systems to perform actions and fulfill customer requests.</li>
<li><strong class="bold">Augment performance and accuracy</strong>: By querying data sources and knowledge bases, agents can enhance their performance and provide more accurate responses.</li>
</ul>
<p>In order to harness<a id="_idIndexMarker864"/> the<a id="_idIndexMarker865"/> power of Agents for Amazon Bedrock, developers follow a straightforward process:</p>
<ol>
<li>Create a knowledge base to store your organization’s private data, which can be used to enhance the agent’s performance and accuracy. This step is <em class="italic">optional </em>because not all agents require access to private organizational data to carry out their assigned objectives. If the agent’s tasks and objectives do not depend on or benefit significantly from access to such data, creating a knowledge base may not be necessary. It depends on the specific use case and requirements of the agent being developed.</li>
<li>Configure an agent for your specific use case, defining the actions it can perform. Lambda functions, written in your preferred programming language, dictate how the agent handles these actions. This is an optional step as an agent doesn’t necessarily require an action group to be created.</li>
<li>Associate the agent with a knowledge base to augment its capabilities further.</li>
<li>Customize the agent’s behavior by modifying prompt templates for preprocessing, orchestration, knowledge-base response generation, and postprocessing steps. Note that <em class="italic">not</em> all agents require extensive modification of prompt templates for their goal. The need for customization depends on the complexity of the tasks the agent is expected to perform and the level of control and fine-tuning desired by developers. For simpler tasks or generic use cases, the default prompt templates may suffice, making extensive customization unnecessary.</li>
<li>Test the agent using the Amazon Bedrock console or API calls, modifying configurations as necessary. Utilize traces to gain insights into the agent’s reasoning process at each step of its orchestration.</li>
<li>When the agent is ready for deployment, create an alias that points to a specific version of the agent.</li>
<li>Integrate your application with the agent alias, enabling seamless API calls and interactions.</li>
<li>Iterate on the agent as needed, creating new versions and aliases to adapt to changing requirements.</li>
</ol>
<p>Throughout the development process, Amazon Bedrock handles the complexities of prompt <a id="_idIndexMarker866"/>engineering, memory <a id="_idIndexMarker867"/>management, monitoring, encryption, user permissions, and API invocation, allowing you to focus on building intelligent agents tailored to your specific use cases.</p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor201"/>Unveiling the inner workings of GenAI agents with Amazon Bedrock</h2>
<p>When delving into<a id="_idIndexMarker868"/> the realm of Amazon Bedrock, one encounters a powerful toolset designed to facilitate the creation and management of intelligent agents. This toolset is composed of two distinct categories of API operations, each serving a specific purpose in the agent’s life cycle:</p>
<ul>
<li>The first category, aptly termed <em class="italic">build-time API operations</em>, enables developers to construct, configure, and oversee their agents and their associated resources. These operations act as the foundational building blocks, enabling the creation of agents tailored to specific requirements and objectives. Through these APIs, developers can fine-tune various aspects of their agents, ensuring they are equipped with the necessary capabilities to tackle the tasks at hand. More details on build-time API operations are listed here: <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Agents_for_Amazon_Bedrock.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Agents_for_Amazon_Bedrock.html</a></li>
<li>The second category, <em class="italic">runtime API operations</em>, breathes life into agents, allowing them to interact with user input and initiate an intricate orchestration process to accomplish their designated tasks. When a user provides input, these APIs enable the agent to process and interpret the information, triggering a sequence of actions that ultimately lead to the desired outcome.</li>
</ul>
<p>Now, let us dive into build-time and runtime configurations.</p>
<h3>Build-time configuration</h3>
<p>During the build phase, an <a id="_idIndexMarker869"/>agent is assembled from the following key components:</p>
<ul>
<li><strong class="bold">FM</strong>: You select a pre-trained language model that the agent employs to interpret user input, generate responses, and guide its decision-making process.</li>
<li><strong class="bold">Instructional prompts</strong>: You craft instructions that delineate the agent’s purpose and desired behavior. With advanced prompting techniques, you can dynamically tailor these instructions at each stage of the agent’s workflow and incorporate custom logic through serverless functions.</li>
<li><strong class="bold">Action groups</strong>: You define actions the agent can perform by providing the following:<ul><li>An OpenAPI schema specification that outlines the operations the agent can invoke.</li><li>A serverless function that executes the specified API operation based on the agent’s input and returns the result.</li></ul></li>
<li><strong class="bold">Knowledge bases</strong>: You can associate knowledge bases with the agent, allowing it to retrieve relevant context to enhance its response generation and decision-making capabilities.</li>
<li><strong class="bold">Prompt templates</strong>: The orchestrator exposes default prompt templates used during various stages of the agent’s life cycle, such as preprocessing input, orchestrating actions, querying knowledge bases, and postprocessing outputs. You can customize these templates to modify the agent’s behavior or disable specific stages as needed.</li>
</ul>
<p>During the build <a id="_idIndexMarker870"/>process, these components are combined to create base prompts that guide the agent’s orchestration flow until the user’s request is fulfilled. With advanced prompting techniques, you can augment these base prompts with additional logic, examples, and metadata to improve the agent’s accuracy and performance at each stage of its invocation. After configuring the agent’s components and security settings, you can prepare the agent for deployment and testing in a runtime environment, as shown in <em class="italic">Figure 10</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 10.3 – Build-time API operations for Agent creation" src="img/B22045_10_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Build-time API operations for Agent creation</p>
<h3>Runtime process</h3>
<p>At the hear<a id="_idIndexMarker871"/>t of this runtime process lies the <code>InvokeAgent</code> API operation, a powerful conductor that sets the agent sequence in motion. The agent’s performance unfolds in three harmonious acts: preprocessing, orchestration, and postprocessing.</p>
<p><strong class="bold">Act I – </strong><strong class="bold">Preprocessing</strong></p>
<p>Before the curtains rise, the preprocessing phase meticulously manages how the agent contextualizes and categorizes user input. This crucial step can also validate the input, ensuring a seamless transition to the subsequent stages.</p>
<p><strong class="bold">Act II – Orchestration – the </strong><strong class="bold">grand performance</strong></p>
<p>The orchestration phase is where the true magic unfolds, a symphonic interplay of interpretation, invocation, and knowledge synthesis. This act consists of the following movements:</p>
<ol>
<li><strong class="bold">Interpretation</strong>: The agent deftly interprets the user input with an FM, generating a rationale that lays out the logical path for the next steps.</li>
<li><strong class="bold">Invocation and synthesis</strong>: Like a skilled conductor, the agent invokes action groups and queries knowledge bases, retrieving additional context and summarizing the data to augment its generation capabilities.</li>
<li><strong class="bold">Observation and augmentation</strong>: From the invoked action groups and summarized knowledge-base results, the agent generates an output, known as an <strong class="bold">observation</strong>. This observation is then used to enrich the base prompt, which is subsequently interpreted by the FM. The agent then determines if further orchestration iterations are necessary.</li>
</ol>
<p>This iterative loop continues until the agent delivers its final response to the user or requires additional information from the user.</p>
<p>Throughout the orchestration phase, the base prompt template is augmented with agent instructions, action groups, and knowledge bases, creating a rich tapestry of information. This enhanced base prompt is then fed into the FM, which predicts the optimal trajectory to fulfill the user’s request. At each iteration, the FM selects the appropriate API <a id="_idIndexMarker872"/>operation or knowledge-base query, resulting in a responsive and contextually accurate output.</p>
<p><strong class="bold">Act III – Postprocessing – </strong><strong class="bold">the finale</strong></p>
<p>In the final act, the postprocessing phase, the agent formats the culmination of its efforts – the final response to be returned to the user. However, this step can be gracefully bypassed, leaving the performance open to interpretation.</p>
<p>During the agent’s performance, users have the option to invoke a trace at runtime, unlocking a window into the agent’s thought process. This trace meticulously tracks the agent’s rationale, actions, queries, and observations at each step of the sequence. It includes the full prompts sent to the FM, as well as outputs from the model, API responses, and knowledge-base queries. By examining this trace, users can gain invaluable insights into the agent’s reasoning, paving the way for continuous improvement and refinement.</p>
<p>As the user’s session with the agent continues through successive <code>InvokeAgent</code> requests, the conversation history is diligently preserved, continually augmenting the orchestration base prompt template with context. This enrichment process aids in enhancing the agent’s accuracy and performance, forging a symbiotic relationship between the user and the AI.</p>
<p>The agent’s process during runtime is a captivating interplay of interpretation, synthesis, and adaptation, as <a id="_idIndexMarker873"/>showcased in <em class="italic">Figure 10</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 10.4 – Runtime process flow for Agent workflow" src="img/B22045_10_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Runtime process flow for Agent workflow</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor202"/>Advancing reasoning capabilities with GenAI – a primer on ReAct</h2>
<p>GenAI models have<a id="_idIndexMarker874"/> demonstrated splendid capabilities in processing and generating human-like text, but their ability to reason through complex tasks and provide step-by-step solutions remains a challenge. Yao et. al have developed a technique called ReAct, as articulated in the paper <em class="italic">ReAct: Synergizing Reasoning and Acting in Language Models</em> (<a href="https://arxiv.org/abs/2210.03629">https://arxiv.org/abs/2210.03629</a>), to enhance the reasoning abilities of these models, enabling them to systematically approach and solve user-requested tasks.</p>
<p>The ReAct technique involves structuring prompts that guide the model through a sequence of reasoning steps and corresponding actions. These prompts consist of a series of <em class="italic">question-thought-action-observation</em> examples, where the following applies:</p>
<ul>
<li>The <em class="italic">question</em> represents the user-requested task or problem to be solved</li>
<li>The <em class="italic">thought</em> is a reasoning step that demonstrates how to approach the problem and identify a potential action</li>
<li>The <em class="italic">action</em> is an API call or function that the model can invoke from a predefined set of allowed operations</li>
<li>The <em class="italic">observation</em> is the result or output obtained from executing the chosen action</li>
</ul>
<p>The set of allowed actions is defined by instructions prepended to the example prompt text. This structured approach encourages the model to engage in a step-by-step reasoning process, breaking down complex tasks into smaller, actionable steps.</p>
<p>To illustrate the <a id="_idIndexMarker875"/>construction of a ReAct prompt, consider the following example prompt structure with question-thought-action-observation sequences:</p>
<p><strong class="bold">Example 1:</strong></p>
<ul>
<li><strong class="bold">Question</strong>: What is the optimal inventory level to minimize stockouts?</li>
<li><strong class="bold">Thought</strong>: To avoid stockouts, we must balance inventory levels based on demand forecasts and reorder points.</li>
<li><code>optimizeInventoryLevels</code> function using historical sales data and demand projections.</li>
<li><strong class="bold">Observation</strong>: Maintaining inventory at 80% of forecasted demand reduced stockouts by 30% while optimizing carrying costs.</li>
</ul>
<p><strong class="bold">Example 2</strong>:</p>
<ul>
<li><strong class="bold">Question</strong>: How can we improve customer satisfaction ratings?</li>
<li><strong class="bold">Thought</strong>: To enhance satisfaction, we should analyze feedback data and implement targeted improvements.</li>
<li><code>analyzeCustomerFeedback</code> API to identify trends and insights.</li>
<li><strong class="bold">Observation</strong>: Based on the analysis, implementing personalized customer support led to a 20% increase in satisfaction scores.</li>
</ul>
<p>These examples demonstrate how the ReAct technique guides the model through reasoning steps, leading to actionable outcomes.</p>
<p>While the process of manually crafting these prompts can be time-consuming and intricate, the Amazon Bedrock Agent streamlines this process by automatically generating the prompts based on the provided information and available actions. Bedrock agents handle the complexities of prompt engineering, allowing researchers and developers to focus on defining the task requirements and available actions.</p>
<p>Readers are encouraged to check out <a href="https://github.com/aws-samples/agentsforbedrock-retailagent">https://github.com/aws-samples/agentsforbedrock-retailagent</a>, which uncovers the creation of an FM-powered customer service bot by leveraging Agents for Amazon Bedrock.</p>
<p>The ReAct technique and Bedrock Agents represent a significant advancement in the field of GenAI, enabling models to demonstrate improved reasoning abilities and tackle complex tasks more effectively. By providing a structured approach to problem-solving and leveraging the power of prompts, this technique has the potential to unlock new possibilities and <a id="_idIndexMarker876"/>applications for GenAI in various domains. Let us explore the functioning of Amazon Bedrock Agents with some practical use cases.</p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor203"/>Practical use case and functioning with Amazon Bedrock Agents</h2>
<p>In this section, we <a id="_idIndexMarker877"/>will dive into real-world applications and operational insights of leveraging Amazon Bedrock Agents in GenAI. Let us consider an example scenario of a multilingual summarizer bot, wherein a GenAI agent can be employed to streamline operations and automate how to translate the content in a summarized fashion in the language of the user’s choice. In order to begin, the developer must access the Bedrock console and initiate the agent creation workflow, as highlighted in <em class="italic">Figure 10</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 10.5 – Agent creation within the Bedrock console" src="img/B22045_10_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Agent creation within the Bedrock console</p>
<p>This process involves <a id="_idIndexMarker878"/>providing essential details, such as the agent’s name, description, and the necessary permissions through an<a id="_idIndexMarker879"/> AWS <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) service role. This role grants the agent access to required services<a id="_idIndexMarker880"/> such as <strong class="bold">Amazon Simple Storage Service</strong> (<strong class="bold">Amazon S3</strong>) and AWS Lambda, as illustrated in <em class="italic">Figure 10</em><em class="italic">.6</em>. As an example, the figure demonstrates the creation of a multilingual document summarizer and translator agent for extracting relevant information from the documents and relaying the information to the user in the translated language:</p>
<div><div><img alt="Figure 10.6 – Bedrock Agent creation process with IAM permissions" src="img/B22045_10_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Bedrock Agent creation process with IAM permissions</p>
<p>By default, Amazon Bedrock employs encryption for agent sessions with users, utilizing a key that AWS owns and manages on your behalf. However, if you prefer to use a customer-managed key<a id="_idIndexMarker881"/> from AWS <strong class="bold">Key Management Service</strong> (<strong class="bold">KMS</strong>) that you have set up, you have<a id="_idIndexMarker882"/> the option to customize your encryption settings accordingly. This allows you to take control of the encryption key used for securing agent-user interactions, aligning with your organization’s security and compliance requirements.</p>
<p>Next, the developer selects an FM from Bedrock that aligns with the desired use case. This step involves providing natural language instructions that define the agent’s task and the persona it should assume. For instance, in the example demonstrated in <em class="italic">Figure 10</em><em class="italic">.7</em>, the instruction could be <code>You are a multi-lingual agent designed to help with extracting inquired information from relevant documents and providing the response in </code><code>translated language</code>:</p>
<div><div><img alt="Figure 10.7 – Amazon Bedrock Agent configuration for model selection and Agent persona" src="img/B22045_10_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Amazon Bedrock Agent configuration for model selection and Agent persona</p>
<p>The console also provides the option for the user to select guardrails to implement application-specific<a id="_idIndexMarker883"/> safeguards abiding by responsible AI policies. For simplicity, we can leave this blank and move to the next section. We will be covering guardrails in detail in <a href="B22045_12.xhtml#_idTextAnchor226"><em class="italic">Chapter 12</em></a>.</p>
<p>Subsequently, the developer adds action groups, which are sets of tasks the agent can perform automatically by making API calls to the company’s systems. This step involves defining an API schema that outlines the APIs for all actions within a group and providing a Lambda function that encapsulates the business logic for each API. For example, an action group named <code>Summarizer_Translator_ActionGroup</code> could handle documents stored either in a database or within a particular location, identifying the information requested by the user and sending a summarized response to the user in the translated language inquired by the user. <em class="italic">Figure 10</em><em class="italic">.8</em> showcases the creation of an action group to handle tasks for agents to execute autonomously:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 10.8 – Creating Bedrock Agent’s action group" src="img/B22045_10_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Creating Bedrock Agent’s action group</p>
<p>As shown previously, you will have to create a Lambda function to handle incoming requests from the agents and select an API schema. Please ensure you have provided the right permissions to your AWS Lambda function to invoke Bedrock agents.</p>
<p>For the case of <a id="_idIndexMarker884"/>document identification, summarization, and translation, we have provided the following Lambda function that users can leverage for executing the workflow:</p>
<pre class="source-code">
import json
import time
import boto3
# Define a mock dictionary with document IDs and content
Document_id  = {
    "doc_1": {
        "title": "The Importance of Mindfulness",
        "author": "Jane Smith",
        "content": "Mindfulness is the practice of being fully present and engaged in the current moment, without judgment or distraction. It involves paying attention to your thoughts, feelings, and bodily sensations with a curious and non-judgmental attitude. By cultivating mindfulness, you can reduce stress, improve emotional regulation, and enhance overall well-being. In this document, we will explore the benefits of mindfulness and provide practical techniques for incorporating it into your daily life."
      },
    "doc_2": {
        "title": "Sustainable Living: A Guide to Eco-Friendly Practices",
        "author": "Michael Johnson",
        "content": "In today's world, it's essential to adopt sustainable living practices to protect our planet's resources and ensure a better future for generations to come. This document will provide you with practical tips and strategies for reducing your environmental impact in various aspects of your life, such as energy consumption, waste management, transportation, and food choices. Together, we can make a significant difference by embracing eco-friendly habits and promoting a more sustainable lifestyle."
      },
    "doc_3": {
        "title": "The Art of Effective Communication",
        "author": "Emily Davis",
        "content": "Effective communication is a crucial skill in both personal and professional settings. It involves the ability to convey your thoughts, ideas, and emotions clearly and respectfully, while also actively listening and understanding the perspectives of others. In this document, we will explore the key elements of effective communication, such as active listening, nonverbal cues, and empathy. By mastering these techniques, you can improve your relationships, resolve conflicts more effectively, and achieve greater success in your personal and professional endeavors."
      }
    }
def getDocID(event):
        docID = event['parameters'][0]['value']
        print("NAME PRINTED: ", docID)
        if(docID== "doc_1" or "doc1"):
            return Document_id["doc_1"]["content"]
        elif docID == "doc_2" or "doc2":
            return Document_id["doc_2"]["content"]
        elif docID == "doc_3" or "doc3":
            return Document_id["doc_3"]["content"]
        else:
            return "No document found by that ID"
def lambda_handler(event, context):
    response_code = 200
    """Main lambda handler directing requests based on the API path, preserving the specified response structure."""
    print("event OUTPUT : ")
    print(event)
    action_group = event.get("actionGroup")
    print("action group :" + str(action_group))
    api_path = event.get("apiPath")
    print ("api_path : " + str(api_path))
    result = ''
    response_code = 200
    if api_path == '/getDoc':
        result = getDocID(event)
        print(result)
    else:
        response_code = 404
        result = f"Unrecognized api path: {action_group}::{api_path}"
    response_body = {
        'application/json': {
            'body': result
        }
    }
    action_response = {
        'actionGroup': event['actionGroup'],
        'apiPath': event['apiPath'],
        'httpMethod': event['httpMethod'],
        'httpStatusCode': response_code,
        'responseBody': response_body
    }
    api_response = {'messageVersion': '1.0', 'response': action_response}
    return api_response</pre>
<p>Users running the <a id="_idIndexMarker885"/>preceding workflow can also use the following OpenAPI schema and store it in S3, as part of this example:</p>
<pre class="source-code">
{
    "openapi": "3.0.1",
    "info": {
        "title": "DocSummarizerTranslator API",
        "version": "1.0.0",
        "description": "APIs for fetching, translating and summarizing docs by fetching the document ID and identifying the language to translate the document"
    },
    "paths": {
        "/getDoc": {
            "get": {
                "description": "Get the document content for a document by document ID.",
                "operationId": "getDoc",
                "parameters": [
                    {
                        "name": "DocID",
                        "in": "query",
                        "description": "ID of the document to retrieve",
                        "required": true,
                        "schema": {
                            "type": "string"}}],
                "responses": {
                    "200": {
                        "description": "Successful response with document content data",
                        "content": {
                            "text/plain": {
                                "schema": {
                                    "type": "string"
                                }}}}}}},
        "/getDoc/summarize": {
            "get": {
                "description": "Summarize the content of the document for given document ID",
                "operationId": "summarizeDoc",
                "parameters": [
                    {
                        "name": "DocID",
                        "in": "query",
                        "description": "ID of the document to summarize",
                        "required": true,
                        "schema": {
                            "type": "string"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful response with the summary of the document content for given document ID",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "type": "string",
                                    "properties": {
                                        "summary": {
                                            "type": "string",
                                            "description": "Summary of the document"}}}}}}}}}}}</pre>
<p>In the next step, users<a id="_idIndexMarker886"/> have the option to select a knowledge base, as depicted in <em class="italic">Figure 10</em><em class="italic">.9</em>. This showcases the power of Bedrock Agents to easily create a RAG-based solution for extracting information from relevant sources stored in the knowledge base, by performing similarity searches and providing desired responses back to the user. For simplicity, we will ignore that and move to the final creation step:</p>
<div><div><img alt="Figure 10.9 – Knowledge-base creation with Bedrock Agents integration" src="img/B22045_10_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Knowledge-base creation with Bedrock Agents integration</p>
<p class="callout-heading">Note</p>
<p class="callout">If you would like to dive deep into use cases involving knowledge-base integration with your agents, you can execute the following code samples: <a href="https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/05_Agents/insurance_claims_agent/with_kb">https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/05_Agents/insurance_claims_agent/with_kb</a>.</p>
<p class="callout">Additional code within the GitHub repository further illustrates how to create and invoke Bedrock Agents with the Python SDK, as evidenced in the following notebook: <a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/05_Agents/insurance_claims_agent/with_kb/create_and_invoke_agent_with_kb.ipynb">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/05_Agents/insurance_claims_agent/with_kb/create_and_invoke_agent_with_kb.ipynb</a>.</p>
<p>Once the<a id="_idIndexMarker887"/> preceding steps are done, you can verify the agent configuration and select <strong class="bold">Create Agent</strong>. Congratulations on creating your Amazon Bedrock Agent (<em class="italic">Figure 10</em><em class="italic">.10</em>)!</p>
<div><div><img alt="Figure 10.10 – Amazon Bedrock Agent version" src="img/B22045_10_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Amazon Bedrock Agent version</p>
<p>On the right<a id="_idIndexMarker888"/> side of the screen, you can easily test your agent by asking it questions about the document and requesting it to summarize and translate the document into your desired language, as shown in <em class="italic">Figure 10</em><em class="italic">.11</em>:</p>
<div><div><img alt="Figure 10.11 – Testing Bedrock Agent within AWS console" src="img/B22045_10_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – Testing Bedrock Agent within AWS console</p>
<p>In this section, we acquired a practical comprehension of developing and evaluating Amazon Bedrock Agents tailored for a text summarization use case. Upon ensuring the agent’s <a id="_idIndexMarker889"/>configuration and functionality align with the designated tasks, it’s time to transition into the deployment phase.</p>
<h1 id="_idParaDest-191"><a id="_idTextAnchor204"/>Deploying an Agent for Amazon Bedrock</h1>
<p>Integrating an Amazon<a id="_idIndexMarker890"/> Bedrock agent into your <a id="_idIndexMarker891"/>application requires creating an alias, which serves as a reference to a specific version of the agent’s code and configuration. Follow these steps to create an alias:</p>
<ol>
<li>Access the Amazon Bedrock console and navigate to the agent you wish to deploy. From the agent’s overview page, navigate to the <strong class="bold">Aliases</strong> section and then click <strong class="bold">Create</strong> to initiate the alias creation process.</li>
<li>Provide a name and description (optional) for the alias. You’ll also need to decide whether you want to associate this alias with a new version of the agent or an existing version that you’ve previously created.</li>
<li>Users also have the option to opt for provisioned throughput for the alias by selecting the <strong class="bold">Provisioned Throughput (PT)</strong> button. Once selected, a drop-down menu will list models created with Provisioned Throughput. No option being displayed will indicate that no PT model exists within the Amazon Bedrock environment. For further information, users can leverage <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html">https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html</a>.</li>
</ol>
<p>By creating an alias, Bedrock takes a snapshot of the agent’s current code and configuration settings and links that snapshot (version) to the alias you’ve defined. You can then use this alias to integrate and interact with that specific version of the agent within your applications. <em class="italic">Figure 10</em><em class="italic">.12</em> showcases two aliases that were created for the summarizer-translator agent:</p>
<div><div><img alt="Figure 10.12 – Aliases for Amazon Bedrock Agents" src="img/B22045_10_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – Aliases for Amazon Bedrock Agents</p>
<p>The alias essentially acts as a stable reference point, allowing you to manage different versions of your agent while ensuring your applications are interacting with the desired version.</p>
<p>Amazon Bedrock agents enable productivity gains, enhanced customer experiences, and automated workflows. Their versatility allows innovative implementations across domains such as task automation, conversational interfaces, and DevOps processes, driving <a id="_idIndexMarker892"/>operational <a id="_idIndexMarker893"/>efficiency and business value.</p>
<p>There can be several other industrial use cases with Bedrock Agents. For instance, in the case of insurance, by leveraging GenAI through Amazon Bedrock, insurance companies can enhance operational efficiency and customer experience. The agent can automate tedious and repetitive tasks, freeing up human resources to focus on more complex and strategic endeavors. Additionally, the agent’s ability to process natural language instructions allows for seamless integration into existing workflows and systems, facilitating a smoother transition toward AI-driven operations.</p>
<p>Moreover, the potential applications of GenAI in the insurance industry extend beyond claim processing. Agents can be trained to assist with personalized policy recommendations, risk assessment, fraud detection, and even customer support through natural language interactions. As technology continues to evolve, the opportunities for innovation and optimization within the insurance sector will undoubtedly expand.</p>
<p>The following link demonstrates an end-to-end scenario to get started with Amazon Bedrock Agents using the AWS Python SDK: <a href="https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py">https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/python/example_code/bedrock-agent/scenario_get_started_with_agents.py</a>.</p>
<p>The notebook uncovers the following steps:</p>
<ol>
<li>Generating an execution role specifically for the Bedrock agent</li>
<li>Instantiating the Bedrock agent and deploying an initial draft version</li>
<li>Building a Lambda function and its corresponding execution role</li>
<li>Granting the necessary IAM permissions to provision the agent to invoke the Lambda function</li>
<li>Establishing an action group that links the agent with the Lambda function</li>
<li>Deploying the fully configured agent using a designated alias</li>
<li>Invoking the agent with user-provided prompts</li>
<li>Removing all resources created during the process</li>
</ol>
<p>Users can execute this scenario end to end in order to imbibe a deeper understanding of creating a GenAI agent on Amazon Bedrock for their utility.</p>
<p>For readers interested in diving further into the world of Agents for Amazon Bedrock, you are highly encouraged to leverage Amazon’s <em class="italic">Building generative AI applications with Amazon Bedrock using agents</em> workshop: <a href="https://catalog.us-east-1.prod.workshops.aws/workshops/f8a7a3f8-1603-4b10-95cb-0b471db272d8/en-US">https://catalog.us-east-1.prod.workshops.aws/workshops/f8a7a3f8-1603-4b10-95cb-0b471db272d8/en-US</a>.</p>
<p>Readers are <a id="_idIndexMarker894"/>further <a id="_idIndexMarker895"/>encouraged to check out workflow orchestration using Amazon Bedrock Agent chaining with a digital insurance Agent use case: <a href="https://github.com/build-on-aws/workflow-orchestration-bedrock-agent-chaining/tree/main">https://github.com/build-on-aws/workflow-orchestration-bedrock-agent-chaining/tree/main</a>.</p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor205"/>Summary</h1>
<p>In this chapter, we explored the intricate concept of Agents within the GenAI universe. We examined various use cases and personas associated with agents, further elucidating practical examples of their applications in real-world scenarios. Additionally, we uncovered the seamless integration of LangChain agents with Amazon Bedrock and explored the creation of Amazon Bedrock Agents through practical code examples, as well as their orchestration workflow for building, testing, and deploying Bedrock agents.</p>
<p>Now that we have gained a thorough understanding of Agents and their orchestration processes, we will explore effective strategies for monitoring Amazon Bedrock models within large-scale enterprises in the next chapter. This will equip us with the necessary knowledge to manage and optimize the security and governance of these models in complex organizational settings, ensuring optimal utilization and efficiency.</p>
</div>


<div><h1 id="_idParaDest-193" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor206"/>Part 3: Model Management and Security Considerations</h1>
<p>In this part, we will understand the core aspects of evaluating and monitoring models, as well as ensuring security and privacy within the Amazon Bedrock environment. <a href="B22045_11.xhtml#_idTextAnchor207"><em class="italic">Chapter 11</em></a> explores techniques for assessing model performance, including automatic model evaluation, human evaluation, and open source tools. Additionally, it covers monitoring techniques such as Amazon CloudWatch, model invocation logging, and integration with AWS CloudTrail and Amazon EventBridge. <a href="B22045_12.xhtml#_idTextAnchor226"><em class="italic">Chapter 12</em></a> centers on data protection, identity and access management, network security, ethical considerations, and implementing guardrails to adhere to responsible AI practices and policies.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B22045_11.xhtml#_idTextAnchor207"><em class="italic">Chapter 11</em></a>, <em class="italic">Evaluating and Monitoring Models with Amazon Bedrock</em></li>
<li><a href="B22045_12.xhtml#_idTextAnchor226"><em class="italic">Chapter 12</em></a>, <em class="italic">Ensuring Security and Privacy in Amazon Bedrock</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>