["```py\n# In a Colab or Jupyter notebook\n!pip install transformers\n# Google Colab Jupyter notebook\nfrom transformers import pipeline\n# Initialize a text generation pipeline with a generative model, say GPT-Neo\ntext_generator = pipeline(\n    'text-generation', model='EleutherAI/gpt-neo-2.7B')\n# Example prompt for product description generation\nprompt = \"This high-tech running shoe with advanced cushioning and support\"\n# Generating the product description\ngenerated_text = text_generator(prompt, max_length=100, do_sample=True)\n# Printing the generated product description\nprint(generated_text[0]['generated_text'])\n```", "```py\nThis high-tech running shoe with advanced cushioning and support combines the best of traditional running shoes and the latest technologies.\n```", "```py\nmkdir StyleSprint\ncd StyleSprint\ngit init\n```", "```py\n# Use an official NVIDIA CUDA runtime as a base image\nFROM nvidia/cuda:11.0-base\n# Set the working directory in the container to /app\nWORKDIR /app\n# Copy the current directory contents into the container at /app\nCOPY . /app\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n# Make port 80 available to the world outside this container\nEXPOSE 80\n# Run app.py when the container launches\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n```", "```py\nfastapi==0.65.2\ntorch==1.9.0\ntransformers==4.9.2\nuvicorn==0.14.0\n```", "```py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import pipeline\n# Load the pre-trained model\ngenerator = pipeline('text-generation', \n    model='EleutherAI/gpt-neo-2.7B')\n# Create the FastAPI app\napp = FastAPI()\n# Define the request body\nclass GenerationInput(BaseModel):\nprompt: str\n# Define the endpoint\n@app.post(\"/generate\")\ndef generate_text(input: GenerationInput):\ntry:\n    # Generate text based on the input prompt\n    generated_text = generator(input.prompt, max_length=150)\n    return {\"generated_text\": generated_text}\nexcept:\n    raise HTTPException(status_code=500,\n        detail=\"Model failed to generate text\")\n```", "```py\n    name: CI/CD Pipeline\n    ```", "```py\n    on:\n    ```", "```py\n      push:\n    ```", "```py\n        branches:\n    ```", "```py\n          - main\n    ```", "```py\n    jobs:\n    ```", "```py\n      build-and-test:\n    ```", "```py\n        runs-on: ubuntu-latest\n    ```", "```py\n      steps:\n    ```", "```py\n        - name: Checkout code\n    ```", "```py\n          uses: actions/checkout@v4\n    ```", "```py\n        - name: Build Docker image\n    ```", "```py\n      # assumes the Dockerfile is in the root (.)\n    ```", "```py\n          run: docker build -t stylesprint .\n    ```", "```py\n        - name: Run tests\n    ```", "```py\n      # assumes a set of unit tests were defined\n    ```", "```py\n          run: docker run stylesprint python -m unittest discover\n    ```", "```py\n    deploy:\n    ```", "```py\n      needs: build-and-test\n    ```", "```py\n      runs-on: ubuntu-latest\n    ```", "```py\n      steps:\n    ```", "```py\n        - name: Checkout code\n    ```", "```py\n          uses: actions/checkout@v4\n    ```", "```py\n        - name: Login to DockerHub\n    ```", "```py\n          run: echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin\n    ```", "```py\n        - name: Push Docker image\n    ```", "```py\n          run: |\n    ```", "```py\n            docker tag stylesprint:latest ${{ secrets.DOCKER_USERNAME }}/stylesprint:latest\n    ```", "```py\n            docker push ${{ secrets.DOCKER_USERNAME }}/stylesprint:latest\n    ```", "```py\n# Verify GPU is available\nimport torch\ntorch.cuda.is_available()\n```", "```py\n    !pip -q install openai langchain huggingface_hub\n    ```", "```py\n    import os\n    ```", "```py\n    os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'\n    ```", "```py\n    os.environ['HUGGINGFACEHUB_API_TOKEN'] = \n    ```", "```py\n        'your_huggingface_token_here'\n    ```", "```py\n    !pip install openai langchain[llms] huggingface_hub\n    ```", "```py\n    from langchain.llms import OpenAI, HuggingFaceHub\n    ```", "```py\n    # Loading GPT-3\n    ```", "```py\n    llm_gpt3 = OpenAI(model_name='text-davinci-003',\n    ```", "```py\n                      temperature=0.9,\n    ```", "```py\n                      max_tokens = 256)\n    ```", "```py\n    # Loading Neo from Hugging Face\n    ```", "```py\n    llm_neo = HuggingFaceHub(repo_id=' EleutherAI/gpt-neo-2.7B',\n    ```", "```py\n                             model_kwargs={\"temperature\":0.9}\n    ```", "```py\n    )\n    ```", "```py\nimport pandas as pd\n# Assume `product_data.csv` is a CSV file with product data\n# The CSV file has two columns: 'product_image' and \n# 'product_description' \n# Load the product data\nproduct_data = pd.read_csv('product_data.csv')\n# Split the data into testing and reference sets\ntest_data = product_data.sample(frac=0.2, random_state=42)\nreference_data = product_data.drop(test_data.index)\n# Checkpoint the testing and reference data\ntest_data.to_csv('test_data.csv', index=False)\nreference_data.to_csv('reference_data.csv', index=False)\n# Extract reference descriptions and image file paths\nreference_descriptions = /\n    reference_data['product_description'].tolist()\nproduct_images = reference_data['product_image'].tolist()\n```", "```py\n# Assume `product_metadata` is a column in the data that contains the collective information about the product including the title of the product and attributes.\n# Format the input data for the models\nmodel_input_data = reference_data['product_metadata].tolist()\nreference_descriptions = \\\n    reference_data['product_description'].tolist()\n```", "```py\nfrom langchain import LLMChain, PromptTemplate\nfrom tqdm.auto import tqdm\ntemplate = \"\"\"\nWrite a creative product description for the following product: {product_metadata}\n\"\"\"\nPROMPT = PromptTemplate(template=template, \n    input_variables=[\"product_metadata\"])\ndef generate_descriptions(\n    llm: object, \n    prompt: PromptTemplate = PROMPT\n) -> list:\n    # Initialize the LLM chain\n    llm_chain = LLMChain(prompt=prompt, llm=llm)\n    descriptions = []\n    for i in tqdm(range(len(model_input_data))):\n        description = llm_chain.run(model_input_data[i])\n        descriptions.append(description)\n    return descriptions\ngpt3_descriptions = generate_descriptions(llm_gpt3)\ngptneo_descriptions = generate_descriptions(llm_neo)\n```", "```py\n!pip install rouge sumeval nltk\n# nltk requires an additional package\nimport nltk\nnltk.download('wordnet')\n from nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\nfrom sumeval.metrics.rouge import RougeCalculator\nfrom nltk.translate.meteor_score import meteor_score\ndef evaluate(\n    reference_descriptions: list, \n    generated_descriptions: list\n) -> tuple:\n    # Calculating BLEU score\n    bleu_scores = [\n        sentence_bleu([ref], gen) \n        for ref, gen in zip(reference_descriptions, generated_descriptions)\n    ]\n    average_bleu = sum(bleu_scores) / len(bleu_scores)\n    # Calculating ROUGE score\n    rouge = RougeCalculator()\n    rouge_scores = [rouge.rouge_n(gen, ref, 2) for ref,\n        gen in zip(reference_descriptions,\n        generated_descriptions)]\n    average_rouge = sum(rouge_scores) / len(rouge_scores)\n    # Calculating METEOR score\n    meteor_scores = [ meteor_score([ref.split() ],\n        gen.split()) for ref,\n        gen in zip(reference_descriptions,\n        generated_descriptions)]\n    average_meteor = sum(meteor_scores) / len(meteor_scores)\n    return average_bleu, average_rouge, average_meteor\naverage_bleu_gpt3, average_rouge_gpt3, average_meteor_gpt3 = \\\n    evaluate(reference_descriptions, gpt3_descriptions)\nprint(average_bleu_gpt3, average_rouge_gpt3, average_meteor_gpt3)\naverage_bleu_neo, average_rouge_neo, average_meteor_neo = \\\n    evaluate(reference_descriptions, gptneo_descriptions)\nprint(average_bleu_neo, average_rouge_neo, average_meteor_neo)\n```", "```py\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\ndef cosine_similarity(reference_descriptions, generated_descriptions):\n    # Calculating cosine similarity for generated descriptions\n    cosine_scores = [util.pytorch_cos_sim(\n        model.encode(ref), model.encode(gen)) for ref,\n        gen in zip(reference_descriptions,\n        generated_descriptions)]\n    average_cosine = sum(cosine_scores) / len(cosine_scores)\n    return average_cosine\naverage_cosine_gpt3 = cosine_similarity(\n    reference_descriptions, gpt3_descriptions)\nprint(average_cosine_gpt3)\naverage_cosine_neo = cosine_similarity(\n    reference_descriptions, gptneo_descriptions)\nprint(average_cosine_neo)\n```", "```py\nclip_model = \"openai/clip-vit-base-patch32\"\ndef clip_scores(images, descriptions,\n                model=clip_model,\n                processor=clip_processor\n):\n    scores = []\n    # Process all images and descriptions together\n    inputs = process_inputs(processor, descriptions, images)\n    # Get model outputs\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image # Image-to-text logits\n    # Diagonal of the matrix gives the scores for each image-description pair\n    for i in range(logits_per_image.size(0)):\n        score = logits_per_image[i, i].item()\n    scores.append(score)\n    return scores\nreference_images = [\n    load_image_from_path(image_path) \n    for image_path in reference_data.product_image_path\n]\ngpt3_generated_scores = clip_scores(\n    reference_images, gpt3_descriptions\n)\nreference_scores = clip_scores(\n    reference_images, reference_descriptions\n)\n# Compare the scores\nfor i, (gen_score, ref_score) in enumerate(\n    zip(gpt3_generated_scores, reference_scores)\n):\n    print(f\"Image {i}: Generated Score = {gen_score:.2f}, \n        Reference Score = {ref_score:.2f}\")\n```", "```py\n    fastapi==0.68.0\n    ```", "```py\n    uvicorn==0.15.0\n    ```", "```py\n    openai==0.27.0\n    ```", "```py\n    langchain==0.1.0\n    ```", "```py\n    # Use an official Python runtime as a base image\n    ```", "```py\n    FROM python:3.8-slim-buster\n    ```", "```py\n    # Set the working directory in the container to /app\n    ```", "```py\n    WORKDIR /app\n    ```", "```py\n    # Copy the current directory contents into the container at /app\n    ```", "```py\n    COPY . /app\n    ```", "```py\n    # Install any needed packages specified in requirements.txt\n    ```", "```py\n    RUN pip install --no-cache-dir -r requirements.txt\n    ```", "```py\n    # Make port 80 available to the world outside this container\n    ```", "```py\n    EXPOSE 80\n    ```", "```py\n    # Define environment variable\n    ```", "```py\n    ENV NAME World\n    ```", "```py\n    # Run app.py when the container launches\n    ```", "```py\n    CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n    ```", "```py\n    from fastapi import FastAPI, HTTPException, Request\n    ```", "```py\n    from langchain.llms import OpenAI\n    ```", "```py\n    import os\n    ```", "```py\n    # Initialize FastAPI app\n    ```", "```py\n    app = FastAPI()\n    ```", "```py\n    # Setup Langchain with GPT-3.5\n    ```", "```py\n    llm = OpenAI(model_name='text-davinci-003',\n    ```", "```py\n                 temperature=0.7,\n    ```", "```py\n                 max_tokens=256,\n    ```", "```py\n                 api_key=os.environ['OPENAI_API_KEY'])\n    ```", "```py\n    @app.post(\"/generate/\")\n    ```", "```py\n    async def generate_text(request: Request):\n    ```", "```py\n        data = await request.json()\n    ```", "```py\n        prompt = data.get('prompt')\n    ```", "```py\n        if not prompt:\n    ```", "```py\n            raise HTTPException(status_code=400,\n    ```", "```py\n                detail=\"Prompt is required\")\n    ```", "```py\n        response = llm(prompt)\n    ```", "```py\n        return {\"generated_text\": response}\n    ```"]