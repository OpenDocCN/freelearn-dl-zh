["```py\n    def load_model_and_tokenizer(model_name=\"gpt2\"):\n        model = GPT2LMHeadModel.from_pretrained(model_name)\n        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        tokenizer.pad_token = tokenizer.eos_token\n        return model, tokenizer\n    ```", "```py\n    def prepare_dataset(dataset_name=\"wikitext\",\n        dataset_config=\"wikitext-2-raw-v1\"\n    ):\n        dataset = load_dataset(dataset_name, dataset_config)\n        return dataset\n    def tokenize_function(examples, tokenizer):\n        return tokenizer(\n            examples[\"text\"], truncation=True,\n            padding=\"max_length\", max_length=512)\n    ```", "```py\n    def fine_tune_lm(model, tokenizer,\n        dataset, output_dir=\"./fine_tuned_model\"\n    ):\n        tokenized_dataset = dataset.map(\n            lambda examples: tokenize_function(examples, tokenizer),\n            batched=True)\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=3,\n            per_device_train_batch_size=8,\n            per_device_eval_batch_size=8,\n            warmup_steps=500,\n            weight_decay=0.01,\n            logging_dir=\"./logs\",\n        )\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_dataset[\"train\"],\n            eval_dataset=tokenized_dataset[\"validation\"],\n        )\n        trainer.train()\n        trainer.save_model()\n    ```", "```py\n    def freeze_layers(model, num_layers_to_freeze):\n        for param in model.base_model.parameters():\n            param.requires_grad = False\n        for i, layer in enumerate(model.base_model.transformer.h):\n            if i >= len(model.base_model.transformer.h) -\\\n                num_layers_to_freeze:\n                for param in layer.parameters():\n                    param.requires_grad = True\n    ```", "```py\n    def gradual_unfreeze(model, trainer, num_epochs, total_layers):\n        layers_per_epoch = total_layers // num_epochs\n        for epoch in range(num_epochs):\n            freeze_layers(model, (epoch + 1) * layers_per_epoch)\n            trainer.train(resume_from_checkpoint=True)\n    ```", "```py\n    training_args = TrainingArguments(\n        output_dir=\"./fine_tuned_model\",\n        num_train_epochs=5,  # Increased epochs for better learning\n        per_device_train_batch_size=16,  # Larger batch size\n        per_device_eval_batch_size=16,\n        warmup_steps=1000,  # More warmup steps\n        learning_rate=2e-5,  # Added learning rate\n        weight_decay=0.1,  # Increased weight decay\n        logging_dir=\"./logs\",\n        save_steps=500,  # Added save frequency\n        eval_steps=500   # Added evaluation frequency\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"validation\"],\n    )\n    ```", "```py\n    from transformers import (\n        get_linear_schedule_with_warmup,\n        get_cosine_schedule_with_warmup)\n    def fine_tune_with_lr_scheduling(\n        model, tokenizer, dataset, scheduler_type=\"linear\",\n        num_epochs=3\n    ):\n        tokenized_dataset = dataset.map(\n            lambda examples: tokenize_function(examples, tokenizer),\n            batched=True)\n    ```", "```py\n    training_args = TrainingArguments(\n        output_dir=\"./fine_tuned_model\",\n        num_train_epochs=3,\n        per_device_train_batch_size=32,  # Increased batch size\n        per_device_eval_batch_size=32,\n        weight_decay=0.1,  # Increased weight decay\n        logging_dir=\"./logs\",\n        learning_rate=2e-5,  # Adjusted learning rate\n        warmup_ratio=0.1,   # Added warmup ratio\n        eval_steps=100,     # Added evaluation frequency\n        save_steps=100      # Added save frequency\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"validation\"],\n    )\n    ```", "```py\n    num_training_steps = len(tokenized_dataset[\"train\"]) //\n        training_args.per_device_train_batch_size * num_epochs\n    if scheduler_type == \"linear\":\n        scheduler = get_linear_schedule_with_warmup(\n            trainer.optimizer,\n            num_warmup_steps=num_training_steps // 10,  # 10% warmup\n            num_training_steps=num_training_steps\n        )\n    elif scheduler_type == \"cosine\":\n        scheduler = get_cosine_schedule_with_warmup(\n            trainer.optimizer,\n            num_warmup_steps=num_training_steps // 10,  # 10% warmup\n            num_training_steps=num_training_steps\n        )\n    else:\n        raise ValueError(\"Unsupported scheduler type\")\n    ```", "```py\n    import torch\n    from transformers import (\n        TextDataset, DataCollatorForLanguageModeling )\n    def prepare_scientific_dataset(file_path, tokenizer):\n        dataset = TextDataset(\n            tokenizer=tokenizer,\n            file_path=file_path,\n            block_size=128,\n        )\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=tokenizer, mlm=False,\n        )\n        return dataset, data_collator\n    ```", "```py\n    def fine_tune_for_scientific_domain(\n        model, tokenizer, train_file, eval_file,\n        output_dir=\"./scientific_model\"\n    ):\n        train_dataset, data_collator =\n            prepare_scientific_dataset(train_file, tokenizer)\n        eval_dataset, _ = prepare_scientific_dataset(\n            eval_file, tokenizer)\n    ```", "```py\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3,            # Reduced epochs\n        per_device_train_batch_size=8, # Increased batch size\n        per_device_eval_batch_size=8,\n        warmup_steps=1000,            # Increased warmup\n        weight_decay=0.1,             # Increased weight decay\n        learning_rate=3e-5,           # Added learning rate\n        logging_dir=\"./logs\",\n        evaluation_strategy=\"steps\",   # Changed to steps\n        eval_steps=500,               # Added eval frequency\n        save_steps=500,               # Added save frequency\n        gradient_accumulation_steps=4  # Added gradient accumulation\n    )\n    ```", "```py\n    def prepare_few_shot_dataset(examples, tokenizer, num_shots=5):\n        few_shot_examples = examples[:num_shots]\n        prompt = \"\\n\\n\".join(\n            [\n                f\"Input: {ex['input']}\\n\"\n                f\"Output: {ex['output']}\"\n                for ex in few_shot_examples\n            ]\n        )\n        prompt += \"\\n\\nInput: {input}\\nOutput:\"\n        def tokenize_function(example):\n            full_prompt = prompt.format(input=example['input'])\n            tokenized_prompt = tokenizer(full_prompt,\n                truncation=True,\n                padding=\"max_length\", max_length=512)\n            tokenized_output = tokenizer(\n                example['output'], truncation=True,\n                padding=\"max_length\", max_length=512)\n            tokenized_prompt['labels'] = \\\n                [-100] * len(tokenized_prompt['input_ids'])\n                + tokenized_output['input_ids']\n            return tokenized_prompt\n        return examples.map(tokenize_function)\n    ```", "```py\n    def few_shot_fine_tune(\n        model, tokenizer, dataset, num_shots=5, num_epochs=3\n    ):\n        few_shot_dataset = prepare_few_shot_dataset(dataset,\n            tokenizer, num_shots)\n        training_args = TrainingArguments(\n            output_dir=\"./few_shot_model\",\n            num_train_epochs=num_epochs,\n            per_device_train_batch_size=4,\n            per_device_eval_batch_size=4,\n            warmup_steps=100,\n            weight_decay=0.01,\n            logging_dir=\"./logs\",\n        )\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=few_shot_dataset,\n        )\n        trainer.train()\n        return trainer\n    ```", "```py\n    # Usage\n    model, tokenizer = load_model_and_tokenizer()\n    dataset = load_dataset(\"your_dataset\")  # Load your few-shot dataset\n    few_shot_trainer = few_shot_fine_tune(model, tokenizer, dataset)\n    ```", "```py\n    import copy\n    def ewc_loss(model, old_model, importance, loss):\n        ewc_lambda = 0.01\n        for n, p in model.named_parameters():\n            if n in importance:\n                loss += ewc_lambda * importance[n]\n                    * (p - old_model[n]).pow(2).sum()\n        return loss\n    def compute_importance(model, dataset):\n        importance = {}\n        model.eval()\n        for batch in dataset:\n            model.zero_grad()\n            output = model(batch)\n            loss = output.loss\n            loss.backward()\n            for n, p in model.named_parameters():\n                if p.grad is not None:\n                    if n not in importance:\n                        importance[n] = p.grad.data.clone().pow(2)\n                    else:\n                        importance[n] += p.grad.data.clone().pow(2)\n        return importance\n    ```", "```py\n    def continual_fine_tune(\n            model, tokenizer, datasets, num_epochs=3\n    ):\n        old_model = None\n        importance = None\n        for i, dataset in enumerate(datasets):\n            if old_model is not None:\n                importance = compute_importance(\n                    old_model, datasets[i-1])\n            old_model = copy.deepcopy(model)\n            tokenized_dataset = dataset.map(\n                lambda examples: tokenize_function(examples,\n                    tokenizer),\n                batched=True)\n    ```", "```py\n    training_args = TrainingArguments(\n        output_dir=f\"./continual_fine_tuned_model_task_{i+1}\",\n        num_train_epochs=8,                # Increased epochs\n        per_device_train_batch_size=20,    # Increased batch size\n        per_device_eval_batch_size=20,\n        warmup_steps=2000,                 # Increased warmup\n        weight_decay=0.2,                  # Increased weight decay\n        learning_rate=2e-5,                # Added learning rate\n        logging_dir=\"./logs\",\n        evaluation_strategy=\"steps\",    # Added evaluation strategy\n        eval_steps=1000,                # Added evaluation frequency\n        save_steps=1000                    # Added save frequency\n    )\n    ```", "```py\n    def project(gradient, memories):\n        for memory in memories:\n            if torch.dot(gradient, memory) < 0:\n                gradient -= (\n                    torch.dot(gradient, memory) / torch.dot(\n                        memory, memory)\n                ) * memory\n        return gradient\n    # This would be integrated into the training loop\n    ```", "```py\n    def lwf_loss(\n        model, old_model, new_data, old_data, temperature=2\n    ):\n        # Compute standard loss on new data\n        new_loss = compute_loss(model, new_data)\n        # Compute distillation loss on old data\n        old_outputs = old_model(old_data)\n        new_outputs = model(old_data)\n        distillation_loss = F.kl_div(\n            F.log_softmax(new_outputs / temperature, dim=1),\n            F.softmax(old_outputs / temperature, dim=1),\n            reduction='batchmean'\n        ) * (temperature  2)\n        return new_loss + distillation_loss\n    # This would replace the standard loss in the training loop\n    ```"]