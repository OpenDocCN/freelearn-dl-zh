- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting RAG Performance with Expert Human Feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Human feedback** (**HF**) is not just useful for generative AI—it’s essential,
    especially when it comes to models using RAG. A generative AI model uses information
    from datasets with various documents during training. The data that trained the
    AI model is set in stone in the model’s parameters; we can’t change it unless
    we train it again. However, in the world of retrieval-based text and multimodal
    datasets, there is information we can see and tweak. That is where HF comes in.
    By providing feedback on what the AI model pulls from its datasets, HF can directly
    influence the quality of its future responses. Engaging with this process makes
    humans an active player in the RAG’s development. It adds a new dimension to AI
    projects: adaptive RAG.'
  prefs: []
  type: TYPE_NORMAL
- en: We have explored and implemented naïve, advanced, and modular RAG so far. Now,
    we will add adaptive RAG to our generative AI toolbox. We know that even the best
    generative AI system with the best metrics cannot convince a dissatisfied user
    that it is helpful if it isn’t. We will introduce adaptive RAG with an HF loop.
    The system thus becomes adaptive because the documents used for retrieval are
    updated. Integrating HF in RAG leads to a pragmatic hybrid approach because it
    involves humans in an otherwise automated generative process. We will thus leverage
    HF, which we will use to build a hybrid adaptive RAG program in Python from scratch,
    going through the key steps of building a RAG-driven generative AI system from
    the ground up. By the end of this chapter, you will have a theoretical understanding
    of the adaptive RAG framework and practical experience in building an AI model
    based on HF.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the adaptive RAG ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying adaptive RAG to augmented retrieval queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating augmented generative AI inputs with HF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating end-user feedback rankings to trigger expert HF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an automated feedback system for a human expert
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating HF with adaptive RAG for GPT-4o
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining adaptive RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No, RAG cannot solve all our problems and challenges. RAG, just like any generative
    model, can also produce irrelevant and incorrect output! RAG might be a useful
    option, however, because we feed pertinent documents to the generative AI model
    that inform its responses. Nonetheless, the quality of RAG outputs depends on
    the accuracy and relevance of the underlying data, which calls for verification!
    That’s where adaptive RAG comes in. Adaptive RAG introduces human, real-life,
    pragmatic feedback that will improve a RAG-driven generative AI ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: The core information in a generative AI model is parametric (stored as weights).
    But in the context of RAG, this data can be visualized and controlled, as we saw
    in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*. Despite
    this, challenges remain; for example, the end-user might write fuzzy queries,
    or the RAG data retrieval might be faulty. An HF process is, therefore, highly
    recommended to ensure the system’s reliability.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1.3* from *Chapter 1*, *Why Retrieval Augmented Generation?*, represents
    the complete RAG framework and ecosystem. Let’s zoom in on the adaptive RAG ecosystem
    and focus on the key processes that come into play, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](img/B31169_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: A variant of an adaptive RAG ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: 'The variant of an adaptive RAG ecosystem in this chapter includes the following
    components, as shown in *Figure 5.1*, for the retriever:'
  prefs: []
  type: TYPE_NORMAL
- en: '**D1**: **Collect and process** Wikipedia articles on LLMs by fetching and
    cleaning the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**D4**: **Retrieval query** to query the retrieval dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The generator’s components are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**G1**: **Input** entered by an end-user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G2**: **Augmented input with HF** that will augment the user’s initial input
    and **prompt engineering** to configure the GPT-4o model’s prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G4**: **Generation and output** to run the generative AI model and obtain
    a response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The evaluator’s components are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E1**: **Metrics** to apply a cosine similarity measurement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E2**: **Human feedback** to obtain and process the ultimate measurement of
    a system through end-user and expert feedback'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will illustrate adaptive RAG by building a hybrid adaptive
    RAG program in Python on Google Colab. We will build this program from scratch
    to acquire a clear understanding of an adaptive process, which may vary depending
    on a project’s goals, but the underlying principles remain the same. Through this
    hands-on experience, you will learn how to develop and customize a RAG system
    when a ready-to-use one fails to meet the users’ expectations. This is important
    because human users can be dissatisfied with a response no matter what the performance
    metrics show. We will also explore the incorporation of human user rankings to
    gather expert feedback on our RAG-driven generative AI system. Finally, we will
    implement an automated ranking system that will decide how to augment the user
    input for the generative model, offering practical insights into how a RAG-driven
    system can be successfully implemented in a company.
  prefs: []
  type: TYPE_NORMAL
- en: We will develop a proof of concept for a hypothetical company called *Company
    C*. This company would like to deploy a conversational agent that explains what
    AI is. The goal is for the employees of this company to understand the basic terms,
    concepts, and applications of AI. The ML engineer in charge of this RAG-driven
    generative AI example would like future users to acquire a better knowledge of
    AI while implementing other AI projects across the sales, production, and delivery
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: Company C currently faces serious issues with customer support. With a growing
    number of products and services, their product line of smartphones of the C-phone
    series has been experiencing technical problems with too many customer requests.
    The IT department would like to set up a conversational agent for these customers.
    However, the teams are not convinced. The IT department has thus decided to first
    set up a conversational agent to explain what an LLM is and how it can be helpful
    in the C-phone series customer support service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program will be hybrid and adaptive to fulfill the needs of Company C:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hybrid**: Real-life scenarios go beyond theoretical frameworks and configurations.
    The system is hybrid because we are integrating HF within the retrieval process
    that can be processed in real time. However, we will not parse the content of
    the documents with a keyword alone. We will label the documents (which are Wikipedia
    URLs in this case), which can be done automatically, controlled, and improved
    *by a human*, if necessary. As we show in this chapter, some documents will be
    replaced by human-expert feedback and relabeled. The program will automatically
    retrieve human-expert feedback documents and raw retrieved documents to form a
    hybrid (human-machine) *dynamic* RAG system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive**: We will introduce human user ranking, expert feedback, and automated
    document re-ranking. This HF loop takes us deep into modular RAG and adaptive
    RAG. Adaptive RAG leverages the flexibility of a RAG system to adapt its responses
    to the queries. In this case, we want HF to be triggered to improve the quality
    of the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Real-life projects will inevitably require an ML engineer to go beyond the
    boundaries of pre-determined categories. Pragmatism and necessity encourage creative
    and innovative solutions. For example, for the hybrid, dynamic, and adaptive aspects
    of the system, ML engineers could imagine any process that works with any type
    of algorithm: classical software functions, ML clustering algorithms, or any function
    that works. In real-life AI, what works, works!'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s time to build a proof of concept to show Company C’s management how hybrid
    adaptive RAG-driven generative AI can successfully help their teams by:'
  prefs: []
  type: TYPE_NORMAL
- en: Proving that AI can work with a proof of concept before scaling and investing
    in a project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Showing that an AI system can be customized for a specific project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing solid ground-up skills to face any AI challenge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the company’s data governance and control of AI systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laying solid grounds to scale the system by solving the problems that will come
    up during the proof of concept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go to our keyboards!
  prefs: []
  type: TYPE_NORMAL
- en: Building hybrid adaptive RAG in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now start building the proof of concept of a hybrid adaptive RAG-driven
    generative AI configuration. Open `Adaptive_RAG.ipynb` on GitHub. We will focus
    on HF and, as such, will not use an existing framework. We will build our own
    pipeline and introduce HF.
  prefs: []
  type: TYPE_NORMAL
- en: 'As established earlier, the program is divided into three separate parts: the
    **retriever**, **generator**, and **evaluator** functions, which can be separate
    agents in a real-life project’s pipeline. Try to separate these functions from
    the start because, in a project, several teams might be working in parallel on
    separate aspects of the RAG framework.'
  prefs: []
  type: TYPE_NORMAL
- en: The titles of each of the following sections correspond exactly to the names
    of each section in the program on GitHub. The retriever functionality comes first.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Retriever
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will first outline the initial steps required to set up the environment for
    a RAG-driven generative AI model. This process begins with the installation of
    essential software components and libraries that facilitate the retrieval and
    processing of data. We specifically cover the downloading of crucial files and
    the installation of packages needed for effective data retrieval and web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Installing the retriever’s environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin by downloading `grequests.py` from the `commons` directory of the
    GitHub repository. This repository contains resources that can be common to several
    programs in the repository, thus avoiding redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The download is standard and built around the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will only need two packages for the retriever since we are building a RAG-driven
    generative AI model from scratch. We will install:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requests`, the HTTP library to retrieve Wikipedia documents:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`beautifulsoup4`, to scrape information from web pages:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We now need a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1\. Preparing the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this proof of concept, we will retrieve Wikipedia documents by scraping
    them through their URLs. The dataset will contain automated or human-crafted labels
    for each document, which is the first step toward indexing the documents of a
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: One or more labels precede each URL. This approach might be sufficient for a
    relatively small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For specific projects, including a proof of concept, this approach can provide
    a solid first step to go from naïve RAG (content search with keywords) to searching
    a dataset with indexes (the labels in this case). We now have to process the data.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2\. Processing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first apply a standard scraping and text-cleaning function to the document
    that will be retrieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The code fetches the document’s content based on its URL, which is, in turn,
    based on its label. This straightforward approach may satisfy a project’s needs
    depending on its goals. An ML engineer or developer must always be careful not
    to overload a system with costly and unprofitable functions. Moreover, labeling
    website URLs can guide a retriever pipeline to the correct locations to process
    data, regardless of the techniques (load balancing, API call optimization, etc.)
    applied. In the end, each project or sub-project will require one or several techniques,
    depending on its specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: Once the fetching and cleaning function is ready, we can implement the retrieval
    process for the user’s input.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3\. Retrieval process for user input
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step here involves identifying a keyword within the user’s input.
    The function `process_query` takes two parameters: `user_input` and `num_words`.
    The number of words to retrieve is restricted by factors like the input limitations
    of the model, cost considerations, and overall system performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon finding a match between a keyword in the user input and the keywords associated
    with URLs, the following functions for fetching and cleaning the data are triggered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `num_words` parameter helps in chunking the text. While this basic approach
    may work for use cases with a manageable volume of data, it’s recommended to embed
    the data into vectors for more complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cleaned and truncated text is then formatted for display:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that the function ultimately returns the first `n` words, providing a concise
    and relevant snippet of information based on the user’s query. This design allows
    the system to manage data retrieval efficiently while also maintaining user engagement.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The generator ecosystem contains several components, several of which overlap
    with the retriever functions and user interfaces in the RAG-driven generative
    AI frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.1\. Adaptive RAG selection based on human rankings**: This will be based
    on the ratings of a user panel over time. In a real-life pipeline, this functionality
    could be a separate program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2.2\. Input**: In a real-life project, a **user interface** (**UI**) will
    manage the input. This interface and the associated process should be carefully
    designed in collaboration with the users, ideally in a workshop setting where
    their needs and preferences can be fully understood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2.3\. Mean ranking simulation scenario**: Calculating the mean value of the
    user evaluation scores and functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2.4\. Checking the input before running the generator**: Displaying the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2.5\. Installing the generative AI environment**: The installation of the
    generative AI model’s environment, in this case, OpenAI, can be part of another
    environment in the pipeline in which other team members may be working, implementing,
    and deploying in production independently of the retriever functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2.6\. Content generation**: In this section of the program, an OpenAI model
    will process the input and provide a response that will be evaluated by the evaluator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by describing the adaptive RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Integrating HF-RAG for augmented document inputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dynamic nature of information retrieval and the necessity for contextually
    relevant data augmentation in generative AI models require a flexible system capable
    of adapting to varying levels of input quality. We introduce an **adaptive RAG
    selection system**, which employs HF scores to determine the optimal retrieval
    strategy for document implementation within the RAG ecosystem. Adaptive functionality
    takes us beyond naïve RAG and constitutes a hybrid RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Human evaluators assign mean scores ranging from 1 to 5 to assess the relevance
    and quality of documents. These scores trigger distinct operational modes, as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a system  Description automatically generated](img/B31169_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Automated RAG triggers'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scores of 1 to 2** indicate a lack of compensatory capability by the RAG
    system, suggesting the need for maintenance or possibly model fine-tuning. RAG
    will be temporarily deactivated until the system is improved. The user input will
    be processed but there will be no retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scores of 3 to 4** initiate an augmentation with human-expert feedback only,
    utilizing flashcards or snippets to refine the output. Document-based RAG will
    be deactivated, but the human-expert feedback data will augment the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scores of 5** initiate keyword-search RAG enhanced by previously gathered
    HF when necessary, utilizing flashcards or targeted information snippets to refine
    the output. The user is not required to provide new feedback in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This program implements one of many scenarios. The scoring system, score levels,
    and triggers will vary from one project to another, depending on the specification
    goals to attain. It is recommended to organize workshops with a panel of users
    to decide how to implement this adaptive RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: This adaptive approach aims to optimize the balance between automated retrieval
    and human insight, ensuring the generative model’s outputs are of the highest
    possible relevance and accuracy. Let’s now enter the input.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Input
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A user of Company C is prompted to enter a question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example and program, we will focus on one question and topic: `What
    is an LLM?`. The question appears and is memorized by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This program is a proof of concept with a strategy and example for the panel
    of users in Company C who wish to understand an LLM. Other topics can be added,
    and the program can be expanded to meet further needs. It is recommended to organize
    workshops with a panel of users to decide the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: We have prepared the environment and will now activate a RAG scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Mean ranking simulation scenario
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the sake of this program, let’s assume that the human user feedback panel
    has been evaluating the hybrid adaptive RAG system for some time with the functions
    provided in sections *3.2\. Human user rating* and *3.3\. Human-expert evaluation*.
    The user feedback panel ranks the responses a number of times, which automatically
    updates by calculating the mean of the ratings and storing it in a ranking variable
    named `ranking`. The `ranking` score will help the management team decide whether
    to downgrade the rank of a document, upgrade it, or suppress documents through
    manual or automated functions. You can even simulate one of the scenarios described
    in the section *2.1\. Integrating HF-RAG for augmented document inputs*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin with a 1 to 5 ranking, which will deactivate RAG so that we can
    see the native response of the generative model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then, we will modify this value to activate RAG without additional human-expert
    feedback with `ranking=5`. Finally, we will modify this value to activate human
    feedback RAG without retrieving documents with `ranking=3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a real-life environment, these rankings will be triggered automatically
    with the functionality described in sections *3.2* and *3.3* after user feedback
    panel workshops are organized to define the system’s expected behavior. If you
    wish to run the three scenarios described in section *2.1*, make sure to initialize
    the `text_input` variable that the generative model processes to respond:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Each time you switch scenarios, make sure to come back and reinitialize `text_input`.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its probabilistic nature, the generative AI model’s output may vary from
    one run to another.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through the three rating categories described in section *2.1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ranking 1–2: No RAG'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The ranking of the generative AI’s output is very low. All RAG functionality
    is deactivated until the management team can analyze and improve the system. In
    this case, `text_input` is equal to `user_input`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The generative AI model, in this case, GPT-4o, will generate the following
    output in section *2.6\. Content generation*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This output cannot satisfy the user panel of Company C in this particular use
    case. They cannot relate this explanation to their customer service issues. Furthermore,
    many users will not bother going further since they have described their needs
    to the management team and expect pertinent responses. Let’s see what human-expert
    feedback RAG can provide.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ranking 3–4: Human-expert feedback RAG'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this scenario, human-expert feedback (see *section 3.4\. Human-expert evaluation*)
    was triggered by poor user feedback ratings with automated RAG documents `(ranking=5)`
    and without RAG `(ranking 1-2)`. The human-expert panel has filled in a flashcard,
    which has now been stored as an expert-level RAG document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program first checks the ranking and activates HF retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The program will then fetch the proper document from an expert panel (selected
    experts within a corporation) dataset based on keywords, embeddings, or other
    search methods that fit the goals of a project. In this case, we assume we have
    found the right flashcard and download it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We verify if the file exists and load its content, clean it, store it in `content`,
    and assign it to `text_input` for the GPT-4 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The content of the file explains both what an LLM is and how it can help Company
    C improve customer support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now run sections *2.4* and *2.5* once and section *2.6* to generate
    the content based on this `text_input`, the response will be satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding response is now much better since it defines LLMs and also shows
    how to improve customer service for Company C’s C-phone series.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take this further in *Chapter 9*, *Empowering AI Models: Fine-Tuning
    RAG Data and Human Feedback*, in which we will fine-tune a generative model daily
    (or as frequently as possible) to improve its responses, thus alleviating the
    volume of RAG data. But for now, let’s see what the system can achieve without
    HF but with RAG documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ranking 5: RAG with no human-expert feedback documents'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some users do not require RAG documents that include human-expert RAG flashcards,
    snippets, or documents. This might be the case, particularly, if software engineers
    are the users.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the maximum number of words is limited to 100 to optimize API
    costs, but can be modified as you wish using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the generative AI model, a reasonable output is produced that software
    engineers can relate to their business:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the output refers to March 2024 data, although GPT-4-turbo’s
    training cutoff date was in December 2023, as explained in OpenAI’s documentation:
    [https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4).'
  prefs: []
  type: TYPE_NORMAL
- en: In production, at the end-user level, the error in the output can come from
    the data retrieved or the generative AI model. This shows the importance of HF.
    In this case, this error will hopefully be corrected in the retrieval documents
    or by the generative AI model. But we left the error in to illustrate that HF
    is not an option but a necessity.
  prefs: []
  type: TYPE_NORMAL
- en: These temporal RAG augmentations clearly justify the need for RAG-driven generative
    AI. However, it remains up to the users to decide if these types of outputs are
    sufficient or require more corporate customization in closed environments, such
    as within or for a company.
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this program, let’s assume `ranking>=5` for the next steps
    to show how the evaluator is implemented in the *Evaluator* section. Let’s install
    the generative AI environment to generate content based on the user input and
    the document retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.–2.5\. Installing the generative AI environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*2.4\. Checking the input before running the generator* displays the user input
    and retrieved document before augmenting the input with this information. Then
    we continue to *2.5\. Installing the generative AI environment*.'
  prefs: []
  type: TYPE_NORMAL
- en: Only run this section once. If you modified the scenario in section *2.3*, you
    can skip this section to run the generative AI model again. This installation
    is not at the top of this notebook because a project team may choose to run this
    part of the program in another environment or even another server in production.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to separate the retriever and generator functions as much
    as possible since they might be activated by different programs and possibly at
    different times. One development team might only work on the retriever functions
    while another team works on the generator functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first install OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we retrieve the API key. Store your OpenAI key in a safe location. In
    this case, it is stored on Google Drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We are now all set for content generation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6\. Content generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To generate content, we first import and set up what we need. We’ve introduced
    `time` to measure the speed of the response and have chosen `gpt-4o` as our conversational
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define a standard Gpt-4o prompt, giving it enough information to respond
    and leaving the rest up to the model and RAG data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The code then formats the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is satisfactory in this case, as we saw in section *2.3*. In the
    `ranking=5` scenario, which is the one we are now evaluating, we get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The response looks fine, but is it really accurate? Let’s run the evaluator
    to find out.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Evaluator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on each project’s specifications and needs, we can implement as many
    mathematical and human evaluation functions as necessary. In this section, we
    will implement two automatic metrics: response time and cosine similarity score.
    We will then implement two interactive evaluation functions: human user rating
    and human-expert evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Response time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The response time was calculated and displayed in the API call with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we can display the response time without further development:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will vary depending on internet connectivity and the capacity of
    OpenAI’s servers. In this case, the output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: It seems long, but online conversational agents take some time to answer as
    well. Deciding if this performance is sufficient remains a management decision.
    Let’s run the cosine similarity score next.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Cosine similarity score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cosine similarity measures the cosine of the angle between two non-zero vectors.
    In the context of text analysis, these vectors are typically **TF-IDF** (**Term
    Frequency-Inverse Document Frequency**) representations of the text, which weigh
    terms based on their importance relative to the document and a corpus.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o’s input, which is `text_input`, and the model’s response, which is `gpt4_response`,
    are treated by TF-IDF as two separate “documents.” The `vectorizer` transforms
    the documents into vectors. Then, vectorization considers how terms are shared
    and emphasized between the input and the response with the `vectorizer.fit_transform([text1,
    text2])`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to quantify the thematic and lexical overlap through the following
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Cosine similarity relies on `TfidfVectorizer` to transform the two documents
    into TF-IDF vectors. The `cosine_similarity` function then calculates the similarity
    between these vectors. A result of `1` indicates identical texts, while `0` shows
    no similarity. The output of the function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The score shows a strong similarity between the input and the output of the
    model. But how will a human user rate this response? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Human user rating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The human user rating interface provides human user feedback. As reiterated
    throughout this chapter, I recommend designing this interface and process after
    fully understanding user needs through a workshop with them. In this section,
    we will assume that the human user panel is a group of software developers testing
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code begins with the interface’s parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In this simulation, the parameters show that the system has computed human
    feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: '`counter=20` shows the number of ratings already entered by the users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score_history=60` shows the total score of the 20 ratings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold=4` states the minimum mean rating, `score_history/counter`, to obtain
    without triggering a human-expert feedback request'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now run the interface to add an instance to these parameters. The provided
    Python code defines the `evaluate_response` function, designed to assess the relevance
    and coherence of responses generated by a language model such as GPT-4\. Users
    rate the generated text on a scale from `1` (poor) to `5` (excellent), with the
    function ensuring valid input through recursive checks. The code calculates statistical
    metrics like mean scores to gauge the model’s performance over multiple evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation function is a straightforward feedback request to obtain values
    between `1` and `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The function first displays the response, as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the user enters an evaluation score between 1 and 5, which is `1` in
    this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The code then computes the statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows a relatively very low rating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluator score is `3`, the overall ranking is `3`, and the score history
    is `3` also! Yet, the cosine similarity was positive. The human-expert evaluation
    request will be triggered because we set the threshold to `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: What’s going on? Let’s ask an expert and find out!
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Human-expert evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Metrics such as cosine similarity indeed measure similarity but not in-depth
    accuracy. Time performance will not determine the accuracy of a response either.
    But if the rating is too low, why is that? Because the user is not satisfied with
    the response!
  prefs: []
  type: TYPE_NORMAL
- en: 'The code first downloads thumbs-up and thumbs-down images for the human-expert
    user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters to trigger an expert’s feedback are `counter_threshold` and
    `score_threshold`. The number of user ratings must exceed the expert’s threshold
    counter, which is `counter_threshold=10`. The threshold of the mean score of the
    ratings is `4` in this scenario: `score_threshold=4`. We can now simulate the
    triggering of an expert feedback request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the output will confirm the expert feedback loop because of the
    poor mean ratings and the number of times the users rated the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The code will display the icons shown in the following figure. If the expert
    user presses the thumbs-down icon, they will be prompted to enter feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '![A thumbs down and thumbs down  Description automatically generated](img/B31169_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Feedback icons'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add a function for thumbs-down meaning that the response was incorrect
    and that the management team has to communicate with the user panel or add a prompt
    to the user feedback interface. This is a management decision, of course. In our
    scenario, the human expert pressed the thumbs-down icon and was prompted to enter
    a response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: “Enter feedback” prompt'
  prefs: []
  type: TYPE_NORMAL
- en: 'The human expert provided the response, which was saved in `''/content/expert_feedback.txt''`.
    Through this, we have finally discovered the inaccuracy, which is in the content
    of the file displayed in the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The preceding expert’s feedback can then be used to improve the RAG dataset.
    With this, we have explored the depths of HF-RAG interactions. Let’s summarize
    our journey and move on to the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we wrap up our hands-on approach to pragmatic AI implementations, it’s worth
    reflecting on the transformative journey we’ve embarked on together, exploring
    the dynamic world of adaptive RAG. We first examined how HF not only complements
    but also critically enhances generative AI, making it a more powerful tool customized
    to real-world needs. We described the adaptive RAG ecosystem and then went hands-on,
    building from the ground up. Starting with data collection, processing, and querying,
    we integrated these elements into a RAG-driven generative AI system. Our approach
    wasn’t just about coding; it was about adding adaptability to AI through continuous
    HF loops.
  prefs: []
  type: TYPE_NORMAL
- en: By augmenting GPT-4’s capabilities with expert insights from previous sessions
    and end-user evaluations, we demonstrated the practical application and significant
    impact of HF. We implemented a system where the output is not only generated but
    also ranked by end-users. Low rankings triggered an expert feedback loop, emphasizing
    the importance of human intervention in refining AI responses. Building an adaptive
    RAG program from scratch ensured a deep understanding of how integrating HF can
    shift a standard AI system to one that evolves and improves over time.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter wasn’t just about learning; it was about doing, reflecting, and
    transforming theoretical knowledge into practical skills. We are now ready to
    scale RAG-driven AI to production-level volumes and complexity in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with *Yes* or *No*:'
  prefs: []
  type: TYPE_NORMAL
- en: Is human feedback essential in improving RAG-driven generative AI systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can the core data in a generative AI model be changed without retraining the
    model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does Adaptive RAG involve real-time human feedback loops to improve retrieval?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the primary focus of Adaptive RAG to replace all human input with automated
    responses?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can human feedback in Adaptive RAG trigger changes in the retrieved documents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does Company C use Adaptive RAG solely for customer support issues?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is human feedback used only when the AI responses have high user ratings?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the program in this chapter provide only text-based retrieval outputs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the Hybrid Adaptive RAG system static, meaning it cannot adjust based on
    feedback?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are user rankings completely ignored in determining the relevance of AI responses?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts*
    by Evgenii Kortukov, Alexander Rubinstein, Elisa Nguyen, Seong Joon Oh: [https://arxiv.org/abs/2404.16032](https://arxiv.org/abs/2404.16032)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI models: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the vectorizer and cosine similarity functionality
    implemented in this chapter, use the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature extraction – `TfidfVectorizer`: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.metrics` – `cosine_similarity`: [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
