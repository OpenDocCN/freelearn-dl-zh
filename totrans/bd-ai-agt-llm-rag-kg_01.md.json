["```py\nimport numpy as np\ndef one_hot_encoding(sentence):\n    words = sentence.lower().split()\n    vocabulary = sorted(set(words))\n    word_to_index = {word: i for i,\n        word in enumerate(vocabulary)}\n    one_hot_matrix = np.zeros((\n        len(words), len(vocabulary)), dtype=int)\n    for i, word in enumerate(words):\n        one_hot_matrix[i, word_to_index[word]] = 1\n    return one_hot_matrix, vocabulary\n```", "```py\nsentence = \"Should we go to a pizzeria or do you prefer a restaurant?\"\none_hot_matrix, vocabulary = one_hot_encoding(sentence)\nprint(\"Vocabulary:\", vocabulary)\nprint(\"One-Hot Encoding Matrix:\\n\", one_hot_matrix)\n```", "```py\nimport numpy as np\ndef bag_of_words(sentences):\n    \"\"\"\n    Creates a bag-of-words representation of a list of documents.\n    \"\"\"\n    tokenized_sentences = [\n        sentence.lower().split() for sentence in sentences\n    ]\n    flat_words = [\n        word for sublist in tokenized_sentences for word in sublist\n    ]\n    vocabulary = sorted(set(flat_words))\n    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n    bow_matrix = np.zeros((\n        len(sentences), len(vocabulary)), dtype=int)\n    for i, sentence in enumerate(tokenized_sentences):\n        for word in sentence:\n            if word in word_to_index:\n                bow_matrix[i, word_to_index[word]] += 1\n    return vocabulary, bow_matrix\n```", "```py\ncorpus = [\"This movie is awesome awesome\",\n          \"I do not say is good, but neither awesome\",\n          \"Awesome? Only a fool can say that\"]\nvocabulary, bow_matrix = bag_of_words(corpus)\nprint(\"Vocabulary:\", vocabulary)\nprint(\"Bag of Words Matrix:\\n\", bow_matrix)\n```", "```py\nimport numpy as np\ndef compute_tf(sentences):\n    \"\"\"Compute the term frequency matrix for a list of sentences.\"\"\"\n    vocabulary = sorted(set(\n        word for sentence in sentences\n        for word in sentence.lower().split()))\n    word_index = {word: i for i, word in enumerate(vocabulary)}\n    tf = np.zeros((\n        len(sentences), len(vocabulary)), dtype=np.float32)\n    for i, sentence in enumerate(sentences):\n        words = sentence.lower().split()\n        word_count = len(words)\n        for word in words:\n            if word in word_index:\n                tf[i, word_index[word]] += 1 / word_count\n    return tf, vocabulary\ndef compute_idf(sentences, vocabulary):\n    \"\"\"Compute the inverse document frequency for a list of sentences.\"\"\"\n    num_documents = len(sentences)\n    idf = np.zeros(len(vocabulary), dtype=np.float32)\n    word_index = {word: i for i, word in enumerate(vocabulary)}\n    for word in vocabulary:\n        df = sum(\n            1 for sentence in sentences\n            if word in sentence.lower().split()\n        )\n        idf[word_index[word]] = np.log(\n            num_documents / (1 + df)) + 1  # Smoothing\n    return idf\ndef tf_idf(sentences):\n    \"\"\"Generate a TF-IDF matrix for a list of sentences.\"\"\"\n    tf, vocabulary = compute_tf(sentences)\n    idf = compute_idf(sentences, vocabulary)\n    tf_idf_matrix = tf * idf\n    return vocabulary, tf_idf_matrix\nvocabulary, tf_idf_matrix = tf_idf(corpus)\nprint(\"Vocabulary:\", vocabulary)\nprint(\"TF-IDF Matrix:\\n\", tf_idf_matrix)\n```", "```py\nfrom gensim.models import Word2Vec\nmodel = Word2Vec(sentences=list_of_tokens,\n                 sg=1,\n                 vector_size=100,\n                 window=5,\n                 workers=4)\n```", "```py\nword_1 = \"good\"\nsyn = \"great\"\nant = \"bad\"\nmost_sim =model.wv.most_similar(\"good\")\nprint(\"Top 3 most similar words to {} are :{}\".format(\n    word_1, most_sim[:3]))\nsynonyms_dist = model.wv.distance(word_1, syn)\nantonyms_dist = model.wv.distance(word_1, ant)\nprint(\"Synonyms {}, {} have cosine distance: {}\".format(\n    word_1, syn, synonyms_dist))\nprint(\"Antonyms {}, {} have cosine distance: {}\".format(\n    word_1, ant, antonyms_dist))\na = 'king'\na_star = 'man'\nb = 'woman'\nb_star= model.wv.most_similar(positive=[a, b], negative=[a_star])\nprint(\"{} is to {} as {} is to: {} \".format(\n    a, a_star, b, b_star[0][0]))\n```", "```py\narray = np.random.random((10, 5, 3))\n# Convert the numpy array to a PyTorch tensor\ndata_tensor = torch.tensor(array, dtype=torch.float32)\nRNN = nn.RNN(input_size=3, hidden_size=10,\n             num_layers=1, batch_first=True)\noutput, hidden = RNN(data_tensor)\noutput.shape\n```", "```py\ndata_tensor = torch.tensor(np.random.random((10, 5, 3)),\n                           dtype=torch.float32)\nLSTM =nn.LSTM(input_size=3, hidden_size=10,\n              num_layers=1, batch_first=True)\noutput, (hidden, cell) = LSTM(data_tensor)\noutput.shape\n```", "```py\ndata_tensor = torch.tensor(np.random.random((10, 5, 3)),\n                           dtype=torch.float32)\nGRU =nn.GRU(input_size=3, hidden_size=10,\n            num_layers=1, batch_first=True)\noutput, hidden = GRU(data_tensor)\noutput.shape\n```", "```py\ndata_tensor = torch.tensor(np.random.random((10, 5, 3)),\n                           dtype=torch.float32)\nConv1d = nn.Conv1d(in_channels=5, out_channels=16,\n                   kernel_size=3, stride=1, padding=1)\noutput = Conv1d(data_tensor)\noutput.shape\n```", "```py\ndf['sentiment_encoded'] = np.where(\n    df['sentiment']=='positive',0,1)\nX,y = df['review'].values, df['sentiment_encoded'].values\nx_train,x_test,y_train,y_test = train_test_split(\n    X,y,stratify=y, test_size=.2)\nx_train,x_val,y_train,y_val = train_test_split(\n    x_train,y_train,stratify=y_train, test_size=.1)\ny_train, y_val, y_test = np.array(y_train), np.array(y_val), \\\n    np.array(y_test)\n```", "```py\n# Hyperparameters\nno_layers = 3\nvocab_size = len(vocab) + 1  # extra 1 for padding\nembedding_dim = 300\noutput_dim = 1\nhidden_dim = 256\n# Initialize the model\nmodel = SentimentRNN(no_layers, vocab_size, hidden_dim,\n                     embedding_dim, drop_prob=0.5)\n```", "```py\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nepoch_tr_loss, epoch_vl_loss = [], []\nepoch_tr_acc, epoch_vl_acc = [], []\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    h = model.init_hidden(50)\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        h = h.data\n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        train_losses.append(loss.item())\n        accuracy = acc(output, labels)\n        train_acc += accuracy\n        optimizer.step()\n```"]