["```py\nimport spacy\nfrom spacy.tokens import DocBin\nfrom spacy.training import Example\ndef create_training_data(texts, annotations):\n    nlp = spacy.blank(\"en\")\n    db = DocBin()\n    for text, annot in zip(texts, annotations):\n        doc = nlp.make_doc(text)\n        ents = []\n        for start, end, label in annot:\n            span = doc.char_span(start, end, label=label)\n            if span:\n                ents.append(span)\n        doc.ents = ents\n        db.add(doc)\n    return db\ntexts = [\n    \"Apple Inc. is planning to open a new store in New York.\",\n    \"Microsoft CEO Satya Nadella announced new AI features.\"\n]\nannotations = [\n    [(0, 9, \"ORG\"), (41, 49, \"GPE\")],\n    [(0, 9, \"ORG\"), (14, 27, \"PERSON\")]\n]\ntraining_data = create_training_data(texts, annotations)\ntraining_data.to_disk(\"./train.spacy\")\n```", "```py\n    from datasets import Dataset\n    texts = [\n        \"This movie was fantastic!\",\n        \"The service was terrible.\",\n        \"The weather is nice today.\"\n    ]\n    labels = [1, 0, 2]  # 1: positive, 0: negative, 2: neutral\n    dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})\n    print(dataset[0])\n    # Output: {'text': 'This movie was fantastic!', 'label': 1}\n    ```", "```py\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntext = \"Apple Inc. was founded by Steve Jobs\"\nlabels = [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\"]\ntokens = tokenizer.tokenize(text)\ninputs = tokenizer(text, return_tensors=\"pt\")\nprint(list(zip(tokens, labels)))\n```", "```py\n    context = \"The capital of France is Paris. It is known for its iconic Eiffel Tower.\"\n    question = \"What is the capital of France?\"\n    answer = \"Paris\"\n    start_idx = context.index(answer)\n    end_idx = start_idx + len(answer)\n    print(f\"Answer: {context[start_idx:end_idx]}\")\n    print(f\"Start index: {start_idx}, End index: {end_idx}\")\n    ```", "```py\nimport json\nfrom transformers import (\n    AutoTokenizer, AutoModelForTokenClassification)\ndef load_doccano_ner(file_path):\n    with open(file_path, 'r') as f:\n        data = [json.loads(line) for line in f]\n    return data\ndoccano_data = load_doccano_ner('doccano_export.jsonl')\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"bert-base-uncased\")\nfor item in doccano_data:\n    text = item['text']\n    labels = item['labels']\n    # Process annotations and prepare for model input\n    tokens = tokenizer.tokenize(text)\n    ner_tags = ['O'] * len(tokens)\n    for start, end, label in labels:\n        start_token = len(tokenizer.tokenize(text[:start]))\n        end_token = len(tokenizer.tokenize(text[:end]))\n        ner_tags[start_token] = f'B-{label}'\n        for i in range(start_token + 1, end_token):\n            ner_tags[i] = f'I-{label}'\n    # Now you can use tokens and ner_tags for model training or inference\n```", "```py\ntext = \"The majestic Bengal tiger prowled through the Sundarbans, a habitat it shares with spotted deer.\"\nlabels = [[13, 25, \"ANIMAL\"], [47, 57, \"GPE\"], [81, 93, \"ANIMAL\"]]\ntokens = ['The', 'majestic', 'Bengal', 'tiger', 'prowled', 'through', \n    'the', 'Sundarbans', ',', 'a', 'habitat', 'it', 'shares', 'with',\n    'spotted', 'deer', '.']\nner_tags = ['O', 'O', 'B-ANIMAL', 'I-ANIMAL', 'O', 'O', 'O', 'B-GPE',\n    'O', 'O', 'O', 'O', 'O', 'O', 'B-ANIMAL', 'I-ANIMAL', 'O']\n```", "```py\n    from sklearn.metrics import cohen_kappa_score\n    annotator1 = [0, 1, 2, 0, 1]\n    annotator2 = [0, 1, 1, 0, 1]\n    kappa = cohen_kappa_score(annotator1, annotator2)\n    print(f\"Cohen's Kappa: {kappa}\")\n    ```", "```py\n    def calculate_accuracy(gold_standard, annotations):\n        return sum(\n            g == a for g, a in zip(\n                gold_standard, annotations\n            )\n        ) / len(gold_standard)\n    gold_standard = [0, 1, 2, 0, 1]\n    annotator_result = [0, 1, 1, 0, 1]\n    accuracy = calculate_accuracy(gold_standard, annotator_result)\n    print(f\"Accuracy: {accuracy}\")\n    ```", "```py\nfrom collections import Counter\ndef aggregate_annotations(annotations):\n    return Counter(annotations).most_common(1)[0][0]\ncrowd_annotations = [\n    ['PERSON', 'PERSON', 'ORG', 'PERSON'],\n    ['PERSON', 'ORG', 'ORG', 'PERSON'],\n    ['PERSON', 'PERSON', 'ORG', 'LOC']\n]\naggregated = [aggregate_annotations(annot) \n    for annot in zip(*crowd_annotations)]\nprint(f\"Aggregated annotations: {aggregated}\")\n```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndef semi_automated_ner(text):\n    doc = nlp(text)\n    return [(ent.start_char, ent.end_char, ent.label_)\n    for ent in doc.ents]\ntext = \"Apple Inc. was founded by Steve Jobs in Cupertino.\"\nauto_annotations = semi_automated_ner(text)\nprint(f\"Auto-generated annotations: {auto_annotations}\")\n# Human annotator would then verify and correct these annotations\n```", "```py\n    import numpy as np\n    from sklearn.ensemble import RandomForestClassifier\n    from modAL.models import ActiveLearner\n    # Simulated unlabeled dataset\n    X_pool = np.random.rand(1000, 10)\n    # Initialize active learner\n    learner = ActiveLearner(\n        estimator=RandomForestClassifier(),\n        X_training=X_pool[:10],\n        y_training=np.random.randint(0, 2, 10)\n    )\n    # Active learning loop\n    n_queries = 100\n    for _ in range(n_queries):\n        query_idx, query_inst = learner.query(X_pool)\n        # In real scenario, get human annotation here\n        y_new = np.random.randint(0, 2, 1)\n        learner.teach(X_pool[query_idx], y_new)\n        X_pool = np.delete(X_pool, query_idx, axis=0)\n    print(\n        f\"Model accuracy after active learning: \"\n        f\"{learner.score(\n            X_pool, np.random.randint(0, 2, len(X_pool)))}\"\n    )\n    ```"]