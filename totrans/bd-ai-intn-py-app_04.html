<html><head></head><body>
		<div class="Content" id="_idContainer044">
			<h1 class="chapter-number" id="_idParaDest-60"><a id="_idTextAnchor061"/><a id="_idTextAnchor062"/>4</h1>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor063"/>Embedding Models</h1>
			<p><strong class="bold">Embedding models</strong> are powerful machine learning techniques that simplify high-dimensional data into lower-dimensional space, while preserving essential features. Crucial in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), they transform sparse word representations into dense vectors, capturing semantic similarities between words. Embedding models also process images, audio, video, and structured data, enhancing applications in recommendation systems, anomaly detection, <span class="No-Break">and clustering.</span></p>
			<p><a id="_idTextAnchor064"/>Here is an example of an embedding model in action. Suppose the full plot in a database of movies has been previously embedded using OpenAI’s <strong class="source-inline">text-embedding-ada-002</strong> embedding model. Your goal is to find all movies and animations for <em class="italic">Guardians of the Galaxy</em>, but not by traditional phonetic or lexical matching (where you would type some of the words in the title). Instead, you will search by semantic means, say, the phrase <strong class="source-inline">Awkward team of space defenders</strong>. You will then use the same embedding model again to embed this phrase and query the embedded movie plots. <em class="italic">Table 4.1</em> shows an excerpt of the <span class="No-Break">resulting embedding:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Dimension</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Value</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.00262913</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.031449784</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.0020321296</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">1535</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.01821267</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">1536</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.0014683881</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1: Excerpt of embedding</p>
			<p>This chapter will help you understand embedding models in depth. You’ll also implement an example using the Python language and the <span class="No-Break"><strong class="source-inline">langchain-openai</strong></span><span class="No-Break"> library.</span><a id="_idTextAnchor065"/><a id="_idTextAnchor066"/></p>
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Differentiation between embedding models <span class="No-Break">and LLMs</span></li>
				<li>Types of <span class="No-Break">embedding models</span></li>
				<li>How to choose an <span class="No-Break">embedding model</span></li>
				<li><span class="No-Break">Vector representations</span></li>
			</ul>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor067"/>Technical requirements</h1>
			<p>To follow the examples in this chapter, you will need the <span class="No-Break">following prerequisites:</span></p>
			<ul>
				<li>A MongoDB Atlas cluster. An Atlas <strong class="source-inline">M0</strong> free cluster should be sufficient as you will store a small set of documents and create only one <span class="No-Break">vector index.</span></li>
				<li>An OpenAI account and API key with access to the <span class="No-Break"><strong class="source-inline">text-embedding-3-large</strong></span><span class="No-Break"> model.</span></li>
				<li>A Python 3 <span class="No-Break">working environment.</span></li>
			</ul>
			<p>You will also need to have installed Python libraries for MongoDB, LangChain, and OpenAI. You can install these libraries in your Python 3 environment <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
%pip3 install --upgrade --quiet pymongo pythondns langchain langchain-community langchain-mongodb langchain-openai </pre>			<p>To successfully execute the example in this chapter, you will need a MongoDB Atlas Vector Index created on the MongoDB Atlas cluster. The index name must be <strong class="source-inline">text_vector_index</strong>, created on the <strong class="source-inline">embeddings.text</strong> collection <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
{
  "fields": [
    {
      "numDimensions": 1024,
      "path": "embedding",
      "similarity": "cosine",
      "type": "vector"
    }
  ]
}</pre>			<h1 id="_idParaDest-63"><a id="_idTextAnchor068"/>What is an embedding model?</h1>
			<p><a id="_idTextAnchor069"/>Embedding models are a type of tool used in machine learning and artificial intelligence that simplifies large and complex data into a more manageable form. This process, known as <strong class="bold">embedding</strong>, involves reducing the <span class="No-Break">data’s dimensions.</span></p>
			<p>Imagine going from a detailed world map with highways, railroads, rivers, trails, and so on, to a simpler, summarized version with only country boundaries and capital cities. This not only makes computation faster and less resource-intensive, but also helps identify and understand relationships within the data. Because embedding models streamline the processing and analyzing of large datasets, they are particularly useful in areas of language (text) processing, image and sound recognition, and <span class="No-Break">recommendation systems.</span></p>
			<p>Consider a vast library where each book stands for one point in high dimensions. Embedding models can help reorganize the library to improve ease of navigation, such as by grouping the books on related topics closer together and reducing the library’s overall size. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em> illustrates <span class="No-Break">this concept:</span><a id="_idTextAnchor070"/></p>
			<div>
				<div class="IMG---Figure" id="_idContainer040">
					<img alt="" role="presentation" src="image/B22495_04_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor071"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1: An embedding model example for a library use case</p>
			<p>T<a id="_idTextAnchor072"/>his conversion or reduction from a high-dimensional or original representation to a lower-dimensional representation created the basis for advancements in NLP, computer vision, <span class="No-Break">and more.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor073"/>H<a id="_idTextAnchor074"/>ow do embedding models differ from LLMs?</h2>
			<p>Embedding models are specialized algorithms that reduce high-dimensional data (such as text, images, or sound) into a low-dimensional space of dense vectors. On the other hand, LLMs are effective artificial neural networks pre-trained on gigantic corpora of <span class="No-Break">textual data.</span></p>
			<p>W<a id="_idTextAnchor075"/>hile both are rooted in neural networks, they employ distinct methodologies. LLMs are designed for generating coherent and contextually relevant text. LLMs leverage massive amounts of data to understand and predict language patterns. Their basic building blocks include transformer architectures, attention mechanisms, and large-scale pre-training followed <span class="No-Break">by fine-tuning.</span></p>
			<p>In contrast, embedding models focus on mapping words, phrases, or even entire sentences into dense vector spaces where semantic relationships are preserved. They often use techniques such as <strong class="bold">contrastive loss</strong>, which helps in distinguishing between similar and dissimilar pairs during training. Positive and negative sampling is another technique employed by embedding models. <strong class="bold">Positive samples</strong> are similar items (such as synonyms or related sentences), while <strong class="bold">negative samples</strong> are dissimilar items (such as unrelated words or sentences). <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em> visualizes an example of contrastive loss and positive and negative sampling in 2D space. This sampling aids the model in learning meaningful representations by minimizing the distance between positive pairs and maximizing the distance between negative pairs in the <span class="No-Break">vector space.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer041">
					<img alt="" role="presentation" src="image/B22495_04_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2: 2D visualization of contrastive loss and positive and negative sampling</p>
			<p>To summarize, while LLMs excel in language generation tasks, embedding models are optimized for capturing and leveraging semantic similarities. Both enhance NLP by enabling machines to grasp and produce human language more effectively. Now, let’s look at an example <span class="No-Break">of each.</span></p>
			<p><strong class="bold">Word2vec</strong> (developed by Google) transforms words into vectors and discerns semantic relationships, such as “king” is to “man” as “queen” is to “woman.” It’s useful for sentiment analysis, translation, and content recommendations, enhancing natural language understanding <span class="No-Break">for machines.</span></p>
			<p><strong class="bold">GPT-4</strong> (developed by OpenAI) is an LLM that is characterized by its ability to generate human-like text based on the input it receives. GPT-4 excels in a range of language-based tasks, including conversation, content generation, summarization, and translation. Its architecture allows it to comprehend the intricate details and nuances of language, enabling it to perform tasks that require a deep understanding of context, humor, irony, and <span class="No-Break">cultural references.</span></p>
			<h2 id="_idParaDest-65">W<a id="_idTextAnchor076"/>hen to use embedding models versus LLMs</h2>
			<p>Embedding models are used in scenarios where the goal is to capture and leverage the relationships within data. They are the ideal choice for the <span class="No-Break">following tasks:</span></p>
			<ul>
				<li><strong class="bold">S<a id="_idTextAnchor077"/>emantic similarity</strong>: Finding or recommending items (such as documents or products) that are like a <span class="No-Break">given item.</span></li>
				<li><strong class="bold">Clustering</strong>: Grouping entities based on their <span class="No-Break">semantic properties.</span></li>
				<li><strong class="bold">Information retrieval</strong>: Enhancing search functionalities by understanding the semantic content <span class="No-Break">of queries.</span></li>
			</ul>
			<p>LLMs are the go-to for tasks that require text understanding, generation, or both, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Content creation</strong>: Generating text that is coherent, contextually relevant, and stylistically appropriate. For example, generating a synopsis from the full plot of <span class="No-Break">a movie.</span></li>
				<li><strong class="bold">Conversational AI</strong>: Building chatbots and virtual assistants that can understand and engage in human-like dialogue, such as answering questions about employment policies and <span class="No-Break">employee benefits.</span></li>
				<li><strong class="bold">Language translation</strong>: The extensive training on language-diverse datasets allows LLMs to handle idiomatic expressions, cultural nuances, and <span class="No-Break">specialized terminology.</span></li>
			</ul>
			<p>Embedding models and LLMs both play crucial roles in AI. Embedding models capture and manipulate semantic properties compactly, while LLMs excel in generating and interpreting text. Using both, and selecting the right embedding models based on your goals, can unlock AI’s full potential in <span class="No-Break">your projects.</span></p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor078"/>Types of embedding models</h2>
			<p>Wo<a id="_idTextAnchor079"/><a id="_idTextAnchor080"/>rd-level models, including <strong class="bold">Global Vectors for Word Representation</strong> (<strong class="bold">GloVe</strong>) and <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), capture broader textual meanings. Specialized models such as <strong class="bold">fastText</strong> adapt to linguistic challenges. All of these reflect the evolving landscape of <span class="No-Break">embedding models.</span></p>
			<p>In this section, you will explore  many types of embedding models: word, sentence, document, contextual, specialized, non-text, <span class="No-Break">and multi-modal.</span></p>
			<h3>Word embeddings</h3>
			<p><strong class="bold">Wo<a id="_idTextAnchor081"/>rd embedding models</strong> capture semantic meanings based on context within extensive text corpora. One common approach involves a neural network that learns word associations either by predicting a word from its surrounding context or vice versa. Another method combines matrix factorization with context window techniques to generate embeddings by summarizing word co-occurrence frequencies in large matrices. A further enhancement treats each word as a collection of character n-grams (a sequence of <strong class="source-inline" lang="en-US" xml:lang="en-US">n</strong> adjacent symbols in a particular order), which helps to better handle prefixes, suffixes, and rare words. Word2vec and GloVe are examples of <span class="No-Break">these models.</span></p>
			<p><strong class="bold">Word2vec </strong>was the first attempt of embedding models to learn the representation of words as vectors based on their contextual similarities. Developed by a team from Google, it uses two architectures: <strong class="bold" lang="en-US" xml:lang="en-US">Continuous Bag of Words</strong> (<strong class="bold" lang="en-US" xml:lang="en-US">CBOW</strong>), which predicts a word given a context, and <strong class="bold">skip-gram</strong>, which predicts a context for a given word. Word2vec has been seen to capture the relationship in the syntax of words, evidenced by its ability to deduce meanings from arithmetic operations performed with <span class="No-Break">word vectors.</span></p>
			<p><strong class="bold" lang="en-US" xml:lang="en-US">GloVe</strong>, developed at Stanford University, merges the benefits of two leading word representation approaches: global matrix factorization with co-occurrence statistics and context window methods. By constructing a co-occurrence matrix from the corpus and applying dimensionality reduction techniques, GloVe captures both global statistics and local context, which is invaluable for tasks that require a deep understanding of <span class="No-Break">word relationships.</span></p>
			<h3>Sentence and document embeddings</h3>
			<p><strong class="bold">Sentence and document embedding models</strong> capture the overall semantic meaning of text blocks by considering word context and arrangement. A common approach aggregates word vectors into a coherent vector for the whole text unit. These models are useful in document similarity, information retrieval, and text summarization, such as synopses versus full movie plots. Notable models include Doc2vec <span class="No-Break">and BERT.</span></p>
			<p>B<a id="_idTextAnchor082"/>uilding on Word2vec, <strong class="bold">Doc2vec</strong>, which is also known as <strong class="bold">Paragraph Vector</strong>, encapsulates whole sentences or documents as vectors. Introducing a document ID token that allows the model to learn document-level embeddings alongside word embeddings aids significantly in tasks such as document classification and <span class="No-Break">similarity comparison.</span></p>
			<p>Google’s <strong class="bold">BERT</strong> employs context-aware embeddings, reading the entire sequence of words concurrently, unlike its predecessors that processed text linearly. This approach enables BERT to understand a word’s context from all surrounding words, resulting in more dynamic and nuanced embeddings and setting new standards across various <span class="No-Break">NLP tasks.</span></p>
			<h3>Contextual embeddings</h3>
			<p><strong class="bold">Co<a id="_idTextAnchor083"/>ntextual embedding models</strong> are designed to produce word vectors that vary according to the context of use in a sentence. These models use deep learning architectures by examining the whole sentence, or at times the surrounding sentences. The contextual model produces dynamic embeddings that capture nuances based on a word’s particular context and linguistic environment. A model architecture of this kind uses a bi-directional framework to process text both forward and in reverse, thereby capturing fine semantic and syntactic dependencies within the preceding and following contexts. They are useful in sentiment analysis (such as to interpret the tone of the text in an IT support ticket) and question-answering tasks where the exact meaning of words for interpretation is necessary. ELMo and GPT are <span class="No-Break">two examples.</span></p>
			<p><strong class="bold">Embeddings from Language Models</strong> (<strong class="bold">ELMo</strong>) introduced dynamic, context-dependent embeddings, producing variable embeddings based on a word’s linguistic context. This approach greatly enhances performance on downstream NLP tasks by providing a richer <span class="No-Break">language understanding.</span></p>
			<p>OpenAI’s <strong class="bold">GPT series</strong> leverages transformer technology to offer embeddings pre-trained on extensive text corpora and fine-tuned for specific tasks. GPT’s success underscores the efficacy of combining large-scale language models with transformer architectures <span class="No-Break">in NLP.</span></p>
			<h3>Specialized embeddings</h3>
			<p><strong class="bold">Specialized embedding models</strong> capture specific linguistic properties, such as places, people, tone, and mood, in vector space. Some are language- or dialect-specific, while others analyze sentiment and emotional dimensions. Applications include legal document analysis, support ticket triage, sentiment analysis in marketing, and multilingual <span class="No-Break">content management.</span></p>
			<p><strong class="bold">f<a id="_idTextAnchor084"/>astText</strong> is an example of a specialized embedding model. Developed by Facebook’s AI Research lab, fastText enhances Word2vec by treating words as bags of character n-grams, which proves particularly helpful for handling <strong class="bold">out-of-vocabulary</strong> (<strong class="bold">OOV</strong>) words. OOV words are words not seen during training and thus lack pre-learned vector representations, posing challenges for traditional models. fastText enables embeddings for OOV words through the summation of their sub-word embeddings. This makes it especially suitable for handling rare words and morphologically complex languages, which are languages with rich and varied word structures that use extensive prefixes, suffixes, and inflections to convey different grammatical meanings, such as Finnish, Turkish, <span class="No-Break">and Arabic.</span></p>
			<h3>Other non-text embedding models</h3>
			<p>Embedding models go beyond converting only text to vector representations. Images, audio, video, and even JSON data itself can be represented in <span class="No-Break">vector form:</span></p>
			<ul>
				<li><strong class="bold">Images</strong>: Models such as <strong class="bold">Visual Geometry Group</strong> (<strong class="bold">VGG</strong>) and <strong class="bold">Residual Network</strong> (<strong class="bold">ResNet</strong>) set benchmarks for the translation of raw images into dense vectors. These models capture important visual features, such as edges, textures, and color gradients, which are vital to many computer vision tasks, including image classification and object recognition. VGG works well at recognizing visual patterns, while ResNet improves accuracy in complex image-processing tasks, such as image segmentation or <span class="No-Break">photo tagging.</span></li>
				<li><strong class="bold" lang="en-US" xml:lang="en-US">Audio</strong>: OpenL3 and VGGish are models for audio. <strong class="bold">OpenL3</strong> is a model adapted from the L3-Net architecture that is used in audio event detection and environmental sound classification to embed audio into a temporal and spectral context-rich space. <strong class="bold">VGGish</strong> is born out of the VGG architecture for images, and so follows the same principle of converting sound waves into patterns of small, compact vectors. This simplifies tasks such as recognition of speech and <span class="No-Break">music genres.</span></li>
				<li><strong class="bold" lang="en-US" xml:lang="en-US">Video</strong>: <strong class="bold">3D Convolutional Neural Networks</strong> (<strong class="bold">3D CNNs</strong> or <strong class="bold">3D ConvNets</strong>) and <strong class="bold">Inflated 3D</strong> (<strong class="bold">I3D</strong>) expand the capabilities of image embeddings in perceiving the temporal dynamics paramount to both action recognition and for video content analysis. 3D ConvNets apply convolutional filters in three dimensions (height, width, time) capturing spatial and temporal dependencies in volumetric data, making them particularly effective for spatiotemporal data, such as video analysis, medical imaging, and 3D object recognition. I3D uses a spatiotemporal architecture that combines the outputs of two 3D ConvNets: one processes RGB frames, while the other handles optical flow predictions between consecutive frames. I3D models are useful for sports analytics and <span class="No-Break">surveillance systems.</span></li>
				<li><strong class="bold" lang="en-US" xml:lang="en-US">Graph data</strong>: Node2vec and DeepWalk capture connectivity patterns of nodes within a graph and are applied in the domains of social network analysis, fraud detection, and recommendation systems. <strong class="bold">Node2vec</strong> learns continuous vector representations for nodes by performing biased random walks on the graph. This captures the diverse node relationships and community structures, improving the performance of tasks such as node classification and link prediction. <strong class="bold">DeepWalk</strong> treats random walks as sequences of nodes like sentences in NLP by capturing the structural relationships between nodes and encodes them into continuous vector representations, which can be used for node classification <span class="No-Break">and clustering.</span></li>
				<li><strong class="bold" lang="en-US" xml:lang="en-US">JSON data</strong>: There are even JSON data embedding models, such as <strong class="bold">Tree-LSTM</strong>, which is a variation of the traditional <strong class="bold" lang="en-US" xml:lang="en-US">long short-term memory</strong> (<strong class="bold" lang="en-US" xml:lang="en-US">LSTM</strong>) networks, adapted specifically to handle data with a hierarchical tree structure, such as JSON. Unlike standard LSTM units that process data sequentially, Tree-LSTM operates over tree-structured data by incorporating states from multiple child nodes into a parent node, effectively capturing the dependencies in nested structures. This makes it particularly suitable for tasks such as semantic parsing and sentiment analysis, where understanding the hierarchical relationships within data can significantly improve performance. <strong class="bold">json2vec</strong> is an implementation of this kind of <span class="No-Break">embedding model.</span></li>
			</ul>
			<p>After single-mode models, you can explore multi-modal models. These analyze multiple data types simultaneously and are crucial for applications such as autonomous driving, where merging data from sensors, cameras, and LiDAR builds a comprehensive view of the <span class="No-Break">driving environment.</span></p>
			<h3>Multi-modal models</h3>
			<p><strong class="bold">Mu<a id="_idTextAnchor085"/>lti-modal embedding models</strong> process and integrate information from many types of data sources into a unified embedding space. This approach is incredibly useful when different modalities complement or reinforce each other and together can lead to better AI applications. Multi-modal models are excellent for in-depth comprehension of multisensory input content, such as the tasks of multi-media search engines, automated content moderation, and interactive AI systems that can engage the user via visual and verbal interaction. Here are a <span class="No-Break">few examples:</span></p>
			<ul>
				<li><strong class="bold">CLIP</strong>: A well-known multi-modal model by OpenAI. It learns how to correlate visual images with textual descriptions in such a way that it can recognize images it has never seen during training, based on natural <span class="No-Break">language queries.</span></li>
				<li><strong class="bold">LXMERT</strong>: A model that focuses on processing both visual and text inputs. It can improve the performance of tasks such as answering questions with a visual aspect, which includes <span class="No-Break">object detection.</span></li>
				<li><strong class="bold">ViLBERT</strong>: <strong class="bold">Vision-and-Language BERT</strong> (<strong class="bold">ViLBERT</strong>) extends the BERT architecture to process both visual and textual inputs simultaneously by using a two-stream model where one stream handles visual features extracted from images using a pre-trained <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong> or <strong class="bold">ConvNet</strong>), and the other processes textual data with cross-attention layers facilitating interaction between the two modalities. ViLBERT is used for tasks such as visual question answering and visual commonsense reasoning, where understanding image-text relationships <span class="No-Break">is essential.</span></li>
				<li><strong class="bold">VisualBERT</strong>: Integrates visual and textual information by combining image features with contextualized word embeddings from a BERT-like architecture. It is commonly used for tasks such as image-text retrieval and image captioning, where aligning and understanding both visual and textual information <span class="No-Break">are essential.</span></li>
			</ul>
			<p>You have now explored word, image, and multi-modal embeddings. Next, you’ll learn how to choose embedding models based on your <span class="No-Break">application’s needs.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor086"/>Choosing embedding models</h1>
			<p>Embedding models impact an application’s performance, its ability to understand language and other forms of data, and ultimately, a project’s success. The following sections look at the parameters for choosing the right embedding model that aligns with the task requirements, characteristics of your dataset, and computational resources. This section explains vector dimensionality and model leaderboards as additional information to consider when choosing embedding models. For a quick overview of this section, you can consult <span class="No-Break"><em class="italic">Table 4.2</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor087"/>Task requirements</h2>
			<p>Each type of task may benefit from different embedding models based on how they process and represent text data. For instance, tasks such as text classification and sentiment analysis often require a deep understanding of semantic relationships at the word level. Word2vec or GloVe are particularly beneficial in these cases, as they provide robust word-level embeddings that capture <span class="No-Break">semantic meanings.</span></p>
			<p>For more complex linguistic tasks such as <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>) and <strong class="bold">part-of-speech</strong> (<strong class="bold">POS</strong>) tagging, the ability to understand the context in which a word is used becomes critical. Here, models such as BERT or ELMo show their strengths as they generate embeddings that vary dynamically based on the surrounding text, providing a richer and more precise understanding of each word’s role within a sentence. This deep contextual awareness is essential for accurately identifying entities and tagging parts of speech, as it allows the model to differentiate between words with multiple meanings based on <span class="No-Break">their usage.</span></p>
			<p>Advanced models such as BERT, GPT, and Doc2vec are ideal for tasks requiring nuanced language understanding, such as question answering, machine translation, document similarity, and clustering. These models handle complex dependencies within text, making them suitable for analyzing entire documents. Doc2vec excels in comparing thematic similarities between documents, like finding similar news or <span class="No-Break">sports articles.</span></p>
			<h2 id="_idParaDest-69">D<a id="_idTextAnchor088"/>ataset characteristics</h2>
			<p>When choosing an embedding model, consider the dataset’s size and characteristics. For morphologically rich languages or datasets with many OOV words, models such as fastText, which capture sub-word information, are advantageous. They handle new or rare words effectively. For texts with polysemous words (words with multiple meanings), contextual embeddings such as ELMo or BERT are essential, as they provide dynamic, <span class="No-Break">context-specific representations.</span></p>
			<p>The dataset size influences the choice of embedding model. Larger datasets benefit from complex models such as BERT, GPT, and OpenAI’s <strong class="source-inline">text-embedding-3-large</strong>, which capture deep linguistic nuances but require substantial computational power. Smaller datasets might benefit from simpler models such as <strong class="source-inline">text-embedding-3-small</strong>, offering robust performance with less computational demand. This ensures even modest datasets can yield significant insights with the <span class="No-Break">appropriate model.</span></p>
			<h2 id="_idParaDest-70">Co<a id="_idTextAnchor089"/>mputational resources</h2>
			<p>Computational cost is crucial when selecting an embedding model due to varying resource demands. Larger models such as GPT-4 require extensive computational power, making them less accessible to smaller organizations or projects with <span class="No-Break">limited budgets.</span></p>
			<p>Choosing a lightweight model or fine-tuning one for specific tasks can reduce computational needs, speed up development, and improve response times. Efficient models are essential for real-time tasks such as translation, speech recognition, and instant recommendations in gaming, media streaming, <span class="No-Break">and e-commerce.</span></p>
			<p>Some level of iterative experimentation helps identify the most suitable models. Staying updated on the latest developments is critical, as newer models frequently supersede older ones. Model leaderboards can help track advancements in the field and are covered later in <span class="No-Break">this section.</span></p>
			<h2 id="_idParaDest-71">V<a id="_idTextAnchor090"/>e<a id="_idTextAnchor091"/>ctor representations</h2>
			<p>Th<a id="_idTextAnchor092"/><a id="_idTextAnchor093"/>e size of a vector in an embedding model affects its ability to capture data complexity. Large vectors encode more information, allowing finer distinctions, but require more computation. Small vectors are more efficient but might miss subtle nuances. Choosing a vector size involves balancing detailed representation with practical constraints like memory <span class="No-Break">and speed.</span></p>
			<h3>Why do vector dimensions matter?</h3>
			<p>Kn<a id="_idTextAnchor094"/><a id="_idTextAnchor095"/>owing the relationship between a vector, its size, and the second-last layer of a neural network is crucial for understanding the quality of the model’s output. The penultimate or second-last layer often serves as a feature extractor, where the dimensions of the output vector represent the learned features of the input data, as visualized in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em>. The size of this vector directly influences the granularity of <span class="No-Break">the representation.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer042">
					<img alt="" role="presentation" src="image/B22495_04_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3: Penultimate layer of a neural network</p>
			<p>To <a id="_idTextAnchor096"/>obtain these vectors, the output layer (the last layer) of the neural network is removed, and the output from the preceding layer—the penultimate or second-last layer—is captured. Typically, the final layer outputs the model’s prediction, prompting the use of the output from the layer just before it. The data that is fed into the network’s predictive layer is known as <span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">vector embedding</strong></span><span class="No-Break">.</span></p>
			<p>The dimensionality of a vector embedding aligns with the size of the penultimate layer of the underlying neural network of the model being used, making it synonymous with the vector’s size or length. Dimensionalities such as 384 (by SBERT’s <strong class="source-inline">all-MiniLM-L6-v2</strong>), 768 (by SBERT’s <strong class="source-inline">all-mpnet-base-v2</strong>), 1,536 (by OpenAI’s <strong class="source-inline">text-embedding-ada-002</strong>), and 2,048 (from ResNet-50 by Microsoft Research) are common. Larger vectors are becoming available now, such as 3,072 by <span class="No-Break">OpenAI’s </span><span class="No-Break"><strong class="source-inline">text-embedding-3-large</strong></span><span class="No-Break">.</span></p>
			<h3>Wha<a id="_idTextAnchor097"/>t does a vector embedding mean, and how is it typically used?</h3>
			<p>Vector embeddings are the output of an embedding model, expressed as an array of floating-point numbers that typically range from –1.0 to +1.0. Each position in the array represents <span class="No-Break">a dimension.</span></p>
			<p>Vector embeddings play a key role in context-retrieval use cases, such as semantic search in chatbots. Data is embedded and stored in a vector database upfront, and queries must use the same embedding model for accurate results. Each embedding model produces unique embeddings based on its training data, making them specific to the model’s domain and not interchangeable. For example, the embedding obtained from a model trained on full documents of legal text will differ from one trained on healthcare data for <span class="No-Break">patient history.</span></p>
			<p>You may recall the example of trying to find movies for <em class="italic" lang="en-US" xml:lang="en-US">Guardians of the Galaxy</em> at the beginning of this chapter. You now understand why you had to embed the search string (which is also called the query vector) using the same embedding model. This workflow, common in AI applications, is explained in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer043">
					<img alt="" role="presentation" src="image/B22495_04_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Fig<a id="_idTextAnchor098"/>u<a id="_idTextAnchor099"/>re 4.4: Typical data flow for embedding source data into the vector store and query vectors</p>
			<p>The workflow shows the <em class="italic">Transform into embedding</em> step twice: one for embedding existing data into a vector database (on the left) and another for real-time embedding of the query (on the right). Both steps must use the same <span class="No-Break">embedding model.</span></p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor100"/>Embedding model leaderboards</h2>
			<p>With such a variety of existing models and new models constantly evolving, how can you stay up to date? <strong class="bold" lang="en-US" xml:lang="en-US">Embedding model leaderboards</strong>, such as those offered by platforms like Hugging Face, help gauge the performance of various models across numerous tasks. They provide transparent and competitive rankings of models based on criteria, such as accuracy and efficiency. By measuring models against standardized datasets and benchmark tasks, these leaderboards pinpoint state-of-the-art models and <span class="No-Break">their trade-offs.</span></p>
			<p>The <strong class="bold">Massive Text Embedding Benchmark </strong>(<strong class="bold">MTEB</strong>) leaderboard from Hugging Face is a critical resource. It offers a comprehensive overview of the performance benchmarks of text embedding models. To see which models are setting the standard, visit the Hugging Face MTEB <span class="No-Break">leaderboard: </span><a href="https://huggingface.co/spaces/mteb/leaderboard"><span class="No-Break"><span class="P---URL">https://huggingface.co/spaces/mteb/leaderboard</span></span></a><span class="No-Break">.</span></p>
			<p>You can also consult other leaderboards as you select the components of your AI/ML application architecture. Hugging Face hosts the Open LLM leaderboard (<a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard"><span class="P---URL">https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard</span></a>) and language-specific leaderboards, such as the Open Portuguese LLM leaderboard, the Open Ko-LLM leaderboard (Korean), and the Spanish Embeddings leaderboard. There are even industry-specific leaderboards, such as the Open <span class="No-Break">Medical-LLM leaderboard.</span></p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor101"/>Embedding models overview</h2>
			<p><em class="italic">Table 4.2</em> provides a quick overview of some of the embedding models covered in this chapter, focusing on their quality and ease of use. Each model’s description includes the quality of embeddings based on factors such as accuracy in downstream tasks and the richness of semantic representation, ease of use, documentation quality, and <span class="No-Break">computational requirements.</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Embedding model</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Embedding quality and ease </strong><span class="No-Break"><strong class="bold">of use</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Word2vec</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High-quality, contextually rich embeddings. Available on TensorFlow and others, but limited <span class="No-Break">availability online.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GloVe</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Robust embeddings, especially for less frequent words. Available on TensorFlow and others, but limited <span class="No-Break">availability online.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">BERT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Contextualized embeddings that are rich and adaptable. <span class="No-Break">Available online.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High-quality embeddings that excel in generative and language understanding tasks. <span class="No-Break">Available online.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Doc2vec</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Suitable for document-level tasks; embeddings reflect broader context than <span class="No-Break">word-level models.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">fastText</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Captures OOV words effectively. Open source and remarkably lightweight. Works on standard hardware and can produce models small enough for <span class="No-Break">mobile devices.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">text-embedding-3-large</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>High-quality embeddings for sophisticated NLP tasks, capturing nuanced context. Replaced OpenAI’s <strong class="source-inline">text-embedding-ada-002</strong>. Can produce smaller vectors while maintaining high <span class="No-Break">embedding quality.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">text-embedding-3-small</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Good-quality embeddings for standard NLP tasks, balancing performance and <span class="No-Break">computational requirements.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.2: Embedding quality and ease of use in various embedding models</p>
			<p>While this comparison should serve as a guide to selecting the most suitable embedding model for specific needs, the MTEB leaderboard mentioned previously, as well as online documentation, should always be consulted given the fast-moving development in <span class="No-Break">this space.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor102"/>Do you always need an embedding model?</h2>
			<p>No, you don’t need an embedding model <em class="italic">always</em>. Not all situations call for the intricate details of an embedding model to represent data in the required vector form. For some applications, more straightforward vectorization methods are <span class="No-Break">entirely adequate.</span><a id="_idTextAnchor103"/></p>
			<p>In some cases, complex public embedding models or bespoke models are unnecessary. Tasks with narrow focus, clear rules, or structured data can thrive on simple vector representations. This approach suits straightforward clustering, precise similarity measurements, and situations with limited <span class="No-Break">computing power.</span></p>
			<p><a id="_idTextAnchor104"/><a id="_idTextAnchor105"/>For instance, <strong class="bold">one-hot encoding</strong> is a straightforward technique that turns categorical data into binary vectors, fitting perfectly for cases where categories are nominal without any intrinsic order. Similarly, <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>) vectors adeptly convey text significance for information retrieval and ranking tasks by highlighting the relevance of terms within documents in relation to the <span class="No-Break">whole corpus.</span></p>
			<p><a id="_idTextAnchor106"/><a id="_idTextAnchor107"/><a id="_idTextAnchor108"/>These alternatives may lack the semantic depth of embedding models but provide computational efficiency and simplicity for tasks where intricate context isn’t required. Opting for simple vector representations enhances transparency, reduces computational demands or advanced scientific skill, and is ideal for swift performance or resource-limited environments, such as embedded systems or <span class="No-Break">mobile devices.</span></p>
			<p>With your understanding of embedding models established, you can now move on to a practical demonstration using Python, LangChain, MongoDB Atlas, <span class="No-Break">and OpenAI.</span><a id="_idTextAnchor109"/></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor110"/>Executing code from LangChain</h2>
			<p>Now that you have explored the diverse types of embedding models, you will see what it is like to use them with working code. The following Python script (named <strong class="source-inline">semantic_search.py</strong>) uses the <strong class="source-inline">langchain-openai</strong> library to embed textual data with OpenAI’s <strong class="source-inline">text-embedding-3-large</strong> model, tailored to produce 1,024 dimensional vectors <span class="No-Break">versus 3,072:</span></p>
			<pre class="source-code">
import os, pprint, time
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_openai import OpenAIEmbeddings
from pymongo import MongoClient
 
os.environ["OPENAI_API_KEY"] = "YOUR-OPENAI-API-KEY"
MONGODB_ATLAS_CONNECTION_STRING = "YOUR-MONGODB_ATLAS-CONNSTRING"
client = MongoClient(MONGODB_ATLAS_CONNECTION_STRING, tls=True, tlsAllowInvalidCertificates=True)
 
db_name = "embeddings"
collection_name = "text"
coll = client[db_name][collection_name]
vector_search_index = "text_vector_index"
 
coll.delete_many({})
 
texts = []
texts.append("A martial artist agrees to spy on a reclusive crime lord using his invitation to a tournament there as cover.")
texts.append("A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.")
texts.append("When a boy wishes to be big at a magic wish machine, he wakes up the next morning and finds himself in an adult body.")
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-large", 
    dimensions=1024,
    disallowed_special=()
)
 
embeddings = embedding_model.embed_documents(texts)
 
docs = []
for i in range(len(texts)):
    docs.append(
        {
            "text": texts[i], 
            "embedding": embeddings[i]
        }
    )
 
coll.insert_many(docs)
print("Documents embedded and inserted successfully.")
 
time.sleep(3) # allow vector store (Atlas) to undergo indexing
 
semantic_queries = []
semantic_queries.append("Secret agent captures underworld boss.")
semantic_queries.append("Awkward team of space defenders.")
semantic_queries.append("A magical tale of growing up.")
 
vector_search = MongoDBAtlasVectorSearch(
    collection= coll,
    embedding= OpenAIEmbeddings(
      model="text-embedding-3-large", 
      dimensions=1024,
      disallowed_special=()),
    index_name= vector_search_index
)
 
for q in semantic_queries:
    results = vector_search.similarity_search_with_score(
        query = q, 
        k = 3
    )
    print("SEMANTIC QUERY: " + q)
    print("RANKED RESULTS: ")
    pprint.pprint(results)
    print("")</pre>			<p>The console output will be <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
(myenv) % python3 semantic_search.py
0
1
2
Documents embedded and inserted successfully.
SEMANTIC QUERY: Secret agent captures underworld boss.
RANKED RESULTS:
[(Document(metadata={'_id': '66aada5537ef2109b3058ccb'}, page_content='A martial artist agrees to spy on a reclusive crime lord using his invitation to a tournament there as cover.'),
  0.770392894744873),
(Document(metadata={'_id': '66aada5537ef2109b3058ccc'}, page_content='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.'),
  0.6555435657501221),
(Document(metadata={'_id': '66aada5537ef2109b3058ccd'}, page_content='When a boy wishes to be big at a magic wish machine, he wakes up the next morning and finds himself in an adult body.'),
  0.5847723484039307)]
 
SEMANTIC QUERY: Awkward team of space defenders.
RANKED RESULTS:
[(Document(metadata={'_id': '66aada5537ef2109b3058ccc'}, page_content='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.'),
  0.7871642112731934),
(Document(metadata={'_id': '66aada5537ef2109b3058ccb'}, page_content='A martial artist agrees to spy on a reclusive crime lord using his invitation to a tournament there as cover.'),
  0.6236412525177002),
(Document(metadata={'_id': '66aada5537ef2109b3058ccd'}, page_content='When a boy wishes to be big at a magic wish machine, he wakes up the next morning and finds himself in an adult body.'),
  0.5492569208145142)]
 
SEMANTIC QUERY: A magical tale of growing up.
RANKED RESULTS:
[(Document(metadata={'_id': '66aada5537ef2109b3058ccd'}, page_content='When a boy wishes to be big at a magic wish machine, he wakes up the next morning and finds himself in an adult body.'),
  0.7488957047462463),
(Document(metadata={'_id': '66aada5537ef2109b3058ccb'}, page_content='A martial artist agrees to spy on a reclusive crime lord using his invitation to a tournament there as cover.'),
  0.5904781222343445),
(Document(metadata={'_id': '66aada5537ef2109b3058ccc'}, page_content='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.'),
  0.5809941291809082)]</pre>			<p>The example sets up the environment, authenticating to OpenAI with API keys, and connecting to MongoDB Atlas. Plots for three movies are then embedded and stored in MongoDB Atlas (the vector store) and different vector searches are then executed to demonstrate semantic search with <span class="No-Break">ranked results.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor111"/>Best practices</h1>
			<p>Selecting the most appropriate embedding models and vector size is not merely a technical decision, but a strategic one that aligns with the unique characteristics, technical and organizational constraints, and objectives of <span class="No-Break">your project.</span></p>
			<p>Maintaining computational efficiency and cost is another cornerstone of effectively using embedding models. As some models can be resource-intensive and have higher response times and higher cost, optimizing the computational aspects without sacrificing the quality of the output is essential. Designing your system to use different embedding models depending on the task at hand will yield a more resilient <span class="No-Break">application architecture.</span></p>
			<p>It’s imperative to regularly evaluate your embedding model to ensure your AI/ML application continues to perform as expected. This involves routinely checking performance metrics and making necessary adjustments. Tweaking your model usage could mean altering vector sizes to avoid <strong class="bold">overfitting</strong>—where the model is too finely tuned to training data and performs poorly on <span class="No-Break">unseen data.</span></p>
			<p>It is essential to monitor vector search response times versus the embedding models being used and vector sizes, as these impact the user experience of AI-driven applications. Also consider the costs of maintaining and updating embedding models, including monetary, time, and resource expenses for re-embedding data. Planning for these helps make informed decisions on when updates are needed and balancing performance, cost-efficiency, and <span class="No-Break">technological advancement.</span></p>
			<h1 id="_idParaDest-77">S<a id="_idTextAnchor112"/><a id="_idTextAnchor113"/>ummary</h1>
			<p>T<a id="_idTextAnchor114"/>his chapter covered the realm of embedding models, which are essential tools in AI/ML applications. They facilitate the transformation of high-dimensional data into a more manageable, lower-dimensional space. This process, known as embedding, significantly boosts computational efficiency and enhances the ability to describe and quantify relationships within data. Selecting the right embedding models for different types of data, such as text, audio, video, images, and structured data, is essential for expanding the reach of use cases and <span class="No-Break">different workloads.</span></p>
			<p>The chapter also highlighted the importance of consulting leaderboards to gauge the effectiveness across the vast list of available models and the delicate balance necessary when choosing vector sizes, emphasizing the trade-offs between detail, efficiency, performance, and cost. While embedding models provide deep, contextual insights, simpler vectorization methods might be adequate for <span class="No-Break">certain tasks.</span></p>
			<p>The next chapter will delve into aspects of vector databases, examining the role of vector search in AI/ML applications with <span class="No-Break">use cases.</span></p>
		</div>
	</body></html>