- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing Text Data with Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language is one of the most amazing abilities of human beings; it evolves during
    the individual’s lifetime and is capable of conveying a message with complex meaning.
    Language in its natural form is not understandable to machines, and it is extremely
    challenging to develop an algorithm that can pick up the different nuances. Therefore,
    in this chapter, we will discuss how to represent text in a form that is digestible
    by machines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In natural form, text cannot be directly fed to a **deep learning** model. In
    this chapter, we will discuss how text can be represented in a form that can be
    used by **machine learning** models. Starting with natural text, we will transform
    the text into numerical vectors that are increasingly sophisticated (one-hot encoding,
    **bag of words** (**BoW**), **term frequency-inverse document frequency** (**TF-IDF**))
    until we create vectors of real numbers that represent the meaning of a word (or
    document) and allow us to conduct operations (word2vec). In this chapter, we introduce
    deep learning models, such as **recurrent neural networks** (**RNNs**), l**ong
    short-term memory** (**LSTM**), **gated recurrent units** (**GRUs**), and **convolutional
    neural network** (**CNNs**), to analyze sequences and discuss their strengths
    as well as the problems associated with them. Finally, we will assemble these
    models all together to conduct text classification, showing the power of the learned
    approaches.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will be able to take a corpus of text and use
    deep learning to analyze it. These are the bases that will help us understand
    how a **large language model** (**LLM**) (such as ChatGPT) works internally.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Representing text for AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding, application, and representation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs, LSTMs, GRUs, and CNNs for text
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing sentiment analysis with embedding and deep learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use standard libraries for Python. The necessary libraries
    can be found within each of the Jupyter notebooks that are in the GitHub repository
    for this chapter: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1).
    The code can be executed on a CPU, but a GPU is advised.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Representing text for AI
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to other types of data (such as images or tables), it is much more
    challenging to represent text in a digestible representation for computers, especially
    because there is no unique relationship between the meaning of a word (signified)
    and the symbol that represents it (signifier). In fact, the meaning of a word
    changes from the context and the author’s intentions in using it in a sentence.
    In addition, native text has to be transformed into a numerical representation
    to be ingested by an algorithm, which is not a trivial task. Nevertheless, several
    approaches were initially developed to be able to find a vector representation
    of a text. These vector representations have the advantage that they can then
    be used as input to a computer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: First, a collection of texts (**corpus**) should be divided into fundamental
    units (words). This process requires making certain decisions and process operations
    that collectively are called **text normalization**. A sentence, therefore, is
    divided into words by exploiting the natural division of spaces (**text segmentation**);
    each punctuation mark is also considered a single word. In fact, punctuation marks
    are considered to be the boundaries of sentences and convey important information
    (change of topic, questions, exclamations).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is the definition of what a word is and whether some terms
    in the corpus should be directly joined under the same vocabulary instance. For
    example, “He” and “he” represent the same instance; the former is only capitalized.
    Since an algorithm does not include such nuances, one must normalize the text
    in lowercase. In some cases, we want to conduct more sophisticated normalizations
    such as **lemmatization** (joining words with the same root: “came” and “comes”
    are two forms of the verb) or **stemming** (stripping all suffixes of words).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenization** is the task of transforming a text into fundamental units.
    This is because, in addition to words, a text may also include percentages, numbers,
    websites, and other components. We will return to this later, but in the meantime,
    we will look at some simpler forms of tokenization.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In traditional **natural language processing** (**NLP**), text representation
    is conducted using discrete symbols. The simplest example is one-hot encoding.
    From a sequence of text in a corpus (consisting of *n* different words), we obtain
    an *n*-dimensional vector. In fact, the first step is to compute the set of different
    words present in the whole text corpus called vocabulary. For each word, we obtain
    a vector as long as the size of the vocabulary. Then for each word, we will have
    a long vector composed mainly of zeros and ones to represent the word (one-hot
    vectors). This system is mainly used when we want a matrix of features and then
    train a model. This process is also called **vectorization**; here''s a sparse
    vector for the following two words:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>r</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>t</mi><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center
    center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto"
    rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>p</mi><mi>i</mi><mi>z</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>a</mi><mo>=</mo><mfenced
    close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em
    0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center
    center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto"
    rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable
    columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto
    auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: There are different problems associated with this representation. First, it
    captures only the presence (or the absence) of a word in a document. Thus, we
    are losing all the semantic relationships between the words. Second, an average
    language has about 200,000 words, so for each word, we would have a vector of
    length 200,000\. This leads to very sparse and high-dimensional vectors. For large
    corpora, we need high memory to store the vectors and high computational capacity
    to handle them. In addition, there is no notion of similarity. The two words in
    the preceding example are two places that sell food, and we would like the vectors
    representing these words to encode this similarity. If the vectors had a notion
    of similarity, we could conduct clustering, and the synonyms would be in the same
    cluster.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to obtain such a matrix, we must do the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Standardize the text before tokenization. In this case, we simply transform
    everything into lowercase.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We construct a vocabulary constituted of unique words and save the vocabulary
    so that in a case from a vector, we can get the corresponding word.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们构建一个由唯一单词组成的词汇表，并将其保存下来，以便在向量中可以获取相应的单词。
- en: We create an array and then populate it with `1` at the index of the word in
    the vocabulary; `0`s elsewhere.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建一个数组，然后在其词汇表中单词的索引位置填充 `1`，其他位置填充 `0`。
- en: 'Let’s take a look at how this works in code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码是如何实现这一点的：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at a specific example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We get the following output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/B21257_01_Figure_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21257_01_Figure_01.jpg)'
- en: Important note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Observe how choosing another sentence will result in a different matrix and
    how, by increasing the length of the sentence, the matrix grows proportionally
    to the number of different words. Also, note that for repeated words, we get equal
    vectors. Check the preceding output.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 观察选择另一个句子将如何导致不同的矩阵，以及通过增加句子的长度，矩阵如何成比例地增长到不同单词的数量。此外，请注意，对于重复的单词，我们得到相同的向量。检查前面的输出。
- en: Even if it is a simple method, we have obtained a first representation of text
    in a vectorial form.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这是一个简单的方法，我们也得到了文本向量化形式的第一种表示。
- en: Bag-of-words
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: In the previous section, we discussed one-hot encoding and some of the problems
    associated with this form of text representation. In the previous example, we
    worked with a single sentence, but a corpus is made up of thousands if not millions
    of documents; each of these documents contains several words with a different
    frequency. We want a system that preserves this frequency information, as it is
    important for the classification of text. In fact, documents that have similar
    content are similar, and their meaning will also be similar.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了一热编码以及与这种文本表示形式相关的一些问题。在之前的例子中，我们处理了一个句子，但语料库由成千上万甚至数百万个文档组成；每个文档都包含不同频率的几个单词。我们希望有一个系统可以保留这些频率信息，因为它对于文本的分类很重要。事实上，内容相似的文档是相似的，它们的含义也将是相似的。
- en: '**BoW** is an algorithm for extracting features from text that preserves this
    frequency property. BoW is a very simple algorithm that ignores the position of
    words in the text and only considers this frequency property. The name “bag” comes
    precisely from the fact that any information concerning sentence order and structure
    is not preserved by the algorithm. For BoW, we only need a vocabulary and a way
    to be able to count words. In this case, the idea is to create document vectors:
    a single vector represents a document and the frequency of words contained in
    the vocabulary. *Figure 1**.1* visualizes this concept with a few lines from *Hamlet*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**BoW** 是一种从文本中提取特征并保留此频率属性的算法。BoW 是一个非常简单的算法，它忽略了文本中单词的位置，只考虑这种频率属性。名称“bag”正是由于算法不保留关于句子顺序和结构的信息。对于
    BoW，我们只需要一个词汇表和一种能够计数单词的方法。在这种情况下，我们的想法是创建文档向量：一个向量代表一个文档，以及词汇表中包含的单词的频率。*图 1**.1*
    使用《哈姆雷特》的一些行来可视化这个概念：'
- en: '![Figure 1.1 – Representation of the BoW algorithm](img/B21257_01_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 1.1 – BoW 算法的表示](img/B21257_01_01.jpg)'
- en: Figure 1.1 – Representation of the BoW algorithm
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – BoW 算法的表示
- en: Even this representation is not without problems. Again, as the vocabulary grows,
    so will the size of the vectors (the size of each vector is equal to the length
    of the vocabulary). In addition, these vectors tend to be scattered, especially
    when the documents are very different from each other. High-dimensional or sparse
    vectors are not only problematic for memory and computational costs but for algorithms
    as well (the longer the vectors, the more weight you need in the algorithm, leading
    to a risk of overfitting). This is called the **curse of dimensionality**; the
    greater the number of features, the less meaningful the distances between examples.
    For large corpora, some solutions have been proposed, such as ignoring punctuation,
    correcting misspelled words, stemming algorithms, or ignoring words with high
    frequency that don’t add information (articles, prepositions, and so on).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get a BoW matrix for a list of documents, we need to do the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize each document to get a list of words.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create our vocabulary of unique words and map each word to the corresponding
    index in the vocabulary.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a matrix where each row represents a document and each column, instead,
    a word in the vocabulary (the documents are the examples, and the words are the
    associated features).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at the code again:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here’s an example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This prints the following output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21257_01_Figure_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Important note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Note how in the example, the word “`awesome`” is associated with a review with
    a positive, neutral, or negative meaning. Without context, the frequency of the
    word “awesome” alone does not tell us the sentiment of the review.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have learned how to transform text in a vectorial form while keeping
    the notion of frequency for each word.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we obtained a document-term matrix. However, the raw
    frequency is very skewed and does not always allow us to discriminate between
    two documents. The document-term matrix was born in information retrieval to find
    documents, though words such as “good” or “bad” are not very discriminative since
    they are often used in text with a generic meaning. In contrast, words with low
    frequency are much more informative, so we are interested more in relative than
    absolute frequency:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Intuition of the components of TF-IDF](img/B21257_01_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Intuition of the components of TF-IDF
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using raw frequency, we can use the logarithm in base 10, because
    a word that occurs 100 times in a document is not 100 times more relevant to its
    meaning in the document. Of course, since vectors can be very sparse, we assign
    0 if the frequency is 0\. Second, we want to pay more attention to words that
    are present only in some documents. These words will be more relevant to the meaning
    of the document, and we want to preserve this information. To do this, we normalize
    by IDF. IDF is defined as the ratio of the total number of documents in the corpus
    to how many documents a term is present in. To summarize, to obtain the TF-IDF,
    we multiply TF by the logarithm of IDF.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'This is demonstrated in the following code block:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This generates the following output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21257_01_Figure_03.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Important note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we used the same corpus as in the previous section. Note how
    word frequencies changed after this normalization.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how we can normalize text to decrease the impact
    of the most frequent words and give relevance to words that are specific to a
    subset of documents. Next, we’ll discuss embedding.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Embedding, application, and representation
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we discussed how to use vectors to represent text.
    These vectors are digestible for a computer, but they still suffer from some problems
    (sparsity, high dimensionality, etc.). According to the distributional hypothesis,
    words with a similar meaning frequently appear close together (or words that appear
    often in the same context have the same meaning). Similarly, a word can have a
    different meaning depending on its context: “I went to deposit money in the *bank*”
    or “We went to do a picnic on the river *bank*.” In the following diagram, we
    have a high-level representation of the embedding process. So, we want a process
    that allows us to start from text to obtain an array of vectors, where each vector
    corresponds to the representation of a word. In this case, we want a model that
    will then allow us to map each word to a vector representation. In the next section,
    we will describe the process in detail and discuss the theory behind it.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – High-level representation of the embedding process](img/B21257_01_03.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – High-level representation of the embedding process
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: We would, therefore, like to generate vectors that are small in size, composed
    of real (dense) numbers, and that preserve this contextual information. Thus,
    the purpose is to have vectors of limited size that can represent the meaning
    of a word. The scattered vectors we obtained earlier cannot be used efficiently
    for mathematical operations or downstream tasks. Also, the more words there are
    in the vocabulary, the larger the size of the vectors we get. Therefore, we want
    dense vectors (with real numbers) that are small in size and whose size does not
    increase as the number of words in the vocabulary increases.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: In addition, these vectors have a distributed representation of the meaning
    of the word (whereas in sparse vectors, it was local or where the `1` was located).
    As we will see a little later, these dense vectors can be used for different operations
    because they better represent the concept of similarity between words. These dense
    vectors are called **word embeddings**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: This concept was introduced in 2013 by Mikolov with a framework called **word2vec**,
    which will be described in detail in the next section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The intuition behind word2vec is simple: predict a word *w* from its context.
    To do this, we need a **neural network** and a large corpus. The revolutionary
    idea is that by training this neural network to predict which words *c* are needed
    near the target word *w*, the weights of the neural network will be the embedding
    vectors. This model is self-supervised; the labels in this case are implicit,
    and we do not provide them.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Word2vec simplifies this idea by making the system extremely fast and effective
    in two ways: by turning the task into binary classification (Is the word *c* needed
    in the context of the word *w*? Yes or no?) and using a logistic regression classifier:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – In word2vec, we slide a context window (here represented as
    a  three-word context window), and then we randomly sample some negative words](img/B21257_01_04.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – In word2vec, we slide a context window (here represented as a three-word
    context window), and then we randomly sample some negative words
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Given a text *t*, we scroll a window *c* (our context) for a word *w* in the
    center of our window; the words around it are examples of the positive class.
    After that, we select other random words as negative examples. Finally, we train
    a model to classify the positive and negative examples; the weights of the model
    are our embeddings.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a word *w* and a word *c*, we want the probability that the word *c*
    is in the context of *w* to be similar to its embedding similarity. In other words,
    if the vector representing *w* and *c* are similar, *c* must often be in the context
    of *w* (word2vec is based on the notion of context similarity). We define this
    embedding similarity by the dot product between the two embedding vectors for
    *w* and *c* (we use the sigmoid function to transform this dot product into a
    probability and thus allow comparison). So, the probability that *c* is in the
    context of *w* is equal to the probability that their embeddings are similar:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mi>σ</mi><mfenced
    close=")" open="("><mrow><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done for all words in context *L*. To simplify, we assume that all
    words in the context window are independent, so we can multiply the probabilities
    of the various words *c*. Similarly, we want to ensure that this dot product is
    minimal for words that are not in the context of word *w*. So, on the one hand,
    we maximize the probability for words in the context, and on the other hand, we
    minimize the probability for words that are not in the context. In fact, words
    that are not in the context of *w* are randomly extracted during training, and
    the process is the same:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi
    mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we take the logarithm of probability:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi
    mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced
    close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi
    mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'The matrix of weights *w* is our embedding; it is what we will use from now
    on. Actually, the model learns two matrices of vectors (one for *w* and one for
    *c*), but the two matrices are very similar, so we take just one. We then use
    cross-entropy to train the models and learn the weights for each vector:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mo>-</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'This is represented visually in the following diagram:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Word and context embedding](img/B21257_01_05.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Word and context embedding
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'The following choices affect the quality of embedding:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '*Data quality is critical*. For example, leveraging Wikipedia allows better
    embedding for semantic tasks, while using news improves performance for syntactic
    tasks (a mixture of the two is recommended). Using Twitter or other social networks
    can insert bias.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, *a larger amount of text improves embedding performance*.
    A large amount of text can partially compensate for poor quality but at the cost
    of much longer training (for example, Common Crawl is a huge dataset downloaded
    from the internet that is pretty dirty, though).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The number of dimensions is another important factor*. The larger the size
    of the embedding, the better its performance. 300 is considered a sweet spot because,
    beyond this size, number performance does not increase significantly.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The size of the context window also has an impact*. Generally, a context window
    of 4 is used, but a context window of 2 allows for vectors that better identify
    parts of speech. In contrast, long context windows are more useful if we are interested
    in similarity more broadly.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Python, we can easily get an embedding from lists of tokens using the following
    code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Important note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The complete code is present in the GitHub repository. We used an embedding
    of 100 dimensions and a window of 5 words.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our embedding, we can visualize it. For example, if we try **clustering**
    the vectors of some words, words that have similar meanings should be closer together:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Clustering of some of the vectors obtained from the embedding](img/B21257_01_06.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Clustering of some of the vectors obtained from the embedding
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to visualize vectors is to use dimensionality reduction techniques.
    Vectors are multidimensional (100-1,024), so it is more convenient to reduce them
    to two or three dimensions so that they can be visualized more easily. Some of
    the most commonly used techniques are **principal component analysis** (**PCA**)
    and **t-distributed stochastic neighbor embedding** (**t-SNE)**. **Uniform Manifold
    Approximation and Projection** (**UMAP**), on the other hand, is a technique that
    has become the first choice for visualizing multidimensional data in recent years:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – 2D projection of word2vec embedding highlighting some examples](img/B21257_01_07.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – 2D projection of word2vec embedding highlighting some examples
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: UMAP has emerged because it produces visualizations that better preserve semantic
    meaning and relationships between examples and also better represent local and
    global structures. This makes for better clusters, and UMAP can also be used in
    preprocessing steps before a classification task on vectors.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: A notion of similarity for text
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have obtained vector representations, we need a method to calculate
    the similarity between them. This is crucial in many applications—for instance,
    to find words in an embedding space that are most similar to a given word, we
    compute the similarity between its vector and those of other words. Similarly,
    given a query sentence, we can retrieve the most relevant documents by comparing
    its vector with document embeddings and selecting those with the highest similarity.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Most similarity measures are based on the **dot product**. This is because
    the dot product is high when the two vectors have values in the same dimension.
    In contrast, vectors that have zero alternately will have a dot product of zero,
    thus orthogonal or dissimilar. This is why the dot product was used as a similarity
    measure for word co-occurrence matrices or with vectors derived from document
    TF matrices:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">o</mi><mi
    mathvariant="bold-italic">t</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><mi
    mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">u</mi><mi
    mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">t</mi><mo>:</mo><mi
    mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><msub><mi>a</mi><mn>1</mn></msub><mo>×</mo><msub><mi>b</mi><mn>1</mn></msub><mo>+</mo><msub><mi>a</mi><mn>2</mn></msub><mo>×</mo><msub><mi>b</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><msub><mi>a</mi><mi>n</mi></msub><mo>×</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>m</mi><mi>a</mi><mi>g</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>u</mi><mi>d</mi><mi>e</mi><mo>=</mo><mfenced
    close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mo>=</mo><mroot><mrow><munderover><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mrow>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'The dot product has several problems, though:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: It tends to favor vectors with long dimensions
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It favors vectors with high values (which, in general, are those of very frequent
    and, therefore, useless words)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value of the dot product has no limits
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, alternatives have been sought, such as a normalized version of the
    dot product. The normalized dot product is equivalent to the cosine of the angle
    between the two vectors, hence **cosine similarity**:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">Θ</mi><mo>=</mo><mfrac><mrow><mi
    mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi></mrow><mrow><mfenced
    close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mfenced close="|"
    open="|"><mi mathvariant="bold-italic">b</mi></mfenced></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub></mrow></mrow><mrow><mroot><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot><mroot><mrow><msubsup><mo>∑</mo><mrow><mi
    mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mfrac></mrow></mrow>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Cosine similarity has some interesting properties:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: It is between -1 and 1\. Opposite or totally dissimilar vectors will have a
    value of -1, 0 for orthogonal vectors (or totally dissimilar for scattered vectors),
    and 1 for perfectly similar vectors. Since it measures the angle between two vectors,
    the interpretation is easier and is within a specific range, so it allows one
    intuitively to understand the similarity or dissimilarity.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is fast and cheap to compute.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is less sensitive to word frequency and, thus, more robust to outliers.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is scale-invariant, meaning that it is not influenced by the magnitude of
    the vectors.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being normalized, it can also be used with high-dimensional data.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For 2D vectors, we can plot to observe these properties:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Example of cosine similarity between two vectors](img/B21257_01_08.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Example of cosine similarity between two vectors
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we can define the properties of our trained embedding using
    this notion of similarity.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Properties of embeddings
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embeddings are a surprisingly flexible method and manage to encode different
    syntactic and semantic properties that can both be visualized and exploited for
    different operations. Once we have a notion of similarity, we can search for the
    words that are most similar to a word *w*. Note that similarity is defined as
    appearing in the same context window; the model cannot differentiate synonyms
    and antonyms.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the model is also capable of representing grammatical relations
    such as superlatives or verb forms.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting relationship we can study is analogies. The parallelogram
    model is a system for representing analogies in a cognitive space. The classic
    example is *king:queen::man:?* (which in a formula would be *a:b::a*:?*). Given
    that we have vectors, we can turn this into an *a-a*+b* operation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test this in Python using the embedding model we have trained:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: We can check the most similar words
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can test the analogy
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can then test the capacity to identify synonyms and antonyms
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this process is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Important note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: This is done with the embedding we have trained before. Notice how the model
    is not handling antonyms well.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The method is not exactly perfect, so sometimes the right answer is not the
    first result, but it could be among the first three inputs. Also, this system
    works for entities that are frequent within the text (city names, common words)
    but much less with rarer entities.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Embeddings can also be used as a tool to study how the meaning of a word changes
    over time, especially if you have text corpora that span several decades. This
    is demonstrated in the following diagram, which shows how the meanings of the
    words *gay*, *broadcast*, and *awful* have changed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – 2D visualization of how the semantic meaning of a word changes
    over the years; these projections are obtained using text from different decades
    and embeddings (https://arxiv.org/abs/1605.09096)](img/B21257_01_09.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – 2D visualization of how the semantic meaning of a word changes
    over the years; these projections are obtained using text from different decades
    and embeddings ([https://arxiv.org/abs/1605.09096](https://arxiv.org/abs/1605.09096))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 – 2D可视化展示了单词的语义意义如何随时间变化；这些投影是通过使用不同年代的文本和嵌入获得的([https://arxiv.org/abs/1605.09096](https://arxiv.org/abs/1605.09096))
- en: 'Finally, a word can still have multiple meanings! For example, common words
    such as “good” have more than one meaning depending on the context. One may wonder
    whether a vector for a word in an embedding represents only one meaning or whether
    it represents the set of meanings of a word. Fortunately, embedding vectors represent
    a weighted sum of the various meanings of a word (linear superposition). The weights
    of each meaning are proportional to the frequency of that meaning in the text.
    Although these meanings reside in the same vector, when we add or subtract during
    the calculation of analogies, we are working with these components. For example,
    “apple” is both a fruit and the name of a company; if we conduct the operation
    *apple:red::banana:?*, we are subtracting only a very specific semantic component
    from the apple vector (the component that is similar to red). This flexibility
    can be useful when we want to disambiguate meanings. Also, since the vector space
    is sparse, by exploiting sparse coding, we can separate the various meanings:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个单词仍然可以有多个意义！例如，像“good”这样的常用词根据上下文有不同的意义。有人可能会想知道一个单词在嵌入中的向量是否只代表一个意义，或者它是否代表了一个单词的所有意义集合。幸运的是，嵌入向量代表了一个单词各种意义的加权总和（线性叠加）。每个意义的权重与该意义在文本中的频率成正比。尽管这些意义位于同一个向量中，但在计算类比时，我们处理的是这些组成部分。例如，“apple”既是水果，也是一家公司的名字；如果我们进行操作
    *apple:red::banana:?*，我们只是在苹果向量中减去一个非常具体的语义成分（与红色相似的成分）。这种灵活性在我们想要消除歧义时非常有用。此外，由于向量空间是稀疏的，通过利用稀疏编码，我们可以分离出不同的意义：
- en: '![Figure 1.10 – Table showing how a vector in word2vec is encoding for different
    meanings at the same time (https://aclanthology.org/Q18-1034/)](img/B21257_01_10.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图1.10 – 表格展示了word2vec中的向量如何同时编码不同的意义 (https://aclanthology.org/Q18-1034/)](img/B21257_01_10.jpg)'
- en: Figure 1.10 – Table showing how a vector in word2vec is encoding for different
    meanings at the same time ([https://aclanthology.org/Q18-1034/](https://aclanthology.org/Q18-1034/))
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 表格展示了word2vec中的向量如何同时编码不同的意义([https://aclanthology.org/Q18-1034/](https://aclanthology.org/Q18-1034/))
- en: These vectors are now providing contextual and semantic meaning for each word
    in the text. We can use this rich source of information for tasks such as text
    classification. What we need now are models that can handle the sequential nature
    of text, which we will learn about in the next section.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量现在为文本中的每个单词提供了上下文和语义意义。我们可以利用这个丰富的信息源来完成诸如文本分类等任务。我们现在需要的是能够处理文本序列性质的模型，我们将在下一节中学习到这方面的内容。
- en: RNNs, LSTMs, GRUs, and CNNs for text
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNNs，LSTMs，GRUs和CNNs用于文本
- en: So far, we have discussed how to represent text in a way that is digestible
    for the model; in this section, we will discuss how to analyze the text once a
    representation has been obtained. Traditionally, once we obtained a representation
    of the text, it was fed to models such as naïve Bayes or even algorithms such
    as logistic regression. The success of neural networks has made these machine
    learning algorithms outdated. In this section, we will discuss deep learning models
    that can be used for various tasks.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了如何以模型可消化的方式表示文本；在本节中，我们将讨论在获得表示后如何分析文本。传统上，一旦我们获得了文本的表示，它就会被输入到诸如朴素贝叶斯或甚至逻辑回归等模型中。神经网络的成功使得这些机器学习算法变得过时。在本节中，我们将讨论可用于各种任务的深度学习模型。
- en: RNNs
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNNs
- en: 'The problem with classical neural networks is that they have no memory. This
    is especially problematic for time series and text inputs. In a sequence of words
    *t*, the word *w* at time *t* depends on the *w* at time *t-1*. In fact, in a
    sentence, the last word is often dependent on several words in the sentence. Therefore,
    we want an NN model that maintains a memory of previous inputs. An **RNN** maintains
    an internal state that maintains this memory; that is, it stores information about
    previous inputs, and the outputs it produces are affected by previous inputs.
    These networks perform the same operation on all elements of the sequence (hence
    recurrent) and maintain the memory of this operation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Simple example of an RNN (https://arxiv.org/pdf/1506.00019)](img/B21257_01_11.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – Simple example of an RNN ([https://arxiv.org/pdf/1506.00019](https://arxiv.org/pdf/1506.00019))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'A classical neural network (**feedforward neural network**) considers inputs
    to be independent, and one layer of a neural network performs the following operation
    for a vector representing the element at time *t*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi
    mathvariant="bold-italic">W</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:math>
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in a simple RNN, the following operations are conducted:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><msup><mi mathvariant="bold-italic">a</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi
    mathvariant="bold-italic">b</mi><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mi
    mathvariant="bold-italic">W</mi><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup></mrow></mrow>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi
    mathvariant="bold-italic">c</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: These operations may seem complicated, but in fact, we are simply maintaining
    a hidden state that considers the previous iterations. The first equation is a
    normal feedforward layer modified, in which we multiply the previously hidden
    state *h* by a set of weights *U*. This matrix *U* allows us to control how the
    neural network uses the previous context to bind input and past inputs (how the
    past influences the output for input at time *t*). In the second equation, we
    create a new hidden state that will then be used for subsequent computations but
    also for the next input. In the third equation, we are creating the output; we
    use a bias vector and a matrix to compute the output. In the last equation, it
    is simply passed as a nonlinearity function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'These RNNs can be seen as unrolled entities throughout time, in which we can
    represent the network and its computations throughout the sequence:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – Simple example of an RNN unrolled through the sequence (https://arxiv.org/pdf/1506.00019)](img/B21257_01_12.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – Simple example of an RNN unrolled through the sequence ([https://arxiv.org/pdf/1506.00019](https://arxiv.org/pdf/1506.00019))
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test in Python with a PyTorch RNN to see how it is transforming the
    data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Important note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the model is transforming the data; we can also access the hidden
    state.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see several interesting things:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The RNN is not limited by the size of the input; it is a cyclic operation that
    is conducted over the entire sequence. RNNs basically process one word at a time.
    This cyclicality also means backpropagation is conducted for each time step. Although
    this model works well for series analysis, its sequential nature does not allow
    it to be parallelized.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretically, this model could be trained with infinite sequences of words;
    theoretically, after a few time steps, it begins to forget the initial inputs.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training can become inefficient due to the vanishing gradient problem, where
    gradients must propagate from the final cell back to the initial one. During this
    process, they can shrink exponentially and approach zero, making it difficult
    for the model to learn long-range dependencies. Conversely, the exploding gradient
    problem can also occur, where gradients grow uncontrollably large, leading to
    unstable training.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs are not the only form of deep learning models that are relevant to this
    topic.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In theory, RNNs should be able to process long sequences and remember the initial
    input. However, in reality, the information inside the hidden state is local rather
    than global, and for a time *t*, it considers only the previous time steps and
    not the entire sequence. The main problem with such a simple model is that the
    hidden state must simultaneously fulfill two roles: provide information relevant
    to the output at time *t* and store memory for future decisions.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: An **LSTM** is an extension of RNNs, designed with the idea that the model can
    forget information that is not important and keep only the important context.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – Internal structure of an LSTM cell (https://arxiv.org/pdf/2304.11461)](img/B21257_01_13.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – Internal structure of an LSTM cell ([https://arxiv.org/pdf/2304.11461](https://arxiv.org/pdf/2304.11461))
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: An LSTM has internal mechanisms to control the information (gates) within the
    layer; additionally, it has a dedicated context layer. So, we have two hidden
    states in which the first, *h*, serves for information at time *t* (short memory)
    and the other, *c*, for information at long term. The gates can be open (1) or
    closed (0); this is achieved by a feed-forward layer with sigmoid activation to
    squeeze values between zero and one. After that, we use the **Hadamard product**
    (or pointwise multiplication) for the gating mechanism of a layer. This multiplication
    acts as a binary gate, allowing information to pass if the value is close to 1
    or blocking it when the value is close to 0\. These gates allow a dynamic system
    in which during a time step, we decide how much information we preserve and how
    much we forget.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'The first gate is called the **forget gate** because it is used to forget information
    that is no longer needed from the context and, therefore, will no longer be needed
    in the next time step. So, we will use the output of the forget gate to multiply
    the context. At this time, we extract both information from the input and the
    previous time step’s hidden state. Each gate has a set of gate-specific *U* weights:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">f</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">f</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to extract information from the input and decide which of
    that information will be added to the context. This is controlled by an **input
    gate** *i* that controls how much information will then be added. The context
    is then obtained by the sum of what we add and what we forget:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">g</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><msub><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">g</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">i</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">i</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi
    mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is the calculation of the output; this is achieved with a final
    gate. The output or final layer decision is also used to update the hidden state:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">o</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">o</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: <mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi
    mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi
    mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'These gates are independent of each other so that efficient implementations
    of the LSTM can parallelize them. We can test in Python with a PyTorch RNN to
    see how it is transforming the data:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Important note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the model is transforming the data; we can access the hidden state
    as well as the cell state.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also note some other interesting properties:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Computation augmentation is internal to layers, which means we can easily substitute
    LSTMs for RNNs.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LSTM manages to preserve information for a long time because it retains
    only the relevant part of the information and forgets what is not needed.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard practice is to initialize an LSTM with vector 1 (preserves everything),
    after which it learns by itself what to forget and what to add.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An LSTM, as opposed to an RNN, can remember up to 100 time steps (the RNN after
    7 time steps starts forgetting). The plus operation makes vanishing or exploding
    gradients less likely.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at another model option that is computationally lighter but still
    has this notion of context vector.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: GRUs
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GRUs** are another variant of RNNs to solve the vanishing gradient problem,
    thus making them more effective in remembering information. They are very similar
    to LSTMs since they have internal gates, but they are much simpler and lighter.
    Despite having fewer parameters, GRUs can converge faster than LSTMs and still
    achieve comparable performance. GRUs exploit some of the elements that have made
    LSTMs so effective: the plus operation, the Hadamard product, the presence of
    a context, and the control of information within the layer:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Internal structure of a GRU cell (https://arxiv.org/pdf/2304.11461)](img/B21257_01_14.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Internal structure of a GRU cell ([https://arxiv.org/pdf/2304.11461](https://arxiv.org/pdf/2304.11461))
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'In a GRU, the forget gate is called the **update gate**, but it has the same
    purpose: important information is retained (values near 1) and unimportant information
    is rewritten during the update (values near 0). In a GRU, the input gate is called
    the **reset gate** and is not independent as in an LSTM, but connected to the
    update gate.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is the update gate *z*, which is practically the same as the
    forget gate in an LSTM. At the same time, we calculate the reset gate *r*:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">z</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">r</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi
    mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">r</mi></msub><mo>+</mo><msub><mi
    mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi
    mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to update the hidden state; this depends on the reset gate.
    In this way, we decide what new information is put into the hidden state and what
    relevant information from the past is saved. This is called the **current** **memory
    gate**:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msup><mover><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi mathvariant="normal">t</mi><mi
    mathvariant="normal">a</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">h</mi><mo>(</mo><msub><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">h</mi></msub><msup><mi
    mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>+</mo><msup><mi mathvariant="bold-italic">r</mi><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msub><mi mathvariant="bold-italic">U</mi><mi
    mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced
    close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>)</mo></mrow></mrow></mrow>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have the final update of the hidden state in which we also
    use the update gate:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: <mrow><mrow><mrow><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup><mo>=</mo><msup><mi mathvariant="bold-italic">z</mi><mfenced
    close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msup><mi mathvariant="bold-italic">h</mi><mfenced
    close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi
    mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>)</mo><mo>⊙</mo><msup><mover><mi
    mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced close=")"
    open="("><mi>t</mi></mfenced></msup></mrow></mrow></mrow>
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test in Python with a PyTorch RNN to see how it is transforming the
    data:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the model is transforming the data; we can also access the hidden
    state.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see some interesting elements here as well:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: GRU networks are similar to LSTM networks, but they have the advantage of fewer
    parameters and are computationally more efficient. This means, though, they are
    more prone to overfitting.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can handle long sequences of data without forgetting previous inputs. For
    many textual tasks (but also speech recognition and music generation) they perform
    quite well, though they are less efficient than LSTMs when it comes to modeling
    long-term dependencies or complex patterns.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we’ll look at CNNs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: CNNs for text
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**CNNs** are designed to find patterns in images (or other 2D matrixes) by
    running a filter (a matrix or kernel) along them. The convolution is conducted
    pixel by pixel, and the filter values are multiplied by the pixels in the image
    and then summed. During training, a weight is learned for each of the filter entries.
    For each filter, we get a different scan of the image that can be visualized;
    this is called a feature map.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional networks have been successful in **computer vision** because
    of their ability to extract local information and recognize complex patterns.
    For this reason, convolutional networks have been proposed for sequences. In this
    case, 1-dimensional convolutional networks are exploited, but the idea is the
    same. In fact, on a sequence, 1D convolution is used to extract a feature map
    (instead of being a 2-dimensional filter or matrix, we have a uni-dimensional
    filter that can be seen as the context window of word2vec):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.15 – In 1D convolution, we are sliding a 1D filter over the sequence](img/B21257_01_15.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – In 1D convolution, we are sliding a 1D filter over the sequence
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we scroll a uni-dimensional filter over the sequence;
    the process is very fast, and the filter can have an arbitrary size (three to
    seven words or even more). The model tries to learn patterns among the various
    words found within this kernel. It can also be used on vectors previously obtained
    from an embedding, and we can also use multiple kernels (so as to learn different
    patterns for each sequence). As with image CNNs, we can add operations such as
    max pooling to extract the most important features.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test in Python with a PyTorch RNN to see how it is transforming the
    data:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Important note
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the model is transforming the data and how this is different from
    what we have seen before.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a method to transform text into numerical representation (while
    preserving the contextual information) and models that can handle this representation,
    we can combine them to obtain an end-to-end system.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Performing sentiment analysis with embedding and deep learning
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will train a model for conducting sentiment analysis on
    movie reviews. The model we will train will be able to classify reviews as positive
    or negative. To build and train the model, we will exploit the elements we have
    encountered so far. In brief, we’re doing the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: We are preprocessing the dataset, transforming in numerical vectors, and harmonizing
    the vectors
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are defining a neural network with an embedding and training it
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dataset consists of 50,000 positive and negative reviews. We can see that
    it contains a heterogeneous length for reviews and that on average, there are
    230 words:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.16 – Graphs showing the distribution of the length of the review
    in the text; the left plot is for positive reviews, while the right plot is for
    negative reviews](img/B21257_01_16.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – Graphs showing the distribution of the length of the review in
    the text; the left plot is for positive reviews, while the right plot is for negative
    reviews
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the most prevalent words are, obviously, “*movie*” and “*film*”:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.17 – Word cloud for the most frequent words in positive (left plot)
    and negative (right plot) reviews](img/B21257_01_17.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 – Word cloud for the most frequent words in positive (left plot)
    and negative (right plot) reviews
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'The text is messy and must be cleaned before the model can be trained. The
    first step is binary encoding of the label (“positive” equals 0, “negative” equals
    1). After that, we divide the features and the labels (for a dataset in `X` are
    the features and `y` are the labels). Next, we create three balanced datasets
    for training, validation, and testing:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A few steps are necessary before proceeding with the training:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: A **preprocessing** step in which we remove excessive spaces, special characters,
    and punctuation.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **tokenization** step in which we convert the various reviews into tokens.
    In this step, we also remove stopwords and single-character words. We extract
    for each review only the 1,000 most popular words (this step is only to reduce
    computation time during training).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformation of the the words into indices (**vectorization**) according to
    our vocabulary to make the model work with numerical values.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the reviews have different lengths, we apply padding to harmonize the
    length of the review to a fixed number (we need this for the training).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These preprocessing steps depend on the dataset. The code is in the GitHub repository.
    Note, however, that the tokenization and preprocessing choices alter the properties
    of the reviews – in this case, the summary statistics.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.18 – Graph showing the distribution of review length after tokenization](img/B21257_01_18.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – Graph showing the distribution of review length after tokenization
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'We are defining the model with its hyperparameters. In this case, we are training
    a neural network to predict sentiment data composed of embeddings and GRUs. To
    make the training more stable, we add regularization (dropout). The linear layer
    is to map these features that we extracted to a single representation. We use
    this representation to calculate the probability that the review is positive or
    negative:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that in this case, we use binary cross-entropy loss because we have only
    two categories (positive and negative). Also, we use `Adam` as an optimizer, but
    one can test others. In this case, we conduct batch training since we have thousands
    of reviews:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following graph displays the accuracy and loss for the training and validation
    sets:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.19 – Training curves for training and validation set, for accuracy
    and loss](img/B21257_01_19.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 – Training curves for training and validation set, for accuracy
    and loss
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'The model achieves good accuracy, as we can see from the following confusion
    matrix:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.20 – Confusion matrix for the test set](img/B21257_01_20.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: Figure 1.20 – Confusion matrix for the test set
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, if we look at the projection of reviews before and after the training,
    we can see that the model has learned how to separate positive and negative reviews:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21 – Embedding projection obtained from the model before (left plot)
    and after (right plot) training](img/B21257_01_21.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: Figure 1.21 – Embedding projection obtained from the model before (left plot)
    and after (right plot) training
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: We have now trained a model that can take a review in plain text and classify
    it as positive or negative. We did that by combining the elements we saw previously
    in the chapter. The same approach can be followed with any other dataset; that
    is the power of deep learning.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to transform text to an increasingly complex vector
    representation. This numerical representation of text allowed us to be able to
    use machine learning models. We saw how to preserve the contextual information
    (word embedding) of a text and how this can then be used for later analysis (for
    example, searching synonyms or clustering words). In addition, we saw how neural
    networks (RNNs, LSTM, GRUs) can be used to analyze text and perform tasks (for
    example, sentiment analysis).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to solve some of the remaining unsolved
    challenges and see how this will lead to the natural evolution of the models seen
    here.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
