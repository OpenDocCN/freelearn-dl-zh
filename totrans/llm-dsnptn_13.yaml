- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll dive into **quantization** methods that can optimize
    LLMs for deployment on resource-constrained devices, such as mobile phones, embedded
    systems, or edge computing environments.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization is a technique that reduces the precision of numerical representations,
    thus shrinking the model’s size and improving its inference speed without heavily
    compromising its performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantization is particularly beneficial in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource-constrained deployment**: When deploying models on devices with
    limited memory, storage, or computational power, such as mobile phones, IoT devices,
    or edge computing platforms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency-sensitive applications**: When real-time or near-real-time responses
    are required, quantization can significantly reduce inference time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large-scale deployment**: When deploying models at scale, even modest reductions
    in model size and inference time can translate to substantial cost savings in
    infrastructure and energy consumption'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bandwidth-limited scenarios**: When models need to be downloaded to devices
    over limited bandwidth connections, smaller quantized models reduce transmission
    time and data usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models with redundant precision**: When many LLMs are trained with higher
    precision than necessary for good performance, they become excellent candidates
    for quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, quantization may not be suitable in some cases, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Highly precision-sensitive tasks**: For applications where even minor degradation
    in accuracy is unacceptable, such as certain medical diagnostics or critical financial
    models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models already optimized for low precision**: If a model was specifically
    designed or trained to operate efficiently at lower precisions, further quantization
    may cause significant performance drops'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small models**: For already compact models, the overhead of quantization
    operations might outweigh the benefits in some hardware configurations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development and fine-tuning phases**: During active development and experimentation,
    working with full-precision models is often preferable for maximum flexibility
    and to avoid masking potential issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware incompatibility**: Target hardware may lack efficient support for
    the specific quantized formats you’re planning to use (e.g., some devices may
    not have optimized INT8 or INT4 computation capabilities)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex architectures with varying sensitivity**: Some parts of an LLM architecture
    (such as attention mechanisms) may be more sensitive to quantization than others,
    requiring more sophisticated mixed-precision approaches rather than naive quantization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding these considerations, you can make informed decisions about
    whether and how to apply quantization techniques to your LLM deployments, balancing
    performance requirements against resource constraints.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about different quantization strategies, and
    by the end of this chapter, you’ll be able to apply quantization methods to make
    your LLMs more efficient, while ensuring that any reduction in precision has minimal
    impact on the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixed-precision quantization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware-specific considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing quantization strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining quantization with other optimization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantization refers to reducing the precision of the weights and activations
    of a model, typically from **32-bit floating point** (**FP32**) to lower precision
    formats such as **16-bit** (**FP16**) or even **8-bit integers** (**INT8**). The
    goal is to decrease memory usage, speed up computation, and make the model more
    deployable on hardware with limited computational capacity. While quantization
    can lead to performance degradation, carefully tuned quantization schemes usually
    result in only minor losses in accuracy, especially for LLMs with robust architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two primary quantization methods: **dynamic quantization** and **static
    quantization**.'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.quantization.quantize_dynamic` to dynamically quantize the linear layers
    of a pre-trained LLM:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This immediately reduces memory requirements and increases inference speed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`torch.quantization.prepare` and `torch.quantization.convert`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This statically quantized model uses fixed scale and zero-point parameters for
    each quantized tensor, allowing hardware accelerators to achieve higher inference
    efficiency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In contrast to dynamic quantization, static quantization requires a calibration
    phase with representative data before inference. During this phase, the model
    is run in evaluation mode to collect activation statistics, which are then used
    to compute quantization parameters. The weights and activations are then quantized
    ahead of time and remain fixed during inference, allowing for faster execution
    and more predictable performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are also two main quantization approaches based on when quantization
    is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Post-training quantization (PTQ)**: Applies quantization after the model
    has been fully trained, with minimal or no additional training. Can be implemented
    as either static (with calibration) or dynamic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantization-aware training (QAT)**: Simulates quantization effects during
    training by adding fake quantization operations in the forward pass while keeping
    gradients in full precision. Typically results in static quantization for deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PTQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PTQ is the most straightforward form of quantization and is applied after a
    model has been fully trained. It doesn’t require model retraining and works by
    converting the high-precision weights and activations into lower-precision formats,
    typically INT8\. PTQ is ideal for models where retraining is expensive or impractical,
    and it works best for tasks that are not overly sensitive to precision loss.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that some PTQ methods often require a calibration step on a representative
    dataset to determine optimal quantization parameters such as scaling factors and
    zero points, capture activation distributions during inference, and minimize the
    error between original and quantized model outputs. This calibration process helps
    the quantization algorithm understand the numerical range and distribution of
    weights and activations across the network, allowing more accurate mapping from
    higher precision formats (such as FP32) to lower precision formats (such as INT8
    or INT4), ultimately preserving model accuracy while reducing memory footprint
    and computational requirements for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example demonstrates static PTQ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The model is first put in evaluation mode using `.eval()`, then prepared for
    quantization using the `.prepare()` method, and finally converted into a quantized
    model. This method provides an efficient means of deploying LLMs on low-power
    devices with minimal overhead.
  prefs: []
  type: TYPE_NORMAL
- en: QAT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: QAT goes beyond simple PTQ by incorporating the effects of quantization into
    the training process itself. This allows the model to learn how to compensate
    for the quantization-induced noise, often resulting in better performance than
    PTQ, particularly for more complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: During QAT, both weights and activations are simulated at lower precision during
    training but are kept at higher precision for gradient calculations. This method
    is particularly useful when the application requires high performance with aggressive
    quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we configure the model for QAT using `get_default_qat_qconfig()`,
    which simulates the quantized behavior during the training phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once the model has been trained, it is converted to a quantized version suitable
    for deployment. QAT typically results in better model accuracy compared to PTQ,
    particularly for more complex or critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Mixed-precision quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mixed-precision quantization** is a more flexible approach that leverages
    multiple levels of numerical precision within a single model. For instance, less
    critical layers of the model can use INT8, while more sensitive layers remain
    in FP16 or FP32\. This allows greater control over the trade-off between performance
    and precision. Using mixed-precision quantization can significantly reduce model
    size and inference time while keeping critical aspects of the LLM intact.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates an example of quantization to optimize memory
    usage and speed in LLM training or inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use the `autocast()` function from PyTorch’s **Automatic
    Mixed Precision** (**AMP**) library to enable FP16 computation in parts of the
    model where precision is less critical, while FP32 is retained for more sensitive
    layers. This method helps reduce memory usage and inference time without severely
    affecting performance.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware-specific considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different hardware platforms—such as GPUs, CPUs, or specialized accelerators
    such as TPUs—can have vastly different capabilities and performance characteristics
    when it comes to handling quantized models. For instance, some hardware may natively
    support INT8 operations, while others are optimized for FP16.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the target deployment hardware is crucial for selecting the right
    quantization technique. For example, NVIDIA GPUs are well-suited to FP16 computations
    due to their support for mixed-precision training and inference, while CPUs often
    perform better with INT8 quantization because of hardware-accelerated integer
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: When deploying LLMs in production, it is important to experiment with quantization
    strategies tailored to your specific hardware and ensure that your model leverages
    the strengths of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing quantization strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When comparing different quantization strategies, each approach offers distinct
    advantages and challenges, which can be measured through factors such as implementation
    complexity, accuracy preservation, performance impact, and resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of implementation complexity, PTQ is the simplest to execute, requiring
    minimal additional work beyond the training of the original model. Dynamic quantization
    is more complex, as it involves more runtime considerations due to the dynamic
    handling of activations. Mixed-precision quantization introduces more complexity
    since it requires a granular, layer-by-layer assessment of precision sensitivity
    and potentially custom kernel development for optimized execution. QAT ranks as
    the most complex, requiring the integration of fake quantization nodes into the
    training graph and extended training times to account for the noise introduced
    by quantization.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to accuracy preservation, QAT performs the best, maintaining accuracy
    within a small margin of floating-point performance, especially when targeting
    aggressive quantization (sub-8-bit). Mixed-precision quantization also ranks high
    in accuracy retention since it allows critical layers to remain in higher precision,
    balancing performance and accuracy well. PTQ generally maintains accuracy within
    acceptable limits, though more complex architectures may suffer higher losses
    in precision. Dynamic quantization typically retains better accuracy than PTQ
    in RNN-based models, but struggles in CNN architectures, particularly when activations
    are sensitive to input distribution changes.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of resource requirements, PTQ demands the fewest resources, making
    it ideal for fast deployment scenarios with limited computational availability.
    Dynamic quantization ranks slightly higher in resource consumption because it
    handles activation quantization at runtime, though this is offset by the reduced
    burden on memory and storage. Mixed-precision quantization, while requiring more
    resources during implementation due to sensitivity analysis, can be efficient
    during inference, particularly on hardware that supports multiple precisions.
    QAT is the most resource-intensive, as it necessitates additional training time,
    higher memory usage during training, and more compute resources to adapt the model
    to quantization.
  prefs: []
  type: TYPE_NORMAL
- en: From a performance perspective, PTQ offers considerable improvements in memory
    savings and computational speedup, typically reducing storage by 75% and achieving
    2–4x acceleration on compatible hardware. However, QAT, while similar in compression
    ratio, adds overhead during training but compensates by producing models that
    can handle more aggressive quantization without significant performance loss.
    Dynamic quantization provides similar memory savings as PTQ, but its compute acceleration
    is generally lower due to runtime overhead. Mixed-precision quantization can offer
    near-floating-point performance, with speedups dependent on how efficiently the
    hardware can execute models with varying precision levels.
  prefs: []
  type: TYPE_NORMAL
- en: The decision framework for choosing the optimal quantization strategy hinges
    on specific project requirements. PTQ is appropriate when fast deployment is a
    priority, the model architecture is relatively simple, and slight accuracy loss
    is acceptable. QAT is the best choice when accuracy is paramount, retraining resources
    are available, and aggressive quantization is needed. Dynamic quantization fits
    scenarios that require runtime flexibility and the handling of varying input distributions,
    especially in RNN-based architectures. Mixed-precision quantization is optimal
    for complex models with varying precision needs, where both high accuracy and
    performance are required, and where the hardware can efficiently manage multiple
    precision formats.
  prefs: []
  type: TYPE_NORMAL
- en: Each quantization strategy serves a different purpose based on the trade-off
    between accuracy, complexity, performance, and resources, allowing users to tailor
    their approach to the specific needs of their deployment environment.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 13.1* compares each strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strategy** | **Accuracy** | **Complexity** | **Performance** | **Resources**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PTQ | Good for simple models; declines with complexity | Low; minimal setup
    | 75% storage reduction; 2–4x speedup | Low; minimal compute needed |'
  prefs: []
  type: TYPE_TB
- en: '| QAT | Highest; best for sub-8-bit | High; requires extended training | High
    compression with the best accuracy | High; intensive training needs |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamic | Good for RNNs; weak for CNNs | Medium; runtime overhead | Good
    memory savings; slower compute | Medium; runtime processing |'
  prefs: []
  type: TYPE_TB
- en: '| Mixed-Precision | High; flexible precision options | Medium-high; layer-specific
    tuning | Hardware-dependent speedup | Medium-high during setup |'
  prefs: []
  type: TYPE_TB
- en: Table 13.1 – Comparison of quantization strategies
  prefs: []
  type: TYPE_NORMAL
- en: In practice, some scenarios may benefit from combining strategies. For example,
    you might initially apply PTQ to achieve quick deployment, then use QAT selectively
    on accuracy-sensitive layers. Another approach could involve using mixed-precision
    for specific layers while applying dynamic quantization for activations to balance
    runtime flexibility and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Combining quantization with other optimization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantization can be combined with other optimization techniques, such as pruning
    and knowledge distillation, to create highly efficient models that are suitable
    for deployment on resource-constrained devices. By leveraging multiple methods,
    you can significantly reduce model size while maintaining or minimally impacting
    performance. This is especially useful when deploying LLMs on edge devices or
    mobile platforms where computational and memory resources are limited.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning and quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most effective combinations is **pruning** followed by quantization.
    First, pruning removes redundant weights from the model, reducing the number of
    parameters. Quantization then reduces the precision of the remaining weights,
    which further decreases the model size and improves inference speed. Here’s an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, pruning is applied to remove 50% of the weights in all linear
    layers, and dynamic quantization reduces the precision of the remaining weights
    to INT8 for further size reduction.
  prefs: []
  type: TYPE_NORMAL
- en: The result is a compact, highly optimized model that consumes fewer computational
    resources, making it suitable for deployment on devices with limited hardware
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation and quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another powerful combination is knowledge distillation followed by quantization.
    In this scenario, a smaller student model is trained to replicate the behavior
    of a larger teacher model. Once the student model is trained, quantization is
    applied to further optimize the student model for deployment. This combination
    is particularly useful when you need to maintain high performance with minimal
    computational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define teacher and student models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a knowledge distillation loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a training loop for knowledge distillation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Forward pass through the teacher and student models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The forward pass through both the teacher and student models generates their
    respective output logits for the same input data. This parallel inference step
    is necessary to compute the distillation loss, which quantifies how closely the
    student replicates the teacher’s behavior. By comparing these outputs, the training
    process can guide the student to internalize the teacher’s knowledge without requiring
    the original labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the distillation loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Computing the distillation loss allows the student model to learn from the teacher
    by minimizing the discrepancy between their output distributions. This guides
    the student to approximate the behavior of the larger, more accurate teacher model
    while maintaining its own compact structure. By backpropagating this loss and
    updating the model parameters through optimization, the student progressively
    aligns its predictions with the teacher, leading to improved performance with
    reduced model complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Quantize the distilled student model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check size and efficiency improvements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Knowledge distillation is used to train a smaller student model that mimics
    the behavior of the larger teacher model, and quantization is applied to the student
    model, reducing the precision of its weights to further optimize it for deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This method helps maintain performance while drastically reducing the model’s
    size, making it well-suited for low-power or real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: By combining quantization with pruning and knowledge distillation, you can achieve
    highly optimized models that balance size, efficiency, and performance. These
    models are especially useful for deployment on edge devices or environments with
    stringent resource constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored different quantization techniques for optimizing
    LLMs, including PTQ, QAT, and mixed-precision quantization. We also covered hardware-specific
    considerations and methods for evaluating quantized models. By combining quantization
    with other optimization methods, such as pruning or knowledge distillation, LLMs
    can be made both efficient and powerful for real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into the process of evaluating LLMs, focusing
    on metrics for text generation, language understanding, and dialogue systems.
    Understanding these evaluation methods is key to ensuring your optimized models
    perform as expected across diverse tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Evaluation and Interpretation of Large Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we focus on methods for evaluating and interpreting LLMs to ensure
    that they meet performance expectations and align with the intended use cases.
    You will learn how to use evaluation metrics tailored to various NLP tasks and
    apply cross-validation techniques to reliably assess your models. We explore interpretability
    methods that allow you to understand the inner workings of LLMs, as well as techniques
    for identifying and addressing biases in their outputs. Adversarial robustness
    is another key area covered, helping you defend models against attacks. Additionally,
    we introduce Reinforcement Learning from Human Feedback (RLHF) as a powerful method
    for aligning LLMs with user preferences. By mastering these evaluation and interpretation
    techniques, you will be able to fine-tune your models to achieve transparency,
    fairness, and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B31249_14.xhtml#_idTextAnchor230), *Evaluation Metrics*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B31249_15.xhtml#_idTextAnchor247), *Cross-Validation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B31249_16.xhtml#_idTextAnchor265), *Interpretability*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B31249_17.xhtml#_idTextAnchor276), *Fairness and Bias Detection*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 18*](B31249_18.xhtml#_idTextAnchor286), *Adversarial Robustness*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 19*](B31249_19.xhtml#_idTextAnchor295), *Reinforcement Learning from
    Human Feedback*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
