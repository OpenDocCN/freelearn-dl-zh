- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating and Summarizing Text with Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore architecture patterns for generating and summarizing
    text with Amazon Bedrock. You will learn about applications of text generation
    and how text generation works with Amazon Bedrock. Then, we will use some prompt
    engineering techniques, including contextual prompting, and orchestration using
    LangChain. After, we will explore text summarization using small texts/files,
    summarizing large articles and books, and discover use cases and patterns for
    text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to understand and implement text
    generation and summarization with Amazon Bedrock in real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a secure serverless solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have access to an AWS account. If you don’t have
    one already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, you will need to install and configure the AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
    You will use this to access Amazon Bedrock FMs from your local machine. Since
    the majority of the code cells we will be executing are based in Python, setting
    up an AWS Python SDK (Boto3) ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html))
    would be beneficial at this point. You can carry out the Python setup in any way:
    install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or
    leverage Amazon SageMaker. If you’re using Jupyter Notebook with the AWS Python
    SDK to interact with Amazon Bedrock, make sure you run the following code cell
    in the notebook to import the essential libraries and create a Bedrock runtime
    client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There will be a charge associated with invoking and customizing the FMs of Amazon
    Bedrock. Please refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Generating text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text generation plays a crucial role in various sectors, from marketing and
    advertising to journalism and creative writing. The significance of this technique
    lies in its capacity to streamline content creation processes, boost productivity,
    and unlock new realms of creativity.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of text generation is its potential to save valuable
    time and resources. Traditional content creation methods can be time-consuming
    and labor-intensive, often requiring extensive research, writing, and editing
    efforts. But by using generative AI models, businesses and individuals can quickly
    produce initial drafts, outlines, or complete pieces of content, freeing up valuable
    time for other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, text generation empowers content creators to explore new narrative
    avenues and push the boundaries of their creativity. By providing a starting point
    or a framework, these tools can spark fresh ideas and facilitate the exploration
    of unconventional storytelling techniques or unique writing styles. This capability
    is particularly valuable in industries where originality and distinctiveness are
    highly prized, such as fiction writing, advertising campaigns, or brand storytelling
    initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to creative applications, text generation also holds immense potential
    in fields that demand high volumes of informative and factual content. For instance,
    news reporting, scientific publications, technical documentation, and text generation
    can aid in the rapid dissemination of accurate and up-to-date information. By
    leveraging vast data repositories and subject matter expertise, these tools can
    generate comprehensive reports, summaries, or articles, ensuring that relevant
    information is readily available to the intended audience.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, text generation offers exciting opportunities for personalization
    and customization. By analyzing user preferences, demographics, and contextual
    data, these tools can tailor content so that it resonates with specific target
    audiences, enhancing engagement and fostering stronger connections with readers
    or customers.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some real-world applications of text generation in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the applications of text generation are endless, here are a few examples
    to get you started:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating product descriptions**: Amazon Bedrock’s text generation capabilities
    can be leveraged to automate the creation of product descriptions for marketing
    teams. By inputting the product’s features, specifications, and key benefits,
    the FM can generate compelling and SEO-optimized descriptions that highlight the
    unique selling points of the product. This can significantly streamline the process
    of creating product descriptions, saving time and resources for marketing teams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated descriptions can be tailored to different target audiences, tone,
    and style preferences, ensuring a consistent and engaging brand voice across various
    channels. Additionally, the FM can be customized on existing product descriptions,
    allowing it to learn and mimic the desired writing style and formatting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Media articles and marketing campaigns generation**: Amazon Bedrock’s text
    generation capabilities can be utilized for creating high-quality content for
    media articles, blog posts, and marketing campaigns. By providing relevant information,
    data, and guidelines, the FM can generate well-structured and coherent articles
    that can be used for content marketing, thought leadership, or news dissemination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FM can be trained on existing content, enabling it to understand and mimic
    the tone, style, and formatting preferences of specific publications or brands.
    It can also generate attention-grabbing headlines, engaging introductions, and
    compelling **calls to action** (**CTAs**) for marketing campaigns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Personalized email and message composition**: Amazon Bedrock can be utilized
    to compose personalized emails, messages, and other written communications for
    customer outreach, marketing campaigns, or even internal communications. By leveraging
    customer data and preferences, the FM can generate highly tailored and engaging
    content, enhancing customer experience and increasing brand loyalty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**: Clinical documentation is a critical aspect of healthcare,
    but it can be time-consuming and prone to errors. Bedrock can assist healthcare
    professionals in streamlining the note-taking and documentation process by generating
    accurate and comprehensive clinical notes based on conversations or dictations
    during patient encounters. Amazon offers another service called *AWS HealthScribe*
    that’s powered by Amazon Bedrock and is specifically designed to do that. To learn
    more about AWS HealthScribe, go to [https://aws.amazon.com/healthscribe/](https://aws.amazon.com/healthscribe/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bedrock can be employed to generate personalized health and wellness recommendations
    tailored to an individual’s unique health profile, lifestyle, and preferences.
    By analyzing data from various sources, such as **electronic health records**
    (**EHRs**), wearable devices, and self-reported information, Bedrock can provide
    tailored recommendations for diet, exercise, stress management, and preventive
    care.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Text generation systems with Amazon Bedrock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have been following the previous chapters, you may have already tried
    generating text on Amazon Bedrock. But just as a reminder, a simple text generation
    system looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Simple text generation system](img/B22045_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Simple text generation system
  prefs: []
  type: TYPE_NORMAL
- en: 'You provide a prompt to the model and say something like `Compose an email
    to a customer support team`. Even if you don’t provide any context, the model
    will generate a sample email for you (as shown in *Figure 6**.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Generating an email](img/B22045_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Generating an email
  prefs: []
  type: TYPE_NORMAL
- en: 'In your Jupyter Notebook environment with the AWS Python SDK, run the following
    sample script to invoke the AI21 Jurassic model. Make sure you import the essential
    libraries and create the Bedrock runtime client first, as mentioned in the *Technical*
    *requirements* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, based on the model that you select, the response’s structure and output
    may vary. *Figure 6**.3* shows the response from the AI21 Jurassic model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – AI21 Jurassic output](img/B22045_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – AI21 Jurassic output
  prefs: []
  type: TYPE_NORMAL
- en: Here, we provided a simple prompt without providing any context or information.
    Now, let’s move on to the advanced architecture patterns of text generation and
    understand contextual prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Generating text using prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we looked at a pattern of text generation where we
    did not provide any context or information to the model. Let’s use some of the
    prompt engineering techniques we learned about in [*Chapter 3*](B22045_03.xhtml#_idTextAnchor053):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-shot contextual prompting**: Here, we will provide detailed context
    in the prompt in a zero-shot fashion:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the preceding code will generate a response similar to the one shown
    in *Figure 6**.4*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Zero-shot contextual prompt response](img/B22045_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Zero-shot contextual prompt response
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding scenario, we used the Amazon Bedrock API – `invoke_model` –
    and passed the prompt, configuration parameters, and model ID. If you want to
    learn more about the various Bedrock APIs that are available, you are encouraged
    to revisit [*Chapter 2*](B22045_02.xhtml#_idTextAnchor034).
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-shot contextual prompting**: Here, we will provide some examples in our
    prompt so that the model can start to generate reasonable continuations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we provided three examples in our prompt to tell the model how our response
    should look. Then, we invoked the model to generate a product description for
    `Sony A7 III Mirrorless Camera`. We received the response shown in *Figure 6**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Few-shot contextual prompting response](img/B22045_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Few-shot contextual prompting response
  prefs: []
  type: TYPE_NORMAL
- en: '`invoke_model` API in this case):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, we used `langchain_community.llms` library to import
    *Bedrock*. However, based on the updates from the LangChain community, it may
    be susceptible to change. For updated information on importing the LangChain package,
    please visit [https://python.langchain.com/v0.2/docs/integrations/platforms/](https://python.langchain.com/v0.2/docs/integrations/platforms/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.6 – Zero-shot prompting with LangChain](img/B22045_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Zero-shot prompting with LangChain
  prefs: []
  type: TYPE_NORMAL
- en: '**Contextual generation with LangChain**: Here, we will provide instructions
    and context in our prompts before sending them to the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this scenario, we used the LangChain implementation of Bedrock. We defined
    a prompt template for creating a product description and invoked the Anthropic
    Claude model to generate a product description of a smart home security camera.
    The prompt template is essentially a reusable template for constructing prompts.
    Within the prompt template, you can provide the context, input variables, task,
    and some few-shot examples for the model to reference. To learn more about prompt
    templates, go to [https://python.langchain.com/v0.2/docs/concepts/#prompt-templates](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following figure shows the response from providing the preceding code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Contextual generation with LangChain](img/B22045_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Contextual generation with LangChain
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at various text generation patterns, let’s look at how
    we can perform summarization using Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text summarization is a highly sought-after capability that holds immense value
    across diverse domains. It involves the intricate task of condensing lengthy text
    documents into concise and coherent summaries that capture the essence of the
    original content. These summaries aim to preserve the most salient information
    while omitting redundant or irrelevant details, thereby enabling efficient consumption
    and comprehension of extensive textual data.
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization finds applications in a wide range of sectors, from research
    and academia to journalism, business intelligence, and legal documentation. With
    the exponential growth of textual data generated daily, the need for effective
    summarization techniques has become increasingly paramount. Imagine sifting through
    voluminous reports, news articles, or legal documents – text summarization emerges
    as a powerful tool to distill the core information, saving time and cognitive
    effort for professionals and researchers alike.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some of the real-world applications of text summarization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content curation**: In today’s information-rich world, text summarization
    plays a pivotal role in curating and condensing vast amounts of data. This allows
    users to quickly grasp the essence of lengthy articles, reports, or online content
    without having to read every word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**News aggregation**: News aggregators and media platforms can leverage text
    summarization to provide concise summaries of breaking news stories, enabling
    users to stay informed about the latest developments without getting bogged down
    by extensive details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Research assistance**: Researchers and academics can benefit from text summarization
    techniques to quickly identify the most pertinent information from a vast corpus
    of literature, saving them valuable time and effort.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer service**: Text summarization can enhance customer service by automatically
    generating concise summaries of lengthy customer inquiries or feedback, allowing
    support agents to quickly comprehend the crux of the issue and provide timely
    responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal and financial domains**: In industries where accurate representation
    of original text is critical, such as legal or financial sectors, text summarization
    techniques can be employed to generate summaries of contracts, agreements, or
    reports, ensuring that key information is not overlooked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Email management**: Email clients or productivity tools can leverage text
    summarization to provide concise overviews of long email threads or conversations,
    helping users quickly grasp the key points without having to read through every
    message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meeting recap**: Text summarization can be applied to meeting transcripts
    or notes, generating succinct summaries that capture the most important discussions,
    decisions, and action items, enabling participants to quickly review and follow
    up on critical points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social media monitoring**: Businesses and organizations can utilize text
    summarization to analyze and summarize vast amounts of social media data, such
    as customer feedback, product reviews, or brand mentions, enabling them to stay
    informed about public sentiment and respond promptly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge extraction**: Text summarization techniques can be used to extract
    and summarize relevant knowledge from large datasets or knowledge bases, making
    it easier to access and leverage valuable information for various applications,
    such as decision-making or knowledge management systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Educational resources**: Text summarization can be applied to educational
    materials, such as textbooks or online courses, to generate concise summaries
    or study aids, helping students grasp key concepts and prepare for exams more
    efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the list of applications is endless and spans across every industry,
    let’s look at how summarization systems work with Amazon Bedrock. We will learn
    about two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Summarization of small files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarization of large files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By small files, we mean pieces of text that fit into the context length of the
    model. This could range from a couple of sentences to a few paragraphs. On the
    other hand, by large files, we mean large documents or book(s) worth of information
    that does not fit into the context length of the model. It is important to note
    that there is no one-size-fits-all that works across all models. Every model,
    including their different versions, might have a different context length. For
    example, Cohere Command R+ has a context length of 128K tokens, while Cohere Command
    Light has a context length of 4,000 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization of small files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Small files can include meeting notes, blog posts, news articles, email messages,
    and call transcripts. These files are then used as a context for the prompt and
    sent to the model. The prompt here could be as simple as `Summarize the content`.
    The model then processes the file and provides you with the summarized response.
    *Figure 6**.8* shows the process of small file summarization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Small file summarization](img/B22045_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Small file summarization
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an example of a news article from Yahoo! Finance. Since the
    news article fits into the context length of the model, we will use that as a
    context in the prompt, `Summarize the following news article`, and send it to
    the model. The model will then process the request and provide the summarized
    response, as shown in *Figure 6**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Summarization of a news article](img/B22045_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Summarization of a news article
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of ways to summarize small files in Bedrock. If you’re using
    the AWS Python SDK, you can pass the small file text into the prompt directly,
    as shown in the following code. However, if you would like to summarize a couple
    of paragraphs, you can utilize *prompt templates* to place the text dynamically
    within the prompts and use LangChain to invoke the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is shown in *Figure 6**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Small file summarization response](img/B22045_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Small file summarization response
  prefs: []
  type: TYPE_NORMAL
- en: We have parsed a news article ([https://finance.yahoo.com/news/us-174-time-put-amazon-110026932.html](https://finance.yahoo.com/news/us-174-time-put-amazon-110026932.html))
    from Yahoo! Finance as a sample context to the prompt and invoked the Titan text
    model to generate the summarized response, as shown in the preceding figure.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the techniques for summarizing large files.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization of large files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large files can include large documents or book(s) worth of information that
    do not fit into the context length of the model. When we say large documents,
    this includes 10-K reports, **Federal Open Market Committee** (**FOMC**) reports,
    public health reports, clinical study reports, e-magazines, service documentation,
    and more. The 10-K report for Amazon is an example of a large file: [https://www.sec.gov/Archives/edgar/data/1018724/000101872424000008/amzn-20231231.htm](https://www.sec.gov/Archives/edgar/data/1018724/000101872424000008/amzn-20231231.htm).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with large files for summarizing text, several challenges are
    involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context length limitations**: All FMs, such as the ones used in Amazon Bedrock,
    have a maximum context length or input size that they can process at once. This
    limit varies from model to model, but it is typically in the range of a few thousand
    tokens (words or word pieces). For example, you can find FMs such as the Anthropic
    Claude 3 family with 200k tokens. When working with large documents that exceed
    this context length, it becomes impossible to summarize the entire document accurately
    and coherently. The model may miss important information or fail to capture the
    overall context and nuances present in the original text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hallucinations**: Hallucination is a phenomenon where models generate output
    that is not grounded in the input data or contains factual inconsistencies. This
    issue can become more prevalent when dealing with large documents as the model
    might struggle to maintain coherence and faithfulness to the original text. As
    the input size increases, the model may start generating plausible-sounding but
    factually incorrect information, potentially leading to inaccurate summaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory and computational constraints**: Summarizing large documents can be
    computationally intensive and may require significant memory resources. Generative
    AI models need to process and store the entire input text, as well as intermediate
    representations and generated outputs. When working with very large documents,
    you might experience performance degradation due to the high computational demands
    if they’re not handled with dedicated compute capacity (see the *Provisioned throughput
    architecture* section in [*Chapter 12*](B22045_12.xhtml#_idTextAnchor226)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context understanding**: Large documents often contain complex structures,
    such as sections, subsections, and cross-references. Generative AI models may
    struggle to accurately capture and understand the relationships and dependencies
    between different parts of the document. This can lead to summaries that lack
    coherence or fail to accurately represent the overall structure and flow of the
    original content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic drift and coherence**: As the length of the input text increases, it
    becomes more challenging for the models to maintain focus and coherence throughout
    the summarization process. The model may drift away from the main topic or fail
    to properly connect and transition between different aspects of the document,
    resulting in summaries that lack cohesion or clarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address these challenges, let’s look at summarizing large files using LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization using LangChain’s summarization chain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using LangChain, we are going to break down large files into smaller, more
    manageable chunks and process them sequentially. *Figure 6**.11* shows the architecture
    of large text summarization using LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Large file summarization in LangChain](img/B22045_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Large file summarization in LangChain
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how the process works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion**: The first step in the process is to load a large document
    or file into the system. This involves loading the file from an Amazon S3 bucket
    or downloading it directly from the internet. The files you can provide can be
    in the form of text, PDF, Word documents, and more.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`RecursiveCharacterTextSplitter` is recommended for general text, as per the
    LangChain documentation: [https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It recursively splits the text into smaller chunks until each chunk’s size falls
    below a specified threshold. The splitting process leverages separators (`"\n\n"`,
    `"\n"`), which ensures that individual paragraphs remain intact within a single
    chunk, rather than being fragmented across multiple chunks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stuff`: As the name suggests, this chain stuffs all the chunks into a single
    prompt.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`map_reduce`: The map-reduce chain is a powerful pattern that allows you to
    split a large task into smaller subtasks, process them independently, and then
    combine the results. In the context of text summarization, this chain type is
    used to break down a long text document into smaller chunks, summarize each chunk
    independently using an LLM, and then combine the summaries into a final summarized
    output.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`refine`: This chain starts by summarizing the first chunk. Then, `refine`
    takes this summary and combines it with the second chunk to generate a new summary
    that encompasses both pieces of information. This process continues, where the
    latest summary is combined with the next chunk, and a new summary is generated.
    This iterative approach repeats until all the chunks have been incorporated into
    the final summary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To load any of these summarization chains, you can call `load_summarize_chain`
    and provide the chain type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '**Final summary**: Based on the summarization chain you select and once all
    the chunks have been processed, the final summary represents a condensed version
    of the entire original document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The notebook at [https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/00_Langchain_TextGeneration_examples/05_long-text-summarization-titan%20Langchain.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/00_Langchain_TextGeneration_examples/05_long-text-summarization-titan%20Langchain.ipynb)
    showcases the use of long text summarization using LangChain. In this example,
    it uses `map_reduce` as the chain type. We recommend that you try out different
    chain types and provide any blog posts, files, or news articles as a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve summarized large files using the LangChain chain type, let’s
    say we want to summarize a whole book or multiple books’ worth of information.
    In such scenarios, where large manuscripts or books need to be summarized, the
    RAG approach can be potentially beneficial. However, please note that the summarized
    response might not contain some essential elements from the book – in other words,
    there could be information loss. Various advanced RAG techniques, such as query
    refinement, can be utilized to retrieve the summarized response and essential
    elements from the text. To learn more about query refinement for RAG, please take
    a look at the paper *RQ-RAG: Learning to Refine Queries for Retrieval Augmented*
    *Generation* ([https://arxiv.org/html/2404.00610v1](https://arxiv.org/html/2404.00610v1)).'
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about how RAG works and some advanced RAG techniques, please refer
    to [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at text summarization via Amazon Bedrock Knowledge Base.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock Knowledge Base
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090), we looked at how Amazon
    Bedrock Knowledge Base works and how to set it up. Let’s see an example of summarization
    using Knowledge Base.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have put the *Attention is All You Need* research paper in our data store
    Amazon S3 bucket and synced it with our Bedrock Knowledge base, as shown in *Figure
    6**.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Knowledge Base data source](img/B22045_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Knowledge Base data source
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the model and provide a prompt to summarize the content. You will see
    the response from the LLM, as shown in *Figure 6**.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Test Knowledge base](img/B22045_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Test Knowledge base
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to try this via APIs, you can call the **Retrieve** API or
    the **RetrieveAndGenerate** API. The Retrieve API accesses and retrieves the relevant
    data from Knowledge Base, whereas the RetrieveAndGenerate API, in addition to
    retrieving the data, generates the response based on the retrieved results. For
    more details on Amazon Bedrock Knowledge Base, please refer to [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how to utilize text summarization systems with
    Amazon Bedrock. Summarizing small files is straightforward and involves utilizing
    the model’s context length. However, summarizing large files requires chunking
    and specialized techniques such as LangChain’s summarization chains, RAG, or Amazon
    Bedrock Knowledge Base to handle context length limitations, hallucination, computational
    constraints, and coherence issues.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at generating and summarizing text using Amazon Bedrock,
    let’s look at how organizations can use these techniques and create a secure serverless
    solution involving other AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a secure serverless solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with Generative AI models from Amazon Bedrock, organizations can
    develop an application that is secure and serverless. Instead of interacting directly
    with Amazon Bedrock using SDKs, they can have an interactive chatbot that abstracts
    away any complexity, provides a rich customer experience, and boosts overall productivity.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.14* shows an architecture diagram of how the user can interact
    with the web-based chatbot developed via AWS Amplify and have conversations, generate
    text in various forms, and perform language translation, text summarization, and
    more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Serverless enterprise application with Amazon Bedrock](img/B22045_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Serverless enterprise application with Amazon Bedrock
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User interaction with the chatbot on AWS Amplify**: AWS Amplify is a comprehensive
    set of tools and services that simplify the development and deployment of full-stack
    cloud-powered web and mobile applications. The user initiates the workflow by
    interacting with a chatbot integrated into a web application developed using AWS
    Amplify.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**User authentication and authorization with Amazon Cognito**: Amazon Cognito
    is a robust user identity management service provided by AWS. When the user interacts
    with the chatbot, AWS communicates with Amazon Cognito to perform user authentication
    and authorization. Amazon Cognito supports various authentication methods, including
    traditional username/password combinations, social identity providers (for example,
    Google or Facebook), and multi-factor authentication. It also provides features
    for user registration, account recovery, and secure storage of user data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**API Gateway as a centralized entry point**: Once the user has been authenticated
    and authorized, their request is routed through an API gateway, which acts as
    a centralized entry point for APIs. API Gateway is a fully managed service that
    simplifies the process of creating, publishing, maintaining, monitoring, and securing
    APIs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`/text`) to an AWS Lambda function that performs invocation calls to Amazon
    Bedrock LLMs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This Lambda function will take the user’s input or prompt and pass it to Amazon
    Bedrock LLMs to generate relevant and coherent text. For example, the user can
    ask to generate an email or prepare a travel itinerary for a particular destination.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the Amazon Bedrock LLMs have generated the requested text, the Lambda function
    receives the response and sends it back to the user through API Gateway. Here,
    API Gateway acts as an intermediary, facilitating the communication between the
    client (that is, the chatbot) and the backend services (Lambda functions and Amazon
    Bedrock LLMs).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`/summarize`) to another AWS Lambda function specifically designed to perform
    invocation calls to Amazon Bedrock LLMs for summarization tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This Lambda function performs invocation calls to Amazon Bedrock LLMs to summarize
    text based on the user’s input or prompt and the provided context (small or large
    files).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After the Amazon Bedrock LLM has generated the summarized text, the Lambda function
    receives the response and sends it back to the user via API Gateway.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By separating the text generation and summarization tasks into different Lambda
    functions and API Gateway routes, the application can efficiently handle different
    types of requests and leverage the specialized capabilities of Amazon Bedrock
    LLMs for each task.
  prefs: []
  type: TYPE_NORMAL
- en: This workflow highlights the flexibility and modular nature of AWS services,
    allowing multiple components to be integrated to build complex applications. AWS
    Lambda functions act as computational engines that make invocation calls to Amazon
    Bedrock LLMs to perform text generation and summarization.
  prefs: []
  type: TYPE_NORMAL
- en: By breaking down the application into smaller, independent components, developers
    can easily maintain, update, and scale individual parts of the system without
    affecting the entire application.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re curious about trying out the serverless chatbot with Amazon Bedrock,
    check out [https://github.com/aws-samples/amazon-serverless-chatbot-using-bedrock](https://github.com/aws-samples/amazon-serverless-chatbot-using-bedrock).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should understand and be able to implement text generation
    and summarization with Amazon Bedrock in real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we dived into the architecture patterns for generating and
    summarizing text using Amazon Bedrock. The first part of this chapter covered
    text generation. We looked at the fundamentals of text generation through prompt
    engineering techniques, in-line context training, and orchestration with LangChain.
    Then, we explored various use cases and patterns for text generation that you
    can apply to real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of this chapter covered text summarization. We discussed both
    extractive and abstractive summarization approaches and their respective applications.
    Furthermore, we examined the systems and techniques that can be employed for text
    summarization using Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore building question-answering and conversational
    interfaces.
  prefs: []
  type: TYPE_NORMAL
