<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-44"><a id="_idTextAnchor043"/>3</h1>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Conceptualizing the Practical Robot Design Process</h1>
<p>This chapter represents a <em class="italic">bridge</em> between the preceding chapters on general theory, introduction, and setup, and the following chapters, where we will apply problem-solving methods that use <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) techniques to robotics. The first step is to clearly state our problem, from the perspective of the use of the robot, which is different from our view as the designer/builder of the robot. Then, we need to decide how to approach each of the hardware- and software-based challenges that we and the robot will attempt. By the end of this chapter, you will be able to understand the process of how to design a robot systematically.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>A systems engineering-based approach to robotics</li>
<li>Understanding our task – cleaning up the playroom</li>
<li>How to state the problem with the help of use cases</li>
<li>How to approach solving problems with storyboards</li>
<li><a id="_idTextAnchor045"/>Understanding the scope of our use case</li>
<li>Identifying our hardware needs</li>
<li>Breaking down our software needs</li>
<li>Writing a specification</li>
</ul>
<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/>A systems engineering-based approach to robotics</h1>
<p>When you set out to create a complex robot with AI-based software, you can’t just jump in and start slinging code and throwing things together without some sort of game plan as to how the robot goes together and how all the parts communicate with one another. We will discuss a systematic approach to robot design based on <strong class="bold">systems engineering</strong> principles. We will be learning about use cases and will use storyboards as techniques to understand what we are building and what parts – hardware and software – are needed.</p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/>Understanding our task – cleaning up the playroom</h1>
<p>We have already talked a bit about our main task for Albert, our example robot for this book, which is to clean up the playroom in my house after my grandchildren come to visit. We need to provide a more formal definition of our problem, and then turn that into a list of tasks for the robot to perform along with a plan of action on how we might accomplish those tasks.</p>
<p>Why are we doing this? Well, consider this quote by Steve Maraboli:</p>
<p class="uthor-quote">“If you don’t know where you are going, how do you know when you get there?”</p>
<div><div><img alt="Figure 3.1 – It’s important to know what your robot does" src="img/B19846_03_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – It’s important to know what your robot does</p>
<p>The internet and various robot websites are littered with dozens of robots that share one fatal character flaw: the robot and its software were designed first and then they went out to look for a job for it. In the robot business, this is called the <strong class="bold">ready, fire, aim problem</strong>. The task, the customer, the purpose, the use, and the job of the robot comes first.<a id="_idTextAnchor048"/> Another way of saying this is: to create an effective tool, the first step is to decide what you do with it.</p>
<p>I could have <a id="_idIndexMarker236"/>written this book as a set of theories and exercises that would have worked well in a classroom setting, which would have introduced you to a whole lot of new tools you would not know how to apply. However, this chapter is here to provide you with tools and methods to provide a path from having a good idea to having a good robot, with as little misdirection, pain, suffering, tears, and torn-out hair as possible.</p>
<p class="callout-heading">Important note</p>
<p class="callout">You are on your own with burns; please be careful with the soldering iron.</p>
<p>The process we will use is straightforward:</p>
<ol>
<li>The first step is to look at the robot from the user’s perspective and then describe what it does. We will call these descriptions <strong class="bold">use cases</strong> – examples of how the robot will be used.</li>
<li>Next, we will break each use case down into <strong class="bold">storyboards</strong> (step-by-step illustrations), which can be word pictures or actual pictures. From the storyboards, we can extract tasks – a to-do list for our robot to accomplish.</li>
<li>The final step for this part of the process is to separate the to-do list into things we can do with software and things we will need hardware for. This will give us detailed information for designing our robot and its AI-based software. Keep in mind that one of the robot’s uses<a id="_idTextAnchor049"/> is to be a good example for this book.</li>
</ol>
<p>Let’s start by looking at use cases.<a id="_idTextAnchor050"/></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor051"/>Use cases</h1>
<p>Let’s begin <a id="_idIndexMarker237"/>our task with a statement of the problem<a id="_idTextAnchor052"/><a id="_idTextAnchor053"/>.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor054"/>Our robot’s task – part 1</h2>
<p>About once or twice a month, my five delightful, intelligent, and playful grandchildren come to <a id="_idIndexMarker238"/>visit me and my wife. Like most grandparents, we keep a box full of toys in our upstairs playroom for them to play with during their visits. The first thing they do upon arrival – at least the older grandkids– is take every single toy out of the toy box and start playing. This results in the scene shown in the following photograph – toys randomly and uniformly distributed throughout the playro<a id="_idTextAnchor055"/>om:</p>
<div><div><img alt="Figure 3.2 – The playroom in the aftermath of the grandchildren" src="img/B19846_03_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The playroom in the aftermath of the grandchildren</p>
<p>Honestly, you could not get a better random distribution. They are really good at this. Since, as grandparents, our desire is to maximize the amount of time that our grandchildren have fun at our house and we want them to associate Granddad and Grandmother’s house with having fun, we don’t make them pick up the toys when they go home. You can see where this is heading.</p>
<p>By the way, if you are a parent, let me apologize to you in advance; this is indeed an evil plot on our, the grandparents, part, and you’ll understand when you get grandkids of your own – and you will do this, too.</p>
<p>Where were we...? Yes, a room full of randomly and uniformly distributed foreign objects – toys – scattered <a id="_idIndexMarker239"/>about an otherwise serviceable playroom, which need to be removed. Normally, I’d just have to sigh heavily and pick up all this stuff myself, but I am a robot designer, so what I want to do is to make a robot that does the following:</p>
<ol>
<li>Pick up the toys – and not the furniture, lights, books, speakers, or other items in the room that are not toys.</li>
<li>Put them in the toy box.</li>
<li>Continue to do this until there are no more toys to be found and then stop.</li>
</ol>
<p>Here is a visual representation of this process:</p>
<p class="IMG---Figure"><a id="_idTextAnchor056"/></p>
<div><div><img alt="Figure 3.3 – Use case: pick up toys" src="img/B19846_03_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Use case: pick up toys</p>
<p>Now we can ask some pertinent questions. I took journalism classes in school and I was taught the usefulness of the <em class="italic">5 Ws and an H</em> – <em class="italic">Who</em>, <em class="italic">What</em>, <em class="italic">When</em>, <em class="italic">Where</em>, <em class="italic">Why</em>, and <em class="italic">How</em>. These are just as useful for examining use cases. I’ve got one firm rule here in this section: no implementation details. Don’t worry about how you are going to do this. Just worry about <a id="_idIndexMarker240"/>defining the results. So we’ll leave out the <em class="italic">H</em> for now and focus on the <em class="italic">Ws</em>. Let’s give this<a id="_idTextAnchor057"/> a tr<a id="_idTextAnchor058"/>y:</p>
<ul>
<li><strong class="bold">Who</strong>: The robot. That was easy. We want the robot to do something, as in the robot does this and not m<a id="_idTextAnchor059"/>e. What do we want the robot to d<a id="_idTextAnchor060"/>o?</li>
<li><strong class="bold">What</strong>: This question can be answered in two ways:<ul><li><strong class="bold">Pick up toys and put them in the toy box</strong>: What does this answer tell us? It says we are going to be grasping and lifting something – toys. What are toys? We could also rephrase this as a negative, which brings us to the second answe<a id="_idTextAnchor061"/><a id="_idTextAnchor062"/><a id="_idTextAnchor063"/>r.</li><li><strong class="bold">Pick up and put away in the toy box the items that were not previously in the room</strong>: The toys were not in the room before the grandkids pulled them all out. So we either want to classify items as toys or as things that were not in the room before. <em class="italic">Not in the room</em> implies that the robot somehow knows what belongs in the room, possibly by making a survey prior to the children’s arrival. However, <em class="italic">toys</em> implies that the robot can classify objects at least as <em class="italic">toys</em> and <em class="italic">not toys</em>. Let’s stick with that for now. We may have some items in the room that are not toys but are out of place, and thus don’t belong in the toy box. Y<a id="_idTextAnchor064"/>ou can already see these questions shaping what comes later in this proces<a id="_idTextAnchor065"/>s.</li></ul></li>
<li><strong class="bold">When</strong>: After the grandchildren have visited and they have left, continue to pick up toys until there are none left.<p class="list-inset">That gives us two conditions for <em class="italic">when</em>: a start and a stop. In this case, the start is defined as the grandkids have visited and they have left. Now, it is perfectly fair for me to state in the use case that I’ll tell the robot when these conditions are met, since that is not putting me out. I’ll be here, and I know when the room needs to be cleane<a id="_idTextAnchor066"/>d. Besides, I need to get the robot out and put it into the room. When not working, it stays on a bookshelf. So, let’s change our <em class="italic">when</em> statement to the following<a id="_idTextAnchor067"/>:</p><p class="list-inset"><em class="italic">When I (the user) tell you to, and don’t stop until there are no more toys to </em><em class="italic">be found.</em></p><p class="list-inset">Now, we could have decided that the robot needs to figure this out for itself and turn itself on after the grandchildren leave, but what is the return on investment for that? That would be a lot of work for not a lot of gain. The pain point for me, the user, is picking up toys, not deciding when to do it. This is a lot simpler.</p><p class="list-inset">Note that my <em class="italic">when</em> statement has a start and an end. Anyone who watched Mickey Mouse in the <em class="italic">Sorcerer’s Apprentice</em> segment of <em class="italic">Fantasia</em> understands that when <a id="_idIndexMarker241"/>you have a robot, telling it when to stop can be important. Another important concept is defining the end condition. I did not say <em class="italic">stop when all of the toys are picked up</em> because that would imply the robot needed to know all of the toys, either by sight or number. It is easier as a task definition to say <em class="italic">stop when you see no more toys</em> instead, which accomplishes what we want without adding additional requirements to our ro<a id="_idTextAnchor068"/>bot.</p><p class="list-inset">It is perfectly normal to have to revisit use cases as the robot designer understands more about the problem – sometimes you can be working hard to solve a problem that is not relevant to solving the user’s task. You can imagine some robot engineer in a team being given the task of <em class="italic">pick up all the toys</em> as meaning all toys ever invented, in all cultures, in all parts of the world! Then, you get a request for a $500,000 database software license and a server farm to house it. We just want to pick up the toys found in the playro<a id="_idTextAnchor069"/><a id="_idTextAnchor070"/>om.</p></li>
<li><strong class="bold">Where</strong>: The playroom upstairs. Now we have some tricky parts. The area to be cleaned is a specific area of the house, but it is not really bound by walls. And it is upstairs – there is a stairway going down in the playroom that we don’t want our robot tumbling down. How would you have known this? You won’t unless you ask these kinds of questions! The environment the robot operates in is just as important as what it does. In this case, let’s go back and ask the user. I’ll stick in a floor plan for you here to define what I mean by <em class="italic">playroom</em>. On the bright side, we don’t need to climb or descend stairs in this task. But we do need to look out for <a id="_idIndexMarker242"/>the staircase as a hazard:</li>
</ul>
<div><div><img alt="Figure 3.4 – The floor plan of my house, upstairs" src="img/B19846_03_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – The floor plan of my house, upstairs</p>
<ul>
<li><strong class="bold">Why</strong>: So why is the robot picking up toys? I’m tempted to just write “Because <em class="italic">someone</em> has to do it.” However, the answer is that I don’t want the grandkids to pick up toys so that they have the maximum time to play, and I don’t want to do it, either. So we are making a robot for this task. One maxim in the robot world is that proper tasks for robots are <em class="italic">dirty</em>, <em class="italic">dull</em>, or <em class="italic">dangerous</em>. This one definitely falls into the <em class="italic">dull</em> category.</li>
</ul>
<p>Our robot has more than one use case – it has more than one function to perfo<a id="_idTextAnchor071"/><a id="_idTextAnchor072"/><a id="_idTextAnchor073"/>rm.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor074"/>Our robot’s task – part 2</h2>
<p>The robot needs to interact with my grandchildren. Why is this important? As I told you in <a href="B19846_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, the grandchildren were introduced to some of my other robots, and the oldest grandkid, William, always tries to talk to the robots. I have three grandchildren who are on <a id="_idIndexMarker243"/>the autistic spectrum, so this is not an idle desire – I’ve read the research, such as <em class="italic">Robots for Autism</em> (<a href="https://www.robokind.com/">https://www.robokind.com/</a>), which states that robots can be helpful in such situations. While I’m not trying to do therapy, I’d like my robot to interact with my grandchildren verbally. I also have one specific request – the robot must be able to tell knock-knock jokes and respond to them, as they are a favorite of William. I want this robot to be verbally interactive.</p>
<p>So, here is a diagram of this use case:</p>
<div><div><img alt="Figure 3.5 – Use case: interact with people" src="img/B19846_03_5.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Use case: interact with people</p>
<p>So let’s go through the same exercise with this use case. We ask the pertinent questio<a id="_idTextAnchor075"/>ns: <em class="italic">who</em>, <em class="italic">what</em>, <em class="italic">when</em>, <em class="italic">where</em>, and <em class="italic">why</em>? Let’s break these do<a id="_idTextAnchor076"/>wn:</p>
<ul>
<li><strong class="bold">Who</strong>: The robot, the user (granddad), and the grandchildren.<p class="list-inset">In this case, user interaction is part of the task. Who are we interacting with? I need to be able to command the robot to begin to interact. Then, we want the robot to both talk to and listen to the childr<a id="_idTextAnchor077"/><a id="_idTextAnchor078"/><a id="_idTextAnchor079"/>en.</p></li>
<li><strong class="bold">What</strong>: Receive commands and verbally interact (hold a conversation) with children, which must include knock-knock jokes. We keep the two kinds of functions: receive commands from – let’s call me the <strong class="bold">robot controller</strong>, to make this more generic. The other function is to have a conversation with the children, including telling knock-knock jokes. We’ll define <em class="italic">conversation</em> further on in our breakdown. You can refer to <a href="B19846_06.xhtml#_idTextAnchor205"><em class="italic">Chapter 6</em></a> on using the robot as a <strong class="bold">digital assistant</strong>. We are <a id="_idIndexMarker244"/>going to use an open source digital assistant called <em class="italic">Mycroft</em> to act <a id="_idIndexMarker245"/>as a voice interface for the robot. We will add our own skills to the base Mycroft capability, which is actually quite versatile. The robot can get the weather, set timers, play music, look up information on Google (such as how many tablespoons in a quarter cup), and even <a id="_idIndexMarker246"/>tell you where the International Space Station is right now. But what it can’t do is tell knock-knock jokes – until now, as we are adding this feature to the robot. Fortunately for us, the knock-knock joke has a very structured form based on puns and a call-and-response format that goes like this:<p class="list-inset"><em class="italic">Robot</em>: Knock knock.</p><p class="list-inset"><em class="italic">Child</em>: Who’s there?</p><p class="list-inset"><em class="italic">Robot</em>: Lettuce.</p><p class="list-inset"><em class="italic">Child</em>: Lettuce who?</p><p class="list-inset"><em class="italic">Robot</em>: Lettuce (let us) in, we’re freezing out here!</p><p class="list-inset">I’ll leave diagramming the opposite form – responding to a knock-knock joke – <a id="_idTextAnchor080"/><a id="_idTextAnchor081"/><a id="_idTextAnchor082"/>to you.</p></li>
<li><strong class="bold">When</strong>: As requested by the robot controller, then when the child speaks to the robot.<p class="list-inset">I think this is fairly self-explanatory: the robot interacts when sent a command to do so. It then waits for someone to talk to it. One thing we can extrapolate from this information is that when we are picking up toys, we are not expecting the robot to talk – the two activiti<a id="_idTextAnchor083"/>es are exclusive. We only pick up toys after the kids are gone, ergo there is one to ta<a id="_idTextAnchor084"/>lk to.</p></li>
<li><strong class="bold">Where</strong>: In the playroom, within about six feet of the robot.<p class="list-inset">We have to set some limits on how far we can hear – there is a limit on how sensitive our <a id="_idIndexMarker247"/>microphone can be. I’m suggesting six feet as a maximum distance. We may revisit this later. When you come to a requirement like this, you can ask the customer <em class="italic">Why six feet?</em> They may say, <em class="italic">Well, that sounds like a reasonable distance</em>. You can then ask, <em class="italic">Well, if it was five feet, would that be a failure of this function?</em> And the user might respond, <em class="italic">No, but it would not be as comfortable</em>. You can continue to ask questions on distances until you get a feeling for the required distance (how far away to not fail), which might be three feet in this case (so that the child does not have to bend over the robot to be heard), and the desired distance, which is how far the user wants the function to work. These are im<a id="_idTextAnchor085"/>portant distinctions when we get around to testing. Where is the pass-fail line for this requirement?</p></li>
<li><strong class="bold">Why</strong>: Because my grandchildren want to talk to the robot, and have it respond (i.e., the users have specifically requested this feature).</li>
</ul>
<p>Now, let’s delve deeper into our robot’s<a id="_idTextAnchor086"/> tasks.</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor087"/>What is our robot to do?</h2>
<p>Now we are going to do some detailed analysis of what the robot needs to do by using the storyboard process. This works like this: We take each of our two tasks and break them down as completely as we can based on the answers to all of our <em class="italic">W</em> questions. Then we picturize each step. The pictures can be either a drawing or a word picture (a paragraph) describing what happens in that step. I like to start the decomposition process by describing the <a id="_idIndexMarker248"/>robot in terms of a <strong class="bold">state machine</strong>, which, for the first part of our problem, may be a good approach to understanding what is going on inside the robot at each step.</p>
<p>You are probably familiar with <strong class="bold">state machine diagrams</strong>, but just in case, a state machine diagram describes the robot’s behavior as a series of discrete states or sets of conditions that <a id="_idIndexMarker249"/>define what actions are available to the robot:</p>
<div><div><img alt="Figure 3.6 – Robot state machine﻿ diagram" src="img/B19846_03_6.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Robot state machine<a id="_idTextAnchor088"/> diagram</p>
<p>Our first state is simply <em class="italic">Off</em> – the robot has no power turned on.</p>
<p>Each state is <a id="_idIndexMarker250"/>an event (or events) that causes the state to change. These are called <strong class="bold">transitions</strong>. To get from the <em class="italic">Off</em> state to whatever is next, some event has to occur – such as the human operator turning on the power. We’ll call that transition event <em class="italic">Power applied</em>. Now what state are we in? There is some amount of time to get the computer booted and the programs loaded (<em class="italic">Initializing</em>). Once everything boots up and initializes, the robot will be ready to accept commands. Let’s call this state <em class="italic">Standby</em>. The robot is just sitting waiting for instructions. Now we want to start cleaning the room. I send a <em class="italic">Begin cleaning</em> command to the robot, which changes the state to – what? What do we need to happen next? We could define a state called <em class="italic">Cleaning</em>, but that would encompass a lot of complex functions and we would not learn much from that. We need the robot to look for toys using its camera. If it does not find a toy, it needs to drive forward a short distance – avoiding obstacles – and then look again. In practice, we should be able to look for toys while driving without pausing constantly. We will need to make the <em class="italic">Look for toys</em> function interrupt driving when it sees a toy.</p>
<p>If it does find a toy, then the robot needs to position itself so that the toy is within reach of the robot arm. In the state machine diagram, we already added the transition <em class="italic">Begin cleaning</em>, which changes the state from <em class="italic">Standby</em> to <em class="italic">Look for toys</em>. Now we can add two more transitions: one called <em class="italic">Toy = no</em> and one called <em class="italic">Toy = yes</em>. The <em class="italic">Toy = no</em> branch goes to a state called <em class="italic">Drive ahead</em>, where the robot moves forward – while avoiding obstacles – and then goes back to the <em class="italic">Look for toys</em> state and tries again to find a toy. We will <a id="_idIndexMarker251"/>need some sort of means to tell the software how often to look for toys. We could use a simple timer – so many seconds elapsed. Or we could use some sort of distance function based on whe<a id="_idTextAnchor089"/>el motion.</p>
<p>So, now we have found a toy, what do we do? We need to drive to the toy, which puts it in range of our robot arm. We try to grip the toy with the robot’s arm and hand. We may not be successful on the first try, in which case we want to try again. The loop transition, which is labeled <em class="italic">Grip unsuccessful</em>, says to go back and try again if you don’t succeed the first time. Where have I heard that before? You can see the same with <em class="italic">Pick up toy</em>. Why are there two parts? We need to first get a hold of the toy before we can lift it. So I thought it needed two states, since we may fail to get a grip – the toy falls out of the hand, separately from picking the toy up, where the toy is too heavy or awkward to lift.</p>
<p>OK, we found a toy and picked it up. What is next? We need to put it in the toy box. The next state is <em class="italic">Drive to toy box</em>. Don’t worry about <em class="italic">how</em> at this stage; this is just what we need to do. Later, we can further decompose this state into a more detailed version. We drive until we get to the event <em class="italic">Toy box found</em>. That means we see the toy box. Then we go to the <em class="italic">Position for drop</em> state, which moves the robot to a place where it can drop the toy in the box. The final state, <em class="italic">Drop toy</em>, is self-explanatory. We’ve dropped the toy, the robot has nothing in its gripper, and guess what? We start over by returning to the <em class="italic">Look for toys</em> state. If the robot decides that the drop was not successful (the toy is still in the gripper), then we have it try that step again, by repositioning the hand over the toy box and trying to drop the toy again by opening the hand. How do we know whether the gripper is empty? We try to close the grip and see what position the hand servo is in. If the gripper can close (go to a minimum state), then it is empty. If the toy falls outside the toy box (the robot misses the box entirely), then it is once again a toy on the floor, and will be treated in the normal manner – the robot will find it, pick it up, and try again.</p>
<p>This is all well and good, and our little robot goes around forever looking for toys, right? We’ve left out two important transitions. We need a <em class="italic">No more toys</em> event, and we need a way to get back to the <em class="italic">Off</em> state. Getting to <em class="italic">Off</em> is easy – the user turns off the power. I use the shorthand method of having a block labeled <em class="italic">Any state</em> since we can hit the off button at any time, no matter what else the robot is doing, and there is nothing the robot can or <a id="_idIndexMarker252"/>should do about it. It may be more proper to draw a line from each state back to <em class="italic">Off</em>, but that clutters the diagram, and this notation still gets the mean<a id="_idTextAnchor090"/>ing across. The new state machine diagram looks like this:</p>
<div><div><img alt="Figure 3.7 – New state machine diagram" src="img/B19846_03_7.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – New state machine diagram</p>
<p>Let’s take a minute and talk about the concept of <em class="italic">No more toys</em>. How do we define this? This may take some experimentation, but for now, we’ll say if we have not found a toy after 10 minutes of trying, then we are satisfied that there are no more toys to be found. Later, we can adjust that time as necessary. It is possible that 5 minutes is totally adequate for a room our size. Note that the <em class="italic">No more toys</em> event can only come from the <em class="italic">Look for toys</em> state, which should make sense.</p>
<p>We mentioned that the robot needs to avoid obstacles. But we don’t have a state called <em class="italic">Avoid obstacles</em>. Why is that? That is because several of the states include driving, and each of those includes avoiding obstacles. It would not be appropriate to have a state for avoiding obstacles, since it is not unique to one state. What we need is a separate state machine that describes the robot’s driving. As I mentioned in the <em class="italic">Introducing subsumption architecture</em> section in the last chapter, we can have more than one goal operational at a time.</p>
<p>The task of <a id="_idIndexMarker253"/>picking up toys is the mission, which is the overall goal of the robot. <em class="italic">Avoid obstacles</em> is a goal of the driving engine, the mid-level manager of our robot.</p>
<p>We’ve discussed our use cases and drawn a state machine diagram, so now let’s move on to the next step, which is to create our <a id="_idTextAnchor091"/><a id="_idTextAnchor092"/><a id="_idTextAnchor093"/>storyboards.</p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor094"/>Using storyboards</h1>
<p>In this section, we are going to decompose our use cases further in order to understand the various tasks <a id="_idIndexMarker254"/>our robot must undertake on our behalf in the course of its two missions. I’ve created some <strong class="bold">storyboards</strong> – quick little drawings – to illustrate each point.</p>
<p>The concept of storyboards is borrowed from the movie industry, where a comic-strip-like narration is used to translate words on a page in the script into a series of pictures or cartoons that convey additional information not found in the script, such as framing, context, movement, props, sets, and camera moves. The practice of storyboarding goes all the way back to silent movies and is still used today.</p>
<p>We can use storyboards in robotics design for the same reasons: to convey additional information not found in the words of the use cases. Storyboards should be simple, quick, and just convey enough information to help you understand what is going on.</p>
<p>Let’s get started. We are not going to create storyboards for <em class="italic">Power applied</em>, <em class="italic">Initializing</em>, or <em class="italic">Standby</em> because a storyboard is not really needed for those simple concepts. We wil<a id="_idTextAnchor095"/>l jump ahead to the <em class="italic">Begin cleaning</em> event in our st<a id="_idTextAnchor096"/>ate diagram.</p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor097"/>Storyboard – put away the toys</h2>
<p>When our story begins, what is the robot doing? It has been turned on, and is in a standby state waiting to be told what to do. How does it receive a command? A nice, hands-free <a id="_idIndexMarker255"/>way would be to receive a voice command to <em class="italic">begin cleaning</em>, or some similar words that mean the same thing.</p>
<p>The next step in our process after <em class="italic">Begin cleaning</em> is <em class="italic">Look for toys</em>. This storyboard frame is <em class="italic">what the robot sees</em> as it is commanded to start cleaning. It sees the room, which has three kinds of objects visible – that is, toys, things that are not toys (the ottoman and the fireplace), and the room itself, including the walls, and the floor:</p>
<div><div><img alt="Figure 3.8 – Waiting for a voice command to begin cleaning" src="img/B19846_03_8.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Waiting for a voice command to begin cleaning</p>
<p>We could select any sort of sensor to detect our toys and direct our robot. We could have a LiDAR, thermal, or sonar scanner. Let’s hypothesize that the best sensor tool for this task is a regular USB camera. We have control of the lighting, the toys are not particularly <a id="_idIndexMarker256"/>warmer or cooler than the surroundings, and we need enough information to identify objects by type. So, video it is. We will determine later exactly what kind of camera we need, so add that to our <em class="italic">t<a id="_idTextAnchor098"/>o-do list</em>.</p>
<div><div><img alt="Figure 3.9 – Look for toys" src="img/B19846_03_9.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Look for toys</p>
<p>Our next storyboard is to <em class="italic">look for toys</em>. We need to run some sort of algorithm or technique to classify objects by type. The results of that algorithm are to find the objects – separate them from the background of the floor – and then classify each object as a toy or not a toy. We don’t really care to have any more breakdown than that – we leave all <em class="italic">Not toy</em> objects alone, and pick up all <em class="italic">Toy</em> objects. Note that we draw circles around the objects that are toys, which is another way of saying that we must locate them in the camera frame.</p>
<p>So what does <a id="_idIndexMarker257"/>this simple picture tell us we did not know before? It tells us the following:</p>
<ul>
<li>We need to segment the camera image by objects</li>
<li>We need to locate the objects in the camera frame</li>
<li>We need to classify the objects as either <em class="italic">Toy</em> or <em class="italic">Not toy</em></li>
<li>We need to be able to store and remember this information</li>
</ul>
<p>We only can pick up and move one toy at a time – we only have one hand, and nobody said in the use cases that we need to pick up more than one at a time. So, we only care about one toy – and let’s arbitrarily say we pick up the closest one to the robot:</p>
<div><div><img alt="Figure 3.10 – Select nearest toy" src="img/B19846_03_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Select nearest toy</p>
<p>We might also say that it’s the toy that is easiest to get to, which might be a slightly different <a id="_idIndexMarker258"/>process than choosing the closest one. We set that toy to be the target for our next action, wh<a id="_idTextAnchor099"/>ich is what? If you said to drive to the toy, you would be correct. However, we must not just drive to the toy but put the robot’s body in a position to use the robot arm to grasp the toy. By the way, that means the robot arm must be able to reach the ground or very close to the ground, as we have some small toys.</p>
<p>Our robot must plan a route from its current position to a spot where it can attempt to pick up the toy. We set a target goal an arms-length away from the center of the toy:</p>
<div><div><img alt="Figure 3.11 – Plan route to target" src="img/B19846_03_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – Plan route to target</p>
<p>The robot needs <a id="_idIndexMarker259"/>to make sure that there are no obstacles en route. There are two ways of doing this. As illustrated, we can clear the path that the robot is traveling on by adding the width of the robot (plus a bit of extra) and see whether any obstacles are in that area, or we can add a border around obstacles and see whether our path goes into those boundaries. Regardless, we need to have a path free of obstacles:</p>
<div><div><img alt="Figure 3.12 – Look for obstacles on the route" src="img/B19846_03_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – Look for obstacles on the route</p>
<p>The robot <a id="_idIndexMarker260"/>determines for itself the proper alignment to prepare to pick up the toy:</p>
<div><div><img alt="Figure 3.13 – Position robot hand" src="img/B19846_03_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – Position robot hand</p>
<p>Now that the <a id="_idIndexMarker261"/>robot has completed its drive, the robot can move the robot hand to a position to pick up the toy. We need to put the robot hand over the center of mass of the toy, and then rotate the hand to match a narrow part of the toy so we can pick it up. One of our goals for this project is to not dictate how the robot does this, but rather to let it learn for itself. So, we can say for this storyboard panel that the robot uses its training and machine learning to use an appropriate hand pose to prepare to grasp the object. We can surmise that that includes lining the hand up:</p>
<div><div><img alt="Figure 3.14 – Pick up toy" src="img/B19846_03_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – Pick up toy</p>
<p>Probably <em class="italic">storyboard 6</em> is the hard part (<em class="italic">Figure 3</em><em class="italic">.13</em>), and in <em class="italic">storyboard 7</em>, the robot completes the <a id="_idIndexMarker262"/>grasp of the object and picks it up (<em class="italic">Figure 3</em><em class="italic">.14</em>). The robot has to be able to determine whether the pick-up was successful, and if not, try again. That was in the state machine diagram we did before. We have now picked up the toy. What’s next? Find the toy box!</p>
<div><div><img alt="Figure 3.15 – Find toy box" src="img/B19846_03_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – Find toy box</p>
<p>Now we need <a id="_idIndexMarker263"/>the robot to find the toy box. Again, we don’t care how at this point. We are still worried about <em class="italic">what</em> and not <em class="italic">how</em>. Somehow, the robot looks around and finds the toy box, which, in this case, is large, against the wall, and has a distinctive color. Regardless, the robot has to find the toy box on its own. The labels in the picture indicate that the robot can distinguish the toy box and that it considers all other objects it perceives as obstacles. We can see we don’t need to have the <em class="italic">Toy</em>/<em class="italic">Not toy</em> capability active at the same time, only the <em class="italic">Toy box</em>/<em class="italic">Not toy box</em> decision-making process. This does reduce some of the required processing and will make machine lear<a id="_idTextAnchor100"/>ning easier.</p>
<p>Now that we have found the toy box, we illustrate a slightly more complex task of navigating around an obstacle to get there. In this example, we show the purple outline of the robot’s base, compared to a red outline around the obstacle, which I labeled <em class="italic">Keep out zone</em>. This gives us more guidance on how to avoid obstacles:</p>
<div><div><img alt="Figure 3.16 – Plan path to toy box" src="img/B19846_03_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – Plan path to toy box</p>
<p>We want to <a id="_idIndexMarker264"/>keep the center of the robot out of the <em class="italic">Keep out zone</em>. We need to get close enough to the toy box to drop our toy into it:</p>
<div><div><img alt="Figure 3.17 – Align toy with box" src="img/B19846_03_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Align toy with box</p>
<p>In <em class="italic">storyboard 10</em>, we lift the toy high above the top of the toy box and position our toy to fall inside <a id="_idIndexMarker265"/>the toy box when we let go of it. Make a note that we have to have the toy lifted before the final few inches to the toy box. We put the robot hand over the top of the opening of the toy box, just as far forward as we can and in the middle of the toy box.</p>
<div><div><img alt="Figure 3.18 – Drop toy in box" src="img/B19846_03_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – Drop toy in box</p>
<p>Our final step in the toy saga is to open the robot hand and let the toy hopefully fall into the toy box. I predict that we will have to spend some trial and error time getting this right. We may <a id="_idIndexMarker266"/>have to tilt the open hand right and left to get the toy to drop. If the toy falls outside of the box, then it is not put away and we have to start all over and try to put it away again. We don’t need a new state for this because it returns to being a toy on the floor, and we already have a state for that.</p>
<p>I hope that you have seen in the storyboard process how this provides insight into visualizing the robot’s tasks. I would say the most important benefit is that it forces you to think about what the robot is doing and to break down each step into smaller and smaller parts. Don’t hesitate to take this storyboard and break an individual panel down into its own storyboard, if that is what you feel you<a id="_idTextAnchor101"/><a id="_idTextAnchor102"/><a id="_idTextAnchor103"/> need to do.</p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor104"/>Project goals</h2>
<p>Since this is an AI/machine learning project, we must add to our project goals not just putting away <a id="_idIndexMarker267"/>toys but also using machine learning, adaptive systems, neural networks, and other tools to provide a new approach to solving these sorts of problems. You may think, “<em class="italic">Why bother? You can do this better with a standard programming approach.</em>” I would say from experience that these problems are difficult to solve that way, and you can do your own research to see where companies, large and small, have tried to solve this sort of problem and failed – or at least not succeeded. This problem is not easily solved by any means, and using an AI-based approach has a far greater chance of success than standard programming techniques. Now, I’m not saying we are going to succeed beyond our wildest dreams at this task in this book, but our objective is to learn a whole lot along the way!</p>
<p>So, we pause at this point in defining our project to say that we are deliberately choosing to use artificial intelligence and machine learning as an approach to solving a problem that has proven to be difficult with other means.</p>
<p>Since we are going to be teaching the robot various tasks, it will be more effective if we can teleoperate the robot and drive it around like a radio-controlled car, in order to collect data and take pictures that we will use for object recognition later. We don’t need this for operations, we need this for training. We will add this required operation to our <em class="italic">to-do list.</em></p>
<p>In our next step, we are going to extract from all of our hard work the <strong class="bold">hardware</strong> and <strong class="bold">software</strong> tasks that our robot will have to accomplish. But before we do this, let’s pause for a moment to discuss a common mistake made in defining the scope of<a id="_idTextAnchor105"/> the use case.</p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor106"/>Understanding the scope of our use case</h1>
<p><strong class="bold">Desirements</strong> (a word made up by combining <em class="italic">desire</em> and <em class="italic">requirements</em>) are functions that would be <em class="italic">nice to have</em> but not strictly necessary. For example, if we decided to add flashing lights <a id="_idIndexMarker268"/>to the robot because it looks cool, that would be a desirement. You may want to have it, but it does not contribute to the mission of the robot or the task it needs to perform.</p>
<p>Another example would be if we added that the robot must operate in the dark. There is no reason for this in the current context, and nothing we’ve stated in the use cases said that the robot would operate in the dark – just in an indoor room. This would be an example of <strong class="bold">scope creep</strong>, or extending the operation conditions without a solid reason why. It’s important to work very hard to keep requirements and use cases to a minimum, and even to throw out use cases that are unnecessary or redundant. I might have added <a id="_idIndexMarker269"/>a requirement for sorting the toys by color, but sorting does not help with picking up the toys, and besides, I’ve only got one toy box. I might have added the task in the interest of education for you, my readers, but it does not help with that objective either, so color sorting is not included.</p>
<p>Now, let’s proceed to identifying our hardwar<a id="_idTextAnchor107"/>e requirements.</p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor108"/>Identifying our hardware needs</h1>
<p>Based on <a id="_idIndexMarker270"/>our storyboards, I extracted or derived the following hardware tasks:</p>
<ul>
<li>Drive the robot base</li>
<li>Carry the robot arm</li>
<li>Lift toys</li>
<li>Put toys in the toy box (arm length)</li>
<li>Sensors:<ul><li>Arm location</li><li>Hand sta<a id="_idTextAnchor109"/>tus (open/close)</li><li>Robot vision (camera) for obstacle avoidance</li></ul></li>
<li>Provide power for all systems:<ul><li>5V for Nvidia Nano</li><li>5V for Arduino</li><li>Arm power – 7.2V</li><li>Motor power – 7.2V</li></ul></li>
<li>Onboard computers:<ul><li>A computer that can receive commands remotely (Wi-Fi Nano):<ul><li>Runs ROS 2</li><li>Runs Python 3</li></ul></li><li>A computer that can interface with a camera</li><li>A computer that can control motors (Arduino)</li><li>An interface <a id="_idIndexMarker271"/>that can drive servo motors for the robot arm (servo controller)</li></ul></li>
</ul>
<p>Now, let’s take a look at the softw<a id="_idTextAnchor110"/><a id="_idTextAnchor111"/>are requirements.</p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor112"/>Breaking down our software needs</h1>
<p>This list of software tasks was composed by reviewing the state machine diagram, the use cases, and <a id="_idIndexMarker272"/>the storyboards. I’ve highlighted the steps that will require AI and will be covered in detail in the coming chapters:</p>
<ol>
<li><strong class="bold">Power on </strong><strong class="bold">self-test</strong> (<strong class="bold">POST</strong>):<ol><li class="upper-roman">Start <a id="_idIndexMarker273"/>up robot programs.</li><li class="upper-roman">Check that the Nano can talk to the Arduino and back.</li><li class="upper-roman">Try to establish communications with the control station.</li><li class="upper-roman">Report POST success or failure as appropriate and enter in the log.</li></ol></li>
<li>Receive commands via Wi-Fi for teleoperation:<ul><li>Drive motor base (right/left/forward/back)</li><li>Move hand up/down/right/left/in/out/twist</li><li>Record video or record pictures as image files</li></ul></li>
<li>Send telemetry via Wi-Fi.</li>
<li>Monitor progress.</li>
<li>Send video.</li>
<li>Navigate safely:<ul><li>Learn to avoid obstacles</li><li>Learn to not fall down stairs</li></ul></li>
<li>Find <a id="_idIndexMarker274"/>toys:<ul><li>Detect objects</li><li>Learn to classify objects (Toy/Not toy)</li><li>Determine which toy is closest</li></ul></li>
<li>Pick up toys:<ol><li class="upper-roman">Move to the position where the arm can reach the toy</li><li class="upper-roman">Devise a strategy for grasp</li><li class="upper-roman">Attempt grasp</li><li class="upper-roman">Determine whether grasping was successful</li><li class="upper-roman">If not, try again with a different strategy</li><li class="upper-roman">Reweight grasp technique score based on success</li></ol></li>
<li>Put toys in the toy box:<ul><li>Learn to identify the toy box</li><li>Find the toy box</li><li>Drive to the known toy box location using navigation</li><li>Move to the dump location:<ul><li>Avoid obstacles</li><li>Lift the toy above the toy box lid</li></ul></li><li>Drop the toy</li><li>Check to see whether the toy drop was successful</li><li>If not, rep<a id="_idTextAnchor113"/>osition and try again</li><li>If the toy misses the toy box, we treat it as a toy on the floor again</li></ul></li>
<li>Determine there are no more toys.</li>
<li>Stand <a id="_idIndexMarker275"/>by for instructions.</li>
<li>Teleoperate:<ul><li>Move base forward/backward/left/right</li><li>Move arm up/down/right/left</li><li>Move hand in/out/twist/open/close</li><li>Record video/take pictures</li></ul></li>
<li>Simulate personality:<ul><li>Talk</li><li>Listen/recognize words</li><li>Understand some commands</li><li>Tell knock-knock jokes</li><li>Understand knock-knock jokes</li></ul></li>
<li>Voice commands:<ul><li>Clean-up room</li><li>Put this away</li><li>Come here</li><li>Stop</li><li>Wait</li><li>Resume</li><li>Go home</li><li>Turn left/ right</li><li>Forward/ back</li><li>Hand up/hand down</li><li>Hand left/hand right</li><li>Open hand/close hand</li></ul></li>
</ol>
<p>In this list, where did I get <em class="italic">teleoperate</em>? We don’t remember discussing that in the use cases and <a id="_idIndexMarker276"/>storyboards. We are going to need to teach the robot to navigate and find toys, and for that, we need to move the robot around and take pictures. One easy way to do that is by driving the robot around with <strong class="bold">teleopera<a id="_idTextAnchor114"/><a id="_idTextAnchor115"/><a id="_idTextAnchor116"/>tions</strong> (remote control).</p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor117"/>Writing a specification</h1>
<p>Our next task is to write specifications for our various components. I’ll go through an example here <a id="_idIndexMarker277"/>that we must do as part of our toy-grasping robot project: we need to <em class="italic">select a camera</em>. Just any old camera will not do – we need one that meets our needs. But what are those needs? We need to write a camera specification so that when we are looking at cameras to buy, we can tell which one will do the job.</p>
<p>We’ve created our storyboard and our use cases, so we have the information we need to figure out what our camera needs to do. We can reverse engineer this process somewhat: let’s discuss what things make one camera different from another. First of all is the interface: this camera goes on board the robot, so it has to interface with the robot’s computer, which has USB, Ethernet, and a special camera bus. What other things about cameras do we care about? We certainly care about cost. We don’t want (or need) to use a $1,000 camera for our inexpensive robot. Cameras have resolution: the number of pixels in each image. That can vary from 320 x 240 to 4,000 x 2,000 (4K). Cameras also have a field of view, which is the number of angular degrees the camera can see. This can vary from 2.5 degrees (very narrow) to 180 degrees (very wide). There are also cameras that see in the dark or have various types of infrared sensitivity. Finally, there is size and weight; we need a small camera that fits on our robot.</p>
<p>This makes the parameters that we need to decide the following:</p>
<ul>
<li><strong class="bold">Field of view</strong>: [180 - &gt; 2.5]</li>
<li><strong class="bold">Resolution</strong>: [320 x 280 -&gt; 4,000 x 2,000]</li>
<li><strong class="bold">Cost</strong>: (low to high) – cheaper is better</li>
<li><strong class="bold">Sees in the </strong><strong class="bold">dark</strong>: Yes/no</li>
<li><strong class="bold">Size and weight</strong>: Smaller and lighter is much better; must fit on the robot</li>
<li><strong class="bold">Interface</strong>: USB, Ethernet, or camera bus; power &gt;11V</li>
</ul>
<p>The reason for listing these parameters like this is that we can now concentrate on those features <a id="_idIndexMarker278"/>that we can select, so we are not wasting time looking at other parameters that we don’t care about. Let’s see whether we can knock off some of the parameters:</p>
<ul>
<li>If we use USB as the <strong class="bold">interface</strong>, the power is provided by the connector, and we don’t need extra cables or routers. This is also the lowest cost method, so we choose USB as the interface.</li>
<li>We also don’t have any requirements in our use cases to <strong class="bold">see in the dark</strong>, so we don’t need a special infrared camera.</li>
<li>The next question is to determine the <strong class="bold">field of view</strong>. We need to see the entire area where the robot arm can move in as it picks up a toy. We also need enough field of view to see when we are dr<a id="_idTextAnchor118"/>iving to avoid obstacles. We can take some measurements from the robot, but we can quickly see that we mostly need to see close to the robot, and we can’t see past the tracks on either side. This sets the field of view required to be close to 90 degrees. More field of view than this is acceptable, less is not.</li>
<li>Our final problem is determining the <strong class="bold">resolution</strong> we need to perform our object recognition. For that, we need an additional data point – how many pixels do we need to recognize an object as a toy? That is what we will do with this camera – recognize toys and things that are not toys. We also have to pick a distance at which we can recognize the toy. We don’t have a firm requirement out of the use cases, so we have to make an educated guess. We know that our room is 17 feet long, and it has furniture in it. Let’s guess that we need 8 feet of distance. How do we know this is correct? We do a thought experiment. If we can identify a toy 8 feet away, can we accomplish our task? We can see the toy half a room away. That gives <a id="_idIndexMarker279"/>the robot plenty of space to go drive to the toy and it won’t spend much time looking for toys. As a check, if the robot had to be 4 feet away to recognize a toy, would that be unusable? The answer is probably not – the robot would work OK. How about 3 feet? Now we are getting to the point where the robot has to drive right up to the toy to determine what it is, and that might result in more complicated logic to examine toys. So, we say that 3 feet is not enough, 4 feet is acceptable, and 8 feet would be great.<p class="list-inset">What resolution is required in the camera to recognize a toy at 8 feet with a 90-degree lens? I can tell you that the ImageNet database requires a sample 35 pixels wide to recognize an object, so we can use that as a benchmark. We assume at this point that we need an image at least 35 pixels across. Let’s start with a camera with <em class="italic">1,024 x 768</em> pixels, which is 1,024 pixels wide. We divide by 90 degrees to get that each degree has 11.3 pixels (<em class="italic">1,024/90</em>). How big is our smallest toy at 8 feet? Our smallest toy is a Hot Wheels toy, which is approximately 3 inches long. At 8 feet, this is 1.79 degrees or 20.23 pixels (<em class="italic">1.79 degrees x 11.3 pixels/degree</em>). That is not enough. Solving the distance equation for 3 inches, we get a maximum distance of 4.77 feet for a camera with <em class="italic">1,024 x 768</em> pixels. That is just barely acceptable. What if we had an HD sensor with <em class="italic">1,900 x 1200</em> pixels? Then, at 8 feet, I get 75 pixels – more than enough to give us the best possible distance. If we use a sensor 1,200 pixels wide, we have a recognition distance of 5.46 feet, which is adequate but not great.</p></li>
</ul>
<p>I walked you through this process to show you how to write a specification and the types of questions you should be asking yourself as you decide what sensors to <a id="_idTextAnchor119"/>acquire for your project.</p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor120"/>Summary</h1>
<p>This chapter outlined a suggested process for developing your to-do list as you develop your robot project. This process is called systems engineering. Our first step was to create use cases or descriptions of how the robot is to behave from a user’s perspective. Then, we created more detail behind the use cases by creating storyboards, where we went step by step through the use case. Our example followed the robot finding and recognizing toys, before picking them up and putting them in the toy box. We extracted our hardware and software needs, creating a to-do list of what the robot will be able to do. Finally, we wrote a specification for one of our critical sensors: the camera.</p>
<p>In the next chapter, we will dive into our first robot task – teaching the robot to recognize toys using computer vi<a id="_idTextAnchor121"/>sion and neural networks.</p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor122"/>Questions</h1>
<ol>
<li>Describe some of the differences between a storyboard for a movie or cartoon and a storyboard for a software program.</li>
<li>What are the five <em class="italic">W</em> questions? Can you think of any more questions that would be relevant to examine a use case?</li>
<li>Complete this sentence: A use case shows what the robot does but not _____.</li>
<li>Take <em class="italic">storyboard 9</em> in <em class="italic">Figure 3</em><em class="italic">.16</em>, where the robot is driving to the toy box, and break it down into more sequenced steps in your own storyboard. Think about all that must happen between <em class="italic">frames 9 </em><em class="italic">and 10</em>.</li>
<li>Complete the reply form of the knock-knock joke, where the robot answers the user telling the joke. What do you think is the last step?</li>
<li>Look at the teleoperate operations. Would you add any more, or does this look like a good list?</li>
<li>Write a specification for a sensor that uses distance measurement to prevent the robot from driving downstairs.</li>
<li>What is the distance at which a camera with 320 x 200 pixels and a 30-degree field of view can see a 6-inch wide stuffed animal, still assuming we need 3<a id="_idTextAnchor123"/>5 pixels for recognition?</li>
</ol>
<h1 id="_idParaDest-61"><a id="_idTextAnchor124"/>Further reading</h1>
<p>For more information on the topics in this chapter, you can refer to the following resources:</p>
<ul>
<li><em class="italic">A Practical Guide to SysML: The Systems Modeling Language</em>, by Sanford Friedenthal, Alan Moore, and Rick Steiner, published by Morgan Kaufman; this is the standard introduction to <strong class="bold">Model-Based Systems </strong><strong class="bold">Engineering</strong> (<strong class="bold">MBSE</strong>)</li>
<li><em class="italic">The Agile Developer’s Handbook</em> by Paul Flewelling, published by Packt</li>
</ul>
</div>


<div><h1 id="_idParaDest-62" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor125"/>Part 2: Adding Perception, Learning, and Interaction to Robotics</h1>
<p>To see, understand, and interact with the environment, robots need to have perception. AI is one approach that can be used for recognizing objects and navigation. This part empowers you with the essential skills to efficiently operate your robots using AI techniques. Our example in this book is creating a robot that picks up toys, so we start with recognizing toys with a <strong class="bold">neural network</strong>. Then we work with the robot arm to pick up toys using tools such as <strong class="bold">reinforcement learning</strong> and <strong class="bold">genetic algorithms</strong>. The next chapter covers the creation of a robot digital assistant that can listen and understand your commands, and even tell knock-knock jokes.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B19846_04.xhtml#_idTextAnchor126"><em class="italic">Chapter 4</em></a>, <em class="italic">Recognizing Objects Using Neural Networks and Supervised Learning</em></li>
<li><a href="B19846_05.xhtml#_idTextAnchor159"><em class="italic">Chapter 5</em></a>, <em class="italic">Picking Up and Putting Away Toys Using Reinforcement Learning and Genetic Algorithms</em></li>
<li><a href="B19846_06.xhtml#_idTextAnchor205"><em class="italic">Chapter 6</em></a>, <em class="italic">Teaching the Robot to Listen</em></li>
</ul>
</div>
<div><div></div>
</div>
</body></html>