<html><head></head><body>
  <div><h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-149" class="chapterTitle">Scaling RAG Bank Customer Data with Pinecone</h1>
    <p class="normal">Scaling up RAG documents, whether text-based or multimodal, isn’t just about piling on and accumulating more data—it fundamentally changes how an application works. Firstly, scaling is about finding the right amount of data, not just more of it. Secondly, as you add more data, the demands on an application can change—it might need new features to handle the bigger load. Finally, cost monitoring and speed performance will constrain our projects when scaling. Hence, this chapter is designed to equip you with cutting-edge techniques for leveraging AI in solving the real-world scaling challenges you may face in your projects. For this, we will be building a recommendation system based on pattern-matching using Pinecone to minimize bank customer churn (customers choosing to leave a bank).</p>
    <p class="normal">We will start with a step-by-step approach to developing the first program of our pipeline. Here, you will learn how to download a Kaggle bank customer dataset and perform <strong class="keyWord">exploratory data analysis</strong> (<strong class="keyWord">EDA</strong>). This foundational step is crucial as it guides and supports you in preparing your dataset and your RAG strategy for the next stages of processing. The second program of our pipeline introduces you to the powerful combination of Pinecone—a vector database suited for handling large-scale vector search—and OpenAI’s <code class="inlineCode">text-embedding-3-small</code> model. Here, you’ll chunk and embed your data before upserting (updating or inserting records) it into a Pinecone index that we will scale up to 1,000,000+ vectors. We will ready it for complex query retrieval at a satisfactory speed. Finally, the third program of our pipeline will show you how to build RAG queries using Pinecone, augment user input, and leverage GPT-4o to generate AI-driven recommendations. The goal is to reduce churn in banking by offering personalized, insightful recommendations. By the end of this chapter, you’ll have a good understanding of how to apply the power of Pinecone and OpenAI technologies to your RAG projects.</p>
    <p class="normal">To sum up, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">The key aspects of scaling RAG vector stores</li>
      <li class="bulletList">EDA for data preparation</li>
      <li class="bulletList">Scaling with Pinecone vector storage</li>
      <li class="bulletList">Chunking strategy for customer bank information</li>
      <li class="bulletList">Embedding data with OpenAI embedding models</li>
      <li class="bulletList">Upserting data</li>
      <li class="bulletList">Using Pinecone for RAG</li>
      <li class="bulletList">Generative AI-driven recommendations with GPT-4o to reduce bank customer churn</li>
    </ul>
    <p class="normal">Let’s begin by defining how we will scale with Pinecone.</p>
    <h1 id="_idParaDest-150" class="heading-1">Scaling with Pinecone</h1>
    <p class="normal">We will be implementing Pinecone’s innovative vector database technology with OpenAI’s powerful <a id="_idIndexMarker358"/>embedding capabilities to construct data processing and querying systems. The goal is to build a recommendation system to <a id="_idIndexMarker359"/>encourage customers to continue their association with a bank. Once you understand this approach, you will be able to apply it to any domain requiring recommendations (leisure, medical, or legal). To understand and optimize the complex processes involved, we will build the programs from scratch with a minimal number of components. In this chapter, we will use the Pinecone vector database and the OpenAI LLM model.</p>
    <p class="normal">Selecting and designing an architecture depends on a project’s specific goals. Depending on your project’s needs, you can apply this methodology to other platforms. In this chapter and architecture, the combination of a vector store and a generative AI model is designed to streamline operations and facilitate scalability. With that context in place, let’s go through the architecture we will be building in Python.</p>
    <h2 id="_idParaDest-151" class="heading-2">Architecture</h2>
    <p class="normal">In this chapter, we will implement <a id="_idIndexMarker360"/>vector-based similarity search functionality, as we did in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>, and <em class="chapterRef">Chapter 3</em>, <em class="italic">Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI</em>. We will take the structure of the three pipelines we designed in those chapters and apply them to our recommendation system, as shown in <em class="italic">Figure 6.1</em>. If necessary, take the time to go through those chapters before implementing the code in this chapter.</p>
    <figure class="mediaobject"><img src="img/B31169_06_01.png" alt="A diagram of a process  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.1: Scaling RAG-driven generative AI pipelines</p>
    <p class="normal">The key features of the scaled recommendation system we will build can be summarized in the three<a id="_idIndexMarker361"/> pipelines shown in the preceding figure:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline 1: Collecting and preparing the dataset</strong></li>
    </ul>
    <p class="normal-one">In this pipeline, we will perform EDA on the dataset with standard queries and k-means clustering.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline 2: Scaling a Pinecone index (vector store)</strong></li>
    </ul>
    <p class="normal-one">In this pipeline, we will see how to chunk, embed, and upsert 1,000,000+ documents to a Pinecone index (vector store).</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline 3: RAG generative AI</strong></li>
    </ul>
    <p class="normal-one">This pipeline will take us to fully scaled RAG when we query a 1,000,000+ vector store and augment the input of a GPT-4o model to make targeted recommendations.</p>
    <p class="normal">The main theoretical and practical applications of the three programs we will explore include:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Scalable and serverless infrastructure</strong>: We begin by understanding Pinecone’s serverless architecture, which eliminates the complexities of server management and scaling. We don’t need to manage storage resources or machine usage. It’s a pay-as-you-go approach based on serverless indexes<a id="_idIndexMarker362"/> formed by a cloud and region, for example, <strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>) in <code class="inlineCode">us-east-1</code>. Scaling and billing are thus<a id="_idIndexMarker363"/> simplified, although we still have to monitor and minimize the costs!</li>
      <li class="bulletList"><strong class="keyWord">Lightweight and simplified development environment</strong>: Our integration strategy will minimize the use of external libraries, maintaining a lightweight development stack. Directly using OpenAI to generate embeddings and Pinecone to store and query these embeddings simplifies the data processing pipeline and increases system efficiency. Although this approach can prove effective, other methods are possible depending on your project, as implemented in other chapters of this book.</li>
      <li class="bulletList"><strong class="keyWord">Optimized scalability and performance</strong>: Pinecone’s vector database is engineered to handle large-scale datasets effectively, ensuring that application performance remains satisfactory as the data volume grows. As for all cloud platforms and APIs, examine the privacy and security constraints when implementing Pinecone and OpenAI. Also, continually monitor the system’s performance and costs, as we will see in the <em class="italic">Pipeline 2: Scaling a Pinecone index (vector store)</em> section of this chapter.</li>
    </ul>
    <p class="normal">Let’s now go to our keyboards to collect and process the <code class="inlineCode">Bank Customer Churn</code> dataset.</p>
    <h1 id="_idParaDest-152" class="heading-1">Pipeline 1: Collecting and preparing the dataset</h1>
    <p class="normal">This section will focus on handling and analyzing the <code class="inlineCode">Bank Customer Churn</code> dataset. We will guide you through the steps of <a id="_idIndexMarker364"/>setting up your environment, manipulating<a id="_idIndexMarker365"/> data, and applying <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) techniques. It is important to get the “feel” of a dataset with human analysis <a id="_idIndexMarker366"/>before using algorithms as tools. Human insights will always remain critical because of the flexibility of human creativity. As such, we will implement data collection and preparation in Python in three main steps:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Collecting and processing the dataset</strong>:<ul>
          <li class="bulletList level-2">Setting up the Kaggle environment to authenticate and download datasets</li>
          <li class="bulletList level-2">Collecting and unzipping the <code class="inlineCode">Bank Customer Churn</code> dataset</li>
          <li class="bulletList level-2">Simplifying the dataset by removing unnecessary columns</li>
        </ul>
      </li>
      <li class="numberedList"><strong class="keyWord">Exploratory data analysis</strong>:<ul>
          <li class="bulletList level-2">Performing initial data inspections to understand the structure and type of data we have</li>
          <li class="bulletList level-2">Investigating relationships between customer complaints and churn (closing accounts)</li>
          <li class="bulletList level-2">Exploring <a id="_idIndexMarker367"/>how age and salary<a id="_idIndexMarker368"/> levels relate to customer churn</li>
          <li class="bulletList level-2">Generating a heatmap to visualize correlations between numerical features</li>
        </ul>
      </li>
      <li class="numberedList"><strong class="keyWord">Training an ML model</strong>:<ul>
          <li class="bulletList level-2">Preparing the data for ML</li>
          <li class="bulletList level-2">Applying clustering techniques to discover patterns in customer behavior</li>
          <li class="bulletList level-2">Assessing the effectiveness of different cluster configurations</li>
          <li class="bulletList level-2">Concluding and moving on to RAG-driven generative AI</li>
        </ul>
      </li>
    </ol>
    <p class="normal">Our goal is to analyze the dataset and prepare it for <em class="italic">Pipeline 2: Scaling a Pinecone index (vector store)</em>. To achieve that goal, we need to perform a preliminary EDA of the dataset. Moreover, each section is designed to be a hands-on walkthrough of the code from scratch, ensuring you gain practical experience and insights into data science workflows. Let’s get started by collecting the dataset.</p>
    <h2 id="_idParaDest-153" class="heading-2">1. Collecting and processing the dataset</h2>
    <p class="normal">Let’s first collect the <code class="inlineCode">Bank </code><code class="inlineCode"><a id="_idIndexMarker369"/></code><code class="inlineCode">Customer Churn</code> dataset on Kaggle and <a id="_idIndexMarker370"/>process it:</p>
    <p class="normal"><a href="https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn">https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn</a></p>
    <p class="normal">The file <code class="inlineCode">Customer-Churn-Records.csv</code> contains data on 10,000 records of customers from a bank focusing on various aspects that might influence customer churn. The dataset was uploaded by Radheshyam Kollipara, who rightly states:</p>
    <p class="normal">As we know, it is much more expensive to sign in a new client than keeping an existing one. It is advantageous for banks to know what leads a client towards the decision to leave the company. Churn prevention allows companies to develop loyalty programs and retention campaigns to keep as many customers as possible.</p>
    <p class="normal">Here are the details of <a id="_idIndexMarker371"/>the columns included in the dataset that<a id="_idIndexMarker372"/> follow the description on Kaggle:</p>
    <p class="normal"><code class="inlineCode">RowNumber—corresponds to the record (row) number and has no effect on the output.</code></p>
    <p class="normal"><code class="inlineCode">CustomerId—contains random values and has no effect on customers leaving the bank.</code></p>
    <p class="normal"><code class="inlineCode">Surname—the surname of a customer has no impact on their decision to leave the bank.</code></p>
    <p class="normal"><code class="inlineCode">CreditScore—can have an effect on customer churn since a customer with a higher credit score is less likely to leave the bank.</code></p>
    <p class="normal"><code class="inlineCode">Geography—a customer's location can affect their decision to leave the bank.</code></p>
    <p class="normal"><code class="inlineCode">Gender—it's interesting to explore whether gender plays a role in a customer leaving the bank.</code></p>
    <p class="normal"><code class="inlineCode">Age—this is certainly relevant since older customers are less likely to leave their bank than younger ones.</code></p>
    <p class="normal"><code class="inlineCode">Tenure—refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.</code></p>
    <p class="normal"><code class="inlineCode">Balance—is also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.</code></p>
    <p class="normal"><code class="inlineCode">NumOfProducts—refers to the number of products that a customer has purchased through the bank.</code></p>
    <p class="normal"><code class="inlineCode">HasCrCard—denotes whether or not a customer has a credit card. This column is also relevant since people with a credit card are less likely to leave the bank.</code></p>
    <p class="normal"><code class="inlineCode">IsActiveMember—active customers are less likely to leave the bank.</code></p>
    <p class="normal"><code class="inlineCode">EstimatedSalary—as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.</code></p>
    <p class="normal"><code class="inlineCode">Exited—whether or not the customer left the bank.</code></p>
    <p class="normal"><code class="inlineCode">Complain—customer has complained or not.</code></p>
    <p class="normal"><code class="inlineCode">Satisfaction Score—Score provided by the customer for their complaint resolution.</code></p>
    <p class="normal"><code class="inlineCode">Card Type—the type of card held by the customer.</code></p>
    <p class="normal"><code class="inlineCode">Points Earned—the points earned by the customer for using a credit card.</code></p>
    <p class="normal">Now that we know <a id="_idIndexMarker373"/>what the dataset contains, we need to collect it<a id="_idIndexMarker374"/> and process it for EDA. Let’s install the environment.</p>
    <h3 id="_idParaDest-154" class="heading-3">Installing the environment for Kaggle</h3>
    <p class="normal">To collect datasets from Kaggle automatically, you will<a id="_idIndexMarker375"/> need to sign up and create an API key at <a href="https://www.kaggle.com/">https://www.kaggle.com/</a>. At the time of writing this notebook, downloading datasets is free. Follow the<a id="_idIndexMarker376"/> instructions to save and use your Kaggle API key. Store your key in a safe location. In this case, the key is in a file on Google Drive that we need to mount:</p>
    <pre class="programlisting code"><code class="hljs-code">#API Key
#Store your key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)
from google.colab import drive
drive.mount('/content/drive')
</code></pre>
    <p class="normal">The program now reads the JSON file and sets environment variables for Kaggle authentication using your username and an API key:</p>
    <pre class="programlisting code"><code class="hljs-code">import os
import json
with open(os.path.expanduser("drive/MyDrive/files/kaggle.json"), "r") as f:
    kaggle_credentials = json.load(f)
kaggle_username = kaggle_credentials["username"]
kaggle_key = kaggle_credentials["key"]
os.environ["KAGGLE_USERNAME"] = kaggle_username
os.environ["KAGGLE_KEY"] = kaggle_key
</code></pre>
    <p class="normal">We are now ready to install Kaggle and authenticate it:</p>
    <pre class="programlisting code"><code class="hljs-code">try:
  import kaggle
except:
  !pip install kaggle
import kaggle
kaggle.api.authenticate()
</code></pre>
    <p class="normal">And that’s it! That’s <a id="_idIndexMarker377"/>all we need. We are now ready to collect the <code class="inlineCode">Bank Customer Churn</code> dataset.</p>
    <h3 id="_idParaDest-155" class="heading-3">Collecting the dataset</h3>
    <p class="normal">We will now <a id="_idIndexMarker378"/>download the zipped dataset, extract the CSV file, upload it into a pandas DataFrame, drop columns that we will not use, and display the result. Let’s first download the zipped dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">!kaggle datasets download -d radheshyamkollipara/bank-customer-churn
</code></pre>
    <p class="normal">The output displays the source of the data:</p>
    <pre class="programlisting con"><code class="hljs-con">Dataset URL: https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn
License(s): other
bank-customer-churn.zip: Skipping, found more recently modified local copy (use --force to force download)
</code></pre>
    <p class="normal">We can now unzip the data:</p>
    <pre class="programlisting code"><code class="hljs-code">import zipfile
with zipfile.ZipFile('/content/bank-customer-churn.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/')
print("File Unzipped!")
</code></pre>
    <p class="normal">The output should confirm that the file is unzipped:</p>
    <pre class="programlisting con"><code class="hljs-con">File Unzipped!
</code></pre>
    <p class="normal">The CSV file is uploaded to a pandas DataFrame named <code class="inlineCode">data1</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
# Load the CSV file
file_path = '/content/Customer-Churn-Records.csv'
data1 = pd.read_csv(file_path)
</code></pre>
    <p class="normal">We will now drop the following four columns in this scenario:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">RowNumber</code>: We don’t need these columns because we will be creating a unique index for each record.</li>
      <li class="bulletList"><code class="inlineCode">Surname</code>: The goal in this scenario is to anonymize the data and not display surnames. We will focus on customer profiles and behaviors, such as complaints and credit card consumption (points earned).</li>
      <li class="bulletList"><code class="inlineCode">Gender</code>: Consumer perceptions and behavior have evolved in the 2020s. It is more ethical and just as efficient to leave this information out in the context of a sample project.</li>
      <li class="bulletList"><code class="inlineCode">Geography</code>: This field <a id="_idIndexMarker379"/>might be interesting in some cases. For this scenario, let’s leave this feature out to avoid overfitting outputs based on cultural clichés. Furthermore, including this feature would require more information if we wanted to calculate distances for delivery services, for example:</li>
    </ul>
    <pre class="programlisting code"><code class="hljs-code"># Drop columns and update the DataFrame in place
data1.drop(columns=['RowNumber','Surname', 'Gender','Geography'], inplace=True)
data1
</code></pre>
    <p class="normal">The output triggered by <code class="inlineCode">data1</code> shows a simplified yet sufficient dataset:</p>
    <figure class="mediaobject"><img src="img/B31169_06_02.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.2: Triggered output</p>
    <p class="normal">This approach’s advantage is that it optimizes the size of the data that will be inserted into the Pinecone index (vector store). Optimizing the data size before inserting data into Pinecone and reducing the dataset by removing unnecessary fields can be very beneficial. It reduces the amount of data that needs to be transferred, stored, and processed in the vector store. When scaling, smaller data sizes can lead to faster query performance and lower costs, as Pinecone pricing can depend on the amount of <a id="_idIndexMarker380"/>data stored and the computational resources used for queries.</p>
    <p class="normal">We can now save the new pandas DataFrame in a safe location:</p>
    <pre class="programlisting code"><code class="hljs-code">data1.to_csv('data1.csv', index=False)
!cp /content/data1.csv /content/drive/MyDrive/files/rag_c6/data1.csv
</code></pre>
    <p class="normal">You can save it in the location that is best for you. Just make sure to save it because we will use it in the <em class="italic">Pipeline 2: Scaling a Pinecone index (vector store)</em> section of this chapter. We will now explore the optimized dataset before deciding how to implement it in a vector store.</p>
    <h2 id="_idParaDest-156" class="heading-2">2. Exploratory data analysis</h2>
    <p class="normal">In this section, we will perform EDA using the data that pandas has just defined, which contains customer data from a <a id="_idIndexMarker381"/>bank. EDA is a critical step before applying any RAG techniques with vector stores, as it helps us understand the underlying patterns and trends within the data.</p>
    <p class="normal">For instance, our preliminary analysis shows a direct correlation between customer complaints and churn rates, indicating that customers who have lodged complaints are more likely to leave the bank. Additionally, our data reveals that customers aged 50 and above are less likely to churn compared to younger customers. Interestingly, income levels (particularly the threshold of $100,000) do not appear to significantly influence churn decisions.</p>
    <p class="normal">Through the careful examination of these insights, we’ll demonstrate why jumping straight into complex ML models, especially deep learning, may not always be necessary or efficient for drawing basic conclusions. In scenarios where the relationships within the data are evident and the patterns straightforward, simpler statistical methods or even basic data analysis techniques might be more appropriate and resource-efficient. For example, k-means clustering can be effective, and we will implement it in the <em class="italic">Training an ML model</em> section of this chapter.</p>
    <p class="normal">However, this is not to understate the power of advanced RAG techniques, which we will explore in the <em class="italic">Pipeline 2: Scaling a Pinecone index</em> <em class="italic">(vector store)</em> section of this chapter. In that section, we will employ deep learning within vector stores to uncover more subtle patterns and intricate relationships that are not readily apparent through classic EDA.</p>
    <p class="normal">If we display the <a id="_idIndexMarker382"/>columns of the DataFrame, we can see that it is challenging to find patterns:</p>
    <pre class="programlisting con"><code class="hljs-con">#   Column              Non-Null Count  Dtype
---  ------              --------------  -----
 0   CustomerId          10000 non-null  int64
 1   CreditScore         10000 non-null  int64
<strong class="hljs-con-slc"> 2   Age                 10000 non-null  int64</strong>
 3   Tenure              10000 non-null  int64
 4   Balance             10000 non-null  float64
 5   NumOfProducts       10000 non-null  int64
 6   HasCrCard           10000 non-null  int64
 7   IsActiveMember      10000 non-null  int64
<strong class="hljs-con-slc"> 8   EstimatedSalary     10000 non-null  float64</strong>
<strong class="hljs-con-slc"> 9   Exited              10000 non-null  int64</strong>
<strong class="hljs-con-slc"> 10  Complain            10000 non-null  int64</strong>
 11  Satisfaction Score  10000 non-null  int64
 12  Card Type           10000 non-null  object
 13  Point Earned        10000 non-null  int64
</code></pre>
    <p class="normal"><code class="inlineCode">Age</code>, <code class="inlineCode">EstimatedSalary</code>, and <code class="inlineCode">Complain</code> are possible determining features that could be correlated with <code class="inlineCode">Exited</code>. We can also display the DataFrame to gain insights, as shown in the excerpt of <code class="inlineCode">data1</code> in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_06_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.3: Visualizing the strong correlation between customer complaints and bank churning (Exited)</p>
    <p class="normal">The main feature seems to be <code class="inlineCode">Complain</code>, which leads to <code class="inlineCode">Exited</code> (churn), as shown by running a standard<a id="_idIndexMarker383"/> calculation on the DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code"># Calculate the percentage of complain over exited
if sum_exited &gt; 0:  # To avoid division by zero
    percentage_complain_over_exited = (sum_complain/ sum_exited) * 100
else:
    percentage_complain_over_exited = 0
# Print results
print(f"Sum of Exited = {sum_exited}")
print(f"Sum of Complain = {sum_complain}")
print(f"Percentage of complain over exited = {percentage_complain_over_exited:.2f}%")
</code></pre>
    <p class="normal">The output shows a very high 100.29% ratio between complaints and customers leaving the bank (churning). This means that customers who complained did in fact leave the bank, which is a natural market trend:</p>
    <pre class="programlisting con"><code class="hljs-con">Sum of Exited = 2038
Sum of Complain = 2044
Percentage of complain over exited = 100.29%
</code></pre>
    <p class="normal">We can see that only a few exited the bank (six customers) without complaining.</p>
    <p class="normal">Run the following cells from GitHub; these contain Python functions that are variations of the <code class="inlineCode">exited</code> and <code class="inlineCode">complain</code> ratios and will produce the following outputs:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Age</code> and <code class="inlineCode">Exited</code> with a threshold of <code class="inlineCode">age=50</code> shows that persons over 50 seem less likely to leave a bank:
        <pre class="programlisting con-one"><code class="hljs-con">Sum of Age 50 and Over among Exited = 634
Sum of Exited = 2038
Percentage of Age 50 and Over among Exited = 31.11%
</code></pre>
      </li>
    </ul>
    <p class="normal-one">Conversely, the output shows that younger customers seem more likely to leave a bank if they are dissatisfied. You can explore different age thresholds to analyze the dataset further.</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Salary</code><strong class="keyWord"> </strong>and <code class="inlineCode">Exited</code><strong class="keyWord"> </strong>with a threshold of <code class="inlineCode">salary_threshold=100000</code> doesn’t seem to be a significant feature, as shown in this output:
        <pre class="programlisting con-one"><code class="hljs-con">Sum of Estimated Salary over 100000 among Exited = 1045
Sum of Exited = 2038
Percentage of Estimated Salary over 100000 among Exited = 51.28%
</code></pre>
      </li>
    </ul>
    <p class="normal-one">Try exploring different thresholds to analyze the dataset to confirm or refute this trend.</p>
    <p class="normal">Let’s create a<a id="_idIndexMarker384"/> heatmap based on the <code class="inlineCode">data1</code> pandas DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">import seaborn as sns
import matplotlib.pyplot as plt
# Select only numerical columns for the correlation heatmap
numerical_columns = data1.select_dtypes(include=['float64', 'int64']).columns
# Correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(data1[numerical_columns].corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()
</code></pre>
    <p class="normal">We can see that the highest correlation is between <code class="inlineCode">Complain</code> and <code class="inlineCode">Exited</code>:</p>
    <figure class="mediaobject"><img src="img/B31169_06_04.png" alt="A screenshot of a graph  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.4: Excerpt of the heatmap</p>
    <p class="normal">The preceding heatmap visualizes the correlation between each pair of features (variables) in the dataset. It shows the correlation coefficients between each pair of variables, which can range from <code class="inlineCode">-1</code>(low correlation) to <code class="inlineCode">1</code>(high correlation), with <code class="inlineCode">0</code> indicating no correlation.</p>
    <p class="normal">With that, we have explored several features. Let’s build an ML model to take this exploration further.</p>
    <h2 id="_idParaDest-157" class="heading-2">3. Training an ML model</h2>
    <p class="normal">Let’s continue our EDA and drill into the dataset further with an ML model. This section implements the training of an ML model using clustering techniques, specifically k-means clustering, to <a id="_idIndexMarker385"/>explore patterns within our dataset. We’ll prepare and process data for analysis, apply clustering, and then evaluate the results using different metrics. This approach is valuable for <a id="_idIndexMarker386"/>extracting insights without immediately resorting to more complex deep learning methods.</p>
    <div><p class="normal">k-means clustering is an unsupervised ML algorithm that partitions a dataset into k distinct, non-overlapping clusters by minimizing the variance within each cluster. The algorithm iteratively assigns data points to one of the k clusters based on the nearest mean (centroid), which is recalculated after each iteration until convergence.</p>
    </div>
    <p class="normal">Now, let’s break down the code section by section.</p>
    <h3 id="_idParaDest-158" class="heading-3">Data preparation and clustering</h3>
    <p class="normal">We will first copy our chapter’s dataset <code class="inlineCode">data1</code> to <code class="inlineCode">data2</code> to be able to go back to <code class="inlineCode">data1</code> if necessary if we <a id="_idIndexMarker387"/>wish to try other ML models:</p>
    <pre class="programlisting code"><code class="hljs-code"># Copying data1 to data2
data2 = data1.copy()
</code></pre>
    <p class="normal">You can explore the data with various scenarios of feature sets. In this case, we will select <code class="inlineCode">'</code><code class="inlineCode">CreditScore'</code>, <code class="inlineCode">'Age'</code>, <code class="inlineCode">'EstimatedSalary'</code>, <code class="inlineCode">'Exited'</code>, <code class="inlineCode">'Complain'</code>, and <code class="inlineCode">'Point Earned'</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"># Import necessary libraries
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score , davies_bouldin_score
# Assuming you have a dataframe named data1 loaded as described
# Selecting relevant features
features = data2[['CreditScore', 'Age', 'EstimatedSalary', 'Exited', 'Complain', 'Point Earned']]
</code></pre>
    <p class="normal">As in standard practice, let’s scale the features before running an ML model:</p>
    <pre class="programlisting code"><code class="hljs-code"># Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
</code></pre>
    <p class="normal">The credit score, estimated salary, and points earned (reflecting credit card spending) are good indicators of a customer’s financial standing with the bank. The age factor, combined<a id="_idIndexMarker388"/> with these other factors, might influence older customers to remain with the bank. However, the important point to note is that complaints may lead any market segment to consider leaving since complaints and churn are strongly correlated.</p>
    <p class="normal">We will now try to find two to four clusters to find the optimal number of clusters for this set of features:</p>
    <pre class="programlisting code"><code class="hljs-code"># Experiment with different numbers of clusters
for n_clusters in range(2, 5):  # Example range from 2 to 5
    kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=0)
    cluster_labels = kmeans.fit_predict(features_scaled)
    silhouette_avg = silhouette_score(features_scaled, cluster_labels)
    db_index = davies_bouldin_score(features_scaled, cluster_labels)
    print(f'For n_clusters={n_clusters}, the silhouette score is {silhouette_avg:.4f} and the Davies-Bouldin Index is {db_index:.4f}')
</code></pre>
    <p class="normal">The output contains an evaluation of clustering performance using two metrics—the silhouette score and the Davies-Bouldin index—across different numbers of clusters (ranging from 2 to 4):</p>
    <pre class="programlisting con"><code class="hljs-con">For n_clusters=2, the silhouette score is 0.6129 and the Davies-Bouldin Index is 0.6144
For n_clusters=3, the silhouette score is 0.3391 and the Davies-Bouldin Index is 1.1511
For n_clusters=4, the silhouette score is 0.3243 and the Davies-Bouldin Index is 1.0802
</code></pre>
    <div><p class="normal"><strong class="keyWord">Silhouette score</strong>: This metric measures the quality of clustering by calculating the mean intra-cluster <a id="_idIndexMarker389"/>distance (how close each point in one cluster is to points in the same cluster) and the mean nearest cluster distance (how close each point is to points in the next nearest cluster). The score ranges from -1 to 1, where a high value indicates that clusters are well-separated and internally cohesive. In this output, the highest silhouette score is 0.6129 for 2 clusters, suggesting better cluster separation and cohesion compared to 3 or 4 clusters.</p>
      <p class="normal"><strong class="keyWord">Davies-Bouldin index</strong>: This index evaluates clustering quality by comparing the ratio of within-cluster <a id="_idIndexMarker390"/>distances to between-cluster distances. Lower values of this index indicate better clustering, as they suggest lower intra-cluster variance and higher separation between clusters. The smallest Davies-Bouldin index in the output is 0.6144 for 2 clusters, indicating that this configuration likely provides the most effective separation of data points among the evaluated options.</p>
    </div>
    <p class="normal">For two clusters, the silhouette score and Davies-Bouldin index both suggest relatively good <a id="_idIndexMarker391"/>clustering performance. But as the number of clusters increases to three and four, both metrics indicate a decline in clustering quality, with lower silhouette scores and higher Davies-Bouldin indices, pointing to less distinct and less cohesive clusters.</p>
    <h3 id="_idParaDest-159" class="heading-3">Implementation and evaluation of clustering</h3>
    <p class="normal">Since two clusters seem to<a id="_idIndexMarker392"/> be the best choice for this <a id="_idIndexMarker393"/>dataset and set of features, let’s run the model with <code class="inlineCode">n_clusters=2</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"># Perform K-means clustering with a chosen number of clusters
kmeans = KMeans(n_clusters=2, n_init=10, random_state=0)  # Explicitly setting n_init to 10
data2['class'] = kmeans.fit_predict(features_scaled)
# Display the first few rows of the dataframe to verify the 'class' column
data2
</code></pre>
    <p class="normal">Once again, as shown in the <em class="italic">2. Exploratory data analysis</em> section, the correlation between complaints and exiting is established, as shown in the excerpt of the pandas DataFrame in <em class="italic">Figure 6.5</em>:</p>
    <figure class="mediaobject"><img src="img/B31169_06_05.png" alt="A screenshot of a game  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.5: Excerpt of the output of k-means clustering</p>
    <p class="normal">The first cluster is <code class="inlineCode">class=0</code>, which represents customers who complained (<code class="inlineCode">Complain</code>) and left (<code class="inlineCode">Exited</code>) the <a id="_idIndexMarker394"/>bank.</p>
    <p class="normal">If we count the rows<a id="_idIndexMarker395"/> for which <code class="inlineCode">Sum where 'class' == 0 and 'Exited' == 1</code>, we will obtain a strong correlation between complaints and customers leaving the bank:</p>
    <pre class="programlisting code"><code class="hljs-code"># 1. Sum where 'class' == 0
sum_class_0 = (data2['class'] == 0).sum()
# 2. Sum where 'class' == 0 and 'Complain' == 1
sum_class_0_complain_1 = data2[(data2['class'] == 0) &amp; (data2['Complain'] == 1)].shape[0]
# 3. Sum where 'class' == 0 and 'Exited' == 1
sum_class_0_exited_1 = data2[(data2['class'] == 0) &amp; (data2['Exited'] == 1)].shape[0]
# Print the results
print(f"Sum of 'class' == 0: {sum_class_0}")
print(f"Sum of 'class' == 0 and 'Complain' == 1: {sum_class_0_complain_1}")
print(f"Sum of 'class' == 0 and 'Exited' == 1: {sum_class_0_exited_1}")
</code></pre>
    <p class="normal">The output confirms that complaints and churn (customers leaving the bank) are closely related:</p>
    <pre class="programlisting con"><code class="hljs-con">Sum of 'class' == 0: 2039
Sum of 'class' == 0 and 'Complain' == 1: 2036
Sum of 'class' == 0 and 'Exited' == 1: 2037
</code></pre>
    <p class="normal">The following cell for the second class where <code class="inlineCode">'class' == 1 and 'Complain' == 1</code> confirms that few <a id="_idIndexMarker396"/>customers that complain stay with the bank:</p>
    <pre class="programlisting code"><code class="hljs-code"># 2. Sum where 'class' == 1 and 'Complain' == 1
sum_class_1_complain_1 = data2[(data2['class'] == 1) &amp; (data2['Complain'] == 1)].shape[0]
</code></pre>
    <p class="normal">The output is consistent with<a id="_idIndexMarker397"/> the correlations we have observed:</p>
    <pre class="programlisting con"><code class="hljs-con">Sum of 'class' == 1: 7961
Sum of 'class' == 1 and 'Complain' == 1: 8
Sum of 'class' == 1 and 'Exited' == 1: 1
</code></pre>
    <p class="normal">We saw that finding the features that could help us keep customers is challenging with classical methods that can be effective. However, our strategy will now be to transform the customer records into vectors with OpenAI and query a Pinecone index to find deeper patterns within the dataset with queries that don’t exactly match the dataset.</p>
    <h1 id="_idParaDest-160" class="heading-1">Pipeline 2: Scaling a Pinecone index (vector store)</h1>
    <p class="normal">The goal of this section is to build a Pinecone index with our dataset and scale it from 10,000 records up to 1,000,000 records. Although we are building on the knowledge acquired in<a id="_idIndexMarker398"/> the previous chapters, the essence of scaling is different from managing sample datasets.</p>
    <p class="normal">The clarity of each process of this pipeline is deceptively simple: data preparation, embedding, uploading to a vector store, and querying to retrieve documents. We have already gone through each of these processes in <em class="chapterRef">Chapters 2</em> and <em class="chapterRef">3</em>.</p>
    <p class="normal">Furthermore, beyond implementing Pinecone instead of Deep Lake and using OpenAI models in a slightly different way, we are performing the same functions as in <em class="chapterRef">Chapters 2</em>, <em class="chapterRef">3</em>, and <em class="chapterRef">4</em> for the vector store phase:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Data preparation</strong>: We will start by preparing our dataset using Python for chunking.</li>
      <li class="numberedList"><strong class="keyWord">Chunking and embedding</strong>: We will chunk the prepared data and then embed the chunked data.</li>
      <li class="numberedList"><strong class="keyWord">Creating the Pinecone index</strong>: We will create a Pinecone index (vector store).</li>
      <li class="numberedList"><strong class="keyWord">Upserting</strong>: We will upload the embedded documents (in this case, customer records) and the text of each record as metadata.</li>
      <li class="numberedList"><strong class="keyWord">Querying the Pinecone index</strong>: Finally, we will run a query to retrieve relevant documents to prepare <em class="italic">Pipeline 3: RAG generative AI</em>.</li>
    </ol>
    <div><p class="normal">Take all the time you need, if necessary, to go through <em class="chapterRef">Chapters 2</em>,<em class="chapterRef">3</em>, and <em class="chapterRef">4</em> again for the data preparation, chunking, embedding, and querying functions.</p>
    </div>
    <p class="normal">We know how to implement each phase because we’ve already done that with Deep Lake, and Pinecone is a type of vector store, too. So, what’s the issue here? The real issue is the hidden real-life project challenges on which we will focus, starting with the size, cost, and operations involved.</p>
    <h2 id="_idParaDest-161" class="heading-2">The challenges of vector store management</h2>
    <p class="normal">Usually, we begin a section by jumping into the code. That’s fine for small volumes, but scaling requires project management decisions before getting started! Why? When we run a program with a bad decision or an error on small datasets, the consequences are limited. But <a id="_idIndexMarker399"/>scaling is a different story! The fundamental principle and risk of scaling is that errors are scaled exponentially, too.</p>
    <p class="normal">Let’s list the pain points you must face before running a single line of code. You can apply this methodology to any platform or model. However, we have limited the platforms in this chapter to OpenAI and Pinecone to focus on processes, not platform management. Using other platforms involves careful risk management, which isn’t the objective of this chapter.</p>
    <p class="normal">Let’s begin with OpenAI models:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">OpenAI models for embedding</strong>: OpenAI continually improves and offers new models for embedding. Make<a id="_idIndexMarker400"/> sure you examine the characteristics of each one before embedding, including speed, cost, input limits, and API call rates, at <a href="https://platform.openai.com/docs/models/embeddings">https://platform.openai.com/docs/models/embeddings</a>.</li>
      <li class="bulletList"><strong class="keyWord">OpenAI models for generation</strong>:<strong class="keyWord"> </strong>OpenAI continually releases new models and abandons older ones. Google does <a id="_idIndexMarker401"/>the same. Think of these models as racing cars. Can you win a race today with a 1930 racing car? When scaling, you need the most efficient models. Check the speed, cost, input limits, output size, and API call rates at <a href="https://platform.openai.com/docs/models">https://platform.openai.com/docs/models</a>.</li>
    </ul>
    <p class="normal">This means that you must <a id="_idIndexMarker402"/>continually take the evolution of models into account for speed and cost reasons when scaling. Then, beyond technical considerations, you must have a real-time view of the pay-as-you-go billing perspective and technical constraints, such as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Billing management</strong>: <a href="https://platform.openai.com/settings/organization/billing/overview">https://platform.openai.com/settings/organization/billing/overview</a></li>
      <li class="bulletList"><strong class="keyWord">Limits including rate limits</strong>: <a href="https://platform.openai.com/settings/organization/limits">https://platform.openai.com/settings/organization/limits</a></li>
    </ul>
    <p class="normal">Now, let’s examine Pinecone constraints once you have created an account:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Cloud and region</strong>: The<a id="_idIndexMarker403"/> choice of the cloud (AWS, Google, or other) and region (location of the serverless storage) have pricing implications.</li>
      <li class="bulletList"><strong class="keyWord">Usage</strong>: This includes read units, write units, and storage costs, including cloud backups. Read more at <a href="https://docs.pinecone.io/guides/indexes/back-up-an-index">https://docs.pinecone.io/guides/indexes/back-up-an-index</a>.</li>
    </ul>
    <p class="normal">You must continually monitor the price and usage of Pinecone as for any other cloud environment. You can do so using these links: <a href="https://www.pinecone.io/pricing/">https://www.pinecone.io/pricing/</a> and <a href="https://docs.pinecone.io/guides/operations/monitoring">https://docs.pinecone.io/guides/operations/monitoring</a>.</p>
    <p class="normal">The scenario we are implementing is one of many other ways of achieving the goals in this chapter with other platforms and frameworks. However, the constraints are invariants, including pricing, usage, speed performances, and limits.</p>
    <p class="normal">Let’s now implement <a id="_idIndexMarker404"/><em class="italic">Pipeline 2</em> by focusing on the pain points beyond the functionality we have already explored in previous chapters. You may open <code class="inlineCode">Pipeline_2_Scaling_a_Pinecone_Index.ipynb</code> in the GitHub repository. The program begins with installing the environment.</p>
    <h2 id="_idParaDest-162" class="heading-2">Installing the environment</h2>
    <p class="normal">As mentioned earlier, the<a id="_idIndexMarker405"/> program is limited to Pinecone and OpenAI, which has the advantage of avoiding any intermediate software, platforms, and constraints. Store your API keys in a safe location. In this case, the API keys are stored on Google Drive:</p>
    <pre class="programlisting code"><code class="hljs-code">#API Key
#Store your key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)
from google.colab import drive
drive.mount('/content/drive')
</code></pre>
    <p class="normal">Now, we install OpenAI and Pinecone:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install openai==1.40.3
!pip install pinecone-client==5.0.1
</code></pre>
    <p class="normal">Finally, the program initializes the API keys:</p>
    <pre class="programlisting code"><code class="hljs-code">f = open("drive/MyDrive/files/pinecone.txt", "r")
PINECONE_API_KEY=f.readline()
f.close()
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline()
f.close()
#The OpenAI Key
import os
import openai
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
</code></pre>
    <p class="normal">The program now <a id="_idIndexMarker406"/>processes the <code class="inlineCode">Bank Customer Churn</code> dataset.</p>
    <h2 id="_idParaDest-163" class="heading-2">Processing the dataset</h2>
    <p class="normal">This section will focus on <a id="_idIndexMarker407"/>preparing the dataset for chunking, which splits it into optimized chunks of text to embed. The program first retrieves the <code class="inlineCode">data1.csv</code> dataset that we prepared and saved in the <em class="italic">Pipeline 1: Collecting and preparing the dataset</em> section of this chapter:</p>
    <pre class="programlisting code"><code class="hljs-code">!cp /content/drive/MyDrive/files/rag_c6/data1.csv /content/data1.csv
</code></pre>
    <p class="normal">Then, we load the dataset in a pandas DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
# Load the CSV file
file_path = '/content/data1.csv'
data1 = pd.read_csv(file_path)
</code></pre>
    <p class="normal">We make sure that the 10,000 lines of the dataset are loaded:</p>
    <pre class="programlisting code"><code class="hljs-code"># Count the chunks
number_of_lines = len(data1)
print("Number of lines: ",number_of_lines)
</code></pre>
    <p class="normal">The output confirms that the lines are indeed present:</p>
    <pre class="programlisting con"><code class="hljs-con">Number of lines:  10000
</code></pre>
    <p class="normal">The following code is important in this scenario. Each line that represents a customer record will become a line in the <code class="inlineCode">output_lines</code> list:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
# Initialize an empty list to store the lines
output_lines = []
# Iterate over each row in the DataFrame
for index, row in data1.iterrows():
    # Create a list of "column_name: value" for each column in the row
    row_data = [f"{col}: {row[col]}" for col in data1.columns]
    # Join the list into a single string separated by spaces
    line = ' '.join(row_data)
    # Append the line to the output list
    output_lines.append(line)
# Display or further process `output_lines` as needed
for line in output_lines[:5]:  # Displaying first 5 lines for preview
    print(line)
</code></pre>
    <p class="normal">The output shows that each line in the <code class="inlineCode">output_lines</code> list is a separate customer record text:</p>
    <pre class="programlisting con"><code class="hljs-con">CustomerId: 15634602 CreditScore: 619 Age: 42 Tenure: 2 Balance: 0.0 NumOfProducts: 1 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 101348.88 Exited: 1 Complain: 1 Satisfaction Score: 2 Card Type: DIAMOND Point Earned: 464…
</code></pre>
    <p class="normal">We are sure that <a id="_idIndexMarker408"/>each line is a separate pre-chunk with a clearly defined customer record. Let’s now copy <code class="inlineCode">output_lines</code> to <code class="inlineCode">lines</code> for the chunking process:</p>
    <pre class="programlisting code"><code class="hljs-code">lines = output_lines.copy()
</code></pre>
    <p class="normal">The program runs a quality control on the <code class="inlineCode">lines</code> list to make sure we haven’t lost a line in the process:</p>
    <pre class="programlisting code"><code class="hljs-code"># Count the lines
number_of_lines = len(lines)
print("Number of lines: ",number_of_lines)
</code></pre>
    <p class="normal">The output confirms that 10,000 lines are present:</p>
    <pre class="programlisting con"><code class="hljs-con">Number of lines:  10000
</code></pre>
    <p class="normal">And just like that, the<a id="_idIndexMarker409"/> data is ready to be chunked.</p>
    <h2 id="_idParaDest-164" class="heading-2">Chunking and embedding the dataset</h2>
    <p class="normal">In this section, we will chunk and embed the pre-chunks in the <code class="inlineCode">lines</code> list. Building a pre-chunks list with structured data is not possible every time, but when it is, it increases a model’s traceability, clarity, and querying performance. The chunking process is straightforward.</p>
    <h3 id="_idParaDest-165" class="heading-3">Chunking</h3>
    <p class="normal">The practice of chunking<a id="_idIndexMarker410"/> pre-chunks is important for dataset management. We can create our chunks from a list of pre-chunks stored as lines:</p>
    <pre class="programlisting code"><code class="hljs-code"># Initialize an empty list for the chunks
chunks = []
# Add each line as a separate chunk to the chunks list
for line in lines:
    chunks.append(line)  # Each line becomes its own chunk
# Now, each line is treated as a separate chunk
print(f"Total number of chunks: {len(chunks)}")
</code></pre>
    <p class="normal">The output shows that we have not lost any data during the process:</p>
    <pre class="programlisting con"><code class="hljs-con">Total number of chunks: 10000
</code></pre>
    <p class="normal">So why bother creating chunks and not just use the lines directly? In many cases, lines may require additional quality control and processing, such as data errors that somehow slipped through in the previous steps. We might even have a few chunks that exceed the input limit (which is continually evolving) of an embedding model at a given time.</p>
    <p class="normal">To better understand the structure of the chunked data, you can examine the length and content of the chunks using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"># Print the length and content of the first 10 chunks
for i in range(3):
    print(len(chunks[i]))
    print(chunks[i])
</code></pre>
    <p class="normal">The output will help a human controller visualize the chunked data, providing a snapshot like so:</p>
    <pre class="programlisting con"><code class="hljs-con">224
CustomerId: 15634602 CreditScore: 619 Age: 42 Tenure: 2 Balance: 0.0 NumOfProducts: 1 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 101348.88 Exited: 1 Complain: 1 Satisfaction Score: 2 Card Type: DIAMOND Point Earned: 464…
</code></pre>
    <p class="normal">The chunks will now be embedded.</p>
    <h3 id="_idParaDest-166" class="heading-3">Embedding</h3>
    <p class="normal">This section will require careful testing and consideration of the issues. We will realize that <em class="italic">scaling requires more thinking than doing</em>. Each project will require specific amounts of data through design and testing to provide effective responses. We must also take into account the<a id="_idIndexMarker411"/> cost and benefit of each component of the pipeline. For example, initializing the embedding model is no easy task!</p>
    <p class="normal">At the time of writing, OpenAI offers three embedding models that we can test:</p>
    <pre class="programlisting code"><code class="hljs-code">import openai
import time
embedding_model="text-embedding-3-small"
#embedding_model="text-embedding-3-large"
#embedding_model="text-embedding-ada-002"
</code></pre>
    <p class="normal">In this section, we will use <code class="inlineCode">text-embedding-3-small</code>. However, you can evaluate the other models by uncommenting the code. The <code class="inlineCode">embedding</code> function will accept the model you select:</p>
    <pre class="programlisting code"><code class="hljs-code"># Initialize the OpenAI client
client = openai.OpenAI()
def get_embedding(text, model=embedding_model):
    text = text.replace("\n", " ")
    response = client.embeddings.create(input=[text], model=model)
    embedding = response.data[0].embedding
    return embedding
</code></pre>
    <p class="normal">Make sure to check the cost and features of each embedding model before running one of your choice: <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models">https://platform.openai.com/docs/guides/embeddings/embedding-models</a>.</p>
    <p class="normal">The program now embeds the chunks, but the embedding process requires strategic choices, particularly to manage large datasets and API rate limits effectively. In this case, we will create batches of chunks to embed:</p>
    <pre class="programlisting code"><code class="hljs-code">import openai
import time
# Initialize the OpenAI client
client = openai.OpenAI()
# Initialize variables
start_time = time.time()  # Start timing before the request
chunk_start = 0
chunk_end = 1000
pause_time = 3
embeddings = []
counter = 1
</code></pre>
    <p class="normal">We will embed 1,000 chunks at a time with <code class="inlineCode">chunk_start = 0</code> and <code class="inlineCode">chunk_end = 1000</code>. To avoid possible OpenAI API rate limits, <code class="inlineCode">pause_time = 3</code> was added to pause for 3 seconds between each batch. We will store the embeddings in <code class="inlineCode">embeddings = []</code> and count the batches starting with <code class="inlineCode">counter = 1.</code></p>
    <p class="normal">The code is divided into<a id="_idIndexMarker412"/> three main parts, as explained in the following excerpts:</p>
    <ul>
      <li class="bulletList">Iterating through all the chunks with batches:
        <pre class="programlisting code-one"><code class="hljs-code">while chunk_end &lt;= len(chunks):
    # Select the current batch of chunks
    chunks_to_embed = chunks[chunk_start:chunk_end]…
</code></pre>
      </li>
      <li class="bulletList">Embedding a batch of <code class="inlineCode">chunks_to_embed</code>:
        <pre class="programlisting code-one"><code class="hljs-code">for chunk in chunks_to_embed:
      embedding = get_embedding(chunk, model=embedding_model)
      current_embeddings.append(embedding)…
</code></pre>
      </li>
      <li class="bulletList">Updating the start and end values of the chunks to embed for the next batch:
        <pre class="programlisting code-one"><code class="hljs-code"> # Update the chunk indices
    chunk_start += 1000
    chunk_end += 1000
</code></pre>
      </li>
    </ul>
    <p class="normal">A function was added in case the batches are not perfect multiples of the batch size:</p>
    <pre class="programlisting code"><code class="hljs-code"># Process the remaining chunks if any
if chunk_end &lt; len(chunks):
    remaining_chunks = chunks[chunk_end:]
    remaining_embeddings = [get_embedding(chunk, model=embedding_model) for chunk in remaining_chunks]
    embeddings.extend(remaining_embeddings)
</code></pre>
    <p class="normal">The output displays the counter and the processing time:</p>
    <pre class="programlisting con"><code class="hljs-con">All chunks processed.
Batch 1 embedded.
...
Batch 10 embedded.
Response Time: 2689.46  seconds
</code></pre>
    <p class="normal">The response time may seem long and may vary for each run, but that is what scaling is all about! We cannot expect to process large volumes of data in a very short time and not face performance challenges.</p>
    <p class="normal">We can display an <a id="_idIndexMarker413"/>embedding if we wish to check that everything went well:</p>
    <pre class="programlisting code"><code class="hljs-code">print("First embedding:", embeddings[0])
</code></pre>
    <p class="normal">The output displays the embedding:</p>
    <pre class="programlisting con"><code class="hljs-con">First embedding: [-0.024449337273836136, -0.00936567410826683,…
</code></pre>
    <p class="normal">Let’s verify if we have the same number of text chunks (customer records) and vectors (embeddings):</p>
    <pre class="programlisting code"><code class="hljs-code"># Check the lengths of the chunks and embeddings
num_chunks = len(chunks)
print(f"Number of chunks: {num_chunks}")
print(f"Number of embeddings: {len(embeddings)}")
</code></pre>
    <p class="normal">The output confirms that we are ready to move to Pinecone:</p>
    <pre class="programlisting con"><code class="hljs-con">Number of chunks: 10000
Number of embeddings: 10000
</code></pre>
    <p class="normal">We have now chunked and embedded the data. We will duplicate the data to simulate scaling in this notebook.</p>
    <h3 id="_idParaDest-167" class="heading-3">Duplicating data</h3>
    <p class="normal">We will duplicate the <a id="_idIndexMarker414"/>chunked and embedded data; this way, you can simulate volumes without paying for the OpenAI embeddings. The cost of the embedding data and the time performances are linear. So we can simulate scaling with a corpus of 50,000 data points, for example, and extrapolate the response times and cost to any size we need.</p>
    <p class="normal">The code is straightforward. We first determine the number of times we want to duplicate the data:</p>
    <pre class="programlisting code"><code class="hljs-code"># Define the duplication size
dsize = 5  # You can set this to any value between 1 and n as per your experimentation requirements
total=dsize * len(chunks)
print("Total size", total)
</code></pre>
    <p class="normal">The program will then duplicate the chunks and the embeddings:</p>
    <pre class="programlisting code"><code class="hljs-code"># Initialize new lists for duplicated chunks and embeddings
duplicated_chunks = []
duplicated_embeddings = []
# Loop through the original lists and duplicate each entry
for i in range(len(chunks)):
    for _ in range(dsize):
        duplicated_chunks.append(chunks[i])
        duplicated_embeddings.append(embeddings[i])
</code></pre>
    <p class="normal">The code then checks if the<a id="_idIndexMarker415"/> number of chunks fits the number of embeddings:</p>
    <pre class="programlisting code"><code class="hljs-code"># Checking the lengths of the duplicated lists
print(f"Number of duplicated chunks: {len(duplicated_chunks)}")
print(f"Number of duplicated embeddings: {len(duplicated_embeddings)}")
</code></pre>
    <p class="normal">Finally, the output confirms that we duplicated the data five times:</p>
    <pre class="programlisting con"><code class="hljs-con">Total size 50000
Number of duplicated chunks: 50000
Number of duplicated embeddings: 50000
</code></pre>
    <p class="normal">50,000 data points is a good volume to begin with, giving us the necessary data to populate a vector store. Let’s now create the Pinecone index.</p>
    <h2 id="_idParaDest-168" class="heading-2">Creating the Pinecone index</h2>
    <p class="normal">The first step is to make <a id="_idIndexMarker416"/>sure our API key is initialized with the name of the variable we prefer and then create a Pinecone instance:</p>
    <pre class="programlisting code"><code class="hljs-code">import os
from pinecone import Pinecone, ServerlessSpec
# initialize connection to pinecone (get API key at app.pinecone.io)
api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'
pc = Pinecone(api_key=PINECONE_API_KEY)
</code></pre>
    <p class="normal">The Pinecone instance, <code class="inlineCode">pc</code>, has been created. Now, we will choose the index name, our cloud, and region:</p>
    <pre class="programlisting code"><code class="hljs-code">from pinecone import ServerlessSpec
index_name = [YOUR INDEX NAME] #'bank-index-900'for example
cloud = os.environ.get('PINECONE_CLOUD') or 'aws'
region = os.environ.get('PINECONE_REGION') or 'us-east-1'
spec = ServerlessSpec(cloud=cloud, region=region)
</code></pre>
    <p class="normal">We have now indicated that <a id="_idIndexMarker417"/>we want a serverless cloud instance (<code class="inlineCode">spec</code>) with AWS in the <code class="inlineCode">'us-east-1'</code> location. We are ready to create the index (the type of vector store) named <code class="inlineCode">'bank-index-50000'</code> with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
import pinecone
# check if index already exists (it shouldn't if this is first time)
if index_name not in pc.list_indexes().names():
    # if does not exist, create index
    pc.create_index(
        index_name,
        dimension=1536,  #Dimension of the embedding model
        metric='cosine',
        spec=spec
    )
    # wait for index to be initialized
    time.sleep(1)
# connect to index
index = pc.Index(index_name)
# view index stats
index.describe_index_stats()
</code></pre>
    <p class="normal">We added the following two parameters to <code class="inlineCode">index_name</code> and <code class="inlineCode">spec</code>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">dimension=1536</code> represents the length of the embeddings vector that you can adapt to the embedding model of your choice.</li>
      <li class="bulletList"><code class="inlineCode">metric='cosine'</code> is the metric we will use for vector similarity between the embedded vectors. You can <a id="_idIndexMarker418"/>also choose other metrics, such as Euclidean distance: <a href="https://www.pinecone.io/learn/vector-similarity/">https://www.pinecone.io/learn/vector-similarity/</a>.</li>
    </ul>
    <p class="normal">When the index is created, the program displays the description of the index:</p>
    <pre class="programlisting con"><code class="hljs-con">{'dimension': 1536,
 'index_fullness': 0.0,
 'namespaces': {},
 'total_vector_count': 0}
</code></pre>
    <p class="normal">The vector count and index fullness are <code class="inlineCode">0</code> since we haven’t been populating the vector store. Great, now<a id="_idIndexMarker419"/> we are ready to upsert!</p>
    <h2 id="_idParaDest-169" class="heading-2">Upserting</h2>
    <p class="normal">The section’s goal is to<a id="_idIndexMarker420"/> populate the vector store with our 50,000 embedded vectors and their associated metadata (chunks). The objective is to fully understand the scaling process and use synthetic data to reach the 50,000+ vector level. You can go back to the previous section and duplicate the data up to any value you wish. However, bear in mind that the upserting time to a Pinecone index is linear. You simply need to extrapolate the performances to the size you want to evaluate to obtain the approximate time it would take. Check the Pinecone pricing <a id="_idIndexMarker421"/>before running the upserting process: <a href="https://www.pinecone.io/pricing/">https://www.pinecone.io/pricing/</a>.</p>
    <p class="normal">We will populate (upsert) the vector store with three fields:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ids</code>: Contains a unique identifier for each chunk, which will be a counter we increment as we upsert the data</li>
      <li class="bulletList"><code class="inlineCode">embedding</code>: Contains the vectors (embedded chunks) we created</li>
      <li class="bulletList"><code class="inlineCode">chunks</code>: Contains the chunks in plain text, which is the metadata</li>
    </ul>
    <p class="normal">The code will populate the data in batches. Let’s first define the batch upserting function:</p>
    <pre class="programlisting code"><code class="hljs-code"># upsert function
def upsert_to_pinecone(data, batch_size):
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        index.upsert(vectors=batch)
        #time.sleep(1)  # Optional: add delay to avoid rate limits
</code></pre>
    <p class="normal">We will measure the time it takes to process our corpus:</p>
    <pre class="programlisting code"><code class="hljs-code">import pinecone
import time
import sys
start_time = time.time()  # Start timing before the request
</code></pre>
    <p class="normal">Now, we create a function that will calculate the size of the batches and limit them to 4 MB, which is close to the present Pinecone upsert batch size limit:</p>
    <pre class="programlisting code"><code class="hljs-code"># Function to calculate the size of a batch
def get_batch_size(data, limit=4000000):  # limit set slightly below 4MB to be safe
    total_size = 0
    batch_size = 0
    for item in data:
        item_size = sum([sys.getsizeof(v) for v in item.values()])
        if total_size + item_size &gt; limit:
            break
        total_size += item_size
        batch_size += 1
    return batch_size
</code></pre>
    <p class="normal">We can now create our<a id="_idIndexMarker422"/> <code class="inlineCode">upsert</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">def batch_upsert(data):
    total = len(data)
    i = 0
    while i &lt; total:
        batch_size = get_batch_size(data[i:])
        batch = data[i:i + batch_size]
        if batch:
            upsert_to_pinecone(batch,batch_size)
            i += batch_size
            print(f"Upserted {i}/{total} items...")  # Display current progress
        else:
            break
    print("Upsert complete.")
</code></pre>
    <p class="normal">We need to generate unique IDs for the data we upsert:</p>
    <pre class="programlisting code"><code class="hljs-code"># Generate IDs for each data item
ids = [str(i) for i in range(1, len(duplicated_chunks) + 1)]
</code></pre>
    <p class="normal">We will create the metadata to upsert the dataset to Pinecone:</p>
    <pre class="programlisting code"><code class="hljs-code"># Prepare data for upsert
data_for_upsert = [
    {"id": str(id), "values": emb, "metadata": {"text": chunk}}
    for id, (chunk, emb) in zip(ids, zip(duplicated_chunks, duplicated_embeddings))
]
</code></pre>
    <p class="normal">We now have everything we need to upsert in <code class="inlineCode">data_for_upsert</code>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">"id": str(ids[i])</code> contains the IDs we created with the seed.</li>
      <li class="bulletList"><code class="inlineCode">"values": emb</code> contains the chunks we embedded into vectors.</li>
      <li class="bulletList"><code class="inlineCode">"metadata": {"text": chunk}</code> contains the chunks we embedded.</li>
    </ul>
    <p class="normal">We now run the batch upsert process:</p>
    <pre class="programlisting code"><code class="hljs-code"># Upsert data in batches
batch_upsert(data_for_upsert)
</code></pre>
    <p class="normal">Finally, we measure the response time:</p>
    <pre class="programlisting code"><code class="hljs-code">response_time = time.time() - start_time  # Measure response time
print(f"Upsertion response time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The output contains<a id="_idIndexMarker423"/> useful information that shows the batch progression:</p>
    <pre class="programlisting con"><code class="hljs-con">Upserted 316/50000 items...
Upserted 632/50000 items...
Upserted 948/50000 items...
…
Upserted 49612/50000 items...
Upserted 49928/50000 items...
Upserted 50000/50000 items...
Upsert complete.
Upsertion response time: 560.66 seconds
</code></pre>
    <p class="normal">The time shows that it takes just under one minute (56 seconds) per 10,000 data points. You can try a larger corpus. The time should remain linear.</p>
    <p class="normal">We can also view the Pinecone index statistics to see how many vectors were uploaded:</p>
    <pre class="programlisting code"><code class="hljs-code">print("Index stats")
print(index.describe_index_stats(include_metadata=True))
</code></pre>
    <p class="normal">The output confirms that the upserting process was successful:</p>
    <pre class="programlisting con"><code class="hljs-con">Index stats
{'dimension': 1536,
 'index_fullness': 0.0,
 'namespaces': {'': {'vector_count': 50000}},
 'total_vector_count': 50000}
</code></pre>
    <p class="normal">The upsert output shows<a id="_idIndexMarker424"/> that we upserted 50,000 data points but the output shows less, most probably due to duplicates within the data.</p>
    <h2 id="_idParaDest-170" class="heading-2">Querying the Pinecone index</h2>
    <p class="normal">The task now is to verify the<a id="_idIndexMarker425"/> response times with a large Pinecone index. Let’s create a function to query the vector store and display the results:</p>
    <pre class="programlisting code"><code class="hljs-code"># Print the query results along with metadata
def display_results(query_results):
  for match in query_results['matches']:
    print(f"ID: {match['id']}, Score: {match['score']}")
    if 'metadata' in match and 'text' in match['metadata']:
        print(f"Text: {match['metadata']['text']}")
    else:
        print("No metadata available.")
</code></pre>
    <p class="normal">We need an embedding function for the query using the same embedding model as we implemented to embed the chunks of the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">embedding_model = "text-embedding-3-small"
def get_embedding(text, model=embedding_model):
    text = text.replace("\n", " ")
    response = client.embeddings.create(input=[text], model=model)
    embedding = response.data[0].embedding
    return embedding
</code></pre>
    <p class="normal">We can now query the<a id="_idIndexMarker426"/> Pinecone vector store to conduct a unit test and display the results and response time. We first initialize the OpenAI client and start time:</p>
    <pre class="programlisting code"><code class="hljs-code">import openai
# Initialize the OpenAI client
client = openai.OpenAI()
print("Querying vector store")
start_time = time.time()  # Start timing before the request
</code></pre>
    <p class="normal">We then query the vector store with a customer profile that does not exist in the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">query_text = "Customer Robertson CreditScore 632Age 21 Tenure 2Balance 0.0NumOfProducts 1HasCrCard 1IsActiveMember 1EstimatedSalary 99000 Exited 1Complain 1Satisfaction Score 2Card Type DIAMONDPoint Earned 399"
</code></pre>
    <p class="normal">The query is embedded with the same model as the one used to embed the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">query_embedding = get_embedding(query_text,model=embedding_model)
</code></pre>
    <p class="normal">We run the query and display the output:</p>
    <pre class="programlisting code"><code class="hljs-code">query_results = index.query(vector=query_embedding, top_k=1, include_metadata=True)  # Request metadata
#print("raw query_results",query_results)
print("processed query results")
display_results(query_results) #display results
response_time = time.time() - start_time              # Measure response time
print(f"Querying response time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The output displays the query response and time:</p>
    <pre class="programlisting con"><code class="hljs-con">Querying vector store
Querying vector store
processed query results
ID: 46366, Score: 0.823366046
Text: CustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Balance: 0.0 NumOfProducts: 2 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 54706.75 Exited: 0 Complain: 0 Satisfaction Score: 3 Card Type: DIAMOND Point Earned: 852
Querying response time: 0.74 seconds
</code></pre>
    <p class="normal">We can see that the response quality is satisfactory because it found a similar profile. The time is excellent: <code class="inlineCode">0.74 seconds</code>. When reaching a 1,000,000 vector count, for example, the<a id="_idIndexMarker427"/> response time should still be constant at less than a second. That is the magic of the Pinecone index!</p>
    <p class="normal">If we go to our organization on <a id="_idIndexMarker428"/>Pinecone, <a href="https://app.pinecone.io/organizations/">https://app.pinecone.io/organizations/</a>, and click on our index, we can monitor our statistics, analyze our usage, and more, as illustrated here:</p>
    <figure class="mediaobject"> <img src="img/B31169_06_06.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.6: Visualizing the Pinecone index vector count in the Pinecone console</p>
    <p class="normal">Our Pinecone index is now<a id="_idIndexMarker429"/> ready to augment inputs and generate content.</p>
    <h1 id="_idParaDest-171" class="heading-1">Pipeline 3: RAG generative AI</h1>
    <p class="normal">In this section, we will use RAG generative AI to automate a customized and engaging marketing message to the customers of the bank to encourage them to remain loyal. We will be building on <a id="_idIndexMarker430"/>our programs on data preparation and Pinecone indexing; we will leverage the Pinecone vector database for advanced search functionalities. We will choose a target vector that represents a market segment to query the Pinecone index. The response will be processed to extract the top k similar vectors. We will then augment the user input with this target market to ask OpenAI to make recommendations to the market segment targeted with customized messages.</p>
    <p class="normal">You may open <code class="inlineCode">Pipeline-3_RAG_Generative AI.ipynb</code> on GitHub. The first code section in this notebook, <em class="italic">Installing the environment</em>, is the same as in <code class="inlineCode">2-Pincone_vector_store-1M.ipynb</code>, built in the <em class="italic">Pipeline 2: Scaling a Pinecone index (vector store)</em> section earlier in this chapter. The <em class="italic">Pinecone index</em> in the second code section is also the same as in <code class="inlineCode">2-Pincone_vector_store-1M.ipynb</code>. However, this time, the Pinecone index code checks whether a Pinecone index exists and connects to it if it does, rather than creating a new index.</p>
    <p class="normal">Let’s run an example of RAG with GPT-4o.</p>
    <h2 id="_idParaDest-172" class="heading-2">RAG with GPT-4o</h2>
    <p class="normal">In this section of the code, we will <a id="_idIndexMarker431"/>query the Pinecone vector store, augment the<a id="_idIndexMarker432"/> user input, and generate a response with GPT-4o. It is the same process as with Deep Lake and an OpenAI generative model in <em class="italic">Chapter 3</em>, <em class="italic">Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI</em>, for example. However, the nature and usage of the Pinecone query is quite different in this case for the following reasons:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Target vector</strong>: The user input is not a question in the classical sense. In this case, it is a target vector representing the profile of a market segment.</li>
      <li class="bulletList"><strong class="keyWord">Usage</strong>:<strong class="keyWord"> </strong>The usage isn’t to augment the generative AI in the classical dialog sense (questions, summaries). In this case, we expect GPT-4o to write an engaging, customized email to offer products and services.</li>
      <li class="bulletList"><strong class="keyWord">Query time</strong>:<strong class="keyWord"> </strong>Speed is <a id="_idIndexMarker433"/>critical when scaling an application. We will measure the query time on the Pinecone index that contains 1,000,000+ vectors.</li>
    </ul>
    <h3 id="_idParaDest-173" class="heading-3">Querying the dataset</h3>
    <p class="normal">We will need an embedding <a id="_idIndexMarker434"/>function to embed the input. We will simplify and use the same embedding model we used in the <em class="italic">Embedding</em> section of <em class="italic">Pipeline 2: Scaling a Pinecone index (vector store)</em> for compatibility reasons:</p>
    <pre class="programlisting code"><code class="hljs-code">import openai
import time
embedding_model= "text-embedding-3-small"
# Initialize the OpenAI client
client = openai.OpenAI()
def get_embedding(text, model=embedding_model):
    text = text.replace("\n", " ")
    response = client.embeddings.create(input=[text], model=model)
    embedding = response.data[0].embedding
    return embedding
</code></pre>
    <p class="normal">We are now ready to query the Pinecone index.</p>
    <h3 id="_idParaDest-174" class="heading-3">Querying a target vector</h3>
    <p class="normal">A target vector represents a <a id="_idIndexMarker435"/>market segment that a marketing team wants to focus on for recommendations to increase customer loyalty. Your imagination and creativity are the only limits! Usually, the marketing team will be part of the design team for this pipeline. You might want to organize workshops to try various scenarios until the marketing team is satisfied. If you are part of the marketing team, then you want to help design target vectors. In any case, human insights into our adaptive creativity will lead to many ways of organizing target vectors and queries.</p>
    <p class="normal">In this case, we will target a market segment of customers around the age of 42 (<code class="inlineCode">Age 42</code>). We don’t need the age to be strictly 42 or an age bracket. We’ll let AI do the work for us. We are also targeting a customer that has a 100,000+ (<code class="inlineCode">EstimatedSalary 101348.88</code>) estimated <a id="_idIndexMarker436"/>salary, which would be a loss for the bank. We’re choosing a customer who has complained (<code class="inlineCode">Complain 1</code>) and seems to be exiting (<code class="inlineCode">Exited 1</code>) the bank. Let’s suppose that <code class="inlineCode">Exited 1</code>, in this scenario, means that the customer has made a request to close an account but it hasn’t been finalized yet. Let’s also consider that the marketing department chose the target vector.</p>
    <p class="normal"><code class="inlineCode">query_text</code> represents the customer profiles we are searching for:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
start_time = time.time()  # Start timing before the request
# Target vector
 "
# Target vector
query_text = "Customer Henderson CreditScore 599 Age 37Tenure 2Balance 0.0NumOfProducts 1HasCrCard 1IsActiveMember 1EstimatedSalary 107000.88Exited 1Complain 1Satisfaction Score 2Card Type DIAMONDPoint Earned 501"
query_embedding = get_embedding(text,model=embedding_model)
</code></pre>
    <p class="normal">We have embedded the query. Let’s now retrieve the top-k customer profiles that fit the target vector and parse the result:</p>
    <pre class="programlisting code"><code class="hljs-code"># Perform the query using the embedding
query_results = index.query(
    vector=query_embedding,
    top_k=5,
    include_metadata=True,
)
</code></pre>
    <p class="normal">We now print the response and the metadata:</p>
    <pre class="programlisting code"><code class="hljs-code"># Print the query results along with metadata
print("Query Results:")
for match in query_results['matches']:
    print(f"ID: {match['id']}, Score: {match['score']}")
    if 'metadata' in match and 'text' in match['metadata']:
        print(f"Text: {match['metadata']['text']}")
    else:
        print("No metadata available.")
response_time = time.time() - start_time              # Measure response time
print(f"Querying response time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The result is parsed to find the top-k matches to display their scores and content, as shown in the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">Query Results:
ID: 46366, Score: 0.854999781
Text: CustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Balance: 0.0 NumOfProducts: 2 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 54706.75 Exited: 0 Complain: 0 Satisfaction Score: 3 Card Type: DIAMOND Point Earned: 852
Querying response time: 0.63 seconds
</code></pre>
    <p class="normal">We have retrieved valuable information:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Ranking</strong> through<a id="_idIndexMarker437"/> the <code class="inlineCode">top-k</code> vectors that match the target vector. From one to another, depending on the target vector, the ranking will be automatically recalculated by the OpenAI generative AI model.</li>
      <li class="bulletList"><strong class="keyWord">Score metric </strong>through the score provided. A score is returned providing a metric for the response.</li>
      <li class="bulletList"><strong class="keyWord">Content </strong>that contains the top-ranked and best scores.</li>
    </ul>
    <p class="normal">It’s an all-in-one automated process! AI is taking us to new heights but we, of course, need human control to confirm the output, as described in the previous chapter on human feedback.</p>
    <p class="normal">We now need to extract the relevant information to augment the input.</p>
    <h2 id="_idParaDest-175" class="heading-2">Extracting relevant texts</h2>
    <p class="normal">The following code goes<a id="_idIndexMarker438"/> through the top-ranking vectors, searches for the matching text metadata, and combines the content to prepare the augmentation phase:</p>
    <pre class="programlisting code"><code class="hljs-code">relevant_texts = [match['metadata']['text'] for match in query_results['matches'] if 'metadata' in match and 'text' in match['metadata']]
# Join all items in the list into a single string separated by a specific delimiter (e.g., a newline or space)
combined_text = '\n'.join(relevant_texts)  # Using newline as a separator for readability
print(combined_text)
</code></pre>
    <p class="normal">The output displays <code class="inlineCode">combined_text</code>, relevant text we need to augment the input:</p>
    <pre class="programlisting con"><code class="hljs-con">CustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Balance: 0.0 NumOfProducts: 2 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 54706.75 Exited: 0 Complain: 0 Satisfaction Score: 3 Card Type: DIAMOND Point Earned: 852
</code></pre>
    <p class="normal">We are now ready to augment the <a id="_idIndexMarker439"/>prompt before AI generation.</p>
    <h2 id="_idParaDest-176" class="heading-2">Augmented prompt</h2>
    <p class="normal">We will now engineer our prompt<a id="_idIndexMarker440"/> by adding three texts:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">query_prompt</code>: The instructions for the generative AI model</li>
      <li class="bulletList"><code class="inlineCode">query_text</code>: The target vector containing the target profile chosen by the marketing team</li>
      <li class="bulletList"><code class="inlineCode">combined_context</code>: The concentrated metadata text of the similar vectors selected by the query</li>
    </ul>
    <p class="normal"><code class="inlineCode">itext</code> contains these three variables:</p>
    <pre class="programlisting code"><code class="hljs-code"># Combine texts into a single string, separated by new lines
combined_context = "\n".join(relevant_texts)
#prompt
query_prompt="I have this customer bank record with interesting information on age, credit score and more and similar customers. What could I suggest to keep them in my bank in an email with an url to get new advantages based on the fields for each Customer ID:"
itext=query_prompt+ query_text+combined_context
# Augmented input
print("Prompt for the Generative AI model:", itext)
</code></pre>
    <p class="normal">The output is the core input for the generative AI model:</p>
    <pre class="programlisting con"><code class="hljs-con">Prompt for GPT-4: I have this customer bank record with interesting information on age, credit score and more and similar customers. What could I suggest to keep them in my bank in an email with an url to get new advantages based on the fields for each Customer ID:…
</code></pre>
    <p class="normal">We can now prepare the request for the generative AI model.</p>
    <h2 id="_idParaDest-177" class="heading-2">Augmented generation</h2>
    <p class="normal">In this section, we will submit the<a id="_idIndexMarker441"/> augmented input to an OpenAI generative AI model. The goal is to obtain a customized email to send the customers in the Pinecone index marketing segment we obtained through the target vector.</p>
    <p class="normal">We will first create an OpenAI client and choose GPT-4o as the generative AI model:</p>
    <pre class="programlisting code"><code class="hljs-code">from openai import OpenAI
client = OpenAI()
gpt_model = "gpt-4o
</code></pre>
    <p class="normal">We then introduce a time performance measurement:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
start_time = time.time()  # Start timing before the request
</code></pre>
    <p class="normal">The response time should be<a id="_idIndexMarker442"/> relatively constant since we are only sending one request at a time in this scenario. We now begin to create our completion request:</p>
    <pre class="programlisting code"><code class="hljs-code">response = client.chat.completions.create(
  model=gpt_model,
  messages=[
</code></pre>
    <p class="normal">The system role provides general instructions to the model:</p>
    <pre class="programlisting code"><code class="hljs-code">    {
      "role": "system",
      "content": "You are the community manager can write engaging email based on the text you have. Do not use a surname but simply Dear Valued Customer instead."
    },
</code></pre>
    <p class="normal">The user role contains the engineered <code class="inlineCode">itext</code> prompt we designed:</p>
    <pre class="programlisting code"><code class="hljs-code">    {
      "role": "user",
      "content": itext
    }
  ],
</code></pre>
    <p class="normal">Now, we set the parameters for the request:</p>
    <pre class="programlisting code"><code class="hljs-code">  temperature=0,
  max_tokens=300,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0
)
</code></pre>
    <p class="normal">The parameters are designed to obtain a low random yet “creative” output:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">temperature=0</code>: Low randomness in response</li>
      <li class="bulletList"><code class="inlineCode">max_tokens=300</code>: Limits response length to 300 tokens</li>
      <li class="bulletList"><code class="inlineCode">top_p=1</code>: Considers all possible tokens; full diversity</li>
      <li class="bulletList"><code class="inlineCode">frequency_penalty=0</code>: No penalty for frequent word repetition to allow the response to remain open</li>
      <li class="bulletList"><code class="inlineCode">presence_penalty=0</code>: No penalty for introducing new topics to allow the response to find ideas for our prompt</li>
    </ul>
    <p class="normal">We send the request and <a id="_idIndexMarker443"/>display the response:</p>
    <pre class="programlisting code"><code class="hljs-code">print(response.choices[0].message.content)
</code></pre>
    <p class="normal">The output is satisfactory for this market segment:</p>
    <pre class="programlisting con"><code class="hljs-con">Subject: Exclusive Benefits Await You at Our Bank!
Dear Valued Customer,
We hope this email finds you well. At our bank, we are constantly striving to enhance your banking experience and provide you with the best possible services. We have noticed that you are a valued customer with a DIAMOND card, and we would like to offer you some exclusive benefits tailored just for you!
Based on your profile, we have identified several opportunities that could enhance your banking experience:
1. **Personalized Financial Advice**: Our financial advisors are available to help you make the most of your finances. Whether it's planning for the future or managing your current assets, we are here to assist you.
2. **Exclusive Rewards and Offers**: As a DIAMOND cardholder, you are eligible for special rewards and offers. Earn more points and enjoy exclusive discounts on various products and services.
3. **Enhanced Credit Options**: With your current credit score, you may qualify for better credit options. We can help you explore these opportunities to improve your financial standing.
4. **Complimentary Financial Health Check**: We understand the importance of financial well-being. Schedule a complimentary financial health check to ensure you are on the right track.
5. **Loyalty Programs**: Participate in our loyalty programs and earn more points for every transaction. Redeem these points for exciting rewards and benefits.
To explore these new advantages and more, please visit the following link: [Exclusive Benefits](https://www.yourbank
</code></pre>
    <p class="normal">Since the goal of the marketing team is to convince customers not to leave and to increase their loyalty to <a id="_idIndexMarker444"/>the bank, I’d say the email we received as output is good enough. Let’s display the time it took to obtain a response:</p>
    <pre class="programlisting code"><code class="hljs-code">response_time = time.time() - start_time              # Measure response time
print(f"Querying response time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The response time is displayed:</p>
    <pre class="programlisting con"><code class="hljs-con">Querying response time: 2.83 seconds
</code></pre>
    <p class="normal">We have successfully produced a customized response based on a target vector. This approach might be sufficient for some projects, whatever the domain. Let’s summarize the RAG-driven generative recommendation system built in this chapter and continue our journey.</p>
    <h1 id="_idParaDest-178" class="heading-1">Summary</h1>
    <p class="normal">This chapter aimed to develop a scaled RAG-driven generative AI recommendation system using a Pinecone index and OpenAI models tailored to mitigate bank customer churn. Using a Kaggle dataset, we demonstrated the process of identifying and addressing factors leading to customer dissatisfaction and account closures. Our approach involved three key pipelines.</p>
    <p class="normal">When building <em class="italic">Pipeline 1</em>, we streamlined the dataset by removing non-essential columns, reducing both data complexity and storage costs. Through EDA, we discovered a strong correlation between customer complaints and account closures, which a k-means clustering model further validated. We then designed <em class="italic">Pipeline 2</em> to prepare our RAG-driven system to generate personalized recommendations. We processed data chunks with an OpenAI model, embedding these into a Pinecone index. Pinecone’s consistent upsert capabilities ensured efficient data handling, regardless of volume. Finally, we built <em class="italic">Pipeline 3</em> to leverage over 1,000,000 vectors within Pinecone to target specific market segments with tailored offers, aiming to boost loyalty and reduce attrition. Using GPT-4o, we augmented our queries to generate compelling recommendations.</p>
    <p class="normal">The successful application of a targeted vector representing a key market segment illustrated our system’s potential to craft impactful customer retention strategies. However, we can improve the recommendations by expanding the Pinecone index into a multimodal knowledge base, which we will implement in the next chapter.</p>
    <h1 id="_idParaDest-179" class="heading-1">Questions</h1>
    <ol>
      <li class="numberedList" value="1">Does using a Kaggle dataset typically involve downloading and processing real-world data for analysis?</li>
      <li class="numberedList">Is Pinecone capable of efficiently managing large-scale vector storage for AI applications?</li>
      <li class="numberedList">Can k-means clustering help validate relationships between features such as customer complaints and churn?</li>
      <li class="numberedList">Does leveraging over a million vectors in a database hinder the ability to personalize customer interactions?</li>
      <li class="numberedList">Is the primary objective of using generative AI in business applications to automate and improve decision-making processes?</li>
      <li class="numberedList">Are lightweight development environments advantageous for rapid prototyping and application development?</li>
      <li class="numberedList">Can Pinecone’s architecture automatically scale to accommodate increasing data loads without manual intervention?</li>
      <li class="numberedList">Is generative AI typically employed to create dynamic content and recommendations based on user data?</li>
      <li class="numberedList">Does the integration of AI technologies like Pinecone and OpenAI require significant manual configuration and maintenance?</li>
      <li class="numberedList">Are projects that use vector databases and AI expected to effectively handle complex queries and large datasets?</li>
    </ol>
    <h1 id="_idParaDest-180" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Pinecone documentation: <a href="https://docs.pinecone.io/guides/get-started/quickstart">https://docs.pinecone.io/guides/get-started/quickstart</a></li>
      <li class="bulletList">OpenAI embedding and generative models: <a href="https://platform.openai.com/docs/models">https://platform.openai.com/docs/models</a></li>
    </ul>
    <h1 id="_idParaDest-181" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Han, Y., Liu, C., &amp; Wang, P. (2023). <em class="italic">A comprehensive survey on vector database: Storage and retrieval technique, challenge</em>.</li>
    </ul>
    <h1 id="_idParaDest-182" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>