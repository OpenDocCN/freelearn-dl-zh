<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer026">
			<h1 id="_idParaDest-140" class="chapter-number"><a id="_idTextAnchor181"/>11</h1>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor182"/>Fine-Tuning</h1>
			<p>In this design pattern, you’ll learn about effective strategies for <strong class="bold">fine-tuning</strong> pre-trained <span class="No-Break">language models.</span></p>
			<p>Fine-tuning LLMs addresses a fundamental optimization problem in transfer learning: Pre-training on large datasets helps LLMs learn general language skills and knowledge, but the differences between the pre-training data and the data for specific tasks can reduce performance. Fine-tuning uses a smaller, carefully chosen dataset for the task to update the model, making it better suited to the task’s needs. This process retains useful knowledge from pre-training while refining the model’s ability to perform effectively on the <span class="No-Break">target task.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Implementing transfer learning <span class="No-Break">and fine-tuning</span></li>
				<li>Strategies for freezing and <span class="No-Break">unfreezing layers</span></li>
				<li>Learning <span class="No-Break">rate scheduling</span></li>
				<li><span class="No-Break">Domain-specific techniques</span></li>
				<li>Few-shot and <span class="No-Break">zero-shot fine-tuning</span></li>
				<li>Continual fine-tuning and <span class="No-Break">catastrophic forgetting</span></li>
			</ul>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor183"/>Implementing transfer learning and fine-tuning</h1>
			<p>We will use the following code blocks to <a id="_idIndexMarker558"/>demonstrate transfer learning with GPT-2, handling model initialization, data processing, and the fine-tuning workflow. We<a id="_idIndexMarker559"/> will use the Transformers library and WikiText dataset to fine-tune a pre-trained <span class="No-Break">language model:</span></p>
			<ol>
				<li>First, we load and initialize the GPT-2 model and tokenizer with <span class="No-Break">configured padding:</span><pre class="source-code">
def load_model_and_tokenizer(model_name="gpt2"):
    model = GPT2LMHeadModel.from_pretrained(model_name)
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer</pre></li>				<li>Then, the following <a id="_idIndexMarker560"/>code block manages dataset loading and<a id="_idIndexMarker561"/> text tokenization with a sequence length <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">512</strong></span><span class="No-Break">:</span><pre class="source-code">
def prepare_dataset(dataset_name="wikitext",
    dataset_config="wikitext-2-raw-v1"
):
    dataset = load_dataset(dataset_name, dataset_config)
    return dataset
def tokenize_function(examples, tokenizer):
    return tokenizer(
        examples["text"], truncation=True,
        padding="max_length", max_length=512)</pre></li>				<li>Finally, we set up our training configuration, initialize the trainer, and <span class="No-Break">execute fine-tuning:</span><pre class="source-code">
def fine_tune_lm(model, tokenizer,
    dataset, output_dir="./fine_tuned_model"
):
    tokenized_dataset = dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True)
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir="./logs",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation"],
    )
    trainer.train()
    trainer.save_model()</pre><p class="list-inset">The code sets up a <strong class="source-inline">fine_tune_lm</strong> function that prepares and executes language model fine-tuning. It first<a id="_idIndexMarker562"/> tokenizes the dataset using batched processing, then<a id="_idIndexMarker563"/> configures training parameters including epochs, batch sizes, warmup steps, and weight decay. Next, it initializes a trainer with the model, arguments, and datasets, runs the training process, and finally saves the <span class="No-Break">fine-tuned model.</span></p></li>			</ol>
			<p>Batch size has a significant impact on both training stability and performance. Larger batch sizes allow for more parallelization and faster training on capable hardware but require more memory. They can provide more stable gradient estimates by averaging over more examples, potentially leading to better convergence. However, very large batches may generalize poorly compared to smaller ones, as they can cause the model to converge to sharper minima. Smaller batch <a id="_idIndexMarker564"/>sizes introduce more noise in gradient updates, which can help escape<a id="_idIndexMarker565"/> local minima and potentially find better solutions, but training takes longer. Finding the optimal batch size involves balancing hardware constraints, convergence stability, and generalization performance for your specific model <span class="No-Break">and dataset.</span></p>
			<p>When fine-tuning LLMs, we often don’t need to update all the model’s parameters. Selectively <strong class="bold">freezing</strong> and <strong class="bold">unfreezing</strong> layers can lead to more efficient and <span class="No-Break">effective fine-tuning.</span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor184"/>Strategies for freezing and unfreezing layers</h1>
			<p>The idea behind selectively freezing and unfreezing layers is rooted in how knowledge is structured and distributed across a deep neural network. Lower layers in LLMs tend to capture more general language<a id="_idIndexMarker566"/> representations—such as syntax, part-of-speech, and morphology—while higher layers are more specialized and task-dependent. This hierarchical <a id="_idIndexMarker567"/>organization allows us to leverage the general-purpose linguistic knowledge already encoded in the early layers while fine-tuning only the task-specific portions of <span class="No-Break">the network.</span></p>
			<p>By freezing the lower layers, we preserve their pre-trained capabilities and prevent catastrophic forgetting, which can occur if the entire model is updated indiscriminately on a narrow domain dataset. This also drastically reduces the number of trainable parameters, leading to lower memory usage and faster convergence. Meanwhile, selectively unfreezing the upper layers allows the model to adapt its representations for new tasks or domains without disturbing its core language <span class="No-Break">understanding capabilities.</span></p>
			<p>Let’s look at how we can <span class="No-Break">implement this:</span></p>
			<ol>
				<li>First, we implement selective layer freezing by disabling gradients for all layers except the specified number of <span class="No-Break">final layers:</span><pre class="source-code">
def freeze_layers(model, num_layers_to_freeze):
    for param in model.base_model.parameters():
        param.requires_grad = False
    for i, layer in enumerate(model.base_model.transformer.h):
        if i &gt;= len(model.base_model.transformer.h) -\
            num_layers_to_freeze:
            for param in layer.parameters():
                param.requires_grad = True</pre></li>				<li>Then, we manage<a id="_idIndexMarker568"/> progressive layer unfreezing across <span class="No-Break">training</span><span class="No-Break"><a id="_idIndexMarker569"/></span><span class="No-Break"> epochs:</span><pre class="source-code">
def gradual_unfreeze(model, trainer, num_epochs, total_layers):
    layers_per_epoch = total_layers // num_epochs
    for epoch in range(num_epochs):
        freeze_layers(model, (epoch + 1) * layers_per_epoch)
        trainer.train(resume_from_checkpoint=True)</pre></li>				<li>Finally, we configure optimized training parameters for the gradual <span class="No-Break">unfreezing process:</span><pre class="source-code">
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",
    num_train_epochs=5,  # Increased epochs for better learning
    per_device_train_batch_size=16,  # Larger batch size
    per_device_eval_batch_size=16,
    warmup_steps=1000,  # More warmup steps
    learning_rate=2e-5,  # Added learning rate
    weight_decay=0.1,  # Increased weight decay
    logging_dir="./logs",
    save_steps=500,  # Added save frequency
    eval_steps=500   # Added evaluation frequency
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
)</pre></li>			</ol>
			<p>This implementation introduces<a id="_idIndexMarker570"/> two <span class="No-Break">key strategies:</span></p>
			<ul>
				<li><strong class="source-inline">freeze_layers</strong>: This function<a id="_idIndexMarker571"/> freezes all layers except for the <span class="No-Break">last </span><span class="No-Break"><strong class="source-inline">num_layers_to_freeze</strong></span></li>
				<li><strong class="source-inline">gradual_unfreeze</strong>: This function gradually unfreezes layers over the course <span class="No-Break">of training</span></li>
			</ul>
			<p>The gradual unfreezing approach allows the model to adapt its higher-level features first, then progressively fine-tune lower-level features. This can lead to better performance and help prevent <span class="No-Break">catastrophic forgetting.</span></p>
			<p>Catastrophic forgetting is reduced because of the <span class="No-Break">following reasons:</span></p>
			<ul>
				<li>Layer freezing preserves<a id="_idIndexMarker572"/> knowledge in earlier layers by disabling gradient updates for them, maintaining the fundamental representations learned during pre-training while only adapting task-specific later layers. This retains the model’s general knowledge while allowing adaptation to <span class="No-Break">new tasks.</span></li>
				<li>Gradual unfreezing implements a staged approach where training begins with only the final layers unfrozen (which contain more task-specific representations), then progressively unfreezes earlier layers. This allows the model to adapt higher-level features first before making more fundamental changes, providing a gentle transition that helps maintain previously <span class="No-Break">learned patterns.</span></li>
				<li>The training configuration supports these approaches with a carefully balanced learning rate, increased <a id="_idIndexMarker573"/>warmup steps, and higher weight decay that further prevents drastic parameter shifts. The increased epochs allow for more gradual adaptation while save <a id="_idIndexMarker574"/>and evaluation checkpoints provide monitoring<a id="_idIndexMarker575"/> to prevent overfitting during the <span class="No-Break">unfreezing process.</span></li>
			</ul>
			<p>Together, these techniques create a more controlled fine-tuning process that preserves general knowledge while effectively adapting to <span class="No-Break">new tasks.</span></p>
			<p>Finetuning performance can be significantly improved by applying appropriate learning rate scheduling, which we’ll <span class="No-Break">visit next.</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor185"/>Learning rate scheduling</h1>
			<p>As mentioned, proper <strong class="bold">learning rate scheduling</strong> is often used for effective fine-tuning. The <a id="_idIndexMarker576"/>following code demonstrates common learning rate scheduling techniques for LLM fine-tuning, offering both <strong class="bold">linear</strong> and <strong class="bold">cosine warmup</strong> strategies to <a id="_idIndexMarker577"/><span class="No-Break">optimize training:</span></p>
			<ol>
				<li>First, we set up the scheduling framework with the required imports and <span class="No-Break">function initialization:</span><pre class="source-code">
from transformers import (
    get_linear_schedule_with_warmup,
    get_cosine_schedule_with_warmup)
def fine_tune_with_lr_scheduling(
    model, tokenizer, dataset, scheduler_type="linear",
    num_epochs=3
):
    tokenized_dataset = dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True)</pre></li>				<li>Next, we configure <a id="_idIndexMarker578"/>optimized training parameters with <span class="No-Break">improved defaults:</span><pre class="source-code">
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",
    num_train_epochs=3,
    per_device_train_batch_size=32,  # Increased batch size
    per_device_eval_batch_size=32,
    weight_decay=0.1,  # Increased weight decay
    logging_dir="./logs",
    learning_rate=2e-5,  # Adjusted learning rate
    warmup_ratio=0.1,   # Added warmup ratio
    eval_steps=100,     # Added evaluation frequency
    save_steps=100      # Added save frequency
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
)</pre></li>				<li>Finally, we implement learning rate scheduling with dynamic warmup <span class="No-Break">steps calculation:</span><pre class="source-code">
num_training_steps = len(tokenized_dataset["train"]) //
    training_args.per_device_train_batch_size * num_epochs
if scheduler_type == "linear":
    scheduler = get_linear_schedule_with_warmup(
        trainer.optimizer,
        num_warmup_steps=num_training_steps // 10,  # 10% warmup
        num_training_steps=num_training_steps
    )
elif scheduler_type == "cosine":
    scheduler = get_cosine_schedule_with_warmup(
        trainer.optimizer,
        num_warmup_steps=num_training_steps // 10,  # 10% warmup
        num_training_steps=num_training_steps
    )
else:
    raise ValueError("Unsupported scheduler type")</pre><p class="list-inset">This implementation <a id="_idIndexMarker579"/>provides two common learning rate <span class="No-Break">scheduling strategies:</span></p><ul><li><strong class="bold">Linear schedule with warmup</strong>: The learning rate increases linearly from <strong class="source-inline">0</strong> to the initial <strong class="source-inline">lr</strong> during warmup, then decreases linearly to <strong class="source-inline">0</strong>. We discussed this in <a href="B31249_07.xhtml#_idTextAnchor108"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> under the <em class="italic">Loss functions and optimization strategies</em> section. However, it is important to note that we need to use the same warmup schedule for fine-tuning as well. Warmup helps prevent sudden weight updates early in training, ensuring <span class="No-Break">smoother convergence.</span></li><li><strong class="bold">Cosine schedule with warmup</strong>: Similar to the linear schedule, but in this case, the decrease follows a <span class="No-Break">cosine curve.</span></li></ul></li>			</ol>
			<p>These scheduling strategies<a id="_idIndexMarker580"/> can help stabilize training and potentially lead to <span class="No-Break">better convergence.</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor186"/>Domain-specific fine-tuning techniques</h1>
			<p>When fine-tuning LLMs for specific domains, we often need to adapt our approach. Let’s look at an example of<a id="_idIndexMarker581"/> domain-specific fine-tuning for a scientific corpus. The following code implements domain-specific fine-tuning for scientific text using custom dataset preparation and <span class="No-Break">training configuration:</span></p>
			<ol>
				<li>First, we set up the dataset preparation for scientific text with a specified block size and language <span class="No-Break">modeling collator:</span><pre class="source-code">
import torch
from transformers import (
    TextDataset, DataCollatorForLanguageModeling )
def prepare_scientific_dataset(file_path, tokenizer):
    dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=128,
    )
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False,
    )
    return dataset, data_collator</pre></li>				<li>Next, we handle dataset <a id="_idIndexMarker582"/>preparation for both training <span class="No-Break">and evaluation:</span><pre class="source-code">
def fine_tune_for_scientific_domain(
    model, tokenizer, train_file, eval_file,
    output_dir="./scientific_model"
):
    train_dataset, data_collator =
        prepare_scientific_dataset(train_file, tokenizer)
    eval_dataset, _ = prepare_scientific_dataset(
        eval_file, tokenizer)</pre></li>				<li>Finally, we configure optimized training parameters for scientific <span class="No-Break">domain adaptation:</span><pre class="source-code">
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,            # Reduced epochs
    per_device_train_batch_size=8, # Increased batch size
    per_device_eval_batch_size=8,
    warmup_steps=1000,            # Increased warmup
    weight_decay=0.1,             # Increased weight decay
    learning_rate=3e-5,           # Added learning rate
    logging_dir="./logs",
    evaluation_strategy="steps",   # Changed to steps
    eval_steps=500,               # Added eval frequency
    save_steps=500,               # Added save frequency
    gradient_accumulation_steps=4  # Added gradient accumulation
)</pre></li>			</ol>
			<p>This implementation includes <a id="_idIndexMarker583"/>several <span class="No-Break">domain-specific considerations:</span></p>
			<ul>
				<li><strong class="bold">Custom dataset preparation</strong>: We use <strong class="source-inline">TextDataset</strong> to handle domain-specific <span class="No-Break">text files</span></li>
				<li><strong class="bold">Smaller batch size</strong>: Scientific texts often have longer sequences, so we reduce the <span class="No-Break">batch size</span></li>
				<li><strong class="bold">More epochs</strong>: Domain adaptation might require more <span class="No-Break">training iterations</span></li>
				<li><strong class="bold">Regular evaluation</strong>: After each epoch, we evaluate the model to track validation loss and key domain-specific metrics, ensuring <span class="No-Break">proper adaptation</span></li>
			</ul>
			<p>When fine-tuning for specific domains, consider the <span class="No-Break">following steps:</span></p>
			<ul>
				<li>Adapting the vocabulary for <span class="No-Break">domain-specific terms</span></li>
				<li>Using domain-specific <span class="No-Break">evaluation metrics</span></li>
				<li>Potentially modifying the model architecture for <span class="No-Break">domain-specific features</span></li>
			</ul>
			<p>In the following section, we’ll explore a couple of strategies for fine-tuning models with little to no labeled data from the <span class="No-Break">target domain.</span></p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor187"/>Few-shot and zero-shot fine-tuning</h1>
			<p><strong class="bold">Few-shot</strong> and <strong class="bold">zero-shot learning</strong> are powerful techniques for adapting LLMs to new tasks with minimal or no task-specific<a id="_idIndexMarker584"/> training data. Let’s implement a few-shot<a id="_idIndexMarker585"/> <span class="No-Break">fine-tuning approach:</span></p>
			<ol>
				<li>We create a prompt that includes a few examples of <span class="No-Break">the task:</span><pre class="source-code">
def prepare_few_shot_dataset(examples, tokenizer, num_shots=5):
    few_shot_examples = examples[:num_shots]
    prompt = "\n\n".join(
        [
            f"Input: {ex['input']}\n"
            f"Output: {ex['output']}"
            for ex in few_shot_examples
        ]
    )
    prompt += "\n\nInput: {input}\nOutput:"
    def tokenize_function(example):
        full_prompt = prompt.format(input=example['input'])
        tokenized_prompt = tokenizer(full_prompt,
            truncation=True,
            padding="max_length", max_length=512)
        tokenized_output = tokenizer(
            example['output'], truncation=True,
            padding="max_length", max_length=512)
        tokenized_prompt['labels'] = \
            [-100] * len(tokenized_prompt['input_ids'])
            + tokenized_output['input_ids']
        return tokenized_prompt
    return examples.map(tokenize_function)</pre></li>				<li>The model is<a id="_idIndexMarker586"/> then fine-tuned on<a id="_idIndexMarker587"/> this <span class="No-Break">prompt-based dataset:</span><pre class="source-code">
def few_shot_fine_tune(
    model, tokenizer, dataset, num_shots=5, num_epochs=3
):
    few_shot_dataset = prepare_few_shot_dataset(dataset,
        tokenizer, num_shots)
    training_args = TrainingArguments(
        output_dir="./few_shot_model",
        num_train_epochs=num_epochs,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir="./logs",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=few_shot_dataset,
    )
    trainer.train()
    return trainer</pre><p class="list-inset">The <strong class="source-inline">few_shot_fine_tune</strong> function implements few-shot fine-tuning, which adapts a pre-trained model to new tasks using minimal examples. It takes a model, tokenizer, dataset, and configuration parameters (<strong class="source-inline">num_shots=5</strong>, <strong class="source-inline">num_epochs=3</strong>), then prepares a small subset of the data with <strong class="source-inline">prepare_few_shot_dataset</strong>, configures training with <strong class="source-inline">TrainingArguments</strong> (specifying output locations, batch sizes, and optimization parameters), initializes a <strong class="source-inline">Trainer</strong> object with these components, executes the training process via <strong class="source-inline">trainer.train()</strong>, and finally returns the trained model wrapped in the <strong class="source-inline">Trainer</strong> object—all using the Hugging Face Transformers library framework commonly used for <span class="No-Break">language models.</span></p></li>				<li>The fine-tuned model can then generalize to new instances of <span class="No-Break">the task:</span><pre class="source-code">
# Usage
model, tokenizer = load_model_and_tokenizer()
dataset = load_dataset("your_dataset")  # Load your few-shot dataset
few_shot_trainer = few_shot_fine_tune(model, tokenizer, dataset)</pre></li>			</ol>
			<p>This implementation demonstrates<a id="_idIndexMarker588"/> <span class="No-Break">few-shot fine-tuning.</span></p>
			<p>For zero-shot learning, you would <a id="_idIndexMarker589"/>typically rely on the pre-trained model’s ability to understand task descriptions without any task-specific examples <span class="No-Break">or fine-tuning.</span></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor188"/>Continual fine-tuning and catastrophic forgetting</h1>
			<p><strong class="bold">Continual fine-tuning</strong> involves <a id="_idIndexMarker590"/>adapting a model to new tasks while retaining performance on previous tasks. However, this can lead to <strong class="bold">catastrophic forgetting</strong>. Catastrophic forgetting<a id="_idIndexMarker591"/> in LLMs refers to the phenomenon where a model loses previously learned information when fine-tuned on new tasks or data without appropriate mechanisms to preserve <span class="No-Break">prior knowledge.</span></p>
			<p>Let’s implement a simple strategy to <span class="No-Break">mitigate this:</span></p>
			<ol>
				<li>First, we calculate parameter importance <a id="_idIndexMarker592"/>and implement <strong class="bold">elastic weight consolidation</strong> (<strong class="bold">EWC</strong>) loss for preserving <span class="No-Break">critical weights:</span><pre class="source-code">
import copy
def ewc_loss(model, old_model, importance, loss):
    ewc_lambda = 0.01
    for n, p in model.named_parameters():
        if n in importance:
            loss += ewc_lambda * importance[n]
                * (p - old_model[n]).pow(2).sum()
    return loss
def compute_importance(model, dataset):
    importance = {}
    model.eval()
    for batch in dataset:
        model.zero_grad()
        output = model(batch)
        loss = output.loss
        loss.backward()
        for n, p in model.named_parameters():
            if p.grad is not None:
                if n not in importance:
                    importance[n] = p.grad.data.clone().pow(2)
                else:
                    importance[n] += p.grad.data.clone().pow(2)
    return importance</pre></li>				<li>We then implement the <a id="_idIndexMarker593"/>following code, which manages sequential <a id="_idIndexMarker594"/>training on multiple tasks while maintaining <span class="No-Break">previous knowledge:</span><pre class="source-code">
def continual_fine_tune(
        model, tokenizer, datasets, num_epochs=3
):
    old_model = None
    importance = None
    for i, dataset in enumerate(datasets):
        if old_model is not None:
            importance = compute_importance(
                old_model, datasets[i-1])
        old_model = copy.deepcopy(model)
        tokenized_dataset = dataset.map(
            lambda examples: tokenize_function(examples,
                tokenizer),
            batched=True)</pre></li>				<li>Finally, we define <a id="_idIndexMarker595"/>optimized training parameters for <span class="No-Break">continual </span><span class="No-Break"><a id="_idIndexMarker596"/></span><span class="No-Break">learning:</span><pre class="source-code">
training_args = TrainingArguments(
    output_dir=f"./continual_fine_tuned_model_task_{i+1}",
    num_train_epochs=8,                # Increased epochs
    per_device_train_batch_size=20,    # Increased batch size
    per_device_eval_batch_size=20,
    warmup_steps=2000,                 # Increased warmup
    weight_decay=0.2,                  # Increased weight decay
    learning_rate=2e-5,                # Added learning rate
    logging_dir="./logs",
    evaluation_strategy="steps",    # Added evaluation strategy
    eval_steps=1000,                # Added evaluation frequency
    save_steps=1000                    # Added save frequency
)</pre></li>			</ol>
			<p>This implementation introduces <a id="_idIndexMarker597"/>several key concepts for <span class="No-Break">continual fine-tuning:</span></p>
			<ul>
				<li><strong class="bold">EWC</strong>: We implement a <a id="_idIndexMarker598"/>simplified version of EWC, which adds a penalty term to the loss function to prevent drastic changes to important parameters for <span class="No-Break">previous tasks</span></li>
				<li><strong class="bold">Importance computation</strong>: We calculate the importance of each parameter based on its gradient magnitude on the <span class="No-Break">previous task</span></li>
				<li><strong class="bold">Continual fine-tuning loop</strong>: We fine-tune the model on each task sequentially, using EWC to <span class="No-Break">mitigate forgetting</span></li>
				<li><strong class="bold">Evaluation on all tasks</strong>: After fine-tuning on each new task, we evaluate the model’s performance on all previous tasks to <span class="No-Break">monitor forgetting</span></li>
			</ul>
			<p>The key considerations for continual fine-tuning are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Balance between plasticity and stability</strong>: EWC helps maintain this balance, allowing the model to learn new tasks<a id="_idIndexMarker599"/> while preserving knowledge of <span class="No-Break">previous ones</span></li>
				<li><strong class="bold">Computational overhead</strong>: Computing importance and applying EWC increases the computational cost <span class="No-Break">of training</span></li>
				<li><strong class="bold">Task similarity</strong>: The effectiveness of continual fine-tuning can depend on the similarity <span class="No-Break">between tasks</span></li>
			</ul>
			<p>Additional strategies to consider for mitigating catastrophic forgetting include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Gradient episodic memory</strong> (<strong class="bold">GEM</strong>): In this approach, a small episodic memory of data from<a id="_idIndexMarker600"/> previous tasks is stored and used to constrain the <a id="_idIndexMarker601"/>gradient updates on new tasks, <span class="No-Break">as </span><span class="No-Break"><a id="_idIndexMarker602"/></span><span class="No-Break">follows:</span><pre class="source-code">
def project(gradient, memories):
    for memory in memories:
        if torch.dot(gradient, memory) &lt; 0:
            gradient -= (
                torch.dot(gradient, memory) / torch.dot(
                    memory, memory)
            ) * memory
    return gradient
# This would be integrated into the training loop</pre></li>				<li><strong class="bold">Progressive neural networks</strong>: Here, a new “column” of layers for each new task is created, while lateral connections to previously learned features <span class="No-Break">are maintained.</span></li>
				<li><strong class="bold">Learning without Forgetting (LwF)</strong>: In this approach, knowledge distillation is employed to preserve the<a id="_idIndexMarker603"/> model’s performance on <span class="No-Break">previous tasks:</span><pre class="source-code">
def lwf_loss(
    model, old_model, new_data, old_data, temperature=2
):
    # Compute standard loss on new data
    new_loss = compute_loss(model, new_data)
    # Compute distillation loss on old data
    old_outputs = old_model(old_data)
    new_outputs = model(old_data)
    distillation_loss = F.kl_div(
        F.log_softmax(new_outputs / temperature, dim=1),
        F.softmax(old_outputs / temperature, dim=1),
        reduction='batchmean'
    ) * (temperature  2)
    return new_loss + distillation_loss
# This would replace the standard loss in the training loop</pre></li>			</ul>
			<p>These advanced techniques<a id="_idIndexMarker604"/> can be particularly useful when fine-tuning LL<a id="_idTextAnchor189"/>Ms across a <a id="_idIndexMarker605"/>diverse range of tasks <span class="No-Break">or domains.</span></p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor190"/>Summary</h1>
			<p>Fine-tuning patterns for LLMs encompass a wide range of techniques, from basic transfer learning to advanced continual learning strategies. By mastering these patterns, you can effectively adapt pre-trained models to new tasks and domains, optimize performance, and mitigate issues such as catastrophic forgetting. As the field of LLMs continues to evolve, staying updated with the latest fine-tuning techniques will be crucial for developing state-of-the-art language models tailored to <span class="No-Break">specific applications.</span></p>
			<p>Here are the key takeaways from <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><strong class="bold">Fine-tuning adapts pre-trained LLMs</strong>: Fine-tuning is the key process for adapting general-purpose, pre-trained LLMs to specific tasks and datasets, bridging the gap between general language understanding and <span class="No-Break">specialized performance</span></li>
				<li><strong class="bold">Layer management is crucial</strong>: Strategically freezing and unfreezing layers (especially gradual unfreezing) is critical for balancing the preservation of pre-trained knowledge with adaptation to the <span class="No-Break">new task</span></li>
				<li><strong class="bold">Learning rate scheduling stabilizes training</strong>: Using learning rate schedules with warmup (linear or cosine) is essential for stable and effective fine-tuning, preventing drastic early updates and <span class="No-Break">promoting convergence</span></li>
				<li><strong class="bold">Domain/task specificity matters</strong>: Techniques such as domain-specific vocabulary adaptation, custom data handling, and few-shot/zero-shot approaches are vital for maximizing performance on <span class="No-Break">specialized tasks</span></li>
				<li><strong class="bold">Catastrophic forgetting must be addressed</strong>: In continual learning scenarios, techniques such as EWC, GEM, and others are necessary to prevent the model from losing previously learned information when trained on <span class="No-Break">new tasks</span></li>
			</ul>
			<p>We will explore model pruning in the next chapter. Model pruning systematically removes redundant or less important neural connections in LLMs while preserving core functionality, essentially creating a lighter, more efficient version that maintains similar performance but requires fewer <span class="No-Break">computational resources.</span></p>
		</div>
	</div></div></body></html>