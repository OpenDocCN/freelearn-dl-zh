<html><head></head><body><div><div><div><h1 id="_idParaDest-140" class="chapter-number"><a id="_idTextAnchor181"/>11</h1>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor182"/>Fine-Tuning</h1>
			<p>In this design pattern, you’ll learn about effective strategies for <strong class="bold">fine-tuning</strong> pre-trained language models.</p>
			<p>Fine-tuning LLMs addresses a fundamental optimization problem in transfer learning: Pre-training on large datasets helps LLMs learn general language skills and knowledge, but the differences between the pre-training data and the data for specific tasks can reduce performance. Fine-tuning uses a smaller, carefully chosen dataset for the task to update the model, making it better suited to the task’s needs. This process retains useful knowledge from pre-training while refining the model’s ability to perform effectively on the target task.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Implementing transfer learning and fine-tuning</li>
				<li>Strategies for freezing and unfreezing layers</li>
				<li>Learning rate scheduling</li>
				<li>Domain-specific techniques</li>
				<li>Few-shot and zero-shot fine-tuning</li>
				<li>Continual fine-tuning and catastrophic forgetting</li>
			</ul>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor183"/>Implementing transfer learning and fine-tuning</h1>
			<p>We will use the following code blocks to <a id="_idIndexMarker558"/>demonstrate transfer learning with GPT-2, handling model initialization, data processing, and the fine-tuning workflow. We<a id="_idIndexMarker559"/> will use the Transformers library and WikiText dataset to fine-tune a pre-trained language model:</p>
			<ol>
				<li>First, we load and initialize the GPT-2 model and tokenizer with configured padding:<pre class="source-code">
def load_model_and_tokenizer(model_name="gpt2"):
    model = GPT2LMHeadModel.from_pretrained(model_name)
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    return model, tokenizer</pre></li>				<li>Then, the following <a id="_idIndexMarker560"/>code block manages dataset loading and<a id="_idIndexMarker561"/> text tokenization with a sequence length of <code>512</code>:<pre class="source-code">
def prepare_dataset(dataset_name="wikitext",
    dataset_config="wikitext-2-raw-v1"
):
    dataset = load_dataset(dataset_name, dataset_config)
    return dataset
def tokenize_function(examples, tokenizer):
    return tokenizer(
        examples["text"], truncation=True,
        padding="max_length", max_length=512)</pre></li>				<li>Finally, we set up our training configuration, initialize the trainer, and execute fine-tuning:<pre class="source-code">
def fine_tune_lm(model, tokenizer,
    dataset, output_dir="./fine_tuned_model"
):
    tokenized_dataset = dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True)
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir="./logs",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation"],
    )
    trainer.train()
    trainer.save_model()</pre><p class="list-inset">The code sets up a <code>fine_tune_lm</code> function that prepares and executes language model fine-tuning. It first<a id="_idIndexMarker562"/> tokenizes the dataset using batched processing, then<a id="_idIndexMarker563"/> configures training parameters including epochs, batch sizes, warmup steps, and weight decay. Next, it initializes a trainer with the model, arguments, and datasets, runs the training process, and finally saves the fine-tuned model.</p></li>			</ol>
			<p>Batch size has a significant impact on both training stability and performance. Larger batch sizes allow for more parallelization and faster training on capable hardware but require more memory. They can provide more stable gradient estimates by averaging over more examples, potentially leading to better convergence. However, very large batches may generalize poorly compared to smaller ones, as they can cause the model to converge to sharper minima. Smaller batch <a id="_idIndexMarker564"/>sizes introduce more noise in gradient updates, which can help escape<a id="_idIndexMarker565"/> local minima and potentially find better solutions, but training takes longer. Finding the optimal batch size involves balancing hardware constraints, convergence stability, and generalization performance for your specific model and dataset.</p>
			<p>When fine-tuning LLMs, we often don’t need to update all the model’s parameters. Selectively <strong class="bold">freezing</strong> and <strong class="bold">unfreezing</strong> layers can lead to more efficient and effective fine-tuning.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor184"/>Strategies for freezing and unfreezing layers</h1>
			<p>The idea behind selectively freezing and unfreezing layers is rooted in how knowledge is structured and distributed across a deep neural network. Lower layers in LLMs tend to capture more general language<a id="_idIndexMarker566"/> representations—such as syntax, part-of-speech, and morphology—while higher layers are more specialized and task-dependent. This hierarchical <a id="_idIndexMarker567"/>organization allows us to leverage the general-purpose linguistic knowledge already encoded in the early layers while fine-tuning only the task-specific portions of the network.</p>
			<p>By freezing the lower layers, we preserve their pre-trained capabilities and prevent catastrophic forgetting, which can occur if the entire model is updated indiscriminately on a narrow domain dataset. This also drastically reduces the number of trainable parameters, leading to lower memory usage and faster convergence. Meanwhile, selectively unfreezing the upper layers allows the model to adapt its representations for new tasks or domains without disturbing its core language understanding capabilities.</p>
			<p>Let’s look at how we can implement this:</p>
			<ol>
				<li>First, we implement selective layer freezing by disabling gradients for all layers except the specified number of final layers:<pre class="source-code">
def freeze_layers(model, num_layers_to_freeze):
    for param in model.base_model.parameters():
        param.requires_grad = False
    for i, layer in enumerate(model.base_model.transformer.h):
        if i &gt;= len(model.base_model.transformer.h) -\
            num_layers_to_freeze:
            for param in layer.parameters():
                param.requires_grad = True</pre></li>				<li>Then, we manage<a id="_idIndexMarker568"/> progressive layer unfreezing across training<a id="_idIndexMarker569"/> epochs:<pre class="source-code">
def gradual_unfreeze(model, trainer, num_epochs, total_layers):
    layers_per_epoch = total_layers // num_epochs
    for epoch in range(num_epochs):
        freeze_layers(model, (epoch + 1) * layers_per_epoch)
        trainer.train(resume_from_checkpoint=True)</pre></li>				<li>Finally, we configure optimized training parameters for the gradual unfreezing process:<pre class="source-code">
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",
    num_train_epochs=5,  # Increased epochs for better learning
    per_device_train_batch_size=16,  # Larger batch size
    per_device_eval_batch_size=16,
    warmup_steps=1000,  # More warmup steps
    learning_rate=2e-5,  # Added learning rate
    weight_decay=0.1,  # Increased weight decay
    logging_dir="./logs",
    save_steps=500,  # Added save frequency
    eval_steps=500   # Added evaluation frequency
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
)</pre></li>			</ol>
			<p>This implementation introduces<a id="_idIndexMarker570"/> two key strategies:</p>
			<ul>
				<li><code>freeze_layers</code>: This function<a id="_idIndexMarker571"/> freezes all layers except for the last <code>num_layers_to_freeze</code></li>
				<li><code>gradual_unfreeze</code>: This function gradually unfreezes layers over the course of training</li>
			</ul>
			<p>The gradual unfreezing approach allows the model to adapt its higher-level features first, then progressively fine-tune lower-level features. This can lead to better performance and help prevent catastrophic forgetting.</p>
			<p>Catastrophic forgetting is reduced because of the following reasons:</p>
			<ul>
				<li>Layer freezing preserves<a id="_idIndexMarker572"/> knowledge in earlier layers by disabling gradient updates for them, maintaining the fundamental representations learned during pre-training while only adapting task-specific later layers. This retains the model’s general knowledge while allowing adaptation to new tasks.</li>
				<li>Gradual unfreezing implements a staged approach where training begins with only the final layers unfrozen (which contain more task-specific representations), then progressively unfreezes earlier layers. This allows the model to adapt higher-level features first before making more fundamental changes, providing a gentle transition that helps maintain previously learned patterns.</li>
				<li>The training configuration supports these approaches with a carefully balanced learning rate, increased <a id="_idIndexMarker573"/>warmup steps, and higher weight decay that further prevents drastic parameter shifts. The increased epochs allow for more gradual adaptation while save <a id="_idIndexMarker574"/>and evaluation checkpoints provide monitoring<a id="_idIndexMarker575"/> to prevent overfitting during the unfreezing process.</li>
			</ul>
			<p>Together, these techniques create a more controlled fine-tuning process that preserves general knowledge while effectively adapting to new tasks.</p>
			<p>Finetuning performance can be significantly improved by applying appropriate learning rate scheduling, which we’ll visit next.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor185"/>Learning rate scheduling</h1>
			<p>As mentioned, proper <strong class="bold">learning rate scheduling</strong> is often used for effective fine-tuning. The <a id="_idIndexMarker576"/>following code demonstrates common learning rate scheduling techniques for LLM fine-tuning, offering both <strong class="bold">linear</strong> and <strong class="bold">cosine warmup</strong> strategies to <a id="_idIndexMarker577"/>optimize training:</p>
			<ol>
				<li>First, we set up the scheduling framework with the required imports and function initialization:<pre class="source-code">
from transformers import (
    get_linear_schedule_with_warmup,
    get_cosine_schedule_with_warmup)
def fine_tune_with_lr_scheduling(
    model, tokenizer, dataset, scheduler_type="linear",
    num_epochs=3
):
    tokenized_dataset = dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True)</pre></li>				<li>Next, we configure <a id="_idIndexMarker578"/>optimized training parameters with improved defaults:<pre class="source-code">
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",
    num_train_epochs=3,
    per_device_train_batch_size=32,  # Increased batch size
    per_device_eval_batch_size=32,
    weight_decay=0.1,  # Increased weight decay
    logging_dir="./logs",
    learning_rate=2e-5,  # Adjusted learning rate
    warmup_ratio=0.1,   # Added warmup ratio
    eval_steps=100,     # Added evaluation frequency
    save_steps=100      # Added save frequency
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
)</pre></li>				<li>Finally, we implement learning rate scheduling with dynamic warmup steps calculation:<pre class="source-code">
num_training_steps = len(tokenized_dataset["train"]) //
    training_args.per_device_train_batch_size * num_epochs
if scheduler_type == "linear":
    scheduler = get_linear_schedule_with_warmup(
        trainer.optimizer,
        num_warmup_steps=num_training_steps // 10,  # 10% warmup
        num_training_steps=num_training_steps
    )
elif scheduler_type == "cosine":
    scheduler = get_cosine_schedule_with_warmup(
        trainer.optimizer,
        num_warmup_steps=num_training_steps // 10,  # 10% warmup
        num_training_steps=num_training_steps
    )
else:
    raise ValueError("Unsupported scheduler type")</pre><p class="list-inset">This implementation <a id="_idIndexMarker579"/>provides two common learning rate scheduling strategies:</p><ul><li><code>0</code> to the initial <code>lr</code> during warmup, then decreases linearly to <code>0</code>. We discussed this in <a href="B31249_07.xhtml#_idTextAnchor108"><em class="italic">Chapter 7</em></a> under the <em class="italic">Loss functions and optimization strategies</em> section. However, it is important to note that we need to use the same warmup schedule for fine-tuning as well. Warmup helps prevent sudden weight updates early in training, ensuring smoother convergence.</li><li><strong class="bold">Cosine schedule with warmup</strong>: Similar to the linear schedule, but in this case, the decrease follows a cosine curve.</li></ul></li>			</ol>
			<p>These scheduling strategies<a id="_idIndexMarker580"/> can help stabilize training and potentially lead to better convergence.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor186"/>Domain-specific fine-tuning techniques</h1>
			<p>When fine-tuning LLMs for specific domains, we often need to adapt our approach. Let’s look at an example of<a id="_idIndexMarker581"/> domain-specific fine-tuning for a scientific corpus. The following code implements domain-specific fine-tuning for scientific text using custom dataset preparation and training configuration:</p>
			<ol>
				<li>First, we set up the dataset preparation for scientific text with a specified block size and language modeling collator:<pre class="source-code">
import torch
from transformers import (
    TextDataset, DataCollatorForLanguageModeling )
def prepare_scientific_dataset(file_path, tokenizer):
    dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=128,
    )
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False,
    )
    return dataset, data_collator</pre></li>				<li>Next, we handle dataset <a id="_idIndexMarker582"/>preparation for both training and evaluation:<pre class="source-code">
def fine_tune_for_scientific_domain(
    model, tokenizer, train_file, eval_file,
    output_dir="./scientific_model"
):
    train_dataset, data_collator =
        prepare_scientific_dataset(train_file, tokenizer)
    eval_dataset, _ = prepare_scientific_dataset(
        eval_file, tokenizer)</pre></li>				<li>Finally, we configure optimized training parameters for scientific domain adaptation:<pre class="source-code">
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,            # Reduced epochs
    per_device_train_batch_size=8, # Increased batch size
    per_device_eval_batch_size=8,
    warmup_steps=1000,            # Increased warmup
    weight_decay=0.1,             # Increased weight decay
    learning_rate=3e-5,           # Added learning rate
    logging_dir="./logs",
    evaluation_strategy="steps",   # Changed to steps
    eval_steps=500,               # Added eval frequency
    save_steps=500,               # Added save frequency
    gradient_accumulation_steps=4  # Added gradient accumulation
)</pre></li>			</ol>
			<p>This implementation includes <a id="_idIndexMarker583"/>several domain-specific considerations:</p>
			<ul>
				<li><code>TextDataset</code> to handle domain-specific text files</li>
				<li><strong class="bold">Smaller batch size</strong>: Scientific texts often have longer sequences, so we reduce the batch size</li>
				<li><strong class="bold">More epochs</strong>: Domain adaptation might require more training iterations</li>
				<li><strong class="bold">Regular evaluation</strong>: After each epoch, we evaluate the model to track validation loss and key domain-specific metrics, ensuring proper adaptation</li>
			</ul>
			<p>When fine-tuning for specific domains, consider the following steps:</p>
			<ul>
				<li>Adapting the vocabulary for domain-specific terms</li>
				<li>Using domain-specific evaluation metrics</li>
				<li>Potentially modifying the model architecture for domain-specific features</li>
			</ul>
			<p>In the following section, we’ll explore a couple of strategies for fine-tuning models with little to no labeled data from the target domain.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor187"/>Few-shot and zero-shot fine-tuning</h1>
			<p><strong class="bold">Few-shot</strong> and <strong class="bold">zero-shot learning</strong> are powerful techniques for adapting LLMs to new tasks with minimal or no task-specific<a id="_idIndexMarker584"/> training data. Let’s implement a few-shot<a id="_idIndexMarker585"/> fine-tuning approach:</p>
			<ol>
				<li>We create a prompt that includes a few examples of the task:<pre class="source-code">
def prepare_few_shot_dataset(examples, tokenizer, num_shots=5):
    few_shot_examples = examples[:num_shots]
    prompt = "\n\n".join(
        [
            f"Input: {ex['input']}\n"
            f"Output: {ex['output']}"
            for ex in few_shot_examples
        ]
    )
    prompt += "\n\nInput: {input}\nOutput:"
    def tokenize_function(example):
        full_prompt = prompt.format(input=example['input'])
        tokenized_prompt = tokenizer(full_prompt,
            truncation=True,
            padding="max_length", max_length=512)
        tokenized_output = tokenizer(
            example['output'], truncation=True,
            padding="max_length", max_length=512)
        tokenized_prompt['labels'] = \
            [-100] * len(tokenized_prompt['input_ids'])
            + tokenized_output['input_ids']
        return tokenized_prompt
    return examples.map(tokenize_function)</pre></li>				<li>The model is<a id="_idIndexMarker586"/> then fine-tuned on<a id="_idIndexMarker587"/> this prompt-based dataset:<pre class="source-code">
def few_shot_fine_tune(
    model, tokenizer, dataset, num_shots=5, num_epochs=3
):
    few_shot_dataset = prepare_few_shot_dataset(dataset,
        tokenizer, num_shots)
    training_args = TrainingArguments(
        output_dir="./few_shot_model",
        num_train_epochs=num_epochs,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir="./logs",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=few_shot_dataset,
    )
    trainer.train()
    return trainer</pre><p class="list-inset">The <code>few_shot_fine_tune</code> function implements few-shot fine-tuning, which adapts a pre-trained model to new tasks using minimal examples. It takes a model, tokenizer, dataset, and configuration parameters (<code>num_shots=5</code>, <code>num_epochs=3</code>), then prepares a small subset of the data with <code>prepare_few_shot_dataset</code>, configures training with <code>TrainingArguments</code> (specifying output locations, batch sizes, and optimization parameters), initializes a <code>Trainer</code> object with these components, executes the training process via <code>trainer.train()</code>, and finally returns the trained model wrapped in the <code>Trainer</code> object—all using the Hugging Face Transformers library framework commonly used for language models.</p></li>				<li>The fine-tuned model can then generalize to new instances of the task:<pre class="source-code">
# Usage
model, tokenizer = load_model_and_tokenizer()
dataset = load_dataset("your_dataset")  # Load your few-shot dataset
few_shot_trainer = few_shot_fine_tune(model, tokenizer, dataset)</pre></li>			</ol>
			<p>This implementation demonstrates<a id="_idIndexMarker588"/> few-shot fine-tuning.</p>
			<p>For zero-shot learning, you would <a id="_idIndexMarker589"/>typically rely on the pre-trained model’s ability to understand task descriptions without any task-specific examples or fine-tuning.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor188"/>Continual fine-tuning and catastrophic forgetting</h1>
			<p><strong class="bold">Continual fine-tuning</strong> involves <a id="_idIndexMarker590"/>adapting a model to new tasks while retaining performance on previous tasks. However, this can lead to <strong class="bold">catastrophic forgetting</strong>. Catastrophic forgetting<a id="_idIndexMarker591"/> in LLMs refers to the phenomenon where a model loses previously learned information when fine-tuned on new tasks or data without appropriate mechanisms to preserve prior knowledge.</p>
			<p>Let’s implement a simple strategy to mitigate this:</p>
			<ol>
				<li>First, we calculate parameter importance <a id="_idIndexMarker592"/>and implement <strong class="bold">elastic weight consolidation</strong> (<strong class="bold">EWC</strong>) loss for preserving critical weights:<pre class="source-code">
import copy
def ewc_loss(model, old_model, importance, loss):
    ewc_lambda = 0.01
    for n, p in model.named_parameters():
        if n in importance:
            loss += ewc_lambda * importance[n]
                * (p - old_model[n]).pow(2).sum()
    return loss
def compute_importance(model, dataset):
    importance = {}
    model.eval()
    for batch in dataset:
        model.zero_grad()
        output = model(batch)
        loss = output.loss
        loss.backward()
        for n, p in model.named_parameters():
            if p.grad is not None:
                if n not in importance:
                    importance[n] = p.grad.data.clone().pow(2)
                else:
                    importance[n] += p.grad.data.clone().pow(2)
    return importance</pre></li>				<li>We then implement the <a id="_idIndexMarker593"/>following code, which manages sequential <a id="_idIndexMarker594"/>training on multiple tasks while maintaining previous knowledge:<pre class="source-code">
def continual_fine_tune(
        model, tokenizer, datasets, num_epochs=3
):
    old_model = None
    importance = None
    for i, dataset in enumerate(datasets):
        if old_model is not None:
            importance = compute_importance(
                old_model, datasets[i-1])
        old_model = copy.deepcopy(model)
        tokenized_dataset = dataset.map(
            lambda examples: tokenize_function(examples,
                tokenizer),
            batched=True)</pre></li>				<li>Finally, we define <a id="_idIndexMarker595"/>optimized training parameters for continual <a id="_idIndexMarker596"/>learning:<pre class="source-code">
training_args = TrainingArguments(
    output_dir=f"./continual_fine_tuned_model_task_{i+1}",
    num_train_epochs=8,                # Increased epochs
    per_device_train_batch_size=20,    # Increased batch size
    per_device_eval_batch_size=20,
    warmup_steps=2000,                 # Increased warmup
    weight_decay=0.2,                  # Increased weight decay
    learning_rate=2e-5,                # Added learning rate
    logging_dir="./logs",
    evaluation_strategy="steps",    # Added evaluation strategy
    eval_steps=1000,                # Added evaluation frequency
    save_steps=1000                    # Added save frequency
)</pre></li>			</ol>
			<p>This implementation introduces <a id="_idIndexMarker597"/>several key concepts for continual fine-tuning:</p>
			<ul>
				<li><strong class="bold">EWC</strong>: We implement a <a id="_idIndexMarker598"/>simplified version of EWC, which adds a penalty term to the loss function to prevent drastic changes to important parameters for previous tasks</li>
				<li><strong class="bold">Importance computation</strong>: We calculate the importance of each parameter based on its gradient magnitude on the previous task</li>
				<li><strong class="bold">Continual fine-tuning loop</strong>: We fine-tune the model on each task sequentially, using EWC to mitigate forgetting</li>
				<li><strong class="bold">Evaluation on all tasks</strong>: After fine-tuning on each new task, we evaluate the model’s performance on all previous tasks to monitor forgetting</li>
			</ul>
			<p>The key considerations for continual fine-tuning are as follows:</p>
			<ul>
				<li><strong class="bold">Balance between plasticity and stability</strong>: EWC helps maintain this balance, allowing the model to learn new tasks<a id="_idIndexMarker599"/> while preserving knowledge of previous ones</li>
				<li><strong class="bold">Computational overhead</strong>: Computing importance and applying EWC increases the computational cost of training</li>
				<li><strong class="bold">Task similarity</strong>: The effectiveness of continual fine-tuning can depend on the similarity between tasks</li>
			</ul>
			<p>Additional strategies to consider for mitigating catastrophic forgetting include the following:</p>
			<ul>
				<li><strong class="bold">Gradient episodic memory</strong> (<strong class="bold">GEM</strong>): In this approach, a small episodic memory of data from<a id="_idIndexMarker600"/> previous tasks is stored and used to constrain the <a id="_idIndexMarker601"/>gradient updates on new tasks, as <a id="_idIndexMarker602"/>follows:<pre class="source-code">
def project(gradient, memories):
    for memory in memories:
        if torch.dot(gradient, memory) &lt; 0:
            gradient -= (
                torch.dot(gradient, memory) / torch.dot(
                    memory, memory)
            ) * memory
    return gradient
# This would be integrated into the training loop</pre></li>				<li><strong class="bold">Progressive neural networks</strong>: Here, a new “column” of layers for each new task is created, while lateral connections to previously learned features are maintained.</li>
				<li><strong class="bold">Learning without Forgetting (LwF)</strong>: In this approach, knowledge distillation is employed to preserve the<a id="_idIndexMarker603"/> model’s performance on previous tasks:<pre class="source-code">
def lwf_loss(
    model, old_model, new_data, old_data, temperature=2
):
    # Compute standard loss on new data
    new_loss = compute_loss(model, new_data)
    # Compute distillation loss on old data
    old_outputs = old_model(old_data)
    new_outputs = model(old_data)
    distillation_loss = F.kl_div(
        F.log_softmax(new_outputs / temperature, dim=1),
        F.softmax(old_outputs / temperature, dim=1),
        reduction='batchmean'
    ) * (temperature  2)
    return new_loss + distillation_loss
# This would replace the standard loss in the training loop</pre></li>			</ul>
			<p>These advanced techniques<a id="_idIndexMarker604"/> can be particularly useful when fine-tuning LL<a id="_idTextAnchor189"/>Ms across a <a id="_idIndexMarker605"/>diverse range of tasks or domains.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor190"/>Summary</h1>
			<p>Fine-tuning patterns for LLMs encompass a wide range of techniques, from basic transfer learning to advanced continual learning strategies. By mastering these patterns, you can effectively adapt pre-trained models to new tasks and domains, optimize performance, and mitigate issues such as catastrophic forgetting. As the field of LLMs continues to evolve, staying updated with the latest fine-tuning techniques will be crucial for developing state-of-the-art language models tailored to specific applications.</p>
			<p>Here are the key takeaways from this chapter:</p>
			<ul>
				<li><strong class="bold">Fine-tuning adapts pre-trained LLMs</strong>: Fine-tuning is the key process for adapting general-purpose, pre-trained LLMs to specific tasks and datasets, bridging the gap between general language understanding and specialized performance</li>
				<li><strong class="bold">Layer management is crucial</strong>: Strategically freezing and unfreezing layers (especially gradual unfreezing) is critical for balancing the preservation of pre-trained knowledge with adaptation to the new task</li>
				<li><strong class="bold">Learning rate scheduling stabilizes training</strong>: Using learning rate schedules with warmup (linear or cosine) is essential for stable and effective fine-tuning, preventing drastic early updates and promoting convergence</li>
				<li><strong class="bold">Domain/task specificity matters</strong>: Techniques such as domain-specific vocabulary adaptation, custom data handling, and few-shot/zero-shot approaches are vital for maximizing performance on specialized tasks</li>
				<li><strong class="bold">Catastrophic forgetting must be addressed</strong>: In continual learning scenarios, techniques such as EWC, GEM, and others are necessary to prevent the model from losing previously learned information when trained on new tasks</li>
			</ul>
			<p>We will explore model pruning in the next chapter. Model pruning systematically removes redundant or less important neural connections in LLMs while preserving core functionality, essentially creating a lighter, more efficient version that maintains similar performance but requires fewer computational resources.</p>
		</div>
	</div></div></body></html>