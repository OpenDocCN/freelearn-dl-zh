["```py\nurl = \"https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py\"\noutput_file = \"grequests.py\" \n```", "```py\n    !pip install requests==2.32.3 \n    ```", "```py\n    !pip install beautifulsoup4==4.12.3 \n    ```", "```py\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n# URLs of the Wikipedia articles mapped to keywords\nurls = {\n    \"prompt engineering\": \"https://en.wikipedia.org/wiki/Prompt_engineering\",\n    \"artificial intelligence\":\"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n    \"llm\": \"https://en.wikipedia.org/wiki/Large_language_model\",\n    \"llms\": \"https://en.wikipedia.org/wiki/Large_language_model\"\n} \n```", "```py\ndef fetch_and_clean(url):\n    # Fetch the content of the URL\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    # Find the main content of the article, ignoring side boxes and headers\n    content = soup.find('div', {'class': 'mw-parser-output'})\n    # Remove less relevant sections such as \"See also\", \"References\", etc.\n    for section_title in ['References', 'Bibliography', 'External links', 'See also']:\n        section = content.find('span', {'id': section_title})\n        if section:\n            for sib in section.parent.find_next_siblings():\n                sib.decompose()\n            section.parent.decompose()\n    # Focus on extracting and cleaning text from paragraph tags only\n    paragraphs = content.find_all('p')\n    cleaned_text = ' '.join(paragraph.get_text(separator=' ', strip=True) for paragraph in paragraphs)\n    cleaned_text = re.sub(r'\\[\\d+\\]', '', cleaned_text)  # Remove citation markers like [1], [2], etc.\n    return cleaned_text \n```", "```py\nimport textwrap\ndef process_query(user_input, num_words):\n    user_input = user_input.lower()\n    # Check for any of the specified keywords in the input\n    matched_keyword = next((keyword for keyword in urls if keyword in user_input), None) \n```", "```py\nif matched_keyword:\n    print(f\"Fetching data from: {urls[matched_keyword]}\")\n    cleaned_text = fetch_and_clean(urls[matched_keyword])\n\n    # Limit the display to the specified number of words from the cleaned text\n    words = cleaned_text.split()  # Split the text into words\n    first_n_words = ' '.join(words[:num_words])  # Join the first n words into a single string \n```", "```py\n # Wrap the first n words to 80 characters wide for display\n    wrapped_text = textwrap.fill(first_n_words, width=80)\n    print(\"\\nFirst {} words of the cleaned text:\".format(num_words))\n    print(wrapped_text)  # Print the first n words as a well-formatted paragraph\n    # Use the exact same first_n_words for the GPT-4 prompt to ensure consistency\n    prompt = f\"Summarize the following information about {matched_keyword}:\\n{first_n_words}\"\n    wrapped_prompt = textwrap.fill(prompt, width=80)  # Wrap prompt text\n    print(\"\\nPrompt for Generator:\", wrapped_prompt)\n    # Return the specified number of words\n    return first_n_words\nelse:\n    print(\"No relevant keywords found. Please enter a query related to 'LLM', 'LLMs', or 'Prompt Engineering'.\")\n    return None \n```", "```py\n# Request user input for keyword parsing\nuser_input = input(\"Enter your query: \").lower() \n```", "```py\nEnter your query: What is an LLM? \n```", "```py\n#Select a score between 1 and 5 to run the simulation\nranking=1 \n```", "```py\n# initializing the text for the generative AI model simulations\ntext_input=[] \n```", "```py\nif ranking>=1 and ranking<3:\n  text_input=user_input \n```", "```py\nGPT-4 Response:\n---------------\nIt seems like you're asking about \"LLM\" which stands for \"Language Model for Dialogue Applications\" or more commonly referred to as a \"Large Language Model.\"\nAn LLM is a type of artificial intelligence model designed to understand, generate, and interact with human language. These models are trained on vast amounts of text data and use this training to generate text, answer questions, summarize information, translate languages, and perform other language-related tasks. They are a subset of machine learning models known as transformers, which have been revolutionary in the field of natural language processing (NLP).\nExamples of LLMs include OpenAI's GPT (Generative Pre-trained Transformer) series and Google's BERT (Bidirectional Encoder Representations from\nTransformers).\n--------------- \n```", "```py\nhf=False\nif ranking>3 and ranking<5:\n  hf=True \n```", "```py\nif hf==True:\n  from grequests import download\n  directory = \"Chapter05\"\n  filename = \"human_feedback.txt\"\n  download(directory, filename, private_token) \n```", "```py\nif hf==True:\n  # Check if 'human_feedback.txt' exists\n    efile = os.path.exists('human_feedback.txt')\n    if efile:\n        # Read and clean the file content\n        with open('human_feedback.txt', 'r') as file:\n            content = file.read().replace('\\n', ' ').replace('#', '')  # Removing new line and markdown characters\n            #print(content)  # Uncomment for debugging or maintenance display\n        text_input=content\n        print(text_input)\n    else:\n      print(\"File not found\")\n      hf=False \n```", "```py\nA Large Language Model (LLM) is an advanced AI system trained on vast amounts of text data to generate human-like text responses. It understands and generates language based on the patterns and information it has learned during training. LLMs are highly effective in various language-based tasks, including answering questions, making recommendations, and facilitating conversations. They can be continually updated with new information and trained to understand specific domains or industries.For the C-phone series customer support, incorporating an LLM could significantly enhance service quality and efficiency. The conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and freeing up human agents for more complex issues. It can be programmed to handle common technical questions about the C-phone series, troubleshoot problems, guide users through setup processes, and offer tips for optimizing device performance. Additionally, it can be used to gather customer feedback, providing valuable insights into user experiences and product performance. This feedback can then be used to improve products and services. Furthermore, the LLM can be designed to escalate issues to human agents when necessary, ensuring that customers receive the best possible support at all levels. The agent can also provide personalized recommendations for customers based on their usage patterns and preferences, enhancing user satisfaction and loyalty. \n```", "```py\nGPT-4 Response:\n---------------\nA Large Language Model (LLM) is a sophisticated AI system trained on extensive\ntext data to generate human-like text responses. It understands and generates\nlanguage based on patterns and information learned during training. LLMs are\nhighly effective in various language-based tasks such as answering questions,\nmaking recommendations, and facilitating conversations. They can be continuously updated with new information and trained to understand specific domains or industries.  For the C-phone series customer support, incorporating an LLM could significantly enhance service quality and efficiency. The conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and freeing up human agents for more complex issues. \nIt can be programmed to handle common technical questions about the C-phone series,\ntroubleshoot problems, guide users through setup processes, and offer tips for\noptimizing device performance. Additionally, it can be used to gather customer\nfeedback, providing valuable insights into user experiences and product\nperformance. This feedback can then be used to improve products and services.\nFurthermore, the LLM can be designed to escalate issues to human agents when\nnecessary, ensuring that customers receive the best possible support at all\nlevels. The agent can also provide personalized recommendations for customers\nbased on their usage patterns and preferences, enhancing user satisfaction and\nloyalty.\n--------------- \n```", "```py\nif ranking>=5:\n  max_words=100 #Limit: the size of the data we can add to the input\n  rdata=process_query(user_input,max_words)\n  print(rdata) # for maintenance if necessary\n  if rdata:\n        rdata_clean = rdata.replace('\\n', ' ').replace('#', '')\n        rdata_sentences = rdata_clean.split('. ')\n        print(rdata)\n  text_input=rdata\n  print(text_input) \n```", "```py\nGPT-4 Response:\n---------------\nA large language model (LLM) is a type of language model known for its\ncapability to perform general-purpose language generation and other natural language processing tasks such as classification. LLMs develop these abilities by learning statistical relationships from text documents through a computationally intensive training process that includes both self-supervised\nand semi-supervised learning. These models can generate text, a form of\ngenerative AI, by taking an input text and repeatedly predicting the next token or word. LLMs are based on artificial neural networks. As of March 2024, the most advanced and capable LLMs are constructed using a decoder-only transformer architecture.\n--------------- \n```", "```py\n!pip install openai==1.40.3 \n```", "```py\n#API Key\n#Store your key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\nfrom google.colab import drive\ndrive.mount('/content/drive')\nf = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\nAPI_KEY=f.readline().strip()\nf.close()\n#The OpenAI Key\nimport os\nimport openai\nos.environ['OPENAI_API_KEY'] =API_KEY\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\") \n```", "```py\nimport openai\nfrom openai import OpenAI\nimport time\nclient = OpenAI()\ngptmodel=\"gpt-4o\"\nstart_time = time.time()  # Start timing before the request \n```", "```py\ndef call_gpt4_with_full_text(itext):\n    # Join all lines to form a single string\n    text_input = '\\n'.join(itext)\n    prompt = f\"Please summarize or elaborate on the following content:\\n{text_input}\"\n    try:\n      response = client.chat.completions.create(\n         model=gptmodel,\n         messages=[\n            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n            {\"role\": \"user\", \"content\": prompt}\n         ],\n         temperature=0.1  # Add the temperature parameter here and other parameters you need\n        )\n      return response.choices[0].message.content.strip()\n    except Exception as e:\n        return str(e) \n```", "```py\nimport textwrap\ndef print_formatted_response(response):\n    # Define the width for wrapping the text\n    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n    wrapped_text = wrapper.fill(text=response)\n    # Print the formatted response with a header and footer\n    print(\"GPT-4 Response:\")\n    print(\"---------------\")\n    print(wrapped_text)\n    print(\"---------------\\n\")\n# Assuming 'gpt4_response' contains the response from the previous GPT-4 call\nprint_formatted_response(gpt4_response) \n```", "```py\nGPT-4 Response:\n---------------\nGPT-4 Response:\n---------------\n### Summary: A large language model (LLM) is a computational model known for its ability to perform general-purpose language generation and other natural language processing tasks, such as classification. LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a computationally intensive self-supervised and semi-supervised training process.They can be used for text generation, a form of generative AI, by taking input text and repeatedly predicting the next token or word. LLMs are artificial neural networks that use the transformer architecture… \n```", "```py\nimport time\n…\nstart_time = time.time()  # Start timing before the request\n…\nresponse_time = time.time() - start_time  # Measure response time\nprint(f\"Response Time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nprint(f\"Response Time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nResponse Time: 7.88 seconds \n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef calculate_cosine_similarity(text1, text2):\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform([text1, text2])\n    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n    return similarity[0][0]\n# Example usage with your existing functions\nsimilarity_score = calculate_cosine_similarity(text_input, gpt4_response)\nprint(f\"Cosine Similarity Score: {similarity_score:.3f}\") \n```", "```py\nCosine Similarity Score: 0.697 \n```", "```py\n# Score parameters\ncounter=20                     # number of feedback queries\nscore_history=30               # human feedback\nthreshold=4                    # minimum rankings to trigger human expert feedback \n```", "```py\nimport numpy as np\ndef evaluate_response(response):\n    print(\"\\nGenerated Response:\")\n    print(response)\n    print(\"\\nPlease evaluate the response based on the following criteria:\")\n    print(\"1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent\")\n    score = input(\"Enter the relevance and coherence score (1-5): \")\n    try:\n        score = int(score)\n        if 1 <= score <= 5:\n            return score\n        else:\n            print(\"Invalid score. Please enter a number between 1 and 5.\")\n            return evaluate_response(response)  # Recursive call if the input is invalid\n    except ValueError:\n        print(\"Invalid input. Please enter a numerical value.\")\n        return evaluate_response(response)  # Recursive call if the input is invalid \n```", "```py\nscore = evaluate_response(gpt4_response)\nprint(\"Evaluator Score:\", score) \n```", "```py\nGenerated Response:\n### Summary:\nA large language model (LLM) is a computational model… \n```", "```py\nPlease evaluate the response based on the following criteria:\n1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent\nEnter the relevance and coherence score (1-5): 3 \n```", "```py\ncounter+=1\nscore_history+=score\nmean_score=round(np.mean(score_history/counter), 2)\nif counter>0:\n  print(\"Rankings      :\", counter)\n  print(\"Score history : \", mean_score) \n```", "```py\nEvaluator Score: 3\nRankings      : 21\nScore history :  3.0 \n```", "```py\nthreshold=4 \n```", "```py\nfrom grequests import download\n# Define your variables\ndirectory = \"commons\"\nfilename = \"thumbs_up.png\"\ndownload(directory, filename, private_token)\n# Define your variables\ndirectory = \"commons\"\nfilename = \"thumbs_down.png\"\ndownload(directory, filename, private_token) \n```", "```py\nif counter>counter_threshold and score_history<=score_threshold:\n  print(\"Human expert evaluation is required for the feedback loop.\") \n```", "```py\nHuman expert evaluation is required for the feedback loop. \nexpert_feedback.txt, as shown in the following excerpt of the code:\n```", "```py\nimport base64\nfrom google.colab import output\nfrom IPython.display import display, HTML\ndef image_to_data_uri(file_path):\n    \"\"\"\n    Convert an image to a data URI.\n    \"\"\"\n    with open(file_path, 'rb') as image_file:\n        encoded_string = base64.b64encode(image_file.read()).decode()\n        return f'data:image/png;base64,{encoded_string}'\nthumbs_up_data_uri = image_to_data_uri('/content/thumbs_up.png')\nthumbs_down_data_uri = image_to_data_uri('/content/thumbs_down.png')\ndef display_icons():\n    # Define the HTML content with the two clickable images\n…/…\ndef save_feedback(feedback):\n    with open('/content/expert_feedback.txt', 'w') as f:\n        f.write(feedback)\n    print(\"Feedback saved successfully.\")\n# Register the callback\noutput.register_callback('notebook.save_feedback', save_feedback)\nprint(\"Human Expert Adaptive RAG activated\")\n# Display the icons with click handlers\ndisplay_icons() \n```", "```py\nThere is an inaccurate statement in the text:\n\"As of March 2024, the largest and most capable LLMs are built with a decoder-only transformer-based architecture.\"\nThis statement is not accurate because the largest and most capable Large Language Models, such as Meta's Llama models, have a transformer-based architecture, but they are not \"decoder-only.\" These models use the architecture of the transformer, which includes both encoder and decoder components. \n```"]