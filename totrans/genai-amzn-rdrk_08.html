<html><head></head><body>
<div id="_idContainer126">
<h1 class="chapter-number" id="_idParaDest-141"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-142"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.2.1">Extracting Entities and Generating Code with Amazon Bedrock</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This chapter uncovers the realm of entity extraction, a crucial technique in NLP. </span><span class="koboSpan" id="kobo.3.2">We will explore the intricacies of entity extraction applications, providing a comprehensive understanding of implementing entity extraction using Amazon Bedrock. </span><span class="koboSpan" id="kobo.3.3">Through real-world use cases, you will gain insights into the practical applications of entity extraction across </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">various domains.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Furthermore, the chapter will guide you through the exciting world of generative AI for code generation. </span><span class="koboSpan" id="kobo.5.2">We will investigate the underlying principles and methodologies that enable AI systems to generate code snippets, functions, and even entire applications. </span><span class="koboSpan" id="kobo.5.3">You will learn how to leverage Amazon Bedrock to streamline your development workflows and </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">enhance productivity.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">By mastering these techniques, you will be equipped with the knowledge and skills to tackle complex NLP tasks and harness the power of generative AI in your </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">coding endeavors.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">The following topics will be covered </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">in detail:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Entity extraction – a </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">comprehensive exploration</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Industrial use cases of entity extraction – unleashing the power of </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">unstructured data</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Entity extraction with </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">Amazon Bedrock</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Code generation with LLMs – unleashing the power of </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">AI-driven development</span></span></li>
</ul>
<h1 id="_idParaDest-143"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.19.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.20.1">This chapter requires you to have access to an AWS account. </span><span class="koboSpan" id="kobo.20.2">If you don’t have one already, you can go to </span><a href="https://aws.amazon.com/getting-started/"><span class="koboSpan" id="kobo.21.1">https://aws.amazon.com/getting-started/</span></a><span class="koboSpan" id="kobo.22.1"> and create an </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">AWS account.</span></span></p>
<p><span class="koboSpan" id="kobo.24.1">Secondly, you will need to install and configure the AWS CLI from </span><a href="https://aws.amazon.com/cli/"><span class="koboSpan" id="kobo.25.1">https://aws.amazon.com/cli/</span></a><span class="koboSpan" id="kobo.26.1"> after you create an account, which will be needed to access Amazon Bedrock FMs from your local machine. </span><span class="koboSpan" id="kobo.26.2">Since the majority of the code cells we will be executing are based on Python, setting up an AWS Python SDK (Boto3) at </span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html"><span class="koboSpan" id="kobo.27.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</span></a><span class="koboSpan" id="kobo.28.1"> would be beneficial at this point. </span><span class="koboSpan" id="kobo.28.2">You can carry out the Python setup in the following manner: install it on your local machine, use AWS Cloud9 or AWS Lambda, or leverage </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">Amazon SageMaker.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.30.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.31.1">There will be a charge associated with the invocation and customization of Amazon Bedrock FMs. </span><span class="koboSpan" id="kobo.31.2">Please refer to </span><a href="https://aws.amazon.com/bedrock/pricing/"><span class="koboSpan" id="kobo.32.1">https://aws.amazon.com/bedrock/pricing/</span></a><span class="koboSpan" id="kobo.33.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">learn more.</span></span></p>
<h1 id="_idParaDest-144"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.35.1">Entity extraction – a comprehensive exploration</span></h1>
<p><span class="koboSpan" id="kobo.36.1">In the era of big data and information overload, the ability to extract meaningful insights from unstructured text data has become increasingly </span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.37.1">valuable. </span><strong class="bold"><span class="koboSpan" id="kobo.38.1">Entity extraction</span></strong><span class="koboSpan" id="kobo.39.1">, a subfield of NLP, plays a pivotal role in this endeavor by identifying and classifying named entities within text, such as people, organizations, locations, and more. </span><span class="koboSpan" id="kobo.39.2">This process not only facilitates information retrieval and knowledge management but also enables a wide</span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.40.1"> range of applications, including </span><strong class="bold"><span class="koboSpan" id="kobo.41.1">question-answering</span></strong><span class="koboSpan" id="kobo.42.1">, sentiment</span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.43.1"> analysis, and </span><strong class="bold"><span class="koboSpan" id="kobo.44.1">decision support </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.45.1">systems</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.46.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.47.1">DSSs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">The journey of entity extraction began with simple pattern-matching and rule-based systems, which relied heavily on manually crafted rules and lexicons. </span><span class="koboSpan" id="kobo.49.2">These methods, while useful, lacked scalability and robustness when dealing with diverse and </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">complex datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">Hence, traditionally, entity extraction has been a challenging task, requiring extensive manual effort and domain-specific knowledge.  </span><span class="koboSpan" id="kobo.51.2">However, the advent of generative AI, particularly LLMs, has revolutionized this field, offering more accurate, scalable, and efficient solutions. </span><span class="koboSpan" id="kobo.51.3">In this chapter, we will explore the various techniques employed by LLMs on Amazon Bedrock for entity extraction, diving into their underlying architectures, strengths, </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">and limitations.</span></span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.53.1">Deep learning approaches</span></h2>
<p><span class="koboSpan" id="kobo.54.1">The advent of</span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.55.1"> machine learning introduced statistical models that leveraged feature engineering. </span><span class="koboSpan" id="kobo.55.2">These models, including </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">hidden Markov models</span></strong><span class="koboSpan" id="kobo.57.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.58.1">HMMs</span></strong><span class="koboSpan" id="kobo.59.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.60.1">conditional random fields</span></strong><span class="koboSpan" id="kobo.61.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.62.1">CRFs</span></strong><span class="koboSpan" id="kobo.63.1">), represented a </span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.64.1">significant step forward. </span><span class="koboSpan" id="kobo.64.2">They utilized </span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.65.1">hand-crafted features and probabilistic frameworks to improve extraction accuracy. </span><span class="koboSpan" id="kobo.65.2">However, their performance was still limited by the quality and comprehensiveness of the features engineered </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">by experts.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">Neural networks marked a paradigm shift in entity extraction by automating feature learning and capturing intricate patterns within the data. </span><span class="koboSpan" id="kobo.67.2">Early applications of neural networks, such</span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.68.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.69.1">recurrent NNs</span></strong><span class="koboSpan" id="kobo.70.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.71.1">RNNs</span></strong><span class="koboSpan" id="kobo.72.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.73.1">long short-term memory networks</span></strong><span class="koboSpan" id="kobo.74.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.75.1">LSTMs</span></strong><span class="koboSpan" id="kobo.76.1">), demonstrated the potential of deep learning in </span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.77.1">handling sequential data and extracting entities with </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">greater accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.79.1">While models such as BERT and its successors represent a significant leap in NLP, our focus will remain on models and techniques that align with the practical applications and tools used in Bedrock. </span><span class="koboSpan" id="kobo.79.2">We will explore some deep learning approaches and models that have proven effective in various scenarios and are relevant to </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">our framework.</span></span></p>
<h3><span class="koboSpan" id="kobo.81.1">Transformer-based models</span></h3>
<p><span class="koboSpan" id="kobo.82.1">Transformer </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.83.1">architectures, introduced by the seminal paper </span><em class="italic"><span class="koboSpan" id="kobo.84.1">Attention is All You Need</span></em><span class="koboSpan" id="kobo.85.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.86.1">Vaswani et al.</span></em><span class="koboSpan" id="kobo.87.1">, </span><em class="italic"><span class="koboSpan" id="kobo.88.1">2017</span></em><span class="koboSpan" id="kobo.89.1">: </span><a href="https://arxiv.org/abs/1706.03762"><span class="koboSpan" id="kobo.90.1">https://arxiv.org/abs/1706.03762</span></a><span class="koboSpan" id="kobo.91.1">), have become the backbone of many SOTA LLMs for entity extraction. </span><span class="koboSpan" id="kobo.91.2">These models employ self-attention mechanisms to capture long-range dependencies within the input text, enabling them to better understand the context and relationships </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">between entities.</span></span></p>
<p><span class="koboSpan" id="kobo.93.1">BERT, developed by Google AI, is a prominent example of a transformer-based model that has achieved exceptional results in various NLP tasks, including entity extraction. </span><span class="koboSpan" id="kobo.93.2">It is a bidirectional model, meaning it can process text in both directions simultaneously, allowing it to capture contextual information more effectively than </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">its predecessors.</span></span></p>
<h3><span class="koboSpan" id="kobo.95.1">Sequence labeling and CRFs</span></h3>
<p><span class="koboSpan" id="kobo.96.1">Entity extraction</span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.97.1"> can be framed as a sequence labeling problem, where each token in the input text is assigned a label indicating its entity type (for example, person, organization, location) or a non-entity label. </span><span class="koboSpan" id="kobo.97.2">LLMs can be trained to perform this task by leveraging techniques such as CRFs or the more</span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.98.1"> recent </span><strong class="bold"><span class="koboSpan" id="kobo.99.1">bidirectional LSTM with CRF</span></strong><span class="koboSpan" id="kobo.100.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.101.1">BiLSTM-CRF</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">) architecture.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.103.1">CRFs</span></strong><span class="koboSpan" id="kobo.104.1"> are probabilistic</span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.105.1"> graphical models that can effectively capture the dependencies between labels in a sequence, making them well suited for entity extraction tasks. </span><span class="koboSpan" id="kobo.105.2">They model the conditional probability of label sequences given the input text, allowing for the incorporation of rich features and </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">contextual information.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.107.1">BiLSTM-CRF models</span></strong><span class="koboSpan" id="kobo.108.1"> combine </span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.109.1">the strengths of BiLSTMs for capturing long-range dependencies and CRFs for sequence labeling. </span><span class="koboSpan" id="kobo.109.2">This hybrid approach has shown impressive performance in entity extraction, particularly in scenarios where entities may span multiple</span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.110.1"> tokens or have </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">complex structures.</span></span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.112.1">Rule-based systems</span></h2>
<p><span class="koboSpan" id="kobo.113.1">While deep learning approaches</span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.114.1"> have gained significant </span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.115.1">traction in recent years, rule-based systems remain valuable tools in the entity extraction domain. </span><span class="koboSpan" id="kobo.115.2">These systems rely on manually crafted rules and patterns to identify and classify entities within text, leveraging domain-specific knowledge and expert insights. </span><span class="koboSpan" id="kobo.115.3">These rules can be augmented to the prompt template when invoking Amazon Bedrock in order to generate a desirable response from the FMs. </span><span class="koboSpan" id="kobo.115.4">For example, in a medical application, the rule-based component might identify drug names, dosages, and patient information using </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">predefined patterns.</span></span></p>
<h3><span class="koboSpan" id="kobo.117.1">Regular expressions and pattern matching</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.118.1">Regular expressions</span></strong><span class="koboSpan" id="kobo.119.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.120.1">pattern-matching</span></strong><span class="koboSpan" id="kobo.121.1"> techniques </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.122.1">are fundamental building blocks of rule-based </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.123.1">entity </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.124.1">extraction systems. </span><span class="koboSpan" id="kobo.124.2">These methods allow for the definition of patterns that can match and extract specific entity types, such as phone numbers, email addresses, or specific named entities (for example, company names and </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">product names).</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">While regular expressions can be effective for well-defined and structured entity types, they may struggle with more complex or ambiguous entities that require contextual understanding. </span><span class="koboSpan" id="kobo.126.2">Nevertheless, they remain valuable tools, particularly in combination with other techniques or as a preprocessing step for more advanced methods. </span><span class="koboSpan" id="kobo.126.3">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">some examples:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.128.1">Ruleset</span></strong><span class="koboSpan" id="kobo.129.1">: Define rules using regular expressions and pattern matching to identify specific entities such as drug names, dosages, and </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">patient information</span></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.131.1">Example rules</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">:</span></span><ul><li><span class="koboSpan" id="kobo.133.1">Drug names can be identified using a dictionary of known </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">drug names</span></span></li><li><span class="koboSpan" id="kobo.135.1">Dosages can be extracted using patterns such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.136.1">\d+mg</span></strong><span class="koboSpan" id="kobo.137.1"> (for </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">example, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.139.1">500mg</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">)</span></span></li><li><span class="koboSpan" id="kobo.141.1">Patient information </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.142.1">can </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.143.1">be identified </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.144.1">through patterns such as </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">Patient: [A-Za-z]+</span></strong></span></li></ul></li>
</ul>
<h3><span class="koboSpan" id="kobo.146.1">Gazetteer lists and dictionaries</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.147.1">Gazetteer lists</span></strong><span class="koboSpan" id="kobo.148.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.149.1">dictionaries</span></strong><span class="koboSpan" id="kobo.150.1"> are curated </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.151.1">collections </span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.152.1">of known entities, often organized by entity type </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.153.1">or domain. </span><span class="koboSpan" id="kobo.153.2">These resources can be used to match and extract entities within text by performing lookups against </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">predefined lists.</span></span></p>
<p><span class="koboSpan" id="kobo.155.1">For example, a gazetteer of geographic locations can be employed to identify and extract mentions of cities, countries, or other places in a given text. </span><span class="koboSpan" id="kobo.155.2">Similarly, dictionaries of person names or organization names can ease the extraction of these </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">entity types.</span></span></p>
<p><span class="koboSpan" id="kobo.157.1">While gazetteer lists and dictionaries can be highly accurate for the entities they cover, they may struggle with ambiguity, variations, or newly emerging entities not present in the predefined lists. </span><span class="koboSpan" id="kobo.157.2">Additionally, maintaining and updating these resources can be a labor-intensive process, especially in rapidly </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">evolving domains.</span></span></p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.159.1">Hybrid approaches</span></h2>
<p><span class="koboSpan" id="kobo.160.1">In practice, many entity </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.161.1">extraction systems employ a </span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.162.1">combination of deep learning and rule-based techniques, leveraging the strengths of both approaches to achieve optimal performance. </span><span class="koboSpan" id="kobo.162.2">These hybrid approaches aim to strike a balance between the flexibility and generalization capabilities of deep learning models and the precision and interpretability of </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">rule-based systems.</span></span></p>
<h3><span class="koboSpan" id="kobo.164.1">Ensemble methods</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.165.1">Ensemble methods</span></strong><span class="koboSpan" id="kobo.166.1"> involve combining the outputs of multiple entity extraction models, potentially </span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.167.1">using different architectures </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.168.1">or techniques, to improve overall performance. </span><span class="koboSpan" id="kobo.168.2">This approach can leverage the strengths of individual models while mitigating their weaknesses, resulting in more robust and accurate </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">entity extraction.</span></span></p>
<p><span class="koboSpan" id="kobo.170.1">For example, an ensemble system might combine the predictions of a transformer-based model such as BERT with those of a rule-based system or a gazetteer lookup. </span><span class="koboSpan" id="kobo.170.2">The outputs of these models can be combined using various strategies, such as majority voting, weighted averaging, or more sophisticated ensemble </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">learning techniques.</span></span></p>
<h3><span class="koboSpan" id="kobo.172.1">Hybrid architectures</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.173.1">Hybrid architectures</span></strong><span class="koboSpan" id="kobo.174.1"> integrate</span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.175.1"> deep learning and rule-based </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.176.1">components within a single model, allowing for the seamless integration of both approaches. </span><span class="koboSpan" id="kobo.176.2">These architectures often involve a deep learning component for learning representations and capturing contextual information, combined with rule-based components for incorporating domain-specific knowledge or handling well-defined </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">entity types.</span></span></p>
<p><span class="koboSpan" id="kobo.178.1">One example of a hybrid architecture is the use of LLMs for entity representation learning, followed by a rule-based component for entity classification or extraction. </span><span class="koboSpan" id="kobo.178.2">The LLM component can learn rich representations of the input text, capturing contextual information and long-range dependencies, while the rule-based component can leverage expert knowledge and precise patterns for entity identification and classification. </span><span class="koboSpan" id="kobo.178.3">For instance, consider an application designed to extract financial information from corporate earnings reports. </span><span class="koboSpan" id="kobo.178.4">Here’s a detailed example of how a hybrid architecture can </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">be implemented:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.180.1">LLM for initial text processing</span></strong><span class="koboSpan" id="kobo.181.1">: FMs available on Amazon Bedrock can process the entire earnings report, generating detailed representations of text segments. </span><span class="koboSpan" id="kobo.181.2">They capture the context and nuances of financial terminology, ensuring a deep understanding of phrases such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">revenue</span></strong><span class="koboSpan" id="kobo.183.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">net income</span></strong><span class="koboSpan" id="kobo.185.1">, and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.186.1">operating expenses</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.188.1">Ruleset</span></strong><span class="koboSpan" id="kobo.189.1">: Develop a set of rules tailored to financial documents. </span><span class="koboSpan" id="kobo.189.2">For instance, rules might include </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">the following:</span></span><ul><li><span class="koboSpan" id="kobo.191.1">Identifying dollar amounts following keywords such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">revenue</span></strong><span class="koboSpan" id="kobo.193.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">net income</span></strong><span class="koboSpan" id="kobo.195.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">or </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">expenses</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">.</span></span></li><li><span class="koboSpan" id="kobo.199.1">Extracting dates and fiscal periods using </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">regular expressions.</span></span></li><li><span class="koboSpan" id="kobo.201.1">Recognizing company-specific terminology </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">and abbreviations.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.203.1">The rule-based system analyzes the LLM-generated representations, applying these rules to accurately extract specific </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">financial entities.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">Let’s now look at how the representations are integrated </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">and optimized:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.207.1">Pipeline</span></strong><span class="koboSpan" id="kobo.208.1">: The system processes the earnings report through the LLM, which outputs rich text representations. </span><span class="koboSpan" id="kobo.208.2">These representations are then fed into the </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">rule-based component.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.210.1">Output</span></strong><span class="koboSpan" id="kobo.211.1">: The final output includes precisely extracted financial entities, such as revenue figures, net income amounts, and fiscal periods, all verified and categorized according to the </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">predefined rules.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.213.1">By employing</span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.214.1"> such </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.215.1">a hybrid approach on Amazon Bedrock, the application leverages the comprehensive text understanding provided by LLMs and the precision and reliability of rule-based extraction methods. </span><span class="koboSpan" id="kobo.215.2">This approach ensures that entity extraction is more accurate and contextually aware, making it useful for complex domains such as </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">financial analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">In order to gain a deeper understanding of hybrid LLM frameworks, readers are encouraged to read these papers: </span><em class="italic"><span class="koboSpan" id="kobo.218.1">Hybrid LLM-Rule-based Approaches to Business Insights Generation from Structured Data</span></em><span class="koboSpan" id="kobo.219.1"> (https://arxiv.org/pdf/2404.15604) and </span><em class="italic"><span class="koboSpan" id="kobo.220.1">An innovative hybrid approach for extracting named entities from unstructured text </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.221.1">data </span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">(</span></span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">https://www.researchgate.net/publication/332676137_An_innovative_hybrid_approach_for_extracting_named_entities_from_unstructured_text_data</span></span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">In this section, we covered different approaches (deep learning, rule-based, and hybrid approaches) associated</span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.226.1"> with entity extraction. </span><span class="koboSpan" id="kobo.226.2">Now that we have a basic understanding of these approaches, let us </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.227.1">dive into some industrial use cases of </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">entity extraction.</span></span></p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.229.1">Industrial use cases of entity extraction – unleashing the power of unstructured data</span></h1>
<p><span class="koboSpan" id="kobo.230.1">Entity extraction</span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.231.1"> has numerous applications across various domains, ranging from information retrieval and knowledge management to DSSs </span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.232.1">and </span><strong class="bold"><span class="koboSpan" id="kobo.233.1">business intelligence</span></strong><span class="koboSpan" id="kobo.234.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.235.1">BI</span></strong><span class="koboSpan" id="kobo.236.1">). </span><span class="koboSpan" id="kobo.236.2">In this section, we will explore some practical use cases and applications of entity extraction </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">with GenAI:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.238.1">Knowledge graph construction</span></strong><span class="koboSpan" id="kobo.239.1">: Knowledge graphs are structured representations of entities and their relationships, enabling efficient storage, retrieval, and reasoning over vast amounts of information. </span><span class="koboSpan" id="kobo.239.2">Entity extraction plays a crucial role in the construction of knowledge graphs by identifying and classifying entities from unstructured text data, which can then be used to populate the graph’s nodes and edges. </span><span class="koboSpan" id="kobo.239.3">For instance, let us use the </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">following </span></span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.242.1">Text</span></strong><span class="koboSpan" id="kobo.243.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">Google was founded by Larry Page and Sergey Brin while they were Ph.D. </span><span class="koboSpan" id="kobo.244.2">students at </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">Stanford University.</span></strong></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.246.1">With entity extraction, the following information can </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">be extracted:</span></span></p><ul><li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.248.1">Entities identified</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">:</span></span><ul><li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">Google (Organization)</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.251.1">Larry </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">Page (Person)</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.253.1">Sergey </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">Brin (Person)</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.255.1">Stanford </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">University (Organization</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">)</span></span></li></ul></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.258.1">Role in knowledge graph construction</span></strong><span class="koboSpan" id="kobo.259.1">: These identified entities are classified and used to populate nodes within the knowledge graph. </span><span class="koboSpan" id="kobo.259.2">The relationships between the entities, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">founded by</span></strong><span class="koboSpan" id="kobo.261.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.262.1">Google</span></strong><span class="koboSpan" id="kobo.263.1"> -&gt; </span><strong class="source-inline"><span class="koboSpan" id="kobo.264.1">Larry Page and Sergey Brin</span></strong><span class="koboSpan" id="kobo.265.1">) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">studied at</span></strong><span class="koboSpan" id="kobo.267.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">Larry Page and Sergey Brin</span></strong><span class="koboSpan" id="kobo.269.1"> -&gt; </span><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">Stanford University</span></strong><span class="koboSpan" id="kobo.271.1">), are established as edges connecting the nodes. </span><span class="koboSpan" id="kobo.271.2">This structured representation </span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.272.1">allows for efficient querying and reasoning over </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">the information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.274.1">Resulting knowledge </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.275.1">graph snippet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">:</span></span><ul><li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.277.1">Nodes</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">:</span></span><ul><li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">Google (Organization)</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">Larry </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">Page (Person)</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.282.1">Sergey </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">Brin (Person)</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.284.1">Stanford </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">University (Organization)</span></strong></span></li></ul></li><li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.286.1">Edges</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">Google -&gt; Founded by -&gt; </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">Larry Page</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.290.1">Google -&gt; Founded by -&gt; </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.291.1">Sergey Brin</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">Larry Page -&gt; Studied at -&gt; </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">Stanford University</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">Sergey Brin -&gt; Studied at -&gt; </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">Stanford University</span></strong></span></li></ul></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.296.1">LLMs on Amazon Bedrock can be employed for accurate and scalable entity extraction, facilitating the creation of comprehensive knowledge graphs from diverse data sources, such as news articles, scientific publications, or social media posts. </span><span class="koboSpan" id="kobo.296.2">These knowledge graphs can power various applications, including question answering systems, recommendation engines, and decision support tools. </span><span class="koboSpan" id="kobo.296.3">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">some examples:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.298.1">Biomedical and scientific literature analysis</span></strong><span class="koboSpan" id="kobo.299.1">: Entity extraction is particularly valuable in the biomedical and scientific domains, where vast amounts of unstructured text data are generated through research publications, clinical notes, and other sources. </span><span class="koboSpan" id="kobo.299.2">Identifying and classifying entities such as genes, proteins, diseases, and chemical compounds can enable researchers and healthcare professionals to quickly navigate and extract insights from this wealth </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">of information.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.301.1">LLMs in Amazon Bedrock can be fine-tuned on domain-specific datasets to achieve high accuracy in extracting biomedical and scientific entities. </span><span class="koboSpan" id="kobo.301.2">These models can assist in literature review processes, drug discovery pipelines, and the development of knowledge bases for precision medicine and </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">personalized healthcare.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.303.1">BI and competitive analysis</span></strong><span class="koboSpan" id="kobo.304.1">: In the business world, entity extraction can be leveraged for competitive analysis, market research, and BI applications. </span><span class="koboSpan" id="kobo.304.2">By extracting entities such as company names, product names, and industry-specific terms from news articles, social media posts, and other online sources, businesses can gain valuable insights into their competitors, market trends, and </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">customer sentiment.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.306.1">Amazon Bedrock APIs can be coupled </span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.307.1">with </span><strong class="bold"><span class="koboSpan" id="kobo.308.1">BI platforms</span></strong><span class="koboSpan" id="kobo.309.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.310.1">BIPs</span></strong><span class="koboSpan" id="kobo.311.1">) and analytics tools, enabling real-time entity extraction and analysis of vast amounts of unstructured data. </span><span class="koboSpan" id="kobo.311.2">This can empower data-driven decision-making, strategic planning, and the identification of new </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">business opportunities.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.313.1">Social media monitoring and sentiment analysis</span></strong><span class="koboSpan" id="kobo.314.1">: Social media platforms generate a constant stream of user-generated content, containing valuable information about public opinion, trends, and sentiment toward various entities, such as brands, products, or public figures. </span><span class="koboSpan" id="kobo.314.2">Entity extraction plays a crucial role in social media monitoring and sentiment analysis by identifying the relevant entities within this </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">unstructured data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.316.1">LLMs in Amazon </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.317.1">Bedrock can be employed to accurately extract entities from social media posts, enabling sentiment analysis and opinion mining around these entities. </span><span class="koboSpan" id="kobo.317.2">This can provide businesses with valuable insights into customer feedback, brand perception, and potential issues or opportunities, allowing them to respond proactively and shape their marketing and communication </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">strategies accordingly.</span></span></p>
<p><span class="koboSpan" id="kobo.319.1">In this section, we covered industrial applications applicable in the context of entity extraction. </span><span class="koboSpan" id="kobo.319.2">Keep in mind that the number of these use cases can increase exponentially as we uncover more diverse scenarios across different industries. </span><span class="koboSpan" id="kobo.319.3">Now, let us learn how to leverage </span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.320.1">Amazon Bedrock for entity extraction </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">use cases.</span></span></p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.322.1">Entity extraction with Amazon Bedrock</span></h1>
<p><span class="koboSpan" id="kobo.323.1">At its core, entity </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.324.1">extraction with GenAI involves providing </span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.325.1">a prompt that instructs the model to identify and classify relevant entities within a given text input. </span><span class="koboSpan" id="kobo.325.2">The key is constructing prompts that are clear, consistent, and provide enough examples for the model to understand the </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">desired behavior.</span></span></p>
<p><span class="koboSpan" id="kobo.327.1">The Amazon Bedrock service, with the ability to invoke LLMs in a serverless manner, provides a scalable and cost-effective solution for entity extraction. </span><span class="koboSpan" id="kobo.327.2">This service allows developers to leverage pre-trained models or fine-tune them on custom datasets, enabling tailored entity extraction for specific domains or </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">use cases.</span></span></p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.329.1">Structuring prompts for entity extraction</span></h2>
<p><span class="koboSpan" id="kobo.330.1">When designing </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.331.1">prompts for entity extraction tasks, it’s essential to provide clear instructions and examples to the model. </span><span class="koboSpan" id="kobo.331.2">A well-structured prompt typically includes the </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">following components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.333.1">Task description</span></strong><span class="koboSpan" id="kobo.334.1">: Begin by explicitly stating the task at hand, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">Identify and classify the following entities in the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">given text</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.338.1">Entity types</span></strong><span class="koboSpan" id="kobo.339.1">: Provide a list of entity types the model should recognize, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">Person</span></strong><span class="koboSpan" id="kobo.341.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.342.1">Organization</span></strong><span class="koboSpan" id="kobo.343.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.344.1">Location</span></strong><span class="koboSpan" id="kobo.345.1">, and </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.347.1">Example inputs and outputs</span></strong><span class="koboSpan" id="kobo.348.1">: Include one or more examples of input text with the corresponding entities annotated. </span><span class="koboSpan" id="kobo.348.2">This helps the model understand the desired output format and learn from </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">real-world instances.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.350.1">The following is an </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">example prompt:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.352.1">'''</span></strong></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.353.1">Task</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.354.1">: Identify and classify the following entities in the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">given text:</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.356.1">Entity Types: Person, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.357.1">Organization, Location</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.358.1">Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.359.1">: "Michael Jordan, the legendary basketball player for the Chicago Bulls, announced his retirement from the NBA after an </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.360.1">illustrious career."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">The output looks </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">like this:</span></strong></span></p>
<pre class="console"><span class="koboSpan" id="kobo.363.1">
[Person: Michael Jordan], [Organization: Chicago Bulls], [Location: NBA]</span></pre>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.364.1">Let's look at </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.365.1">another example:</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.366.1">Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.367.1">: "Apple Inc., the tech giant based in Cupertino, California, unveiled its latest iPhone model at a </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">press event."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.369.1">The output looks </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">like this:</span></strong></span></p>
<pre class="console"><span class="koboSpan" id="kobo.371.1">
[Organization: Apple Inc.], [Location: Cupertino], [Location: California]</span></pre>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">'''</span></strong></p>
<p><span class="koboSpan" id="kobo.373.1">Let’s explore these use cases through a code example and generate an output by invoking an Anthropic</span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.374.1"> Claude 3 Sonnet FM on </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">Amazon Bedrock.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.376.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.377.1">Please ensure that you have the required libraries, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">boto3</span></strong><span class="koboSpan" id="kobo.379.1">, installed to run the code. </span><span class="koboSpan" id="kobo.379.2">If not, please install the library using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.380.1">pip install boto3</span></strong><span class="koboSpan" id="kobo.381.1"> command in </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">your editor.</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.383.1">Additionally, ensure that you have enabled access to the models available on Amazon Bedrock. </span><span class="koboSpan" id="kobo.383.2">For further documentation on model access on Bedrock, please </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">visit </span></span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html</span></span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.387.1">
# Import the respective libraries
import boto3
import botocore
import os
import json
import sys
#Create client-side Amazon Bedrock connection with Boto3 library
region = os.environ.get("AWS_REGION")
bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name=region)
prompt_data = """
Human: You are a helpful AI assistant. </span><span class="koboSpan" id="kobo.387.2">If you are unsure about the answer, say I do not know. </span><span class="koboSpan" id="kobo.387.3">Skip the preamble.
</span><span class="koboSpan" id="kobo.387.4">Task: Identify and classify the following entities in the given text:
Entity Types: Person, Organization, Location
Input Text: "Michael Jordan, the legendary basketball player for the Chicago Bulls, announced his retirement from the NBA after an illustrious career."
</span><span class="koboSpan" id="kobo.387.5">Assistant:
"""
messages=[{ "role":'user', "content":[{'type':'text','text': prompt_data}]}]
body=json.dumps(
        {
            "anthropic_version": "bedrock-2023-05-31",
            «max_tokens»: 512,
            «messages»: messages,
            «temperature»: 0.1,
            "top_p": 1
        }
    )
response = bedrock_runtime.invoke_model(body=body, modelId="anthropic.claude-3-sonnet-20240229-v1:0")
response_body = json.loads(response.get('body').read())
print(response_body['content'][0].get("text"))</span></pre>
<p><span class="koboSpan" id="kobo.388.1">Here’s a sample output from </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">the FM:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<span class="koboSpan" id="kobo.390.1"><img alt="Figure 8.1 – Sample output" src="image/B22045_08_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.391.1">Figure 8.1 – Sample output</span></p>
<p><span class="koboSpan" id="kobo.392.1">While this </span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.393.1">basic structure works for simple cases, more advanced prompting techniques are needed for robust, production-level </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">entity extraction.</span></span></p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.395.1">Incorporating context and domain knowledge</span></h2>
<p><span class="koboSpan" id="kobo.396.1">Entity extraction scenarios </span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.397.1">often benefit from contextual information and domain-specific knowledge. </span><span class="koboSpan" id="kobo.397.2">By providing relevant background or domain-specific details within the prompt, you can enhance the model’s understanding and improve its ability to accurately </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">identify entities.</span></span></p>
<p><span class="koboSpan" id="kobo.399.1">Here’s an example prompt </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">with context:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.401.1">```</span></strong></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.402.1">Task</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.403.1">: Identify and classify entities related to sports in the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.404.1">given text.</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">Entity Types: Athlete, Team, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">Tournament, Sport</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.407.1">Context</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">: This text discusses sports events, teams, and athletes involved in various </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.409.1">sports competitions.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.410.1">Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.411.1">: "Serena Williams, a well renowned Tennis player, defeated Venus Williams to win 23rd Grand Slam title at the 2017 </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">Australian Open."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">The output looks </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">like this:</span></strong></span></p>
<pre class="console"><span class="koboSpan" id="kobo.415.1">
[Athlete: Serena Williams], [Athlete: Venus Williams], [Tournament: Grand Slam], [Tournament: Australian Open], [Sport: tennis]</span></pre>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">```</span></strong></p>
<p><span class="koboSpan" id="kobo.417.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.418.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.419.1">.2</span></em><span class="koboSpan" id="kobo.420.1">, the code</span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.421.1"> sample for the preceding use case is depicted. </span><span class="koboSpan" id="kobo.421.2">It’s important to note that the code does not explicitly mention the installed libraries. </span><span class="koboSpan" id="kobo.421.3">It is assumed that users have already pre-installed the required Python packages and libraries, as detailed in the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">code sample:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<span class="koboSpan" id="kobo.423.1"><img alt="Figure 8.2 – Prompting Amazon Bedrock FM for entity extraction with contextual information" src="image/B22045_08_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.424.1">Figure 8.2 – Prompting Amazon Bedrock FM for entity extraction with contextual information</span></p>
<p><span class="koboSpan" id="kobo.425.1">It might produce favorable output for certain FMs based on the input instructions. </span><span class="koboSpan" id="kobo.425.2">However, in other</span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.426.1"> scenarios, it has the potential to generate hallucinated or irrelevant additional information, as demonstrated in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.427.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.428.1">.3</span></em><span class="koboSpan" id="kobo.429.1">. </span><span class="koboSpan" id="kobo.429.2">Therefore, employing few-shot prompting can be advantageous for entity extraction in </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">such cases:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<span class="koboSpan" id="kobo.431.1"><img alt="Figure 8.3 – AI21 Labs J2 Jumbo Instruct FM output" src="image/B22045_08_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.432.1">Figure 8.3 – AI21 Labs J2 Jumbo Instruct FM output</span></p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.433.1">Leveraging few-shot learning</span></h2>
<p><span class="koboSpan" id="kobo.434.1">As you are aware, few-shot </span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.435.1">learning involves providing the model with a small number of labeled examples during training or inference. </span><span class="koboSpan" id="kobo.435.2">This approach can be particularly effective for entity extraction tasks, as it allows the model to learn from a limited set of high-quality examples and generalize to new, </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">unseen data.</span></span></p>
<p><span class="koboSpan" id="kobo.437.1">Here’s an example prompt with </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">few-shot learning:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.439.1">```</span></strong></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.440.1">Task</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.441.1">: Identify and classify entities related to technology companies in the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.442.1">given text.</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.443.1">Entity Types</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.444.1">: Company, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.445.1">Product, Location</span></strong></span></p>
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.446.1">Few-Shot Examples</span></strong></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.447.1">:</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.448.1">Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">: "Microsoft, based in Redmond, Washington, unveiled its latest operating system, Windows 11, at a </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.450.1">virtual event."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">The output looks </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">like this:</span></strong></span></p>
<pre class="console"><span class="koboSpan" id="kobo.453.1">
[Company: Microsoft], [Product: Windows 11], [Location: Redmond], [Location: Washington]</span></pre>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.454.1">Here's </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.455.1">another example:</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.456.1">Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.457.1">: "Google's parent company, Alphabet Inc., announced plans to expand its data center operations in Iowa </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.458.1">and Nevada."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">The output looks </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.460.1">like this:</span></strong></span></p>
<pre class="console"><span class="koboSpan" id="kobo.461.1">
[Company: Alphabet Inc.], [Company: Google], [Location: Iowa], [Location: Nevada]</span></pre>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.462.1">Let's look at </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">another example:</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.464.1">Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">: "Samsung Electronics, the South Korean tech giant, launched its new flagship smartphone, the Galaxy S22, featuring a powerful camera and improved </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.466.1">battery life."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">The output looks </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">like this:</span></strong></span></p>
<pre class="console"><span class="koboSpan" id="kobo.469.1">
[Company: Samsung Electronics], [Product: Galaxy S22], [Location: South Korea]</span></pre>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.470.1">Now, look at the following </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">use case:</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.472.1">Your Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.473.1">: "Amazon, the e-commerce behemoth based in Seattle, Washington, unveiled its latest line of Echo smart speakers and Alexa-powered devices at a </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.474.1">hardware event."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">```</span></strong></p>
<p><span class="koboSpan" id="kobo.476.1">Let’s craft a code sample for the preceding use case and invoke the Amazon Titan model on </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">Amazon</span></span><span class="No-Break"><a id="_idIndexMarker686"/></span><span class="No-Break"><span class="koboSpan" id="kobo.478.1"> Bedrock:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.479.1">
# Import the respective libraries
import boto3
import botocore
import os
import json
import sys
#Create client-side Amazon Bedrock connection with Boto3 library
region = os.environ.get("AWS_REGION")
bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name=region)
prompt_data = """Task: Identify and classify entities related to technology companies in the given text.
</span><span class="koboSpan" id="kobo.479.2">Entity Types: Company, Product, Location
Few-Shot Examples:
Input Text: "Microsoft, based in Redmond, Washington, unveiled its latest operating system, Windows 11, at a virtual event."
</span><span class="koboSpan" id="kobo.479.3">Output: [Company: Microsoft], [Product: Windows 11], [Location: Redmond], [Location: Washington]
Input Text: "Google's parent company, Alphabet Inc., announced plans to expand its data center operations in Iowa and Nevada."
</span><span class="koboSpan" id="kobo.479.4">Output: [Company: Alphabet Inc.], [Company: Google], [Location: Iowa], [Location: Nevada]
Input Text: "Samsung Electronics, the South Korean tech giant, launched its new flagship smartphone, the Galaxy S22, featuring a powerful camera and improved battery life."
</span><span class="koboSpan" id="kobo.479.5">Output: [Company: Samsung Electronics], [Product: Galaxy S22], [Location: South Korea]
Your Input Text: "Amazon, the e-commerce behemoth based in Seattle, Washington, unveiled its latest line of Echo smart speakers and Alexa-powered devices at a hardware event."
</span><span class="koboSpan" id="kobo.479.6">Output:
"""
body = {
    "inputText": prompt_data
}
modelId = "amazon.titan-tg1-large"
accept = «application/json»
contentType = «application/json»
response = invoke_model(body, modelId, accept, contentType)
response_body = json.loads(response.get("body").read())
print(response_body.get("results")[0].get("outputText"))</span></pre>
<p><span class="koboSpan" id="kobo.480.1">Executing the preceding code generates the following output, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.481.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.482.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.484.1">
[Company: Amazon], [Product: Echo smart speakers, Alexa-powered devices], [Location: Seattle], [Location: Washington]</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer123">
<span class="koboSpan" id="kobo.485.1"><img alt="Figure 8.4 – Generated output from Amazon Titan FM" src="image/B22045_08_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.486.1">Figure 8.4 – Generated output from Amazon Titan FM</span></p>
<p><span class="koboSpan" id="kobo.487.1">Therefore, in this example, the prompt offers a set of labeled instances to assist the model in understanding the entity extraction task within the technology domain. </span><span class="koboSpan" id="kobo.487.2">Through the</span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.488.1"> utilization of few-shot learning, the model can proficiently generalize to unfamiliar input text, all while upholding a high level </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">of accuracy.</span></span></p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.490.1">Iterative refinement and evaluation</span></h2>
<p><span class="koboSpan" id="kobo.491.1">Prompt engineering </span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.492.1">constitutes an iterative process that frequently necessitates refinement and evaluation. </span><span class="koboSpan" id="kobo.492.2">As you explore various prompts and techniques, it’s vital to assess the model’s performance through automatic model evaluation or human evaluation methods, as elaborated upon in </span><a href="B22045_11.xhtml#_idTextAnchor207"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.493.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.494.1">. </span><span class="koboSpan" id="kobo.494.2">Through careful analysis of the model’s outputs and identifying areas for enhancement, you can iteratively refine your prompts, thereby augmenting the overall accuracy of your entity </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">extraction system.</span></span></p>
<p><span class="koboSpan" id="kobo.496.1">Take a look at the following example of model analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">and refinement:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.498.1">'''</span></strong></p>
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.499.1">Initial Prompt</span></strong></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.500.1">:</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.501.1">Task</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.502.1">: Identify and classify entities in the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">given text.</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.504.1">Entity Types: Person, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">Organization, Location</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.506.1">Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.507.1">: "Elon Musk, the CEO of Tesla Inc., announced plans to build a new Gigafactory in </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.508.1">Austin, Texas."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.509.1">The output looks </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">like this:</span></strong></span></p>
<pre class="console"><span class="koboSpan" id="kobo.511.1">
[Person: Elon Musk], [Organization: Tesla Inc.], [Location: Austin]</span></pre>
<p><strong class="bold"><span class="koboSpan" id="kobo.512.1">Output Analysis</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.513.1">: The model correctly identified the person and organization entities but missed the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.514.1">location "Texas."</span></strong></span></p>
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.515.1">Refined Prompt</span></strong></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">:</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.517.1">Task</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.518.1">: Identify and classify entities in the given text, including nested or </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.519.1">multi-word entities.</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">Entity Types: Person, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.521.1">Organization, Location</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.522.1">Input Text</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.523.1">: "Elon Musk, the CEO of Tesla Inc., announced plans to build a new Gigafactory in </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">Austin, Texas."</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.525.1">The output looks </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.526.1">like this:</span></strong></span></p>
<pre class="console"><span class="koboSpan" id="kobo.527.1">
[Person: Elon Musk], [Organization: Tesla Inc.], [Location: Austin, Texas]</span></pre>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">'''</span></strong></p>
<p><span class="koboSpan" id="kobo.529.1">By refining the prompt to include instructions for handling nested or multi-word entities, the model’s performance improved, correctly identifying the location as </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">Austin, Texas.</span></span></p>
<p><span class="koboSpan" id="kobo.531.1">We encourage </span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.532.1">users to run the provided code on Amazon Bedrock to extract pertinent entities using the Claude 3 model and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">Messages</span></strong><span class="koboSpan" id="kobo.534.1"> API. </span><span class="koboSpan" id="kobo.534.2">As mentioned earlier, please ensure that access to these models on Amazon Bedrock is enabled. </span><span class="koboSpan" id="kobo.534.3">For further documentation on accessing models on Bedrock, please </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">visit </span></span><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html"><span class="No-Break"><span class="koboSpan" id="kobo.536.1">https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.537.1">.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.538.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.539.1">Make sure you have the </span><strong class="source-inline"><span class="koboSpan" id="kobo.540.1">boto3</span></strong><span class="koboSpan" id="kobo.541.1"> library installed, as explained in the previous chapters. </span><span class="koboSpan" id="kobo.541.2">If not, please install the latest version using the following command: </span><strong class="source-inline"><span class="koboSpan" id="kobo.542.1">pip </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.543.1">install boto3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.545.1">
#importing the relevant libraries
import boto3
import json
#Creating Bedrock client and region
bedrock_client = boto3.client('bedrock-runtime',region_name='us-east-1')
prompt = """
Task: Identify and classify entities in the given text.
</span><span class="koboSpan" id="kobo.545.2">Entity Types: Person, Organization, Location
Input Text: "Elon Musk, the CEO of Tesla Inc., announced plans to build a new Gigafactory in Austin, Texas.
</span><span class="koboSpan" id="kobo.545.3">Output:
"""
messages = [{ "role":'user', "content":[{'type':'text','text': prompt}]}]
max_tokens=512
top_p=1
temp=0.5
system = "You are an AI Assistant"
body=json.dumps(
        {
            "anthropic_version": "bedrock-2023-05-31",
            «max_tokens»: max_tokens,
            «messages»: messages,
            "temperature": temp,
            "top_p": top_p,
            "system": system
        }
    )
response = bedrock_client.invoke_model(body= body, modelId = "anthropic.claude-3-sonnet-20240229-v1:0")
response_body = json.loads(response.get('body').read())
print(response_body)</span></pre>
<p><span class="koboSpan" id="kobo.546.1">Printing </span><strong class="source-inline"><span class="koboSpan" id="kobo.547.1">response_body</span></strong><span class="koboSpan" id="kobo.548.1"> as shown in </span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.549.1">the preceding snippet might yield the following output, </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">as expected:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.551.1">
{'id': 'msg_01RqxLfg6hEEu1K8jY3g8gzq',
 'type': 'message',
 'role': 'assistant',
 'content': [{'type': 'text',
   'text': 'Person: Elon Musk\nOrganization: Tesla Inc.\nLocation: Austin, Texas'}],
 'model': 'claude-3-sonnet-28k-20240229',
 'stop_reason': 'end_turn',
 'stop_sequence': None,
 'usage': {'input_tokens': 71, 'output_tokens': 23}}</span></pre>
<p><span class="koboSpan" id="kobo.552.1">Hence, by leveraging effective prompt engineering techniques with Amazon Bedrock, such as providing clear instructions, relevant examples, and handling ambiguity, GenAI models can be guided to perform high-quality entity extraction across several use cases and different domains. </span><span class="koboSpan" id="kobo.552.2">As with any AI application, it requires careful design, testing, and refinement to build a truly </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">production-ready system.</span></span></p>
<p><span class="koboSpan" id="kobo.554.1">As LLMs continue to grow in size and complexity, their capabilities in entity extraction are expected to further improve, enabling more accurate and </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">robust solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.556.1">Ongoing research also focuses on integrating external knowledge sources such as knowledge graphs or ontologies into LLMs for entity extraction. </span><span class="koboSpan" id="kobo.556.2">By embedding structured knowledge into the model’s architecture or training regimen, these methods have the potential to enrich the model’s comprehension of entities and their interconnections, thereby potentially enhancing both performance </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">and interpretability.</span></span></p>
<p><span class="koboSpan" id="kobo.558.1">Check the following AWS blog showcasing the integration of </span><strong class="bold"><span class="koboSpan" id="kobo.559.1">intelligent document processing</span></strong><span class="koboSpan" id="kobo.560.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.561.1">IDP</span></strong><span class="koboSpan" id="kobo.562.1">) in the </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.563.1">context of entity extraction automation using AWS AI/ML services such as Amazon Textract with Amazon Bedrock and </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">LangChain: </span></span><a href="https://aws.amazon.com/blogs/machine-learning/intelligent-document-processing-with-amazon-textract-amazon-bedrock-and-langchain/"><span class="No-Break"><span class="koboSpan" id="kobo.565.1">https://aws.amazon.com/blogs/machine-learning/intelligent-document-processing-with-amazon-textract-amazon-bedrock-and-langchain/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.566.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.567.1">This solution proves particularly beneficial for handling handwritten or scanned documents, encompassing the extraction of pertinent data from various file formats such as PDF, PNG, TIFF, and JPEG, regardless of the document layout. </span><span class="koboSpan" id="kobo.567.2">The Amazon Textract service</span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.568.1"> facilitates the automatic extraction of text, handwriting, and data from such </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">scanned documents.</span></span></p>
<p><span class="koboSpan" id="kobo.570.1">Consequently, this solution capitalizes on the strengths of each component: Amazon Textract for precise data extraction, Amazon Bedrock for streamlined data processing pipelines, and LangChain for seamlessly integrating LLMs into the workflow. </span><span class="koboSpan" id="kobo.570.2">Overall, the blog post offers a pragmatic solution for automating document processing tasks, underscoring the advantages of leveraging AWS services and open source frameworks such as LangChain to develop intelligent applications. </span><span class="koboSpan" id="kobo.570.3">Therefore, it holds substantial potential for diverse document processing scenarios, providing dynamic adaptability to evolving </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">data patterns.</span></span></p>
<p><span class="koboSpan" id="kobo.572.1">Additional examples of entity extraction with Bedrock have been added here: </span><a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/01_Text_generation/04_entity_extraction.ipynb"><span class="koboSpan" id="kobo.573.1">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/01_Text_generation/04_entity_extraction.ipynb</span></a><span class="koboSpan" id="kobo.574.1">. </span><span class="koboSpan" id="kobo.574.2">Users are encouraged to run and execute the code cells to gain a much better understanding of entity extraction using Amazon Bedrock for GenAI </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">use cases.</span></span></p>
<p><span class="koboSpan" id="kobo.576.1">Now that you</span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.577.1"> have grasped the concepts of entity extraction in more detail, we will dive into more code generation scenarios in the universe of </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">Amazon Bedrock.</span></span></p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.579.1">Code generation with LLMs – unleashing the power of AI-driven development</span></h1>
<p><span class="koboSpan" id="kobo.580.1">As the field of AI</span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.581.1"> continues to evolve, one of the most exciting and promising areas is the use of LLMs for code generation, especially in the case of developer productivity gains. </span><span class="koboSpan" id="kobo.581.2">Customers can leverage state-of-the-art LLMs available on Amazon Bedrock to generate high-quality code, revolutionizing the way developers approach </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">software development.</span></span></p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.583.1">The code generation process</span></h2>
<p><span class="koboSpan" id="kobo.584.1">The code generation</span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.585.1"> process with Amazon Bedrock is straightforward and user-friendly. </span><span class="koboSpan" id="kobo.585.2">Developers can interact with the platform through a web-based interface or via an API, as discussed in the previous chapters. </span><span class="koboSpan" id="kobo.585.3">The process typically involves the </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">following steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.587.1">Problem description</span></strong><span class="koboSpan" id="kobo.588.1">: The developer provides a natural language description of the desired functionality or task that they want the code </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">to perform.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.590.1">Context and constraints</span></strong><span class="koboSpan" id="kobo.591.1">: The developer can optionally provide additional context, such as programming language preferences, coding styles, or specific libraries or frameworks to </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">be used.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.593.1">LLM code generation</span></strong><span class="koboSpan" id="kobo.594.1">: Amazon Bedrock’s LLMs analyze the problem description and any provided context and generate the </span><span class="No-Break"><span class="koboSpan" id="kobo.595.1">corresponding code.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.596.1">Code refinement</span></strong><span class="koboSpan" id="kobo.597.1">: The generated code can be iteratively refined through additional prompts or feedback from the developer, allowing for a collaborative and </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">interactive process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.599.1">Code integration</span></strong><span class="koboSpan" id="kobo.600.1">: The final generated code can be seamlessly integrated into the developer’s </span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.601.1">project or </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">code base.</span></span></li>
</ol>
<h2 id="_idParaDest-156"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.603.1">Benefits of code generation with Amazon Bedrock</span></h2>
<p><span class="koboSpan" id="kobo.604.1">Leveraging LLMs for code</span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.605.1"> generation offers numerous benefits to developers, including </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.607.1">Increased productivity</span></strong><span class="koboSpan" id="kobo.608.1">: With Amazon Bedrock, developers can quickly generate code for various tasks and functionalities, reducing the time and effort required for </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">manual coding</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.610.1">Improved code quality</span></strong><span class="koboSpan" id="kobo.611.1">: The code generated by Amazon Bedrock’s LLMs can provide high-quality outputs, adhering to best practices and coding standards based on the iterative refinement of </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">the prompts</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.613.1">Reduced errors</span></strong><span class="koboSpan" id="kobo.614.1">: LLMs can help reduce the likelihood of common coding errors, such as syntax errors or logical flaws, by generating correct and coherent code with </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">prompt engineering</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.616.1">Exploration and prototyping</span></strong><span class="koboSpan" id="kobo.617.1">: Bedrock enables developers to rapidly explore and prototype different ideas and approaches, facilitating more efficient and </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">creative problem-solving</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.619.1">Accessibility</span></strong><span class="koboSpan" id="kobo.620.1">: By leveraging natural language descriptions and FMs for code generation purposes (Llama, Claude, Titan, Mistral, and so on), Amazon Bedrock makes code generation more accessible to developers with varying levels of expertise </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">or backgrounds</span></span></li>
</ul>
<h2 id="_idParaDest-157"><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.622.1">Limitations and considerations</span></h2>
<p><span class="koboSpan" id="kobo.623.1">While LLM-based code generation offers numerous advantages, it is important to be aware of their limitations </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">and</span></span><span class="No-Break"><a id="_idIndexMarker698"/></span><span class="No-Break"><span class="koboSpan" id="kobo.625.1"> considerations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.626.1">Specialized domain knowledge</span></strong><span class="koboSpan" id="kobo.627.1">: LLMs may not always generate code that requires highly specialized domain knowledge or complex algorithms. </span><span class="koboSpan" id="kobo.627.2">Human expertise and review may still be necessary in </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">certain cases.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.629.1">Security and compliance</span></strong><span class="koboSpan" id="kobo.630.1">: Generated code should be thoroughly reviewed and tested to ensure it adheres to security best practices and any relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">compliance requirements.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.632.1">Integration and maintenance</span></strong><span class="koboSpan" id="kobo.633.1">: Generated code may need to be adapted and maintained over time as requirements or </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">dependencies change.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.635.1">Ethical considerations</span></strong><span class="koboSpan" id="kobo.636.1">: As with any AI system, it is crucial to ensure LLMs are used responsibly</span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.637.1"> and ethically, considering potential biases or </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">unintended consequences.</span></span></li>
</ul>
<h2 id="_idParaDest-158"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.639.1">Use cases and examples</span></h2>
<p><span class="koboSpan" id="kobo.640.1">Amazon Bedrock’s code</span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.641.1"> generation capabilities can be applied to a wide range of use cases across various domains and programming languages. </span><span class="koboSpan" id="kobo.641.2">Some examples include </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.643.1">Web development</span></strong><span class="koboSpan" id="kobo.644.1">: Developers can generate code using Bedrock for web applications, APIs, or user interfaces using languages such as JavaScript, Python, </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">or Ruby.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.646.1">Data processing and analysis</span></strong><span class="koboSpan" id="kobo.647.1">: Developers can leverage Bedrock to write code for data manipulation, analysis, and visualization tasks using languages such as Python </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">or R.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.649.1">Mobile app development</span></strong><span class="koboSpan" id="kobo.650.1">: Bedrock can be utilized to generate code for mobile applications using languages such as Swift, Kotlin, or </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">React Native.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.652.1">Embedded systems and Internet of Things (IoT) devices</span></strong><span class="koboSpan" id="kobo.653.1">: Developers can create code for embedded systems, microcontrollers, or IoT devices using languages such as C, C++, or Rust with the assistance of </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">Bedrock models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.655.1">Scientific computing</span></strong><span class="koboSpan" id="kobo.656.1">: Bedrock can aid in writing code for scientific simulations, numerical calculations, or data processing tasks using languages such as MATLAB, Julia, or Fortran through its code </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">generation features.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.658.1">Now, let’s look at a few examples of code generation, debugging, or code transformation</span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.659.1"> use cases with </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">Amazon Bedrock.</span></span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.661.1">Prompt engineering examples with Amazon Bedrock</span></h2>
<p><span class="koboSpan" id="kobo.662.1">Here is a sample</span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.663.1"> prompt</span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.664.1"> given to a Claude 3 Sonnet model within Amazon Bedrock to adopt the role of a Python developer and perform a code </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">generation task:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.666.1">Human: You are an expert Python developer tasked with coding a web scraper for an experienced developer. </span><span class="koboSpan" id="kobo.666.2">The scraper should extract data from multiple web pages and store the results in a SQLite database. </span><span class="koboSpan" id="kobo.666.3">Write clean, high-quality Python code for this task, including necessary imports. </span><span class="koboSpan" id="kobo.666.4">Do not write anything before the ```python block. </span><span class="koboSpan" id="kobo.666.5">After writing the code, carefully check for errors. </span><span class="koboSpan" id="kobo.666.6">If errors exist, list them within &lt;error&gt; tags and provide a new corrected version. </span><span class="koboSpan" id="kobo.666.7">If no errors, write "CHECKED: NO ERRORS" within &lt;</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.667.1">error&gt; tags.</span></strong></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.668.1">Assistant:</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.669.1">Let us execute this prompt in code using Anthropic Claude 3 model on Amazon Bedrock. </span><span class="koboSpan" id="kobo.669.2">As covered in the previous sections, please ensure you have the necessary libraries installed and have the required permissions to invoke the model on </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.670.1">Amazon Bedrock:</span></strong></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.671.1">
# Import the respective libraries
import boto3
import botocore
import os
import json
import sys
#Create client side Amazon Bedrock connection with Boto3 library
region = os.environ.get("AWS_REGION")
bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name=region)
# Adding prompt example here:
prompt_data = """Human: You are an expert Python developer tasked with coding a web scraper for an experienced developer. </span><span class="koboSpan" id="kobo.671.2">The scraper should extract data from multiple web pages and store the results in a SQLite database. </span><span class="koboSpan" id="kobo.671.3">Write clean, high-quality Python code for this task, including necessary imports. </span><span class="koboSpan" id="kobo.671.4">Do not write anything before the ```python block. </span><span class="koboSpan" id="kobo.671.5">After writing the code, carefully check for errors. </span><span class="koboSpan" id="kobo.671.6">If errors exist, list them within &lt;error&gt; tags and provide a new corrected version. </span><span class="koboSpan" id="kobo.671.7">If no errors, write "CHECKED: NO ERRORS" within &lt;error&gt; tags.
</span><span class="koboSpan" id="kobo.671.8">Assistant:
"""
# Using Messages API with Anthropic Claude
messages=[{ "role":'user', "content":[{'type':'text','text': prompt_data}]}]
body=json.dumps(
        {
            "anthropic_version": "bedrock-2023-05-31",
            «max_tokens»: 512,
            «messages»: messages,
            «temperature»: 0.1,
            "top_p": 1
        }
    )
response = bedrock_runtime.invoke_model(body=body, modelId="anthropic.claude-3-sonnet-20240229-v1:0")
response_body = json.loads(response.get('body').read())</span></pre>
<p><span class="koboSpan" id="kobo.672.1">We won’t dive into the entirety of the output generated, but provided in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.673.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.674.1">.5</span></em><span class="koboSpan" id="kobo.675.1"> is a code snippet generated as a result of invoking a Claude 3 Sonnet model via Amazon Bedrock API</span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.676.1"> with</span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.677.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">preceding prompt:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<span class="koboSpan" id="kobo.679.1"><img alt="Figure 8.5 – Output code snippet generated by invoking Claude 3 Sonnet model via Amazon Bedrock" src="image/B22045_08_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.680.1">Figure 8.5 – Output code snippet generated by invoking Claude 3 Sonnet model via Amazon Bedrock</span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.681.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.682.1">.6</span></em><span class="koboSpan" id="kobo.683.1"> shows yet another example of a code debugging use case, leveraging a Llama 2 Chat 13B model available on Amazon Bedrock within the </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">chat playground:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer125">
<span class="koboSpan" id="kobo.685.1"><img alt="Figure 8.6 – Code debugging using Llama 2 Chat 13B model on Amazon Bedrock" src="image/B22045_08_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.686.1">Figure 8.6 – Code debugging using Llama 2 Chat 13B model on Amazon Bedrock</span></p>
<p><span class="koboSpan" id="kobo.687.1">Now, let’s take a look at a code translation scenario. </span><span class="koboSpan" id="kobo.687.2">Here’s an example prompt for a code translation </span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.688.1">use</span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.689.1"> case with a Mixtral 8X7B instruct model on Amazon Bedrock, followed by a </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">generated output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.691.1">
# Import the respective libraries
import boto3
import botocore
import os
import json
import sys
#Create client-side Amazon Bedrock connection with Boto3 library
region = os.environ.get("AWS_REGION")
bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name=region)
prompt="""
[INST] You are an AI code translator specialized in converting code between different programming languages while preserving functionality, readability, and style. </span><span class="koboSpan" id="kobo.691.2">Your task is to translate the provided Python code to JavaScript.
</span><span class="koboSpan" id="kobo.691.3">Here is a brief description of the code's purpose:
This code defines a class called 'BankAccount' that represents a basic bank account. </span><span class="koboSpan" id="kobo.691.4">It has methods to deposit and withdraw money, as well as to check the account balance.
</span><span class="koboSpan" id="kobo.691.5">Python code is in the following code block:
```python
class BankAccount:
    def __init__(self, initial_balance=0):
        self.balance = initial_balance
    def deposit(self, amount):
        self.balance += amount
    def withdraw(self, amount):
        if self.balance &gt;= amount:
            self.balance -= amount
        else:
            print("Insufficient funds")
    def check_balance(self):
        return self.balance
```
Translate the above Python code to JavaScript, maintaining clear variable naming, proper indentation, and adding comments to explain any non-trivial logic or language-specific constructs used in the translation.
</span><span class="koboSpan" id="kobo.691.6">After translating the code, carefully review your work for any errors or inconsistencies. </span><span class="koboSpan" id="kobo.691.7">If errors are found, list them within `&lt;error&gt;` tags and provide a corrected version. </span><span class="koboSpan" id="kobo.691.8">If no errors are present, write "CHECKED: NO ERRORS" within `&lt;error&gt;` tags.
</span><span class="koboSpan" id="kobo.691.9">[/INST]"""
#Model ID and parameter details
modelId = 'mistral.mixtral-8x7b-instruct-v0:1'
accept = 'application/json'
contentType = 'application/json'
body = json.dumps({
    'prompt': prompt,
    'max_tokens': 1000,
    'top_p': 0.9,
    'temperature': 0.2,
})
#Output response
response = bedrock_runtime.invoke_model(body=body.encode('utf-8'), # Encode to bytes
      modelId=modelId,
      accept=accept,
      contentType=contentType)
response_body = json.loads(response.get('body').read().decode('utf-8'))
print(response_body.get('outputs')[0].get('text'))</span></pre>
<p><span class="koboSpan" id="kobo.692.1">The translated</span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.693.1"> JavaScript </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.694.1">code output looks </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">like this:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.696.1">
Here's the Python code translated into JavaScript with clear variable naming, proper indentation, and comments explaining non-trivial logic or language-specific constructs used in the translation:
```javascript
// Define the BankAccount class
class BankAccount {
  // Initialize the balance property with an initial_balance of 0
  constructor(initial_balance = 0) {
    this.balance = initial_balance;
  }
  // Deposit method to add amount to the balance
  deposit(amount) {
    this.balance += amount;
  }
  // Withdraw method to subtract amount from the balance if there are sufficient funds
  withdraw(amount) {
    if (this.balance &gt;= amount) {
      this.balance -= amount;
    } else {
      console.log("Insufficient funds");
    }
  }
  // Check_balance method to return the current balance
  check_balance() {
    return this.balance;
  }
}
```
After reviewing the translated code, no errors or inconsistencies were found.
</span><span class="koboSpan" id="kobo.696.2">&lt;error&gt;NO ERRORS&lt;/error&gt;</span></pre>
<p><span class="koboSpan" id="kobo.697.1">In this example, the prompt provides context about the code’s purpose and the original Python code and instructs Code Llama to translate it to JavaScript. </span><span class="koboSpan" id="kobo.697.2">The model is asked to maintain clear </span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.698.1">variable</span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.699.1"> naming and proper indentation and add comments to explain non-trivial logic or </span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">language-specific constructs.</span></span></p>
<p><span class="koboSpan" id="kobo.701.1">The generated output shows the translated JavaScript code, with the class structure and methods translated correctly while preserving the original functionality. </span><span class="koboSpan" id="kobo.701.2">After translating the code, the model has carefully reviewed its work and indicated </span><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">CHECKED: NO ERRORS</span></strong><span class="koboSpan" id="kobo.703.1"> within </span><strong class="source-inline"><span class="koboSpan" id="kobo.704.1">&lt;error&gt;</span></strong><span class="koboSpan" id="kobo.705.1"> tags, signifying that the translation is correct </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">and error-free.</span></span></p>
<p><span class="koboSpan" id="kobo.707.1">This example demonstrates how a prompt can be crafted to guide Code Llama (or similar AI code models) to perform code translation tasks while ensuring the translated code is verified and correct. </span><span class="koboSpan" id="kobo.707.2">Note that it is always a best practice to perform a human evaluation of the generated output to verify the accuracy of these models and rectify </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">any issues.</span></span></p>
<p><span class="koboSpan" id="kobo.709.1">Users are encouraged to try these examples within the Amazon Bedrock playground or leveraging Amazon Bedrock APIs with several other models such as Amazon Titan, Cohere Command, Meta Llama, and alternate variations of Anthropic Claude or Mistral models to test the generated output and refine </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">it further.</span></span></p>
<p><span class="koboSpan" id="kobo.711.1">Users are further invited to explore this code sample where Amazon Bedrock LLMs are being invoked with zero-shot prompting to generate SQL and Python </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">programs: </span></span><a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/01_Text_generation/01_code_generation_w_bedrock.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.713.1">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/01_Text_generation/01_code_generation_w_bedrock.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.714.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.715.1">Entity extraction with GenAI represents a significant step forward in our ability to extract valuable insights from unstructured text data. </span><span class="koboSpan" id="kobo.715.2">By leveraging the power of LLMs and combining them with rule-based techniques, hybrid approaches offer accurate, scalable, and domain-adaptable solutions for a wide range of applications. </span><span class="koboSpan" id="kobo.715.3">As we continue to push the boundaries of these areas, we can expect to unlock new opportunities for knowledge discovery, decision support, and data-driven innovation across various industries </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">and domains.</span></span></p>
<p><span class="koboSpan" id="kobo.717.1">The field of LLM-based code generation is also rapidly evolving, and Amazon Bedrock is at the forefront of this exciting development. </span><span class="koboSpan" id="kobo.717.2">As LLMs become more advanced and the available training data continues to grow, the capabilities and applications of code generation will expand further. </span><span class="koboSpan" id="kobo.717.3">Amazon Bedrock represents a significant step forward in the realm of code generation, empowering developers to leverage the power of LLMs to increase productivity, improve code quality, and explore new ideas more efficiently. </span><span class="koboSpan" id="kobo.717.4">As this technology </span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.718.1">continues </span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.719.1">to mature, it has the potential to revolutionize the way software is developed and open up new possibilities for innovation across various industries </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">and domains.</span></span></p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.721.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.722.1">This chapter commenced with an in-depth exploration of entity extraction, uncovering its fundamentals, techniques, and best practices. </span><span class="koboSpan" id="kobo.722.2">It then transitioned to showcasing potential industrial applications of entity extraction, highlighting real-world use cases that demonstrate the power of unlocking valuable insights from unstructured data across </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">various sectors.</span></span></p>
<p><span class="koboSpan" id="kobo.724.1">Recognizing the pivotal role of prompt engineering, the chapter further provided a comprehensive guide to crafting effective prompts, equipping readers with strategies and guidelines to optimize entity extraction performance. </span><span class="koboSpan" id="kobo.724.2">Shifting gears, the discussion then centered on the transformative potential of code generation with LLMs on Amazon Bedrock. </span><span class="koboSpan" id="kobo.724.3">We gained insights into the capabilities and limitations of LLMs in driving AI-based development, as well as methodologies for leveraging these </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">cutting-edge models.</span></span></p>
<p><span class="koboSpan" id="kobo.726.1">Finally, the chapter culminated with a compelling exploration of practical use cases for code generation, demonstrating how this technology can accelerate innovation and boost productivity across various domains. </span><span class="koboSpan" id="kobo.726.2">Through real-world examples and case studies, readers witnessed firsthand the profound impact of code generation on streamlining development processes and unleashing new possibilities. </span><span class="koboSpan" id="kobo.726.3">In the following chapter, we are going to explore image generation use cases with Amazon Bedrock, along with its potential applications. </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">Stay tuned!</span></span></p>
</div>
</body></html>