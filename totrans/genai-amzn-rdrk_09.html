<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-161"><a id="_idTextAnchor171"/>9</h1>
<h1 id="_idParaDest-162"><a id="_idTextAnchor172"/>Generating and Transforming Images Using Amazon Bedrock</h1>
<p>By now, we have explored several LLMs capable of generating textual responses. This chapter explores generating images using select FMs that are available on Amazon Bedrock. We will start with an overview of image generation, wherein we will examine model architectures such as<a id="_idIndexMarker714"/> GANs and <strong class="bold">variational autoencoders</strong> (<strong class="bold">VAEs</strong>) Then, we will cover some real-world applications of image generation and multimodal models available within Amazon Bedrock. Furthermore, we will dive deeper into several multimodal design patterns, as well as some ethical considerations and safeguards that are available with Amazon Bedrock.</p>
<p>By the end of this chapter, you will have gained an understanding of implementing image generation and its design patterns with Amazon Bedrock for real-world use cases.</p>
<p>Here are the key topics that will be covered in this chapter:<a id="_idTextAnchor173"/><a id="_idTextAnchor174"/><a id="_idTextAnchor175"/></p>
<ul>
<li>Image generation overview</li>
<li>Multimodal models</li>
<li>Multimodal design patterns</li>
<li>Ethical considerations and safeguards</li>
</ul>
<h1 id="_idParaDest-163"><a id="_idTextAnchor176"/>Technical requirements</h1>
<p>This chapter requires you to have access to an AWS account. If you don’t have one already, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create one.</p>
<p>Secondly, you will need to install and configure AWS CLI at <a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a> after you create an account, which will be needed to access Amazon Bedrock FMs from your local machine. Since the majority of the code cells that we will be executing are based in Python, setting up an AWS Python SDK (Boto3) at <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a> would be beneficial at this point. You can carry out the Python setup in any way. Install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or leverage Amazon SageMaker.</p>
<p class="callout-heading">Note</p>
<p class="callout">There will be a charge associated with the invocation and customization of the FMs of Amazon Bedrock. Please refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor177"/>Image generation overview</h1>
<p>Image generation<a id="_idIndexMarker715"/> has been a fascinating and rapidly evolving field. Since the dawn of advanced deep learning techniques and increasing computational power, machines have gained the remarkable ability to create highly realistic and sophisticated images from scratch or based on textual prompts. This ability has opened up a vast array of applications across various domains, including the creative industries, media and entertainment, advertising, product packaging, and many others.</p>
<p>The history of image generation can be traced back to early developments in computer vision and pattern recognition. Researchers and scientists have long sought to understand and replicate the human visual perception system, paving the way for the initial techniques in image synthesis and manipulation. However, the true breakthrough in image generation came with the emergence of deep learning, specifically the introduction of<a id="_idIndexMarker716"/> GANs and <a id="_idIndexMarker717"/>VAEs.</p>
<p>Please note that we are highlighting these techniques for historical reference. Current image generation FMs do not use these techniques.</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor178"/>What are GANs and VAEs?</h2>
<p>GANs, introduced<a id="_idIndexMarker718"/> by Ian Goodfellow and his colleagues in 2014, revolutionized the field of image generation. You can read more about it on <a href="https://arxiv.org/pdf/1406.2661">https://arxiv.org/pdf/1406.2661</a>. GANs employ a unique training approach whereby two neural networks are pitted against each other in competition. The first network is known as<a id="_idIndexMarker719"/> the <strong class="bold">generator</strong>, which is tasked with generating synthetic samples that mimic real data. For example, the generator could produce new images, texts, or audio clips. The second network is called<a id="_idIndexMarker720"/> the <strong class="bold">discriminator</strong>. Its role is to analyze examples, both real and synthetic, to classify which ones are genuine and which have been artificially generated.</p>
<p>Through this <a id="_idIndexMarker721"/>adversarial process, the generator learns to produce increasingly convincing fakes that can fool the discriminator. Meanwhile, the discriminator evolves in its ability to detect subtle anomalies that reveal synthetic samples. Their competing goals drive both networks to continuously improve. A demonstration of GANs can be seen at <a href="https://thispersondoesnotexist.com/">https://thispersondoesnotexist.com/</a>. By refreshing the page endlessly, users are presented with an endless stream of novel human faces. However, none of those faces are real – all are synthetic portraits created solely by a GAN trained on vast databases of images of real human faces. The site offers a glimpse into how GANs can synthesize highly realistic outputs across many domains.</p>
<p>Since the inception of GANs, numerous advancements and variations have been implemented, leading to remarkable achievements in image generation. Techniques such as StyleGAN, BigGAN, and diffusion models have pushed the boundaries of image quality, resolution, and diversity. These models can generate photorealistic images of human faces, landscapes, objects, and even artistic creations, blurring the line between artificial and real.</p>
<p>VAEs, on the <a id="_idIndexMarker722"/>other hand, are a simpler means to train generative AI algorithms. They also utilize two neural <a id="_idIndexMarker723"/>networks: <strong class="bold">encoders</strong> and <strong class="bold">decoders</strong>. Encoders <a id="_idIndexMarker724"/>learn the patterns in the data by mapping it into lower-dimensional latent space; decoders use these patterns from the latent space and generate realistic samples.</p>
<p>One of the most exciting developments in image generation has been the integration of NLP capabilities. Models such as DALL-E, Stable Diffusion, and Midjourney have empowered users to generate images simply by providing textual descriptions or prompts. This fusion of language and vision has opened up new avenues for creative expression, rapid prototyping, and data augmentation for various ML tasks.</p>
<p>While the advancements in image generation are remarkable, it is crucial to address the ethical considerations and potential risks associated with this technology. Issues such as deepfakes, biases, and misuse for malicious purposes must be carefully addressed to ensure the responsible and ethical deployment of these powerful tools. We will look at this topic in detail in the <em class="italic">Ethical considerations and safeguards</em> section of this chapter.</p>
<p>Let us look at some real-world applications for image generation models.</p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor179"/>Real-world applications</h2>
<p>The <a id="_idIndexMarker725"/>applications of image generation are endless. Here are some of the real-world applications of image generation:</p>
<ul>
<li><strong class="bold">Advertising and marketing</strong>: In the world of advertising and marketing, visuals play a crucial role in capturing attention and conveying messages effectively. With image generation, you can revolutionize marketing campaigns by producing unique, visually striking images tailored to specific target audiences. Marketers can leverage Bedrock models to generate personalized product advertisements, social media visuals, and eye-catching graphics that resonate with their desired demographics. Furthermore, marketers can create variations of images based on customer preferences, ensuring that marketing materials are highly relevant and engaging.</li>
<li><strong class="bold">Graphic design and content creation</strong>: Graphic designers and content creators often face the challenge of conceptualizing and visualizing ideas before executing them. With Bedrock’s image generation models, you can streamline this process by relying on this powerful tool for generating initial concepts, illustrations, and visual assets. Designers can use image generation models to explore different styles, compositions, and color schemes, facilitating quick iterations and experimentation. Additionally, content creators can leverage Bedrock models to generate unique and captivating images for blog posts, articles, or other marketing materials, enhancing their visual appeal and potential for engagement.</li>
<li><strong class="bold">Product visualization and prototyping</strong>: Effective product visualization is essential for iterating designs, gathering feedback, and showcasing offerings. With Bedrock image generation models, businesses can generate realistic visualizations of product designs, allowing for rapid prototyping and evaluation before investing in physical prototypes. Bedrock models can create images of products in various environments or from different angles, providing stakeholders with a comprehensive understanding of the product’s appearance and functionality. This capability can significantly accelerate the product development cycle and aid in marketing and sales efforts.</li>
<li><strong class="bold">Gaming and virtual environments</strong>: The gaming and <strong class="bold">Virtual Reality</strong> (<strong class="bold">VR</strong>) industries <a id="_idIndexMarker726"/>heavily rely on visually immersive <a id="_idIndexMarker727"/>experiences. Bedrock’s image generation models can empower developers to create unique textures, environments, and assets for video<a id="_idIndexMarker728"/> games, VR, or <strong class="bold">Augmented Reality</strong> (<strong class="bold">AR</strong>) applications. Bedrock image models can generate custom avatars, character designs, and intricate visual elements based on user specifications or game narratives. In addition, developers can enhance the realism and diversity of their virtual worlds, offering players a more engaging and personalized experience.</li>
<li><strong class="bold">Architecture and interior design</strong>: Visualizing architectural designs and interior spaces is crucial for architects and interior designers, as well as their clients. Bedrock image models can generate realistic renderings of proposed designs, allowing stakeholders to immerse themselves in the envisioned spaces before construction or renovation. Bedrock’s capabilities can aid in visualizing different materials, furniture arrangements, and lighting conditions, enabling architects and designers to refine their concepts and present compelling proposals to clients or decision-makers.</li>
<li><strong class="bold">Fashion and apparel</strong>: In the fashion and apparel industry, Amazon Bedrock image models can generate unique textile designs, patterns, and clothing styles, enabling fashion designers to explore new concepts and stay ahead of trends. Additionally, Bedrock can create visualizations of clothing items on different body types or in various environments, allowing customers to preview how garments would look in real life before making a purchase. This capability can enhance the shopping experience and reduce return rates.</li>
<li><strong class="bold">Scientific visualization</strong>: Effective communication of scientific data, phenomena, and simulations is crucial in research and education. Amazon Bedrock’s image generation models can assist scientists and researchers in creating visual representations of complex concepts, making them more accessible and understandable. Bedrock models can generate illustrations, diagrams, or 3D models for scientific publications, presentations, or educational materials, facilitating knowledge transfer and fostering a deeper understanding of intricate topics.</li>
<li><strong class="bold">Art and creative expression</strong>: Artists can leverage Bedrock image models to explore new styles, techniques, and concepts by generating unique and imaginative images based on textual prompts or conceptual frameworks.</li>
<li><strong class="bold">E-commerce and product catalogs</strong>: In the e-commerce landscape, high-quality product images are essential for attracting customers and driving sales. Amazon Bedrock image models can generate visually appealing and accurate product images for online catalogs or e-commerce platforms, reducing the need for extensive photoshoots and the associated costs. These models can also create visualizations of customized products or configurations based on customer preferences, enhancing the shopping experience and enabling personalization <a id="_idIndexMarker729"/>at scale.</li>
</ul>
<p>Now that we have looked at some real-world applications, let us explore various multimodal models and their inner workings.</p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor180"/>Multimodal models</h1>
<p>So far in this book, we have looked at <a id="_idIndexMarker730"/>single-modal model architecture patterns, such as text-to-text generation, that includes QA, summarization, code generation, and so on. Let us now expand our understanding to another type of generative AI model: multimodal models.</p>
<p>Multimodal models are a type of model that can understand and interpret more than one modality, such as image, audio, and video, as shown in <em class="italic">Figure 9</em><em class="italic">.1</em>.</p>
<div><div><img alt="Figure 9.1 – Multimodality" src="img/B22045_09_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Multimodality</p>
<p>The response<a id="_idIndexMarker731"/> received from these models can also be multimodal. Behind the scenes, these FMs comprise multiple single-modal neural networks that process text, image, audio, and video separately.</p>
<p>Now let us look at the multimodality models that are available within Amazon Bedrock.</p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor181"/>Stable Diffusion</h2>
<p><strong class="bold">Stable Diffusion</strong> is a<a id="_idIndexMarker732"/> state-of-the-art image generation model <a id="_idIndexMarker733"/>that has gained significant attention in the field of generative AI. Unlike many other image generation models, Stable Diffusion employs a unique diffusion-based approach, which sets it apart from other methods or techniques.</p>
<p>At the heart of Stable Diffusion is the concept<a id="_idIndexMarker734"/> of <strong class="bold">diffusion</strong>, which involves a forward and a reverse diffusion process. In <strong class="bold">forward diffusion</strong>, Gaussian noise is progressively added to an image until it <a id="_idIndexMarker735"/>becomes entirely random. The model then learns to reverse this process, gradually removing the noise to reconstruct the original image. This reversal is <a id="_idIndexMarker736"/>called <strong class="bold">reverse diffusion</strong> and is the key to Stable Diffusion’s impressive performance.</p>
<p>This diffusion process has several key components:</p>
<ul>
<li><strong class="bold">Contrastive Language-Image Pre-Training</strong> (<strong class="bold">CLIP</strong>): CLIP<a id="_idIndexMarker737"/> is a neural network trained on a vast dataset of image-text pairs, enabling it to understand the semantic relationships between visual and textual representations. This component plays a crucial role in bridging the gap between natural language prompts and their corresponding visual manifestations.</li>
<li><strong class="bold">U-Net</strong>: This serves as a backbone for the image generation process. U-Net<a id="_idIndexMarker738"/> is a convolutional neural network designed for image-to-image translation tasks such as segmentation and denoising. Segmentation is the process whereby the images are partitioned into multiple segments or sets of pixels to locate objects and boundaries. Denoising removes the noise from an image to improve its quality. In the context of Stable Diffusion, U-Net is responsible for generating and refining the output image based on the input prompt and guidance from CLIP.</li>
<li><strong class="bold">VAE</strong>: This<a id="_idIndexMarker739"/> is another critical component that helps ensure that the generated images are coherent and realistic. In Stable Diffusion, VAE encodes the generated image into a compressed representation, which is then decoded to produce the final output image.</li>
</ul>
<p>As shown in <em class="italic">Figure 9</em><em class="italic">.2</em>, here is a<a id="_idIndexMarker740"/> high-level overview of how the whole <a id="_idIndexMarker741"/>process works:</p>
<ol>
<li>The user provides a natural language prompt describing the desired image.</li>
<li>The CLIP model analyzes the prompt and generates a corresponding embedding, representing the semantic meaning of the text.</li>
<li>The U-Net architecture takes this embedding as input, along with an initial random noise image.</li>
<li>Through a series of convolutional and deconvolutional operations, U-Net iteratively refines the noise image, guided by the CLIP embedding, to produce an image that matches the input prompt.</li>
<li>The generated image is then passed through the VAE, which encodes and decodes it, ensuring coherence and realism.</li>
<li>The final output image is produced, reflecting the user’s prompt.</li>
</ol>
<div><div><img alt="Figure 9.2 –  The Stable Diffusion process" src="img/B22045_09_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 –  The Stable Diffusion process</p>
<p>By combining these<a id="_idIndexMarker742"/> architectural elements, Stable Diffusion is able to <a id="_idIndexMarker743"/>generate high-quality, diverse images that are both visually appealing and semantically coherent with the input prompts. In order to understand the detailed workings of the diffusion process, readers are encouraged to read the research paper <em class="italic">On the Design Fundamentals</em> <em class="italic">of Diffusion Models: A Survey</em>. It can be found on: <a href="https://arxiv.org/pdf/2306.04542.pdf">https://arxiv.org/pdf/2306.04542.pdf</a>.</p>
<p>This paper explains how diffusion models work by gradually adding noise to training data and then learning to reverse that process to generate new samples. The paper highlights the wide range of applications for diffusion models, including image editing, text-to-image generation, and 3D object creation.</p>
<p>Additionally, readers are recommended to explore the <em class="italic">How Diffusion Models Work</em> course from DeepLearning.AI at <a href="https://learn.deeplearning.ai/courses/diffusion-models/">https://learn.deeplearning.ai/courses/diffusion-models/</a>.</p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor182"/>Titan Image Generator G1</h2>
<p><strong class="bold">Titan Image Generator G1</strong> is a <a id="_idIndexMarker744"/>proprietary image generation model by Amazon that <a id="_idIndexMarker745"/>allows users to generate images from text, edit existing images, and create variations of images. The model is designed to make it easy for users to iterate on image concepts by generating multiple image options based on text descriptions. The model is trained on diverse high-quality datasets, so it can understand complex prompts with multiple objects and generate realistic images.</p>
<p>This model supports image editing capabilities such as editing with text prompts using a built-in segmentation model, generating variations of the image, inpainting with an image mask, and outpainting to extend or change the background of an image. You can upload an existing image and provide instructions or prompts to modify specific aspects of the image. The model can intelligently alter the composition, add or remove elements, change colors, or apply <a id="_idIndexMarker746"/>various artistic styles, all while preserving the overall coherence and<a id="_idIndexMarker747"/> realism of the original image.</p>
<p>We will dive deeper into these capabilities in the <em class="italic">Multimodal design </em><em class="italic">patterns</em> section.</p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor183"/>Titan Multimodal Embeddings</h2>
<p>The <strong class="bold">Titan Multimodal Embeddings</strong> model<a id="_idIndexMarker748"/> is<a id="_idIndexMarker749"/> part of the Amazon Titan family of models designed for use cases such as image search and similarity-based recommendation with high accuracy and fast response.</p>
<p>The Titan Multimodal Embeddings model’s core strength lies in its ability to generate high-dimensional vector representations for both textual and visual data. These embeddings encapsulate the semantic relationships between different modalities, allowing for efficient and effective search and retrieval operations.</p>
<p>The model supports up to 128 tokens as input text in English, as well as image sizes of up to 25 MB, and converts those to vector embeddings. The default embedding size is 1024 dimensions, providing a rich representation that captures nuanced details and complex relationships. However, you can also configure smaller vector dimensions to optimize for speed and cost, depending on your specific use case and performance requirements.</p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor184"/>Anthropic Claude 3 – Sonnet, Haiku, and Opus</h2>
<p>Anthropic<a id="_idIndexMarker750"/> Claude 3 <a id="_idIndexMarker751"/>Model variants – <em class="italic">Claude 3 Sonnet</em>, <em class="italic">Claude 3 Haiku</em>, and <em class="italic">Claude 3 Opus</em> – are <a id="_idIndexMarker752"/>the most recent and advanced family of<a id="_idIndexMarker753"/> Anthropic Claude <a id="_idIndexMarker754"/>models available on Amazon Bedrock. All these models have multimodal capabilities, meaning that they are able to perceive and analyze images as well as text input, with a 200K context window. We encourage you to refer to the <em class="italic">Anthropic Claude</em> section in <a href="B22045_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a> if you would like to go over their details again.</p>
<p>Now that we have looked at the multimodal models available within Amazon Bedrock, let us explore some of the design patterns.</p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor185"/>Multimodal design patterns</h1>
<p>With multimodal design patterns, we <a id="_idIndexMarker755"/>integrate different modalities, such as text, images, audio, and so on. With the multimodal models available, the ability to generate, manipulate, and understand images from text or other input modalities has become increasingly important in a wide range of applications, from creative design to scientific visualization and beyond.</p>
<p>Numerous patterns can be created with multimodal models. In this section, we are going to cover some of the common patterns.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor186"/>Text-to-image</h2>
<p>With a <a id="_idIndexMarker756"/>text-to-image pattern, you provide the text as a prompt to<a id="_idIndexMarker757"/> the model. The model will then generate an image based on that prompt, as shown in <em class="italic">Figure 9</em><em class="italic">.3.</em></p>
<div><div><img alt="Figure 9.3 – A text-to-image pattern" src="img/B22045_09_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – A text-to-image pattern</p>
<p><strong class="bold">Parameters</strong></p>
<p>At the core of image generation models are a set of customizable inference parameters and controls that allow users to get the desired image from the model. Let us look at these parameters:</p>
<ul>
<li><code>Cloud</code> and <code>seating bench</code> to exclude them from the image, as shown in <em class="italic">Figure 9</em><em class="italic">.4</em>.</li>
</ul>
<div><div><img alt="Figure 9.4 – A text-to-image pattern with negative prompts" src="img/B22045_09_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – A text-to-image pattern with negative prompts</p>
<ul>
<li><strong class="bold">Reference image</strong>: This<a id="_idIndexMarker760"/> provides users the ability to input a reference image to the model, which can be leveraged by the model as a baseline to generate its response (generated image). For instance, if we use the image generated from the preceding figure and pass it as a reference along with the prompt, the prompt would be something like this:</li>
</ul>
<p><code>A futuristic cityscape at night, with towering skyscrapers made of glass and metal. The buildings are illuminated by neon lights in shades of blue, purple, and pink. The streets are lined with holographic billboards </code><code>and advertisements.</code></p>
<p class="list-inset">The model will use the reference image and the prompt to generate a new image, as shown in <em class="italic">Figure 9</em><em class="italic">.5</em>.</p>
<div><div><img alt="Figure 9.5 – A text-to-image pattern using a reference Image" src="img/B22045_09_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – A text-to-image pattern using a reference Image</p>
<ul>
<li><strong class="bold">Prompt Strength</strong><strong class="bold"> (</strong><strong class="bold">cfg_scale</strong><strong class="bold">)</strong>: Prompt<a id="_idIndexMarker761"/> strength, also known as <strong class="bold">Classifier-Free Guidance scale</strong> (<em class="italic">cfg_scale</em>) determines <a id="_idIndexMarker762"/>the degree to which the generated image adheres to the provided text prompt. A higher value indicates that the image generation process will adhere more closely to the text prompt, while a lower value allows for more creative interpretation and diversity in the generated images. Using a cfg_scale value somewhere in the middle (<em class="italic">10-15</em>) is generally recommended, as it strikes a balance between faithfully representing the text prompt and allowing for artistic expression. However, the optimal value may vary <a id="_idIndexMarker763"/>depending on your use case or what you are looking for, complexity of the prompt, and the desired level of detail in the generated image.</li>
<li><strong class="bold">Generation Step</strong><strong class="bold"> (</strong><strong class="bold">steps</strong><strong class="bold">)</strong>: The <em class="italic">steps</em> parameter <a id="_idIndexMarker764"/>in Stable Diffusion refers to the number of iterations or cycles the algorithm goes through to generate an image from the input text. It’s an important setting that affects the quality and detail of the final image. Here is how it works:<ul><li>The process starts with random noise, and with each step, some of that noise is removed, gradually revealing the intended image.</li><li>More steps generally lead to higher-quality images with more detail, but there’s a point of diminishing returns.</li><li>The ideal number of steps can vary depending on the complexity of the image you’re trying to generate and your personal preferences. However, going much higher may not significantly improve the image but will increase generation time.</li><li>For simple subjects or scenes, fewer steps (around <em class="italic">10-15</em>) may be sufficient. But for more <a id="_idIndexMarker765"/>complex or detailed images, you may want to increase the steps to <em class="italic">40-50</em> or <a id="_idIndexMarker766"/>even more, depending on how much detailed you are looking for.</li></ul></li>
</ul>
<p>What we discussed are just some of the parameters. The following figure highlights additional <a id="_idIndexMarker767"/>parameters for Stable<a id="_idIndexMarker768"/> Diffusion.</p>
<div><div><img alt="Figure 9.6 – Stable Diffusion text-to-image parameters" src="img/B22045_09_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Stable Diffusion text-to-image parameters</p>
<p>For a <a id="_idIndexMarker769"/>more detailed description <a id="_idIndexMarker770"/>of these parameters, you can go through the Stable Diffusion documentation at <a href="https://platform.stability.ai/docs/api-reference#tag/Image-to-Image">https://platform.stability.ai/docs/api-reference#tag/Image-to-Image</a>.</p>
<p>If you are <a id="_idIndexMarker771"/>using Amazon Titan Image Generator, here is the list of parameters that you can use: <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html">https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html</a>.</p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor187"/>Image search</h2>
<p>Image search has emerged <a id="_idIndexMarker772"/>as a powerful tool that enables users to explore and leverage vast collections of visual data. With FMs from Amazon Bedrock, you can perform image searches to understand and interpret visual content. You can identify and understand various elements within an image, such as objects, scenes, colors, textures, and even abstract concepts. To illustrate the power of image search, let’s consider a practical example.</p>
<p>Imagine you’re a fashion retailer with an extensive catalog of clothing items. With Bedrock, you can upload your product images and leverage the image search capabilities to enable customers to find visually similar items. For instance, a customer could upload a picture of a dress that they like and Bedrock would return a set of visually similar dresses from your catalog, facilitating a more engaging and personalized shopping experience.</p>
<p>One powerful approach to image search is based on multimodal embeddings, which allow for the representation of both text and images in a vector space. These vectors capture the visual features and semantic information of the images. The vectors, along with metadata such as image paths, are then stored in a searchable index vector database such as OpenSearch Serverless, FAISS, or Pinecone. This technique enables searching for images using text queries or finding similar images based on a given image (or a combination of text and image).</p>
<p>When a user initiates a search, their input (text, image, or both) is also converted into a vector representation using the same multimodal embedding model. The search vector is then compared against the vectors in the index and the most similar vectors are retrieved based on the vector similarity scores. This approach allows for flexible and intuitive image search, as users can search using natural language descriptions, upload example images, or combine text and images for more precise results. For example, you could search for <code>a red sports car on a city street</code> and the model would return relevant images from its data store that match both the visual and textual criteria.</p>
<p>As you might have recognized by now, this process is similar to the RAG process that we discussed in <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>. The difference here is that the model is retrieving the images from its data store and is not generating new images. Here is a great example to try out multimodal embedding and searching: <a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb</a>.</p>
<p>Image search <a id="_idIndexMarker773"/>with multimodal embeddings has numerous real-world applications across various domains. In e-commerce platforms, it can be used to enhance product search and recommendation systems, allowing customers to find visually similar products or to search for items using natural language descriptions or example images. In the media and entertainment industry, it can assist in content organization, tag suggestion, and copyright infringement detection by identifying similar or duplicate images.</p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor188"/>Image understanding</h2>
<p>The Anthropic <a id="_idIndexMarker774"/>Claude 3 models – Sonnet, Haiku, and Opus – introduce the image understanding capability, through which the model can analyze the image and provide you with a response based on what you are looking to know. For example, you can provide an image of a kitchen or a living room and ask the model to provide a detailed description of the image or write a fictional story based on the image.</p>
<h3>Example 1</h3>
<p>Use the<a id="_idIndexMarker775"/> following prompt: <code>Provide a detailed description of </code><code>this image</code>.</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 9.7 – Image understanding and a detailed description in the output" src="img/B22045_09_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Image understanding and a detailed description in the output</p>
<p>In <em class="italic">Figure 9</em><em class="italic">.7</em>, we <a id="_idIndexMarker776"/>have provided the image of a kitchen to the Anthropic Claude 3 model and asked it to provide a detailed description of the image. The model is able to provide minute details such as <strong class="bold">the room features dark wood cabinetry, contrasted by light marble countertops</strong>, and so on.</p>
<h3>Example 2</h3>
<p>Use the following prompt: <code>Write a fictional story based on the </code><code>image attached</code>.</p>
<div><div><img alt="Figure 9.8 – Image understanding with a fictional story" src="img/B22045_09_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Image understanding with a fictional story</p>
<p>In <em class="italic">Figure 9</em><em class="italic">.8</em>, you<a id="_idIndexMarker777"/> can see that the model has generated a fictional story based on the image of a library provided to it.</p>
<h3>Example 3</h3>
<p>Use the following prompt: <code>Provide the list of items/objects present in the image and explain </code><code>each item</code>.</p>
<div><div><img alt="Figure 9.9 – Image understanding with object identification" src="img/B22045_09_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – Image understanding with object identification</p>
<p>In <em class="italic">Figure 9</em><em class="italic">.9</em>, the model is able to identify the items and objects in the image along with their details, showcasing the image classification/object recognition capability.</p>
<p>The Claude <a id="_idIndexMarker778"/>models’ image understanding capabilities are not limited to those discussed in the preceding examples. They can also be utilized for tasks such as image captioning, creating detailed image descriptions, identifying subjects, and answering questions about the contents of an image. You can look at various use cases of image understanding<a id="_idIndexMarker779"/> at <a href="https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities">https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities</a>.</p>
<p>To use this capability within the Amazon Bedrock console, yo<a href="https://console.aws.amazon.com/bedrock-">u can follow the ensuing steps:</a></p>
<ol>
<li><a href="https://console.aws.amazon.com/bedrock-">Go t</a>o Amazon Bedrock Console at https://console.aws.amazon.com/bedrock.</li>
<li>Navigate to <strong class="bold">Chat Playground</strong>.</li>
<li>Click on <strong class="bold">Select model</strong>. Choose the <strong class="bold">Anthropic Claude 3 Sonnet</strong>, <strong class="bold">Anthropic Claude 3 Haiku</strong>, or <strong class="bold">Anthropic Claude 3 </strong><strong class="bold">Opus</strong> model.</li>
<li>Attach the image you want to analyze and provide the prompt based on what you are looking for, as shown in <em class="italic">Figure 9</em><em class="italic">.10</em>.</li>
</ol>
<div><div><img alt="Figure 9.10 – How to analyze an image using the Anthropic Claude 3 models" src="img/B22045_09_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – How to analyze an image using the Anthropic Claude 3 models</p>
<p>If you are using <a id="_idIndexMarker780"/>AWS SDK, you can use Anthropic’s <code>Messages</code> API to create a chat application and provide an image for understanding. Here is some example AWS Python SDK code for a multimodal message to the Claude 3 Sonnet model: <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example">https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example</a>.</p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor189"/>Image-to-image patterns</h2>
<p>When it comes<a id="_idIndexMarker781"/> to image-to-image generation, the model takes an <a id="_idIndexMarker782"/>existing image as input and modifies it based on the prompt or instructions you provide. This is different from text-to-image generation, whereby the model creates an entirely new image from scratch based solely on a textual description or prompt. In image-to-image generation, on the other hand, the model uses the existing image as a starting point and then applies the necessary changes or transformations to produce the desired output image. This can involve adjusting various aspects such as colors, textures, objects, or even the overall composition of the image, all guided by the prompt. It’s like having a clay model and reshaping it to your desired outcome rather than starting from a lump of raw clay. The ability to modify and manipulate existing images opens up a range of creative possibilities and use cases, from enhancing and editing photographs to creating artistic interpretations or visualizations.</p>
<p>A simple example of image-to-image generation is shown in <em class="italic">Figure 9</em><em class="italic">.11</em>.</p>
<div><div><img alt="Figure 9.11 – Simple image-to-image generation" src="img/B22045_09_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – Simple image-to-image generation</p>
<p>When using the<a id="_idIndexMarker783"/> Stable Diffusion model for image-to-image generation, there<a id="_idIndexMarker784"/> are a few additional parameters to consider along with the text-to-text parameters mentioned in <em class="italic">Figure 9</em><em class="italic">.6</em>. These<a id="_idIndexMarker785"/> additional <a id="_idIndexMarker786"/>parameters are highlighted in <em class="italic">Figure 9</em><em class="italic">.12.</em></p>
<div><div><img alt="Figure 9.12 – Stable Diffusion image-to-image parameters" src="img/B22045_09_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 – Stable Diffusion image-to-image parameters</p>
<p>You can <a id="_idIndexMarker787"/>learn more about them here: <a href="https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage">https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage</a>.</p>
<p>Next, let us look at some image-to-image patterns.</p>
<h3>Image variation</h3>
<p><strong class="bold">Image variation</strong>, also <a id="_idIndexMarker788"/>known as <strong class="bold">image-to-image translation</strong> or <strong class="bold">style transfer</strong>, is a <a id="_idIndexMarker789"/>powerful technique in generative AI<a id="_idIndexMarker790"/> that allows for the creation of new and unique images by modifying existing ones. This process involves taking an input image and applying a desired style or transformation to it, resulting in an output image that combines the content of the original with the desired aesthetic or visual characteristics.</p>
<p>One real-world example of image variation is in the field of fashion design. Designers can take an existing garment or accessory and apply various styles, patterns, or textures to create new and innovative designs without starting from scratch. This not only saves time and resources but also allows for rapid experimentation and iteration, enabling designers to explore a vast range of possibilities.</p>
<p>Another example can be found in the art world, where image variation techniques can be used to create unique and expressive artworks. Artists can take a simple photograph or painting and apply various artistic styles, such as impressionism, cubism, or abstract expressionism, to create entirely new pieces that blend the original content with the desired artistic style. This opens up new avenues for creative expression and allows artists to explore unconventional and thought-provoking visual interpretations.</p>
<p>Image variation also has applications in the fields of interior design and architectural visualization. Designers and architects can take existing spaces or structures and apply different materials, textures, or lighting conditions to visualize how a space might look with different design choices. This can be invaluable in helping clients understand and appreciate proposed designs, as well as in enabling designers to quickly iterate and refine their concepts.</p>
<p>With Bedrock, you <a id="_idIndexMarker791"/>can utilize Titan Image Generator to create image variations. Let us try the following prompt and run it through Titan Image Generator:</p>
<p><code>A delicate, nature-inspired pattern with intricate illustrations of birds, butterflies, and foliage, perfect for a romantic, bohemian dress </code><code>or scarf.</code></p>
<div><div><img alt="Figure 9.13 – Image variation" src="img/B22045_09_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 – Image variation</p>
<p>As shown in <em class="italic">Figure 9</em><em class="italic">.13</em>, Titan Image Generator<a id="_idIndexMarker792"/> will create an image (<strong class="bold">Original Image</strong>). You can generate image variations that leverage the <strong class="bold">Original Image</strong> as a reference, along with an optional prompt that can be provided to the model.</p>
<h3>Masking</h3>
<p>Amazon <a id="_idIndexMarker793"/>Bedrock models – Amazon Titan Generator and Stable Diffusion – offer two powerful image editing techniques: <em class="italic">masking</em> and <em class="italic">painting</em>.</p>
<p>With masking, we define specific areas within an image and mask them, either to be preserved or redrawn. This masking can be done either via an image file or a prompt.</p>
<h4>Image masking</h4>
<p>The approach of <strong class="bold">image masking</strong> utilizes a <a id="_idIndexMarker794"/>separate image file, known as the <strong class="bold">mask image</strong>, to <a id="_idIndexMarker795"/>specify the pixels to be masked or preserved in the original image. The mask image must adhere to the following requirements:</p>
<ul>
<li><strong class="bold">Identical dimensions and resolution as the original image</strong>: When using image masking, it’s crucial that the mask image has the exact same dimensions and resolution as the original image that you want to mask. This ensures that each pixel in the mask image corresponds to a pixel in the original image, allowing for precise masking. If the dimensions or resolutions differ, the masking process may produce distorted or undesired results.<p class="list-inset">For example, if your original image has a resolution of 1920 x 1080 pixels, the mask image must also have a resolution of 1920 x 1080 pixels. Any discrepancy in the dimensions or resolution will cause the mask to misalign with the original image, leading to undesirable masking effects.</p></li>
<li><strong class="bold">No alpha channel</strong>: The mask image should not have an alpha channel, which is a separate component in some image formats that represents transparency. While the PNG format supports transparency through an alpha channel, for image masking purposes, the mask image should rely solely on color values (<strong class="bold">Red, Green, Blue</strong> (<strong class="bold">RGB</strong>) or<a id="_idIndexMarker796"/> grayscale) to represent masked and unmasked regions.<p class="list-inset">The absence of an<a id="_idIndexMarker797"/> alpha channel simplifies the masking process and ensures that the masking is based solely on the pixel colors, without any additional transparency information. This approach is often preferred for its simplicity and compatibility with a wide range of image processing tools and libraries.</p></li>
<li><code>0</code>, <code>0</code>, <code>0</code>) as masked areas, while any non-black pixels are considered unmasked regions.</li><li><code>0</code> (black) to <code>255</code> (white). The masking process interprets pixels with a value of <code>0</code> as masked areas, while pixels with non-zero values are considered unmasked regions.</li></ul></li>
</ul>
<p>The choice between RGB and grayscale color modes depends on your specific use case and on the tools or libraries you’re using for image masking. Some tools may have a preference for one color mode over the other.</p>
<p>For example, let’s assume that you work in the Food and Beverages industry and you want to mask out certain food items from an image to create a transparent layer for a menu design. Suppose that you want to mask the bowl of chips in the image that follows and maybe remove it from your menu. <em class="italic">Figure 9</em><em class="italic">.14</em> shows the original image and the masked image, where the masking is done on the bowl of chips.</p>
<div><div><img alt="Figure 9.14 – Image masking" src="img/B22045_09_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.14 – Image masking</p>
<p>If you want to <a id="_idIndexMarker800"/>experiment with image masking, you can use online photo editing tools or apps. There is also the <strong class="bold">Python Image Library</strong> (<strong class="bold">PIL</strong>), a very popular Python library<a id="_idIndexMarker801"/> that is worth checking out at: <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html">https://pillow.readthedocs.io/en/stable/reference/Image.html</a>.</p>
<p>In addition, we recommend that you experiment with the following GitHub examples from the Amazon Bedrock workshop that showcase image masking and painting: <a href="https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal">https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal</a>.</p>
<h4>Mask prompting</h4>
<p><strong class="bold">Mask prompting</strong> involves<a id="_idIndexMarker802"/> masking images through the use of textual prompts. These textual prompts serve as a guide to the model to comprehend the desired masking area within an image.</p>
<p>The benefit of using mask prompting as opposed to image masking lies in the dynamic nature of mask prompting. You can effortlessly modify the masking by simply altering the textual prompt, allowing for rapid iteration and experimentation. This flexibility allows artists, designers, and content creators to explore a vast array of visual concepts and narratives without being constrained by traditional image editing tools.</p>
<p>In addition, mask prompting can be seamlessly integrated into various workflows and applications, enabling seamless collaboration and enhancing productivity. For instance, in the field of visual storytelling, writers and directors can leverage this feature to conceptualize and refine their vision, while designers can quickly prototype and iterate on visual concepts before committing to a final design.</p>
<p>To ensure the integrity and originality of the content generated, Amazon Bedrock has implemented robust measures to safeguard against plagiarism and unethical practices. We will discuss ethical considerations and safeguards in the upcoming section.</p>
<p>Let’s take the same <a id="_idIndexMarker803"/>example from the preceding figure. Instead of image masking, we want to apply a mask prompt. We’ll say that you want to remove the bowl of chips from the original image. With mask prompting, you can provide the <code>only bowl of chips</code> prompt and then further perform painting.</p>
<div><div><img alt="Figure 9.15 – Mask prompting" src="img/B22045_09_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.15 – Mask prompting</p>
<p>In <em class="italic">Figure 9</em><em class="italic">.15</em>, we performed inpainting and changed the bowl of chips to a bowl of apple slices. Let us discuss painting in more detail.</p>
<h3>Painting</h3>
<p><strong class="bold">Painting</strong> is a<a id="_idIndexMarker804"/> technique whereby you can fill in the masked regions <a id="_idIndexMarker805"/>within an image or extend it using an image generation model. There are two methods of painting: inpainting and outpainting.</p>
<h4>Inpainting</h4>
<p>With <strong class="bold">inpainting</strong>, you are<a id="_idIndexMarker806"/> essentially reconstructing or filling in missing, masked, or <a id="_idIndexMarker807"/>corrupted portions of the image. This technique is particularly useful in scenarios where an image has been damaged or obscured, or where it contains unwanted elements that need to be removed or replaced. By providing the image generation model with the surrounding context and effective prompts, it can intelligently generate and blend new content in the designated areas, seamlessly <a id="_idIndexMarker808"/>reconstructing the masked regions.</p>
<p>Let us look at some <a id="_idIndexMarker809"/>examples:</p>
<ul>
<li><code>telephone line</code> and provide an empty prompt text (<code>""</code>).</li>
</ul>
<div><div><img alt="Figure 9.16 – Inpainting removal" src="img/B22045_09_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.16 – Inpainting removal</p>
<ul>
<li><strong class="bold">Inpainting replacement</strong>: Suppose that you want to replace any object or scene within the image. In that case, you can perform inpainting replacement. As shown in <em class="italic">Figure 9</em><em class="italic">.17</em>, you can<a id="_idIndexMarker810"/> specify within the prompt text what you want to replace.</li>
</ul>
<div><div><img alt="Figure 9.17 – Inpainting replacement" src="img/B22045_09_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.17 – Inpainting replacement</p>
<h4>Outpainting</h4>
<p><strong class="bold">Outpainting</strong> is the <a id="_idIndexMarker811"/>process of extending the image beyond its original boundaries, or<a id="_idIndexMarker812"/> in other words painting outside the masked regions. Outpainting is useful in scenarios where the original image or artwork needs to be extended or augmented with additional elements, environments, or perspectives.</p>
<p>Let us look at an example.</p>
<div><div><img alt="Figure 9.18 – Outpainting" src="img/B22045_09_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.18 – Outpainting</p>
<p>In <em class="italic">Figure 9</em><em class="italic">.18</em>, you can see that we are painting outside the masked image or prompt (in this case, <code>Indian curry</code>) to add some details. These include the background of a wooden table, as well as adding a spoon, a knife, a side of rice, and a chai.</p>
<p>If you want <a id="_idIndexMarker813"/>to understand the prompt engineering best practices for the Titan Image Generator model, please check out <a href="https://tinyurl.com/titan-image-generator-prompt">https://tinyurl.com/titan-image-generator-prompt</a>.</p>
<p>Now that we have looked at different patterns of multimodal and image patterns, let us look at the ethical considerations and available safeguards within Amazon Bedrock to ensure the responsible use of Generative AI.</p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor190"/>Ethical considerations and safeguards</h1>
<p>Generative <a id="_idIndexMarker814"/>AI models, particularly those capable of generating highly realistic images, raise significant ethical concerns regarding the potential spread of misinformation and deepfakes. As these models are becoming increasingly powerful and accessible, it is crucial to address these ethical challenges proactively to promote the responsible development and deployment of this technology.</p>
<p>One of the primary ethical concerns surrounding image generation models is the risk of creating and disseminating misleading or manipulated content. With the ability to generate photorealistic images from text prompts comes the potential for malicious actors to create and spread false or fabricated visual information. This can have far-reaching consequences, such as undermining trust in media, spreading disinformation, and even influencing political or social narratives.</p>
<p>To address this major ethical challenge, it is crucial for organizations and researchers to prioritize responsible development and deployment of a generative AI life cycle. When using Amazon Bedrock, users can utilize its <strong class="bold">watermark detection</strong> feature<a id="_idIndexMarker815"/> for images generated by Amazon Titan Image Generator.</p>
<p>The watermark detection capability in Amazon Bedrock is designed to promote transparency and accountability in the use of AI-generated images. By embedding an invisible watermark in every image created by the model, content creators, news organizations, risk analysts, and others can quickly verify whether an image has been generated using Amazon Titan Image Generator.</p>
<p>This approach serves two primary ethical purposes:</p>
<ul>
<li>It helps combat the spread of misinformation and deepfakes by providing a mechanism to verify the authenticity of images. This can help build trust and credibility in visual content, particularly in domains where the integrity of information is critical, such as journalism, law enforcement, and scientific research.</li>
<li>The watermark detection feature promotes transparency and accountability in the use of image generation models. By making it easier to identify AI-generated content, it encourages responsible and ethical practices among content creators and stakeholders, fostering a more open dialogue around the use of this technology.</li>
</ul>
<p>To try out<a id="_idIndexMarker816"/> watermark detection, you can simply <a id="_idIndexMarker817"/>navigate to <strong class="bold">Watermark detection</strong> in the Amazon Bedrock console and upload an image. Amazon Bedrock then analyzes the image to detect watermarks embedded by Amazon Titan.</p>
<p>In addition to detecting the watermark, you will also receive a confidence score, which determines the level of confidence (or certainty) with which the model is able to identify that the image was generated by Amazon Titan. Usually, you will see a high confidence score when there has been little to no modification in the image. However, if you make some modifications to the generated image, you might see a lower confidence score.</p>
<div><div><img alt="Figure 9.19 – Watermark detection" src="img/B22045_09_19.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.19 – Watermark detection</p>
<p>As shown<a id="_idIndexMarker818"/> in <em class="italic">Figure 9</em><em class="italic">.19</em>, we have<a id="_idIndexMarker819"/> uploaded the image generated by Amazon Titan. The watermark detection feature is able to analyze and detect the watermark generated by Titan.</p>
<div><div><img alt="Figure 9.20 – The watermark is not detected" src="img/B22045_09_20.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.20 – The watermark is not detected</p>
<p>In <em class="italic">Figure 9</em><em class="italic">.20</em>, we have uploaded the image generated by Stable Diffusion. We can see that the watermark is not detected.</p>
<p>If you want to<a id="_idIndexMarker820"/> try using API, you can call <code>DetectGeneratedContent</code> to verify whether the watermark exists:</p>
<pre class="source-code">
import boto3
import json
import base64
bedrock_runtime = boto3.client(service_name="bedrock-runtime")
image_path = "landscape.png"
with open(image_path, "rb") as image_file:
    input_landscape = image_file.read()
response = bedrock_runtime.detect_generated_content(
    foundationModelId = "amazon.titan-image-generator-v1",
    content = {
        "imageContent": { "bytes": input_landscape }
    }
)</pre>
<p>Here is how<a id="_idIndexMarker821"/> the response should look:</p>
<pre class="console">
response.get("detectionResult")
'GENERATED'
response.get("confidenceLevel")
'HIGH'</pre>
<p>Here is the demo on watermark detection: <a href="https://www.youtube.com/watch?v=M5Vqb3UoXtc">https://www.youtube.com/watch?v=M5Vqb3UoXtc</a>.</p>
<p>While watermark detection is not a solution for all ethical concerns, it is one of the ways to move in the right direction. We will have a deeper discussion on ethical and responsible AI in <a href="B22045_11.xhtml#_idTextAnchor207"><em class="italic">Chapter 11</em></a> of this book. You should now be able to understand image generation and design patterns with Amazon Bedrock.</p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor191"/>Summary</h1>
<p>In this chapter, we explored how image generation works. We also discussed the workings of multimodal models within Amazon Bedrock. We also covered several real-world applications and multimodal design patterns, including text-to-image, image search, image understanding, and image-to-image patterns such as inpainting and outpainting. We ended the chapter with a brief look at ethical considerations, as well as a look into the watermark detection capability within Amazon Bedrock. Throughout the chapter, we gained a deeper understanding of how we can leverage Amazon Bedrock’s multimodal models to build applications that can generate, understand, and manipulate images based on text and image prompts.</p>
<p>In the next chapter, we will explore the topic of building intelligent agents with Amazon Bedrock.</p>
</div>
</body></html>