<html><head></head><body>
<div id="_idContainer147">
<h1 class="chapter-number" id="_idParaDest-161"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-162"><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.2.1">Generating and Transforming Images Using Amazon Bedrock</span></h1>
<p><span class="koboSpan" id="kobo.3.1">By now, we have explored several LLMs capable of generating textual responses. </span><span class="koboSpan" id="kobo.3.2">This chapter explores generating images using select FMs that are available on Amazon Bedrock. </span><span class="koboSpan" id="kobo.3.3">We will start with an overview of image generation, wherein we will examine model architectures such as</span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.4.1"> GANs and </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">variational autoencoders</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">VAEs</span></strong><span class="koboSpan" id="kobo.8.1">) Then, we will cover some real-world applications of image generation and multimodal models available within Amazon Bedrock. </span><span class="koboSpan" id="kobo.8.2">Furthermore, we will dive deeper into several multimodal design patterns, as well as some ethical considerations and safeguards that are available with </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">Amazon Bedrock.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">By the end of this chapter, you will have gained an understanding of implementing image generation and its design patterns with Amazon Bedrock for real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">use cases.</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">Here are the key topics that will be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">this chapter:</span></span><a id="_idTextAnchor173"/><a id="_idTextAnchor174"/><a id="_idTextAnchor175"/></p>
<ul>
<li><span class="koboSpan" id="kobo.14.1">Image </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">generation overview</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.16.1">Multimodal models</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Multimodal </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">design patterns</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Ethical considerations </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">and safeguards</span></span></li>
</ul>
<h1 id="_idParaDest-163"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.21.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.22.1">This chapter requires you to have access to an AWS account. </span><span class="koboSpan" id="kobo.22.2">If you don’t have one already, you can go to </span><a href="https://aws.amazon.com/getting-started/"><span class="koboSpan" id="kobo.23.1">https://aws.amazon.com/getting-started/</span></a><span class="koboSpan" id="kobo.24.1"> and </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">create one.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">Secondly, you will need to install and configure AWS CLI at </span><a href="https://aws.amazon.com/cli/"><span class="koboSpan" id="kobo.27.1">https://aws.amazon.com/cli/</span></a><span class="koboSpan" id="kobo.28.1"> after you create an account, which will be needed to access Amazon Bedrock FMs from your local machine. </span><span class="koboSpan" id="kobo.28.2">Since the majority of the code cells that we will be executing are based in Python, setting up an AWS Python SDK (Boto3) at </span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html"><span class="koboSpan" id="kobo.29.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</span></a><span class="koboSpan" id="kobo.30.1"> would be beneficial at this point. </span><span class="koboSpan" id="kobo.30.2">You can carry out the Python setup in any way. </span><span class="koboSpan" id="kobo.30.3">Install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or leverage </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">Amazon SageMaker.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.32.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.33.1">There will be a charge associated with the invocation and customization of the FMs of Amazon Bedrock. </span><span class="koboSpan" id="kobo.33.2">Please refer to </span><a href="https://aws.amazon.com/bedrock/pricing/"><span class="koboSpan" id="kobo.34.1">https://aws.amazon.com/bedrock/pricing/</span></a><span class="koboSpan" id="kobo.35.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">learn more.</span></span></p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.37.1">Image generation overview</span></h1>
<p><span class="koboSpan" id="kobo.38.1">Image generation</span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.39.1"> has been a fascinating and rapidly evolving field. </span><span class="koboSpan" id="kobo.39.2">Since the dawn of advanced deep learning techniques and increasing computational power, machines have gained the remarkable ability to create highly realistic and sophisticated images from scratch or based on textual prompts. </span><span class="koboSpan" id="kobo.39.3">This ability has opened up a vast array of applications across various domains, including the creative industries, media and entertainment, advertising, product packaging, and </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">many others.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">The history of image generation can be traced back to early developments in computer vision and pattern recognition. </span><span class="koboSpan" id="kobo.41.2">Researchers and scientists have long sought to understand and replicate the human visual perception system, paving the way for the initial techniques in image synthesis and manipulation. </span><span class="koboSpan" id="kobo.41.3">However, the true breakthrough in image generation came with the emergence of deep learning, specifically the introduction of</span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.42.1"> GANs </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">and </span></span><span class="No-Break"><a id="_idIndexMarker717"/></span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">VAEs.</span></span></p>
<p><span class="koboSpan" id="kobo.45.1">Please note that we are highlighting these techniques for historical reference. </span><span class="koboSpan" id="kobo.45.2">Current image generation FMs do not use </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">these techniques.</span></span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.47.1">What are GANs and VAEs?</span></h2>
<p><span class="koboSpan" id="kobo.48.1">GANs, introduced</span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.49.1"> by Ian Goodfellow and his colleagues in 2014, revolutionized the field of image generation. </span><span class="koboSpan" id="kobo.49.2">You can read more about it on </span><a href="https://arxiv.org/pdf/1406.2661"><span class="koboSpan" id="kobo.50.1">https://arxiv.org/pdf/1406.2661</span></a><span class="koboSpan" id="kobo.51.1">. </span><span class="koboSpan" id="kobo.51.2">GANs employ a unique training approach whereby two neural networks are pitted against each other in competition. </span><span class="koboSpan" id="kobo.51.3">The first network is known as</span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.52.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.53.1">generator</span></strong><span class="koboSpan" id="kobo.54.1">, which is tasked with generating synthetic samples that mimic real data. </span><span class="koboSpan" id="kobo.54.2">For example, the generator could produce new images, texts, or audio clips. </span><span class="koboSpan" id="kobo.54.3">The second network is called</span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.55.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">discriminator</span></strong><span class="koboSpan" id="kobo.57.1">. </span><span class="koboSpan" id="kobo.57.2">Its role is to analyze examples, both real and synthetic, to classify which ones are genuine and which have been </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">artificially generated.</span></span></p>
<p><span class="koboSpan" id="kobo.59.1">Through this </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.60.1">adversarial process, the generator learns to produce increasingly convincing fakes that can fool the discriminator. </span><span class="koboSpan" id="kobo.60.2">Meanwhile, the discriminator evolves in its ability to detect subtle anomalies that reveal synthetic samples. </span><span class="koboSpan" id="kobo.60.3">Their competing goals drive both networks to continuously improve. </span><span class="koboSpan" id="kobo.60.4">A demonstration of GANs can be seen at </span><a href="https://thispersondoesnotexist.com/"><span class="koboSpan" id="kobo.61.1">https://thispersondoesnotexist.com/</span></a><span class="koboSpan" id="kobo.62.1">. </span><span class="koboSpan" id="kobo.62.2">By refreshing the page endlessly, users are presented with an endless stream of novel human faces. </span><span class="koboSpan" id="kobo.62.3">However, none of those faces are real – all are synthetic portraits created solely by a GAN trained on vast databases of images of real human faces. </span><span class="koboSpan" id="kobo.62.4">The site offers a glimpse into how GANs can synthesize highly realistic outputs across </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">many domains.</span></span></p>
<p><span class="koboSpan" id="kobo.64.1">Since the inception of GANs, numerous advancements and variations have been implemented, leading to remarkable achievements in image generation. </span><span class="koboSpan" id="kobo.64.2">Techniques such as StyleGAN, BigGAN, and diffusion models have pushed the boundaries of image quality, resolution, and diversity. </span><span class="koboSpan" id="kobo.64.3">These models can generate photorealistic images of human faces, landscapes, objects, and even artistic creations, blurring the line between artificial </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">and real.</span></span></p>
<p><span class="koboSpan" id="kobo.66.1">VAEs, on the </span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.67.1">other hand, are a simpler means to train generative AI algorithms. </span><span class="koboSpan" id="kobo.67.2">They also utilize two neural </span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.68.1">networks: </span><strong class="bold"><span class="koboSpan" id="kobo.69.1">encoders</span></strong><span class="koboSpan" id="kobo.70.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">decoders</span></strong><span class="koboSpan" id="kobo.72.1">. </span><span class="koboSpan" id="kobo.72.2">Encoders </span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.73.1">learn the patterns in the data by mapping it into lower-dimensional latent space; decoders use these patterns from the latent space and generate </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">realistic samples.</span></span></p>
<p><span class="koboSpan" id="kobo.75.1">One of the most exciting developments in image generation has been the integration of NLP capabilities. </span><span class="koboSpan" id="kobo.75.2">Models such as DALL-E, Stable Diffusion, and Midjourney have empowered users to generate images simply by providing textual descriptions or prompts. </span><span class="koboSpan" id="kobo.75.3">This fusion of language and vision has opened up new avenues for creative expression, rapid prototyping, and data augmentation for various </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">ML tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.77.1">While the advancements in image generation are remarkable, it is crucial to address the ethical considerations and potential risks associated with this technology. </span><span class="koboSpan" id="kobo.77.2">Issues such as deepfakes, biases, and misuse for malicious purposes must be carefully addressed to ensure the responsible and ethical deployment of these powerful tools. </span><span class="koboSpan" id="kobo.77.3">We will look at this topic in detail in the </span><em class="italic"><span class="koboSpan" id="kobo.78.1">Ethical considerations and safeguards</span></em><span class="koboSpan" id="kobo.79.1"> section of </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.81.1">Let us look at some real-world applications for image </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">generation models.</span></span></p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.83.1">Real-world applications</span></h2>
<p><span class="koboSpan" id="kobo.84.1">The </span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.85.1">applications of image generation are endless. </span><span class="koboSpan" id="kobo.85.2">Here are some of the real-world applications of </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">image generation:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">Advertising and marketing</span></strong><span class="koboSpan" id="kobo.88.1">: In the world of advertising and marketing, visuals play a crucial role in capturing attention and conveying messages effectively. </span><span class="koboSpan" id="kobo.88.2">With image generation, you can revolutionize marketing campaigns by producing unique, visually striking images tailored to specific target audiences. </span><span class="koboSpan" id="kobo.88.3">Marketers can leverage Bedrock models to generate personalized product advertisements, social media visuals, and eye-catching graphics that resonate with their desired demographics. </span><span class="koboSpan" id="kobo.88.4">Furthermore, marketers can create variations of images based on customer preferences, ensuring that marketing materials are highly relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">and engaging.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.90.1">Graphic design and content creation</span></strong><span class="koboSpan" id="kobo.91.1">: Graphic designers and content creators often face the challenge of conceptualizing and visualizing ideas before executing them. </span><span class="koboSpan" id="kobo.91.2">With Bedrock’s image generation models, you can streamline this process by relying on this powerful tool for generating initial concepts, illustrations, and visual assets. </span><span class="koboSpan" id="kobo.91.3">Designers can use image generation models to explore different styles, compositions, and color schemes, facilitating quick iterations and experimentation. </span><span class="koboSpan" id="kobo.91.4">Additionally, content creators can leverage Bedrock models to generate unique and captivating images for blog posts, articles, or other marketing materials, enhancing their visual appeal and potential </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">for engagement.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.93.1">Product visualization and prototyping</span></strong><span class="koboSpan" id="kobo.94.1">: Effective product visualization is essential for iterating designs, gathering feedback, and showcasing offerings. </span><span class="koboSpan" id="kobo.94.2">With Bedrock image generation models, businesses can generate realistic visualizations of product designs, allowing for rapid prototyping and evaluation before investing in physical prototypes. </span><span class="koboSpan" id="kobo.94.3">Bedrock models can create images of products in various environments or from different angles, providing stakeholders with a comprehensive understanding of the product’s appearance and functionality. </span><span class="koboSpan" id="kobo.94.4">This capability can significantly accelerate the product development cycle and aid in marketing and </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">sales efforts.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Gaming and virtual environments</span></strong><span class="koboSpan" id="kobo.97.1">: The gaming and </span><strong class="bold"><span class="koboSpan" id="kobo.98.1">Virtual Reality</span></strong><span class="koboSpan" id="kobo.99.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.100.1">VR</span></strong><span class="koboSpan" id="kobo.101.1">) industries </span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.102.1">heavily rely on visually immersive </span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.103.1">experiences. </span><span class="koboSpan" id="kobo.103.2">Bedrock’s image generation models can empower developers to create unique textures, environments, and assets for video</span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.104.1"> games, VR, or </span><strong class="bold"><span class="koboSpan" id="kobo.105.1">Augmented Reality</span></strong><span class="koboSpan" id="kobo.106.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.107.1">AR</span></strong><span class="koboSpan" id="kobo.108.1">) applications. </span><span class="koboSpan" id="kobo.108.2">Bedrock image models can generate custom avatars, character designs, and intricate visual elements based on user specifications or game narratives. </span><span class="koboSpan" id="kobo.108.3">In addition, developers can enhance the realism and diversity of their virtual worlds, offering players a more engaging and </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">personalized experience.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.110.1">Architecture and interior design</span></strong><span class="koboSpan" id="kobo.111.1">: Visualizing architectural designs and interior spaces is crucial for architects and interior designers, as well as their clients. </span><span class="koboSpan" id="kobo.111.2">Bedrock image models can generate realistic renderings of proposed designs, allowing stakeholders to immerse themselves in the envisioned spaces before construction or renovation. </span><span class="koboSpan" id="kobo.111.3">Bedrock’s capabilities can aid in visualizing different materials, furniture arrangements, and lighting conditions, enabling architects and designers to refine their concepts and present compelling proposals to clients </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">or decision-makers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.113.1">Fashion and apparel</span></strong><span class="koboSpan" id="kobo.114.1">: In the fashion and apparel industry, Amazon Bedrock image models can generate unique textile designs, patterns, and clothing styles, enabling fashion designers to explore new concepts and stay ahead of trends. </span><span class="koboSpan" id="kobo.114.2">Additionally, Bedrock can create visualizations of clothing items on different body types or in various environments, allowing customers to preview how garments would look in real life before making a purchase. </span><span class="koboSpan" id="kobo.114.3">This capability can enhance the shopping experience and reduce </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">return rates.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.116.1">Scientific visualization</span></strong><span class="koboSpan" id="kobo.117.1">: Effective communication of scientific data, phenomena, and simulations is crucial in research and education. </span><span class="koboSpan" id="kobo.117.2">Amazon Bedrock’s image generation models can assist scientists and researchers in creating visual representations of complex concepts, making them more accessible and understandable. </span><span class="koboSpan" id="kobo.117.3">Bedrock models can generate illustrations, diagrams, or 3D models for scientific publications, presentations, or educational materials, facilitating knowledge transfer and fostering a deeper understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">intricate topics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.119.1">Art and creative expression</span></strong><span class="koboSpan" id="kobo.120.1">: Artists can leverage Bedrock image models to explore new styles, techniques, and concepts by generating unique and imaginative images based on textual prompts or </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">conceptual frameworks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.122.1">E-commerce and product catalogs</span></strong><span class="koboSpan" id="kobo.123.1">: In the e-commerce landscape, high-quality product images are essential for attracting customers and driving sales. </span><span class="koboSpan" id="kobo.123.2">Amazon Bedrock image models can generate visually appealing and accurate product images for online catalogs or e-commerce platforms, reducing the need for extensive photoshoots and the associated costs. </span><span class="koboSpan" id="kobo.123.3">These models can also create visualizations of customized products or configurations based on customer preferences, enhancing the shopping experience and enabling personalization </span><a id="_idIndexMarker729"/><span class="No-Break"><span class="koboSpan" id="kobo.124.1">at scale.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.125.1">Now that we have looked at some real-world applications, let us explore various multimodal models and their </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">inner workings.</span></span></p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor180"/><span class="koboSpan" id="kobo.127.1">Multimodal models</span></h1>
<p><span class="koboSpan" id="kobo.128.1">So far in this book, we have looked at </span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.129.1">single-modal model architecture patterns, such as text-to-text generation, that includes QA, summarization, code generation, and so on. </span><span class="koboSpan" id="kobo.129.2">Let us now expand our understanding to another type of generative AI model: </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">multimodal models.</span></span></p>
<p><span class="koboSpan" id="kobo.131.1">Multimodal models are a type of model that can understand and interpret more than one modality, such as image, audio, and video, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.132.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.133.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<span class="koboSpan" id="kobo.135.1"><img alt="Figure 9.1 – Multimodality" src="image/B22045_09_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.136.1">Figure 9.1 – Multimodality</span></p>
<p><span class="koboSpan" id="kobo.137.1">The response</span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.138.1"> received from these models can also be multimodal. </span><span class="koboSpan" id="kobo.138.2">Behind the scenes, these FMs comprise multiple single-modal neural networks that process text, image, audio, and </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">video separately.</span></span></p>
<p><span class="koboSpan" id="kobo.140.1">Now let us look at the multimodality models that are available within </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">Amazon Bedrock.</span></span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.142.1">Stable Diffusion</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.143.1">Stable Diffusion</span></strong><span class="koboSpan" id="kobo.144.1"> is a</span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.145.1"> state-of-the-art image generation model </span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.146.1">that has gained significant attention in the field of generative AI. </span><span class="koboSpan" id="kobo.146.2">Unlike many other image generation models, Stable Diffusion employs a unique diffusion-based approach, which sets it apart from other methods </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">or techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.148.1">At the heart of Stable Diffusion is the concept</span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.149.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.150.1">diffusion</span></strong><span class="koboSpan" id="kobo.151.1">, which involves a forward and a reverse diffusion process. </span><span class="koboSpan" id="kobo.151.2">In </span><strong class="bold"><span class="koboSpan" id="kobo.152.1">forward diffusion</span></strong><span class="koboSpan" id="kobo.153.1">, Gaussian noise is progressively added to an image until it </span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.154.1">becomes entirely random. </span><span class="koboSpan" id="kobo.154.2">The model then learns to reverse this process, gradually removing the noise to reconstruct the original image. </span><span class="koboSpan" id="kobo.154.3">This reversal is </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.155.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.156.1">reverse diffusion</span></strong><span class="koboSpan" id="kobo.157.1"> and is the key to Stable Diffusion’s </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">impressive performance.</span></span></p>
<p><span class="koboSpan" id="kobo.159.1">This diffusion process has several </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">key components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.161.1">Contrastive Language-Image Pre-Training</span></strong><span class="koboSpan" id="kobo.162.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.163.1">CLIP</span></strong><span class="koboSpan" id="kobo.164.1">): CLIP</span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.165.1"> is a neural network trained on a vast dataset of image-text pairs, enabling it to understand the semantic relationships between visual and textual representations. </span><span class="koboSpan" id="kobo.165.2">This component plays a crucial role in bridging the gap between natural language prompts and their corresponding </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">visual manifestations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.167.1">U-Net</span></strong><span class="koboSpan" id="kobo.168.1">: This serves as a backbone for the image generation process. </span><span class="koboSpan" id="kobo.168.2">U-Net</span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.169.1"> is a convolutional neural network designed for image-to-image translation tasks such as segmentation and denoising. </span><span class="koboSpan" id="kobo.169.2">Segmentation is the process whereby the images are partitioned into multiple segments or sets of pixels to locate objects and boundaries. </span><span class="koboSpan" id="kobo.169.3">Denoising removes the noise from an image to improve its quality. </span><span class="koboSpan" id="kobo.169.4">In the context of Stable Diffusion, U-Net is responsible for generating and refining the output image based on the input prompt and guidance </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">from CLIP.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.171.1">VAE</span></strong><span class="koboSpan" id="kobo.172.1">: This</span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.173.1"> is another critical component that helps ensure that the generated images are coherent and realistic. </span><span class="koboSpan" id="kobo.173.2">In Stable Diffusion, VAE encodes the generated image into a compressed representation, which is then decoded to produce the final </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">output image.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.175.1">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.176.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.177.1">.2</span></em><span class="koboSpan" id="kobo.178.1">, here is a</span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.179.1"> high-level overview of how the whole </span><a id="_idIndexMarker741"/><span class="No-Break"><span class="koboSpan" id="kobo.180.1">process works:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.181.1">The user provides a natural language prompt describing the </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">desired image.</span></span></li>
<li><span class="koboSpan" id="kobo.183.1">The CLIP model analyzes the prompt and generates a corresponding embedding, representing the semantic meaning of </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">the text.</span></span></li>
<li><span class="koboSpan" id="kobo.185.1">The U-Net architecture takes this embedding as input, along with an initial random </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">noise image.</span></span></li>
<li><span class="koboSpan" id="kobo.187.1">Through a series of convolutional and deconvolutional operations, U-Net iteratively refines the noise image, guided by the CLIP embedding, to produce an image that matches the </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">input prompt.</span></span></li>
<li><span class="koboSpan" id="kobo.189.1">The generated image is then passed through the VAE, which encodes and decodes it, ensuring coherence </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">and realism.</span></span></li>
<li><span class="koboSpan" id="kobo.191.1">The final output image is produced, reflecting the </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">user’s prompt.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer128">
<span class="koboSpan" id="kobo.193.1"><img alt="Figure 9.2 –  The Stable Diffusion process" src="image/B22045_09_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.194.1">Figure 9.2 –  The Stable Diffusion process</span></p>
<p><span class="koboSpan" id="kobo.195.1">By combining these</span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.196.1"> architectural elements, Stable Diffusion is able to </span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.197.1">generate high-quality, diverse images that are both visually appealing and semantically coherent with the input prompts. </span><span class="koboSpan" id="kobo.197.2">In order to understand the detailed workings of the diffusion process, readers are encouraged to read the research paper </span><em class="italic"><span class="koboSpan" id="kobo.198.1">On the Design Fundamentals</span></em> <em class="italic"><span class="koboSpan" id="kobo.199.1">of Diffusion Models: A Survey</span></em><span class="koboSpan" id="kobo.200.1">. </span><span class="koboSpan" id="kobo.200.2">It can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">on</span></span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">: </span></span><a href="https://arxiv.org/pdf/2306.04542.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.203.1">https://arxiv.org/pdf/2306.04542.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.204.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">This paper explains how diffusion models work by gradually adding noise to training data and then learning to reverse that process to generate new samples. </span><span class="koboSpan" id="kobo.205.2">The paper highlights the wide range of applications for diffusion models, including image editing, text-to-image generation, and 3D </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">object creation.</span></span></p>
<p><span class="koboSpan" id="kobo.207.1">Additionally, readers are recommended to explore the </span><em class="italic"><span class="koboSpan" id="kobo.208.1">How Diffusion Models Work</span></em><span class="koboSpan" id="kobo.209.1"> course from DeepLearning.AI </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">at </span></span><a href="https://learn.deeplearning.ai/courses/diffusion-models/"><span class="No-Break"><span class="koboSpan" id="kobo.211.1">https://learn.deeplearning.ai/courses/diffusion-models/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.212.1">.</span></span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.213.1">Titan Image Generator G1</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.214.1">Titan Image Generator G1</span></strong><span class="koboSpan" id="kobo.215.1"> is a </span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.216.1">proprietary image generation model by Amazon that </span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.217.1">allows users to generate images from text, edit existing images, and create variations of images. </span><span class="koboSpan" id="kobo.217.2">The model is designed to make it easy for users to iterate on image concepts by generating multiple image options based on text descriptions. </span><span class="koboSpan" id="kobo.217.3">The model is trained on diverse high-quality datasets, so it can understand complex prompts with multiple objects and generate </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">realistic images.</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">This model supports image editing capabilities such as editing with text prompts using a built-in segmentation model, generating variations of the image, inpainting with an image mask, and outpainting to extend or change the background of an image. </span><span class="koboSpan" id="kobo.219.2">You can upload an existing image and provide instructions or prompts to modify specific aspects of the image. </span><span class="koboSpan" id="kobo.219.3">The model can intelligently alter the composition, add or remove elements, change colors, or apply </span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.220.1">various artistic styles, all while preserving the overall coherence and</span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.221.1"> realism of the </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">original image.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">We will dive deeper into these capabilities in the </span><em class="italic"><span class="koboSpan" id="kobo.224.1">Multimodal design </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.225.1">patterns</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.226.1"> section.</span></span></p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.227.1">Titan Multimodal Embeddings</span></h2>
<p><span class="koboSpan" id="kobo.228.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.229.1">Titan Multimodal Embeddings</span></strong><span class="koboSpan" id="kobo.230.1"> model</span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.231.1"> is</span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.232.1"> part of the Amazon Titan family of models designed for use cases such as image search and similarity-based recommendation with high accuracy and </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">fast response.</span></span></p>
<p><span class="koboSpan" id="kobo.234.1">The Titan Multimodal Embeddings model’s core strength lies in its ability to generate high-dimensional vector representations for both textual and visual data. </span><span class="koboSpan" id="kobo.234.2">These embeddings encapsulate the semantic relationships between different modalities, allowing for efficient and effective search and </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">retrieval operations.</span></span></p>
<p><span class="koboSpan" id="kobo.236.1">The model supports up to 128 tokens as input text in English, as well as image sizes of up to 25 MB, and converts those to vector embeddings. </span><span class="koboSpan" id="kobo.236.2">The default embedding size is 1024 dimensions, providing a rich representation that captures nuanced details and complex relationships. </span><span class="koboSpan" id="kobo.236.3">However, you can also configure smaller vector dimensions to optimize for speed and cost, depending on your specific use case and </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">performance requirements.</span></span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.238.1">Anthropic Claude 3 – Sonnet, Haiku, and Opus</span></h2>
<p><span class="koboSpan" id="kobo.239.1">Anthropic</span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.240.1"> Claude 3 </span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.241.1">Model variants – </span><em class="italic"><span class="koboSpan" id="kobo.242.1">Claude 3 Sonnet</span></em><span class="koboSpan" id="kobo.243.1">, </span><em class="italic"><span class="koboSpan" id="kobo.244.1">Claude 3 Haiku</span></em><span class="koboSpan" id="kobo.245.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.246.1">Claude 3 Opus</span></em><span class="koboSpan" id="kobo.247.1"> – are </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.248.1">the most recent and advanced family of</span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.249.1"> Anthropic Claude </span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.250.1">models available on Amazon Bedrock. </span><span class="koboSpan" id="kobo.250.2">All these models have multimodal capabilities, meaning that they are able to perceive and analyze images as well as text input, with a 200K context window. </span><span class="koboSpan" id="kobo.250.3">We encourage you to refer to the </span><em class="italic"><span class="koboSpan" id="kobo.251.1">Anthropic Claude</span></em><span class="koboSpan" id="kobo.252.1"> section in </span><a href="B22045_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.253.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.254.1"> if you would like to go over their </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">details again.</span></span></p>
<p><span class="koboSpan" id="kobo.256.1">Now that we have looked at the multimodal models available within Amazon Bedrock, let us explore some of the </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">design patterns.</span></span></p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.258.1">Multimodal design patterns</span></h1>
<p><span class="koboSpan" id="kobo.259.1">With multimodal design patterns, we </span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.260.1">integrate different modalities, such as text, images, audio, and so on. </span><span class="koboSpan" id="kobo.260.2">With the multimodal models available, the ability to generate, manipulate, and understand images from text or other input modalities has become increasingly important in a wide range of applications, from creative design to scientific visualization </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">and beyond.</span></span></p>
<p><span class="koboSpan" id="kobo.262.1">Numerous patterns can be created with multimodal models. </span><span class="koboSpan" id="kobo.262.2">In this section, we are going to cover some of the </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">common patterns.</span></span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.264.1">Text-to-image</span></h2>
<p><span class="koboSpan" id="kobo.265.1">With a </span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.266.1">text-to-image pattern, you provide the text as a prompt to</span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.267.1"> the model. </span><span class="koboSpan" id="kobo.267.2">The model will then generate an image based on that prompt, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.268.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.269.1">.3.</span></em></span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<span class="koboSpan" id="kobo.270.1"><img alt="Figure 9.3 – A text-to-image pattern" src="image/B22045_09_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.271.1">Figure 9.3 – A text-to-image pattern</span></p>
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.272.1">Parameters</span></strong></span></p>
<p><span class="koboSpan" id="kobo.273.1">At the core of image generation models are a set of customizable inference parameters and controls that allow users to get the desired image from the model. </span><span class="koboSpan" id="kobo.273.2">Let us look at </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">these parameters:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.275.1">Negative prompt</span></strong><span class="koboSpan" id="kobo.276.1">: Specify </span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.277.1">elements, content, or details </span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.278.1">you want to exclude from the generated image. </span><span class="koboSpan" id="kobo.278.2">For example, we can add negative prompts such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">Cloud</span></strong><span class="koboSpan" id="kobo.280.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">seating bench</span></strong><span class="koboSpan" id="kobo.282.1"> to exclude them from the image, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.283.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.284.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer130">
<span class="koboSpan" id="kobo.286.1"><img alt="Figure 9.4 – A text-to-image pattern with negative prompts" src="image/B22045_09_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.287.1">Figure 9.4 – A text-to-image pattern with negative prompts</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.288.1">Reference image</span></strong><span class="koboSpan" id="kobo.289.1">: This</span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.290.1"> provides users the ability to input a reference image to the model, which can be leveraged by the model as a baseline to generate its response (generated image). </span><span class="koboSpan" id="kobo.290.2">For instance, if we use the image generated from the preceding figure and pass it as a reference along with the prompt, the prompt would be something </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">like this:</span></span></li>
</ul>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">A futuristic cityscape at night, with towering skyscrapers made of glass and metal. </span><span class="koboSpan" id="kobo.292.2">The buildings are illuminated by neon lights in shades of blue, purple, and pink. </span><span class="koboSpan" id="kobo.292.3">The streets are lined with holographic billboards </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">and advertisements.</span></strong></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.294.1">The model will use the reference image and the prompt to generate a new image, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.295.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.296.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<span class="koboSpan" id="kobo.298.1"><img alt="Figure 9.5 – A text-to-image pattern using a reference Image" src="image/B22045_09_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.299.1">Figure 9.5 – A text-to-image pattern using a reference Image</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.300.1">Prompt Strength</span></strong><strong class="bold"><span class="koboSpan" id="kobo.301.1"> (</span></strong><strong class="bold"><span class="koboSpan" id="kobo.302.1">cfg_scale</span></strong><strong class="bold"><span class="koboSpan" id="kobo.303.1">)</span></strong><span class="koboSpan" id="kobo.304.1">: Prompt</span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.305.1"> strength, also known as </span><strong class="bold"><span class="koboSpan" id="kobo.306.1">Classifier-Free Guidance scale</span></strong><span class="koboSpan" id="kobo.307.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.308.1">cfg_scale</span></em><span class="koboSpan" id="kobo.309.1">) determines </span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.310.1">the degree to which the generated image adheres to the provided text prompt. </span><span class="koboSpan" id="kobo.310.2">A higher value indicates that the image generation process will adhere more closely to the text prompt, while a lower value allows for more creative interpretation and diversity in the generated images. </span><span class="koboSpan" id="kobo.310.3">Using a cfg_scale value somewhere in the middle (</span><em class="italic"><span class="koboSpan" id="kobo.311.1">10-15</span></em><span class="koboSpan" id="kobo.312.1">) is generally recommended, as it strikes a balance between faithfully representing the text prompt and allowing for artistic expression. </span><span class="koboSpan" id="kobo.312.2">However, the optimal value may vary </span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.313.1">depending on your use case or what you are looking for, complexity of the prompt, and the desired level of detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">generated image.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.315.1">Generation Step</span></strong><strong class="bold"><span class="koboSpan" id="kobo.316.1"> (</span></strong><strong class="bold"><span class="koboSpan" id="kobo.317.1">steps</span></strong><strong class="bold"><span class="koboSpan" id="kobo.318.1">)</span></strong><span class="koboSpan" id="kobo.319.1">: The </span><em class="italic"><span class="koboSpan" id="kobo.320.1">steps</span></em><span class="koboSpan" id="kobo.321.1"> parameter </span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.322.1">in Stable Diffusion refers to the number of iterations or cycles the algorithm goes through to generate an image from the input text. </span><span class="koboSpan" id="kobo.322.2">It’s an important setting that affects the quality and detail of the final image. </span><span class="koboSpan" id="kobo.322.3">Here is how </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">it works:</span></span><ul><li><span class="koboSpan" id="kobo.324.1">The process starts with random noise, and with each step, some of that noise is removed, gradually revealing the </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">intended image.</span></span></li><li><span class="koboSpan" id="kobo.326.1">More steps generally lead to higher-quality images with more detail, but there’s a point of </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">diminishing returns.</span></span></li><li><span class="koboSpan" id="kobo.328.1">The ideal number of steps can vary depending on the complexity of the image you’re trying to generate and your personal preferences. </span><span class="koboSpan" id="kobo.328.2">However, going much higher may not significantly improve the image but will increase </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">generation time.</span></span></li><li><span class="koboSpan" id="kobo.330.1">For simple subjects or scenes, fewer steps (around </span><em class="italic"><span class="koboSpan" id="kobo.331.1">10-15</span></em><span class="koboSpan" id="kobo.332.1">) may be sufficient. </span><span class="koboSpan" id="kobo.332.2">But for more </span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.333.1">complex or detailed images, you may want to increase the steps to </span><em class="italic"><span class="koboSpan" id="kobo.334.1">40-50</span></em><span class="koboSpan" id="kobo.335.1"> or </span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.336.1">even more, depending on how much detailed you are </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">looking for.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.338.1">What we discussed are just some of the parameters. </span><span class="koboSpan" id="kobo.338.2">The following figure highlights additional </span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.339.1">parameters for </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">Stable</span></span><span class="No-Break"><a id="_idIndexMarker768"/></span><span class="No-Break"><span class="koboSpan" id="kobo.341.1"> Diffusion.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer132">
<span class="koboSpan" id="kobo.342.1"><img alt="Figure 9.6 – Stable Diffusion text-to-image parameters" src="image/B22045_09_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.343.1">Figure 9.6 – Stable Diffusion text-to-image parameters</span></p>
<p><span class="koboSpan" id="kobo.344.1">For a </span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.345.1">more detailed description </span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.346.1">of these parameters, you can go through the Stable Diffusion documentation </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">at </span></span><a href="https://platform.stability.ai/docs/api-reference#tag/Image-to-Image"><span class="No-Break"><span class="koboSpan" id="kobo.348.1">https://platform.stability.ai/docs/api-reference#tag/Image-to-Image</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.349.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.350.1">If you are </span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.351.1">using Amazon Titan Image Generator, here is the list of parameters that you can </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">use: </span></span><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html"><span class="No-Break"><span class="koboSpan" id="kobo.353.1">https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.354.1">.</span></span></p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.355.1">Image search</span></h2>
<p><span class="koboSpan" id="kobo.356.1">Image search has emerged </span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.357.1">as a powerful tool that enables users to explore and leverage vast collections of visual data. </span><span class="koboSpan" id="kobo.357.2">With FMs from Amazon Bedrock, you can perform image searches to understand and interpret visual content. </span><span class="koboSpan" id="kobo.357.3">You can identify and understand various elements within an image, such as objects, scenes, colors, textures, and even abstract concepts. </span><span class="koboSpan" id="kobo.357.4">To illustrate the power of image search, let’s consider a </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">practical example.</span></span></p>
<p><span class="koboSpan" id="kobo.359.1">Imagine you’re a fashion retailer with an extensive catalog of clothing items. </span><span class="koboSpan" id="kobo.359.2">With Bedrock, you can upload your product images and leverage the image search capabilities to enable customers to find visually similar items. </span><span class="koboSpan" id="kobo.359.3">For instance, a customer could upload a picture of a dress that they like and Bedrock would return a set of visually similar dresses from your catalog, facilitating a more engaging and personalized </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">shopping experience.</span></span></p>
<p><span class="koboSpan" id="kobo.361.1">One powerful approach to image search is based on multimodal embeddings, which allow for the representation of both text and images in a vector space. </span><span class="koboSpan" id="kobo.361.2">These vectors capture the visual features and semantic information of the images. </span><span class="koboSpan" id="kobo.361.3">The vectors, along with metadata such as image paths, are then stored in a searchable index vector database such as OpenSearch Serverless, FAISS, or Pinecone. </span><span class="koboSpan" id="kobo.361.4">This technique enables searching for images using text queries or finding similar images based on a given image (or a combination of text </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">and image).</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">When a user initiates a search, their input (text, image, or both) is also converted into a vector representation using the same multimodal embedding model. </span><span class="koboSpan" id="kobo.363.2">The search vector is then compared against the vectors in the index and the most similar vectors are retrieved based on the vector similarity scores. </span><span class="koboSpan" id="kobo.363.3">This approach allows for flexible and intuitive image search, as users can search using natural language descriptions, upload example images, or combine text and images for more precise results. </span><span class="koboSpan" id="kobo.363.4">For example, you could search for </span><strong class="source-inline"><span class="koboSpan" id="kobo.364.1">a red sports car on a city street</span></strong><span class="koboSpan" id="kobo.365.1"> and the model would return relevant images from its data store that match both the visual and </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">textual criteria.</span></span></p>
<p><span class="koboSpan" id="kobo.367.1">As you might have recognized by now, this process is similar to the RAG process that we discussed in </span><a href="B22045_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.368.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.369.1">. </span><span class="koboSpan" id="kobo.369.2">The difference here is that the model is retrieving the images from its data store and is not generating new images. </span><span class="koboSpan" id="kobo.369.3">Here is a great example to try out multimodal embedding and </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">searching: </span></span><a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.371.1">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/04_Image_and_Multimodal/bedrock-titan-multimodal-embeddings.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.372.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.373.1">Image search </span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.374.1">with multimodal embeddings has numerous real-world applications across various domains. </span><span class="koboSpan" id="kobo.374.2">In e-commerce platforms, it can be used to enhance product search and recommendation systems, allowing customers to find visually similar products or to search for items using natural language descriptions or example images. </span><span class="koboSpan" id="kobo.374.3">In the media and entertainment industry, it can assist in content organization, tag suggestion, and copyright infringement detection by identifying similar or </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">duplicate images.</span></span></p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.376.1">Image understanding</span></h2>
<p><span class="koboSpan" id="kobo.377.1">The Anthropic </span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.378.1">Claude 3 models – Sonnet, Haiku, and Opus – introduce the image understanding capability, through which the model can analyze the image and provide you with a response based on what you are looking to know. </span><span class="koboSpan" id="kobo.378.2">For example, you can provide an image of a kitchen or a living room and ask the model to provide a detailed description of the image or write a fictional story based on </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">the image.</span></span></p>
<h3><span class="koboSpan" id="kobo.380.1">Example 1</span></h3>
<p><span class="koboSpan" id="kobo.381.1">Use the</span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.382.1"> following prompt: </span><strong class="source-inline"><span class="koboSpan" id="kobo.383.1">Provide a detailed description of </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.384.1">this image</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">.</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<span class="koboSpan" id="kobo.386.1"><img alt="Figure 9.7 – Image understanding and a detailed description in the output" src="image/B22045_09_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.387.1">Figure 9.7 – Image understanding and a detailed description in the output</span></p>
<p><span class="koboSpan" id="kobo.388.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.389.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.390.1">.7</span></em><span class="koboSpan" id="kobo.391.1">, we </span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.392.1">have provided the image of a kitchen to the Anthropic Claude 3 model and asked it to provide a detailed description of the image. </span><span class="koboSpan" id="kobo.392.2">The model is able to provide minute details such as </span><strong class="bold"><span class="koboSpan" id="kobo.393.1">the room features dark wood cabinetry, contrasted by light marble countertops</span></strong><span class="koboSpan" id="kobo.394.1">, and </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">so on.</span></span></p>
<h3><span class="koboSpan" id="kobo.396.1">Example 2</span></h3>
<p><span class="koboSpan" id="kobo.397.1">Use the following prompt: </span><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">Write a fictional story based on the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.399.1">image attached</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<span class="koboSpan" id="kobo.401.1"><img alt="Figure 9.8 – Image understanding with a fictional story" src="image/B22045_09_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.402.1">Figure 9.8 – Image understanding with a fictional story</span></p>
<p><span class="koboSpan" id="kobo.403.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.404.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.405.1">.8</span></em><span class="koboSpan" id="kobo.406.1">, you</span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.407.1"> can see that the model has generated a fictional story based on the image of a library provided </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">to it.</span></span></p>
<h3><span class="koboSpan" id="kobo.409.1">Example 3</span></h3>
<p><span class="koboSpan" id="kobo.410.1">Use the following prompt: </span><strong class="source-inline"><span class="koboSpan" id="kobo.411.1">Provide the list of items/objects present in the image and explain </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">each item</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<span class="koboSpan" id="kobo.414.1"><img alt="Figure 9.9 – Image understanding with object identification" src="image/B22045_09_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.415.1">Figure 9.9 – Image understanding with object identification</span></p>
<p><span class="koboSpan" id="kobo.416.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.417.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.418.1">.9</span></em><span class="koboSpan" id="kobo.419.1">, the model is able to identify the items and objects in the image along with their details, showcasing the image classification/object </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">recognition capability.</span></span></p>
<p><span class="koboSpan" id="kobo.421.1">The Claude </span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.422.1">models’ image understanding capabilities are not limited to those discussed in the preceding examples. </span><span class="koboSpan" id="kobo.422.2">They can also be utilized for tasks such as image captioning, creating detailed image descriptions, identifying subjects, and answering questions about the contents of an image. </span><span class="koboSpan" id="kobo.422.3">You can look at various use cases of image understanding</span><a id="_idIndexMarker779"/> <span class="No-Break"><span class="koboSpan" id="kobo.423.1">at </span></span><a href="https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities"><span class="No-Break"><span class="koboSpan" id="kobo.424.1">https://docs.anthropic.com/claude/docs/use-cases-and-capabilities#vision-capabilities</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.425.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.426.1">To use this capability within the Amazon Bedrock console, yo</span><a href="https://console.aws.amazon.com/bedrock-"><span class="koboSpan" id="kobo.427.1">u can follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">ensuing steps:</span></span></a></p>
<ol>
<li><a href="https://console.aws.amazon.com/bedrock-"><span class="koboSpan" id="kobo.429.1">Go t</span></a><span class="koboSpan" id="kobo.430.1">o Amazon Bedrock Console </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">at </span></span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">https://console.aws.amazon.com/bedrock</span></span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.434.1">Navigate to </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.435.1">Chat Playground</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.437.1">Click on </span><strong class="bold"><span class="koboSpan" id="kobo.438.1">Select model</span></strong><span class="koboSpan" id="kobo.439.1">. </span><span class="koboSpan" id="kobo.439.2">Choose the </span><strong class="bold"><span class="koboSpan" id="kobo.440.1">Anthropic Claude 3 Sonnet</span></strong><span class="koboSpan" id="kobo.441.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.442.1">Anthropic Claude 3 Haiku</span></strong><span class="koboSpan" id="kobo.443.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.444.1">Anthropic Claude 3 </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.445.1">Opus</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.446.1"> model.</span></span></li>
<li><span class="koboSpan" id="kobo.447.1">Attach the image you want to analyze and provide the prompt based on what you are looking for, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.448.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.449.1">.10</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer136">
<span class="koboSpan" id="kobo.451.1"><img alt="Figure 9.10 – How to analyze an image using the Anthropic Claude 3 models" src="image/B22045_09_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.452.1">Figure 9.10 – How to analyze an image using the Anthropic Claude 3 models</span></p>
<p><span class="koboSpan" id="kobo.453.1">If you are using </span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.454.1">AWS SDK, you can use Anthropic’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.455.1">Messages</span></strong><span class="koboSpan" id="kobo.456.1"> API to create a chat application and provide an image for understanding. </span><span class="koboSpan" id="kobo.456.2">Here is some example AWS Python SDK code for a multimodal message to the Claude 3 Sonnet </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">model: </span></span><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example"><span class="No-Break"><span class="koboSpan" id="kobo.458.1">https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#api-inference-examples-claude-multimodal-code-example</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.459.1">.</span></span></p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.460.1">Image-to-image patterns</span></h2>
<p><span class="koboSpan" id="kobo.461.1">When it comes</span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.462.1"> to image-to-image generation, the model takes an </span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.463.1">existing image as input and modifies it based on the prompt or instructions you provide. </span><span class="koboSpan" id="kobo.463.2">This is different from text-to-image generation, whereby the model creates an entirely new image from scratch based solely on a textual description or prompt. </span><span class="koboSpan" id="kobo.463.3">In image-to-image generation, on the other hand, the model uses the existing image as a starting point and then applies the necessary changes or transformations to produce the desired output image. </span><span class="koboSpan" id="kobo.463.4">This can involve adjusting various aspects such as colors, textures, objects, or even the overall composition of the image, all guided by the prompt. </span><span class="koboSpan" id="kobo.463.5">It’s like having a clay model and reshaping it to your desired outcome rather than starting from a lump of raw clay. </span><span class="koboSpan" id="kobo.463.6">The ability to modify and manipulate existing images opens up a range of creative possibilities and use cases, from enhancing and editing photographs to creating artistic interpretations </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">or visualizations.</span></span></p>
<p><span class="koboSpan" id="kobo.465.1">A simple example of image-to-image generation is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.466.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.467.1">.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<span class="koboSpan" id="kobo.469.1"><img alt="Figure 9.11 – Simple image-to-image generation" src="image/B22045_09_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.470.1">Figure 9.11 – Simple image-to-image generation</span></p>
<p><span class="koboSpan" id="kobo.471.1">When using the</span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.472.1"> Stable Diffusion model for image-to-image generation, there</span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.473.1"> are a few additional parameters to consider along with the text-to-text parameters mentioned in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.474.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.475.1">.6</span></em><span class="koboSpan" id="kobo.476.1">. </span><span class="koboSpan" id="kobo.476.2">These</span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.477.1"> additional </span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.478.1">parameters are highlighted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.479.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.480.1">.12.</span></em></span></p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<span class="koboSpan" id="kobo.481.1"><img alt="Figure 9.12 – Stable Diffusion image-to-image parameters" src="image/B22045_09_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.482.1">Figure 9.12 – Stable Diffusion image-to-image parameters</span></p>
<p><span class="koboSpan" id="kobo.483.1">You can </span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.484.1">learn more about them </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">here: </span></span><a href="https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage"><span class="No-Break"><span class="koboSpan" id="kobo.486.1">https://platform.stability.ai/docs/api-reference#tag/Image-to-Image/operation/imageToImage</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.487.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.488.1">Next, let us look at some </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">image-to-image patterns.</span></span></p>
<h3><span class="koboSpan" id="kobo.490.1">Image variation</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.491.1">Image variation</span></strong><span class="koboSpan" id="kobo.492.1">, also </span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.493.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.494.1">image-to-image translation</span></strong><span class="koboSpan" id="kobo.495.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.496.1">style transfer</span></strong><span class="koboSpan" id="kobo.497.1">, is a </span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.498.1">powerful technique in generative AI</span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.499.1"> that allows for the creation of new and unique images by modifying existing ones. </span><span class="koboSpan" id="kobo.499.2">This process involves taking an input image and applying a desired style or transformation to it, resulting in an output image that combines the content of the original with the desired aesthetic or </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">visual characteristics.</span></span></p>
<p><span class="koboSpan" id="kobo.501.1">One real-world example of image variation is in the field of fashion design. </span><span class="koboSpan" id="kobo.501.2">Designers can take an existing garment or accessory and apply various styles, patterns, or textures to create new and innovative designs without starting from scratch. </span><span class="koboSpan" id="kobo.501.3">This not only saves time and resources but also allows for rapid experimentation and iteration, enabling designers to explore a vast range </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">of possibilities.</span></span></p>
<p><span class="koboSpan" id="kobo.503.1">Another example can be found in the art world, where image variation techniques can be used to create unique and expressive artworks. </span><span class="koboSpan" id="kobo.503.2">Artists can take a simple photograph or painting and apply various artistic styles, such as impressionism, cubism, or abstract expressionism, to create entirely new pieces that blend the original content with the desired artistic style. </span><span class="koboSpan" id="kobo.503.3">This opens up new avenues for creative expression and allows artists to explore unconventional and thought-provoking </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">visual interpretations.</span></span></p>
<p><span class="koboSpan" id="kobo.505.1">Image variation also has applications in the fields of interior design and architectural visualization. </span><span class="koboSpan" id="kobo.505.2">Designers and architects can take existing spaces or structures and apply different materials, textures, or lighting conditions to visualize how a space might look with different design choices. </span><span class="koboSpan" id="kobo.505.3">This can be invaluable in helping clients understand and appreciate proposed designs, as well as in enabling designers to quickly iterate and refine </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">their concepts.</span></span></p>
<p><span class="koboSpan" id="kobo.507.1">With Bedrock, you </span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.508.1">can utilize Titan Image Generator to create image variations. </span><span class="koboSpan" id="kobo.508.2">Let us try the following prompt and run it through Titan </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">Image Generator:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">A delicate, nature-inspired pattern with intricate illustrations of birds, butterflies, and foliage, perfect for a romantic, bohemian dress </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.511.1">or scarf.</span></strong></span></p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<span class="koboSpan" id="kobo.512.1"><img alt="Figure 9.13 – Image variation" src="image/B22045_09_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.513.1">Figure 9.13 – Image variation</span></p>
<p><span class="koboSpan" id="kobo.514.1">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.515.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.516.1">.13</span></em><span class="koboSpan" id="kobo.517.1">, Titan Image Generator</span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.518.1"> will create an image (</span><strong class="bold"><span class="koboSpan" id="kobo.519.1">Original Image</span></strong><span class="koboSpan" id="kobo.520.1">). </span><span class="koboSpan" id="kobo.520.2">You can generate image variations that leverage the </span><strong class="bold"><span class="koboSpan" id="kobo.521.1">Original Image</span></strong><span class="koboSpan" id="kobo.522.1"> as a reference, along with an optional prompt that can be provided to </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">the model.</span></span></p>
<h3><span class="koboSpan" id="kobo.524.1">Masking</span></h3>
<p><span class="koboSpan" id="kobo.525.1">Amazon </span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.526.1">Bedrock models – Amazon Titan Generator and Stable Diffusion – offer two powerful image editing techniques: </span><em class="italic"><span class="koboSpan" id="kobo.527.1">masking</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.528.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.529.1">painting</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.531.1">With masking, we define specific areas within an image and mask them, either to be preserved or redrawn. </span><span class="koboSpan" id="kobo.531.2">This masking can be done either via an image file or </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">a prompt.</span></span></p>
<h4><span class="koboSpan" id="kobo.533.1">Image masking</span></h4>
<p><span class="koboSpan" id="kobo.534.1">The approach of </span><strong class="bold"><span class="koboSpan" id="kobo.535.1">image masking</span></strong><span class="koboSpan" id="kobo.536.1"> utilizes a </span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.537.1">separate image file, known as the </span><strong class="bold"><span class="koboSpan" id="kobo.538.1">mask image</span></strong><span class="koboSpan" id="kobo.539.1">, to </span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.540.1">specify the pixels to be masked or preserved in the original image. </span><span class="koboSpan" id="kobo.540.2">The mask image must adhere to the </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">following requirements:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.542.1">Identical dimensions and resolution as the original image</span></strong><span class="koboSpan" id="kobo.543.1">: When using image masking, it’s crucial that the mask image has the exact same dimensions and resolution as the original image that you want to mask. </span><span class="koboSpan" id="kobo.543.2">This ensures that each pixel in the mask image corresponds to a pixel in the original image, allowing for precise masking. </span><span class="koboSpan" id="kobo.543.3">If the dimensions or resolutions differ, the masking process may produce distorted or </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">undesired results.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.545.1">For example, if your original image has a resolution of 1920 x 1080 pixels, the mask image must also have a resolution of 1920 x 1080 pixels. </span><span class="koboSpan" id="kobo.545.2">Any discrepancy in the dimensions or resolution will cause the mask to misalign with the original image, leading to undesirable </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">masking effects.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.547.1">No alpha channel</span></strong><span class="koboSpan" id="kobo.548.1">: The mask image should not have an alpha channel, which is a separate component in some image formats that represents transparency. </span><span class="koboSpan" id="kobo.548.2">While the PNG format supports transparency through an alpha channel, for image masking purposes, the mask image should rely solely on color values (</span><strong class="bold"><span class="koboSpan" id="kobo.549.1">Red, Green, Blue</span></strong><span class="koboSpan" id="kobo.550.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.551.1">RGB</span></strong><span class="koboSpan" id="kobo.552.1">) or</span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.553.1"> grayscale) to represent masked and </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">unmasked regions.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.555.1">The absence of an</span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.556.1"> alpha channel simplifies the masking process and ensures that the masking is based solely on the pixel colors, without any additional transparency information. </span><span class="koboSpan" id="kobo.556.2">This approach is often preferred for its simplicity and compatibility with a wide range of image processing tools </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">and libraries.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.558.1">RGB or grayscale color mode</span></strong><span class="koboSpan" id="kobo.559.1">: The mask image can be either in RGB or grayscale color mode. </span><span class="koboSpan" id="kobo.559.2">In the context of image masking, the color mode determines how the masking </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">is interpreted:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.561.1">RGB color mode</span></strong><span class="koboSpan" id="kobo.562.1">: In this mode, each pixel in the mask image is represented by a </span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.563.1">combination of red, green, and blue values. </span><span class="koboSpan" id="kobo.563.2">The masking process typically interprets black pixels (ones with RGB values of </span><strong class="source-inline"><span class="koboSpan" id="kobo.564.1">0</span></strong><span class="koboSpan" id="kobo.565.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">0</span></strong><span class="koboSpan" id="kobo.567.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">0</span></strong><span class="koboSpan" id="kobo.569.1">) as masked areas, while any non-black pixels are considered </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">unmasked regions.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.571.1">Grayscale color mode</span></strong><span class="koboSpan" id="kobo.572.1">: In grayscale mode</span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.573.1">, each pixel is represented by a single value ranging from </span><strong class="source-inline"><span class="koboSpan" id="kobo.574.1">0</span></strong><span class="koboSpan" id="kobo.575.1"> (black) to </span><strong class="source-inline"><span class="koboSpan" id="kobo.576.1">255</span></strong><span class="koboSpan" id="kobo.577.1"> (white). </span><span class="koboSpan" id="kobo.577.2">The masking process interprets pixels with a value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.578.1">0</span></strong><span class="koboSpan" id="kobo.579.1"> as masked areas, while pixels with non-zero values are considered </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">unmasked regions.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.581.1">The choice between RGB and grayscale color modes depends on your specific use case and on the tools or libraries you’re using for image masking. </span><span class="koboSpan" id="kobo.581.2">Some tools may have a preference for one color mode over </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">the other.</span></span></p>
<p><span class="koboSpan" id="kobo.583.1">For example, let’s assume that you work in the Food and Beverages industry and you want to mask out certain food items from an image to create a transparent layer for a menu design. </span><span class="koboSpan" id="kobo.583.2">Suppose that you want to mask the bowl of chips in the image that follows and maybe remove it from your menu. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.584.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.585.1">.14</span></em><span class="koboSpan" id="kobo.586.1"> shows the original image and the masked image, where the masking is done on the bowl </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">of chips.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<span class="koboSpan" id="kobo.588.1"><img alt="Figure 9.14 – Image masking" src="image/B22045_09_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.589.1">Figure 9.14 – Image masking</span></p>
<p><span class="koboSpan" id="kobo.590.1">If you want to </span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.591.1">experiment with image masking, you can use online photo editing tools or apps. </span><span class="koboSpan" id="kobo.591.2">There is also the </span><strong class="bold"><span class="koboSpan" id="kobo.592.1">Python Image Library</span></strong><span class="koboSpan" id="kobo.593.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.594.1">PIL</span></strong><span class="koboSpan" id="kobo.595.1">), a very popular Python library</span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.596.1"> that is worth checking out </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">at: </span></span><a href="https://pillow.readthedocs.io/en/stable/reference/Image.html"><span class="No-Break"><span class="koboSpan" id="kobo.598.1">https://pillow.readthedocs.io/en/stable/reference/Image.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.599.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.600.1">In addition, we recommend that you experiment with the following GitHub examples from the Amazon Bedrock workshop that showcase image masking and </span><span class="No-Break"><span class="koboSpan" id="kobo.601.1">painting: </span></span><a href="https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal"><span class="No-Break"><span class="koboSpan" id="kobo.602.1">https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/04_Image_and_Multimodal</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.603.1">.</span></span></p>
<h4><span class="koboSpan" id="kobo.604.1">Mask prompting</span></h4>
<p><strong class="bold"><span class="koboSpan" id="kobo.605.1">Mask prompting</span></strong><span class="koboSpan" id="kobo.606.1"> involves</span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.607.1"> masking images through the use of textual prompts. </span><span class="koboSpan" id="kobo.607.2">These textual prompts serve as a guide to the model to comprehend the desired masking area within </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">an image.</span></span></p>
<p><span class="koboSpan" id="kobo.609.1">The benefit of using mask prompting as opposed to image masking lies in the dynamic nature of mask prompting. </span><span class="koboSpan" id="kobo.609.2">You can effortlessly modify the masking by simply altering the textual prompt, allowing for rapid iteration and experimentation. </span><span class="koboSpan" id="kobo.609.3">This flexibility allows artists, designers, and content creators to explore a vast array of visual concepts and narratives without being constrained by traditional image </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">editing tools.</span></span></p>
<p><span class="koboSpan" id="kobo.611.1">In addition, mask prompting can be seamlessly integrated into various workflows and applications, enabling seamless collaboration and enhancing productivity. </span><span class="koboSpan" id="kobo.611.2">For instance, in the field of visual storytelling, writers and directors can leverage this feature to conceptualize and refine their vision, while designers can quickly prototype and iterate on visual concepts before committing to a </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">final design.</span></span></p>
<p><span class="koboSpan" id="kobo.613.1">To ensure the integrity and originality of the content generated, Amazon Bedrock has implemented robust measures to safeguard against plagiarism and unethical practices. </span><span class="koboSpan" id="kobo.613.2">We will discuss ethical considerations and safeguards in the </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">upcoming section.</span></span></p>
<p><span class="koboSpan" id="kobo.615.1">Let’s take the same </span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.616.1">example from the preceding figure. </span><span class="koboSpan" id="kobo.616.2">Instead of image masking, we want to apply a mask prompt. </span><span class="koboSpan" id="kobo.616.3">We’ll say that you want to remove the bowl of chips from the original image. </span><span class="koboSpan" id="kobo.616.4">With mask prompting, you can provide the </span><strong class="source-inline"><span class="koboSpan" id="kobo.617.1">only bowl of chips</span></strong><span class="koboSpan" id="kobo.618.1"> prompt and then further </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">perform painting.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<span class="koboSpan" id="kobo.620.1"><img alt="Figure 9.15 – Mask prompting" src="image/B22045_09_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.621.1">Figure 9.15 – Mask prompting</span></p>
<p><span class="koboSpan" id="kobo.622.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.623.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.624.1">.15</span></em><span class="koboSpan" id="kobo.625.1">, we performed inpainting and changed the bowl of chips to a bowl of apple slices. </span><span class="koboSpan" id="kobo.625.2">Let us discuss painting in </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">more detail.</span></span></p>
<h3><span class="koboSpan" id="kobo.627.1">Painting</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.628.1">Painting</span></strong><span class="koboSpan" id="kobo.629.1"> is a</span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.630.1"> technique whereby you can fill in the masked regions </span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.631.1">within an image or extend it using an image generation model. </span><span class="koboSpan" id="kobo.631.2">There are two methods of painting: inpainting </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">and outpainting.</span></span></p>
<h4><span class="koboSpan" id="kobo.633.1">Inpainting</span></h4>
<p><span class="koboSpan" id="kobo.634.1">With </span><strong class="bold"><span class="koboSpan" id="kobo.635.1">inpainting</span></strong><span class="koboSpan" id="kobo.636.1">, you are</span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.637.1"> essentially reconstructing or filling in missing, masked, or </span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.638.1">corrupted portions of the image. </span><span class="koboSpan" id="kobo.638.2">This technique is particularly useful in scenarios where an image has been damaged or obscured, or where it contains unwanted elements that need to be removed or replaced. </span><span class="koboSpan" id="kobo.638.3">By providing the image generation model with the surrounding context and effective prompts, it can intelligently generate and blend new content in the designated areas, seamlessly </span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.639.1">reconstructing the </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">masked regions.</span></span></p>
<p><span class="koboSpan" id="kobo.641.1">Let us look at </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">some </span></span><span class="No-Break"><a id="_idIndexMarker809"/></span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">examples:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.644.1">Inpainting removal</span></strong><span class="koboSpan" id="kobo.645.1">: Suppose that you want to remove the masked object within an image. </span><span class="koboSpan" id="kobo.645.2">In that case, you can perform inpainting removal. </span><span class="koboSpan" id="kobo.645.3">Let’s take an example of a beautiful scenic landscape as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.646.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.647.1">.16</span></em><span class="koboSpan" id="kobo.648.1"> and say that you want to remove the telephone line from the original image. </span><span class="koboSpan" id="kobo.648.2">You can specify the mask prompt as </span><strong class="source-inline"><span class="koboSpan" id="kobo.649.1">telephone line</span></strong><span class="koboSpan" id="kobo.650.1"> and provide an empty prompt </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">text (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">""</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">).</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer142">
<span class="koboSpan" id="kobo.654.1"><img alt="Figure 9.16 – Inpainting removal" src="image/B22045_09_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.655.1">Figure 9.16 – Inpainting removal</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.656.1">Inpainting replacement</span></strong><span class="koboSpan" id="kobo.657.1">: Suppose that you want to replace any object or scene within the image. </span><span class="koboSpan" id="kobo.657.2">In that case, you can perform inpainting replacement. </span><span class="koboSpan" id="kobo.657.3">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.658.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.659.1">.17</span></em><span class="koboSpan" id="kobo.660.1">, you can</span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.661.1"> specify within the prompt text what you want </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">to replace.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer143">
<span class="koboSpan" id="kobo.663.1"><img alt="Figure 9.17 – Inpainting replacement" src="image/B22045_09_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.664.1">Figure 9.17 – Inpainting replacement</span></p>
<h4><span class="koboSpan" id="kobo.665.1">Outpainting</span></h4>
<p><strong class="bold"><span class="koboSpan" id="kobo.666.1">Outpainting</span></strong><span class="koboSpan" id="kobo.667.1"> is the </span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.668.1">process of extending the image beyond its original boundaries, or</span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.669.1"> in other words painting outside the masked regions. </span><span class="koboSpan" id="kobo.669.2">Outpainting is useful in scenarios where the original image or artwork needs to be extended or augmented with additional elements, environments, </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">or perspectives.</span></span></p>
<p><span class="koboSpan" id="kobo.671.1">Let us look at </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">an example.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer144">
<span class="koboSpan" id="kobo.673.1"><img alt="Figure 9.18 – Outpainting" src="image/B22045_09_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.674.1">Figure 9.18 – Outpainting</span></p>
<p><span class="koboSpan" id="kobo.675.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.676.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.677.1">.18</span></em><span class="koboSpan" id="kobo.678.1">, you can see that we are painting outside the masked image or prompt (in this case, </span><strong class="source-inline"><span class="koboSpan" id="kobo.679.1">Indian curry</span></strong><span class="koboSpan" id="kobo.680.1">) to add some details. </span><span class="koboSpan" id="kobo.680.2">These include the background of a wooden table, as well as adding a spoon, a knife, a side of rice, and </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">a chai.</span></span></p>
<p><span class="koboSpan" id="kobo.682.1">If you want </span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.683.1">to understand the prompt engineering best practices for the Titan Image Generator model, please check </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">out </span></span><a href="https://tinyurl.com/titan-image-generator-prompt"><span class="No-Break"><span class="koboSpan" id="kobo.685.1">https://tinyurl.com/titan-image-generator-prompt</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.686.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.687.1">Now that we have looked at different patterns of multimodal and image patterns, let us look at the ethical considerations and available safeguards within Amazon Bedrock to ensure the responsible use of </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">Generative AI.</span></span></p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.689.1">Ethical considerations and safeguards</span></h1>
<p><span class="koboSpan" id="kobo.690.1">Generative </span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.691.1">AI models, particularly those capable of generating highly realistic images, raise significant ethical concerns regarding the potential spread of misinformation and deepfakes. </span><span class="koboSpan" id="kobo.691.2">As these models are becoming increasingly powerful and accessible, it is crucial to address these ethical challenges proactively to promote the responsible development and deployment of </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">this technology.</span></span></p>
<p><span class="koboSpan" id="kobo.693.1">One of the primary ethical concerns surrounding image generation models is the risk of creating and disseminating misleading or manipulated content. </span><span class="koboSpan" id="kobo.693.2">With the ability to generate photorealistic images from text prompts comes the potential for malicious actors to create and spread false or fabricated visual information. </span><span class="koboSpan" id="kobo.693.3">This can have far-reaching consequences, such as undermining trust in media, spreading disinformation, and even influencing political or </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">social narratives.</span></span></p>
<p><span class="koboSpan" id="kobo.695.1">To address this major ethical challenge, it is crucial for organizations and researchers to prioritize responsible development and deployment of a generative AI life cycle. </span><span class="koboSpan" id="kobo.695.2">When using Amazon Bedrock, users can utilize its </span><strong class="bold"><span class="koboSpan" id="kobo.696.1">watermark detection</span></strong><span class="koboSpan" id="kobo.697.1"> feature</span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.698.1"> for images generated by Amazon Titan </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">Image Generator.</span></span></p>
<p><span class="koboSpan" id="kobo.700.1">The watermark detection capability in Amazon Bedrock is designed to promote transparency and accountability in the use of AI-generated images. </span><span class="koboSpan" id="kobo.700.2">By embedding an invisible watermark in every image created by the model, content creators, news organizations, risk analysts, and others can quickly verify whether an image has been generated using Amazon Titan </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">Image Generator.</span></span></p>
<p><span class="koboSpan" id="kobo.702.1">This approach serves two primary </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">ethical purposes:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.704.1">It helps combat the spread of misinformation and deepfakes by providing a mechanism to verify the authenticity of images. </span><span class="koboSpan" id="kobo.704.2">This can help build trust and credibility in visual content, particularly in domains where the integrity of information is critical, such as journalism, law enforcement, and </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">scientific research.</span></span></li>
<li><span class="koboSpan" id="kobo.706.1">The watermark detection feature promotes transparency and accountability in the use of image generation models. </span><span class="koboSpan" id="kobo.706.2">By making it easier to identify AI-generated content, it encourages responsible and ethical practices among content creators and stakeholders, fostering a more open dialogue around the use of </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">this technology.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.708.1">To try out</span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.709.1"> watermark detection, you can simply </span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.710.1">navigate to </span><strong class="bold"><span class="koboSpan" id="kobo.711.1">Watermark detection</span></strong><span class="koboSpan" id="kobo.712.1"> in the Amazon Bedrock console and upload an image. </span><span class="koboSpan" id="kobo.712.2">Amazon Bedrock then analyzes the image to detect watermarks embedded by </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">Amazon Titan.</span></span></p>
<p><span class="koboSpan" id="kobo.714.1">In addition to detecting the watermark, you will also receive a confidence score, which determines the level of confidence (or certainty) with which the model is able to identify that the image was generated by Amazon Titan. </span><span class="koboSpan" id="kobo.714.2">Usually, you will see a high confidence score when there has been little to no modification in the image. </span><span class="koboSpan" id="kobo.714.3">However, if you make some modifications to the generated image, you might see a lower </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">confidence score.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<span class="koboSpan" id="kobo.716.1"><img alt="Figure 9.19 – Watermark detection" src="image/B22045_09_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.717.1">Figure 9.19 – Watermark detection</span></p>
<p><span class="koboSpan" id="kobo.718.1">As shown</span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.719.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.720.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.721.1">.19</span></em><span class="koboSpan" id="kobo.722.1">, we have</span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.723.1"> uploaded the image generated by Amazon Titan. </span><span class="koboSpan" id="kobo.723.2">The watermark detection feature is able to analyze and detect the watermark generated </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">by Titan.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<span class="koboSpan" id="kobo.725.1"><img alt="Figure 9.20 – The watermark is not detected" src="image/B22045_09_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.726.1">Figure 9.20 – The watermark is not detected</span></p>
<p><span class="koboSpan" id="kobo.727.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.728.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.729.1">.20</span></em><span class="koboSpan" id="kobo.730.1">, we have uploaded the image generated by Stable Diffusion. </span><span class="koboSpan" id="kobo.730.2">We can see that the watermark is </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">not detected.</span></span></p>
<p><span class="koboSpan" id="kobo.732.1">If you want to</span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.733.1"> try using API, you can call </span><strong class="source-inline"><span class="koboSpan" id="kobo.734.1">DetectGeneratedContent</span></strong><span class="koboSpan" id="kobo.735.1"> to verify whether the </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">watermark exists:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.737.1">
import boto3
import json
import base64
bedrock_runtime = boto3.client(service_name="bedrock-runtime")
image_path = "landscape.png"
with open(image_path, "rb") as image_file:
    input_landscape = image_file.read()
response = bedrock_runtime.detect_generated_content(
    foundationModelId = "amazon.titan-image-generator-v1",
    content = {
        "imageContent": { "bytes": input_landscape }
    }
)</span></pre>
<p><span class="koboSpan" id="kobo.738.1">Here is how</span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.739.1"> the response </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">should look:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.741.1">
response.get("detectionResult")
'GENERATED'
response.get("confidenceLevel")
'HIGH'</span></pre>
<p><span class="koboSpan" id="kobo.742.1">Here is the demo on watermark </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">detection: </span></span><a href="https://www.youtube.com/watch?v=M5Vqb3UoXtc"><span class="No-Break"><span class="koboSpan" id="kobo.744.1">https://www.youtube.com/watch?v=M5Vqb3UoXtc</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.745.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.746.1">While watermark detection is not a solution for all ethical concerns, it is one of the ways to move in the right direction. </span><span class="koboSpan" id="kobo.746.2">We will have a deeper discussion on ethical and responsible AI in </span><a href="B22045_11.xhtml#_idTextAnchor207"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.747.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.748.1"> of this book. </span><span class="koboSpan" id="kobo.748.2">You should now be able to understand image generation and design patterns with </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">Amazon Bedrock.</span></span></p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.750.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.751.1">In this chapter, we explored how image generation works. </span><span class="koboSpan" id="kobo.751.2">We also discussed the workings of multimodal models within Amazon Bedrock. </span><span class="koboSpan" id="kobo.751.3">We also covered several real-world applications and multimodal design patterns, including text-to-image, image search, image understanding, and image-to-image patterns such as inpainting and outpainting. </span><span class="koboSpan" id="kobo.751.4">We ended the chapter with a brief look at ethical considerations, as well as a look into the watermark detection capability within Amazon Bedrock. </span><span class="koboSpan" id="kobo.751.5">Throughout the chapter, we gained a deeper understanding of how we can leverage Amazon Bedrock’s multimodal models to build applications that can generate, understand, and manipulate images based on text and </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">image prompts.</span></span></p>
<p><span class="koboSpan" id="kobo.753.1">In the next chapter, we will explore the topic of building intelligent agents with </span><span class="No-Break"><span class="koboSpan" id="kobo.754.1">Amazon Bedrock.</span></span></p>
</div>
</body></html>