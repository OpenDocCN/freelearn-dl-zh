["```py\npyenv install 3.11.8 \n```", "```py\npyenv versions \n```", "```py\n# * system\n#   3.11.8 \n```", "```py\npyenv global 3.11.8 \n```", "```py\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git \ncd LLM-Engineers-Handbook \n```", "```py\npython --version \n```", "```py\n# Python 3.11.8 \n```", "```py\n[tool.poetry.dependencies]\npython = \"^3.11\"\nrequests = \"^2.25.1\"\nnumpy = \"^1.19.5\"\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\" \n```", "```py\npoetry install --without aws \n```", "```py\n[tool.poe.tasks]\ntest = \"pytest\"\nformat = \"black .\"\nstart = \"python main.py\" \n```", "```py\npoetry poe test\npoetry poe format\npoetry poe start \n```", "```py\npoetry self add 'poethepoet[poetry_plugin]' \n```", "```py\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.gitcd LLM-Engineers-Handbook\npoetry install --without aws\npoetry self add 'poethepoet[poetry_plugin]' \n```", "```py\nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\n@pipeline\ndef digital_data_etl(user_full_name: str, links: list[str]) -> None:\n    user = get_or_create_user(user_full_name)\n    crawl_links(user=user, links=links) \n```", "```py\ndefined the get_or_create_user() step, which works just like a normal Python function but is decorated with @step. We won’t go into the details of the logic, as we will cover the ETL logic in *Chapter 3*. For now, we will focus only on the ZenML functionality.\n```", "```py\nfrom loguru import logger\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument\n@step\ndef get_or_create_user(user_full_name: str) -> Annotated[UserDocument, \"user\"]:\n    logger.info(f\"Getting or creating user: {user_full_name}\")\n    first_name, last_name = utils.split_user_full_name(user_full_name)\n    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)\n    return user \n```", "```py\nhelpful for dataset discovery across your business and projects:\n```", "```py\n… # More imports\nfrom zenml import ArtifactConfig, get_step_context, step\n@step\ndef generate_intruction_dataset(\n    prompts: Annotated[dict[DataCategory, list[GenerateDatasetSamplesPrompt]], \"prompts\"]) -> Annotated[\n    InstructTrainTestSplit,\n    ArtifactConfig(\n        name=\"instruct_datasets\",\n        tags=[\"dataset\", \"instruct\", \"cleaned\"],\n    ),\n]:\n    datasets = … # Generate datasets\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"instruct_datasets\", metadata=_get_metadata_instruct_dataset(datasets))\n    return datasets\ndef _get_metadata_instruct_dataset(datasets: InstructTrainTestSplit) -> dict[str, Any]:\n    instruct_dataset_categories = list(datasets.train.keys())\n    train_num_samples = {\n        category: instruct_dataset.num_samples for category, instruct_dataset in datasets.train.items()\n    }\n    test_num_samples = {category: instruct_dataset.num_samples for category, instruct_dataset in datasets.test.items()}\n    return {\n        \"data_categories\": instruct_dataset_categories,\n        \"test_split_size\": datasets.test_split_size,\n        \"train_num_samples_per_category\": train_num_samples,\n        \"test_num_samples_per_category\": test_num_samples,\n    } \n```", "```py\nfrom zenml.client import Client\nartifact = Client().get_artifact_version('8bba35c4-8ff9-4d8f-a039-08046efc9fdc')\nloaded_artifact = artifact.load() \n```", "```py\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_maxime_labonne.yaml \n```", "```py\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml \n```", "```py\npoetry poe run-digital-data-etl-maxime\npoetry poe run-digital-data-etl-paul \n```", "```py\nconfig_path = root_dir / \"configs\" / etl_config_filename\nassert config_path.exists(), f\"Config file not found: { config_path }\"\nrun_args_etl = {\n \"config_path\": config_path,\n \"run_name\": f\"digital_data_etl_run_{dt.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n}\n digital_data_etl.with_options()(**run_args_etl) \n```", "```py\nparameters:\n  user_full_name: Maxime Labonne # [First Name(s)] [Last Name]\n  links:\n    # Personal Blog\n    - https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html\n    - https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_Agentic_Data_Generation.html\n    # Substack\n    - https://maximelabonne.substack.com/p/uncensor-any-llm-with-abliteration-d30148b7d43e\n    … # More links \n```", "```py\n@pipeline\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str: \n```", "```py\naws_access_key_id = <your_access_key_id>\naws_secret_access_key = <your_secret_access_key> \n```", "```py\n[default]\naws_access_key_id = *************\naws_secret_access_key = ************\nregion = eu-central-1\noutput = json \n```", "```py\nAWS_REGION=\"eu-central-1\" # Change it with your AWS region. By default, we use \"eu-central-1\".\nAWS_ACCESS_KEY=\"<your_aws_access_key>\"\nAWS_SECRET_KEY=\"<your_aws_secret_key>\" \n```"]