- en: '20'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chain-of-Thought Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Chain-of-thought** (**CoT**) **prompting** originated from a research paper
    titled *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*,
    published by Google researchers Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
    Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou in 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: The key innovation of CoT prompting was encouraging language models to break
    down complex reasoning problems into intermediate steps before arriving at a final
    answer. This was done by including demonstrations where the model is shown examples
    of step-by-step reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: The researchers demonstrated that by prompting LLMs with a few examples of reasoning
    chains (such as “Let’s think step by step”), the models could significantly improve
    their performance on complex tasks requiring multi-step reasoning, such as arithmetic,
    commonsense, and symbolic reasoning problems.
  prefs: []
  type: TYPE_NORMAL
- en: Before CoT, most prompting techniques focused on getting direct answers. CoT
    showed that explicitly encouraging models to demonstrate their reasoning process
    led to more accurate results, especially for problems requiring several logical
    steps. CoT is beneficial in promoting transparency and ensuring accuracy by guiding
    the model through logical steps, whereas direct answering, while quicker, can
    miss intermediate steps that could clarify or validate the reasoning behind the
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: This research was particularly significant because it showed that reasoning
    abilities could emerge primarily through scale and prompting rather than requiring
    architectural changes to the models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn to leverage CoT prompting to improve your LLM’s
    performance on complex reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing effective CoT prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CoT prompting for problem solving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining CoT prompting with other techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating CoT prompting outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations of CoT prompting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing effective CoT prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process of creating effective CoT prompts helps in fostering clarity, logical
    progression, and structured reasoning, which in turn ensures more accurate and
    coherent outputs. By providing a well-defined problem statement, breaking the
    task into smaller steps, using explicit markers to guide the reasoning, and including
    a sample CoT response, the model is better equipped to follow a systematic approach
    that aligns with human problem-solving methods, leading to clear and rational
    conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Provide a clear problem statement**: A precise problem statement directs
    the reasoning toward a specific goal, eliminating ambiguity and ensuring that
    the model understands exactly what is being asked. This helps prevent misinterpretations
    and guides the entire reasoning process in the right direction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Break down the problem into logical steps**: Dividing a complex task into
    smaller, manageable steps helps in organizing the reasoning and makes the overall
    problem easier to tackle. This breakdown aids in focusing on one aspect at a time,
    promoting clarity and reducing the risk of missing important details.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Use explicit reasoning markers**: Markers such as “First,” “Next,” and “Finally”
    act as signposts for the logical flow of the reasoning process. They help structure
    the thought process in a clear sequence, ensuring that each part of the problem
    is addressed in the right order, which increases the overall coherence of the
    response.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Include a sample CoT response in the prompt**: Providing an example helps
    establish a standard for the reasoning format and sets clear expectations for
    the process. It also serves as a reference point, guiding the model in how to
    structure its response and making it easier to generate consistent and logically
    sound outputs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s an example of implementing a CoT prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This function generates a CoT prompt for a given problem (`If a train travels
    120 km in 2 hours, what is its average speed in km/h?`), providing a structure
    for step-by-step reasoning. Here are the sample steps using CoT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, the answer is 60 km/h.
  prefs: []
  type: TYPE_NORMAL
- en: CoT prompting can be applied to various problem-solving scenarios. Let’s see
    one such scenario next.
  prefs: []
  type: TYPE_NORMAL
- en: Using CoT prompting for problem solving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s implement a function that uses CoT for mathematical word problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This function applies CoT prompting to solve a mathematical word problem (for
    example, `If a recipe calls for 2 cups of flour for 8 servings, how many cups
    of flour are needed for 12 servings?`), guiding the LLM through a step-by-step
    reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to using CoT prompting for problem solving, we can also combine
    it with other techniques to improve LLM performance.
  prefs: []
  type: TYPE_NORMAL
- en: Combining CoT prompting with other techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CoT can be combined with other prompting techniques to further enhance LLM
    performance. Let’s implement a function that combines CoT with **few-shot** **learning**
    (**FSL**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This function combines FSL with CoT prompting, providing examples of step-by-step
    solutions to guide the LLM in solving a new problem (see the code example for
    `If a train travels 180 km in 3 hours, what is its average speed in km/h?`). Combining
    methods such as CoT + FSL has been shown to improve performance in recent benchmarks
    ([https://aclanthology.org/2023.emnlp-main.782.pdf](https://aclanthology.org/2023.emnlp-main.782.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see how we can evaluate the quality of CoT prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating CoT prompting outputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluating the outputs of CoT prompts involves assessing both the final answer
    and the reasoning process. Let’s implement a simple evaluation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This evaluation function assesses both the correctness of the final answer and
    the quality of the reasoning steps in the CoT output.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of CoT prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While CoT prompting is powerful, it has some limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: High token usage and computation time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential for error propagation in multi-step reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependence on the quality of the initial prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May not be suitable for all types of problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address some of these limitations, consider implementing a dynamic CoT approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `dynamic_cot` function implements a dynamic CoT approach to break down and
    solve a problem step by step using a language model. It starts by creating an
    initial prompt that introduces the problem and instructs the model to solve it
    incrementally. The function then enters a loop, iterating up to `max_steps` times
    (default is `5`), where in each iteration, it feeds the model a growing prompt
    that includes all the steps generated so far. The model processes this prompt,
    generates the next step in the reasoning process, and appends it to the prompt.
    The new step is decoded from tokenized outputs and added to the prompt string.
    The function checks for the phrase `Therefore, the final answer is` in the generated
    step, signaling that the model has reached a conclusion and should stop. If this
    phrase is found, the loop breaks early; otherwise, it continues until the maximum
    steps are reached. Finally, the function returns the complete prompt, which includes
    all the reasoning steps leading to the solution. However, in real-world use, token
    limitations of the model may impact long multi-step prompts. As the prompt grows
    with each new step, it might exceed the model’s maximum token limit, which could
    result in truncated inputs, loss of earlier context, or failure to generate accurate
    steps, especially in complex or lengthy problems. This is a significant consideration
    when dealing with problems requiring many steps or substantial context.
  prefs: []
  type: TYPE_NORMAL
- en: Future directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As CoT prompting continues to evolve, several promising directions emerge:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adaptive CoT**: Dynamically adjusting the reasoning process based on problem
    complexity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-modal CoT**: Incorporating visual or auditory information in the reasoning
    process ([https://arxiv.org/abs/2302.00923](https://arxiv.org/abs/2302.00923))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative CoT**: Combining insights from multiple LLMs or human-AI collaboration
    ([https://arxiv.org/html/2409.07355v1](https://arxiv.org/html/2409.07355v1))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meta-learning for CoT**: Meta-learning and CoT approaches have emerged as
    powerful techniques for addressing the challenges of few-shot relation extraction
    ([https://arxiv.org/abs/2311.05922](https://arxiv.org/abs/2311.05922))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a conceptual implementation of adaptive CoT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This adaptive CoT approach assesses problem complexity and chooses an appropriate
    solving strategy, balancing efficiency and reasoning depth.
  prefs: []
  type: TYPE_NORMAL
- en: The `adaptive_cot` function adapts the CoT approach based on the complexity
    of the problem. It first assesses the problem’s complexity by calling the `assess_problem_complexity`
    function, which could involve analyzing keywords, sentence structure, or other
    features to determine how complex the problem is (though the logic for this is
    yet to be implemented). If the complexity score exceeds a predefined threshold
    (`complexity_threshold`), the function uses a detailed CoT approach via the `detailed_cot`
    function, which would generate a more elaborate, step-by-step solution. For simpler
    problems, it uses a straightforward solving method via the `simple_solve` function,
    which provides a direct answer without breaking down the problem into multiple
    steps. The result is returned based on which approach is deemed appropriate for
    the given problem. This dynamic approach allows the model to choose the most efficient
    method of solving a problem based on its complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to design effective CoT prompts that guide
    LLMs through step-by-step reasoning processes. We covered applications of this
    technique in various problem-solving scenarios and discussed how to combine it
    with other prompting strategies. You also learned how to evaluate the quality
    of CoT outputs and understood the limitations of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: By implementing the strategies and considerations discussed in this chapter,
    you can significantly improve your LLM’s performance on complex problem-solving
    tasks, while also gaining insights into the model’s reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will investigate **tree-of-thoughts** (**ToT**) prompting,
    an advanced technique that extends the concepts of CoT to create even more sophisticated
    reasoning structures.
  prefs: []
  type: TYPE_NORMAL
