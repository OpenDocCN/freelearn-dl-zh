<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer057">
			<h1 id="_idParaDest-326" class="chapter-number"><a id="_idTextAnchor400"/>29</h1>
			<h1 id="_idParaDest-327"><a id="_idTextAnchor401"/>Evaluating RAG Systems</h1>
			<p>RAG systems strive to produce more accurate, relevant, and factually grounded responses. However, evaluating the performance of these systems presents unique challenges. Unlike traditional information <a id="_idIndexMarker1309"/>retrieval or <strong class="bold">question-answering</strong> (<strong class="bold">QA</strong>) systems, RAG evaluation must consider both the quality of the retrieved information and the effectiveness of the LLM in utilizing that information to generate a <span class="No-Break">high-quality response.</span></p>
			<p>In this chapter, we’ll explore the intricacies of evaluating RAG systems. We’ll examine the challenges inherent in this task, dissect the key metrics used to assess retrieval quality and generation performance, and discuss various strategies for conducting <span class="No-Break">comprehensive evaluations.</span></p>
			<p>This chapter aims to provide you with a thorough understanding of the principles and practices of RAG evaluation, equipping you with the knowledge you’ll need to assess and improve these <span class="No-Break">powerful systems.</span></p>
			<p>In this chapter, we will be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Challenges in evaluating RAG systems <span class="No-Break">for LLMs</span></li>
				<li>Metrics for assessing retrieval quality in <span class="No-Break">LLM-based RAG</span></li>
				<li>Considerations for retrieval metrics <span class="No-Break">in RAG</span></li>
				<li>Evaluating the relevance of retrieved information <span class="No-Break">for LLMs</span></li>
				<li>Measuring the impact of retrieval on <span class="No-Break">LLM generation</span></li>
				<li>End-to-end evaluation of RAG systems <span class="No-Break">in LLMs</span></li>
				<li>Human evaluation techniques for <span class="No-Break">LLM-based RAG</span></li>
				<li>Benchmarks and datasets for <span class="No-Break">RAG evaluation</span></li>
			</ul>
			<h1 id="_idParaDest-328"><a id="_idTextAnchor402"/>Challenges in evaluating RAG systems for LLMs</h1>
			<p>Evaluating RAG systems<a id="_idIndexMarker1310"/> presents a unique set of challenges that distinguish it from evaluating traditional information retrieval or QA systems. These challenges stem from the interplay between the retrieval and generation components and the need to assess both the factual accuracy and the quality of the <span class="No-Break">generated text.</span></p>
			<p>The following sections will detail the specific challenges that are encountered when evaluating RAG systems <span class="No-Break">for LLMs<a id="_idTextAnchor403"/>.</span></p>
			<h2 id="_idParaDest-329"><a id="_idTextAnchor404"/>The interplay between retrieval and generation</h2>
			<p>The performance of a RAG system is a product of both its retrieval component and its generation component. Strong retrieval can provide the LLM with relevant and accurate information, leading to a better-generated response. Conversely, poor retrieval can mislead the LLM, resulting in an inaccurate or irrelevant answer, even if the generator itself is highly capable. Therefore, evaluating a RAG system requires assessing not only the quality of the retrieved information but also how effectively the LLM utilizes that information in its <span class="No-Break">generation process<a id="_idTextAnchor405"/>.</span></p>
			<h2 id="_idParaDest-330"><a id="_idTextAnchor406"/>Context-sensitive evaluation</h2>
			<p>Unlike traditional information retrieval, where relevance is often assessed based on the query alone, RAG evaluation must consider the context in which the retrieved information is used. A document might be relevant to the query in isolation but not provide the specific information needed to answer the question accurately in the context of the generated response. This necessitates context-sensitive evaluation metrics that consider both the query and the generated text when assessing the relevance of <span class="No-Break">retrieved documents<a id="_idTextAnchor407"/>.</span></p>
			<h2 id="_idParaDest-331"><a id="_idTextAnchor408"/>Beyond factual accuracy</h2>
			<p>While factual accuracy is a primary concern in RAG evaluation, it is not the only factor that determines the quality of a generated response. The response must also be fluent, coherent, and relevant to the user’s query. These aspects of text quality are typically assessed through human evaluation, which can be expensive and time-consuming. Developing automated metrics that correlate well with human judgments of these qualitative aspects remains an open <span class="No-Break">research challenge<a id="_idTextAnchor409"/>.</span></p>
			<h2 id="_idParaDest-332"><a id="_idTextAnchor410"/>Limitations of automated metrics</h2>
			<p>Automated metrics, such as those borrowed from information retrieval (e.g., precision, recall) or machine translation (e.g., BLEU, ROUGE), can provide useful insights into RAG system performance. However, they often fall short of capturing the full picture. Retrieval metrics might not fully reflect the usefulness of a document for generation, while generation metrics might not adequately assess the factual grounding of the generated text in the <span class="No-Break">retrieved context<a id="_idTextAnchor411"/>.</span></p>
			<h2 id="_idParaDest-333"><a id="_idTextAnchor412"/>Difficulty in error analysis</h2>
			<p>When a RAG system produces an incorrect or low-quality response, it can be challenging to pinpoint the root cause. Was the retrieval component unable to find relevant documents? Did the LLM fail to utilize the retrieved information properly? Did the LLM hallucinate or generate a response that is not grounded in the provided context? Disentangling these<a id="_idIndexMarker1311"/> factors requires careful error analysis and potentially the development of new <span class="No-Break">diagnostic tool<a id="_idTextAnchor413"/>s.</span></p>
			<h2 id="_idParaDest-334"><a id="_idTextAnchor414"/>The need for diverse evaluation scenarios</h2>
			<p>RAG systems can be deployed in a wide range of applications, from open-domain QA to domain-specific chatbots. The specific challenges and evaluation criteria may vary, depending on the use case. Evaluating a RAG system’s performance across diverse scenarios and domains is crucial for understanding its strengths <span class="No-Break">and weaknesse<a id="_idTextAnchor415"/>s.</span></p>
			<h2 id="_idParaDest-335"><a id="_idTextAnchor416"/>Dynamic knowledge and evolving information</h2>
			<p>In many real-world applications, the underlying knowledge base is constantly evolving. New information is added, and existing information is updated or becomes outdated. Evaluating how well a RAG system adapts to these changes and maintains the accuracy of its responses over time is a <span class="No-Break">significant challeng<a id="_idTextAnchor417"/>e.</span></p>
			<h2 id="_idParaDest-336"><a id="_idTextAnchor418"/>Computational cost</h2>
			<p>Evaluating RAG systems, especially those that use bigger LLMs, can be computationally expensive. Running inference with large models and performing human evaluations on a large scale can require significant resources. Finding ways to balance evaluation thoroughness with computational cost is an <span class="No-Break">important consideration.</span></p>
			<p>Let’s take a look at<a id="_idIndexMarker1312"/> some key metrics for evaluating the retrieval component in LLM-based RAG systems while focusing on relevance and utility for <span class="No-Break">response generati<a id="_idTextAnchor419"/>on.</span></p>
			<h1 id="_idParaDest-337"><a id="_idTextAnchor420"/>Metrics for assessing retrieval quality in LLM-based RAG</h1>
			<p>The retrieval <a id="_idIndexMarker1313"/>component plays a crucial role in the overall performance of a RAG system. It is responsible for providing the LLM with relevant and accurate information that serves as the basis for generating a response. Therefore, assessing the quality of the retrieval component is a vital aspect of RAG evaluation. We can adapt traditional information retrieval metrics to the RAG setting, focusing on the ability of the retriever to find documents that are not only relevant to the query but also useful for the LLM to generate a <span class="No-Break">high-quality ans<a id="_idTextAnchor421"/>wer.</span></p>
			<h2 id="_idParaDest-338"><a id="_idTextAnchor422"/>Recall@k</h2>
			<p>Recall@k measures <a id="_idIndexMarker1314"/>the <a id="_idIndexMarker1315"/>proportion of relevant documents that are successfully retrieved within the top <em class="italic">k</em> results. In the context of RAG, we can define a relevant document as one that contains the necessary information to answer the <span class="No-Break">query accurately:</span></p>
			<ul>
				<li><strong class="bold">Formula</strong>: <em class="italic">Recall@k = (Number of relevant documents retrieved in top k) / (Total number of </em><span class="No-Break"><em class="italic">relevant documents)</em></span></li>
				<li><strong class="bold">Interpretation</strong>: A higher Recall@k indicates that the retrieval component can find a larger proportion of the <span class="No-Break">relevant documents</span></li>
				<li><strong class="bold">Example</strong>: If five documents in the entire corpus contain information needed to answer a specific query, and the RAG system retrieves three of them within the top 10 results, then<a id="_idIndexMarker1316"/> Recall@10 for<a id="_idIndexMarker1317"/> that query would be 3<a id="_idTextAnchor423"/>/5 = <span class="No-Break">0.6</span></li>
			</ul>
			<h2 id="_idParaDest-339"><a id="_idTextAnchor424"/>Precision@k</h2>
			<p>Precision@k measures the<a id="_idIndexMarker1318"/> proportion <a id="_idIndexMarker1319"/>of retrieved documents within the top <em class="italic">k</em> results that <span class="No-Break">are relevant:</span></p>
			<ul>
				<li><strong class="bold">Formula</strong>: <em class="italic">Precision@k = (Number of relevant documents retrieved in top k) / (</em><span class="No-Break"><em class="italic">k)</em></span></li>
				<li><strong class="bold">Interpretation</strong>: A higher Precision@k indicates that a larger proportion of the retrieved documents <span class="No-Break">are relevant</span></li>
				<li><strong class="bold">Example</strong>: If a RAG system retrieves 10 documents for a query, and four of them are relevant, then Precision@10 would be <a id="_idTextAnchor425"/>4/10 = <span class="No-Break">0.4</span></li>
			</ul>
			<h2 id="_idParaDest-340"><a id="_idTextAnchor426"/>Mean Reciprocal Rank (MRR)</h2>
			<p>MRR considers the rank <a id="_idIndexMarker1320"/>of the first relevant<a id="_idIndexMarker1321"/> document retrieved. It emphasizes the importance of retrieving relevant documents early in <span class="No-Break">the ranking:</span></p>
			<ul>
				<li><strong class="bold">Formula</strong>: <em class="italic">MRR = (1 / |Q|) * Σ (1 / rank_i) for i = 1 to |Q|</em>, where <em class="italic">|Q|</em> is the number of queries and <em class="italic">rank_i</em> is the rank of the first relevant document for <span class="No-Break">query </span><span class="No-Break"><em class="italic">i</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Interpretation</strong>: A higher MRR indicates that relevant documents are retrieved at higher ranks (closer to <span class="No-Break">the top).</span></li>
				<li><strong class="bold">Example</strong>: If the first relevant document for a query is retrieved at rank three, the reciprocal rank is 1/3. MRR averages these reciprocal ranks across <span class="No-Break">multi<a id="_idTextAnchor427"/>ple queries.</span></li>
			</ul>
			<h2 id="_idParaDest-341"><a id="_idTextAnchor428"/>Normalized Discounted Cumulative Gain (NDCG@k)</h2>
			<p>NDCG@k is a more<a id="_idIndexMarker1322"/> sophisticated<a id="_idIndexMarker1323"/> metric that considers both the relevance of retrieved documents and their position in the ranking. It uses a graded relevance scale (e.g., 0, 1, 2, where 2 is highly relevant) and assigns higher scores to relevant documents retrieved at <span class="No-Break">higher ranks:</span></p>
			<ul>
				<li><strong class="bold">Formula</strong>: NDCG@k involves calculating the <strong class="bold">Discounted Cumulative Gain</strong> (<strong class="bold">DCG</strong>) of the<a id="_idIndexMarker1324"/> retrieved list and normalizing it by <a id="_idIndexMarker1325"/>the <strong class="bold">Ideal Discounted Cumulative Gain</strong> (<strong class="bold">IDCG</strong>), which is the DCG of the perfectly ranked list. The formula is complex but can be easily computed using libraries such <span class="No-Break">as sklearn.</span></li>
				<li><strong class="bold">Interpretation</strong>: A higher NDCG@k indicates that highly relevant documents are retrieved at <span class="No-Break">higher ranks.</span></li>
			</ul>
			<p>Next, let’s discuss how to decide which retrieval<a id="_idTextAnchor429"/> metrics <span class="No-Break">to use.</span></p>
			<h1 id="_idParaDest-342"><a id="_idTextAnchor430"/>Considerations for retrieval metrics in RAG</h1>
			<p>In the context of RAG, we<a id="_idIndexMarker1326"/> need to define relevance carefully. A document might be relevant to the query but not contain the specific information needed to answer it accurately. We might need to use a stricter definition of relevance, such as “contains the answer to the query.” As mentioned earlier, relevance in RAG is often context-sensitive. A document might be relevant to the query in isolation but not be the most helpful document for generating a specific answer given the other <span class="No-Break">retrieved documents.</span></p>
			<p>While metrics such as Recall@k and Precision@k focus on the top <em class="italic">k</em> retrieved documents, it’s also important to consider the overall quality of the retrieval across a wider range of results. Metrics such<a id="_idIndexMarker1327"/> as <strong class="bold">Average Precision</strong> (<strong class="bold">AP</strong>) can provide a m<a id="_idTextAnchor431"/>ore <span class="No-Break">holistic view.</span></p>
			<p>Let’s illustrate how to calculate Recall@k, Precision@k, MRR, and NDCG@k in Python using the <span class="No-Break">sklearn library:</span></p>
			<ol>
				<li>We first import the necessary libraries and define sample data representing a set of queries, the ground truth relevant documents for each query, and the documents that are <a id="_idIndexMarker1328"/>retrieved by a RAG system for <span class="No-Break">each query:</span><pre class="source-code">
import numpy as np
from sklearn.metrics import ndcg_score
# Sample data
queries = [
    "What is the capital of France?",
    "Who painted the Mona Lisa?",
    "What is the highest mountain in the world?"
]
ground_truth = [
    [0, 1, 2],  # Indices of relevant documents for query 1
    [3, 4],     # Indices of relevant documents for query 2
    [5, 6, 7]   # Indices of relevant documents for query 3
]
retrieved = [
    [1, 5, 0, 2, 8, 9, 3, 4, 6, 7],  # Ranked list of retrieved document indices for query 1
    [4, 3, 0, 1, 2, 5, 6, 7, 8, 9],  # Ranked list of retrieved document indices for query 2
    [6, 5, 7, 0, 1, 2, 3, 4, 8, 9]   # Ranked list of retrieved document indices for query 3
]</pre></li>				<li>We then define a function, <strong class="source-inline">calculate_recall_at_k</strong>, to calculate Recall@k for a given set of queries, ground truth relevant documents, and retrieved<a id="_idIndexMarker1329"/> <span class="No-Break">document lists:</span><pre class="source-code">
def calculate_recall_at_k(ground_truth, retrieved, k):
    """Calculates Recall@k for a set of queries."""
    recall_scores = []
    for gt, ret in zip(ground_truth, retrieved):
        num_relevant = len(gt)
        retrieved_k = ret[:k]
        num_relevant_retrieved = len(
            set(gt).intersection(set(retrieved_k))
        )
        recall = (
            num_relevant_retrieved / num_relevant
            if num_relevant &gt; 0 else 0
        )
        recall_scores.append(recall)
    return np.mean(recall_scores)</pre></li>				<li>We next define a function, <strong class="source-inline">calculate_precision_at_k</strong>, to calculate Precision@k for a given set of queries, ground truth, and <span class="No-Break">retrieved lists:</span><pre class="source-code">
def calculate_precision_at_k(ground_truth, retrieved, k):
    """Calculates Precision@k for a set of queries."""
    precision_scores = []
    for gt, ret in zip(ground_truth, retrieved):
        retrieved_k = ret[:k]
        num_relevant_retrieved = len(
            set(gt).intersection(set(retrieved_k))
        )
        precision = num_relevant_retrieved / k if k &gt; 0 else 0
        precision_scores.append(precision)
    return np.mean(precision_scores)</pre></li>				<li>We define a function, <strong class="source-inline">calculate_mrr</strong>, to calculate the MRR for a given set of queries, ground truth, and retrieved lists. A higher MRR indicates that the system consistently retrieves relevant documents at <span class="No-Break">higher ranks:</span><pre class="source-code">
def calculate_mrr(ground_truth, retrieved):
    """Calculates Mean Reciprocal Rank (MRR) for a set of queries."""
    mrr_scores = []
    for gt, ret in zip(ground_truth, retrieved):
        for i, doc_id in enumerate(ret):
            if doc_id in gt:
                mrr_scores.append(1 / (i + 1))
                break
        else:
            mrr_scores.append(0)  # No relevant document found
    return np.mean(mrr_scores)</pre></li>				<li>We also <a id="_idIndexMarker1330"/>define a function, <strong class="source-inline">calculate_ndcg_at_k</strong>, to calculate NDCG@k. We’ll use a simplified version here where relevance scores are binary (0 <span class="No-Break">or 1):</span><pre class="source-code">
def calculate_ndcg_at_k(ground_truth, retrieved, k):
    """Calculates NDCG@k for a set of queries."""
    ndcg_scores = []
    for gt, ret in zip(ground_truth, retrieved):
        relevance_scores = np.zeros(len(ret))
        for i, doc_id in enumerate(ret):
            if doc_id in gt:
                relevance_scores[i] = 1
        # sklearn.metrics.ndcg_score requires 2D array
        true_relevance = np.array([relevance_scores])
        retrieved_relevance = np.array([relevance_scores])
        ndcg = ndcg_score(
            true_relevance, retrieved_relevance, k=k
        )
        ndcg_scores.append(ndcg)
    return np.mean(ndcg_scores)</pre></li>				<li>Finally, we<a id="_idIndexMarker1331"/> calculate and print the retrieval metrics for different values <span class="No-Break">of </span><span class="No-Break"><em class="italic">k</em></span><span class="No-Break">:</span><pre class="source-code">
k_values = [1, 3, 5, 10]
for k in k_values:
    recall_at_k = calculate_recall_at_k(ground_truth,
        retrieved, k)
    precision_at_k = calculate_precision_at_k(
        ground_truth, retrieved, k
    )
    ndcg_at_k = calculate_ndcg_at_k(ground_truth, retrieved, k)
    print(f"Recall@{k}: {recall_at_k:.3f}")
    print(f"Precision@{k}: {precision_at_k:.3f}")
    print(f"NDCG@{k}: {ndcg_at_k:.3f}")
mrr = calculate_mrr(ground_truth, retrieved)
pr<a id="_idTextAnchor432"/>int(f"MRR: {mrr:.3f}")</pre></li>			</ol>
			<h1 id="_idParaDest-343"><a id="_idTextAnchor433"/>Evaluating the relevance of retrieved information for LLMs</h1>
			<p>While the retrieval <a id="_idIndexMarker1332"/>metrics discussed in the previous section provide a general assessment of retrieval quality, they do not fully capture the nuances of relevance in the context of RAG. In RAG, the retrieved information is not the end product but rather an intermediate step that serves as input to an LLM. Therefore, we need to evaluate the relevance of the retrieved information not just to the query but also to the specific task of generating a high-qualit<a id="_idTextAnchor434"/>y response via <span class="No-Break">the LLM.</span></p>
			<p>Traditional information retrieval often focuses on finding documents that are topically relevant to the query. However, in RAG, we need a more nuanced notion of relevance that considers the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Answerability</strong>: Does the retrieved information contain the specific information needed to answer the query accurately? A document might be generally relevant to the query but not contain the <span class="No-Break">precise answer.</span></li>
				<li><strong class="bold">Contextual utility</strong>: Is the retrieved information useful in the context of the other retrieved documents? A document might be relevant in isolation but redundant or even contradictory when combined with other <span class="No-Break">retrieved information.</span></li>
				<li><strong class="bold">LLM compatibility</strong>: Is the retrieved information in a format that the LLM can easily understand and utilize? For example, a long and complex document might be relevant but difficult for the LLM to <span class="No-Break">process effectively.</span></li>
				<li><strong class="bold">Faithfulness support</strong>: Does the retrieved information provide sufficient evidence to support the claims made in the generated answer? This is crucial for ensuring that the <a id="_idIndexMarker1333"/>LLM’s response is grounded in the <span class="No-Break">retrieved context.</span></li>
			</ul>
			<h2 id="_idParaDest-344"><a id="_idTextAnchor435"/>Methods for evaluating the relevance of retrieved information</h2>
			<p>Here are some <a id="_idIndexMarker1334"/>methods for evaluating the relevance of retrieved information that involve moving beyond tra<a id="_idTextAnchor436"/>ditional <span class="No-Break">query relevance:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Human evaluation</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Direct assessment</strong>: Human annotators can directly assess the relevance of retrieved documents to the query and the generated response. They can be asked to rate the relevance on a Likert scale (e.g., 1 to 5) or to provide binary judgments (<span class="No-Break">relevant/not relevant).</span></li><li><strong class="bold">Comparative evaluation</strong>: Annotators can be presented with multiple sets of retrieved documents and asked to rank them based on their usefulness for answering the query or to choose the <span class="No-Break">best set.</span></li><li><strong class="bold">Task-based evaluation</strong>: Annotators can be asked to use the retrieved documents to answer the query themselves. The accuracy and quality of their answers can serve as an indirect measure of the relevance and utility of the <span class="No-Break">retrieved information.</span></li></ul></li>
				<li><strong class="bold">Automated metrics</strong>: Let’s consider some commonly used automated metrics. Keep in mind that while automated metrics provide a quantitative measure of performance, human evaluation offers valuable qualitative insights into the relevance, coherence, and usefulness of the <span class="No-Break">generated responses:</span><ul><li><strong class="bold">Answer overlap</strong>: We can automatically measure the overlap between the generated answer and the retrieved documents using metrics such as ROUGE or BLEU. Higher overlap suggests that the LLM is utilizing the <span class="No-Break">retrieved information.</span></li><li><strong class="bold">QA metrics</strong>: If we have ground truth answers, we can treat the retrieved context as the input to a QA system and evaluate its performance using standard QA metrics<a id="_idIndexMarker1335"/> such as <strong class="bold">Exact Match</strong> (<strong class="bold">EM</strong>) and <span class="No-Break">F1 score.</span></li><li><strong class="bold">Faithfulness metrics</strong>: We can use techniques such as <strong class="bold">Natural Language Inference</strong> (<strong class="bold">NLI</strong>) to<a id="_idIndexMarker1336"/> assess whether the generated answer is entailed by the retrieved context. We’ll discuss NLI models in detail in later sections of <span class="No-Break">this chapter.</span></li><li><strong class="bold">Perplexity</strong>: We can measure the perplexity of the LLM when it’s conditioned on the retrieved context. Lower perplexity suggests that the LLM finds the context informative<a id="_idTextAnchor437"/> and useful <span class="No-Break">for generation.</span></li></ul></li>
			</ul>
			<p>As an example, let’s illustrate how to implement simple answer overlap metrics using the <strong class="source-inline">rouge-score</strong> library<a id="_idIndexMarker1337"/> <span class="No-Break">in Python:</span></p>
			<ol>
				<li>First, we run the following command to install the <strong class="source-inline">rouge-score</strong> library, which provides implementations of ROUGE metrics, and import the <span class="No-Break">necessary modules:</span><pre class="source-code">
  <strong class="bold">   pip install rouge-score</strong>
<strong class="bold">      from rouge_score import rouge_scorer</strong></pre></li>				<li>Then, we define sample data representing a query, a generated answer, and a list of <span class="No-Break">retrieved documents:</span><pre class="source-code">
query = "What is the capital of France?"
answer = "The capital of France is Paris."
retrieved_documents = [
    "Paris is the capital city of France.",
    "France is a country in Europe.",
    "The Eiffel Tower is a famous landmark in Paris.",
    "London is the capital of the United Kingdom."
]</pre></li>				<li>We next define a function, <strong class="source-inline">calculate_rouge_scores</strong>, to calculate ROUGE scores <a id="_idIndexMarker1338"/>between the generated answer and each <span class="No-Break">retrieved document:</span><pre class="source-code">
     def calculate_rouge_scores(answer, documents):
    """Calculates ROUGE scores between the answer and each document."""
    scorer = rouge_scorer.RougeScorer(
        ['rouge1', 'rouge2', 'rougeL'],
        use_stemmer=True
    )
    scores = []
    for doc in documents:
        score = scorer.score(answer, doc)
        scores.append(score)
    return scores</pre></li>				<li>We then calculate and print the ROUGE scores for <span class="No-Break">each document:</span><pre class="source-code">
rouge_scores = calculate_rouge_scores(answer,
    retrieved_documents)
for i, score in enumerate(rouge_scores):
    print(f"Document {i+1}:")
    print(f"  ROUGE-1: {score['rouge1'].fmeasure:.3f}")
    print(f"  ROUGE-2: {score['rouge2'].fmeasure:.3f}")
    print(f"  ROUGE-L: {score['rougeL'].fmeasure:.3f}")</pre></li>				<li>Finally, we <a id="_idIndexMarker1339"/>calculate and print the average ROUGE scores across <span class="No-Break">all documents:</span><pre class="source-code">
avg_rouge1 = sum([score['rouge1'].fmeasure
    for score in rouge_scores]) / len(rouge_scores)
avg_rouge2 = sum([score['rouge2'].fmeasure
    for score in rouge_scores]) / len(rouge_scores)
avg_rougeL = sum([score['rougeL'].fmeasure
    for score in rouge_scores]) / len(rouge_scores)
print(f"\nAverage ROUGE Scores:")
print(f"  Average ROUGE-1: {avg_rouge1:.3f}")
print(f"  Average ROUGE-2: {avg_rouge2:.3f}")
print(f"  Average ROUGE-L: {avg_rougeL:.3f}")</pre></li>			</ol>
			<h2 id="_idParaDest-345"><a id="_idTextAnchor438"/>Challenges in evaluating RAG-specific relevance</h2>
			<p>Having explored <a id="_idIndexMarker1340"/>several <a id="_idIndexMarker1341"/>methods for evaluating the relevance of retrieved information, we now turn to outlining some of the key challenges involved in this <span class="No-Break">evaluation process:</span></p>
			<ul>
				<li><strong class="bold">Subjectivity</strong>: Relevance judgments can be subjective, especially when considering factors such as contextual utility and <span class="No-Break">LLM compatibility</span></li>
				<li><strong class="bold">Annotation cost</strong>: Human evaluation can be expensive and time-consuming, especially for <span class="No-Break">large-scale evaluations</span></li>
				<li><strong class="bold">Metric limitations</strong>: Automated metrics might not fully capture the nuances of RAG-specific relevance and might not always correlate well with <span class="No-Break">human judgments</span></li>
				<li><strong class="bold">Dynamic contexts</strong>: The relevance of a document can change depending on the other documents that are retrieved and the specific generation strategy used by <span class="No-Break">the LLM</span></li>
			</ul>
			<p>Next, let’s learn<a id="_idIndexMarker1342"/> how to<a id="_idIndexMarker1343"/> measure the impac<a id="_idTextAnchor439"/>t of retrieval on <span class="No-Break">LLM generation.</span></p>
			<h1 id="_idParaDest-346"><a id="_idTextAnchor440"/>Measuring the impact of retrieval on LLM generation</h1>
			<p>In RAG systems, the <a id="_idIndexMarker1344"/>quality of the generated response is heavily influenced by the information that’s retrieved. Good retrieval provides the necessary context and facts, while poor retrieval can lead to irrelevant or incorrect responses. Enhancing retrieval through better models and filtering improves overall performance, which is measured by precision, faithfulness, and <span class="No-Break">user satisfaction.</span></p>
			<p>Therefore, a crucial aspect of evaluation involves measuring the impact of retrieval on LLM generation. Let’s check out some of the key metrics <span class="No-Break">and techniques.</span></p>
			<h2 id="_idParaDest-347"><a id="_idTextAnchor441"/>Key metrics for evaluating retrieval impact</h2>
			<p>As mentioned, the quality <a id="_idIndexMarker1345"/>of a response generated by an LLM is closely tied to the information it retrieves. Therefore, evaluating the impact of retrieval on the final response is crucial. This involves assessing how effectively the LLM leverages the retrieved context to generate answers that are accurate, relevant, and well-grounded. Let’s now examine some of the k<a id="_idTextAnchor442"/>ey metrics used in <span class="No-Break">this evaluation:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Groundedness/faithfulness</strong></span><span class="No-Break">:</span><p class="list-inset">Groundedness, also known as faithfulness, measures the extent to which the generated response is factually supported by the retrieved context. A grounded response should only contain information that can be inferred from the <span class="No-Break">provided documents.</span></p><p class="list-inset">Some of the techniques for evaluating this metric are <span class="No-Break">as follows:</span></p><ul><li><strong class="bold">Human evaluation</strong>: Human annotators can directly assess the groundedness of each statement in the generated response by verifying whether it is supported by the retrieved context. This can involve binary judgments (grounded/not grounded) or more <span class="No-Break">fine-grained ratings.</span></li><li><span class="No-Break"><strong class="bold">Automated metrics</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">NLI</strong>: NLI models can be used to determine whether each sentence in the generated response is entailed by the retrieved context. We treat the concatenation of retrieved documents as the premise and each sentence in the response as the hypothesis. A high entailment score suggests that the sentence is grounded in <span class="No-Break">the context.</span></li><li><strong class="bold">QA-based</strong>: We can formulate questions based on the generated response and check whether a QA model can answer them correctly using the retrieved context as the source of information. A high answerability score indicates that the response <span class="No-Break">is grounded.</span></li><li><strong class="bold">Fact verification models</strong>: These models can be used to check whether each<a id="_idIndexMarker1346"/> fact stated in the generated response is supported by the retrieved documents or external <span class="No-Break">knowledge sources.</span></li></ul></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Answer relevance</strong></span><span class="No-Break">:</span><p class="list-inset">Answer relevance measures how well the generated response addresses the user’s query, given the retrieved context. Even if the retrieved context is imperfect, a good RAG system should still strive to provide a relevant and <span class="No-Break">helpful answer.</span></p><p class="list-inset">Some of the techniques for evaluating this metric are <span class="No-Break">as follows:</span></p><ul><li><strong class="bold">Human evaluation</strong>: Human judges can assess the relevance of the generated response to the query while taking into account the limitations of the retrieved context. They can rate relevance on a Likert scale or provide comparative judgments (e.g., ranking <span class="No-Break">multiple responses).</span></li><li><span class="No-Break"><strong class="bold">Automated metrics</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Query-answer similarity</strong>: We can measure the semantic similarity between the query and the generated response using embedding-based techniques (e.g., cosine similarity) or other <span class="No-Break">similarity metrics.</span></li><li><strong class="bold">Task-specific metrics</strong>: Depending on the specific application, we can use task-specific metrics. For example, in a QA scenario, we can measure the overlap between the generated answer and a gold-standard answer using metrics such as EM or <span class="No-Break">F1 score.</span></li><li><strong class="bold">Information retrieval metrics</strong>: We can treat the generated response as a retrieved document and evaluate its relevance to the query using traditional IR metrics such as precision, recall, or NDCG, assuming we have relevance judgments for <span class="No-Break">query-answer pairs.</span></li></ul></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Context utilization</strong></span><span class="No-Break">:</span><p class="list-inset">This aspect focuses <a id="_idIndexMarker1347"/>on how effectively the LLM utilizes the retrieved context when generating the response. It goes beyond just measuring groundedness and assesses whether the LLM is integrating and synthesizing information from the <span class="No-Break">context appropriately.</span></p><p class="list-inset">Some of the techniques for evaluating this metric are <span class="No-Break">as follows:</span></p><ul><li><strong class="bold">Human evaluation</strong>: Human annotators can assess the extent to which the LLM is using the retrieved context, identifying instances where the model is underutilizing or over-relying on <span class="No-Break">the context</span></li><li><span class="No-Break"><strong class="bold">Automated metrics</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Attribution analysis</strong>: We can use techniques such as attention visualization or gradient-based attribution to identify which parts of the retrieved context the LLM is paying the most attention to <span class="No-Break">during generation.</span></li><li><strong class="bold">Context ablation</strong>: We can measure the change in the generated response when portions of the context are removed or modified. This can help with determining which parts<a id="_idTextAnchor443"/> of the context are <span class="No-Break">most influential.</span></li></ul></li></ul></li>
			</ul>
			<p>As an example, let’s carry out a groundedness evaluation using an NLI model. For this example, we’ll use the <span class="No-Break">Transformers library:</span></p>
			<ol>
				<li>We first run the following command, which installs the <strong class="source-inline">transformers</strong> library. This library provides tools for working with pre-trained transformer models such as <a id="_idIndexMarker1348"/>NLI. We also import the <span class="No-Break">necessary modules:</span><pre class="source-code">
<strong class="bold">pip install transformers torch</strong>
<strong class="bold">from transformers import (</strong>
<strong class="bold">    </strong><strong class="bold">AutoTokenizer, AutoModelForSequenceClassification</strong>
<strong class="bold">)</strong>
<strong class="bold">import torch</strong></pre></li>				<li>We then define sample data representing a query, a generated answer, and the <span class="No-Break">retrieved context:</span><pre class="source-code">
 query = "What is the capital of France?"
answer = "The capital of France is Paris. It is a global center for art, fashion, gastronomy, and culture."
context = """
Paris is the capital city of France. It is situated on the River Seine, in northern France.
Paris has an area of 105 square kilometers and a population of over 2 million people.
France is a country located in Western Europe.
"""</pre></li>				<li>We load a pre-trained NLI model and its corresponding tokenizer. Here, we’re using the <strong class="source-inline">roberta-large-mnli</strong> model, which is a RoBERTa model that’s been fine-tuned on the <span class="No-Break">MultiNLI dataset:</span><pre class="source-code">
model_name = "roberta-large-mnli"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = \
    AutoModelForSequenceClassification.from_
pretrained(
    model_name
)</pre></li>				<li>We then define a function, <strong class="source-inline">calculate_claim_groundedness</strong>, that calculates the entailment score for a single claim (a sentence from the generated answer) given <a id="_idIndexMarker1349"/><span class="No-Break">the context:</span><pre class="source-code">
def calculate_claim_groundedness(context, claim):
    """Calculates the entailment score for a single claim given the context."""
    inputs = tokenizer(context, claim, truncation=True,
        return_tensors="pt")
    outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=1)
    entailment_prob = probs[0][2].item()  # Assuming label 2 corresponds to entailment
    return entailment_prob</pre></li>				<li>We also define a function, <strong class="source-inline">calculate_groundedness</strong>, to calculate the overall groundedness score for the entire generated answer. It splits the answer into sentences, calculates the entailment score for each sentence, and then averages <span class="No-Break">the scores:</span><pre class="source-code">
def calculate_groundedness(context, answer):
    """Calculates the overall groundedness score for the generated answer."""
    claims = answer.split(". ")  # Simple sentence splitting
    if not claims:
        return 0
    claim_scores = []
    for claim in claims:
        if claim:
          score = calculate_claim_groundedness(context, claim)
          claim_scores.append(score)
    return (
        sum(claim_scores) / len(claim_scores)
        if claim_scores
        else 0
    )</pre></li>				<li>Finally, we <a id="_idIndexMarker1350"/>calculate and print the overall groundedness score for the <span class="No-Break">sample data:</span><pre class="source-code">
groundedness_score = calculate_groundedness(context, answer)
print(f"Ground<a id="_idTextAnchor444"/>edness Score: {groundedness_score:.3f}")</pre></li>			</ol>
			<h2 id="_idParaDest-348"><a id="_idTextAnchor445"/>Challenges in measuring the impact of retrieval</h2>
			<p>Now that we’ve<a id="_idIndexMarker1351"/> seen an example, let’s take a look at some of the key challenges that are encountered in the evaluation process of <span class="No-Break">RAG systems:</span></p>
			<ul>
				<li><strong class="bold">Defining the ground truth</strong>: Determining the ground truth for groundedness and answer relevance can be challenging and subjective, especially when dealing with complex or <span class="No-Break">nuanced queries</span></li>
				<li><strong class="bold">Attributing errors</strong>: It can be difficult to determine whether an error in the generated response is due to poor retrieval, limitations of the LLM, or a combination <span class="No-Break">of both</span></li>
				<li><strong class="bold">Computational cost</strong>: Evaluating the impact of retrieval on generation can be computationally expensive, especially when using bigger LLMs or performing <span class="No-Break">human evaluations</span></li>
				<li><strong class="bold">Inter-annotator agreement</strong>: When using human evaluation, ensuring high inter-annotator agreement on subjective judgments such as groundedness and relevance can <span class="No-Break">be difficult</span></li>
			</ul>
			<p>While evaluating the individual components of a RAG system (retrieval and generation) is important, it is <a id="_idIndexMarker1352"/>also crucial to assess the system’s overall performance in <a id="_idTextAnchor446"/>an end-to-end manner. Let’s see <span class="No-Break">that next.</span></p>
			<h1 id="_idParaDest-349"><a id="_idTextAnchor447"/>End-to-end evaluation of RAG systems in LLMs</h1>
			<p>While evaluating the<a id="_idIndexMarker1353"/> individual components of a RAG system (retrieval and generation) is important, it is also crucial to assess the system’s overall performance in an end-to-end manner. End-to-end evaluation considers the entire RAG pipeline, from the initial user query to the final generated response, providing a h<a id="_idTextAnchor448"/>olistic view of the <span class="No-Break">system’s effectiveness.</span></p>
			<p>Let’s take a look at some <span class="No-Break">holistic metrics:</span></p>
			<ul>
				<li><strong class="bold">Task success</strong>: For task-oriented RAG systems (e.g., QA, dialogue), we can measure the overall task success rate. This involves determining whether the generated response completes the intended <span class="No-Break">task successfully.</span><p class="list-inset">Here are some techniques for evaluating <span class="No-Break">this metric:</span></p><ul><li><strong class="bold">Automated evaluation</strong>: For some tasks, we can automatically evaluate task success. For example, in QA, we can check whether the generated answer matches the <span class="No-Break">gold-standard answer.</span></li><li><strong class="bold">Human evaluation</strong>: For more complex tasks, human evaluation might be necessary to judge whether the RAG system successfully achieved the <span class="No-Break">task’s goal.</span></li></ul></li>
				<li><strong class="bold">Answer quality</strong>: This metric assesses the overall quality of the generated response while considering factors such as accuracy, relevance, fluency, coherence, <span class="No-Break">and groundedness.</span><p class="list-inset">Here are some techniques for evaluating <span class="No-Break">this metric:</span></p><ul><li><strong class="bold">Human evaluation</strong>: Human judges can rate the overall quality of the generated response on a Likert scale or by using more detailed rubrics that consider multiple <span class="No-Break">quality dimensions</span></li><li><strong class="bold">Automated metrics</strong>: While answer quality can be challenging to fully automate, some aspects of answer quality can be approximated using metrics such as <span class="No-Break">the following:</span><ul><li><strong class="bold">ROUGE/BLEU</strong>: Measures the overlap between the generated response and a reference answer (<span class="No-Break">if available)</span></li><li><strong class="bold">Perplexity</strong>: Measures how well the LLM predicts the generated response (lower perplexity is <span class="No-Break">generally better)</span></li><li><strong class="bold">Groundedness metrics (NLI, QA-based)</strong>: Assesses the factual consistency of the response with the <span class="No-Break">retrieved context</span></li><li><strong class="bold">Relevance metrics</strong>: Measures the similarity between the query and the <span class="No-Break">generated </span><span class="No-Break"><a id="_idIndexMarker1354"/></span><span class="No-Break">response</span></li></ul></li></ul></li>
			</ul>
			<p>Now, let’s loo<a id="_idTextAnchor449"/>k at some ways we can evaluate <span class="No-Break">RAG systems.</span></p>
			<h2 id="_idParaDest-350"><a id="_idTextAnchor450"/>Evaluation strategies</h2>
			<p>Evaluation strategies for<a id="_idIndexMarker1355"/> RAG systems can be broadly categorized into black-box evaluation, glass-box evaluation, component-wise evaluation, and ablation studies, each offering distinct insights into <span class="No-Break">system performance.</span></p>
			<p>In black-box evaluation, the entire RAG system is treated as a single unit. Evaluators provide input queries and assess only the final generated responses without analyzing the intermediate retrieval or generation steps. This approach is particularly useful for measuring overall system performance and comparing different RAG implementations without having to delve into their <span class="No-Break">internal mechanisms.</span></p>
			<p>Glass-box evaluation, in contrast, involves a detailed examination of the internal workings of the RAG system. This method analyzes the retrieved context, the LLM’s attention patterns, and the intermediate generation steps. By dissecting these elements, glass-box evaluation helps identify system strengths and weaknesses, pinpoint error sources, and provide insights for <span class="No-Break">targeted improvements.</span></p>
			<p>A more granular approach is component-wise evaluation, which assesses the retrieval and generation components separately. Retrieval performance is typically measured using metrics such as Recall@k and NDCG, while the quality of the generated text is evaluated using metrics such as BLEU and ROUGE or through human judgment based on a fixed set of retrieved documents. This method is particularly effective in isolating and diagnosing performance issues within <span class="No-Break">individual components.</span></p>
			<p>Finally, ablation studies offer a systematic way to measure the impact of different components on overall system effectiveness. By removing or modifying specific parts of the RAG system – such as testing performance with and without retrieval or swapping different retrieval and generation models – researchers can better understand how each component<a id="_idIndexMarker1356"/> contributes to t<a id="_idTextAnchor451"/>he system’s functionality and <span class="No-Break">overall success.</span></p>
			<h2 id="_idParaDest-351"><a id="_idTextAnchor452"/>Challenges in end-to-end evaluation</h2>
			<p>Evaluating RAG systems <a id="_idIndexMarker1357"/>holistically presents several challenges, particularly when assessing complex interactions between retrieval and generation components. Some of these challenges are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Defining the ground truth</strong>: For open-ended tasks or tasks that involve generating complex responses, defining the ground truth can be difficult or <span class="No-Break">even impossible</span></li>
				<li><strong class="bold">Attributing errors</strong>: When the system generates an incorrect or low-quality response, it can be challenging to determine whether the error originated in the retrieval or <span class="No-Break">generation component</span></li>
				<li><strong class="bold">Computational cost</strong>: End-to-end evaluation can be computationally expensive, especially when using bigger LLMs or performing human evaluations on a <span class="No-Break">large scale</span></li>
				<li><strong class="bold">Reproducibility</strong>: Ensuring reproducibility can be difficult due to the complex interactions between the retrieval and generation components and the potential use of non-deterministic retrieval mechanisms or stochastic decoding strategies during generation, which may lead to variations in outputs across runs even with the <span class="No-Break">same inputs.</span></li>
			</ul>
			<p>Next, let’s shift focus to the role of human evaluation in assessing LLM-based RAG systems, which complements automated metrics by capturing nuanced aspects such<a id="_idTextAnchor453"/> as relevance, coherence, and<a id="_idIndexMarker1358"/> <span class="No-Break">factual accuracy.</span></p>
			<h1 id="_idParaDest-352"><a id="_idTextAnchor454"/>Human evaluation techniques for LLM-based RAG</h1>
			<p>While automated metrics<a id="_idIndexMarker1359"/> provide valuable insights, human evaluation remains the gold standard for assessing the overall quality and effectiveness of RAG systems. Human judgment is particularly crucial for evaluating aspects that are difficult to capture with automated metrics, such as the nuanced relevance of retrieved information, the coherence and fluency of generated text, and the overall helpfulne<a id="_idTextAnchor455"/>ss of the response in addressing the <span class="No-Break">user’s need.</span></p>
			<p>Human evaluators can assess various aspects of RAG <span class="No-Break">system performance:</span></p>
			<ul>
				<li><strong class="bold">Relevance</strong>: How relevant<a id="_idIndexMarker1360"/> is the generated response to the user’s query? Does it address the specific information needed expressed in <span class="No-Break">the query?</span></li>
				<li><strong class="bold">Groundedness/faithfulness</strong>: Is the generated response factually supported by the retrieved context? Does it avoid hallucinating or contradicting the <span class="No-Break">provided information?</span></li>
				<li><strong class="bold">Coherence and fluency</strong>: Is the generated response well-structured, easy to understand, and written in grammatically correct and <span class="No-Break">natural-sounding language?</span></li>
				<li><strong class="bold">Helpfulness</strong>: Does the response provide a useful and satisfactory answer to the user’s query, considering the limitations of the <span class="No-Break">retrieved context?</span></li>
				<li><strong class="bold">Context utilization</strong>: How effectively does the system utilize the retrieved context in generating the response? Does it integrate and synthesize information from multiple <span class="No-Break">sources appropriately?</span></li>
				<li><strong class="bold">Attribution</strong>: Does the system provide clear citations or links to the sources in the retrieved context that <a id="_idIndexMarker1361"/>support the <span class="No-Break">generated claims?</span></li>
			</ul>
			<p>Several methods can be used for the human evaluation of <span class="No-Break">RAG systems:</span></p>
			<ul>
				<li><strong class="bold">Rating scales (Likert scales)</strong>: Annotators <a id="_idIndexMarker1362"/>rate different aspects of the generated response (e.g., relevance, groundedness, fluency) on a numerical scale, such as 1 to 5, where 1 represents poor quality and 5 represents <span class="No-Break">excellent quality:</span><ul><li><strong class="bold">Advantages</strong>: Simple to implement and easy to collect and <span class="No-Break">aggregate data</span></li><li><strong class="bold">Disadvantages</strong>: Can be subjective, susceptible to annotator bias, and may not capture <span class="No-Break">nuanced differences</span></li></ul></li>
				<li><strong class="bold">Comparative evaluation (ranking/best–worst scaling)</strong>: Annotators are presented with multiple RAG system outputs for the same query and asked to rank them based on their overall quality or <span class="No-Break">specific criteria.</span></li>
				<li><strong class="bold">Best–worst scaling</strong>: A specific form of comparative evaluation where annotators choose the best and worst options from a set <span class="No-Break">of outputs:</span><ul><li><strong class="bold">Advantages</strong>: More reliable than absolute ratings and captures relative differences between <span class="No-Break">systems effectively</span></li><li><strong class="bold">Disadvantages</strong>: More complex to implement than rating scales and requires more effort <span class="No-Break">from annotators</span></li></ul></li>
				<li><strong class="bold">Task-based evaluation</strong>: Annotators are asked to complete a specific task using the RAG system, such as finding an answer to a question, writing a summary, or engaging in a conversation. The quality of the RAG system is assessed based on the annotators’ ability to complete the task successfully and their satisfaction with the <span class="No-Break">system’s performance:</span><ul><li><strong class="bold">Advantages</strong>: More realistic and user-centered and provides a direct measure of the <span class="No-Break">system’s utility</span></li><li><strong class="bold">Disadvantages</strong>: More complex to design and implement and can be time-consuming <span class="No-Break">and expensive</span></li></ul></li>
				<li><strong class="bold">Free-form feedback</strong>: Annotators provide open-ended feedback on the strengths and weaknesses of the RAG <span class="No-Break">system’s output:</span><ul><li><strong class="bold">Advantages</strong>: Captures <a id="_idIndexMarker1363"/>detailed insights and suggestions for improvement and can uncover <span class="No-Break">unexpected issues</span></li><li><strong class="bold">Disadvantages</strong>: More difficult to analyz<a id="_idTextAnchor456"/>e and quantify and can be subjective <span class="No-Break">and inconsistent</span></li></ul></li>
			</ul>
			<h2 id="_idParaDest-353"><a id="_idTextAnchor457"/>Best practices for human evaluation</h2>
			<p>To ensure the reliability and fairness of human evaluation, consider the following <span class="No-Break">best practices:</span></p>
			<ul>
				<li><strong class="bold">Clear guidelines</strong>: Provide<a id="_idIndexMarker1364"/> annotators with clear and detailed guidelines that define the evaluation criteria and <span class="No-Break">annotation procedures</span></li>
				<li><strong class="bold">Training and calibration</strong>: Train annotators on the task and calibrate their judgments using <span class="No-Break">example annotations</span></li>
				<li><strong class="bold">Inter-annotator agreement</strong>: Measure inter-annotator agreement (e.g., using Cohen’s Kappa or Fleiss’ Kappa) to ensure the reliability of <span class="No-Break">the annotations</span></li>
				<li><strong class="bold">Pilot studies</strong>: Conduct pilot studies to refine the evaluation protocol and identify potential issues before launching a <span class="No-Break">large-scale evaluation</span></li>
				<li><strong class="bold">Multiple annotators</strong>: Use multiple annotators for each item to mitigate individual biases and improve the robustness of <span class="No-Break">the evaluation</span></li>
				<li><strong class="bold">Diverse annotator pool</strong>: Recruit a diverse pool of annotators to capture a wider range of perspectives and reduce <span class="No-Break">potential biases</span></li>
				<li><strong class="bold">Quality control</strong>: Implement mechanisms for identifying and c<a id="_idTextAnchor458"/>orrecting errors or inconsistencies in <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker1365"/></span><span class="No-Break">annotations</span></li>
			</ul>
			<h2 id="_idParaDest-354"><a id="_idTextAnchor459"/>Challenges in human evaluation</h2>
			<p>Evaluating the performance<a id="_idIndexMarker1366"/> of RAG systems built on LLMs poses a unique set of challenges. Here, we outline the key obstacles encountered in conducting reliable, consistent, and meaningful human evaluations of <span class="No-Break">such systems:</span></p>
			<ul>
				<li><strong class="bold">Cost and time</strong>: Human evaluation can be expensive and time-consuming, especially for <span class="No-Break">large-scale evaluations</span></li>
				<li><strong class="bold">Subjectivity</strong>: Human judgments can be subjective and influenced by individual preferences <span class="No-Break">and biases</span></li>
				<li><strong class="bold">Annotator training and expertise</strong>: Ensuring that annotators are properly trained and have the necessary expertise to assess RAG system performance can <span class="No-Break">be challenging</span></li>
				<li><strong class="bold">Reproducibility</strong>: Replicating human evaluations can be diff<a id="_idTextAnchor460"/>icult due to the inherent variability in <span class="No-Break">human judgments</span></li>
			</ul>
			<p>In the next section, we’ll explore the role of standardized benchmarks and datasets in evaluating RAG systems, highlight<a id="_idTextAnchor461"/>ing<a id="_idIndexMarker1367"/> key benchmarks, evaluation criteria, <span class="No-Break">and challenges.</span></p>
			<h1 id="_idParaDest-355"><a id="_idTextAnchor462"/>Benchmarks and datasets for RAG evaluation</h1>
			<p>Standardized benchmarks<a id="_idIndexMarker1368"/> and datasets play a crucial role in driving progress in RAG research and development. They provide a common ground for evaluating and comparing different RAG systems, facilitating the process of ident<a id="_idTextAnchor463"/>ifying best practices and tracking advancements <span class="No-Break">over time.</span></p>
			<p>Let’s look at some key benchmarks <span class="No-Break">and datasets:</span></p>
			<ul>
				<li><strong class="bold">Knowledge Intensive Language Tasks (KILT)</strong>: A comprehensive benchmark for <a id="_idIndexMarker1369"/>evaluating knowledge-intensive language tasks, including QA, fact-checking, dialogue, and <span class="No-Break">entity linking:</span><ul><li><strong class="bold">Data source</strong>: Based on Wikipedia, with a unified format for <span class="No-Break">all tasks</span></li><li><strong class="bold">Strengths</strong>: Provides a diverse set of tasks, allows both retrieval and generation to be evaluated, and includes a standardized <span class="No-Break">evaluation framework</span></li><li><strong class="bold">Limitations</strong>: Primarily based on Wikipedia, which might not reflect the diversity of real-world<a id="_idIndexMarker1370"/> <span class="No-Break">knowledge sources</span></li></ul></li>
				<li><strong class="bold">Natural Questions (NQ)</strong>: A large-scale QA<a id="_idIndexMarker1371"/> dataset collected from real user queries that is sent to the Google <span class="No-Break">search engine:</span><ul><li><strong class="bold">Data source</strong>: Contains pairs of questions and Wikipedia pages that contain <span class="No-Break">the answer</span></li><li><strong class="bold">Strengths</strong>: Realistic queries, large scale, and includes both short and long <span class="No-Break">answer annotations</span></li><li><strong class="bold">Limitations</strong>: Since it primarily focuses on factoid questions, it might not be suitable for evaluating more complex reasoning or <span class="No-Break">generation tasks</span></li></ul></li>
				<li><strong class="bold">TriviaQA</strong>: A challenging <a id="_idIndexMarker1372"/>QA dataset containing <span class="No-Break">question-answer-evidence triples:</span><ul><li><strong class="bold">Data source</strong>: Collected from trivia enthusiasts, it includes both web and Wikipedia <span class="No-Break">evidence documents</span></li><li><strong class="bold">Strengths</strong>: More difficult than NQ; it requires reading and understanding multiple <span class="No-Break">evidence documents</span></li><li><strong class="bold">Limitations</strong>: Primarily focused on factoid questions, the writing style of trivia questions might <a id="_idIndexMarker1373"/>not be representative of real-world <span class="No-Break">user queries</span></li></ul></li>
				<li><strong class="bold">Explain Like I’m Five (ELI5)</strong>: A dataset<a id="_idIndexMarker1374"/> of questions and answers from the Reddit forum <em class="italic">Explain Like I’m Five</em>, where users ask for simplified explanations of <span class="No-Break">complex topics:</span><ul><li><strong class="bold">Data source</strong>: Collected from Reddit, it includes questions and answers on a wide range <span class="No-Break">of topics</span></li><li><strong class="bold">Strengths</strong>: Focuses on long-form, explanatory answers, making it suitable for evaluating the generation capabilities of <span class="No-Break">RAG systems</span></li><li><strong class="bold">Limitations</strong>: The quality and accuracy of answers can vary and might require careful filtering <span class="No-Break">or annotation</span></li></ul></li>
				<li><strong class="bold">ASQA</strong>: The first long-form <a id="_idIndexMarker1375"/>QA dataset that <a id="_idIndexMarker1376"/>unifies <span class="No-Break">ambiguous questions:</span><ul><li><strong class="bold">Data source</strong>: The dataset is built from scratch by combining multiple <span class="No-Break">ambiguous questions</span></li><li><strong class="bold">Strengths</strong>: Can help evaluate long-form <span class="No-Break">QA tasks</span></li><li><strong class="bold">Limitations</strong>: Building a high-quality dataset from scratch can <span class="No-Break">be challenging</span></li></ul></li>
				<li><strong class="bold">Microsoft Machine Reading Comprehension (MS MARCO)</strong>: A large-scale dataset for <a id="_idIndexMarker1377"/>machine reading comprehension <span class="No-Break">and QA:</span><ul><li><strong class="bold">Data source</strong>: Contains real anonymized user queries that are sent to the Bing search engine, along with human-generated answers and <span class="No-Break">relevant passages.</span></li><li><strong class="bold">Strengths</strong>: It provides a large-scale, diverse set of queries and answers that includes both passage-level and <span class="No-Break">full-document annotations</span></li><li><strong class="bold">Limitations</strong>: Primarily focused on extractive QA, it might not be ideal for evaluating the<a id="_idIndexMarker1378"/> generation capabilities of <span class="No-Break">RAG systems</span></li></ul></li>
				<li><strong class="bold">Stanford Question Answering Dataset (SQuAD)</strong>: A widely used dataset for reading<a id="_idIndexMarker1379"/> comprehension consisting of questions posed by crowdworkers on a set of <span class="No-Break">Wikipedia articles:</span><ul><li><strong class="bold">Data source</strong>: Contains question-paragraph-answer triples, where the answer is a span of text in <span class="No-Break">the paragraph</span></li><li><strong class="bold">Strengths</strong>: A large-scale, well-established benchmark for <span class="No-Break">reading comprehension</span></li><li><strong class="bold">Limitations</strong>: Primarily focused on extractive QA, it might not <a id="_idTextAnchor464"/><a id="_idTextAnchor465"/>be suitable for evaluating the generation capabilities of <span class="No-Break">RAG systems</span></li></ul></li>
			</ul>
			<p>As an example, let’s illustrate how to use the KILT dataset to evaluate a RAG system. We’ll use the KILT library<a id="_idIndexMarker1380"/> in Python to <span class="No-Break">do so:</span></p>
			<ol>
				<li>Run the following code to install the kilt library and import the <span class="No-Break">necessary modules:</span><pre class="source-code">
<strong class="bold">pip install kilt==0.5.5</strong>
<strong class="bold">from kilt import kilt_utils as utils</strong>
<strong class="bold">from kilt import retrieval</strong>
<strong class="bold">from kilt.eval import answer_evaluation, provenance_evaluation</strong></pre></li>				<li>Next, download a specific KILT task, such as the Wizard of Wikipedia (<span class="No-Break">WoW) dataset:</span><pre class="source-code">
# Download the WoW dataset
utils.download_dataset("wow")</pre></li>				<li>Then, load the downloaded dataset <span class="No-Break">into memory:</span><pre class="source-code">
# Load the dataset
wow_data = utils.load_dataset("wow", split="test")</pre></li>				<li>Define a dummy RAG function that simulates the behavior of a RAG retrieval component. For demonstration purposes, it simply returns a fixed set of Wikipedia pages for each query. In a real-world scenario, you would replace this with your actual<a id="_idIndexMarker1381"/> RAG <span class="No-Break">retrieval implementation:</span><pre class="source-code">
class DummyRetriever(retrieval.base.Retriever):
    def __init__(self, k=1):
          super().__init__(num_return_docs=k)
          self.k = k
    # retrieve some Wikipedia pages (or the entire dataset)
    # based on the query
    def retrieve(self, query, start_paragraph_id=None):
        # Dummy retrieval: return the same set of pages for each query
        dummy_pages = [
            {
                "wikipedia_id": "534366",
                "start_paragraph_id": 1,
                "score": self.k,
                "text": "Paris is the capital of France."
            },
            {
                "wikipedia_id": "21854",
                "start_paragraph_id": 1,
                "score": self.k-1,
                "text": "The Mona Lisa was painted by Leonardo da Vinci."
            },
            {
                "wikipedia_id": "37267",
                "start_paragraph_id": 1,
                "score": self.k-2,
                "text": "Mount Everest is the highest mountain in the world."
            }
          ]
          return dummy_pages[:self.k]
# Example usage
retriever = DummyRetriever(k=2)</pre></li>				<li>Define a dummy RAG generation function that simulates the behavior of a RAG generation <a id="_idIndexMarker1382"/>component. For demonstration purposes, it simply returns a fixed answer for each query. In a real-world scenario, you would replace this with your actual LLM-based <span class="No-Break">generation implementation:</span><pre class="source-code">
     def dummy_generate(query, retrieved_pages):
        """Simulates RAG generation by returning 
        a fixed answer for each query."""
        if "capital of France" in query:
            return "Paris"
        elif "Mona Lisa" in query:
            return "Leonardo da Vinci"
        elif "highest mountain" in query:
            return "Mount Everest"
        else:
            return "I don't know."</pre></li>				<li>Run the dummy RAG pipeline on the dataset, using the dummy retrieval and generation functions, and collect the <span class="No-Break">generated predictions:</span><pre class="source-code">
predictions = []
for element in wow_data[:10]:
    query = element["input"]
    retrieved_pages = retriever.retrieve(query)
    # Add provenance information to the element
    element["output"] = [{"provenance": retrieved_pages}]
    generated_answer = dummy_generate(query, retrieved_pages)
    # Add the generated answer to the element
    element["output"][0]["answer"] = generated_answer
    predictions.append(element)</pre></li>				<li>Finally, evaluate <a id="_idIndexMarker1383"/>the generated predictions using the KILT evaluation functions. Both the retrieval performance (using <strong class="source-inline">provenance_evaluation</strong>) and the answer quality (using <strong class="source-inline">answer_evaluation</strong>) <span class="No-Break">are assessed:</span><pre class="source-code">
kilt_scores = {}
kilt_scores["provenance_MAP@k"] = \
    provenance_evaluation.get_map_at_k(
    predictions, verbose=False
)
kilt_scores["answer_EM"] = answer_evaluation.get_exact_match(
    predictions, verbose=False
)
kilt_scores["answer_F1"] = answer_evaluation.get_f1(
    predictions, verbose=False
)
kilt_scores["answer_ROUGE-L"] = answer_evaluation.get_rouge_l(
    predictions, verbose=False
)
print(kilt_scores)</pre></li>			</ol>
			<p>This code provides a basic example of how to use the KILT framework to evaluate a RAG system. In a real-world scenario, you would replace the dummy retrieval and generation functions with your actual RAG implementation and use a larger portion of the dataset for evaluation. You can adapt this example t<a id="_idTextAnchor466"/>o other KILT tasks by downloading and loading the <a id="_idIndexMarker1384"/><span class="No-Break">corresponding datasets.</span></p>
			<p>Here are a few things to consider when choosing benchmarks <span class="No-Break">and datasets:</span></p>
			<ul>
				<li><strong class="bold">Task alignment</strong>: Select benchmarks and datasets that align with the specific task you are evaluating (e.g., QA, <span class="No-Break">dialogue, summarization)</span></li>
				<li><strong class="bold">Knowledge domain</strong>: Consider the knowledge domain covered by the benchmark. Some benchmarks are based on general knowledge (e.g., Wikipedia), while others focus on specific domains (e.g., scientific literature, <span class="No-Break">medical records)</span></li>
				<li><strong class="bold">Retrieval setting</strong>: Choose benchmarks that are appropriate for the retrieval setting you are using (e.g., open-domain retrieval, closed-domain retrieval, passage retrieval, <span class="No-Break">document retrieval)</span></li>
				<li><strong class="bold">Generation requirements</strong>: Consider the type of generation required by the task (e.g., extractive versus abstractive, short versus <span class="No-Break">long answers)</span></li>
				<li><strong class="bold">Dataset size and quality</strong>: Ensure that the dataset is large enough to provide statistically significant results and that the data is of high quality (e.g., accurate annotations and <span class="No-Break">well-formed questions)</span></li>
				<li><strong class="bold">Evaluation metrics</strong>: Check what evaluation metrics are used by the benchmark and whether<a id="_idIndexMarker1385"/> they are appropriate for your specific <span class="No-Break">evaluation goals</span></li>
			</ul>
			<h1 id="_idParaDest-356"><a id="_idTextAnchor467"/>Summary</h1>
			<p>In this chapter, we discussed a wide range of metrics for evaluating both retrieval quality and generation performance, including traditional information retrieval metrics such as Recall@k, Precision@k, MRR, and NDCG, as well as more RAG-specific metrics such as groundedness, faithfulness, and answer relevance. We explored various techniques for measuring these metrics, including automated methods based on NLI and QA models, and human evaluation approaches using rating scales, comparative judgments, and <span class="No-Break">task-based assessments.</span></p>
			<p>We emphasized the crucial role of human evaluation in capturing the nuanced aspects of RAG performance that are difficult to assess with automated metrics alone. We also discussed best practices for designing and conducting human evaluations, such as providing clear guidelines, training annotators, measuring inter-annotator agreement, and conducting pilot studies. We need to keep in mind that tradeoffs between automated and human evaluation will be important in <span class="No-Break">real-world deployments.</span></p>
			<p>Furthermore, we explored widely used benchmarks and datasets for RAG evaluation, including KILT, NQ, TriviaQA, ELI5, ASQA, MS MARCO, and SQuAD, highlighting their strengths and limitations and providing guidance on selecting appropriate benchmarks for different tasks <span class="No-Break">and domains.</span></p>
			<p>As we conclude, it is clear that evaluating RAG systems is a complex and evolving field. The development of more sophisticated evaluation metrics, the creation of more diverse and challenging benchmarks, and the refinement of human evaluation methodologies will continue to be crucial for driving progress in RAG research <span class="No-Break">and development.</span></p>
			<p>In the next chapter, we’ll explore agentic patterns in LLMs, focusing on how LLMs can perform tasks involving reasoning, planning, and decisi<a id="_idTextAnchor468"/>on-making autonomously using advanced retrieval and <span class="No-Break">generation techniques.</span></p>
		</div>
	</div></div></body></html>