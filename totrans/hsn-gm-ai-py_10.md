# 策略梯度方法

之前，我们的**强化学习**（**RL**）方法主要集中在寻找在任何给定状态下选择特定动作的最大值或最佳值。虽然这在之前的章节中对我们来说效果不错，但这种方法当然也存在着它自己的问题，其中之一就是始终需要确定何时实际采取最大或最佳动作，这就是我们的探索/利用权衡。正如我们所看到的，最佳动作并不总是最佳选择，有时取平均最佳动作可能更好。然而，从数学上来说，平均是危险的，它并不能告诉我们代理在环境中实际采样了什么。理想情况下，我们希望有一种方法可以学习环境中每个状态的动作分布。这引入了强化学习中的一个新类别的方法，称为**策略梯度**（**PG**）方法，这也是本章的重点。

在本章中，我们将探讨PG方法以及它们如何以许多不同的方式改进我们之前的尝试。我们首先了解PG方法背后的直觉，然后转向第一个方法，REINFORCE。之后，我们将探讨优势函数类别，并介绍演员-评论家方法。从那里，我们将继续探讨**深度确定性策略梯度**方法以及它们如何用于解决月球着陆问题。然后，我们将介绍一种称为**信任区域策略优化**的高级方法，以及它是如何根据信任区域估计回报的。

以下是本章我们将重点关注的主要主题总结：

+   理解策略梯度方法

+   介绍 REINFORCE

+   使用优势演员-评论家

+   构建深度确定性策略梯度

+   探索信任区域策略优化

PG方法在数学上比我们之前的尝试要复杂得多，并且更深入地涉及到统计和概率方法。虽然我们将专注于理解这些方法的直觉，而不是数学原理，但对于一些读者来说，这仍然可能令人困惑。如果你发现这样，你可能需要复习一下统计学和概率学，这可能会有所帮助。在下一节中，我们将开始理解PG方法背后的直觉。

本章中所有代码最初都来源于这个GitHub仓库：[https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL)。原始作者出色地收集了原始资料。按照惯例，代码已经进行了重大修改，以适应本书的风格和其他代码的一致性。

# 理解策略梯度方法

我们需要了解PG方法的一点是为什么需要它们，以及它们背后的直觉是什么。然后，我们可以在深入研究代码之前简要地介绍一些数学知识。因此，让我们来探讨使用PG方法背后的动机以及它们希望实现的目标，这些目标超越了之前我们研究过的其他方法。我已经总结了为什么/PG方法做什么以及它们试图解决的问题的主要观点：

+   **确定性函数与随机函数**：我们在科学和数学的早期学习中经常了解到许多问题需要单一的或确定性答案。然而，在现实世界中，我们经常将一定程度的误差等同于确定性计算来量化其准确性。这种量化一个值准确性的方法可以通过随机或概率方法进一步发展。

随机方法通常用于量化风险或不确定性的期望，它们通过找到描述一系列值的分布来实现这一点。而之前我们使用值函数来找到描述动作的最优状态值，现在我们想要了解产生该值的分布。以下图表显示了确定性函数与随机函数输出的一个示例，分布位于均值、中位数、众数和最大值旁边：

![图片](img/5cfb41d8-97a3-488f-851d-8fea1ee8cb20.png)

偏斜的正态分布

以前，我们假设我们的智能体总是从完美的正态分布中进行采样。这个假设允许我们使用最大值甚至平均值。然而，正态分布永远不会仅仅是正常的，在大多数情况下，环境可能甚至没有接近正态分布。

+   **确定性环境与随机环境**：我们对所有事物都是正态分布的假设的另一个问题是它往往不是这样，在现实世界中，我们经常需要将环境解释为随机或随机的。我们之前的环境大部分是静态的，这意味着它们在各个剧集之间变化很小。现实世界环境永远不会完全静态，在游戏中更是如此。因此，我们需要一个能够对环境中的随机变化做出反应的算法。

+   **离散动作空间与连续动作空间**：我们已经花费了一些时间考虑离散与连续的观察空间，并学习了如何通过离散化和深度学习来处理这些环境，但现实世界环境和/或游戏并不总是离散的。也就是说，除了上、下、左、右等离散动作之外，我们现在还需要考虑左10-80%、右10-90%、上10-90%等连续动作。幸运的是，PG方法提供了一种机制，使得连续动作更容易实现。反过来，离散动作空间是可行的，但训练效果不如连续动作空间。

由于算法本身的性质，PG方法在连续动作空间中工作得更好。它们可以用来解决离散动作空间环境，但通常不会像我们后面将要介绍的其他方法表现得那么好。

既然我们已经了解了为什么需要PG方法，那么我们接下来需要在下一节中探讨如何使用它。

# 策略梯度上升

PG方法背后的基本直觉是我们从寻找描述确定性策略的价值函数转变为具有参数的随机策略，这些参数用于定义策略分布。这样思考，我们现在可以假设我们的策略函数需要被定义，以便我们的策略π可以通过调整参数θ来设置，这样我们就能理解在某个状态下采取特定动作的概率。从数学上讲，我们可以简单地定义如下：

![图片](img/00f9e2d1-8818-4ab3-a261-6c999c443811.png)

你应该考虑我们在本章中涵盖的数学知识是理解代码所需的最小知识。如果你确实认真考虑开发自己的PG方法扩展，那么你可能想要花些时间进一步探索数学，使用《强化学习导论》（Barto/Sutton，第2版，2017年）。

π表示由参数θ确定的策略，我们计划通过深度学习网络轻松地找到这些参数。现在，我们已经看到我们如何使用深度学习通过梯度下降最小化网络的损失，我们现在将问题颠倒过来。我们现在想要找到那些给出最佳动作概率的参数，对于给定的状态，这些动作应该最大化到1.0或100%。这意味着我们不再减少一个数字，我们现在需要使用梯度上升来最大化它。这也将我们的更新从价值转变为描述策略的参数，并且我们重新编写我们的更新方程如下：

![图片](img/4f8529fd-d471-4a6f-b120-93ceebfb8fbd.png)

在方程中，我们有以下内容：

+   ![图片](img/13e8748f-339a-4b34-a29a-839da749f190.png) 前一时间步的参数值

+   ![图片](img/74b55c70-2a06-4368-8361-495ce087eda0.png) 学习率

+   ![图片](img/809ad4df-d51c-4332-8e7d-db6729c249d0.png) 针对动作*a*的计算更新梯度，或最优动作

这里的直觉是我们正在推动向产生最佳策略的动作。然而，我们发现，进一步假设所有推动都是相等的同样也是错误的。毕竟，我们应该能够将那些关于价值的确定性预测重新引入前面的方程中，作为对真实价值的进一步指导。我们可以通过以下方式更新最后一个方程：

![图片](img/b4d273e3-613f-495c-926a-3a729c361454.png)

在这里，我们现在引入以下内容：

![图片](img/88a4e9d4-32e1-4fa5-8033-fddde0e88f8e.png)：这是我们对于给定状态和动作对的Q值的猜测。

因此，具有更高估计Q值的州-行动对将比那些没有的受益更多，但除了，我们现在必须退一步重新考虑我们的老朋友探索/利用困境，并考虑我们的算法/代理需要如何选择行动。我们不再希望我们的代理只采取最佳或随机行动，而是使用策略本身的学习。这意味着几件事。我们的代理现在需要不断地从同一策略中采样和学习，这意味着PG是按策略的，但也意味着我们需要更新我们的更新方程来考虑这一点，如下所示：

![图1](img/361bcbbc-97b1-4c07-8f62-21f759b2eaee.png)

在这里，我们现在引入以下内容：

![概率图](img/0ab082c0-c579-4786-a43a-f0aac23e503b.png)：这是给定状态下给定行动的概率——本质上，这是策略本身所预测的。

通过将策略在给定状态采取行动的概率进行除法，可以解释该行动可能被采取的频率。因此，如果一个行动比另一个行动流行两倍，那么它只会更新一半的次数，但可能被采取的次数是两倍。再次强调，这试图消除采样频率更高的行动的偏差，并允许算法相应地更重视那些罕见但有益的行动。

现在你已经理解了我们新的更新和过程的基本直觉，我们可以看到它在实际中的应用。在实践中实现PG方法在数学上更困难，但幸运的是，深度学习通过提供梯度上升来缓解这一点，正如我们将在下一节解决我们的第一个实际算法时看到的。

# 引入REINFORCE

我们将要查看的第一个算法被称为**REINFORCE**。它以一种非常优雅的方式引入了PG的概念，特别是在PyTorch中，它掩盖了此实现中许多数学复杂性。REINFORCE还通过反向解决优化问题来工作。也就是说，它不是使用梯度上升，而是反转数学，这样我们可以将问题表示为损失函数，从而使用梯度下降。更新方程现在转换为以下形式：

![图2](img/8d747008-3ce8-4cb7-9ede-4bb66720b5fa.png)

在这里，我们现在假设以下条件：

+   ![图3](img/4217bf51-fd01-413c-9ae3-048cbf1b2a4d.png) 这是由![优势函数图](img/68346397-7b2a-47a9-89fd-3e0e0f38d3aa.png)表示的相对于基线的优势；我们将在稍后更详细地介绍优势函数。

+   ![梯度图](img/93e9f09b-c099-4b70-8751-c51453a078ed.png) 这是现在表示为损失的梯度，并且与![损失函数图](img/0a3b256d-6a7d-44c2-a32c-60c9c7cc198b.png)等价，假设使用链式法则和对*1/x = log x*的导数。

实质上，我们使用链式法则和性质 *1/x = log x* 来翻转方程。再次强调，详细解析数学内容超出了本书的范围，但这里的关键直觉是使用对数函数作为导数技巧，将我们的方程转换为结合优势函数的损失函数。

**REINFORCE** 代表 **REward Increment = Non-negative Factor *x* Offset Reinforcement *x* Characteristic E**ligibility**。这个缩写试图描述算法本身的数学直觉，其中非负因子代表优势函数，![](img/5940afc2-e6d5-4774-bcd9-c2749e24b14e.png)。偏置强化是梯度本身，表示为 ![](img/2763d48c-1832-4d50-b259-747bfdc4cb5b.png)。然后，我们引入特征有效性，这使我们回到使用 ![](img/274a93f2-167b-4ccd-9d92-986c6638dab3.png) 学习 TD 和有效性迹的学习。通过 ![](img/93a38c62-f037-42e7-a4f0-6f8c3f1aa219.png) 或学习率来缩放整个因子，使我们能够调整算法/代理学习的速度。

能够直观地调整超参数，学习率（alpha）和折扣因子（gamma），应该是一项你已经开始掌握的技能。然而，PG 方法带来了关于代理想要/需要如何学习的不同直觉。因此，请确保花同样多的时间来理解调整这些值是如何改变的。

当然，作为游戏程序员，我们理解这一点的最好方式是与代码一起工作，这正是我们在下一个练习中将要做的。打开示例 `Chapter_8_REINFORCE.py` 并遵循这里的练习：

1.  在 PyTorch 中，REINFORCE 成为一个紧凑的算法，整个代码列表如下所示：

[PRE0]

1.  如同往常，我们从我们常用的导入开始，增加了一个来自 `torch.distributions` 的新导入，名为 `Categorical`。现在，`Categorical` 用于从连续概率空间采样我们的动作空间到离散动作值。之后，我们初始化我们的基本超参数，`learning_rate` 和 `gamma`。

1.  接下来，我们来到一个新的类 `REINFORCE`，它封装了我们的代理算法的功能。我们在 DQN 和 DDQN 配置中已经看到了大部分代码。然而，我们想要关注的是这里所示的训练函数 `train_net`。

[PRE1]

1.  `train_net` 是我们使用损失计算来推动（反向传播）策略网络中的错误的地方。注意，在这个课程中，我们不使用重放缓冲区，而是仅使用一个名为 `data` 的列表。也应该清楚，我们将列表中的所有值都反向通过网络。

1.  在类定义之后，我们跳转到创建环境和设置一些额外的变量，如下所示：

[PRE2]

1.  你可以看到我们又回到了玩月球着陆环境。其他变量与我们之前用来控制训练量和输出结果频率的变量相似。如果你将其更改为不同的环境，你很可能需要调整这些值。

1.  再次，训练迭代代码与我们之前的例子非常相似，唯一的显著区别是我们如何在环境中采样和执行动作。以下是完成这一部分的代码：

[PRE3]

1.  这里要注意的主要事情是我们正在从 REINFORCE 生成的策略中提取动作的概率，使用 `pi.act`。之后，我们使用 `Categorical` 将这个概率转换为分类或离散的值箱。然后我们使用 `m.sample()` 提取离散的动作值。这种转换对于离散动作空间，如月球着陆 v2 环境，是必要的。

之后，我们将看到如何在不需要转换的情况下将其应用于连续空间环境。如果你滚动到 `play_game` 函数，你会注意到相同的代码块用于在玩游戏时从策略中提取动作。特别注意最后一行，其中使用了 `pi.put_data` 来存储结果，并注意我们是如何在 `prop[action]` 值上使用 `torch.log` 的。记住，通过在这里使用对数函数，我们转换或反转了使用梯度上升来最大化动作值的需求。相反，我们可以使用梯度下降和 `backprop` 在我们的策略网络上。

1.  按照你通常的方式运行代码并观察结果。这个算法通常训练得很快。

这个算法的优雅性，尤其是在 PyTorch 中，将这里的复杂数学美妙地掩盖了。不幸的是，除非你理解了直觉，否则这可能不是一件好事。在下一节中，我们将探讨上一节练习中提到的优势函数，并看看这与演员-评论家方法有何关联。

# 使用优势演员-评论家

我们已经在几个前几章中讨论了几次优势的概念，包括最后一个练习。优势通常被认为是在理解将不同的代理/策略应用于相同问题之间的差异。该算法学习优势，从而提供增强奖励的好处。这有点抽象，让我们看看这如何应用于我们之前的一个算法，比如 DDQN。在 DDQN 中，优势是通过理解如何缩小移动到已知目标或目标的差距来定义的。如果你需要复习，请参考[第 7 章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)，《DDQN 深入研究》。

优势的概念可以扩展到我们所说的演员-评论家方法。在演员-评论家方法中，我们通过训练两个网络来定义优势，一个作为演员；也就是说，它对策略做出决策，另一个网络根据预期回报对这些决策进行评论。现在的目标不仅是要优化演员和评论家，而且要以减少意外情况的方式去做。你可以把意外想象成代理可能期望获得一些奖励，但结果却没有或可能获得了更多奖励的时刻。在AC方法中，目标是最小化意外，它通过使用基于价值的批评方法（DQN）作为评论家和PG（REINFORCE）方法作为演员来实现。参见以下图表，了解这是如何结合在一起的：

![图片](img/5796624f-66f6-4219-b37d-a50ad7a3ea96.png)

解释演员评论家方法

在下一节中，我们将深入探讨如何将AC应用于我们之前的PG示例。

# 演员评论家

AC方法使用网络组合来预测价值和策略函数的输出，其中我们的价值函数网络类似于DQN，我们的策略函数使用PG方法（如REINFORCE）定义。现在，这基本上就像听起来那么简单；然而，我们在编码这些实现的方式中有几个细节需要注意。因此，在审查代码时，我们将详细介绍这个实现的细节。打开`Chapter_8_ActorCritic.py`并跟随下一个练习：

1.  由于此代码遵循与之前示例相同的模式，我们只需要详细说明几个部分。最重要的部分是文件顶部的`ActorCritic`类，如下所示：

[PRE4]

1.  从`init`函数开始，我们可以看到我们构建了三个`Linear`网络层：`fc1`和`fc_pi`用于`policy`，`fc_v`用于`value`。然后，在`init`之后，我们看到`pi`和`v`函数。这些函数对每个网络（`pi`和`v`）进行前向传递。注意这两个网络都共享`fc1`作为输入层。这意味着我们网络的第一层将用于以演员和评论家网络共享的形式编码网络状态。在更高级的网络配置中，共享这样的层是常见的。

1.  接下来，我们看到`put_data`函数，它只是将记忆放入重放或经验缓冲区。

1.  之后，我们有一个名为`make_batch`的强大函数，它只是构建我们在经验重放中使用的批量数据。

1.  我们将跳过`ActorCritic`训练函数`train_net`，并跳到下面的迭代训练代码，如下所示：

[PRE5]

1.  你可能没有注意到，但我们的最后一个练习使用了基于事件的训练或我们所说的蒙特卡洛或离线策略训练。这次，我们的训练是在策略下进行的，这意味着我们的代理在接收到新的更新后立即采取行动。否则，代码与其他许多示例非常相似，并且可以运行。

1.  按照常规方式运行示例。训练可能需要一段时间，所以先启动它，然后回到这本书。

现在我们已经了解了示例代码的基本布局，是时候进入下一节训练的细节了。

# 训练优势 AC

使用优势并训练多个网络协同工作，正如您所想象的那样，并不简单。因此，我们想要专注于整个练习来理解 AC 中的训练工作。再次打开 `Chapter_8_ActorCritic.py` 并跟随练习：

1.  我们的主要关注点将是之前看到的 `ActorCritic` 类中的 `train_net` 函数。从前两行开始，我们可以看到这是训练批次首先被创建的地方，我们计算 `td_target`。回想一下，当我们实现 DDQN 时，我们覆盖了 TD 错误计算的形式检查：

[PRE6]

1.  接下来，我们计算目标函数和值函数之间的变化或增量。同样，这在 DDQN 中已经覆盖了，执行此操作的代码如下：

[PRE7]

1.  之后，我们使用 `self.pi` 在 π 网络上执行前向传递，然后收集结果。收集函数本质上是对数据进行对齐或收集。感兴趣的读者应查阅 PyTorch 网站以获取有关 `gather` 的进一步文档。此步骤的代码如下：

[PRE8]

1.  然后，我们使用以下代码计算损失：

[PRE9]

1.  损失是通过更新策略方法计算的，其中我们使用对数来对动作进行逆优化。回想一下，在我们之前的讨论中，介绍了 ![](img/aafe4f10-5c43-4378-8061-8031f956c590.png) 函数。此函数表示优势函数，其中我们取策略的负对数并将其添加到值函数 `v` 和 `td_target` 的 L1 平方误差输出中。张量上的 `detach` 函数仅允许网络在训练时不对这些值进行更新。

1.  最后，我们使用以下代码将损失反向传递到网络中：

[PRE10]

1.  这里没有什么新的内容。代码首先将梯度置零，然后计算批次的平均损失，并通过调用 `backward` 将其反向传递，最后使用 `step` 步进优化器完成。

您将需要调整此示例中的超参数来训练一个能够完成环境的智能体。当然，您现在完全能够接受这个挑战。在下一节中，我们将向上移动并查看另一类 PG 方法。

# 构建深度确定性策略梯度

我们在PG方法中面临的一个问题是可变性或过多的随机性。当然，我们可能期望从随机策略中采样时会出现这种情况。**深度确定性策略梯度**（**DDPG**）方法是在2015年由Tim Lillicrap发表的一篇题为《使用深度强化学习进行连续控制》的论文中提出的。它的目的是解决通过连续动作空间控制动作的问题，这是我们之前一直避免的问题。记住，连续动作空间与离散空间的不同之处在于，动作可以指示一个方向，也可以指示一个量或值，这表达了在该方向上的努力程度，而离散动作中，任何动作选择都被假定为始终是100%的努力。

那么，这有什么关系呢？好吧，在我们上一章的练习中，我们探索了离散动作空间上的PG方法。通过在离散空间中使用这些方法，我们本质上通过将动作概率转换为离散值来缓冲或掩盖了可变性的问题。然而，在具有连续控制或连续动作空间的环境中，这并不奏效。于是出现了DDPG。答案就在其名称中：深度确定性策略梯度，本质上意味着我们正在将确定性重新引入PG方法，以纠正可变性的问题。

本章我们将介绍的最后两种PG方法，即DDPG和TRPO，通常被认为是特定需求的，并且在某些情况下过于复杂，难以有效实现。因此，在过去的几年里，这些方法在更先进的发展中并没有得到太多应用。这些方法的代码已经提供以供完整性，但解释可能有些仓促。

让我们通过打开`Chapter_8_DDPG.py`并跟随下一个练习来看看这代码是如何实现的：

1.  这个示例的完整源代码太大，无法全部列出。相反，我们将通过练习中的相关部分进行讲解，从超参数开始，如下所示：

[PRE11]

1.  看起来我们介绍了一些新的超参数，但实际上我们只介绍了一个新的参数，称为`tau`。其他变量`lr_mu`和`lr_q`是两个不同网络的 学习率。

1.  接下来，我们跳过了`ReplayBuffer`类，这个类我们之前已经见过，用于存储经验，然后继续跳过其他代码，直到我们到达环境设置和更多变量定义的部分，如下所示：

[PRE12]

1.  首先，我们看到新环境的设置，`Pendulum`。现在，`Pendulum` 是一个连续控制环境，需要学习连续空间动作。之后，创建了 `memory` 和 `ReplayBuffer`，接着创建了两个名为 `QNet` 和 `MuNet` 的类。接下来，初始化了更多的控制/监控参数。在最后一行之前，我们看到创建了两个优化器，`mu_optimizer` 和 `q_optimizer`，分别用于 `MuNet` 和 `QNet` 网络。最后，在最后一行，我们看到创建了一个新的张量 `ou_noise`。这里有很多新的事情在进行，但我们很快就会看到这一切是如何结合在一起的：

[PRE13]

1.  接下来，向下移动到前面行中显示的训练循环的顶部。我们确保算法可以完全循环通过一个场景。因此，我们将内循环的范围设置为高于智能体在环境中获得的迭代次数的值：

[PRE14]

1.  接下来是试错训练代码。请注意，`a` 动作是从名为 `mu` 的网络中提取的。然后，在下一行，我们将 `ou_noise` 值添加到其中。之后，我们让智能体迈出一步，将结果存入记忆中，并更新分数和状态。我们这里使用的噪声值基于 Ornstein-Uhlenbeck 过程，并由同名类生成。这个过程生成一个移动的随机值，倾向于收敛到值 ![](img/29efbecd-091a-4a5a-823e-d5b784d76923.png) 或 `mu`。

回想一下，我们在 `OrnsteinUhlenbeckNoise` 类的早期实例化中将此值初始化为零。这里的直觉是我们希望噪声在实验过程中收敛到 0。这具有控制智能体执行探索量的效果。更多的噪声会导致它选择动作的不确定性增加，因此智能体会更随机地选择。你现在可以将动作中的噪声视为智能体在该动作中具有的不确定性以及它需要探索多少以减少这种不确定性。

在这里使用 Ornstein-Uhlenbeck 过程来生成噪声，因为它以随机但可预测的方式收敛，即它总是收敛。当然，你可以在这里使用任何你喜欢的噪声值，甚至更确定性的东西。

1.  在训练循环内部，我们跳转到执行实际训练的代码部分：

[PRE15]

1.  我们可以看到，一旦记忆 `ReplayBuffer` 超过 `2000`，智能体就开始以 10 个循环进行训练。首先，我们看到对 `train` 函数的调用，其中包含构建的各种网络/模型 `mu`、`mu_target`、`q` 和 `q_target`；`memory`；以及 `q_optimizer` 和 `mu_optimizer` 优化器。然后，有两个对 `soft_update` 函数的调用，使用各种模型。这里显示的 `soft_update` 只是通过使用 `tau` 缩放每次迭代的改变量，以迭代方式将输入模型收敛到目标：

[PRE16]

1.  这种从某些演员模型到目标的收敛并不新鲜，但是随着AC的引入，它确实使事情复杂化了。不过，在我们到达那里之前，让我们运行这个示例并看看它是如何运作的。像平常一样运行代码并等待：这个可能需要一段时间。如果你的智能体达到足够高的分数，你将获得以下奖励：

![图片](img/cf2ad031-a0b2-411c-94fc-929479f40949.png)

这是摆锤环境的示例

在运行示例时，特别关注屏幕上分数的更新。尽量感受一下这可能会在图形上看起来如何。在下一节中，我们将探讨这个最后示例的更详细细节。

# 训练DDPG

现在，正如你可能在上一个示例中注意到的，`Chapter_8_DDPG.py`正在使用四个网络/模型进行训练，使用两个网络作为演员，两个作为评论者，但也使用两个网络作为目标，两个作为当前。这给我们以下图示：

![图片](img/a09bd1f4-973d-4c20-b4a6-d11ea0d3237e.png)

演员评论者目标-当前网络图

前一个图中的每个椭圆形代表一个完整的深度学习网络。注意评论者，即价值或Q网络实现，正在接受环境输出的奖励和状态。然后评论者将一个值推回给演员或策略目标网络。

打开示例`Chapter_8_DDPG.py`，然后按照下一个练习来查看代码是如何组合在一起的：

1.  我们将首先查看这里显示的评论者或`QNet`网络类的定义：

[PRE17]

1.  这个网络的构建也略有不同，这里发生的事情是`fc_s`层编码状态，然后`fc_a`编码动作。这两个层在正向传递中连接，创建一个单一的Q层，`fc_q`，然后通过最后一层，`fc_3`输出。

如果你需要帮助想象这些类型的网络，绘制它们通常很有帮助。这里的关键是查看训练函数中的代码，它描述了层是如何连接的。

1.  从评论者网络转移到由`MuNet`类定义的演员网络，如下所示：

[PRE18]

1.  `MuNet`是一个将状态从3个值编码到128个输入神经元`fc1`的简单网络实现，然后是64个隐藏层神经元`fc2`，最后输出到输出层上的单个值`fc_mu`。唯一值得注意的注释是我们如何将`fc_mu`，`forward`函数中的输出层，转换为`mu`输出值。这是为了考虑到`Pendulum`环境中的控制范围，该环境接受-2到2的动作值。如果你将这个示例转换到另一个环境，请确保考虑到动作空间值的变化。

1.  接下来，我们将向下移动到`train`函数的开始，如下所示：

[PRE19]

1.  `train`函数接受所有网络、记忆和优化器作为输入。在第一行，它从`replayBuffer`记忆中提取`s`状态、`a`动作、`r`奖励、`s_prime`下一个状态和`done_mask`：

[PRE20]

1.  函数内部的第一块代码根据`q_target`网络的输出计算目标值，该网络以最后一个状态`s_prime`和从最后一个状态输出的`mu_target`作为输入。然后，我们根据使用`target`值作为目标的输入状态和动作来计算`q_loss`损失。这种有些抽象的转换是为了将值从随机转换为确定性。在最后三行中，我们看到典型的优化器代码用于归零梯度并进行反向传播：

[PRE21]

1.  计算`mu_loss`策略损失要简单得多，我们只需使用`mu`网络的状态和输出动作来获取`q`网络的输出。需要注意的是，我们将损失设置为负值并取平均值。然后，我们使用典型的优化器反向传播来完成`mu_loss`函数。

1.  如果智能体仍在运行之前的练习，请用这种新获得的知识检查结果。考虑如何或可以调整哪些超参数来改善这个示例的结果。

对于一些人来说，DDPG的纯代码解释可能有点抽象，但希望不是。希望到这一点，你可以阅读代码并假设你需要理解概念所需的数学或直觉。在下一节中，我们将探讨被认为是DRL中更复杂的方法之一，即**信任区域策略优化**（**TRPO**）方法。

# 探索信任区域策略优化

PG方法存在几个技术问题，其中一些你可能已经注意到了。这些问题在训练中表现出来，你可能已经观察到缺乏训练收敛或波动。这是由几个我们可以总结的因素造成的：

+   **梯度上升与梯度下降**：在PG中，我们使用梯度上升假设最大动作值位于山顶。然而，我们选择的优化方法（SGD或ADAM）是针对梯度下降或寻找山谷或平坦区域的值进行调优的，这意味着它们在寻找山谷底部时表现良好，但在寻找脊顶时表现不佳，尤其是如果脊或山很陡。这里展示了这种比较：

![图片](img/b74693d2-6ad3-45b6-b363-9016b555cd1d.png)

梯度下降与上升的比较

因此，找到峰值成为问题，尤其是在需要精细控制或窄离散动作的环境中。这通常表现为训练波动，其中智能体不断增加分数，但每隔一段时间就会后退几步。

+   **策略与参数空间映射**：根据其本质，我们通常需要将策略映射到某个已知动作空间，无论是通过连续变换还是离散化变换。这一步，不出所料，并非没有问题。离散化动作空间可能特别有问题，并进一步加剧了之前提到的爬山问题。

+   **静态与动态学习率**：部分原因是前面提到的优化器问题，我们也倾向于发现使用静态学习率是有问题的。也就是说，我们经常发现，随着智能体继续找到那些最大动作山峰的顶峰，我们需要降低学习率。

+   **策略采样效率**：PG方法限制我们在每个轨迹中只能更新策略一次。如果我们尝试更频繁地更新，例如在*x*步之后，我们会看到训练发散。因此，我们被限制在每个轨迹或回合中只更新一次。这可能在具有多个步骤的训练环境中提供非常低的样本效率。

TRPO和另一种名为**近端策略优化（proximal policy optimization）**的PG方法，我们将在第9章（2f6812c0-fd1f-4eda-9df2-6c67c8077aec.xhtml）中探讨，*异步动作和政策*，试图使用几种常见策略解决所有这些问题。

这个TRPO实现的代码直接来源于[https://github.com/ikostrikov/pytorch-trpo](https://github.com/ikostrikov/pytorch-trpo)，在写作时，代码仅稍作修改以允许更容易地运行。这是一个很好的例子，值得进一步探索和改进。

在我们开始审查这些策略之前，让我们打开代码并跟随下一个练习：

1.  TRPO是一个庞大的算法，不容易在一个文件中运行。我们将首先通过在源文件夹中打开TRPO文件夹来查看代码结构。这个例子涵盖了多个文件中的代码，我们在这里只会审查一小部分。建议你在继续之前快速全面地审查源代码。

1.  参考一下`main.py`文件；这是一个启动文件。`main`在运行时接受在这个文件中定义的几个参数作为输入。

1.  滚动到大约中间位置，你将看到环境是如何在主要策略和价值网络上构建的，如图所示：

[PRE22]

1.  接下来，继续向下滚动，直到你到达那个熟悉的训练循环。大部分应该看起来与其他例子相似，除了引入了另一个`while`循环，如下所示：

[PRE23]

1.  这段代码确保一个智能体（agent）的回合（episode）由`batch_size`决定的给定步数组成。然而，我们仍然不会在环境表示回合完成之前打破内部训练循环。但是，现在，只有在达到提供的`batch_size`之后，才会完成一个回合或轨迹更新。这试图解决我们之前讨论过的PG方法采样问题。

1.  快速审查每个源文件；以下列表总结了每个文件的目的：

    +   `main.py`：这是启动源文件，也是智能体训练的主要点。

    +   `conjugate_gradients.py`：这是一个辅助方法，用于共轭或连接梯度。

    +   `models.py`: 此文件定义了网络类 Policy（演员）和 value（评论家）。这些网络的构建有点独特，所以请确保查看。

    +   `replay_memory.py`: 这是一个辅助类，用于包含重放内存。

    +   `running_state.py`: 这是一个辅助类，用于计算状态的运行方差，本质上，是均值和标准差的运行估计。这对于对正态分布进行任意采样是有益的。

    +   `trpo.py`: 这是针对 TRPO 的特定代码，旨在解决我们之前提到的 PG 问题。

    +   `utils.py`: 这提供了一些辅助方法。

1.  使用默认的起始参数，像通常运行 Python 文件一样运行 `main.py`，并观察输出，如下所示：

![](img/fdc9d012-3339-4c9d-ac24-976eb98af9f9.png)

TRPO 样本的输出

此特定实现的输出更为详细，并显示了监控我们提到的问题（PG 方法遭受的问题）的性能因素。在接下来的几节中，我们将通过练习展示 TRPO 如何尝试解决这些问题。

Jonathan Hui ([https://medium.com/@jonathan_hui](https://medium.com/@jonathan_hui)) 在 Medium.com 上有几篇关于 DRL 算法各种实现的优秀文章。他在解释 TRPO 和其他更复杂方法背后的数学方面做得特别出色。

# 共轭梯度

我们需要用策略方法解决的基本问题是将梯度上升转换为自然梯度形式。以前，我们通过简单地应用对数函数来处理这个梯度的共轭。然而，这并不产生自然梯度。自然梯度不易受模型参数化影响，并提供了一种不变的方法来计算稳定的梯度。让我们通过再次打开我们的 IDE 到 TRPO 示例并跟随下一个练习来看看这是如何实现的：

1.  在 `TRPO` 文件夹中打开 `trpo.py` 文件。此文件中的三个函数旨在解决我们遇到的 PG 问题的各种问题。我们遇到的第一问题是反转梯度，执行此操作的代码如下所示：

[PRE24]

1.  `conjugate_gradients` 函数被迭代使用，以产生一个更自然、更稳定的梯度，我们可以用它来进行上升。

1.  滚动到 `trpo_step` 函数，你将看到这个方法如何在代码中展示使用：

[PRE25]

1.  这输出一个 `stepdir` 张量，表示用于移动网络的梯度。我们可以通过输入参数看到，输出共轭梯度将通过一个近似函数 `Fvp` 和损失梯度的逆 `loss_grad` 在 10 次迭代中求解。这与其他一些优化纠缠在一起，所以我们现在暂停。

共轭梯度是我们可以使用的一种方法，以更好地管理 PG 方法中遇到的梯度下降与梯度上升问题。接下来，我们将再次探讨优化，以解决梯度上升的问题。

# 信任区域方法

我们可以对梯度上升进行进一步优化的方法是使用信任区域或更新的控制区域。这些方法当然是 TRPO 的基础，但这个概念被进一步扩展到其他基于策略的方法。在 TRPO 中，我们使用 **最小化-最大化** 或 **MM** 算法在近似函数上扩展信任区域。MM 的直觉是存在一个下界函数，我们可以预期回报/奖励总是高于这个下界。因此，如果我们最大化这个下界函数，我们也会得到最佳策略。默认情况下，梯度下降是一个线搜索算法，但这又引入了超调的问题。相反，我们首先可以近似步长，然后在那个步长内建立一个信任区域。这个信任区域然后成为我们优化的空间。

我们经常用来解释这个概念的类比是让你想象自己在爬一个狭窄的山脊。你面临从山脊两边掉下去的风险，所以使用正常的梯度下降或线搜索变得危险。相反，你决定为了避免掉下去，你想踩在山脊的中心或你信任的中心区域。以下是从 Jonathan Hui 的博客文章中截取的屏幕截图，进一步展示了这个概念：

![图片](img/9dcd8c89-aaae-4fdd-b0a7-86f70eb29def.png)

线搜索与信任区域的比较

我们可以通过打开 TRPO 文件夹并跟随下一个练习来查看代码中的样子：

1.  再次打开 `trpo.py` 并向下滚动到以下代码块：

[PRE26]

1.  `linesearch` 函数用于确定我们想要在山脊上定位下一个信任区域的距离。这个函数用于指示到下一个信任区域的距离，并使用以下代码执行：

[PRE27]

1.  注意到 `neggdotstepdir` 的使用。这个值是从我们在上一个练习中计算的步长方向 `stepdir` 计算出来的，以下代码所示：

[PRE28]

1.  现在我们有了 `neggdotstepdir` 方向和 `linesearch` 数量，我们可以用以下代码确定信任区域：

[PRE29]

1.  `set_flat_params_to` 函数位于 `utils.py` 文件中，代码如下所示：

[PRE30]

1.  这段代码本质上是将参数平坦化到信任区域。这是我们用来测试下一步是否在其中的信任区域，使用 `linesearch` 函数。

现在我们理解了信任区域的概念以及在使用 PG 方法时正确控制步长、方向和数量的必要性。在下一节中，我们将查看步骤本身。

# TRPO 步骤

如你现在所看到的，使用 TRPO 进行步骤或更新并不简单，事情还会变得更加复杂。这个步骤本身要求智能体从更新策略和值函数中学习几个因素，以获得优势，也称为演员-评论家。理解步骤函数的实际细节超出了本书的范围，你再次被推荐参考外部参考资料。然而，回顾 TRPO 中步骤的构成以及这与我们未来将要探讨的其他方法的复杂度比较可能是有帮助的。再次打开样本 TRPO 文件夹，并遵循下一个练习：

1.  打开 `main.py` 文件，找到大约在第 130 行的以下代码行：

[PRE31]

1.  这最后一行代码位于 `update_params` 函数中，这是大部分训练发生的地方。

1.  你可以在 `main.py` 文件几乎最底部看到对 `update_params` 函数的调用，其中 `batch` 是从 `memory` 中抽取的样本，如下面的代码所示：

[PRE32]

1.  滚回 `update_params` 函数，注意第一个循环使用以下代码构建 `returns`、**`deltas`** 和 `advantages`：

[PRE33]

1.  注意我们是如何反转奖励，然后通过它们循环以构建我们的各种列表 `returns`、**`deltas`** 和 `advantages`。

1.  从那里，我们将参数展开并设置值网络，即评论家。然后，我们计算优势、动作均值和标准差。我们在处理分布而非确定性值时这样做。

1.  之后，我们使用 `trpo_step` 函数进行一次训练步骤或策略更新。

你可能已经注意到了源代码中使用了 `kl`。这代表KL散度，我们将在后面的章节中探讨。

保持示例运行大约 5,000 次训练迭代。这可能需要一些时间，所以请耐心等待。如果可能的话，完成整个运行过程是值得的。本节中的 TRPO 示例旨在进行实验和使用，以各种控制环境进行测试。在下一节中，请确保回顾你可以尝试的实验，以探索更多关于这种方法的信息。

# 练习

使用这些练习来享受学习，并获取额外的经验。深度学习和深度强化学习是非常需要通过实际操作示例来提高知识的领域。不要期望在训练智能体时能自然而然地成功；这需要大量的尝试和错误。幸运的是，我们需要的经验量并不像我们那些表现不佳的智能体那么大，但仍然需要投入一些时间。

1.  打开示例文件 `Chapter_8_REINFORCE.py`，将其备份并修改超参数，以查看这对训练有何影响。

1.  打开示例文件 `Chapter_8_ActorCritic.py`，将其备份并修改超参数，以查看这对训练有何影响。

1.  打开示例文件 `Chapter_8_DDPG.py`，将其备份并修改超参数，以查看这对训练有何影响。

1.  如何将 **REINFORCE** 或 `ActorCritic` 示例转换为使用连续动作空间？尝试为例如 `LunarLanderContinous-v2` 的新环境执行此操作。

1.  设置示例 `Chapter_8_DDPG.py` 以使用 `LunarLanderContinuous-v2` 或其他连续环境。您需要将动作状态从 `3` 修改为您选择的环境。

1.  调整 `Chapter_8_DDPG.py` 示例的超参数。这需要您学习和理解新的参数 `tau`。

1.  调整 **TRPO** 示例的超参数。这需要您学习如何从命令行设置超参数，然后调整这些参数。您不应该修改任何代码来完成此练习。

1.  启用MuJoCo环境，并使用TRPO示例运行这些环境之一。

1.  将绘图输出添加到各个示例中。

1.  将单个文件示例中的一个转换为使用主方法，该方法接受参数并允许用户动态训练超参数，而不是修改源代码。

在进入下一节和本章结尾之前，请确保完成前面的1-3个练习。

# 摘要

在本章中，我们介绍了策略梯度方法，我们学习了如何使用随机策略通过REINFORCE算法驱动我们的智能体。之后，我们了解到从随机策略中采样的部分问题是随机采样的随机性。我们发现这可以通过双智能体网络来纠正，其中一个代表动作网络，另一个作为评论家。在这种情况下，动作者是引用评论家网络的策略网络，它使用确定性价值函数。然后，我们看到了如何通过观察DDPG的工作来改进PG。最后，我们探讨了被认为是DRL中更复杂方法之一的TRPO，并了解了它如何试图管理PG方法的几个缺点。

在继续探讨PG方法的基础上，我们将在下一章探索下一代方法，如PPO、AC2、AC2 和 ACER。
