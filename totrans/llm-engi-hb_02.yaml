- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Tooling and Installation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具和安装
- en: This chapter presents all the essential tools that will be used throughout the
    book, especially in implementing and deploying the LLM Twin project. At this point
    in the book, we don’t plan to present in-depth LLM, RAG, MLOps, or LLMOps concepts.
    We will quickly walk you through our tech stack and prerequisites to avoid repeating
    ourselves throughout the book on how to set up a particular tool and why we chose
    it. Starting with *Chapter 3*, we will begin exploring our LLM Twin use case by
    implementing a data collection ETL that crawls data from the internet.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了本书中将使用到的所有基本工具，特别是在实现和部署LLM Twin项目时。在本书的这一部分，我们并不计划深入介绍LLM、RAG、MLOps或LLMOps的概念。我们将快速带您了解我们的技术栈和先决条件，以避免在书中重复介绍如何设置特定工具以及为什么选择它。从**第3章**开始，我们将通过实现一个从互联网爬取数据的ETL数据收集过程，开始探索我们的LLM
    Twin用例。
- en: 'In the first part of the chapter, we will present the tools within the Python
    ecosystem to manage multiple Python versions, create a virtual environment, and
    install the pinned dependencies required for our project to run. Alongside presenting
    these tools, we will also show how to install the `LLM-Engineers-Handbook` repository
    on your local machine (in case you want to try out the code yourself): [https://github.com/PacktPublishing/LLM-Engineers-Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们将介绍Python生态系统中的工具，用于管理多个Python版本，创建虚拟环境，并安装项目运行所需的固定依赖项。在介绍这些工具的同时，我们还将展示如何在本地机器上安装`LLM-Engineers-Handbook`仓库（如果您想亲自尝试代码）：[https://github.com/PacktPublishing/LLM-Engineers-Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook)。
- en: Next, we will explore all the MLOps and LLMOps tools we will use, starting with
    more generic tools, such as a model registry, and moving on to more LLM-oriented
    tools, such as LLM evaluation and prompt monitoring tools. We will also understand
    how to manage a project with multiple ML pipelines using ZenML, an orchestrator
    bridging the gap between ML and MLOps. Also, we will quickly explore what databases
    we will use for NoSQL and vector storage. We will show you how to run all these
    components on your local machine using Docker. Lastly, we will quickly review
    AWS and show you how to create an AWS user and access keys and install and configure
    the AWS CLI to manipulate your cloud resources programmatically. We will also
    explore SageMaker and why we use it to train and deploy our open-source LLMs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索我们将使用的所有MLOps和LLMOps工具，从更通用的工具开始，例如模型注册表，然后转向更面向LLM的工具，例如LLM评估和提示监控工具。我们还将了解如何使用ZenML（一个连接ML和MLOps的桥梁）来管理具有多个ML管道的项目。此外，我们将快速探讨我们将使用哪些数据库来存储NoSQL和矢量数据。我们将向您展示如何使用Docker在本地机器上运行所有这些组件。最后，我们将快速回顾AWS，并展示如何创建AWS用户和访问密钥，以及如何安装和配置AWS
    CLI以程序化地操作云资源。我们还将探索SageMaker以及为什么我们使用它来训练和部署我们的开源LLM。
- en: If you are familiar with these tools, you can safely skip this chapter. We also
    explain how to install the project and set up all the necessary components in
    the repository’s `README`. Thus, you also have the option to use that as more
    concise documentation if you plan to run the code while reading the book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉这些工具，您可以安全地跳过本章。我们还在仓库的`README`中解释了如何安装项目和设置所有必要的组件。因此，如果您计划在阅读本书的同时运行代码，您也可以选择使用它作为更简洁的文档。
- en: 'To sum all that up, in this chapter, we will explore the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在本章中，我们将探讨以下主题：
- en: Python ecosystem and project installation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python生态系统和项目安装
- en: MLOps and LLMOps tooling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLOps和LLMOps工具
- en: Databases for storing unstructured and vector data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储非结构化和矢量数据的数据库
- en: Preparing for AWS
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备AWS
- en: By the end of this chapter, you will be aware of all the tools we will use across
    the book. Also, you will have learned how to install the `LLM-Engineers-Handbook`
    repository, set up the rest of the tools, and use them if you run the code while
    reading the book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解本书中我们将使用的所有工具。此外，您将学会如何安装`LLM-Engineers-Handbook`仓库，设置其余的工具，并在阅读本书时运行代码。
- en: Python ecosystem and project installation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python生态系统和项目安装
- en: 'Any Python project needs three fundamental tools: the Python interpreter, dependency
    management, and a task execution tool. The Python interpreter executes your Python
    project as expected. All the code within the book is tested with Python 3.11.8\.
    You can download the Python interpreter from here: [https://www.python.org/downloads/](https://www.python.org/downloads/).
    We recommend installing the exact Python version (Python 3.11.8) to run the LLM
    Twin project using `pyenv`, making the installation process straightforward.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何Python项目都需要三个基本工具：Python解释器、依赖关系管理和任务执行工具。Python解释器按照预期执行你的Python项目。本书中所有的代码都是使用Python
    3.11.8进行测试的。你可以从这里下载Python解释器：[https://www.python.org/downloads/](https://www.python.org/downloads/)。我们建议安装确切的Python版本（Python
    3.11.8），以便使用`pyenv`运行LLM Twin项目，使安装过程变得简单直接。
- en: 'Instead of installing multiple global Python versions, we recommend managing
    them using `pyenv`, a Python version management tool that lets you manage multiple
    Python versions between projects. You can install it using this link: [https://github.com/pyenv/pyenv?tab=readme-ov-file#installation](https://github.com/pyenv/pyenv?tab=readme-ov-file#installation).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与安装多个全局Python版本相比，我们建议使用`pyenv`来管理它们，这是一个Python版本管理工具，它允许你在项目之间管理多个Python版本。你可以使用此链接安装它：[https://github.com/pyenv/pyenv?tab=readme-ov-file#installation](https://github.com/pyenv/pyenv?tab=readme-ov-file#installation)。
- en: 'After you have installed `pyenv`, you can install the latest version of Python
    3.11, using `pyenv`, as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在你安装了`pyenv`之后，你可以使用`pyenv`安装Python 3.11的最新版本，如下所示：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now list all installed Python versions to see that it was installed correctly:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在列出所有已安装的Python版本，以查看它是否已正确安装：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see something like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似以下内容：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To make Python 3.11.8 the default version across your entire system (whenever
    you open a new terminal), use the following command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要将Python 3.11.8设置为整个系统的默认版本（每次打开新终端时），请使用以下命令：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'However, we aim to use Python 3.11.8 locally only in our repository. To achieve
    that, first, we have to clone the repository and navigate to it:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的目标是只在我们的仓库中本地使用Python 3.11.8。为了实现这一点，首先，我们必须克隆仓库并导航到它：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Because we defined a `.python-version` file within the repository, `pyenv`
    will know to pick up the version from that file and use it locally whenever you
    are working within that folder. To double-check that, run the following command
    while you are in the repository:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在仓库中定义了`.python-version`文件，`pyenv`将知道从该文件中获取版本，并在你在这个文件夹内工作时本地使用它。为了双重检查，请在仓库内运行以下命令：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It should output:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该输出：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To create the `.python-version` file, you must run `pyenv local 3.11.8` once.
    Then, `pyenv` will always know to use that Python version while working within
    a specific directory.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建`.python-version`文件，你必须运行一次`pyenv local 3.11.8`。然后，`pyenv`将始终知道在特定目录内工作时使用该Python版本。
- en: Now that we have installed the correct Python version using `pyenv`, let’s move
    on to Poetry, which we will use as our dependency and virtual environment manager.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用`pyenv`安装了正确的Python版本，让我们继续使用Poetry，我们将将其用作依赖关系和虚拟环境管理器。
- en: 'Poetry: dependency and virtual environment management'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 诗歌：依赖关系和虚拟环境管理
- en: Poetry is one of the most popular dependency and virtual environment managers
    within the Python ecosystem. But let’s start by clarifying what a dependency manager
    is. In Python, a dependency manager allows you to specify, install, update, and
    manage external libraries or packages (dependencies) that a project relies on.
    For example, this is a simple Poetry requirements file that uses Python 3.11 and
    the `requests` and `numpy` Python packages.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Poetry是Python生态系统中最受欢迎的依赖关系和虚拟环境管理器之一。但让我们首先明确什么是依赖关系管理器。在Python中，依赖关系管理器允许你指定、安装、更新和管理项目所依赖的外部库或包（依赖项）。例如，这是一个简单的Poetry需求文件，它使用Python
    3.11和`requests`以及`numpy` Python包。
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By using Poetry to pin your dependencies, you always ensure that you install
    the correct version of the dependencies that your projects work with. Poetry,
    by default, saves all its requirements in `pyproject.toml` files, which are stored
    at the root of your repository, as you can see in the cloned LLM-Engineers-Handbook
    repository.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Poetry来锁定依赖项，你总是确保安装了与你的项目兼容的正确版本的依赖项。默认情况下，Poetry会将所有需求保存在`pyproject.toml`文件中，这些文件存储在你的仓库根目录下，正如你在克隆的LLM-Engineers-Handbook仓库中看到的那样。
- en: Another massive advantage of using Poetry is that it creates a new Python virtual
    environment in which it installs the specified Python version and requirements.
    A virtual environment allows you to isolate your project’s dependencies from your
    global Python dependencies and other projects. By doing so, you ensure there are
    no version clashes between projects. For example, let’s assume that Project A
    needs `numpy == 1.19.5`, and Project B needs `numpy == 1.26.0`. If you keep both
    projects in the global Python environment, that will not work, as Project B will
    override Project A’s `numpy` installation, which will corrupt Project A and stop
    it from working. Using Poetry, you can isolate each project in its own Python
    environment with its own Python dependencies, avoiding any dependency clashes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Poetry的另一个巨大优势是它创建一个新的Python虚拟环境，在其中安装指定的Python版本和需求。虚拟环境允许你将你的项目依赖与全局Python依赖和其他项目隔离开来。通过这样做，你可以确保项目之间没有版本冲突。例如，假设项目A需要`numpy
    == 1.19.5`，而项目B需要`numpy == 1.26.0`。如果你将这两个项目都保留在全局Python环境中，那么这将不起作用，因为项目B将覆盖项目A的`numpy`安装，这将破坏项目A并使其停止工作。使用Poetry，你可以将每个项目隔离在其自己的Python环境中，每个项目都有自己的Python依赖，从而避免任何依赖冲突。
- en: 'You can install Poetry from here: [https://python-poetry.org/docs/](https://python-poetry.org/docs/).
    We use Poetry 1.8.3 throughout the book. Once Poetry is installed, navigate to
    your cloned LLM-Engineers-Handbook repository and run the following command to
    install all the necessary Python dependencies:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从这里安装Poetry：[https://python-poetry.org/docs/](https://python-poetry.org/docs/)。本书中我们使用Poetry
    1.8.3。一旦安装了Poetry，导航到你的克隆的LLM-Engineers-Handbook仓库，并运行以下命令来安装所有必要的Python依赖：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This command knows to pick up all the dependencies from your repository that
    are listed in the `pyproject.toml` and `poetry.lock` files. After the installation,
    you can activate your Poetry environment by running `poetry shell` in your terminal
    or by prefixing all your CLI commands as follows: `poetry run <your command>`.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令知道从你的仓库中提取所有在`pyproject.toml`和`poetry.lock`文件中列出的依赖。安装完成后，你可以在终端中运行`poetry
    shell`或通过以下方式将所有CLI命令作为前缀来激活你的Poetry环境：`poetry run <your command>`。
- en: One final note on Poetry is that it locks down the exact versions of the dependency
    tree in the `poetry.lock` file based on the definitions added to the `project.toml`
    file. While the `pyproject.toml` file may specify version ranges (e.g., `requests
    = "^2.25.1"`), the `poetry.lock` file records the exact version (e.g., `requests
    = "2.25.1"`) that was installed. It also locks the versions of sub-dependencies
    (dependencies of your dependencies), which may not be explicitly listed in your
    `pyproject.toml` file. By locking all the dependencies and sub-dependencies to
    specific versions, the `poetry.lock` file ensures that all project installations
    use the same versions of each package. This consistency leads to predictable behavior,
    reducing the likelihood of encountering “works on my machine” issues.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关于诗歌的一个最后注意事项是，它会在`poetry.lock`文件中锁定基于添加到`project.toml`文件中的定义的依赖树的确切版本。虽然`pyproject.toml`文件可能指定版本范围（例如，`requests
    = "^2.25.1"`），但`poetry.lock`文件会记录安装的确切版本（例如，`requests = "2.25.1"`）。它还会锁定子依赖的版本（你的依赖的依赖），这些版本可能没有在`pyproject.toml`文件中明确列出。通过将所有依赖和子依赖锁定到特定版本，`poetry.lock`文件确保所有项目安装都使用每个包的相同版本。这种一致性导致行为可预测，减少了遇到“在我的机器上工作”问题的可能性。
- en: 'Other tools similar to Poetry are Venv and Conda for creating virtual environments.
    Still, they lack the dependency management option. Thus, you must do it through
    Python’s default `requirements.txt` files, which are less powerful than Poetry’s
    `lock` files. Another option is Pipenv, which feature-wise is more like Poetry
    but slower, and `uv`, which is a replacement for Poetry built in Rust, making
    it blazing fast. `uv` has lots of potential to replace Poetry, making it worthwhile
    to test out: [https://github.com/astral-sh/uv](https://github.com/astral-sh/uv).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与Poetry类似的其他工具包括用于创建虚拟环境的Venv和Conda。但它们缺少依赖管理选项。因此，你必须通过Python的默认`requirements.txt`文件来完成，这些文件比Poetry的`lock`文件功能弱。另一个选项是Pipenv，它在功能上更类似于Poetry，但速度较慢，还有`uv`，它是用Rust构建的Poetry替代品，使其速度极快。`uv`有很大的潜力取代Poetry，因此值得尝试：[https://github.com/astral-sh/uv](https://github.com/astral-sh/uv)。
- en: The final piece of the puzzle is to look at the task execution tool we used
    to manage all our CLI commands.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个要考虑的问题是查看我们用来管理所有CLI命令的任务执行工具。
- en: 'Poe the Poet: task execution tool'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 诗人Poe：任务执行工具
- en: Poe the Poet is a plugin on top of Poetry that is used to manage and execute
    all the CLI commands required to interact with the project. It helps you define
    and run tasks within your Python project, simplifying automation and script execution.
    Other popular options are Makefile, Invoke, or shell scripts, but Poe the Poet
    eliminates the need to write separate shell scripts or Makefiles for managing
    project tasks, making it an elegant way to manage tasks using the same configuration
    file that Poetry already uses for dependencies.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Poe the Poet 是 Poetry 之上的一个插件，用于管理和执行与项目交互所需的所有 CLI 命令。它帮助你在 Python 项目中定义和运行任务，简化了自动化和脚本执行。其他流行的选项包括
    Makefile、Invoke 或 shell 脚本，但 Poe the Poet 消除了为管理项目任务而编写单独的 shell 脚本或 Makefile
    的需求，使其成为使用 Poetry 已经使用的相同配置文件管理任务的一种优雅方式。
- en: 'When working with Poe the Poet, instead of having all your commands documented
    in a README file or other document, you can add them directly to your `pyproject.toml`
    file and execute them in the command line with an alias. For example, using Poe
    the Poet, we can define the following tasks in a `pyproject.toml` file:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Poe the Poet 时，你不必在 README 文件或其他文档中记录所有命令，可以直接将它们添加到 `pyproject.toml` 文件中，并在命令行中使用别名执行它们。例如，使用
    Poe the Poet，我们可以在 `pyproject.toml` 文件中定义以下任务：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can then run these tasks using the `poe` command:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用 `poe` 命令运行这些任务：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can install Poe the Poet as a Poetry plugin, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将 Poe the Poet 作为 Poetry 插件安装，如下所示：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To conclude, using a tool as a façade over all your CLI commands is necessary
    to run your application. It significantly simplifies the application’s complexity
    and enhances collaboration as it acts as out-of-the-box documentation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用工具作为所有 CLI 命令的代理来运行你的应用程序是必要的。它显著简化了应用程序的复杂性，并增强了协作，因为它充当了即用型的文档。
- en: 'Assuming you have `pyenv` and Poetry installed, here are all the commands you
    need to run to clone the repository and install the dependencies and Poe the Poet
    as a Poetry plugin:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经安装了 `pyenv` 和 Poetry，以下是你需要运行的所有命令来克隆存储库、安装依赖项并将 Poe the Poet 作为 Poetry
    插件安装：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To make the project fully operational, there are still a few steps to follow,
    such as filling out a `.env` file with your credentials and getting tokens from
    OpenAI and Hugging Face. But this book isn’t an installation guide, so we’ve moved
    all these details into the repository’s README as they are useful only if you
    plan to run the repository: [https://github.com/PacktPublishing/LLM-Engineers-Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要使项目完全运行，还需要遵循一些步骤，例如填写 `.env` 文件以包含你的凭证，并从 OpenAI 和 Hugging Face 获取令牌。但本书不是安装指南，所以我们已将这些详细信息移至存储库的
    README 文件中，因为它们仅在计划运行存储库时才有用：[https://github.com/PacktPublishing/LLM-Engineers-Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook)。
- en: Now that we have installed our Python project, let’s present the MLOps tools
    we will use in the book. If you are already familiar with these tools, you can
    safely skip the following tooling section and move on to the *Databases for storing
    unstructured and vector data* section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了我们的 Python 项目，让我们来介绍本书中将使用的 MLOps 工具。如果你已经熟悉这些工具，你可以安全地跳过以下工具部分，直接进入
    *存储非结构化和矢量数据的数据库* 部分。
- en: MLOps and LLMOps tooling
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps 和 LLMOps 工具
- en: This section will quickly present all the MLOps and LLMOps tools we will use
    throughout the book and their role in building ML systems using MLOps best practices.
    At this point in the book, we don’t aim to detail all the MLOps components we
    will use to implement the LLM Twin use case, such as model registries and orchestrators,
    but only provide a quick idea of what they are and how to use them. As we develop
    the LLM Twin project throughout the book, you will see hands-on examples of how
    we use all these tools. In *Chapter 11*, we will dive deeply into the theory of
    MLOps and LLMOps and connect all the dots. As the MLOps and LLMOps fields are
    highly practical, we will leave the theory of these aspects to the end, as it
    will be much easier to understand it after you go through the LLM Twin use case
    implementation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将快速介绍本书中将使用的所有 MLOps 和 LLMOps 工具及其在构建使用 MLOps 最佳实践的 ML 系统中的作用。在本书的这一部分，我们并不旨在详细说明我们将用于实现
    LLM Twin 用例的所有 MLOps 组件，例如模型注册表和编排器，而只是提供一个快速了解它们是什么以及如何使用它们的想法。随着我们在本书中开发 LLM
    Twin 项目，你将看到我们如何使用所有这些工具的实战示例。在 *第 11 章* 中，我们将深入探讨 MLOps 和 LLMOps 的理论，并连接所有线索。由于
    MLOps 和 LLMOps 领域高度实用，我们将把这些方面的理论留到后面，因为在你完成 LLM Twin 用例实现后，理解它们会更容易。
- en: Also, this section is not dedicated to showing you how to set up each tool.
    It focuses primarily on what each tool is used for and highlights the core features
    used throughout this book.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本节并非专门介绍如何设置每个工具。它主要关注每个工具的用途，并突出本书中使用的核心功能。
- en: 'Still, using Docker, you can quickly run the whole infrastructure locally.
    If you want to run the steps within the book yourself, you can host the application
    locally with these three simple steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，使用 Docker，你可以在本地快速运行整个基础设施。如果你想自己运行书中的步骤，你可以通过以下三个简单步骤在本地托管应用程序：
- en: Have Docker 27.1.1 (or higher) installed.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保 Docker 已安装 27.1.1（或更高版本）。
- en: Fill your `.env` file with all the necessary credentials as explained in the
    repository README.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照仓库 README 中的说明，将所有必要的凭据填入 `.env` 文件。
- en: Run `poetry` `poe` `local-infrastructure-up` to locally spin up ZenML (`http://127.0.0.1:8237/`)
    and the MongoDB and Qdrant databases.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `poetry` `poe` `local-infrastructure-up` 以在本地启动 ZenML (`http://127.0.0.1:8237/`)
    以及 MongoDB 和 Qdrant 数据库。
- en: 'You can read more details on how to run everything locally in the LLM-Engineers-Handbook
    repository README: [https://github.com/PacktPublishing/LLM-Engineers-Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook).
    Within the book, we will also show you how to deploy each component to the cloud.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 LLM-Engineers-Handbook 仓库的 README 中阅读更多关于如何在本地运行所有内容的细节：[https://github.com/PacktPublishing/LLM-Engineers-Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook)。在书中，我们还将向你展示如何将每个组件部署到云端。
- en: 'Hugging Face: model registry'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hugging Face：模型注册库
- en: A model registry is a centralized repository that manages ML models throughout
    their lifecycle. It stores models along with their metadata, version history,
    and performance metrics, serving as a single source of truth. In MLOps, a model
    registry is crucial for tracking, sharing, and documenting model versions, facilitating
    team collaboration. Also, it is a fundamental element in the deployment process
    as it integrates with **continuous integration** **and** **continuous deployment**
    (**CI/CD**) pipelines.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型注册库是一个集中式存储库，它管理着 ML 模型在其整个生命周期中的状态。它存储模型及其元数据、版本历史和性能指标，作为单一的真实来源。在 MLOps
    中，模型注册库对于跟踪、共享和记录模型版本至关重要，它促进了团队协作。此外，它还是部署过程中的基本元素，因为它与 **持续集成** **和** **持续部署**
    (**CI/CD**) 管道集成。
- en: We used Hugging Face as our model registry, as we can leverage its ecosystem
    to easily share our fine-tuned LLM Twin models with anyone who reads the book.
    Also, by following the Hugging Face model registry interface, we can easily integrate
    the model with all the frameworks around the LLMs ecosystem, such as Unsloth for
    fine-tuning and SageMaker for inference.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Hugging Face 作为我们的模型注册库，因为我们可以利用其生态系统轻松地将我们的微调 LLM Twin 模型与阅读本书的任何人共享。此外，通过遵循
    Hugging Face 模型注册库界面，我们可以轻松地将模型与 LLM 生态系统周围的各个框架集成，例如用于微调的 Unsloth 和用于推理的 SageMaker。
- en: 'Our fine-tuned LLMs are available on Hugging Face at:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们微调的 LLM 可在 Hugging Face 上找到：
- en: '**TwinLlama 3.1 8B** (after fine-tuning): [https://huggingface.co/mlabonne/TwinLlama-3.1-8B](https://huggingface.co/mlabonne/TwinLlama-3.1-8B)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TwinLlama 3.1 8B** (在微调之后): [https://huggingface.co/mlabonne/TwinLlama-3.1-8B](https://huggingface.co/mlabonne/TwinLlama-3.1-8B)'
- en: '**TwinLlama 3.1 8B DPO** (after preference alignment): [https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO](https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TwinLlama 3.1 8B DPO** (在偏好对齐之后): [https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO](https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO)'
- en: '![](img/B31105_02_01.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_02_01.png)'
- en: 'Figure 2.1: Hugging Face model registry example'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：Hugging Face 模型注册库示例
- en: 'For a quick demo, we have them available on Hugging Face Spaces:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速演示，我们在 Hugging Face Spaces 上提供了它们：
- en: '**TwinLlama 3.1 8B:** [https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B](https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TwinLlama 3.1 8B:** [https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B](https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B)'
- en: '**TwinLlama 3.1 8B DPO:** [https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B-DPO](https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B-DPO)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TwinLlama 3.1 8B DPO:** [https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B-DPO](https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B-DPO)'
- en: Most ML tools provide model registry features. For example, ZenML, Comet, and
    SageMaker, which we will present in future sections, also offer their own model
    registries. They are good options, but we picked Hugging Face solely because of
    its ecosystem, which provides easy shareability and integration throughout the
    open-source environment. Thus, you will usually select the model registry that
    integrates the most with your project’s tooling and requirements.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习工具都提供模型注册功能。例如，ZenML、Comet 和 SageMaker（我们将在未来章节中介绍），也提供自己的模型注册功能。它们是不错的选择，但我们选择
    Hugging Face 完全是因为其生态系统，它为开源环境提供了易于共享和集成的功能。因此，您通常会选择与您的项目工具和需求集成度最高的模型注册。
- en: 'ZenML: orchestrator, artifacts, and metadata'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ZenML：编排器、工件和元数据
- en: ZenML acts as the bridge between ML and MLOps. Thus, it offers multiple MLOps
    features that make your ML pipeline traceability, reproducibility, deployment,
    and maintainability easier. At its core, it is designed to create reproducible
    workflows in machine learning. It addresses the challenge of transitioning from
    exploratory research in Jupyter notebooks to a production-ready ML environment.
    It tackles production-based replication issues, such as versioning difficulties,
    reproducing experiments, organizing complex ML workflows, bridging the gap between
    training and deployment, and tracking metadata. Thus, ZenML’s main features are
    orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and
    attaching metadata to artifacts for better observability.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ZenML 作为机器学习和 MLOps 之间的桥梁。因此，它提供了多个 MLOps 功能，使您的机器学习管道的可追溯性、可重复性、部署和维护变得更加容易。在核心上，它旨在创建可重复的机器学习工作流程。它解决了从
    Jupyter 笔记本中的探索性研究过渡到生产就绪的机器学习环境的问题。它解决了基于生产的复制问题，如版本控制困难、重现实验、组织复杂的机器学习工作流程、弥合训练和部署之间的差距以及跟踪元数据。因此，ZenML
    的主要功能是编排机器学习管道、将机器学习管道作为输出存储和版本控制，以及将元数据附加到工件以实现更好的可观察性。
- en: 'Instead of being another ML platform, ZenML introduced the concept of a *stack,*
    which allows you to run ZenML on multiple infrastructure options. A stack will
    enable you to connect ZenML to different cloud services, such as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ZenML 不是另一个机器学习平台，而是引入了 *堆栈* 的概念，这使得您可以在多个基础设施选项上运行 ZenML。堆栈将使您能够将 ZenML 连接到不同的云服务，例如：
- en: An orchestrator and compute engine (for example, AWS SageMaker or Vertex AI)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个编排器和计算引擎（例如，AWS SageMaker 或 Vertex AI）
- en: Remote storage (for instance, AWS S3 or Google Cloud Storage buckets)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 远程存储（例如，AWS S3 或 Google Cloud Storage 存储桶）
- en: A container registry (for example, Docker Registry or AWS ECR)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个容器注册库（例如，Docker Registry 或 AWS ECR）
- en: 'Thus, ZenML acts as a glue that brings all your infrastructure and tools together
    in one place through its *stack* feature, allowing you to quickly iterate through
    your development processes and easily monitor your entire ML system. The beauty
    of this is that ZenML doesn’t vendor-lock you into any cloud platform. It completely
    abstracts away the implementation of your Python code from the infrastructure
    it runs on. For example, in our LLM Twin use case, we used the AWS stack:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ZenML 通过其 *堆栈* 功能充当粘合剂，将您的所有基础设施和工具集中在一个地方，让您能够快速迭代开发过程，并轻松监控整个机器学习系统。这种做法的美妙之处在于
    ZenML 不会将您锁定在任何一个云平台上。它完全抽象化了您的 Python 代码的实现与其运行的基础设施。例如，在我们的 LLM Twin 用例中，我们使用了
    AWS 堆栈：
- en: SageMaker as our orchestrator and compute
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SageMaker 作为我们的编排器和计算引擎
- en: S3 as our remote storage used to store and track artifacts
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3 作为我们用于存储和跟踪工件的远程存储
- en: ECR as our container registry
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ECR 作为我们的容器注册库
- en: 'However, the Python code contains no S3 or ECR particularities, as ZenML takes
    care of them. Thus, we can easily switch to other providers, such as Google Cloud
    Storage or Azure. For more details on ZenML *stacks*, you can start here: [https://docs.zenml.io/user-guide/production-guide/understand-stacks](https://docs.zenml.io/user-guide/production-guide/understand-stacks).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Python 代码不包含 S3 或 ECR 特殊性，因为 ZenML 会处理这些。因此，我们可以轻松切换到其他提供商，例如 Google Cloud
    Storage 或 Azure。有关 ZenML *堆栈* 的更多详细信息，您可以从这里开始：[https://docs.zenml.io/user-guide/production-guide/understand-stacks](https://docs.zenml.io/user-guide/production-guide/understand-stacks)。
- en: 'We will focus only on the ZenML features used throughout the book, such as
    orchestrating, artifacts, and metadata. For more details on ZenML, check out their
    starter guide: [https://docs.zenml.io/user-guide/starter-guide](https://docs.zenml.io/user-guide/starter-guide).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于本书中使用的 ZenML 功能，例如编排、工件和元数据。有关 ZenML 的更多详细信息，请参阅他们的入门指南：[https://docs.zenml.io/user-guide/starter-guide](https://docs.zenml.io/user-guide/starter-guide)。
- en: The local version of the ZenML server comes installed as a Python package. Thus,
    when running `poetry install`, it installs a ZenML debugging server that you can
    use locally. In *Chapter 11*, we will show you how to use their cloud serverless
    option to deploy the ML pipelines to AWS.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ZenML服务器的本地版本作为Python包安装。因此，当运行`poetry install`时，它会安装一个ZenML调试服务器，你可以在本地使用。在*第11章*中，我们将向你展示如何使用他们的云无服务器选项将机器学习管道部署到AWS。
- en: Orchestrator
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管道编排器
- en: An orchestrator is a system that automates, schedules, and coordinates all your
    ML pipelines. It ensures that each pipeline—such as data ingestion, preprocessing,
    model training, and deployment—executes in the correct order and handles dependencies
    efficiently. By managing these processes, an orchestrator optimizes resource utilization,
    handles failures gracefully, and enhances scalability, making complex ML pipelines
    more reliable and easier to manage.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 管道编排器是一个自动化、调度和协调所有机器学习管道的系统。它确保每个管道（如数据摄取、预处理、模型训练和部署）按正确顺序执行并有效地处理依赖关系。通过管理这些流程，编排器优化资源利用，优雅地处理故障，并增强可扩展性，使复杂的机器学习管道更可靠且易于管理。
- en: '**How does ZenML work as an orchestrator?** It works with **pipelines** and
    **steps**. A pipeline is a high-level object that contains multiple steps. A function
    becomes a ZenML pipeline by being decorated with `@pipeline`, and a step when
    decorated with `@step`. This is a standard pattern when using orchestrators: you
    have a high-level function, often called a pipeline, that calls multiple units/steps/tasks.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZenML作为编排器是如何工作的？**它与**管道**和**步骤**一起工作。管道是一个包含多个步骤的高级对象。一个函数通过装饰`@pipeline`成为ZenML管道，一个步骤通过装饰`@step`。这是使用编排器时的标准模式：你有一个高级函数，通常称为管道，它调用多个单元/步骤/任务。'
- en: 'Let’s explore how we can implement a ZenML pipeline with one of the ML pipelines
    implemented for the LLM Twin project. In the code snippet below, we defined a
    ZenML pipeline that queries the database for a user based on its full name and
    crawls all the provided links under that user:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索如何使用LLM Twin项目实现的机器学习管道之一来实现ZenML管道。在下面的代码片段中，我们定义了一个ZenML管道，该管道根据用户的完整名称查询数据库，并爬取该用户下提供的所有链接：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can run the pipeline with the following CLI command: `poetry poe run-digital-data-etl`.
    To visualize the pipeline run, you can go to your ZenML dashboard (at `http://127.0.0.1:8237/`)
    and, on the left panel, click on the **Pipelines** tab and then on the **digital_data_etl**
    pipeline, as illustrated in *Figure 2.2*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下CLI命令运行管道：`poetry poe run-digital-data-etl`。要可视化管道运行，你可以访问你的ZenML仪表板（在`http://127.0.0.1:8237/`），然后在左侧面板中点击**管道**标签，然后点击**digital_data_etl**管道，如图*图2.2*所示：
- en: '![](img/B31105_02_02.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_02_02.png)'
- en: 'Figure 2.2: ZenML Pipelines dashboard'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：ZenML管道仪表板
- en: After clicking on the **digital_data_etl** pipeline, you can visualize all the
    previous and current pipeline runs, as seen in *Figure 2.3*. You can see which
    one succeeded, failed, or is still running. Also, you can see the stack used to
    run the pipeline, where the default stack is the one used to run your ML pipelines
    locally.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**digital_data_etl**管道后，你可以可视化所有之前的和当前的管道运行，如图*图2.3*所示。你可以看到哪个成功了，失败了，或者仍在运行。此外，你还可以看到用于运行管道的堆栈，其中默认堆栈是用于本地运行你的机器学习管道的堆栈。
- en: '![](img/B31105_02_03.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_02_03.png)'
- en: 'Figure 2.3: ZenML digital_data_etl pipeline dashboard. Example of a specific
    pipeline'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：ZenML digital_data_etl管道仪表板。特定管道的示例
- en: Now, after clicking on the latest **digital_data_etl** pipeline run (or any
    other run that succeeded or is still running), we can visualize the pipeline’s
    steps, outputs, and insights, as illustrated in *Figure 2.4*. This structure is
    often called a **directed acyclic graph** (**DAG**). More on DAGs in *Chapter
    11*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在点击最新的**digital_data_etl**管道运行（或任何其他成功运行或仍在运行的运行）后，我们可以可视化管道的步骤、输出和见解，如图*图2.4*所示。这种结构通常被称为**有向无环图**（DAG）。关于DAG的更多内容请见*第11章*。
- en: '![](img/B31105_02_04.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_02_04.png)'
- en: 'Figure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific
    pipeline run)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：ZenML digital_data_etl管道运行仪表板（特定管道运行的示例）
- en: By clicking on a specific step, you can get more insights into its code and
    configuration. It even aggregates the logs output by that specific step to avoid
    switching between tools, as shown in *Figure 2.5*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击特定的步骤，你可以更深入地了解其代码和配置。它甚至聚合了该特定步骤输出的日志，以避免在工具之间切换，如图*图2.5*所示。
- en: '![](img/B31105_02_05.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_02_05.png)'
- en: 'Figure 2.5: Example of insights from a specific step of the digital_data_etl
    pipeline run'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：数字数据 ETL 管道运行特定步骤的见解示例
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Within a ZenML step, you can define any Python logic your use case needs. In
    this simple example, we are just creating or retrieving a user, but we could replace
    that code with anything, starting from data collection to feature engineering
    and training. What is essential to notice is that to integrate ZenML with your
    code, you have to write modular code, where each function does just one thing.
    The modularity of your code makes it easy to decorate your functions with `@step`
    and then glue multiple steps together within a main function decorated with `@pipeline`.
    One design choice that will impact your application is deciding the granularity
    of each step, as each will run as a different unit on a different machine when
    deployed in the cloud.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ZenML 步骤中，你可以定义你的用例需要的任何 Python 逻辑。在这个简单的例子中，我们只是创建或检索一个用户，但我们可以用任何东西替换这段代码，从数据收集到特征工程和训练。重要的是要注意，为了将
    ZenML 集成到你的代码中，你必须编写模块化代码，其中每个函数只做一件事。你代码的模块化使得你可以用 `@step` 装饰你的函数，然后在用 `@pipeline`
    装饰的主函数中粘合多个步骤变得容易。一个会影响你应用程序的设计选择是决定每个步骤的粒度，因为每个步骤在部署到云中时都会作为不同的单元在不同的机器上运行。
- en: To decouple our code from ZenML, we encapsulated all the application and domain
    logic into the `llm_engineering` Python module. We also defined the `pipelines`
    and `steps` folders, where we defined our ZenML logic. Within the `steps` module,
    we only used what we needed from the `llm_engineering` Python module (similar
    to how you use a Python package). In the `pipelines` module, we only aggregated
    ZenML steps to glue them into the final pipeline. Using this design, we can easily
    swap ZenML with another orchestrator or use our application logic in other use
    cases, such as a REST API. We only have to replace the ZenML code without touching
    the `llm_engineering` module where all our logic resides.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的代码与 ZenML 解耦，我们将所有应用程序和领域逻辑封装到了 `llm_engineering` Python 模块中。我们还定义了 `pipelines`
    和 `steps` 文件夹，在其中定义了我们的 ZenML 逻辑。在 `steps` 模块中，我们只使用了从 `llm_engineering` Python
    模块中需要的内容（类似于你使用 Python 包的方式）。在 `pipelines` 模块中，我们只聚合 ZenML 步骤以将它们粘合到最终的管道中。使用这种设计，我们可以轻松地用另一个编排器替换
    ZenML，或者在其他用例中使用我们的应用程序逻辑，例如 REST API。我们只需替换 ZenML 代码，而不必触及存放所有逻辑的 `llm_engineering`
    模块。
- en: 'This folder structure is reflected at the root of the LLM-Engineers-Handbook
    repository, as illustrated in *Figure 2.6*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件夹结构在 LLM-Engineers-Handbook 仓库的根目录中体现，如图 2.6 所示：
- en: '![](img/B31105_02_06.png)*Figure 2.6*: LLM-Engineers-Handbook repository folder
    structure'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B31105_02_06.png)*图 2.6*：LLM-Engineers-Handbook 仓库文件夹结构'
- en: One last thing to consider when writing ZenML steps is that if you return a
    value, it should be serializable. ZenML can serialize most objects that can be
    reduced to primitive data types, but there are a few exceptions. For example,
    we used UUID types as IDs throughout the code, which aren’t natively supported
    by ZenML. Thus, we had to extend ZenML’s materializer to support UUIDs. We raised
    this issue to ZenML. Hence, in future ZenML versions, UUIDs will be supported,
    but it was an excellent example of the serialization aspect of transforming function
    outputs in artifacts.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写 ZenML 步骤时需要考虑的最后一点是，如果你返回一个值，它应该是可序列化的。ZenML 可以序列化大多数可以减少为基本数据类型的对象，但有一些例外。例如，我们在整个代码中使用了
    UUID 类型作为 ID，这些类型不是 ZenML 本地支持的。因此，我们必须扩展 ZenML 的材料化器以支持 UUID。我们将这个问题提交给了 ZenML。因此，在未来的
    ZenML 版本中，UUID 将得到支持，但这是一个很好的例子，说明了在艺术品中转换函数输出的序列化方面。
- en: Artifacts and metadata
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 艺术品和元数据
- en: 'As mentioned in the previous section, ZenML transforms any step output into
    an artifact. First, let’s quickly understand what an artifact is. In MLOps, an
    **artifact** is any file(s) produced during the machine learning lifecycle, such
    as datasets, trained models, checkpoints, or logs. Artifacts are crucial for reproducing
    experiments and deploying models. We can transform anything into an artifact.
    For example, the model registry is a particular use case for an artifact. Thus,
    artifacts have these unique properties: they are versioned, sharable, and have
    metadata attached to them to understand what’s inside quickly. For example, when
    wrapping your dataset with an artifact, you can add to its metadata the size of
    the dataset, the train-test split ratio, the size, types of labels, and anything
    else useful to understand what’s inside the dataset without actually downloading
    it.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，ZenML将任何步骤输出转换为工件。首先，让我们快速了解什么是工件。在MLOps中，**工件**是指在机器学习生命周期中产生的任何文件（s），例如数据集、训练模型、检查点或日志。工件对于重现实验和部署模型至关重要。我们可以将任何东西转换为工件。例如，模型注册表是工件的一个特定用例。因此，工件具有这些独特的属性：它们是版本化的、可共享的，并且附有元数据，以便快速了解其内容。例如，当用工件包装您的数据集时，您可以在其元数据中添加数据集的大小、训练-测试分割比率、大小、标签类型以及其他任何有助于了解数据集内容的有用信息，而无需实际下载它。
- en: Let’s circle back to our **digital_data_etl** pipeline example, where we had
    as a step output an artifact, the crawled links, which are an artifact, as seen
    in *Figure 2.7*
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的**digital_data_etl**管道示例，其中我们作为步骤输出一个工件，即爬取的链接，这是一个工件，如图*图2.7*所示
- en: '![](img/B31105_02_07.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_02_07.png)'
- en: 'Figure 2.7: ZenML artifact example using the digital_data_etl pipeline as an
    example'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：使用digital_data_etl管道作为示例的ZenML工件示例
- en: 'By clicking on the `crawled_links` artifact and navigating to the **Metadata**
    tab, we can quickly see all the domains we crawled for a particular author, the
    number of links we crawled for each domain, and how many were successful, as illustrated
    in *Figure 2.8*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击`crawled_links`工件并导航到**元数据**标签，我们可以快速查看我们为特定作者爬取的所有域名，每个域名爬取的链接数量，以及成功的数量，如图*图2.8*所示：
- en: '![](img/B31105_02_08.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_02_08.png)'
- en: 'Figure 2.8: ZenML metadata example using the digital_data_etl pipeline as an
    example'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：使用digital_data_etl管道作为示例的ZenML元数据示例
- en: A more interesting example of an artifact and its metadata is the generated
    dataset artifact. In *Figure 2.9*, we can visualize the metadata of the `instruct_datasets`
    artifact, which was automatically generated and will be used to fine-tune the
    LLM Twin model. More details on the `instruction datasets` are in *Chapter 5*.
    For now, we want to highlight that within the dataset’s metadata, we have precomputed
    a lot of helpful information about it, such as how many data categories it contains,
    its storage size, and the number of samples per training and testing split.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更有趣的工件及其元数据的例子是生成的数据集工件。在*图2.9*中，我们可以可视化`instruct_datasets`工件的元数据，该工件是自动生成的，将被用于微调LLM
    Twin模型。关于`指令数据集`的更多细节请见*第五章*。现在，我们想强调的是，在数据集的元数据中，我们已经预先计算了关于它的许多有用信息，例如它包含多少数据类别，其存储大小，以及每个训练和测试分割的样本数量。
- en: '![](img/B31105_02_09.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_02_09.png)'
- en: 'Figure 2.9: ZenML metadata example for the instruct_datasets artifact'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：instruct_datasets工件的ZenML元数据示例
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Also, you can easily download and access a specific version of the dataset
    using its **Universally Unique Identifier** (**UUID**), which you can find using
    the ZenML dashboard or CLI:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以使用其**通用唯一标识符**（**UUID**）轻松下载和访问数据集的特定版本，您可以使用ZenML仪表板或CLI找到该UUID：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The last step in exploring ZenML is understanding how to run and configure a
    ZenML pipeline.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 探索ZenML的最后一个步骤是了解如何运行和配置ZenML管道。
- en: How to run and configure a ZenML pipeline
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何运行和配置ZenML管道
- en: 'All the ZenML pipelines can be called from the `run.py` file, accessed at `tools/run.py`
    in our GitHub repository. Within the `run.py` file, we implemented a simple CLI
    that allows you to specify what pipeline to run. For example, to call the `digital_data_etl`
    pipeline to crawl Maxime’s content, you have to run:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的ZenML管道都可以从`run.py`文件中调用，该文件位于我们的GitHub仓库中的`tools/run.py`。在`run.py`文件中，我们实现了一个简单的命令行界面（CLI），允许您指定要运行的管道。例如，要调用`digital_data_etl`管道以爬取Maxime的内容，您必须运行：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Or, to crawl Paul’s content, you can run:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，要爬取Paul的内容，您可以运行：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As explained when introducing Poe the Poet, all our CLI commands used to interact
    with the project will be executed through Poe to simplify and standardize the
    project. Thus, we encapsulated these Python calls under the following `poe` CLI
    commands:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在介绍诗人Poe时解释的那样，我们用于与项目交互的所有CLI命令将通过Poe执行，以简化并标准化项目。因此，我们将这些Python调用封装在以下`poe`
    CLI命令中：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We only change the ETL config file name when scraping content for different
    people. ZenML allows us to inject specific configuration files at runtime as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只在抓取不同人的内容时更改ETL配置文件名。ZenML允许我们在运行时注入特定的配置文件，如下所示：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the config file, we specify all the parameters that will input the pipeline
    as parameters. For example, the `configs/digital_data_etl_maxime_labonne.yaml`
    configuration file looks as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件中，我们指定所有将输入管道的参数。例如，`configs/digital_data_etl_maxime_labonne.yaml`配置文件如下所示：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Where the `digital_data_etl` function signature looks like this:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`digital_data_etl`函数签名看起来如下：'
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This approach allows us to configure each pipeline at runtime without modifying
    the code. We can also clearly track the inputs for all our pipelines, ensuring
    reproducibility. As seen in *Figure 2.10*, we have one or more configs for each
    pipeline.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许我们在运行时配置每个管道，而不需要修改代码。我们还可以清楚地跟踪所有管道的输入，确保可重复性。如*图2.10*所示，我们为每个管道有一个或多个配置。
- en: '![](img/B31105_02_10.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_02_10.png)'
- en: 'Figure 2.10: ZenML pipeline configs'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：ZenML管道配置
- en: Other popular orchestrators similar to ZenML that we’ve personally tested and
    consider powerful are Airflow, Prefect, Metaflow, and Dagster. Also, if you are
    a heavy user of Kubernetes, you can opt for Agro Workflows or Kubeflow, the latter
    of which works only on top of Kubernetes. We still consider ZenML the best trade-off
    between ease of use, features, and costs. Also, none of these tools offer the
    stack feature that is offered by ZenML, which allows it to avoid vendor-locking
    you in to any cloud ecosystem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们亲自测试过并认为功能强大的类似ZenML的流行编排器包括Airflow、Prefect、Metaflow和Dagster。此外，如果你是Kubernetes的重度用户，可以选择Agro
    Workflows或Kubeflow，后者仅在Kubernetes之上运行。我们仍然认为ZenML在易用性、功能和成本之间提供了最佳权衡。此外，这些工具中没有哪一个提供ZenML所提供的堆栈功能，这允许它避免将你锁定在任何云生态系统中。
- en: In *Chapter 11*, we will explore in more depth how to leverage an orchestrator
    to implement MLOps best practices. But now that we understand ZenML, what it is
    helpful for, and how to use it, let’s move on to the experiment tracker.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第11章*中，我们将更深入地探讨如何利用编排器来实现MLOps的最佳实践。但现在我们了解了ZenML，它有什么帮助，以及如何使用它，让我们继续到实验跟踪器。
- en: 'Comet ML: experiment tracker'
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Comet ML：实验跟踪器
- en: Training ML models is an entirely iterative and experimental process. Unlike
    traditional software development, it involves running multiple parallel experiments,
    comparing them based on predefined metrics, and deciding which one should advance
    to production. An experiment tracking tool allows you to log all the necessary
    information, such as metrics and visual representations of your model predictions,
    to compare all your experiments and quickly select the best model. Our LLM project
    is no exception.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型是一个完全迭代和实验的过程。与传统的软件开发不同，它涉及运行多个并行实验，根据预定义的指标进行比较，并决定哪个应该进入生产。一个实验跟踪工具允许你记录所有必要的信息，例如模型预测的指标和可视化表示，以便比较所有实验并快速选择最佳模型。我们的LLM项目也不例外。
- en: As illustrated in *Figure 2.11*, we used Comet to track metrics such as training
    and evaluation loss or the value of the gradient norm across all our experiments.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图2.11*所示，我们使用Comet跟踪训练和评估损失或梯度范数的值等指标。
- en: '![](img/B31105_02_11.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_02_11.png)'
- en: 'Figure 2.11: Comet ML training metrics example'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：Comet ML训练指标示例
- en: Using an experiment tracker, you can go beyond training and evaluation metrics
    and log your training hyperparameters to track different configurations between
    experiments.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实验跟踪器，你可以超越训练和评估指标，并记录你的训练超参数以跟踪实验之间的不同配置。
- en: It also logs out-of-the-box system metrics such as GPU, CPU, or memory utilization
    to give you a clear picture of what resources you need during training and where
    potential bottlenecks slow down your training, as seen in *Figure 2.12*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 它还记录了开箱即用的系统指标，如GPU、CPU或内存利用率，以清楚地了解你在训练期间需要哪些资源以及潜在瓶颈在哪里减慢了你的训练速度，如*图2.12*所示。
- en: '![](img/B31105_02_12.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_02_12.png)'
- en: 'Figure 2.12: Comet ML system metrics example'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12：Comet ML系统指标示例
- en: 'You don’t have to set up Comet locally. We will use their online version for
    free without any constraints throughout this book. Also, if you want to look more
    in-depth into the Comet ML experiment tracker, we made the training experiments
    tracked with Comet ML public while fine-tuning our LLM Twin models. You can access
    them here: [https://www.comet.com/mlabonne/llm-twin-training/view/new/panels](https://www.comet.com/mlabonne/llm-twin-training/view/new/panels).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您无需在本地设置Comet。在本书中，我们将免费使用其在线版本，没有任何限制。此外，如果您想更深入地了解Comet ML实验跟踪器，我们在微调我们的LLM
    Twin模型时，已将使用Comet ML跟踪的训练实验公开。您可以通过以下链接访问：[https://www.comet.com/mlabonne/llm-twin-training/view/new/panels](https://www.comet.com/mlabonne/llm-twin-training/view/new/panels)。
- en: 'Other popular experiment trackers are W&B, MLflow, and Neptune. We’ve worked
    with all of them and can state that they all have mostly the same features, but
    Comet ML differentiates itself through its ease of use and intuitive interface.
    Let’s move on to the final piece of the MLOps puzzle: Opik for prompt monitoring.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 其他流行的实验跟踪器包括W&B、MLflow和Neptune。我们与它们都合作过，可以声明它们都具有大部分相同的功能，但Comet ML通过其易用性和直观界面脱颖而出。让我们继续到最后一个MLOps拼图部分：Opik用于提示监控。
- en: 'Opik: prompt monitoring'
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Opik：提示监控
- en: You cannot use standard tools and techniques when logging and monitoring prompts.
    The reason for this is complicated. We will dig into it in *Chapter 11*. However,
    to quickly give you some understanding, you cannot use standard logging tools
    as prompts are complex and unstructured chains.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在记录和监控提示时，您不能使用标准的工具和技术。原因很复杂。我们将在第11章中深入探讨。然而，为了快速让您了解一些情况，您不能使用标准的日志记录工具，因为提示是复杂且非结构化的链。
- en: When interacting with an LLM application, you chain multiple input prompts and
    the generated output into a trace, where one prompt depends on previous prompts.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLM应用交互时，您将多个输入提示和生成的输出链接成一个跟踪，其中一个提示依赖于前面的提示。
- en: Thus, instead of plain text logs, you need an intuitive way to group these traces
    into a specialized dashboard that makes debugging and monitoring traces of prompts
    easier.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您需要一种直观的方式来将这些跟踪分组到一个专门的仪表板中，以便更容易地调试和监控提示的跟踪。
- en: We used Opik, an open-source tool made by Comet, as our prompt monitoring tool
    because it follows Comet’s philosophy of simplicity and ease of use, which is
    currently relatively rare in the LLM landscape. Other options offering similar
    features are Langfuse (open source, [https://langfuse.com](https://langfuse.com)),
    Galileo (not open source, [rungalileo.io](https://rungalileo.io)), and LangSmith
    (not open source, [https://www.langchain.com/langsmith](https://www.langchain.com/langsmith)),
    but we found their solutions more cumbersome to use and implement. Opik, along
    with its serverless option, also provides a free open-source version that you
    have complete control over. You can read more on Opik at [https://github.com/comet-ml/opik](https://github.com/comet-ml/opik).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Comet制作的开源工具Opik作为我们的提示监控工具，因为它遵循Comet的简洁和易用性哲学，这在当前的LLM领域中相对罕见。提供类似功能的其他选项包括Langfuse（开源，[https://langfuse.com](https://langfuse.com)）、Galileo（非开源，[rungalileo.io](https://rungalileo.io)）和LangSmith（非开源，[https://www.langchain.com/langsmith](https://www.langchain.com/langsmith)），但我们发现它们的解决方案在使用和实施上更为繁琐。Opik及其无服务器选项还提供了一个完全由您控制的免费开源版本。您可以在[https://github.com/comet-ml/opik](https://github.com/comet-ml/opik)上了解更多关于Opik的信息。
- en: Databases for storing unstructured and vector data
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于存储非结构化和矢量数据的数据库
- en: We also want to present the NoSQL and vector databases we will use within our
    examples. When working locally, they are already integrated through Docker. Thus,
    when running `poetry poe local-infrastructure-up`, as instructed a few sections
    above, local images of Docker for both databases will be pulled and run on your
    machine. Also, when deploying the project, we will show you how to use their serverless
    option and integrate it with the rest of the LLM Twin project.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望展示我们将要在示例中使用到的NoSQL和矢量数据库。当在本地工作时，它们已经通过Docker集成。因此，当按照上面几个部分中的说明运行`poetry
    poe local-infrastructure-up`时，本地数据库的Docker镜像将被拉取并在您的机器上运行。此外，在部署项目时，我们将向您展示如何使用它们的无服务器选项并将其集成到LLM
    Twin项目的其余部分。
- en: 'MongoDB: NoSQL database'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MongoDB：NoSQL数据库
- en: MongoDB is one of today’s most popular, robust, fast, and feature-rich NoSQL
    databases. It integrates well with most cloud ecosystems, such as AWS, Google
    Cloud, Azure, and Databricks. Thus, using MongoDB as our NoSQL database was a
    no-brainer.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB是当今最受欢迎、最稳健、最快且功能丰富的NoSQL数据库之一。它与大多数云生态系统集成良好，如AWS、Google Cloud、Azure和Databricks。因此，将MongoDB作为我们的NoSQL数据库是一个显而易见的选择。
- en: When we wrote this book, MongoDB was used by big players such as Novo Nordisk,
    Delivery Hero, Okta, and Volvo. This widespread adoption suggests that MongoDB
    will remain a leading NoSQL database for a long time.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们撰写本书时，MongoDB被Novo Nordisk、Delivery Hero、Okta和Volvo等大公司使用。这种广泛的应用表明MongoDB将长期保持领先地位。
- en: We use MongoDB as a NoSQL database to store the raw data we collect from the
    internet before processing it and pushing it into the vector database. As we work
    with unstructured text data, the flexibility of the NoSQL database fits like a
    charm.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用MongoDB作为NoSQL数据库来存储我们从互联网收集的原始数据，在处理并推送到向量数据库之前。由于我们处理的是非结构化文本数据，NoSQL数据库的灵活性非常适合。
- en: 'Qdrant: vector database'
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Qdrant: 向量数据库'
- en: Qdrant ([https://qdrant.tech/](https://qdrant.tech/)) is one of the most popular,
    robust, and feature-rich vector databases. We could have used almost any vector
    database for our small MVP, but we wanted to pick something light and likely to
    be used in the industry for many years to come.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Qdrant ([https://qdrant.tech/](https://qdrant.tech/)) 是最受欢迎、最稳健且功能丰富的向量数据库之一。我们本来可以用几乎任何向量数据库来构建我们的小型MVP，但我们想选择一个轻量级且可能在未来许多年里在业界被广泛使用的数据库。
- en: We will use Qdrant to store the data from MongoDB after it’s processed and transformed
    for GenAI usability.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Qdrant来存储MongoDB处理和转换后的数据，以便用于GenAI的可用性。
- en: Qdrant is used by big players such as X (formerly Twitter), Disney, Microsoft,
    Discord, and Johnson & Johnson. Thus, it is highly probable that Qdrant will remain
    in the vector database game for a long time.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Qdrant被像X（前Twitter）、迪士尼、微软、Discord和强生这样的行业巨头所使用。因此，Qdrant很可能在向量数据库领域长期保持领先地位。
- en: While writing the book, other popular options were Milvus, Redis, Weaviate,
    Pinecone, Chroma, and pgvector (a PostgreSQL plugin for vector indexes). We found
    that Qdrant offers the best trade-off between RPS, latency, and index time, making
    it a solid choice for many generative AI applications.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，其他流行的选择包括Milvus、Redis、Weaviate、Pinecone、Chroma和pgvector（一个用于向量索引的PostgreSQL插件）。我们发现Qdrant在RPS、延迟和索引时间之间提供了最佳权衡，使其成为许多生成式AI应用的稳固选择。
- en: Comparing all the vector databases in detail could be a chapter in itself. We
    don’t want to do that here. Still, if curious, you can check the *Vector DB Comparison*
    resource from Superlinked at [https://superlinked.com/vector-db-comparison](https://superlinked.com/vector-db-comparison),
    which compares all the top vector databases in terms of everything you can think
    about, from the license and release year to database features, embedding models,
    and frameworks supported.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 详细比较所有向量数据库可能本身就是一个章节。我们不想在这里做这件事。不过，如果您好奇，可以查看Superlinked提供的*向量数据库比较*资源，[https://superlinked.com/vector-db-comparison](https://superlinked.com/vector-db-comparison)，它从许可证和发布年份到数据库功能、嵌入模型和支持的框架等各个方面比较了所有顶级向量数据库。
- en: Preparing for AWS
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备AWS
- en: This last part of the chapter will focus on setting up an AWS account (if you
    don’t already have one), an AWS access key, and the CLI. Also, we will look into
    what SageMaker is and why we use it.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一部分将专注于设置AWS账户（如果您还没有的话）、AWS访问密钥和CLI。我们还将探讨SageMaker是什么以及为什么我们使用它。
- en: We picked AWS as our cloud provider because it’s the most popular out there
    and the cloud in which we (the writers) have the most experience. The reality
    is that other big cloud providers, such as GCP or Azure, offer similar services.
    Thus, depending on your specific application, there is always a trade-off between
    development time (in which you have the most experience), features, and costs.
    But for our MVP, AWS, it’s the perfect option as it provides robust features for
    everything we need, such as S3 (object storage), ECR (container registry), and
    SageMaker (compute for training and inference).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择AWS作为我们的云服务提供商，因为它是最受欢迎的，并且是我们（作者）最有经验的领域。现实是，其他大型云服务提供商，如GCP或Azure，也提供类似的服务。因此，根据您的具体应用，在开发时间（您最有经验的时间）、功能和成本之间总会有权衡。但对我们的小型MVP来说，AWS是完美的选择，因为它为我们需要的所有功能提供了稳健的特性，如S3（对象存储）、ECR（容器注册库）和SageMaker（用于训练和推理的计算）。
- en: Setting up an AWS account, an access key, and the CLI
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置AWS账户、访问密钥和CLI
- en: 'As AWS could change its UI/UX, the best way to instruct you on how to create
    an AWS account is by redirecting you to their official tutorial: [https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.html](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.html).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 AWS 可能会更改其 UI/UX，指导您如何创建 AWS 账户的最佳方式是通过将其重定向到他们的官方教程：[https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.html](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.html)。
- en: After successfully creating an AWS account, you can access the AWS console at
    [http://console.aws.amazon.com](http://console.aws.amazon.com). Select **Sign
    in using root user email** (found under the **Sign in** button), then enter your
    account’s email address and password.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 成功创建 AWS 账户后，您可以通过 [http://console.aws.amazon.com](http://console.aws.amazon.com)
    访问 AWS 控制台。选择 **使用 root 用户邮箱登录**（位于 **登录** 按钮下方），然后输入您的账户邮箱地址和密码。
- en: 'Next, we must generate access keys to access AWS programmatically. The best
    option to do so is first to create an IAM user with administrative access as described
    in this AWS official tutorial: [https://docs.aws.amazon.com/streams/latest/dev/setting-up.html](https://docs.aws.amazon.com/streams/latest/dev/setting-up.html)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须生成访问密钥以程序化地访问 AWS。这样做最好的方法是首先创建一个具有管理访问权限的 IAM 用户，如本 AWS 官方教程中所述：[https://docs.aws.amazon.com/streams/latest/dev/setting-up.html](https://docs.aws.amazon.com/streams/latest/dev/setting-up.html)
- en: For production accounts, it is best practice to grant permissions with a policy
    of least privilege, giving each user only the permissions they require to perform
    their role. However, to simplify the setup of our test account, we will use the
    `AdministratorAccess` managed policy, which gives our user full access, as explained
    in the tutorial above and illustrated in *Figure 2.13*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产账户，最佳实践是以最小权限策略授予权限，为每个用户提供他们执行其角色所需的权限。然而，为了简化我们的测试账户的设置，我们将使用 `AdministratorAccess`
    管理策略，这将在上述教程中解释并在 *图 2.13* 中展示。
- en: '![](img/B31105_02_13.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_02_13.png)'
- en: 'Figure 2.13: IAM user permission policies example'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：IAM 用户权限策略示例
- en: 'Next, you have to create an access key for the IAM user you just created using
    the following tutorial: [https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您必须使用以下教程为您刚才创建的 IAM 用户创建一个访问密钥：[https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)。
- en: 'The access keys will look as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 访问密钥将如下所示：
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Just be careful to store them somewhere safe, as you won’t be able to access
    them after you create them. Also, be cautious with who you share them, as they
    could be used to access your AWS account and manipulate various AWS resources.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 只需小心地将它们存储在安全的地方，因为创建后您将无法访问它们。此外，与谁分享它们时要谨慎，因为它们可能被用来访问您的 AWS 账户并操纵各种 AWS 资源。
- en: 'The last step is to install the AWS CLI and configure it with your newly created
    access keys. You can install the AWS CLI using the following link: [https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是安装 AWS CLI 并使用您新创建的访问密钥进行配置。您可以通过以下链接安装 AWS CLI：[https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)。
- en: 'After installing the AWS CLI, you can configure it by running `aws configure`.
    Here is an example of our AWS configuration:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 AWS CLI 后，您可以通过运行 `aws configure` 来配置它。以下是我们 AWS 配置的示例：
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For more details on how to configure the AWS CLI, check out the following tutorial:
    [https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何配置 AWS CLI 的更多详细信息，请查看以下教程：[https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html](https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html)。
- en: 'Also, to configure the project with your AWS credentials, you must fill in
    the following variables within your `.env` file:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使用您的 AWS 凭据配置项目，您必须在您的 `.env` 文件中填写以下变量：
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**An important note about costs associated with hands-on tasks in this book**'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于本书中与动手实践任务相关的费用的重要说明**'
- en: All the cloud services used across the book stick to their freemium option,
    except AWS. Thus, if you use a personal AWS account, you will be responsible for
    AWS costs as you follow along in this book. While some services may fall under
    AWS Free Tier usage, others will not. Thus, you are responsible for checking your
    billing console regularly.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用的所有云服务都坚持其免费增值选项，除了 AWS。因此，如果您使用个人 AWS 账户，您将负责 AWS 成本，因为您在本书中跟随操作。虽然一些服务可能属于
    AWS 免费层使用，但其他服务则不属于。因此，您需要定期检查您的计费控制台。
- en: Most of the costs will come when testing SageMaker for training and inference.
    Based on our tests, the AWS costs can vary between $50 and $100 using the specifications
    provided in this book and repository.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分成本将在测试 SageMaker 用于训练和推理时产生。根据我们的测试，使用本书和存储库中提供的规范，AWS 成本可能在 50 美元到 100 美元之间变化。
- en: See the AWS documentation on setting up billing alarms to monitor your costs
    at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 AWS 文档中关于设置计费警报以监控您的成本的部分，链接为 [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html)。
- en: 'SageMaker: training and inference compute'
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker：训练和推理计算
- en: 'The last topic of this chapter is understanding SageMaker and why we decided
    to use it. SageMaker is an ML platform used to train and deploy ML models. An
    official definition is as follows: AWS SageMaker is a fully managed machine learning
    service by AWS that enables developers and data scientists to build, train, and
    deploy machine learning models at scale. It simplifies the process by handling
    the underlying infrastructure, allowing users to focus on developing high-quality
    models efficiently.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一个主题是理解 SageMaker 以及我们为什么决定使用它。SageMaker 是一个用于训练和部署机器学习模型的 ML 平台。官方定义如下：AWS
    SageMaker 是 AWS 提供的完全托管机器学习服务，使开发者和数据科学家能够以规模化的方式构建、训练和部署机器学习模型。它通过处理底层基础设施简化了流程，使用户能够专注于高效地开发高质量模型。
- en: We will use SageMaker to fine-tune and operationalize our training pipeline
    on clusters of GPUs and to deploy our custom LLM Twin model as a REST API that
    can be accessed in real time from anywhere in the world.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 SageMaker 在 GPU 集群上微调和操作我们的训练流程，并将我们的自定义 LLM Twin 模型作为 REST API 部署，该 API
    可从世界任何地方实时访问。
- en: Why AWS SageMaker?
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择 AWS SageMaker？
- en: We must also discuss why we chose AWS SageMaker over simpler and more cost-effective
    options, such as AWS Bedrock. First, let’s explain Bedrock and its benefits.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须讨论为什么我们选择 AWS SageMaker 而不是更简单、成本效益更高的选项，例如 AWS Bedrock。首先，让我们解释 Bedrock
    及其优势。
- en: Amazon Bedrock is a serverless solution for deploying LLMs. Serverless means
    that there are no servers or infrastructure to manage. It provides pre-trained
    models, which you can access directly through API calls. When we wrote this book,
    they provided support only for Mistral, Flan, Llama 2, and Llama 3 (quite a limited
    list of options). You can send input data and receive predictions from the models
    without managing the underlying infrastructure or software. This approach significantly
    reduces the complexity and time required to integrate AI capabilities into applications,
    making it more accessible to developers with limited machine learning expertise.
    However, this ease of integration comes at the cost of limited customization options,
    as you’re restricted to the pre-trained models and APIs provided by Amazon Bedrock.
    In terms of pricing, Bedrock uses a simple pricing model based on the number of
    API calls. This straightforward pricing structure makes it more efficient to estimate
    and control costs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Bedrock 是一种无服务器解决方案，用于部署大型语言模型（LLMs）。无服务器意味着没有服务器或基础设施需要管理。它提供了预训练的模型，您可以通过
    API 调用直接访问。当我们编写这本书时，它们只提供了 Mistral、Flan、Llama 2 和 Llama 3 的支持（选项列表相当有限）。您可以直接向模型发送输入数据并接收预测，而无需管理底层基础设施或软件。这种方法显著降低了将
    AI 功能集成到应用程序中的复杂性和时间，使得对机器学习专业知识有限的开发者更容易使用。然而，这种易于集成的便利性是以有限的定制选项为代价的，因为您被限制在
    Amazon Bedrock 提供的预训练模型和 API。在定价方面，Bedrock 使用基于 API 调用次数的简单定价模型。这种简单的定价结构使得成本估算和控制更加高效。
- en: Meanwhile, SageMaker provides a comprehensive platform for building, training,
    and deploying machine learning models. It allows you to customize your ML processes
    entirely or even use the platform for research. That’s why SageMaker is mainly
    used by data scientists and machine learning experts who know how to program,
    understand machine learning concepts, and are comfortable working with cloud platforms
    such as AWS. SageMaker is a double-edged sword regarding costs, following a pay-as-you-go
    pricing model similar to most AWS services. This means you have to pay for the
    usage of computing resources, storage, and any other services required to build
    your applications.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，SageMaker 提供了一个全面的平台，用于构建、训练和部署机器学习模型。它允许您完全定制您的 ML 流程，甚至可以使用该平台进行研究。这就是为什么
    SageMaker 主要被数据科学家和机器学习专家使用，他们知道如何编程，理解机器学习概念，并且熟悉像 AWS 这样的云平台。在成本方面，SageMaker
    是一把双刃剑，采用类似于大多数 AWS 服务的按使用付费定价模式。这意味着您必须为计算资源的使用、存储以及构建应用程序所需的任何其他服务付费。
- en: In contrast to Bedrock, even if the SageMaker endpoint is not used, you will
    still pay for the deployed resources on AWS, such as online EC2 instances. Thus,
    you have to design autoscaling systems that delete unused resources. To conclude,
    Bedrock offers an out-of-the-box solution that allows you to quickly deploy an
    API endpoint powered by one of the available foundation models. Meanwhile, SageMaker
    is a multi-functional platform enabling you to customize your ML logic fully.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Bedrock 相比，即使 SageMaker 端点未被使用，您仍需为 AWS 上部署的资源付费，例如在线 EC2 实例。因此，您必须设计自动扩展系统来删除未使用的资源。总之，Bedrock
    提供了一个即用型的解决方案，允许您快速部署由可用基础模型驱动的 API 端点。同时，SageMaker 是一个多功能平台，使您能够完全定制您的 ML 逻辑。
- en: So why did we choose SageMaker over Bedrock? Bedrock would have been an excellent
    solution for quickly prototyping something, but this is a book on LLM engineering,
    and our goal is to dig into all the engineering aspects that Bedrock tries to
    mask away. Thus, we chose SageMaker because of its high level of customizability,
    allowing us to show you all the engineering required to deploy a model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么我们选择了 SageMaker 而不是 Bedrock？Bedrock 对于快速原型设计来说是一个极好的解决方案，但这是一本关于 LLM 工程的书，我们的目标是深入挖掘
    Bedrock 尝试掩盖的所有工程方面。因此，我们选择了 SageMaker，因为它具有高度的定制性，使我们能够向您展示部署模型所需的全部工程。
- en: In reality, even SageMaker isn’t fully customizable. If you want complete control
    over your deployment, use EKS, AWS’s Kubernetes self-managed service. In this
    case, you have direct access to the virtual machines, allowing you to fully customize
    how you build your ML pipelines, how they interact, and how you manage your resources.
    You could do the same thing with AWS ECS, AWS’s version of Kubernetes. Using EKS
    or ECS, you could also reduce the costs, as these services cost considerably less.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使是 SageMaker 也不是完全可定制的。如果您希望对部署有完全的控制权，请使用 EKS，这是 AWS 的 Kubernetes 自托管服务。在这种情况下，您可以直接访问虚拟机，允许您完全自定义构建您的
    ML 管道的方式、它们如何交互以及您如何管理资源。您也可以使用 AWS ECS，这是 AWS 的 Kubernetes 版本。使用 EKS 或 ECS，您还可以降低成本，因为这些服务的成本要低得多。
- en: To conclude, SageMaker strikes a balance between complete control and customization
    and a fully managed service that hides all the engineering complexity behind the
    scenes. This balance ensures that you have the control you need while also benefiting
    from the managed service’s convenience.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，SageMaker 在完全控制和定制与完全托管服务之间取得了平衡，后者隐藏了所有工程复杂性。这种平衡确保了您拥有所需的控制权，同时也能享受到托管服务的便利性。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we reviewed the core tools used across the book. First, we
    understood how to install the correct version of Python that supports our repository.
    Then, we looked over how to create a virtual environment and install all the dependencies
    using Poetry. Finally, we understood how to use a task execution tool like Poe
    the Poet to aggregate all the commands required to run the application.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了本书中使用的核心工具。首先，我们了解了如何安装支持我们仓库的正确版本的 Python。然后，我们查看了如何使用 Poetry 创建虚拟环境并安装所有依赖项。最后，我们了解了如何使用像
    Poe the Poet 这样的任务执行工具来聚合运行应用程序所需的全部命令。
- en: The next step was to review all the tools used to ensure MLOps best practices,
    such as a model registry to share our models, an experiment tracker to manage
    our training experiments, an orchestrator to manage all our ML pipelines and artifacts,
    and metadata to manage all our files and datasets. We also understood what type
    of databases we need to implement the LLM Twin use case. Finally, we explored
    the process of setting up an AWS account, generating an access key, and configuring
    the AWS CLI for programmatic access to the AWS cloud. We also gained a deep understanding
    of AWS SageMaker and the reasons behind choosing it to build our LLM Twin application.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是审查所有使用的工具，以确保MLOps最佳实践，例如用于共享我们模型的模型注册、用于管理我们的训练实验的实验跟踪器、用于管理所有我们的ML管道和工件的编排器，以及元数据来管理所有我们的文件和数据集。我们还了解了需要哪种类型的数据库来实现LLM
    Twin用例。最后，我们探索了设置AWS账户、生成访问密钥和配置AWS CLI以进行程序化访问AWS云的过程。我们还对AWS SageMaker及其选择用于构建我们的LLM
    Twin应用程序的原因有了深入的了解。
- en: In the next chapter, we will explore the implementation of the LLM Twin project
    by starting with the data collection ETL that scrapes posts, articles, and repositories
    from the internet and stores them in a data warehouse.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从数据收集ETL开始，该ETL从互联网上抓取帖子、文章和存储库，并将它们存储在数据仓库中，来探索LLM Twin项目的实现。
- en: References
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Acsany, P. (2024, February 19). *Dependency Management With Python Poetry*.
    [https://realpython.com/dependency-management-python-poetry/](https://realpython.com/dependency-management-python-poetry/)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Acsany, P. (2024年2月19日). *使用Python Poetry进行依赖管理*. [https://realpython.com/dependency-management-python-poetry/](https://realpython.com/dependency-management-python-poetry/)
- en: 'Comet.ml. (n.d.). *comet-ml/opik: Open-source end-to-end LLM Development Platform*.
    GitHub. [https://github.com/comet-ml/opik](https://github.com/comet-ml/opik)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comet.ml. (未注明). *comet-ml/opik：开源端到端LLM开发平台*. GitHub. [https://github.com/comet-ml/opik](https://github.com/comet-ml/opik)
- en: 'Czakon, J. (2024, September 25). *ML Experiment Tracking: What It Is, Why It
    Matters, and How to Implement It*. neptune.ai. [https://neptune.ai/blog/ml-experiment-tracking](https://neptune.ai/blog/ml-experiment-tracking)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czakon, J. (2024年9月25日). *ML实验跟踪：它是什麼，为什么它很重要，以及如何实现它*. neptune.ai. [https://neptune.ai/blog/ml-experiment-tracking](https://neptune.ai/blog/ml-experiment-tracking)
- en: Hopsworks. (n.d.). *ML Artifacts (ML Assets)?* Hopsworks. [https://www.hopsworks.ai/dictionary/ml-artifacts](https://www.hopsworks.ai/dictionary/ml-artifacts)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopsworks. (未注明). *ML工件（ML资产）？* Hopsworks. [https://www.hopsworks.ai/dictionary/ml-artifacts](https://www.hopsworks.ai/dictionary/ml-artifacts)
- en: '*Introduction | Documentation | Poetry – Python dependency management and packaging
    made easy*. (n.d.). [https://python-poetry.org/docs](https://python-poetry.org/docs)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*介绍 | 文档 | Poetry – 使Python依赖管理和打包变得简单*. (未注明). [https://python-poetry.org/docs](https://python-poetry.org/docs)'
- en: Jones, L. (2024, March 21). *Managing Multiple Python Versions With pyenv*.
    [https://realpython.com/intro-to-pyenv/](https://realpython.com/intro-to-pyenv/)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jones, L. (2024年3月21日). *使用pyenv管理多个Python版本*. [https://realpython.com/intro-to-pyenv/](https://realpython.com/intro-to-pyenv/)
- en: Kaewsanmua, K. (2024, January 3). *Best Machine Learning Workflow and Pipeline
    Orchestration Tools*. neptune.ai. [https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools](https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaewsanmua, K. (2024年1月3日). *最佳机器学习工作流和管道编排工具*. neptune.ai. [https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools](https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools)
- en: MongoDB. (n.d.). *What is NoSQL?* NoSQL databases explained. [https://www.mongodb.com/resources/basics/databases/nosql-explained](https://www.mongodb.com/resources/basics/databases/nosql-explained)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB. (未注明). *什么是NoSQL？* NoSQL数据库解释. [https://www.mongodb.com/resources/basics/databases/nosql-explained](https://www.mongodb.com/resources/basics/databases/nosql-explained)
- en: 'Nat-N. (n.d.). *nat-n/poethepoet: A task runner that works well with poetry.*
    GitHub. [https://github.com/nat-n/poethepoet](https://github.com/nat-n/poethepoet)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nat-N. (未注明). *nat-n/poethepoet：一个与poetry配合良好的任务运行器*. GitHub. [https://github.com/nat-n/poethepoet](https://github.com/nat-n/poethepoet)
- en: 'Oladele, S. (2024, August 29). *ML Model Registry: The Ultimate Guide*. neptune.ai.
    [https://neptune.ai/blog/ml-model-registry](https://neptune.ai/blog/ml-model-registry)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oladele, S. (2024年8月29日). *ML模型注册：终极指南*. neptune.ai. [https://neptune.ai/blog/ml-model-registry](https://neptune.ai/blog/ml-model-registry)
- en: Schwaber-Cohen, R. (n.d.). *What is a Vector Database & How Does it Work? Use
    Cases + Examples*. Pinecone. [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwaber-Cohen, R. (未注明). *什么是向量数据库？它是如何工作的？用例+示例*. Pinecone. [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/)
- en: '*Starter guide | ZenML Documentation*. (n.d.). [https://docs.zenml.io/user-guide/starter-guide](https://docs.zenml.io/user-guide/starter-guide)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*入门指南 | ZenML 文档*. (n.d.). [https://docs.zenml.io/user-guide/starter-guide](https://docs.zenml.io/user-guide/starter-guide)'
- en: '*Vector DB Comparison*. (n.d.). [https://superlinked.com/vector-db-comparison](https://superlinked.com/vector-db-comparison)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量数据库比较*. (n.d.). [https://superlinked.com/vector-db-comparison](https://superlinked.com/vector-db-comparison)'
- en: Join our book’s Discord space
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code79969828252392890.png)'
