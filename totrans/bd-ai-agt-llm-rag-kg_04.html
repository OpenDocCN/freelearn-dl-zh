<html><head></head><body>
<div id="_idContainer132">
<h1 class="chapter-number" id="_idParaDest-59"><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-60"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.2.1">Building a Web Scraping Agent with an LLM</span></h1>
<p><span class="koboSpan" id="kobo.3.1">The French philosopher Denis Diderot said, “</span><em class="italic"><span class="koboSpan" id="kobo.4.1">If they find a parrot who could answer to everything, I would claim it to be an intelligent being without hesitation</span></em><span class="koboSpan" id="kobo.5.1">.” </span><span class="koboSpan" id="kobo.5.2">Diderot was referring to a parrot, but an LLM could be defined as a sophisticated parrot. </span><span class="koboSpan" id="kobo.5.3">There is a difference between </span><em class="italic"><span class="koboSpan" id="kobo.6.1">answering</span></em><span class="koboSpan" id="kobo.7.1"> a question and </span><em class="italic"><span class="koboSpan" id="kobo.8.1">understanding</span></em><span class="koboSpan" id="kobo.9.1"> a question. </span><span class="koboSpan" id="kobo.9.2">Therefore, today, we do not define LLMs as intelligent beings. </span><span class="koboSpan" id="kobo.9.3">They can though answer almost any question amazingly well. </span><span class="koboSpan" id="kobo.9.4">Despite such answering abilities, LLMs cannot perform an action. </span><span class="koboSpan" id="kobo.9.5">Therefore, attempts have been made to solve this main limitation with the use </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">of agents.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">An AI agent is an extension of an LLM, taking it from a language model toward a system that possesses capabilities such as autonomy, reactivity, pro-activeness, and social ability. </span><span class="koboSpan" id="kobo.11.2">This research has focused, on the one hand, on specific applications (such as mastering games such as chess or Go), and on the other, on more general abilities, such as memorization, long-term planning, generalization, and efficient interaction with the user. </span><span class="koboSpan" id="kobo.11.3">These are considered the first steps in the direction of </span><strong class="bold"><span class="koboSpan" id="kobo.12.1">artificial general intelligence</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.14.1">AGI</span></strong><span class="koboSpan" id="kobo.15.1">). </span><span class="koboSpan" id="kobo.15.2">According to </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.16.1">author and philosopher Nick Bostrom, “</span><em class="italic"><span class="koboSpan" id="kobo.17.1">Machine intelligence is the last invention that humanity will ever need to </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.18.1">make</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">” (</span></span><a href="https://x.com/TEDTalks/status/1191035758704037891"><span class="No-Break"><span class="koboSpan" id="kobo.20.1">https://x.com/TEDTalks/status/1191035758704037891</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.21.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.22.1">AGI, in fact, is a type of intelligence that reaches or exceeds human intelligence in a range of cognitive tasks (reasoning, planning, and learning – thus, representing knowledge). </span><span class="koboSpan" id="kobo.22.2">Creating AGI is the goal of several major companies (such as OpenAI and Meta AI). </span><span class="koboSpan" id="kobo.22.3">AGI could be an assistant to (and, according to some, replace) humans in complex tasks such as research. </span><span class="koboSpan" id="kobo.22.4">While many companies view this positively, some influential researchers, such as Geoffrey Hinton, are concerned about such a development. </span><span class="koboSpan" id="kobo.22.5">Prof. </span><span class="koboSpan" id="kobo.22.6">Geoffrey Hinton said there is “</span><em class="italic"><span class="koboSpan" id="kobo.23.1">a 10% chance of the technology triggering a catastrophic outcome for humanity</span></em><span class="koboSpan" id="kobo.24.1">.” </span><span class="koboSpan" id="kobo.24.2">In this chapter and the following, we will focus on models and approaches that focus on increasing a </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">model’s capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">The first step we will look at is how to free an LLM from its “box.” </span><span class="koboSpan" id="kobo.26.2">For example, we will see how to enable a model to be able to retrieve information from </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">the internet.</span></span></p>
<p><span class="koboSpan" id="kobo.28.1">In this chapter, we’ll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.30.1">Understanding the brain, perception, and </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">action paradigm</span></span></li>
<li><span class="koboSpan" id="kobo.32.1">Classifying </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">AI agents</span></span></li>
<li><span class="koboSpan" id="kobo.34.1">Understanding the abilities of single-agent and </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">multiple-agent systems</span></span></li>
<li><span class="koboSpan" id="kobo.36.1">Exploring the </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">principal libraries</span></span></li>
<li><span class="koboSpan" id="kobo.38.1">The general ability of a </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">single agent</span></span></li>
<li><span class="koboSpan" id="kobo.40.1">Creating an agent to search </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">the web</span></span></li>
</ul>
<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.42.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.43.1">Most of this code can be run on a CPU, but it is preferable to run it on a GPU. </span><span class="koboSpan" id="kobo.43.2">The code is written in PyTorch and uses standard libraries for the most part (PyTorch, Hugging Face Transformers, LangChain, pandas, and Matplotlib). </span><span class="koboSpan" id="kobo.43.3">The code can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr4"><span class="No-Break"><span class="koboSpan" id="kobo.45.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr4</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.46.1">.</span></span></p>
<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.47.1">Understanding the brain, perception, and action paradigm</span></h1>
<p><span class="koboSpan" id="kobo.48.1">An </span><strong class="bold"><span class="koboSpan" id="kobo.49.1">agent</span></strong><span class="koboSpan" id="kobo.50.1"> can be defined</span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.51.1"> as an entity that has the capacity to act. </span><span class="koboSpan" id="kobo.51.2">In philosophy, an agent is a being that also possesses desires, beliefs, and intentions. </span><span class="koboSpan" id="kobo.51.3">Traditionally, there is an overlap between an “agent” and a conscious entity. </span><span class="koboSpan" id="kobo.51.4">A conscious entity should also possess its own internal state that enables it to understand the world according to its </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">internal representation.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">An </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">AI agent</span></strong><span class="koboSpan" id="kobo.55.1"> is defined by its</span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.56.1"> ability to perform an action, but it does not possess desires and intentions (unfortunately, as we discussed in the previous chapter, a model inherits the biases of its training set and thus we can vaguely speak of beliefs). </span><span class="koboSpan" id="kobo.56.2">An LLM possesses an internal state, but it is merely a learned representation from the data it was trained with. </span><span class="koboSpan" id="kobo.56.3">So no, an AI agent is </span><em class="italic"><span class="koboSpan" id="kobo.57.1">not</span></em><span class="koboSpan" id="kobo.58.1"> a conscious entity. </span><span class="koboSpan" id="kobo.58.2">Although the term </span><em class="italic"><span class="koboSpan" id="kobo.59.1">agent</span></em><span class="koboSpan" id="kobo.60.1"> (and others such as </span><em class="italic"><span class="koboSpan" id="kobo.61.1">representation</span></em><span class="koboSpan" id="kobo.62.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.63.1">internal state</span></em><span class="koboSpan" id="kobo.64.1">) has a different meaning in philosophy, calling an LLM conscious is an </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">anthropomorphizing fallacy.</span></span></p>
<p><span class="koboSpan" id="kobo.66.1">At the same time, through language modeling, an LLM learns a useful representation of a text and how to put the elements present in context. </span><span class="koboSpan" id="kobo.66.2">An LLM can, then, through in-context learning, relate the instruction of a prompt to what it has learned, and thus solve a task. </span><span class="koboSpan" id="kobo.66.3">During instruction tuning, the model learns to perform tasks, all of which are skills that enable the model to perform an action. </span><span class="koboSpan" id="kobo.66.4">We can define a task as a set of actions that have a definite goal, while an action is an individual </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">accomplishable act.</span></span></p>
<p><span class="koboSpan" id="kobo.68.1">An AI agent can therefore be defined as an artificial entity that perceives its surrounding environment via a set of sensors, makes a decision, and implements it. </span><span class="koboSpan" id="kobo.68.2">This definition is quite broad and has been used to define various systems. </span><span class="koboSpan" id="kobo.68.3">In this book, we will focus on LLM-based agents. </span><span class="koboSpan" id="kobo.68.4">As a further definition, we will use a framework consisting of </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">three parts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.70.1">Brain</span></strong><span class="koboSpan" id="kobo.71.1">: The main </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.72.1">component of the system that integrates information, stores it, and </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">makes decisions</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.74.1">Perception</span></strong><span class="koboSpan" id="kobo.75.1">: The </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.76.1">component that extends the model’s capabilities in the perceptual domain, allowing the system to obtain information from different modalities (textual, auditory, and </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">visual modalities)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.78.1">Action</span></strong><span class="koboSpan" id="kobo.79.1">: The component</span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.80.1"> that enables the model to act and use tools to modify its surroundings (or respond to changes in </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">the environment)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.82.1">We can see how these components are interconnected with each other in the </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<span class="koboSpan" id="kobo.84.1"><img alt="Figure 4.1 – Conceptual framework of an LLM-based agent with three components: brain, perception, and action (https://arxiv.org/pdf/2309.07864)" src="image/B21257_04_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.85.1">Figure 4.1 – Conceptual framework of an LLM-based agent with three components: brain, perception, and action (</span><a href="https://arxiv.org/pdf/2309.07864"><span class="koboSpan" id="kobo.86.1">https://arxiv.org/pdf/2309.07864</span></a><span class="koboSpan" id="kobo.87.1">)</span></p>
<p><span class="koboSpan" id="kobo.88.1">An </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.89.1">LLM can thus be considered the brain of an agent system, where the LLM has access to tools that enable it to perform an action or enable perception. </span><span class="koboSpan" id="kobo.89.2">This system allows the expansion of an LLM’s space of perception and action, and thus its own capabilities (a multimodal extension allows it to integrate information of different types, access to the internet allows it to access real-time information, and an e-commerce tool allows it to conduct transactions). </span><span class="koboSpan" id="kobo.89.3">Indeed, as we saw in the previous chapter, an LLM can exhibit reasoning capabilities, which can be enhanced with techniques such as </span><strong class="bold"><span class="koboSpan" id="kobo.90.1">chain-of-thought</span></strong><span class="koboSpan" id="kobo.91.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.92.1">CoT</span></strong><span class="koboSpan" id="kobo.93.1">) or other </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.94.1">prompting approaches. </span><span class="koboSpan" id="kobo.94.2">In addition, the model, through in-context learning, can generalize its abilities to new tasks. </span><span class="koboSpan" id="kobo.94.3">CoT prompting can integrate feedback from the </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.95.1">environment, thus creating </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">reactive systems.</span></span></p>
<p><span class="koboSpan" id="kobo.97.1">We look for four fundamental properties in </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">an agent:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.99.1">Autonomy</span></strong><span class="koboSpan" id="kobo.100.1">: An agent </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.101.1">should be able to operate without human intervention. </span><span class="koboSpan" id="kobo.101.2">In addition, an agent should not need explicit instructions to complete a task but should execute it without a step-by-step description. </span><span class="koboSpan" id="kobo.101.3">LLMs have shown some creativity and ability to solve tasks without needing to explain all </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">the steps.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.103.1">Reactivity</span></strong><span class="koboSpan" id="kobo.104.1">: An agent </span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.105.1">should respond quickly to changes in the environment, perceive an external state change, and respond appropriately. </span><span class="koboSpan" id="kobo.105.2">This already happens at the textual level for an LLM (for example, in a dialogue where the topic can change). </span><span class="koboSpan" id="kobo.105.3">Extending an LLM multimodally allows different types of information and stimuli to </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">be integrated.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.107.1">Pro-activeness</span></strong><span class="koboSpan" id="kobo.108.1">: The agent’s</span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.109.1"> reaction should not be a mere response but directed toward a goal. </span><span class="koboSpan" id="kobo.109.2">In other words, an agent should be capable of reasoning and conducting plans in response to a change in an environment (these capacities can be stimulated in an LLM with reasoning-directed prompting techniques such </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">as CoT).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.111.1">Social ability</span></strong><span class="koboSpan" id="kobo.112.1">: An agent</span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.113.1"> should be able to interact with humans or other agents. </span><span class="koboSpan" id="kobo.113.2">This is one of the strengths of LLMs that exhibit dialogic and understanding skills. </span><span class="koboSpan" id="kobo.113.3">In fact, environments can be created with different LLM-based agents with different goals and tasks. </span><span class="koboSpan" id="kobo.113.4">Environments with multiple agents promote behavior as teamwork (where </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">agents coordinate).</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer122">
<span class="koboSpan" id="kobo.115.1"><img alt="Figure 4.2 – Screenshot of a simulated environment featuring multiple agents" src="image/B21257_04_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.116.1">Figure 4.2 – Screenshot of a simulated environment featuring multiple agents</span></p>
<p><span class="koboSpan" id="kobo.117.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.118.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.119.1">.2</span></em><span class="koboSpan" id="kobo.120.1">, we can see that agents are interacting not only with the environment but also with each other. </span><span class="koboSpan" id="kobo.120.2">By collaborating, the agents can solve complex tasks. </span><span class="koboSpan" id="kobo.120.3">This shows the sophisticated abilities that can be obtained with </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">LLM-based agents.</span></span></p>
<p><span class="koboSpan" id="kobo.122.1">One example of </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.123.1">collaborative behavior includes 25 agents in a </span><em class="italic"><span class="koboSpan" id="kobo.124.1">The Sims</span></em><span class="koboSpan" id="kobo.125.1">-like environment created for the paper </span><em class="italic"><span class="koboSpan" id="kobo.126.1">Generative Agents: Interactive Simulacra of Human Behavior</span></em><span class="koboSpan" id="kobo.127.1">, by J. </span><span class="koboSpan" id="kobo.127.2">S. </span><span class="koboSpan" id="kobo.127.3">Park et al. </span><span class="koboSpan" id="kobo.127.4">(</span><a href="https://arxiv.org/pdf/2304.03442"><span class="koboSpan" id="kobo.128.1">https://arxiv.org/pdf/2304.03442</span></a><span class="koboSpan" id="kobo.129.1">). </span><span class="koboSpan" id="kobo.129.2">Users can observe and intervene as agents plan their days, share news, form relationships, and coordinate </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">group activities.</span></span></p>
<p><span class="koboSpan" id="kobo.131.1">In the following sections, we analyze the various components of an AI agent (LLM-based), starting with what is called </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">the brain</span></span></p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.133.1">The brain</span></h2>
<p><span class="koboSpan" id="kobo.134.1">The brain is the core </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.135.1">of the system and is responsible for several functions: saving</span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.136.1"> information, finding knowledge, reasoning, and decision-making. </span><span class="koboSpan" id="kobo.136.2">Since the core of this system is an LLM, all interactions are based on natural language. </span><span class="koboSpan" id="kobo.136.3">This is an advantage because these interactions and operations can be understood by humans and therefore monitored (especially in case something goes wrong). </span><span class="koboSpan" id="kobo.136.4">Because an agent can interact with other entities and changes in the surrounding environment, it must be capable of multi-turn interactive conversations (conversations with multiple entities at the same time, different topics, complex structure, and understanding based on previous history). </span><span class="koboSpan" id="kobo.136.5">If the agent has poor conversational skills, humans will become frustrated when interacting with it, so it’s important that the model can communicate clearly. </span><span class="koboSpan" id="kobo.136.6">The model must be able to understand instructions, comprehend incoming information, integrate this information, and respond appropriately to </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">the task.</span></span></p>
<p><span class="koboSpan" id="kobo.138.1">Today’s LLMs are able to conduct this type of conversation with high quality. </span><span class="koboSpan" id="kobo.138.2">Conversational skills have increased exponentially in recent years due to alignment. </span><span class="koboSpan" id="kobo.138.3">Instruction tuning </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.139.1">has enabled LLMs to respond to instructions and</span><a id="_idIndexMarker343"/> <span class="No-Break"><span class="koboSpan" id="kobo.140.1">perform tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.141.1">Another important component is model knowledge, which is categorized into </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.143.1">Linguistic knowledge</span></strong><span class="koboSpan" id="kobo.144.1">: Knowledge of the semantics and syntax of a </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.145.1">language. </span><span class="koboSpan" id="kobo.145.2">This enables the </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.146.1">LLM to interact formally with a human, and today there are LLMs in different languages </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">as needed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.148.1">Common-sense knowledge</span></strong><span class="koboSpan" id="kobo.149.1">: A </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.150.1">set of rules and facts</span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.151.1"> that are known to most individuals. </span><span class="koboSpan" id="kobo.151.2">For instance, anyone can witness the effects of gravity or understand that humans cannot fly. </span><span class="koboSpan" id="kobo.151.3">This kind of information is not specifically mentioned in a prompt or context but is necessary for the model to perform a task or answer a question efficiently (and </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">avoid misunderstandings).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.153.1">Domain knowledge</span></strong><span class="koboSpan" id="kobo.154.1">: Knowledge</span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.155.1"> specific to a professional </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.156.1">domain (e.g., science or medicine) or technical domain (e.g., mathematics or programming). </span><span class="koboSpan" id="kobo.156.2">This is the necessary knowledge or skill to be possessed to succeed in a certain domain. </span><span class="koboSpan" id="kobo.156.3">There are now specialized LLMs in various domains (medicine, finance, and so on), or by starting from a generalist model, you can get a specialized </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">model (fine-tuning).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.158.1">Also, it is important to remember that knowledge of the model is frozen at the time of pre-training. </span><span class="koboSpan" id="kobo.158.2">An LLM cannot acquire information (continual learning) or remember past interactions with the user. </span><span class="koboSpan" id="kobo.158.3">As we will see in the next chapters, there are methods to overcome </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">this limitation.</span></span></p>
<p><span class="koboSpan" id="kobo.160.1">Possessing information is not enough for a brain; our agent must be capable of both </span><strong class="bold"><span class="koboSpan" id="kobo.161.1">reasoning</span></strong><span class="koboSpan" id="kobo.162.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.163.1">planning</span></strong><span class="koboSpan" id="kobo.164.1">. </span><span class="koboSpan" id="kobo.164.2">Techniques with CoT and self-consistency (which we saw in the previous chapter) can help the model reason better about solving a task. </span><span class="koboSpan" id="kobo.164.3">In general, reasoning step by step helps the model have both the task-solving and planning steps needed. </span><span class="koboSpan" id="kobo.164.4">Planning is a critical component for an agent because, to solve a task, the model must select the most appropriate steps to achieve the goal. </span><span class="koboSpan" id="kobo.164.5">In general, this is a </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">two-stage process:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.166.1">Plan formulation</span></strong><span class="koboSpan" id="kobo.167.1">: The model decomposes the task into subtasks. </span><span class="koboSpan" id="kobo.167.2">Depending on the approaches, the LLM decomposes the task into different steps and then executes them all sequentially. </span><span class="koboSpan" id="kobo.167.3">Other studies suggest that an adaptive strategy is better, in which one step at a time is executed (this can be followed </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">by evaluation).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.169.1">Plan reflection</span></strong><span class="koboSpan" id="kobo.170.1">: After formulation, it is recommended to analyze and conduct a feedback analysis of the plan. </span><span class="koboSpan" id="kobo.170.2">Once a plan is described, the LLM can evaluate it, or a second model may be present to evaluate the work of the </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">first model.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.172.1">Suppose we have</span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.173.1"> several LLMs. </span><span class="koboSpan" id="kobo.173.2">Our choice might be affected by space or </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.174.1">memory limitations. </span><span class="koboSpan" id="kobo.174.2">Also, we might have speed problems in inference (user experience will be impacted if the model takes too long to respond). </span><span class="koboSpan" id="kobo.174.3">We can then benchmark by evaluating speed (tokens per second) and relative performance (performance per second per </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">billion parameters).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer123">
<span class="koboSpan" id="kobo.176.1"><img alt="Figure 4.3 – Two graphs benchmarking different LLMs for generation speed: A) shows the number of tokens generated per second, and B) shows the number of tokens per second divided per billion parameters" src="image/B21257_04_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.177.1">Figure 4.3 – Two graphs benchmarking different LLMs for generation speed: A) shows the number of tokens generated per second, and B) shows the number of tokens per second divided per billion parameters</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.178.1">A)</span></strong><span class="koboSpan" id="kobo.179.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.180.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.181.1">.3</span></em><span class="koboSpan" id="kobo.182.1"> shows the number of tokens generated per second in response to the instruction “</span><em class="italic"><span class="koboSpan" id="kobo.183.1">Describe briefly what an AI agent is</span></em><span class="koboSpan" id="kobo.184.1">.” </span><strong class="bold"><span class="koboSpan" id="kobo.185.1">B)</span></strong><span class="koboSpan" id="kobo.186.1"> shows the number of tokens per second divided per billion parameters (a higher number means greater </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">model efficiency).</span></span></p>
<p><span class="koboSpan" id="kobo.188.1">In addition, it is</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.189.1"> always good to evaluate the quality of the response. </span><span class="koboSpan" id="kobo.189.2">An easy way </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.190.1">is to have a model such as GPT-4 (or any model that is larger than the initial models used) evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">the responses.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<span class="koboSpan" id="kobo.192.1"><img alt="Figure 4.4 – Evaluation of models’ answers" src="image/B21257_04_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.193.1">Figure 4.4 – Evaluation of models’ answers</span></p>
<p><span class="koboSpan" id="kobo.194.1">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.195.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.196.1">.4</span></em><span class="koboSpan" id="kobo.197.1">, GPT-4 evaluates the answer generated by the different models. </span><span class="koboSpan" id="kobo.197.2">Each column represents the assigned score (overall quality, completeness, and truthfulness) </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">by GPT-4.</span></span></p>
<p><span class="koboSpan" id="kobo.199.1">In summary, here are some recommendations for choosing which LLM to use as the brain of an AI </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">agent system:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.201.1">The first choice is whether to use a closed source model (such as GPT-4 or Claude) via an API or open source (such as Mistral or Meta’s Llama). </span><span class="koboSpan" id="kobo.201.2">In the first case, it is important to evaluate the costs of a proprietary model (cost per inference, per incorporation into an application). </span><span class="koboSpan" id="kobo.201.3">In the second case, the number of parameters should be chosen by balancing computational cost </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">and performance.</span></span></li>
<li><span class="koboSpan" id="kobo.203.1">Consider the infrastructure cost of a model. </span><span class="koboSpan" id="kobo.203.2">An LLM and other system components must be hosted in an infrastructure. </span><span class="koboSpan" id="kobo.203.3">For example, our LLM may be available on Azure, and the greater the number of parameters, the greater </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">the cost.</span></span></li>
<li><span class="koboSpan" id="kobo.205.1">Almost all models have good knowledge of both linguistics and common sense. </span><span class="koboSpan" id="kobo.205.2">On the other hand, for some specific domains, you may need a model that possesses a knowledge domain. </span><span class="koboSpan" id="kobo.205.3">There are already models that have been adapted and are open source (for example, FinGPT for finance), or if you own the data, you can decide to conduct fine-tuning yourself. </span><span class="koboSpan" id="kobo.205.4">Alternatively, retrieval approaches</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.206.1"> can</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.207.1"> be exploited, which we will see in </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">later chapters.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.209.1">In the next section, we will discuss how to connect this brain to the </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">outside world.</span></span></p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.211.1">The perception</span></h2>
<p><span class="koboSpan" id="kobo.212.1">While LLMs may be the </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.213.1">brain, they can only comprehend </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.214.1">textual input and thus lack visual perception (for example, although ChatGPT-4o can now have not only text but also images as input and can describe what it is in the image, that functionality is not technically part of the LLM itself). </span><span class="koboSpan" id="kobo.214.2">As humans, we rely extremely heavily on our visual perception. </span><span class="koboSpan" id="kobo.214.3">Vision allows us to acquire an enormous amount of information about the external world and the relationships between objects in the environment. </span><span class="koboSpan" id="kobo.214.4">Similarly, there are other modalities that allow us to acquire real-time information about the environment that we wish our agent could perceive. </span><span class="koboSpan" id="kobo.214.5">For example, an agent connected to home appliances could close windows if a sensor detects rain or lower blinds if the camera detects </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">strong sunshine.</span></span></p>
<p><span class="koboSpan" id="kobo.216.1">As sentient beings, we integrate the information we receive from sensory organs and process a response to external stimuli. </span><span class="koboSpan" id="kobo.216.2">For an agent to respond to a change in the environment, it must be able to </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">perceive changes.</span></span></p>
<p><span class="koboSpan" id="kobo.218.1">In order to help the LLM </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.219.1">understand </span><strong class="bold"><span class="koboSpan" id="kobo.220.1">visual input</span></strong><span class="koboSpan" id="kobo.221.1">, the simplest solution is to use another model to conduct image captioning. </span><span class="koboSpan" id="kobo.221.2">These captions can then be inserted into the prompt as additional context for task instruction. </span><span class="koboSpan" id="kobo.221.3">The advantage of this approach is that it is easily interpreted, and pre-trained templates can be used for captioning. </span><span class="koboSpan" id="kobo.221.4">During the captioning process, however, there is a loss of information, and the result does not represent the complexity of the visual information. </span><span class="koboSpan" id="kobo.221.5">Instead </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.222.1">of using captions, PaLM-E (</span><a href="https://arxiv.org/pdf/2303.03378)"><span class="koboSpan" id="kobo.223.1">https://arxiv.org/pdf/2303.03378)</span></a><span class="koboSpan" id="kobo.224.1"> and other</span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.225.1"> works use </span><strong class="bold"><span class="koboSpan" id="kobo.226.1">embodied </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.227.1">language models</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.229.1">PaLM-E is a single general-purpose multimodal language model for embodied reasoning tasks. </span><span class="koboSpan" id="kobo.229.2">The model integrates directly into the embedding images and text, allowing the solving of vision and </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">language tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">Sensory modality inputs (such as images) are directly incorporated into the input for the language model. </span><span class="koboSpan" id="kobo.231.2">The idea is that images are embedded into the same latent embedding as language tokens. </span><span class="koboSpan" id="kobo.231.3">The subsequent embedded vectors are then passed to the transformer blocks as if they were textual inputs. </span><span class="koboSpan" id="kobo.231.4">The model is then fine-tuned and learns how to relate the information present in the </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">various modalities.</span></span></p>
<p><span class="koboSpan" id="kobo.233.1">Alternatively, as we saw with BLIP-2 in the previous chapter, we can achieve multimodality by combining two models that have already been trained and keeping them frozen. </span><span class="koboSpan" id="kobo.233.2">Instead, we </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.234.1">train the </span><strong class="bold"><span class="koboSpan" id="kobo.235.1">Querying Transformer</span></strong><span class="koboSpan" id="kobo.236.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.237.1">Q-Former</span></strong><span class="koboSpan" id="kobo.238.1">) module to put in communication the visual encoder and the LLM. </span><span class="koboSpan" id="kobo.238.2">This approach has the advantage that we only have to conduct training for a module with much fewer parameters. </span><span class="koboSpan" id="kobo.238.3">Also, LLMs do not have</span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.239.1"> visual-language alignment, which can lead to</span><a id="_idIndexMarker363"/> <span class="No-Break"><span class="koboSpan" id="kobo.240.1">catastrophic forgetting.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.241.1">Video input</span></strong><span class="koboSpan" id="kobo.242.1"> can be</span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.243.1"> considered visual input in which we have an added temporal dimension. </span><span class="koboSpan" id="kobo.243.2">A video consists of a stream of continuous image frames (there is, however, a relationship between frames and this information must be preserved). </span><span class="koboSpan" id="kobo.243.3">To prevent the one-moment model from seeing the future and understanding the temporal order, models such as </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.244.1">Flamingo (</span><a href="https://arxiv.org/pdf/2204.14198"><span class="koboSpan" id="kobo.245.1">https://arxiv.org/pdf/2204.14198</span></a><span class="koboSpan" id="kobo.246.1">) use a </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">masked mechanism.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.248.1">Auditory input</span></strong><span class="koboSpan" id="kobo.249.1"> also conveys</span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.250.1"> important information. </span><span class="koboSpan" id="kobo.250.2">For example, in human speech, there is information beyond the content of the message (as intonation, pauses, and so on) or some sounds associated with particular dangers or physical events. </span><span class="koboSpan" id="kobo.250.3">Several models excel in specific tasks associated with auditory signals. </span><span class="koboSpan" id="kobo.250.4">Models such as Whisper can be used for speech-to-text, after which the transcript can be used for an LLM. </span><span class="koboSpan" id="kobo.250.5">In addition, an audio spectrogram is a rich source of information and can be represented as an image (frequency spectrum changes over time). </span><span class="koboSpan" id="kobo.250.6">Many models are basic vision transformers that have been adapted </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">to spectrograms.</span></span></p>
<p><span class="koboSpan" id="kobo.252.1">In fact, an LLM can invoke other models for other auditory tasks (text-to-audio, speech translation and recognition, speech separation, sound extraction, and so on). </span><span class="koboSpan" id="kobo.252.2">As discussed in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.253.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.254.1">, multimodality is when an LLM can</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.255.1"> take as input other modes besides text (such as images, video, audio, and so on). </span><span class="koboSpan" id="kobo.255.2">AudioGPT (</span><a href="https://arxiv.org/pdf/2304.1299"><span class="koboSpan" id="kobo.256.1">https://arxiv.org/pdf/2304.12995</span></a><span class="koboSpan" id="kobo.257.1">) is an example of this approach where we have an LLM interacting with audio foundation models (each of them specialized in a </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">different task).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer125">
<span class="koboSpan" id="kobo.259.1"><img alt="Figure 4.5 – High-level overview of AudioGPT (https://arxiv.org/pdf/2304.12995)" src="image/B21257_04_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.260.1">Figure 4.5 – High-level overview of AudioGPT (</span><a href="https://arxiv.org/pdf/2304.12995"><span class="koboSpan" id="kobo.261.1">https://arxiv.org/pdf/2304.12995</span></a><span class="koboSpan" id="kobo.262.1">)</span></p>
<p><span class="koboSpan" id="kobo.263.1">There are, of course, other types of sensor inputs in the real world. </span><span class="koboSpan" id="kobo.263.2">Modalities such as smell or touch are more complex to conjugate and require sensors. </span><span class="koboSpan" id="kobo.263.3">In various industries, though, there are sensors (temperature, humidity, and so on) that receive input from machines. </span><span class="koboSpan" id="kobo.263.4">An LLM</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.264.1"> could integrate this information directly or through an</span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.265.1"> intermediate model. </span><span class="koboSpan" id="kobo.265.2">Additionally, a model can receive information from other sources, such as LiDAR, GPS, and even </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">the internet.</span></span></p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.267.1">Action</span></h2>
<p><span class="koboSpan" id="kobo.268.1">In living things, we have</span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.269.1"> the perception of a signal and its interpretation, which </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.270.1">is then followed by a response. </span><span class="koboSpan" id="kobo.270.2">For example, an animal can visually perceive the figure of a predator (feel its movements and smell), and its brain will integrate this information and decide the best course of action (hide or run). </span><span class="koboSpan" id="kobo.270.3">For our agent, we can expect something similar, in which perceived signals are integrated and the LLM plans tasks that are then executed by dedicated action modules. </span><span class="koboSpan" id="kobo.270.4">The simplest example of output is text: LLMs have an inherent generation capability and can then generate a text in response to an instruction. </span><span class="koboSpan" id="kobo.270.5">The capabilities of an LLM can be extended through </span><strong class="bold"><span class="koboSpan" id="kobo.271.1">tools</span></strong><span class="koboSpan" id="kobo.272.1">. </span><span class="koboSpan" id="kobo.272.2">In fact, we humans can solve complex tasks by using tools that extend the inherent capabilities of our bodies (or enable tasks to be performed faster or </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">more efficiently).</span></span></p>
<p><span class="koboSpan" id="kobo.274.1">The model obviously needs to understand what tools are available and how to use them. </span><span class="koboSpan" id="kobo.274.2">By itself, an LLM can generalize to a certain limit (zero-shot capabilities), but there are techniques to improve its ability to use these tools (few-shot models, for example). </span><span class="koboSpan" id="kobo.274.3">Some approaches, as we will see later, will be like providing an instruction manual to the model. </span><span class="koboSpan" id="kobo.274.4">Learning how to use these tools can also be conducted as feedback so that the LLM can then </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">conduct adjustments.</span></span></p>
<p><span class="koboSpan" id="kobo.276.1">A more complex aspect of defining actions is a sub-branch </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.277.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.278.1">embodied action</span></strong><span class="koboSpan" id="kobo.279.1">. </span><span class="koboSpan" id="kobo.279.2">So far, the interactions have been within a virtual environment. </span><span class="koboSpan" id="kobo.279.3">In embodiment, we have an extension of the system to the outside world. </span><span class="koboSpan" id="kobo.279.4">According to </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.280.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.281.1">embodiment hypothesis</span></strong><span class="koboSpan" id="kobo.282.1">, humans develop their intelligence through their continuous interaction and feedback with the environment and not simply by reading textbooks about it. </span><span class="koboSpan" id="kobo.282.2">Therefore, if we want to achieve AGI, a model should be able to interact with the environment. </span><span class="koboSpan" id="kobo.282.3">AGI could enable applications that we could previously only imagine because it could monitor the external environment in real time and act with a sophisticated and complex goal (for example, acting to counter global warming, monitoring nuclear fusion, or probes being sent to explore space autonomously). </span><span class="koboSpan" id="kobo.282.4">An LLM could then be embedded in a robot and, thus, provided with a body, being able to explore the</span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.283.1"> environment</span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.284.1"> and learn </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">from it.</span></span></p>
<p><span class="koboSpan" id="kobo.286.1">In the next section, we will dig more into how agents learn and how we can better classify them. </span><span class="koboSpan" id="kobo.286.2">This will help us to better define and plan an AI agent when we </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">need it.</span></span></p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.288.1">Classifying AI agents</span></h1>
<p><span class="koboSpan" id="kobo.289.1">In this section, we will </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.290.1">discuss how best to classify agents and go into more detail about how such a complex system learns. </span><span class="koboSpan" id="kobo.290.2">The first classification is between agents that move only in a virtual environment and </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">embodied agents.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.292.1">Digital agents</span></strong><span class="koboSpan" id="kobo.293.1"> are confined to a virtual environment. </span><span class="koboSpan" id="kobo.293.2">Again, we have varying degrees of interaction with the virtual</span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.294.1"> universe. </span><span class="koboSpan" id="kobo.294.2">The simplest agents have interaction with a single user. </span><span class="koboSpan" id="kobo.294.3">For example, an agent can be programmed in a virtual environment as a Jupyter notebook, and although it can search the internet, it has rather small, and therefore primarily passive, interactions. </span><span class="koboSpan" id="kobo.294.4">There are two subsequent levels </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">of extension:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.296.1">Action agents</span></strong><span class="koboSpan" id="kobo.297.1"> perform</span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.298.1"> actions in a simulated or virtual</span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.299.1"> world. </span><span class="koboSpan" id="kobo.299.2">Gaming agents interact with other agents or users. </span><span class="koboSpan" id="kobo.299.3">These agents usually have a goal (such as winning a game) and must interact with other players to succeed in achieving their goal. </span><span class="koboSpan" id="kobo.299.4">A reinforcement learning algorithm is usually used to train the system by providing a reward to the model when it achieves </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">certain goals.</span></span></li>
<li><span class="koboSpan" id="kobo.301.1">An </span><strong class="bold"><span class="koboSpan" id="kobo.302.1">interactive agent</span></strong><span class="koboSpan" id="kobo.303.1"> is an</span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.304.1"> extension of the action agent. </span><span class="koboSpan" id="kobo.304.2">The model communicates</span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.305.1"> with the world and can modify it (these are not necessarily </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">physical actions).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.307.1">Once we have decided </span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.308.1">the limits of our system’s interaction, it is important to decide how it should approach a task. </span><span class="koboSpan" id="kobo.308.2">Therefore, the question is: how does the model decide how to </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">plan actions?</span></span></p>
<p><span class="koboSpan" id="kobo.310.1">This is one of the fundamental skills of the system: how to decompose a task into actions and what to prioritize. </span><span class="koboSpan" id="kobo.310.2">We will discuss the possible systems at a high level, especially for </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">task planning.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer126">
<span class="koboSpan" id="kobo.312.1"><img alt="Figure 4.6 – Taxonomy for existing LLM-agent planning works (https://arxiv.org/pdf/2402.02716)" src="image/B21257_04_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.313.1">Figure 4.6 – Taxonomy for existing LLM-agent planning works (</span><a href="https://arxiv.org/pdf/2402.02716"><span class="koboSpan" id="kobo.314.1">https://arxiv.org/pdf/2402.02716</span></a><span class="koboSpan" id="kobo.315.1">)</span></p>
<p><span class="koboSpan" id="kobo.316.1">In the real world, tasks are generally complex, and it is virtually impossible to solve them in a single step. </span><span class="koboSpan" id="kobo.316.2">For this reason, agents must divide the task into a series of subtasks that are more manageable (subtasks can also consist of a series of steps to be solved). </span><span class="koboSpan" id="kobo.316.3">In this process, it first</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.317.1"> has to decide how to divide a task into various subtasks and then how to solve them. </span><span class="koboSpan" id="kobo.317.2">There are usually two approaches to solving </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">this challenge:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.319.1">Decomposition-first methods</span></strong><span class="koboSpan" id="kobo.320.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.321.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.322.1">.7a</span></em><span class="koboSpan" id="kobo.323.1">): The LLM divides the task into a series of subgoals and solves them sequentially by creating a plan for each goal after it has solved the previous goal. </span><span class="koboSpan" id="kobo.323.2">This </span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.324.1">system is inspired by zero-shot CoT, and the LLM is asked to conduct the process in two steps with two explicit prompts: “Let’s first devise a plan” and “Let’s carry out the plan.” </span><span class="koboSpan" id="kobo.324.2">This approach has the advantage of giving the model an overview of the task, reducing hallucinations and forgetting. </span><span class="koboSpan" id="kobo.324.3">On the other hand, since everything is planned at the beginning, the model cannot correct errors that may occur at </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">some steps.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.326.1">Interleaved decomposition methods</span></strong><span class="koboSpan" id="kobo.327.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.328.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.329.1">.7b</span></em><span class="koboSpan" id="kobo.330.1">): Task decomposition and planning are interleaved. </span><span class="koboSpan" id="kobo.330.2">In other words, we generate a subtask and solve it with a plan until we</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.331.1"> have solved the whole task. </span><span class="koboSpan" id="kobo.331.2">Alternating reasoning and planning allows the model to improve its planning capabilities because it addresses the entire process in steps. </span><span class="koboSpan" id="kobo.331.3">This approach dynamically adjusts the task solution. </span><span class="koboSpan" id="kobo.331.4">It has the disadvantage that if the problem is too complex, it creates expensive and long reasoning-planning chains without getting </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">a result.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer127">
<span class="koboSpan" id="kobo.333.1"><img alt="Figure 4.7 – Types of task decomposition methods (https://arxiv.org/pdf/2402.02716)" src="image/B21257_04_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.334.1">Figure 4.7 – Types of task decomposition methods (</span><a href="https://arxiv.org/pdf/2402.02716"><span class="koboSpan" id="kobo.335.1">https://arxiv.org/pdf/2402.02716</span></a><span class="koboSpan" id="kobo.336.1">)</span></p>
<p><span class="koboSpan" id="kobo.337.1">There are variations and alternatives to these two approaches. </span><span class="koboSpan" id="kobo.337.2">For example, inspired by a self-consistency prompt (where we sample different reasoning paths for a single question), in the </span><strong class="bold"><span class="koboSpan" id="kobo.338.1">multi-plan selection approach</span></strong><span class="koboSpan" id="kobo.339.1">, several different plans are generated for each task. </span><span class="koboSpan" id="kobo.339.2">This</span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.340.1"> is because even if the model can reason, it might generate a plan that is incorrect or not feasible. </span><span class="koboSpan" id="kobo.340.2">The model generates several candidate plans for a single task, and then we can exploit different algorithms to choose the best plan of action. </span><span class="koboSpan" id="kobo.340.3">In the simplest version, we choose the majority voting strategy, but there are alternatives in which we exploit tree search algorithms or reinforcement learning. </span><span class="koboSpan" id="kobo.340.4">This approach often succeeds </span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.341.1">in solving complex cases, and the use of heuristic algorithms decreases the cost of solving them in extended hypothesis spaces. </span><span class="koboSpan" id="kobo.341.2">On the other hand, generating different paths has a higher computational cost (with the risk of higher time cost as well), and since it uses stochastic processes, it may not </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">be consistent.</span></span></p>
<p><span class="koboSpan" id="kobo.343.1">Another approach</span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.344.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.345.1">external planner-aided planning</span></strong><span class="koboSpan" id="kobo.346.1">, in which external planners are integrated. </span><span class="koboSpan" id="kobo.346.2">For example, symbolic planners can be added to identify the optimal path of resolution. </span><span class="koboSpan" id="kobo.346.3">Today, there are also neural planners (much lighter neural networks) to help LLMs find the optimal plan. </span><span class="koboSpan" id="kobo.346.4">In other words, the LLM conducts reasoning that can be seen as a slow, meditative process, while the planner provides a quick, instinctive response. </span><span class="koboSpan" id="kobo.346.5">This slow and fast thinking can also be alternated, with a fast plan developed first, and then an LLM used to solve any mistakes. </span><span class="koboSpan" id="kobo.346.6">This approach is resource-efficient and seems promising for tasks that require code generation. </span><span class="koboSpan" id="kobo.346.7">The system is complex to develop and </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">implement, however.</span></span></p>
<p><span class="koboSpan" id="kobo.348.1">To avoid hallucinations and</span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.349.1"> other errors, another possible approach is </span><strong class="bold"><span class="koboSpan" id="kobo.350.1">reflection and refinement</span></strong><span class="koboSpan" id="kobo.351.1">. </span><span class="koboSpan" id="kobo.351.2">This can be seen as an interleaved decomposition extension, in which the LLM conducts an iterative process of generation, feedback, and refinement. </span><span class="koboSpan" id="kobo.351.3">After each generation step, the model also generates feedback on the plan and then uses this feedback to conduct refinement. </span><span class="koboSpan" id="kobo.351.4">In more sophisticated versions, there is an additional model that evaluates the plan (evaluator) and proposes feedback. </span><span class="koboSpan" id="kobo.351.5">It is also possible to incorporate environmental changes into the feedback, making the system particularly versatile. </span><span class="koboSpan" id="kobo.351.6">Despite the potential, there is no guarantee that the refinement process will lead the model to solve the goal. </span><span class="koboSpan" id="kobo.351.7">The LLM can get stuck in a continuous chain, especially when the process </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">is complex.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.353.1">Memory-augmented planning</span></strong><span class="koboSpan" id="kobo.354.1"> is an approach </span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.355.1">that seeks to overcome the current context-length limitation of the model. </span><span class="koboSpan" id="kobo.355.2">Memory-augmented planning for an agent refers to the use of an external memory system to enhance the agent’s decision-making and planning capabilities. </span><span class="koboSpan" id="kobo.355.3">This approach allows the agent to store, recall, and utilize past experiences, observations, or computations to improve performance in complex tasks. </span><span class="koboSpan" id="kobo.355.4">Imagine a robot vacuum cleaner tasked with cleaning a house. </span><span class="koboSpan" id="kobo.355.5">Without memory, it randomly navigates the house and might repeatedly clean the same areas or miss some spots. </span><span class="koboSpan" id="kobo.355.6">With memory augmentation, the robot keeps a map (memory) of where it has already cleaned and where obstacles (such as furniture) are located. </span><span class="koboSpan" id="kobo.355.7">This allows the system to plan the </span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.356.1">next move, without revisiting cleaned areas, to efficiently cover </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">the house.</span></span></p>
<p><span class="koboSpan" id="kobo.358.1">In fact, a task can be divided into several subtasks, and these into further subtasks. </span><span class="koboSpan" id="kobo.358.2">Together with planning and intermediate results, more information can be generated than fits in </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">the context.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.360.1">Retrieval-augmented generation</span></strong><span class="koboSpan" id="kobo.361.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.362.1">RAG</span></strong><span class="koboSpan" id="kobo.363.1">) is a technique that allows the retrieval of information for</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.364.1"> later use in generation (we will discuss this in detail in the next two chapters) and can be used to store an agent’s past experience. </span><span class="koboSpan" id="kobo.364.2">In RAG, there is an external memory in the form of a database, and at each user query, we can search for the context needed to answer a question or perform an action (this context becomes part of the model input). </span><span class="koboSpan" id="kobo.364.3">In other words, the model can find previous task planning, other solutions to the task, or additional information that can serve the task solution. </span><span class="koboSpan" id="kobo.364.4">Alternatively, it is possible to use these previous experiences for fine-tuning the model. </span><span class="koboSpan" id="kobo.364.5">Fine-tuning on previous tasks helps generalization to subsequent tasks. </span><span class="koboSpan" id="kobo.364.6">On the one hand, the use of RAG is less costly but requires that retrieval be accurate, and that the found past experiences be relevant to the task. </span><span class="koboSpan" id="kobo.364.7">Fine-tuning is more expensive but allows the model to store experiences (a kind of internalization). </span><span class="koboSpan" id="kobo.364.8">There are even more sophisticated RAG versions in which structures are built to mimic human short-term and long-term memories (the former to store temporary changes in the environment and the latter to consolidate </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">important information).</span></span></p>
<p><span class="koboSpan" id="kobo.366.1"> As we will see in the next section, we can have either a single agent or multiple agents interacting within a single system. </span><span class="koboSpan" id="kobo.366.2">This flexibility allows us to be able to deal with complex tasks by choosing the appropriate architecture (one or more agents). </span><span class="koboSpan" id="kobo.366.3">In </span><em class="italic"><span class="koboSpan" id="kobo.367.1">Chapters 9 and 10</span></em><span class="koboSpan" id="kobo.368.1">, we will return to this topic and look at multi-agent systems </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">in practice.</span></span></p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.370.1">Understanding the abilities of single-agent and multiple-agent systems</span></h1>
<p><span class="koboSpan" id="kobo.371.1">It is important to discuss </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.372.1">what an agent’s capabilities are, and how they can be</span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.373.1"> used to accomplish tasks. </span><span class="koboSpan" id="kobo.373.2">Conceptually, the scenario in which our agent can act must be defined. </span><strong class="bold"><span class="koboSpan" id="kobo.374.1">Task-oriented deployment</span></strong><span class="koboSpan" id="kobo.375.1"> is the</span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.376.1"> simplest scenario in which an agent assists a human in some tasks. </span><span class="koboSpan" id="kobo.376.2">These types of agents need to be able to solve task bases or break them down into manageable subtasks. </span><span class="koboSpan" id="kobo.376.3">The purpose of this agent is to understand a user’s instructions, then understand the task, decompose it into steps, plan, and execute that plan until the goal is achieved. </span><span class="koboSpan" id="kobo.376.4">A single agent can perform these tasks in web or </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">real-life scenarios.</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.378.1">In a web scenario</span></em><span class="koboSpan" id="kobo.379.1">, an agent must be capable of performing actions on the web (and thus be connected to the internet). </span><span class="koboSpan" id="kobo.379.2">An LLM has the potential to automate various tasks such as online shopping, sending emails, and filling out forms. </span><span class="koboSpan" id="kobo.379.3">An agent devoted to these tasks must have the ability to adapt to changes in various websites. </span><span class="koboSpan" id="kobo.379.4">LLM agents are favored in this area, as sites often have large text content. </span><span class="koboSpan" id="kobo.379.5">With too much information, agents can still have problems (performance drop). </span><span class="koboSpan" id="kobo.379.6">In fact, if relevant information is scattered amid too much irrelevant context, the model may hallucinate, fail to resolve the task, or fail to plan. </span><span class="koboSpan" id="kobo.379.7">To improve the model’s capabilities, one of the tools can often </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">read HTML.</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.381.1">In a live scenario</span></em><span class="koboSpan" id="kobo.382.1">, an agent must be capable of being able to perform actions and have common-sense reasoning (for example, an agent making purchases on the internet). </span><span class="koboSpan" id="kobo.382.2">For an LLM that has only been trained with massive amounts of text, these tasks can be especially complex. </span><span class="koboSpan" id="kobo.382.3">For example, although it may be trained on texts about the fact that there is day/night alternation, it is difficult for a model to understand how to orient itself when lighting changes without further instruction. </span><span class="koboSpan" id="kobo.382.4">Also, an agent must have common sense when planning actions (these actions must be feasible and not contradict common sense). </span><span class="koboSpan" id="kobo.382.5">Therefore, the agent will need spatial information in order to understand its environment in a future deployment of </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">embodied robots.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.384.1">Innovation-oriented deployment</span></strong><span class="koboSpan" id="kobo.385.1"> is a more </span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.386.1">complex scenario (and represents future developments in the coming years, not a current use), where the agent does not simply have to perform tasks. </span><span class="koboSpan" id="kobo.386.2">These agents must demonstrate some exploratory capability in science (for example, lab assistants, application planning, or software design). </span><span class="koboSpan" id="kobo.386.3">Complex and innovative projects are difficult to define solely as textual information; they are multidimensional. </span><span class="koboSpan" id="kobo.386.4">An</span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.387.1"> agent will need to have a clear understanding of an </span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.388.1">entire knowledge domain and be able to extrapolate from it. </span><span class="koboSpan" id="kobo.388.2">These kinds of agents can then be used to develop code and software or to create new materials, conduct experiments, and much more. </span><span class="koboSpan" id="kobo.388.3">Although it is an active field of research, and LLMs show some of these required skills, this potential has not yet </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">been reached.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.390.1">Life cycle-oriented deployment</span></strong><span class="koboSpan" id="kobo.391.1"> can be </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.392.1">defined as the ultimate goal for many in the community. </span><span class="koboSpan" id="kobo.392.2">It refers to an agent that is capable of exploring on its own, developing new skills, and operating even in an unfamiliar world. </span><span class="koboSpan" id="kobo.392.3">Today, interesting studies are being conducted on Minecraft on “test beds” for many projects oriented in this direction. </span><span class="koboSpan" id="kobo.392.4">In fact, Minecraft represents a virtual world in which a model must perform both short-term and long-term tasks (in these settings, it is important to have a memory, which we will discuss more in the </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">next chapter).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<span class="koboSpan" id="kobo.394.1"><img alt="Figure 4.8 – Practical applications of the single LLM-based agent in increasingly complex scenarios (https://arxiv.org/pdf/2309.07864)" src="image/B21257_04_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.395.1">Figure 4.8 – Practical applications of the single LLM-based agent in increasingly complex scenarios (</span><a href="https://arxiv.org/pdf/2309.07864"><span class="koboSpan" id="kobo.396.1">https://arxiv.org/pdf/2309.07864</span></a><span class="koboSpan" id="kobo.397.1">)</span></p>
<p><span class="koboSpan" id="kobo.398.1">Human beings, though, learn not only from books but also from other human beings. </span><span class="koboSpan" id="kobo.398.2">In addition, most of our work is done collaboratively. </span><span class="koboSpan" id="kobo.398.3">Also, because of resource issues, the division of labor is much more convenient. </span><span class="koboSpan" id="kobo.398.4">Therefore, several researchers propose that the same approach should be followed with AI. </span><span class="koboSpan" id="kobo.398.5">In </span><strong class="bold"><span class="koboSpan" id="kobo.399.1">multi-agent systems</span></strong><span class="koboSpan" id="kobo.400.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.401.1">MASs</span></strong><span class="koboSpan" id="kobo.402.1">), different agents </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.403.1">collaborate and communicate with each other. </span><span class="koboSpan" id="kobo.403.2">Several LLM agents collaborate and communicate in natural language (which means that their actions are also interpretable by a human observer). </span><span class="koboSpan" id="kobo.403.3">In this case, one can also have several LLMs that are specialized in a particular task, rather than having to use one model that specializes in everything. </span><span class="koboSpan" id="kobo.403.4">In fact, some approaches focus on creating agents that are complementary and can collaborate and share information. </span><span class="koboSpan" id="kobo.403.5">In these settings, models can also make collective decisions and are capable of solving tasks that a single agent cannot solve. </span><span class="koboSpan" id="kobo.403.6">For example, to improve the resolution of a process, agents can provide different responses and conduct a majority vote. </span><span class="koboSpan" id="kobo.403.7">There may be agents who provide feedback or monitor the actions of other agents. </span><span class="koboSpan" id="kobo.403.8">These interactions can be orderly (follow rules or </span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.404.1">have</span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.405.1"> a sequential order) or messy (each agent can voice </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">its opinion).</span></span></p>
<p><span class="koboSpan" id="kobo.407.1">Agents need not cooperate. </span><span class="koboSpan" id="kobo.407.2">In accordance with game theory, exploiting competition among agents can be beneficial to task resolution. </span><span class="koboSpan" id="kobo.407.3">This process has been used to train models to win games. </span><span class="koboSpan" id="kobo.407.4">In fact, AlphaGo (</span><a href="https://www.nature.com/articles/nature24270"><span class="koboSpan" id="kobo.408.1">https://www.nature.com/articles/nature24270</span></a><span class="koboSpan" id="kobo.409.1">) was</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.410.1"> trained to beat itself at Go, so it was able to amass many more game hours. </span><span class="koboSpan" id="kobo.410.2">LLMs can be put into what are called </span><em class="italic"><span class="koboSpan" id="kobo.411.1">adversarial settings</span></em><span class="koboSpan" id="kobo.412.1">, in which they receive feedback from another agent and use it to improve themselves. </span><span class="koboSpan" id="kobo.412.2">There are several approaches in which you might have agents discuss or reflect on another </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">agent’s performance:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<span class="koboSpan" id="kobo.414.1"><img alt="Figure 4.9 – Interaction scenarios for multiple LLM-based agents (https://arxiv.org/pdf/2309.07864)" src="image/B21257_04_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.415.1">Figure 4.9 – Interaction scenarios for multiple LLM-based agents (</span><a href="https://arxiv.org/pdf/2309.07864"><span class="koboSpan" id="kobo.416.1">https://arxiv.org/pdf/2309.07864</span></a><span class="koboSpan" id="kobo.417.1">)</span></p>
<p><span class="koboSpan" id="kobo.418.1">Agents can also interact with humans (human-agent interaction). </span><span class="koboSpan" id="kobo.418.2">This assumes control over agents’ behavior so that their goals are aligned with those of humans. </span><span class="koboSpan" id="kobo.418.3">At the same time, interaction with humans is a source of important information that should be exploited to</span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.419.1"> provide </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.420.1">feedback to agents (performance, safety, and potential bias). </span><span class="koboSpan" id="kobo.420.2">In addition, interaction with humans can be a way to allow agents </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">to evolve.</span></span></p>
<p><span class="koboSpan" id="kobo.422.1">We can have two types of interaction between agents </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">and humans:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.424.1">Unequal interaction</span></strong><span class="koboSpan" id="kobo.425.1">, also</span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.426.1"> called the </span><strong class="bold"><span class="koboSpan" id="kobo.427.1">instructor-executor paradigm</span></strong><span class="koboSpan" id="kobo.428.1">, is an</span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.429.1"> approach in which humans provide </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.430.1">instructions via natural language and agents execute. </span><span class="koboSpan" id="kobo.430.2">This dialogue can be a single prompt (instruction and execution) or interactive (conversational). </span><span class="koboSpan" id="kobo.430.3">In this approach, the agent executes, while the human provides instructions and feedback. </span><span class="koboSpan" id="kobo.430.4">In the simplest format, feedback can be quantitative (binary or rating) or qualitative (natural language, advice, suggestions, or criticism), which the model can use to improve current and future responses. </span><span class="koboSpan" id="kobo.430.5">A sub-branch of this approach, called </span><strong class="bold"><span class="koboSpan" id="kobo.431.1">continual learning</span></strong><span class="koboSpan" id="kobo.432.1">, studies</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.433.1"> a way for the model to learn with </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">each interaction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.435.1">Equal interaction</span></strong><span class="koboSpan" id="kobo.436.1"> is a paradigm</span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.437.1"> in which there is an equal partnership between the</span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.438.1"> agent and the human. </span><span class="koboSpan" id="kobo.438.2">Given the conversational capabilities of current LLMs, an agent can have a collaborative role for humans. </span><span class="koboSpan" id="kobo.438.3">One of the limitations of this approach is the lack of chatbot emotion, which is perceived as problematic by users. </span><span class="koboSpan" id="kobo.438.4">For this reason, several researchers have focused on making chatbots more empathetic. </span><span class="koboSpan" id="kobo.438.5">In addition, these agents need to better understand beliefs and</span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.439.1"> goals </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.440.1">in interacting with humans before gaining equal status </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">in interactions.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer130">
<span class="koboSpan" id="kobo.442.1"><img alt="Figure 4.10 – Two paradigms of human-agent interaction (https://arxiv.org/pdf/2309.07864)" src="image/B21257_04_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.443.1">Figure 4.10 – Two paradigms of human-agent interaction (</span><a href="https://arxiv.org/pdf/2309.07864"><span class="koboSpan" id="kobo.444.1">https://arxiv.org/pdf/2309.07864</span></a><span class="koboSpan" id="kobo.445.1">)</span></p>
<p><span class="koboSpan" id="kobo.446.1">In the next section, we will discuss the principal libraries to </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">create agents.</span></span></p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.448.1">Exploring the principal libraries</span></h1>
<p><span class="koboSpan" id="kobo.449.1">After discussing</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.450.1"> the various components and frameworks on a conceptual and theoretical level, in this section, we will discuss some of the major libraries that allow these concepts to be put into practice. </span><span class="koboSpan" id="kobo.450.2">These libraries make it possible to connect an LLM to the various additional modules. </span><span class="koboSpan" id="kobo.450.3">LLMs remain central but are thus connected to perception modules and execution tools. </span><span class="koboSpan" id="kobo.450.4">In the next chapters, we will </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.451.1">elaborate on some aspects and see different </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">practical examples.</span></span></p>
<p><span class="koboSpan" id="kobo.453.1">In general, the structure of an LLM-based application has </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">several components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.455.1">The interface</span></strong><span class="koboSpan" id="kobo.456.1">: this </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.457.1">connects the user to </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">the system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.459.1">The brain</span></strong><span class="koboSpan" id="kobo.460.1">: an LLM </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.461.1">that can also be connected to additional memory. </span><span class="koboSpan" id="kobo.461.2">An LLM has its own parametric memory (obtained during training), but we can add external memory (such as a vector database or </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">knowledge graph).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.463.1">Perception modules</span></strong><span class="koboSpan" id="kobo.464.1">: these</span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.465.1"> allow the ingestion and transformation of </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">user data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.467.1">Tools</span></strong><span class="koboSpan" id="kobo.468.1">: modules</span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.469.1"> that extend the abilities of the LLM. </span><span class="koboSpan" id="kobo.469.2">These can be built in the library or created by </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">the developer.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.471.1">Prompts</span></strong><span class="koboSpan" id="kobo.472.1">: the user’s </span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.473.1">dialogue with the application in natural language. </span><span class="koboSpan" id="kobo.473.2">The prompt contains both instructions provided by the user (frontend prompt) and information that is not seen by the user (backend prompt). </span><span class="koboSpan" id="kobo.473.3">The backend information is additional instructions that condition the behavior of the LLM. </span><span class="koboSpan" id="kobo.473.4">For example, we can force the LLM to respond only using the information in context or present in the vector database. </span><span class="koboSpan" id="kobo.473.5">Some backend prompts are developed to prevent the model from responding with </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">harmful content.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.475.1">There are several different libraries that enable us to be able to build such a system, and here we will introduce </span><a id="_idIndexMarker421"/><span class="No-Break"><span class="koboSpan" id="kobo.476.1">the following:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.477.1">LangChain</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.478.1">Haystack</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.479.1">LlamaIndex</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.480.1">Semantic Kernel</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.481.1">AutoGen</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.482.1">Let’s look at each </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">of them.</span></span></p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.484.1">LangChain</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.485.1">LangChain</span></strong><span class="koboSpan" id="kobo.486.1"> is a framework</span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.487.1"> for developing applications with </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.488.1">LLMs at their core. </span><span class="koboSpan" id="kobo.488.2">The focus of this framework is the development and deployment of these applications into production. </span><span class="koboSpan" id="kobo.488.3">The LangChain ecosystem consists of three </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">core components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.490.1">LangChain</span></strong><span class="koboSpan" id="kobo.491.1">: Different </span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.492.1">modules allow the incorporation of LLMs with added memory, prompts, and </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">other tools</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.494.1">LangSmith</span></strong><span class="koboSpan" id="kobo.495.1">: This</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.496.1"> component is used to inspect, monitor, and evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">your application</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.498.1">LangServe</span></strong><span class="koboSpan" id="kobo.499.1">: This allows</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.500.1"> you to turn your system into </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">an API</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.502.1">LangChain can be used in both Python and JavaScript (and some modules are also available in Rust). </span><span class="koboSpan" id="kobo.502.2">To date, it is one of the most widely used libraries in the community, and in fact, there are several components that have been developed by the open </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">source community.</span></span></p>
<p><span class="koboSpan" id="kobo.504.1">LangChain is compatible with either models that are closed source (such as OpenAI or Anthropic) or models that are available on Hugging Face. </span><span class="koboSpan" id="kobo.504.2">LangChain is development-oriented (one of the advantages of LangChain is that it allows both parallel execution and asynchronous support) and one of the best libraries for building an application that needs to go into production. </span><span class="koboSpan" id="kobo.504.3">LangChain provides convenient wrappers for LLMs and allows them to be connected to additional tools. </span><span class="koboSpan" id="kobo.504.4">One of the most interesting aspects is that it allows you to build so-called chains (LLMs and add-ons) that can then be tracked and deployed in production. </span><span class="koboSpan" id="kobo.504.5">LangChain also provides several functions to transform different data (CSV, PDF, text, images, and so on). </span><span class="koboSpan" id="kobo.504.6">In addition, the library provides a number of prompt templates to better use the </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">various LLMs.</span></span></p>
<p><span class="koboSpan" id="kobo.506.1">LangChain creates modular abstractions, thus allowing models to be connected to tools efficiently. </span><span class="koboSpan" id="kobo.506.2">By building with chains, you can create efficient (but still versatile and customized) pipelines that can then be easily deployed. </span><span class="koboSpan" id="kobo.506.3">In addition, through LangSmith, you can monitor the system to </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">avoid problems.</span></span></p>
<p><span class="koboSpan" id="kobo.508.1">LangChain has </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">several advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.510.1">Comprehensive library</span></strong><span class="koboSpan" id="kobo.511.1">: It presents</span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.512.1"> a broad library of features with ready-made templates for many applications. </span><span class="koboSpan" id="kobo.512.2">In addition, the design is modular, so you can easily </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">swap components.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.514.1">Extensive integrations</span></strong><span class="koboSpan" id="kobo.515.1">: LangChain offers the ability to connect to a large number of external libraries in an easy way: LLM providers, vector databases, cloud service, and </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.517.1">Precise and clear workflows</span></strong><span class="koboSpan" id="kobo.518.1">: LangChain makes it possible to clearly define inputs and outputs and also allows intermediate products in the chain to be monitored and extensive prompt engineering to </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">be conducted.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.520.1">Active developing community</span></strong><span class="koboSpan" id="kobo.521.1">: There is a large user base, with different solutions that have been developed by the community, and there are many tutorials written on various sites </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">and forums.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.523.1">Flexible framework focused on an end-to-end cycle</span></strong><span class="koboSpan" id="kobo.524.1">: LangChain provides elements for the entire cycle of an application (integration, development, deployment, </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">and observability).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.526.1">At the same time, it also has a couple </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">of disadvantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.528.1">Steeper learning curve</span></strong><span class="koboSpan" id="kobo.529.1">: Users </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.530.1">may require more time to adapt to the library syntax and achieve the full capability of the library. </span><span class="koboSpan" id="kobo.530.2">Abstraction capability comes at a cost; all functions are defined as a class. </span><span class="koboSpan" id="kobo.530.3">For example, a simple prompt must be abstracted into a </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.531.1">prompt template</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.533.1">Documentation</span></strong><span class="koboSpan" id="kobo.534.1">: Many users have complained that the documentation is out of date or not easily understood, generalized but not specialized. </span><span class="koboSpan" id="kobo.534.2">Versatility is also a disadvantage because, for several specific applications, there are systems that have more functionality (for example, for </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">RAG applications).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.536.1">LangChain is</span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.537.1"> the most widely used library, especially for building complex agents. </span><span class="koboSpan" id="kobo.537.2">However, it also has the steepest learning curve. </span><span class="koboSpan" id="kobo.537.3">For this reason, many project users often prefer a </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">simpler library.</span></span></p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.539.1">Haystack</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.540.1">Haystack</span></strong><span class="koboSpan" id="kobo.541.1"> is an open </span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.542.1">source framework for building production-ready LLM </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.543.1">applications. </span><span class="koboSpan" id="kobo.543.2">Like LangChain, it is compatible with the major LLM sources and deployment platforms. </span><span class="koboSpan" id="kobo.543.3">Haystack also allows you to connect tools to LLMs and has a whole set of tools designed to put the system into production (including evaluation, monitoring, and data ingestion). </span><span class="koboSpan" id="kobo.543.4">It is designed to be able to easily create LLMs with associated external storage, chatbots, and agents, but also multimodal systems. </span><span class="koboSpan" id="kobo.543.5">One of the advantages of Haystack is that it has several pre-built features that can then be inserted into one’s own pipeline </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">with ease.</span></span></p>
<p><span class="koboSpan" id="kobo.545.1">Haystack is built on the idea that everything can be composable with ease, the main elements being </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.547.1">Components</span></strong><span class="koboSpan" id="kobo.548.1">: These are </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.549.1">building blocks dedicated to document retrieval, text generation, or creating embeddings. </span><span class="koboSpan" id="kobo.549.2">These components can be viewed as nodes, and the library presents many that have already been built and are ready to use. </span><span class="koboSpan" id="kobo.549.3">The user still has the option of creating their </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">own nodes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.551.1">Pipelines</span></strong><span class="koboSpan" id="kobo.552.1">: These are a convenient abstraction for understanding how data flows in the application. </span><span class="koboSpan" id="kobo.552.2">A pipeline consists of several components that are connected. </span><span class="koboSpan" id="kobo.552.3">Haystack facilitates the system because it allows for versatile pipeline control (you can join pipelines, create loops, and so on). </span><span class="koboSpan" id="kobo.552.4">In Haystack, you can see them as graphs where the components are nodes that can be interconnected in </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">sophisticated ways.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.554.1">Haystack has </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">several advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.556.1">Specialized components</span></strong><span class="koboSpan" id="kobo.557.1">: Haystack</span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.558.1"> provides a number of excellent components for data processing, embedding, ranking, and writing. </span><span class="koboSpan" id="kobo.558.2">In addition, the library specializes in searching </span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.559.1">and </span><strong class="bold"><span class="koboSpan" id="kobo.560.1">question and answer</span></strong><span class="koboSpan" id="kobo.561.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.562.1">Q&amp;A</span></strong><span class="koboSpan" id="kobo.563.1">) systems. </span><span class="koboSpan" id="kobo.563.2">For this user case, it provides components that </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">are optimized.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.565.1">Extensive documentation and community</span></strong><span class="koboSpan" id="kobo.566.1">: Haystack is adopted by a large community and there are now many community-developed components. </span><span class="koboSpan" id="kobo.566.2">It also presents quality documentation and there are </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">many tutorials.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.568.1">Gentler learning</span></strong><strong class="bold"><span class="koboSpan" id="kobo.569.1"> curve</span></strong><span class="koboSpan" id="kobo.570.1">: Haystack is considered an easy-to-learn framework. </span><span class="koboSpan" id="kobo.570.2">It is versatile, and it is easy to</span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.571.1"> adapt it to </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">different cases.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.573.1">However, it also has </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">several disadvantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.575.1">Smaller user base</span></strong><span class="koboSpan" id="kobo.576.1">: The </span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.577.1">community is active but smaller than other frameworks such as LangChain </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">or LlamaIndex.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.579.1">Less integration</span></strong><span class="koboSpan" id="kobo.580.1">: There are fewer dedicated integrations than other frameworks. </span><span class="koboSpan" id="kobo.580.2">Despite this, the system is flexible and many custom </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">tools exist.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.582.1">Narrower scope</span></strong><span class="koboSpan" id="kobo.583.1">: Haystack is more focused on retrieval and document-understanding tasks, so it has fewer tools and parsers for other NLP applications. </span><span class="koboSpan" id="kobo.583.2">This is a limitation when you have to develop applications that include dialogues, chatbots, or </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">other tools.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.585.1">Scalability</span></strong><span class="koboSpan" id="kobo.586.1">: Many users complain of problems when they have to scale the system or have to handle </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">large datasets.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.588.1">Haystack is an easy</span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.589.1"> library and can be a great choice for RAG-based applications, but less so for more sophisticated applications that </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">involve agents.</span></span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.591.1">LlamaIndex</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.592.1">LlamaIndex</span></strong><span class="koboSpan" id="kobo.593.1"> is another</span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.594.1"> framework focused on building a system around</span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.595.1"> an LLM. </span><span class="koboSpan" id="kobo.595.2">LlamaIndex began as a data framework that focuses on building RAG. </span><span class="koboSpan" id="kobo.595.3">For this reason, the system has several data connectors to both integrate external sources and ingest different types of data. </span><span class="koboSpan" id="kobo.595.4">One of the interesting points of LlamaIndex is that it allows knowledge graphs to be easily integrated as well. </span><span class="koboSpan" id="kobo.595.5">It can also be integrated with different types of models, but it also has integration for other frameworks (Docker, OpenAI, LangChain, Flask, and </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.597.1">LlamaIndex can be used to nimbly build chatbots and connect them with external storage. </span><span class="koboSpan" id="kobo.597.2">It also allows you to build autonomous agents that can search the internet or conduct actions. </span><span class="koboSpan" id="kobo.597.3">There are several tools and features already constituted and others that have been</span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.598.1"> developed by the community </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">around it.</span></span></p>
<p><span class="koboSpan" id="kobo.600.1">LlamaIndex also has </span><span class="No-Break"><span class="koboSpan" id="kobo.601.1">several advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.602.1">Handling different data sources</span></strong><span class="koboSpan" id="kobo.603.1">: LlamaIndex</span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.604.1"> can handle over 160 data sources, making it efficient for many types of data commonly found in the enterprise. </span><span class="koboSpan" id="kobo.604.2">It is ideal for when you have complex and diverse datasets. </span><span class="koboSpan" id="kobo.604.3">It also supports </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">multimodal integration.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.606.1">Indexing and efficient retrieval</span></strong><span class="koboSpan" id="kobo.607.1">: This is the strength of LlamaIndex; it was designed with accurate and fast retrieval of information in mind. </span><span class="koboSpan" id="kobo.607.2">The library offers several tools and features for RAG and other </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">retrieval paradigms.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.609.1">Customization</span></strong><span class="koboSpan" id="kobo.610.1">: Especially for retrieval, LlamaIndex offers a high possibility </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">of customization.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.612.1">There are also </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">some disadvantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.614.1">Complexity</span></strong><span class="koboSpan" id="kobo.615.1">: LlamaIndex has a</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.616.1"> steep learning curve compared to other frameworks. </span><span class="koboSpan" id="kobo.616.2">In order to use it best, it assumes that you have a clear idea of </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">information retrieval.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.618.1">Limited functionality</span></strong><span class="koboSpan" id="kobo.619.1">: LlamaIndex has a focus on retrieval tasks but has limited functionality regarding other NLP tasks. </span><span class="koboSpan" id="kobo.619.2">This results in a lack </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">of versatility.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.621.1">LlamaIndex is</span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.622.1"> the first choice for RAG-based applications and a good solution </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">for agents.</span></span></p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.624.1">Semantic Kernel</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.625.1">Semantic Kernel</span></strong><span class="koboSpan" id="kobo.626.1"> is an open</span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.627.1"> source framework developed </span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.628.1">by Microsoft to build agents. </span><span class="koboSpan" id="kobo.628.2">This library can also connect with OpenAI, Hugging Face, and other frameworks. </span><span class="koboSpan" id="kobo.628.3">Semantic Kernel was originally written in C#, but today there is also a version in Python. </span><span class="koboSpan" id="kobo.628.4">The idea behind this library is to provide the ability to create functions that are the result of combining various functions (also</span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.629.1"> known as </span><strong class="bold"><span class="koboSpan" id="kobo.630.1">function composition</span></strong><span class="koboSpan" id="kobo.631.1">). </span><span class="koboSpan" id="kobo.631.2">In other words, Semantic Kernel is structured on the idea that various components can be tied together to build </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">versatile pipelines.</span></span></p>
<p><span class="koboSpan" id="kobo.633.1">In Semantic Kernel, the core is an LLM, but we can add code that we have developed as plugins so that the LLM can then execute it. </span><span class="koboSpan" id="kobo.633.2">In addition, it allows an LLM to have memory that can be either in the form of files or vector databases. </span><span class="koboSpan" id="kobo.633.3">An interesting element is that one can create native functions that are dedicated to performing a task. </span><span class="koboSpan" id="kobo.633.4">It also implements a planner that takes your task as input and returns a set of actions, functions, or plugins to succeed in solving </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">the task.</span></span></p>
<p><span class="koboSpan" id="kobo.635.1">Semantic Kernel is</span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.636.1"> versatile, supports several libraries, and has lightweight support for C# and .NET frameworks. </span><span class="koboSpan" id="kobo.636.2">It is </span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.637.1">inspired by the Copilot framework, which is stable and a good choice </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">for enterprises.</span></span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.639.1">AutoGen</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.640.1">AutoGen</span></strong><span class="koboSpan" id="kobo.641.1"> is a Python-based</span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.642.1"> LLM framework developed by Microsoft </span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.643.1">and other entities (University of Washington, Penn State, and Xidian). </span><span class="koboSpan" id="kobo.643.2">The idea behind it is to build applications that configure multiple agents that communicate with each other to complete a task. </span><span class="koboSpan" id="kobo.643.3">Agents and applications work through conversational programming, and a chain is built from the agents’ interactions. </span><span class="koboSpan" id="kobo.643.4">There are three types of AutoGen agents: </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">UserProxyAgent</span></strong><span class="koboSpan" id="kobo.645.1"> (which collects information from users and passes the information to other agents), </span><strong class="source-inline"><span class="koboSpan" id="kobo.646.1">AssistantAgent</span></strong><span class="koboSpan" id="kobo.647.1"> (which receives data from another </span><strong class="source-inline"><span class="koboSpan" id="kobo.648.1">AssistantAgent</span></strong><span class="koboSpan" id="kobo.649.1"> instance and </span><strong class="source-inline"><span class="koboSpan" id="kobo.650.1">UserProxyAgent</span></strong><span class="koboSpan" id="kobo.651.1">, processes it, and completes a task), and </span><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">GroupChatManager</span></strong><span class="koboSpan" id="kobo.653.1"> (which controls and directs communication between agents). </span><span class="koboSpan" id="kobo.653.2">AutoGen supports several complex conversation patterns that allow complex workflows to take place without human intervention. </span><span class="koboSpan" id="kobo.653.3">Systems involving several agents communicating with each other in complex ways can </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">be configured.</span></span></p>
<p><span class="koboSpan" id="kobo.655.1">This library has the </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">following advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.657.1">Simplicity</span></strong><span class="koboSpan" id="kobo.658.1">: Abstraction </span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.659.1">makes it possible to intuit how agents converse and arrive at accomplishing a task. </span><span class="koboSpan" id="kobo.659.2">In addition, this makes it easier to explain the system to non-technical staff and </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">other stakeholders.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.661.1">Customization</span></strong><span class="koboSpan" id="kobo.662.1">: The process is</span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.663.1"> intuitive and allows easy customization with </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">little code.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.665.1">There are a couple of </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">drawbacks, however:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.667.1">Harder to debug</span></strong><span class="koboSpan" id="kobo.668.1">: Agents </span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.669.1">are interdependent, making </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">debugging difficult</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.671.1">Less support</span></strong><span class="koboSpan" id="kobo.672.1">: It is less adopted by the community, so there are fewer users to turn to for help when you </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">need it</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.674.1">AutoGen is an</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.675.1"> interesting and promising library, but at this stage, it can be hard to start a </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">project with.</span></span></p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.677.1">Choosing an LLM agent framework</span></h2>
<p><span class="koboSpan" id="kobo.678.1">In general, the different frameworks offer </span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.679.1">similar components and are inspired by the same philosophy (extending the capabilities of an LLM). </span><span class="koboSpan" id="kobo.679.2">In addition, almost all libraries today are mature and have purpose-built components. </span><span class="koboSpan" id="kobo.679.3">For those functions that are not natively present within the library, there are many resources today that have been built by the community. </span><span class="koboSpan" id="kobo.679.4">The main component is the LLM, and the capabilities of the LLM are those that most condition the result of </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">the application.</span></span></p>
<p><span class="koboSpan" id="kobo.681.1">The first factor that may affect the choice is the programming language of the library. </span><span class="koboSpan" id="kobo.681.2">Often, these libraries must be integrated into existing systems that cannot be modified. </span><span class="koboSpan" id="kobo.681.3">Almost all libraries are written in Python but also have modules that can be written in other languages and facilitate their integration. </span><span class="koboSpan" id="kobo.681.4">In some cases, although support is not native, it has been developed by the open source community. </span><span class="koboSpan" id="kobo.681.5">For example, LangChain has support for Rust and there are unofficial implementations in other languages (C#, R, and so on). </span><span class="koboSpan" id="kobo.681.6">The tasks that the system has to accomplish are another determining factor. </span><span class="koboSpan" id="kobo.681.7">The increased complexity of the system requires that the framework be both robust and flexible at the same time. </span><span class="koboSpan" id="kobo.681.8">Some systems are designed with a greater focus on information retrieval (LlamaIndex) and thus are better choices if our system is to focus on chat and retrieval. </span><span class="koboSpan" id="kobo.681.9">In other contexts, we are more interested in system scalability and performance, so we might be interested in all the monitoring and evaluation ecosystems that LangChain and </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">Haystack provide.</span></span></p>
<p><span class="koboSpan" id="kobo.683.1">In addition, it is desirable that there be an active community of developers and that the library itself be actively maintained. </span><span class="koboSpan" id="kobo.683.2">When one adopts a framework, it is important that there are resources and a community to ask to avoid getting stuck or not finding a solution to a bug. </span><span class="koboSpan" id="kobo.683.3">A community-adopted library will have a large supply of tutorials and examples to help you learn </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">the system.</span></span></p>
<p><span class="koboSpan" id="kobo.685.1">Another factor is the level of customization. </span><span class="koboSpan" id="kobo.685.2">Although all libraries offer predefined features, these features do not cover all user cases, and in-house solutions will need to be developed. </span><span class="koboSpan" id="kobo.685.3">An ideal library should have this versatility and the ability to modify and integrate components. </span><span class="koboSpan" id="kobo.685.4">It may happen that we want to change our LLM or one of the components because we need a different performance. </span><span class="koboSpan" id="kobo.685.5">Similarly, the default parameters may not be optimal for our application, and it is better to have a library where it is not complex to adapt </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">the parameters.</span></span></p>
<p><span class="koboSpan" id="kobo.687.1">The best library is determined mainly by the use case. </span><span class="koboSpan" id="kobo.687.2">Each of them has strengths and weaknesses. </span><span class="koboSpan" id="kobo.687.3">For example, if the application is focused on retrieval, LlamaIndex may be the winning choice, or Haystack if the core of the application is Q&amp;A. </span><span class="koboSpan" id="kobo.687.4">LangChain is a more natural choice for a broad scope, but if the system is to be integrated into .NET, you might choose Semantic Kernel. </span><span class="koboSpan" id="kobo.687.5">The chosen framework should be evaluated taking into consideration additional constraints and what the main focus of the </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">application is.</span></span></p>
<p><span class="koboSpan" id="kobo.689.1">We have </span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.690.1">now seen the most important libraries for AI agents. </span><span class="koboSpan" id="kobo.690.2">In the next section, we will start to test them </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">in action.</span></span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.692.1">Creating an agent to search the web</span></h1>
<p><span class="koboSpan" id="kobo.693.1">We typically associate</span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.694.1"> internet searches with search engines like</span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.695.1"> Google. </span><span class="koboSpan" id="kobo.695.2">The search for a query is conducted using an algorithm, but it is not an AI algorithm. </span><span class="koboSpan" id="kobo.695.3">PageRank is, in fact, a graph search algorithm that does not involve learning. </span><span class="koboSpan" id="kobo.695.4">A search algorithm presupposes two </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">main steps:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.697.1">Matching</span></strong><span class="koboSpan" id="kobo.698.1">: Finding documents that are relevant to a given </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">user query</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.700.1">Ranking</span></strong><span class="koboSpan" id="kobo.701.1">: Ordering these documents from most relevant to </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">least relevant</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.703.1">These two steps do not necessarily require AI. </span><span class="koboSpan" id="kobo.703.2">However, searching with the use of AI can bring a better service and solve some of the problems that plague search algorithms. </span><span class="koboSpan" id="kobo.703.3">A user today expects an AI algorithm to be able to distinguish the entity and terminology and to contextualize and localize it. </span><span class="koboSpan" id="kobo.703.4">For example, a user searching for “the best pizzeria” expects the search engine to return the best restaurants nearby. </span><span class="koboSpan" id="kobo.703.5">The search process of the future will also integrate other elements, such as a conversation about the results, complex responses (summarization and action), </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">and multimodality.</span></span></p>
<p><span class="koboSpan" id="kobo.705.1">There is another aspect. </span><span class="koboSpan" id="kobo.705.2">In today’s systems, search results should in many cases be user-related. </span><span class="koboSpan" id="kobo.705.3">Searches can be ambiguous and some results will be more relevant by considering the user’s history. </span><span class="koboSpan" id="kobo.705.4">In addition, a user may want a search conditional on other parameters and want to express it in natural language (for example, the difference between “best pizzeria in Paris” and expressing “best pizzeria AND Paris”). </span><span class="koboSpan" id="kobo.705.5">Once the results are found, the user may have other questions that require reasoning (“Which of these pizzerias is suitable for a dinner </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">with children?”).</span></span></p>
<p><span class="koboSpan" id="kobo.707.1">An AI-enhanced search can meet these needs because it has an LLM at its core. </span><span class="koboSpan" id="kobo.707.2">An LLM can understand the difference between the various keywords in the query, as well as understand which domain the user is looking for (for example, “Transformer” can be an AI model or a toy). </span><span class="koboSpan" id="kobo.707.3">In addition, by accessing the history of previous interactions, the LLM is aware of the user’s preferences (so you do not have to state your preferences each time). </span><span class="koboSpan" id="kobo.707.4">This allows for a more relevant ranking of the results since they are in order not only for the query</span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.708.1"> but </span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.709.1">also for a user’s preferences. </span><span class="koboSpan" id="kobo.709.2">The model can also conduct reasoning and give more importance to results that are more implicitly relevant (for example, when searching for a family restaurant, a multimodal model will give more preference to a restaurant with a picture of a playground even if it is not explicitly stated in the description that it is for families). </span><span class="koboSpan" id="kobo.709.3">The user can also ask questions about the results and the model using agents can </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">perform operations.</span></span></p>
<p><span class="koboSpan" id="kobo.711.1">The traditional search returns links in order of importance for a query. </span><span class="koboSpan" id="kobo.711.2">Today, however, information can be extracted from sites and analyzed by an LLM. </span><span class="koboSpan" id="kobo.711.3">For a given query, the model can now either extract the</span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.712.1"> most relevant passage in the site (</span><strong class="bold"><span class="koboSpan" id="kobo.713.1">extractive question-answering</span></strong><span class="koboSpan" id="kobo.714.1">) or propose a summary of the first results that directly answer the </span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.715.1">query (</span><strong class="bold"><span class="koboSpan" id="kobo.716.1">abstractive question-answering</span></strong><span class="koboSpan" id="kobo.717.1">). </span><span class="koboSpan" id="kobo.717.2">In this way, the user does not even have to click the links but has the answer directly. </span><span class="koboSpan" id="kobo.717.3">This system can then be integrated with other tools, such as external memory (such as RAG and knowledge graphs, which we will see in the next </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">three chapters).</span></span></p>
<p><span class="koboSpan" id="kobo.719.1">In addition, an LLM has generative capabilities, so these can be used in combination with the query (“search for sources on the French Revolution” or “search for code in Python for convolution and translate it into R”). </span><span class="koboSpan" id="kobo.719.2">One of the problems with this type of generative search is the risk of hallucination, so the system must preserve the sources it has used so that backtracking can </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">be conducted.</span></span></p>
<p><span class="koboSpan" id="kobo.721.1">In the simplest form of searching with an LLM plus agents, we have an LLM connected with an interface that allows it to receive a query and a tool that allows it to search the internet. </span><span class="koboSpan" id="kobo.721.2">In this basic case, the LLM must analyze the query, plan to use the tool to search the internet, analyze the results found, and answer the query. </span><span class="koboSpan" id="kobo.721.3">In more sophisticated forms, the LLM may have more tools at its disposal. </span><span class="koboSpan" id="kobo.721.4">For instance, it can execute code, a </span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.722.1">calculator, and</span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.723.1"> external memory to save data or call up models that perform NLP tasks (entity identification in text, extracting passages, and </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">so on).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<span class="koboSpan" id="kobo.725.1"><img alt="Figure 4.11 – Representation of an LLM agent’s system for internet research" src="image/B21257_04_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.726.1">Figure 4.11 – Representation of an LLM agent’s system for internet research</span></p>
<p><span class="koboSpan" id="kobo.727.1">Let’s break down </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.728.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.729.1">.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">:</span></span></p>
<ol>
<li class="Alphabets"><span class="koboSpan" id="kobo.731.1">The user formulates </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">a query.</span></span></li>
<li class="Alphabets"><span class="koboSpan" id="kobo.733.1">The model analyzes the query and plans actions to solve </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">the task.</span></span></li>
<li class="Alphabets"><span class="koboSpan" id="kobo.735.1">The appropriate tool (in this case, internet search) </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">is selected.</span></span></li>
<li class="Alphabets"><span class="koboSpan" id="kobo.737.1">Documents are identified during the internet search. </span><span class="koboSpan" id="kobo.737.2">The information is sent back to the model, which analyzes it and decides whether further action is required or whether the task </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">is accomplished.</span></span></li>
<li class="Alphabets"><span class="koboSpan" id="kobo.739.1">The model generates the answer, which is sent back to </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">the user.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.741.1">LangChain</span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.742.1"> allows the LLM to be able to connect to the web through the use of tools. </span><span class="koboSpan" id="kobo.742.2">In general, the most widely used approach is known </span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.743.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.744.1">Reasoning and Acting</span></strong><span class="koboSpan" id="kobo.745.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.746.1">ReAct</span></strong><span class="koboSpan" id="kobo.747.1">) prompting. </span><span class="koboSpan" id="kobo.747.2">During the first stage (reasoning), the model considers the best strategy to be able to arrive at the answer, and in the second stage (acting), it executes the plan. </span><span class="koboSpan" id="kobo.747.3">In this approach, the model also tracks the reasoning steps and can guide it to the solution. </span><span class="koboSpan" id="kobo.747.4">This approach also allows flexibility because it enables the model to mold the prompt to </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">its needs.</span></span></p>
<p><span class="koboSpan" id="kobo.749.1">By itself, LangChain offers a number of tools that are designed to extend the model and find the information it needs. </span><span class="koboSpan" id="kobo.749.2">For example, it offers several tools for searching the internet. </span><span class="koboSpan" id="kobo.749.3">The DuckDuckGo</span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.750.1"> tool allows people to use the DuckDuckGo search engine. </span><span class="koboSpan" id="kobo.750.2">This search engine is free and does not track user data (it also filters out pages that are full of advertising or </span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.751.1">have </span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.752.1">articles written only to rank highly in the Google </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">search engine).</span></span></p>
<p><span class="koboSpan" id="kobo.754.1">In order to use it, we need to install a specific Python library and then import the tool </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">into LangChain:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.756.1">
!pip install duckduckgo-search
from langchain.tools import DuckDuckGoSearchRun
ddg_search = DuckDuckGoSearchRun()
ddg_search.run('Who is the current president of Italy?')</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.757.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.758.1">You can directly use the tool, and it will provide you with the </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">search results.</span></span></p>
<p><span class="koboSpan" id="kobo.760.1">One of the strengths of LangChain is the ability to build a list of tools that can then be used by the LLM. </span><span class="koboSpan" id="kobo.760.2">To do this, we have to give a name, explain what the function to be performed is, and provide a description. </span><span class="koboSpan" id="kobo.760.3">This way, the LLM is informed about what tool it can use when it has to do an </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">internet search:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.762.1">
from langchain.agents import Tool
tools = [
   Tool(
       name="DuckDuckGo Search",
       func=ddg_search.run,
       description="A web search tool to extract information from Internet.",
   )
]</span></pre> <p><span class="koboSpan" id="kobo.763.1">It is possible to use Google Serper, a low-cost API (albeit with some limitations) that enables you to use the</span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.764.1"> Google search engine (you have to register at </span><a href="https://serper.dev/"><span class="koboSpan" id="kobo.765.1">https://serper.dev/</span></a><span class="koboSpan" id="kobo.766.1"> to get an API key); however, there is a fee for the service. </span><span class="koboSpan" id="kobo.766.2">In fact, the Google API is much </span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.767.1">more</span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.768.1"> expensive and the LLM cannot access directly the search engine (Serper allows us to use the Google search engine through their API, and they provide some </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">free credits):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.770.1">
#!pip install google-serp-api
import os
SERPER_API_KEY = 'your_key'
os.environ["SERPER_API_KEY"] = SERPER_API_KEY
from langchain.utilities import GoogleSerperAPIWrapper
google_search = GoogleSerperAPIWrapper()
tools.append(
   Tool(
       name="Google Web Search",
       func=google_search.run,
       description="Google search tool to extract information from Internet.",
   )
)</span></pre> <p><span class="koboSpan" id="kobo.771.1">Additionally, we can </span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.772.1">add </span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.773.1">Wikipedia as a reliable source </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">of information:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.775.1">
from langchain.tools import WikipediaQueryRun
from langchain.utilities import WikipediaAPIWrapper
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
tools.append(
   Tool(
       name="Wikipedia Web Search",
       func=wikipedia.run,
       description="Useful tool to search Wikipedia.",
   )
)</span></pre> <p><span class="koboSpan" id="kobo.776.1">Once we have the various tools, we can then initialize an agent and conduct </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">a query:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.778.1">
from langchain.agents import initialize_agent
agent = initialize_agent(
   tools, llm, agent="zero-shot-react-description",
   verbose=True
)
agent.run('Query')</span></pre> <p><span class="koboSpan" id="kobo.779.1">The model </span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.780.1">will choose </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.781.1">which results will be interesting to our query. </span><span class="koboSpan" id="kobo.781.2">The model can access different information but has nowhere to save it. </span><span class="koboSpan" id="kobo.781.3">Using an LLM for internet research can be useful for different fields (from medicine to finance). </span><span class="koboSpan" id="kobo.781.4">For example, models have been used to access genetic sequences and conduct comparisons, or to search for information about drugs, chemical structures, and more. </span><span class="koboSpan" id="kobo.781.5">Similarly, a model can search for the latest financial and economic news. </span><span class="koboSpan" id="kobo.781.6">Another tool </span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.782.1">can be the </span><strong class="bold"><span class="koboSpan" id="kobo.783.1">OpenStreetMap</span></strong><span class="koboSpan" id="kobo.784.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.785.1">OSM</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">) search.</span></span></p>
<p><span class="koboSpan" id="kobo.787.1">In the next chapter, we will discuss how the model can save and access this memory. </span><span class="koboSpan" id="kobo.787.2">For example, we may want our model to be able to access conversation history or extend its knowledge. </span><span class="koboSpan" id="kobo.787.3">This can be useful in both business applications, but also in fields such as finance </span><span class="No-Break"><span class="koboSpan" id="kobo.788.1">and healthcare.</span></span></p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.789.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.790.1">In this chapter, we introduced how an LLM can be the brain of a sophisticated and complex system. </span><span class="koboSpan" id="kobo.790.2">We can use the conversational and reasoning abilities of the LLM to solve a task. </span><span class="koboSpan" id="kobo.790.3">As we said, this brain can be extended by providing perceptual systems (senses) and tools (hands). </span><span class="koboSpan" id="kobo.790.4">In fact, we can allow the model to search the internet by connecting with APIs, but also to ingest information from other modalities (audio, images, or video). </span><span class="koboSpan" id="kobo.790.5">Similarly, the model uses this received information to solve user tasks. </span><span class="koboSpan" id="kobo.790.6">If we can imagine agents performing and automating routine tasks for users today, it is not difficult to imagine a world in which agents interact with humans and other agents in increasingly sophisticated and </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">complex ways.</span></span></p>
<p><span class="koboSpan" id="kobo.792.1">In the next chapter, we will see how a model can have a memory, as well as how to store information and be able to find it again to be </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">more efficient.</span></span></p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.794.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.795.1">Silver, </span><em class="italic"><span class="koboSpan" id="kobo.796.1">Mastering the game of Go without human knowledge</span></em><span class="koboSpan" id="kobo.797.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">2017: </span></span><a href="https://www.nature.com/articles/nature24270"><span class="No-Break"><span class="koboSpan" id="kobo.799.1">https://www.nature.com/articles/nature24270</span></span></a></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.800.1">LangChain: </span></span><a href="https://python.langchain.com/v0.2/docs/introduction/"><span class="No-Break"><span class="koboSpan" id="kobo.801.1">https://python.langchain.com/v0.2/docs/introduction/</span></span></a></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.802.1">Haystack: </span></span><a href="https://haystack.deepset.ai/overview/intro"><span class="No-Break"><span class="koboSpan" id="kobo.803.1">https://haystack.deepset.ai/overview/intro</span></span></a></li>
<li><span class="koboSpan" id="kobo.804.1">Semantic </span><span class="No-Break"><span class="koboSpan" id="kobo.805.1">Kernel: </span></span><a href="https://learn.microsoft.com/en-us/semantic-kernel/overview/"><span class="No-Break"><span class="koboSpan" id="kobo.806.1">https://learn.microsoft.com/en-us/semantic-kernel/overview/</span></span></a></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.807.1">AutoGen: </span></span><a href="https://microsoft.github.io/autogen/"><span class="No-Break"><span class="koboSpan" id="kobo.808.1">https://microsoft.github.io/autogen/</span></span></a></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.809.1">LlamaIndex: </span></span><a href="https://www.llamaindex.ai/"><span class="No-Break"><span class="koboSpan" id="kobo.810.1">https://www.llamaindex.ai/</span></span></a></li>
<li><span class="koboSpan" id="kobo.811.1">LangChain </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">tools: </span></span><a href="https://python.langchain.com/v0.2/docs/integrations/tools/"><span class="No-Break"><span class="koboSpan" id="kobo.813.1">https://python.langchain.com/v0.2/docs/integrations/tools/</span></span></a></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.814.1">DuckDuckGo: </span></span><a href="https://duckduckgo.com/"><span class="No-Break"><span class="koboSpan" id="kobo.815.1">https://duckduckgo.com/</span></span></a></li>
<li><span class="koboSpan" id="kobo.816.1">Guardian, </span><em class="italic"><span class="koboSpan" id="kobo.817.1">‘Godfather of AI’ shortens odds of the technology wiping out humanity over next 30 years</span></em><span class="koboSpan" id="kobo.818.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">2024: </span></span><a href="https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years"><span class="No-Break"><span class="koboSpan" id="kobo.820.1">https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years</span></span></a></li>
</ul>
</div>
</body></html>