- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swapping the Face Back into the Video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll complete the deepfake process by converting the videos
    to swap faces using the models trained in the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion is the last step of deepfaking, and it is the part that actually
    puts the new face onto the existing video. This requires you to already have a
    video that you have fully processed through the extraction process in [*Chapter
    5*](B17535_05.xhtml#_idTextAnchor090), *Extracting Faces*, and uses a trained
    model from [*Chapter 6*](B17535_06.xhtml#_idTextAnchor107), *Training a* *Deepfake
    Model*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing to convert video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting hands-on with the convert code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the video from images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you’ll need a `conda` environment setup. If you set this
    up in earlier chapters, the same `conda` environment will work fine. To get into
    the `conda` environment, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you have not created a `conda` environment to run the code, it’s recommended
    that you go to the Git repository and follow the instructions there. You can find
    the full repository at [https://github.com/PacktPublishing/Exploring-Deepfakes](https://github.com/PacktPublishing/Exploring-Deepfakes).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing to convert video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Conversion is not just a “one-and-done” script. It requires you to have turned
    a video into a series of frames and run `C5-face_detection.py` on those frames.
    This gets the data that you need for the conversion process in the right form.
    The conversion process will require the full extraction of every frame, as well
    as the `face_alignments.json` file that is generated by the extract process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Example of a folder that has been extracted already. Note the
    face_images folder created by the extract process](img/B17535_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Example of a folder that has been extracted already. Note the face_images
    folder created by the extract process
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t done the extract process on the video you want to convert, then
    you should go back to [*Chapter 5*](B17535_05.xhtml#_idTextAnchor090), *Extracting
    Faces*, and extract the video.
  prefs: []
  type: TYPE_NORMAL
- en: We need to do this because this is how the model knows which faces to convert.
    AI can detect all faces in a frame but won’t know which ones should be swapped,
    meaning that all faces will be swapped. By running the extract process and cleaning
    out the faces we *don’t* want to swap from the folder of extracted faces, we can
    control which faces get swapped.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you would probably want to include the frames you’re going to convert
    in your training data, in what we call “fit training,” which makes sure your model
    has some experience with the exact frames you’re converting. To do this, go back
    to [*Chapter 6*](B17535_06.xhtml#_idTextAnchor107), *Training a Deepfake Model*,
    and point the “*A*” side of your model to the directory containing the frames
    you’re going to use to convert.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in an “on the fly” conversion process that swaps all faces,
    you can check the exercises page at the end of this chapter, where we raise that
    exact question. In fact, every chapter in this section has a list of exercises
    for you to get your hands dirty and get the experience of writing your own code
    for deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at the convert code.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with the convert code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the rest of the chapters in this section, we’ll be going through the code
    line by line to talk about how it works and what it’s doing.
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we will initialize and prepare the code to run the convert process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like all Python code, we’ll start with the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These libraries are all ones we’ve already seen in previous chapters. This is
    because the conversion process is not really doing anything too different from
    what we’ve done before. We’ll see that as we go through the code to covert the
    face back into the original images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll check whether we’re running from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code doesn’t actually do anything but is a common way to set up a Python
    file to run when called. It allows you to import the script into other scripts
    without running those commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll parse the arguments for the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code uses the standard `ArgumentParser` library from Python to parse command-line
    arguments. This lets us set defaults to some options and change them if we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we process the arguments, add the path to applicable variables, and pass
    these arguments to the `main()` function, which will actually process the conversion.
  prefs: []
  type: TYPE_NORMAL
- en: We adjust the path variables to add in the default path. This lets us have JSON
    and export folders within subfolders of the data folder. Otherwise, each would
    have to be specified separately and could be in very different places. You are
    still able to specify a specific folder if you want, but the defaults help keep
    things organized.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now move back up to the start of the main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Again, this is just the main function that does the work. It doesn’t actually
    do anything except organize our code and allow us to keep things to a normal “pythonic”
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next step is to make sure that the folders we’re going to write into exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This section checks the export path and ensures that it already exists, creating
    it if it doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to load the AI and put it onto the device that it needs to
    be on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we check whether `cuda` is available and whether the CPU override was
    given:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code checks whether `cuda` is enabled in PyTorch, and if it is and the
    user hasn’t disabled it with a command line switch, and enables `cuda` for the
    rest of the code accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we build the AI models with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code establishes the encoder and decoder. Unlike when we were training,
    we only need one decoder. This is because training requires both faces to be able
    to learn successfully, but once trained, we only need the decoder for the face
    we’re swapping in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading model weights comes next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code loads the weights from the trained model. We first load the encoder
    weights. These are always the same, so they get pulled in from the `encoder.pth`
    file, which holds the trained weights for the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: For the decoder, by default, we want to load the “b” weights, which are stored
    in the `decoderb.pth` file, but you may want the “a” face to be swapped into the
    “b” image, so we included a command line switch that will load the “a” weights
    from the `decodera.pth` file instead. The weights work identically and correlate
    to the faces that were used to train originally. The exact order doesn’t matter
    since we included the `swap` flag, but only one direction could be default, so
    the “b” face onto the “a” image is the assumption unless overridden here.
  prefs: []
  type: TYPE_NORMAL
- en: No matter which decoder is loaded, we first load the weights and then assign
    them to the state dictionary of the model. PyTorch handles all the specifics of
    getting the dictionary loaded as matrices and into a form ready to handle tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we move the models to the GPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the device is set to `cuda`, we load the models onto the GPU. To do this,
    we tell PyTorch to use `cuda` on the models, which will handle the nitty-gritty
    of moving the models from the CPU to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we need to get the data loaded and into a format that PyTorch expects:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the alignment data from a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code loads the alignment data from the JSON file saved by the extract process.
    This file includes all the information that we need to be able to pull the face
    from the original image, convert it, and paste it back into the image. This uses
    the information from the extraction instead of doing it on the fly because that
    information is already generated when we created the training data, and re-using
    that data saves a lot of time as well as enables clean up and manual editing of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: You can specify the JSON file to load with the data for the image data that
    you’re converting, but, if left blank, the default locations will be looked at
    which, unless you changed it during extraction, should find the file.
  prefs: []
  type: TYPE_NORMAL
- en: We use `json_tricks` again because of its very powerful handling of NumPy arrays,
    which automatically loads the arrays back into the correct datatype and matrix
    shape.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: While the inclusion or the description of tools that edit these alignments are
    outside the scope of this book, the Faceswap project does include advanced alignment
    modification tools, including an advanced “manual” tool that allows click-and-drag
    editing of landmarks and faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to get a list of images to convert:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code loads all the images from the folder and then filters the images by
    throwing out any that don’t exist in the alignment data we loaded from the JSON
    data file. This makes sure that we have all the information to convert the image
    since even if a new image were added to the folder, we would need to extract information
    to be able to convert the file anyway.
  prefs: []
  type: TYPE_NORMAL
- en: The conversion loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we begin the loop that will go through each individual image one at a
    time to convert them:'
  prefs: []
  type: TYPE_NORMAL
- en: We’re now entering the loop and loading images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code loads the image and prepares it for use. First, it gets the filename
    and extension into variables so we can use them again later. It then loads the
    file in **blue, green, red** (**BGR**) color order and converts it into a **red,
    green, blue** (**RGB**)-ordered image as expected by our AI. Then, it saves the
    width, height, and color channels into variables so we can use them again later.
    Finally, it creates a working copy of the output image so that we can swap any
    faces in that image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we start another loop, this time for faces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This loop will process each face that is found in the alignment file and swap
    the faces that are found in it.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing it does is pull the face from the frame using the pre-computed
    warp matrix that was saved in the alignment file. This matrix allows us to align
    the face and generate a 256x256 image of it.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we convert that face image into a tensor and move the channels into the
    order that PyTorch expects them. The first part of the tensor conversion is to
    convert from an integer range of 0–255 to a standard range of 0–1\. We do this
    by dividing by 255\. Then we use `permute` to reorder the matrix because PyTorch
    wants the channels to be first, while OpenCV has them last.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a smaller 64x64 copy of the tensor, which is what we’ll actually
    feed into the model. Since we’re doing this one image at a time, we’re effectively
    working with a batch size of 1, but we need to use `unsqueeze` on the tensor to
    create the batch channel of the tensor. This just adds a new dimension of size
    1, which contains the image we want to convert.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if we are using `cuda`, we move the smaller aligned face tensor onto
    the GPU so that we can put it through the model there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we send the image through the AI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code does the actual AI swapping, and it’s rather astonishing how small
    it is.
  prefs: []
  type: TYPE_NORMAL
- en: We start this section by telling PyTorch that we want to run the AI in this
    section without keeping track of gradients by using `torch.no_grad()`. We can
    save a lot of VRAM and run the conversion faster. This isn’t strictly necessary
    here, but it is a good habit to get into.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we put the tensor containing the 64x64 face through the encoder and then
    the decoder to get a swapped face. The encoder’s output is fed straight into the
    decoder because we don’t need to do anything with the latent encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we apply the mask to the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We want to apply the mask so that we don’t swap a big square box of noise around
    the face. To do this, we will load the mask image and use it to cut out just the
    face from the swap.
  prefs: []
  type: TYPE_NORMAL
- en: First, we resize the swapped face up to a 256x256 image. This is done because
    the mask is a 256x256 image, and applying it at a higher resolution helps to get
    the best detail on the edge instead of downscaling the mask to 64x64.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we load the mask image. To do this, we use the aligned face filename
    to generate the mask image filename. We then load that as a grayscale image using
    OpenCV’s image reader:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – An example of a mask image](img/B17535_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – An example of a mask image
  prefs: []
  type: TYPE_NORMAL
- en: That image is then converted into a tensor using a cutoff point where if a pixel
    of the grayscale mask image’s value is higher than 200 (in a range of 0-255),
    then treat it as a `1`; otherwise, treat it as a `0`. This gives us a clean binary
    mask where the `1` value is a face to swap and `0` is unimportant background.
    We can then use that mask to paste just the swapped face back into the original
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we apply the mask to the image. This is done by multiplying the output
    face by the mask and multiplying the original face by the inverse of the mask.
    Effectively, this combines the face from the swap result with the rest of the
    image being pulled from the pre-swapped aligned image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll put the face back in the original image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code section completes the face loop. To do this, we apply the face back
    to the output image.
  prefs: []
  type: TYPE_NORMAL
- en: First, we convert the face tensor back into a NumPy array that OpenCV can work
    with. To do this, we grab the first instance in the tensor; this effectively removes
    the batch size dimension. Then, we’ll use `permute` to move the channels back
    to the end of the matrix. We then have to multiply by 255 to get into the 0–255
    range of an integer. Finally, we convert the variable into an integer, making
    it usable in OpenCV as a proper image.
  prefs: []
  type: TYPE_NORMAL
- en: We then use OpenCV’s `cv2.warpAffine` with a couple of flags to copy the face
    back into the original image in its original orientation. The first flag we use
    is `cv2.BORDER_TRANSPARENT`, which makes it so that only the area of the smaller
    aligned face gets changed; the rest of the image is left as it was. Without that
    flag, the image would only include the replaced face square; the rest of the image
    would be black. The other flag we use is `cv2.WARP_INVERSE_MAP`, which tells `cv2.warpAffine`
    that we’re copying the image back into the original image instead of copying part
    of the original image out.
  prefs: []
  type: TYPE_NORMAL
- en: With those two flags, the aligned image of the face gets put back into the correct
    place of the original full-sized image. We do this with a copy of the original
    image so we can copy multiple faces onto the image if multiple faces were found.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we output the new image with the faces swapped:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last step of the image loop is to write the images in memory to separate
    image files. To do this, we first convert the images back into the BGR color order
    that OpenCV expects. Then we write the file out to the extract path using the
    same original filename with a PNG file type.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Example of originals (top) and swaps (bottom) of Bryan (top
    left) and Matt (top right), the authors](img/B17535_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Example of originals (top) and swaps (bottom) of Bryan (top left)
    and Matt (top right), the authors
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve run the conversion on the frames, we need to turn the images
    back into a video. Let’s do that now.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the video from images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The conversion code included produces swapped images, but if we want to create
    a video, we’ll need to combine the output into a video file. There are multiple
    options here, depending on what you want to include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is for including just the images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command line will convert all the frames into a video with some default
    options. The `Output.mp4` file will include the frames but won’t include any audio
    and will be at a default frame rate of 25 frames per second. This will be close
    enough to accurate for videos that came from film sources, such as Blu-rays or
    DVDs. If the video looks too fast or too slow, then your frame rate is incorrect,
    and you should look at the next option instead to match the correct frame rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Including the images at a specific frame rate:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command line will include the frames at a specific frame rate. The frame
    rate is something you’ll need to find yourself from your original video. One way
    to do it using `ffmpeg` is to run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output a lot of information, most of which will not be useful to
    us. What we need to do is look for a line containing the “stream” information
    for the video. It will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The important information here is where it says `59.32 fps`. In this case, we’d
    want to put the `59.32` into the framerate of the `ffmpeg` command.
  prefs: []
  type: TYPE_NORMAL
- en: This option still won’t include any audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Including audio with the video:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command will convert the video while also copying over audio from the original
    video file. It’s important to use the exact same file for the audio to line up.
    If the audio doesn’t line up correctly, you may want to double-check the frame
    rate and the number of frames.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we ran the convert process on a folder full of images, replacing
    the faces using a trained model. We also turned the images back into a video,
    including changes to account for frame rate and copying audio.
  prefs: []
  type: TYPE_NORMAL
- en: We started by going over how to prepare a video for conversion. The convert
    process requires data created by the extract process from [*Chapter 5*](B17535_05.xhtml#_idTextAnchor090),
    *Extracting Faces*, and a trained AI model from [*Chapter 6*](B17535_06.xhtml#_idTextAnchor107),
    *Training a Deepfake Model*. With all the parts from the previous chapters, we
    were ready to convert.
  prefs: []
  type: TYPE_NORMAL
- en: We then walked through the code for the conversion process. This involved looking
    at the initialization, where we covered getting the Python script ready to operate.
    We then loaded the AI models and got them set up to work on a GPU if we have one.
    Next, we got the data ready for us to convert the faces in each frame. Finally,
    we ran two nested loops, which processed every face in every frame, swapping them
    to the other face. This part gave us a folder filled with swapped faces.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we looked at some commands that took the folder of swapped face
    images and returned them into a video form, this involved taking every frame into
    the video, ensuring that the frame rate was correct, and the audio was copied
    if desired.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll start looking into the potential future of deepfakes,
    with the next chapter looking at applying the techniques we’ve learned about deepfakes
    to solve other problems.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use the mask to cut out the swapped face from the rest of the image but then
    copy it over to the aligned face. This means that the areas of the aligned image
    that aren’t the face also get a lower resolution. One way to fix this would be
    to apply the mask to the original image instead of the aligned image. To do this,
    you’ll need to call `cv2.warpAffine` separately for the mask and the aligned image,
    then use the mask to get just the face copied over. You may want to view the documentation
    for OpenCV’s `warpAffine` at [https://docs.opencv.org/3.4/d4/d61/tutorial_warp_affine.html](https://docs.opencv.org/3.4/d4/d61/tutorial_warp_affine.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be sure to account for the fact that OpenCV’s documentation is based on the
    C++ implementation, and things can be a bit different in the Python library. The
    tutorial pages have a **Python** button that lets you switch the tutorial to using
    the Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: We rely on pre-extracted faces in order to convert. This is because a lot of
    the data is already processed in the extract process and is already available,
    allowing you to filter images/faces that you don’t want to be converted. But if
    you’re running a lot of videos or planning on running conversion on live video,
    it might make sense to allow conversion to run on the fly. To do this, you can
    combine the extract process with convert and run the extraction steps as needed
    before you convert. You can look at the code in `C5-extract.py` and add the appropriate
    parts to the convert process to enable it to work directly on the images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We operated the convert process entirely on images, but it’s actually possible
    for Python to work directly with video files. To do this, try installing and using
    a library such as PyAV from [https://github.com/PyAV-Org/PyAV](https://github.com/PyAV-Org/PyAV)
    to read and write directly to video files instead of images. Remember that you
    may need to account for audio data and frame rate in the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One problem with the techniques used in this chapter is that the swapped-in
    face can look pretty obvious at the edges. This is because of a lack of color
    matching and edge blending. Both these techniques can improve the swap’s edges.
    There are a lot of color-matching techniques available; one option is histogram
    matching ([https://docs.opencv.org/3.4/d4/d1b/tutorial_histogram_equalization.html](https://docs.opencv.org/3.4/d4/d1b/tutorial_histogram_equalization.html)).
    You’ll need to match the RGB channels separately. Edge blending is usually done
    by blurring the mask; you can accomplish this by smoothing the mask image with
    OpenCV ([https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html](https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html)).
    This can dull the sharp edges of the swap.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The results from our AI here are limited to just 64x64 pixels. There are newer
    models that go higher but are still limited heavily by available GPU memory and
    can take a lot longer to train. To get around this, you could run the output through
    an AI upscaler, such as ESRGAN ([https://github.com/xinntao/Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN)),
    or a face-specific restoration tool, such as GFP-GAN ([https://github.com/TencentARC/GFPGAN](https://github.com/TencentARC/GFPGAN)).
    See if you can run the model’s output through these before returning the face
    to the original image to get a higher-quality result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Where to Now?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like all inventions, the development of deepfakes is just the beginning. Now
    that you know how deepfakes work, where can you take that knowledge and what can
    you do with it? You might be surprised at how flexible the techniques can be.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this part, we’ll examine some hypothetical projects and how techniques in
    deepfakes could be used to make them easier, as well as solving complicated issues
    that might otherwise stump the average developer (you’re not one of those – after
    all, you bought this book!). Then, we’ll ask the ultimate question: what will
    the future bring? We’ll try to answer it by looking at where generative AI might
    go in the near future, including looking at the limitations and challenges that
    these AI technologies must overcome.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B17535_08.xhtml#_idTextAnchor136), *Applying the Lessons of Deepfakes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17535_09.xhtml#_idTextAnchor152), *The Future of Generative
    AI*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
