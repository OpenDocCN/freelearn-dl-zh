- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Search and Recommendation Engines with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the core steps involved in building conversational
    applications. We started with a plain vanilla chatbot, then added more complex
    components, such as memory, non-parametric knowledge, and external tools. All
    of this was made straightforward with the pre-built components of LangChain, as
    well as Streamlit for UI rendering. Even though conversational applications are
    often seen as the “comfort zone” of generative AI and LLMs, those models do embrace
    a wider spectrum of applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to cover how LLMs can enhance recommendation systems,
    using both embeddings and generative models. We will learn how to create our own
    recommendation system application leveraging state-of-the-art LLMs using LangChain
    as the framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition and evolutions of recommendation systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How LLMs are impacting this field of research
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building recommendation systems with LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the tasks in this book, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face account and a user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI account and a user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python version 3.7.1 or later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Make sure to have the following Python packages installed: `langchain`, `python-dotenv`,
    `huggingface_hub`, `streamlit`, `lancedb`, `openai`, and `tiktoken`. These can
    be easily installed via `pip install` in your terminal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll find the code for this chapter in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to recommendation systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recommendation system is a computer program that recommends items for users
    of digital platforms such as e-commerce websites and social networks. It uses
    large datasets to develop models of users’ likes and interests, and then recommends
    similar items to individual users.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of recommendation systems, depending on the methods
    and data they use. Some of the common types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaborative filtering**: This type of recommendation system uses the ratings
    or feedback of other users who have similar preferences to the target user. It
    assumes that users who liked certain items in the past will like similar items
    in the future. For example, if user A and user B both liked movies X and Y, then
    the algorithm may recommend movie Z to user A if user B also liked it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collaborative filtering can be further divided into two subtypes: user-based
    and item-based:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User-based collaborative filtering** finds similar users to the target user
    and recommends items that they liked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Item-based collaborative filtering** finds similar items to the ones that
    the target user liked and recommends them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content-based filtering**: This type of recommendation system uses the features
    or attributes of the items themselves to recommend items that are similar to the
    ones that the target user has liked or interacted with before. It assumes that
    users who liked certain features of an item will like other items with similar
    features. The main difference with item-based collaborative filtering is that,
    while this latter item-based uses patterns of user behavior to make recommendations,
    content-based filtering uses information about the items themselves. For example,
    if user A liked movie X, which is a comedy with actor Y, then the algorithm may
    recommend movie Z, which is also a comedy with actor Y.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid filtering**: This type of recommendation system combines both collaborative
    and content-based filtering methods to overcome some of their limitations and
    provide more accurate and diverse recommendations. For example, YouTube uses hybrid
    filtering to recommend videos based on both the ratings and views of other users
    who have watched similar videos, and the features and categories of the videos
    themselves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge-based filtering**: This type of recommendation system uses explicit
    knowledge or rules about the domain and the user’s needs or preferences to recommend
    items that satisfy certain criteria or constraints. It does not rely on ratings
    or feedback from other users, but rather on the user’s input or query. For example,
    if user A wants to buy a laptop with certain specifications and budget, then the
    algorithm may recommend a laptop that satisfies those criteria. Knowledge-based
    recommender systems work well when there is no or little rating history available,
    or when the items are complex and customizable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within the above frameworks, there are then various machine learning techniques
    that can be used, which we will cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Existing recommendation systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Modern recommendation systems use **machine learning** (**ML**) techniques
    to make better predictions about users’ preferences, based on the available data
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User behavior data**:Insights about user interaction with a product. This
    data can be acquired from factors like user ratings, clicks, and purchase records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User demographic data**: This refers to personal information about users,
    including details like age, educational background, income level, and geographical
    location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product attribute data**: This involves information about the characteristics
    of a product, such as genres of books, casts of movies, or specific cuisines in
    the context of food.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As of today, some of the most popular ML techniques are K-nearest neighbors,
    dimensionality reduction, and neural networks. Let’s look at these methods in
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**K-nearest neighbors** (**KNN**) is an ML algorithm that can be used for both
    classification and regression problems. It works by finding the *k* closest data
    points (where *k* refers to the number of nearest data point you want to find,
    and is set by the user before initializing the algorithm) to a new data point
    and using their labels or values to make a prediction. KNN is based on the assumption
    that similar data points are likely to have similar labels or values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'KNN can be applied to recommendation systems in the context of collaborative
    filtering, both user-based and item-based:'
  prefs: []
  type: TYPE_NORMAL
- en: User-based KNN is a type of collaborative filtering, which uses the ratings
    or feedback of other users who have similar tastes or preferences to the target
    user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let’s say we have three users: Alice, Bob, and Charlie. They all
    buy books online and rate them. Alice and Bob both liked (rated highly) the series,
    *Harry Potter*, and the book, *The Hobbit*. The system sees this pattern and considers
    Alice and Bob to be similar.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, if Bob also liked the book *A Game of Thrones*, which Alice hasn’t read
    yet, the system will recommend *A Game of Thrones* to Alice. This is because it
    assumes that since Alice and Bob have similar tastes, Alice might also like *A
    Game of Thrones*.
  prefs: []
  type: TYPE_NORMAL
- en: Item-based KNN is another type of collaborative filtering, which uses the attributes
    or features of the items to recommend similar items to the target user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, let’s consider the same users and their ratings for the books.
    The system notices that the *Harry Potter* series and the book, *The Hobbit* are
    both liked by Alice and Bob. So, it considers these two books to be similar.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if Charlie reads and likes *Harry Potter*, the system will recommend *The
    Hobbit* to Charlie. This is because it assumes that since *Harry Potter* and *The
    Hobbit* are similar (both liked by the same users), Charlie might also like *The
    Hobbit*.
  prefs: []
  type: TYPE_NORMAL
- en: 'KNN is a popular technique in recommendation systems, but it has some pitfalls:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: KNN can become computationally expensive and slow when dealing
    with large datasets, as it requires calculating distances between all pairs of
    items or users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cold-start problem**: KNN struggles with new items or users that have limited
    or no interaction history, as it relies on finding neighbors based on historical
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data sparsity**: KNN performance can degrade in sparse datasets where there
    are many missing values, making it challenging to find meaningful neighbors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature relevance**: KNN treats all features equally and assumes that all
    features contribute equally to similarity calculations. This may not hold true
    in scenarios where some features are more relevant than others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice of K**: Selecting the appropriate value of K (number of neighbors)
    can be subjective and impact the quality of recommendations. A small K may result
    in noise, while a large K may lead to overly broad recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally speaking, KNN is recommended in scenarios with small datasets with
    minimal noise (so that outliers, missing values and other noises do not impact
    the distance metric) and dynamic data (KNN is an instance-based method that doesn’t
    require retraining and can adapt to changes quickly).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, further techniques are widely used in the file of recommendation
    systems, such as matrix factorization.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix factorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matrix factorization is a technique used in recommendation systems to analyze
    and predict user preferences or behaviors based on historical data. It involves
    decomposing a large matrix into two or more smaller matrices to uncover latent
    features that contribute to the observed data patterns and address the so-called
    “curse of dimensionality.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality refers to challenges that arise when dealing with
    high-dimensional data. It leads to increased complexity, sparse data, and difficulties
    in analysis and modeling due to the exponential growth of data requirements and
    potential overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of recommendation systems, this technique is employed to predict
    missing values in the user-item interaction matrix, which represents users’ interactions
    with various items (such as movies, products, or books).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the following example. Imagine you have a matrix where rows
    represent users, columns represent movies, and the cells contain ratings (from
    1 as lowest to 5 as highest). However, not all users have rated all movies, resulting
    in a matrix with many missing entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Movie 1 | Movie 2 | Movie 3 | Movie 4 |'
  prefs: []
  type: TYPE_TB
- en: '| User 1 | 4 | - | 5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| User 2 | - | 3 | - | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| User 3 | 5 | 4 | - | 3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.1: Example of a dataset with missing data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix factorization aims to break down this matrix into two matrices: one
    for users and another for movies, with a reduced number of dimensions (latent
    factors). These latent factors could represent attributes like genre preferences
    or specific movie characteristics. By multiplying these matrices, you can predict
    the missing ratings and recommend movies that the users might enjoy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different algorithms for matrix factorization, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Singular value decomposition** (**SVD**) decomposes a matrix into three separate
    matrices, where the middle matrix contains singular values that represent the
    importance of different components in the data. It’s widely used in data compression,
    dimensionality reduction, and collaborative filtering in recommendation systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) is a technique to reduce the dimensionality
    of data by transforming it into a new coordinate system aligned with the principal
    components. These components capture the most significant variability in the data,
    allowing efficient analysis and visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-negative matrix factorization** (**NMF**) decomposes a matrix into two
    matrices with non-negative values. It’s often used for topic modeling, image processing,
    and feature extraction, where the components represent non-negative attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the context of recommendation systems, probably the most popular technique
    is SVD (thanks to its interpretability, flexibility, and ability to handle missing
    values and performance), so let’s use this one to go on with our example. We will
    use the Python `numpy` module to apply SVD as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `U` matrix contains user-related information, the `s` matrix
    contains singular values, and the `V` matrix contains movie-related information.
    By selecting a certain number of latent factors (`num_latent_factors`), you can
    reconstruct the original matrix with reduced dimensions, while setting the `full_matrices=False`
    parameter in the `np.linalg.svd` function ensures that the decomposed matrices
    are truncated to have dimensions consistent with the selected number of latent
    factors.
  prefs: []
  type: TYPE_NORMAL
- en: These predicted ratings can then be used to recommend movies with higher predicted
    ratings to users. Matrix factorization enables recommendation systems to uncover
    hidden patterns in user preferences and make personalized recommendations based
    on those patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix factorization has been a widely used technique in recommendation systems,
    especially when dealing with large datasets containing a substantial number of
    users and items, since it efficiently captures latent factors even in such scenarios;
    or when you want personalized recommendations based on latent factors, since it
    learns unique latent representations for each user and item. However, it has some
    pitfalls (some similar to the KNN’s technique):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cold-start problem**: Similar to KNN, matrix factorization struggles with
    new items or users that have limited or no interaction history. Since it relies
    on historical data, it can’t effectively provide recommendations for new items
    or users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data sparsity**: As the number of users and items grows, the user-item interaction
    matrix becomes increasingly sparse, leading to challenges in accurately predicting
    missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: For large datasets, performing matrix factorization can be
    computationally expensive and time-consuming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited context**: Matrix factorization typically only considers user-item
    interactions, ignoring contextual information like time, location, or additional
    user attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, **neural networks** (**NNs**) have been explored as an alternative to
    mitigate these pitfalls in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NNs are used in recommendation systems to improve the accuracy and personalization
    of recommendations by learning intricate patterns from data. Here’s how neural
    networks are commonly applied in this context:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaborative filtering with neural networks**: Neural networks can model
    user-item interactions by embedding users and items into continuous vector spaces.
    These embeddings capture latent features that represent user preferences and item
    characteristics. Neural collaborative filtering models combine these embeddings
    with neural network architectures to predict ratings or interactions between users
    and items.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content-based recommendations**: In content-based recommendation systems,
    neural networks can learn representations of item content, such as text, images,
    or audio. These representations capture item characteristics and user preferences.
    Neural networks like **convolutional neural networks** (**CNNs**) and **recurrent
    neural networks** (**RNNs**) are used to process and learn from item content,
    enabling personalized content-based recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential models**: In scenarios where user interactions have a temporal
    sequence, such as clickstreams or browsing history, RNNs or variants such as **long
    short-term memory** (**LSTM**) networks can capture temporal dependencies in the
    user behavior and make sequential recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoencoders and variational autoencoders** (**VAEs**) can be used to learn
    low-dimensional representations of users and items.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are a type of neural network architecture used for unsupervised
    learning and dimensionality reduction. They consist of an encoder and a decoder.
    The encoder maps the input data into a lower-dimensional latent space representation,
    while the decoder attempts to reconstruct the original input data from the encoded
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: VAEs are an extension of traditional autoencoders that introduce probabilistic
    elements. VAEs not only learn to encode the input data into a latent space but
    also model the distribution of this latent space using probabilistic methods.
    This allows for the generation of new data samples from the learned latent space.
    VAEs are used for generative tasks like image synthesis, anomaly detection, and
    data imputation.
  prefs: []
  type: TYPE_NORMAL
- en: In both autoencoders and VAEs, the idea is to learn a compressed and meaningful
    representation of the input data in the latent space, which can be useful for
    various tasks including feature extraction, data generation, and dimensionality
    reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'These representations can then be used to make recommendations by identifying
    similar users and items in the latent space. In fact, the unique architecture
    that features NNs allows for the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Side information integration**: NNs can incorporate additional user and item
    attributes, such as demographic information, location, or social connections,
    to improve recommendations by learning from diverse data sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep reinforcement learning**: In certain scenarios, deep reinforcement learning
    can be used to optimize recommendations over time, learning from user feedback
    to suggest actions that maximize long-term rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NNs offer flexibility and the ability to capture complex patterns in data,
    making them well suited for recommendation systems. However, they also require
    careful design, training, and tuning to achieve optimal performance. NNs also
    bring their own challenges, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increased complexity**: NNs, especially **deep neural networks** (**DNNs**),
    can become incredibly complex due to their layered architecture. As we add more
    hidden layers and neurons, the model’s capacity to learn intricate patterns increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training requirements**: NNs are heavy models whose training requires special
    hardware requirements including GPUs, which might be very expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Potential overfitting**: Overfitting occurs when an ANN learns to perform
    exceptionally well on the training data but fails to generalize to unseen data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting appropriate architectures, handling large datasets, and tuning hyperparameters
    are essential to effectively use NNs in recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though relevant advancements have been made in recent years, the aforementioned
    techniques still suffer from some pitfalls, primarily their being task-specific.
    For example, a rating-prediction recommendation system will not be able to tackle
    a task where we need to recommend the top *k* items that likely match the user’s
    taste. Actually, if we extend this limitation to other “pre-LLMs” AI solutions,
    we might see some similarities: it is indeed the task-specific situation that
    LLMs and, more generally, Large Foundation Models are revolutionizing, being highly
    generalized and adaptable to various tasks, depending on user’s prompts and instructions.
    Henceforth, extensive research in the field of recommendation systems is being
    done into what extent LLMs can enhance the current models. In the following sections,
    we will cover the theory behind these new approaches referring to recent papers
    and blogs about this emerging domain.'
  prefs: []
  type: TYPE_NORMAL
- en: How LLMs are changing recommendation systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw in previous chapters how LLMs can be customized in three main ways:
    pre-training, fine-tuning, and prompting. According to the paper *Recommender
    systems in the Era of Large Language Models (LLMs)* from Wenqi Fan et al., these
    techniques can also be used to tailor an LLM to be a recommender system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-training**: Pre-training LLMs for recommender systems is an important
    step to enable LLMs to acquire extensive world knowledge and user preferences,
    and to adapt to different recommendation tasks with zero or few shots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a recommendation system LLM is P5, introduced by Shijie Gang
    et al. in their paper *Recommendation as Language Processing (RLP): A Unified
    Pretrain, Personalized Prompt & Predict Paradigm (P5)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'P5 is a unified text-to-text paradigm for building recommender systems using
    **large language models** (**LLMs**). It consists of three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pretrain: A foundation language model based on T5 architecture is pretrained
    on a large-scale web corpus and fine-tuned on recommendation tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Personalized prompt: A personalized prompt is generated for each user based
    on their behavior data and contextual features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Predict: The personalized prompt is fed into the pretrained language model
    to generate recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P5 is based on the idea that LLMs can encode extensive world knowledge and user
    preferences and can be adapted to different recommendation tasks with zero or
    few shots.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning**: Training an LLM from scratch is a highly computational-intensive
    activity. An alternative and less intrusive approach to customize an LLM for recommendation
    systems might be fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More specifically, the authors of the paper review two main strategies for
    fine-tuning LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Full-model fine-tuning** involves changing the entire model’s weights based
    on task-specific recommendation datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameter-efficient fine-tuning** aims to change only a small part of weights
    or develop trainable adapters to fit specific tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompting**: The third and “lightest” way of tailoring LLMs to be recommender
    systems is prompting. According to the authors, there are three main techniques
    for prompting LLMs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conventional prompting** aims to unify downstream tasks into language generation
    tasks by designing text templates or providing a few input-output examples.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In-context learning** enables LLMs to learn new tasks based on contextual
    information without fine-tuning.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain-of-thought** enhances the reasoning abilities of LLMs by providing
    multiple demonstrations to describe the chain of thought as examples within the
    prompt. The authors also discuss the advantages and challenges of each technique
    and provide some examples of existing methods that adopt them.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of the typology, prompting is the fastest way to test whether a general-purpose
    LLM can tackle recommendation systems’ tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The application of LLMs within the recommendation system domain is raising interest
    in the research field, and there is already some interesting evidence of the results
    as seen above.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to implement our own recommendation application
    using the prompting approach and leveraging the capabilities of LangChain as an
    AI orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an LLM-powered recommendation system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have covered some theory about recommendation systems and emerging
    research on how LLMs can enhance them, let’s start building our recommendation
    app, which will be a movie recommender system called MovieHarbor. The goal will
    be to make it as general as possible, meaning that we want our app to be able
    to address various recommendations tasks with a conversational interface. The
    scenario we are going to simulate will be that of the so-called “cold start,”
    concerning the first interaction of a user with the recommendation system where
    we do not have the user’s preference history. We will leverage a movie database
    with textual descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, we will use the *Movie recommendation data* dataset, available
    on Kaggle at [https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data](https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data).
  prefs: []
  type: TYPE_NORMAL
- en: The reason for using a dataset with a textual description of each movie (alongside
    information such as ratings and movie titles) is so that we can get the embeddings
    of the text. So let’s start building our MovieHarbor application.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to apply LLMs to our dataset, we first need to preprocess the data.
    The initial dataset included several columns; however, the ones we are interested
    in are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Genres**: A list of applicable genres for the movie.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Title**: The movie’s title.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overview**: Textual description of the plot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vote_average**: A rating from 1 to 10 for a given movie'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vote_count**: The number of votes for a given movie.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I won’t report here the whole code (you can find it in the GitHub repo of this
    book at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml)),
    however, I will share the main steps of data preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we format the `genres` column into a `numpy` array, which is easier
    to handle than the original dictionary format in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we merge the `vote_average` and `vote_count` columns into a single column,
    which is the weighted ratings with respect to the number of votes. I’ve also limited
    the rows to the 95^(th) percentile of the number of votes, so that we can get
    rid of minimum vote counts to prevent skewed results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a new column called `combined_info` where we are going to merge
    all the elements that will be provided as context to the LLMs. Those elements
    are the movie title, overview, genres, and ratings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We tokenize the movie `combined_info` so that we will get better results while
    embedding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: '`cl100k_base` is the name of a tokenizer used by OpenAI’s embeddings API. A
    tokenizer is a tool that splits a text string into units called tokens, which
    can then be processed by a neural network. Different tokenizers have different
    rules and vocabularies for how to split the text and what tokens to use.'
  prefs: []
  type: TYPE_NORMAL
- en: The `cl100k_base` tokenizer is based on the **byte pair encoding** (**BPE**)
    algorithm, which learns a vocabulary of subword units from a large corpus of text.
    The `cl100k_base` tokenizer has a vocabulary of 100,000 tokens, which are mostly
    common words and word pieces, but also include some special tokens for punctuation,
    formatting, and control. It can handle texts in multiple languages and domains,
    and can encode up to 8,191 tokens per input.
  prefs: []
  type: TYPE_NORMAL
- en: 'We embed the text with `text-embedding-ada-002`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After changing some columns’ names and dropping unnecessary columns, the final
    dataset looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21714_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Sample of the final movies dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at a random row of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The last change we will make is modifying some naming conventions and data
    types as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our final dataset, we need to store it in a VectorDB. For
    this purpose, we are going to leverage **LanceDB**, an open-source database for
    vector-search built with persistent storage, which greatly simplifies the retrieval,
    filtering, and management of embeddings and also offers a native integration with
    LangChain. You can easily install LanceDB via `pip install lancedb`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have all our ingredients, we can start working with those embeddings
    and start building our recommendation system. We will start with a simple task
    in a cold-start scenario, adding progressive layers of complexity with LangChain
    components. Afterwards, we will also try a content-based scenario to challenge
    our LLMs with diverse tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Building a QA recommendation chatbot in a cold-start scenario
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous sections, we saw how the cold-start scenario – that means interacting
    with a user for the first time without their backstory – is a problem often encountered
    by recommendation systems. The less information we have about a user, the harder
    it is to match the recommendations to their preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to simulate a cold-start scenario with LangChain
    and OpenAI’s LLMs with the following high-level architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer  Description automatically generated](img/B21714_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: High-level architecture of recommendation system in a cold-start
    scenario'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we’ve already saved our embeddings in LanceDB. Now,
    we are going to build a LangChain RetrievalQA retriever, a chain component designed
    for question-answering against an index. In our case, we will use the vector store
    as our index retriever. The idea is that the chain returns the top *k* most similar
    movies upon the user’s query, using cosine similarity as the distance metric (which
    is the default).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s start building the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using only the movie overview as information input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the corresponding output (I will display a truncated version
    of the output, showing only the first out of four document sources):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, alongside each `Document`, all variables are reported as metadata,
    plus the distance is also reported as a score. The lower the distance, the greater
    the proximity between the user’s query and the movie’s text embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have gathered the most similar documents, we want a conversational
    response. For this goal, in addition to the embedding models, we will also use
    OpenAI’s completion model GPT-3 and combine it in RetrievalQA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we set the `return_source_documents=True` parameter, we can also retrieve
    the document sources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that the first document reported is not the one the model suggested. This
    occurred probably because of the rating, which is lower than Transformers (which
    was only the third result). This is a great example of how the LLM was able to
    consider multiple factors, on top of similarity, to suggest a movie to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model was able to generate a conversational answer, however, it is still
    using only a part of the available information – the textual overview. What if
    we want our MovieHarbor system to also leverage the other variables? We can approach
    the task in two ways:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The “filter” way**: This approach consists of adding some filters as **kwargs**
    to our retriever, which might be required by the application before responding
    to the user. Those questions might be, for example, about the genre of a movie.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let’s say we want to provide results featuring only those movies
    for which the genre is tagged as comedy. You can achieve this with the following
    code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The filter can also operate at the metadata level, as shown in the following
    example, where we want to filter only results with a rating above 7:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**The “agentic” way**: This is probably the most innovative way to approach
    the problem. Making our chain agentic means converting the retriever to a tool
    that the agent can leverage if needed, including the additional variables. By
    doing so, it would be sufficient for the user to provide their preferences in
    natural language so that the agent can retrieve the most promising recommendation
    if needed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see how to implement this with code, asking specifically for an action
    movie (thus filtering on the `genre` variable):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see a glimpse of the chain of thoughts and the output produced (always
    based on the four most similar movies according to cosine similarity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we might also want to make our application more tailored toward its
    goal of being a recommender system. To do so, we need to do some prompt engineering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: One of the advantages of using LangChain’s pre-built components, such as the
    RetrievalQA chain, is that they come with a pre-configured, well-curated prompt
    template. Before overriding the existing prompt, it’s a good practice to inspect
    it, so that you can also see which variables (within `{}`) are already expected
    from the component.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore the existing prompt, you can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s say, for example, that we want our system to return three suggestions
    for each user’s request, with a short description of the plot and the reason why
    the user might like it. The following is a sample prompt that could match this
    goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to pass it into our chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Another thing that we might want to implement in our prompt is the information
    gathered with the conversational preliminary questions that we might want to set
    as a welcome page. For example, before letting the user input their natural language
    question, we might want to ask their age, gender, and favorite movie genre. To
    do so, we can insert in our prompt a section where we can format the input variables
    with those shared by the user, and then combine this prompt chunk in the final
    prompt we are going to pass to the chain. Below you can find an example (for simplicity,
    we are going to set the variables without asking the user):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s format the prompt and pass it into our chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the system considered the user’s information provided. When
    we build the front-end of MovieHarbor, we will make this information dynamic as
    preliminary questions proposed to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Building a content-based system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we covered the cold-start scenario where the system
    knew nothing about the user. Sometimes, recommender systems already have some
    backstory about users, and it is extremely useful to embed this knowledge in our
    application. Let’s imagine, for example, that we have a users database where the
    system has stored all the registered user’s information (such as age, gender,
    country, etc.) as well as the movies the user has already watched alongside their
    rating.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we will need to set a custom prompt that is able to retrieve this
    information from a source. For simplicity, we will create a sample dataset with
    users’ information with just two records, corresponding to two users. Each user
    will exhibit the following variables: username, age, gender, and a dictionary
    containing movies already watched alongside with the rating they gave to them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-level architecture is represented by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer flowchart  Description automatically generated](img/B21714_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: High-level architecture of a content-based recommendation system'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down this architecture and examine each step to build the final
    chat for this content-based system, starting from the available users’ data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed earlier, we now have a bit of information about our users’ preferences.
    More specifically, imagine we have a dataset containing users’ attributes (name,
    age, gender) along with their reviews (a score from 1 to 10) of some movies. The
    following is the code used to create the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A black and white screen with white text  Description automatically generated](img/B21714_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Sample users dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we want to do now is apply the same logic of the prompt of the cold start
    with the formatting with variables. The difference here is that, rather than asking
    the user to provide the values for those variables, we will directly collect them
    from our user dataset. So, we first define our prompt chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then format the `user_info` chunk as follows (assuming that the user interacting
    with the system is `Alice`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now use this prompt within our chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model is now able to recommend a list of movies to Alice
    based on the user’s information about past preferences, retrieved as context within
    the model’s metaprompt.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in this scenario, we used as dataset a simple pandas dataframe. In
    production scenarios, a best practice for storing variables related to a task
    to be addressed (such as a recommendation task) is that of using a feature store.
    Feature stores are data systems that are designed to support machine learning
    workflows. They allow data teams to store, manage, and access features that are
    used for training and deploying machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, LangChain offers native integrations towards some of the most
    popular features stores:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feast:** This is an open-source feature store for machine learning. It allows
    teams to define, manage, discover, and serve features. Feast supports batch and
    streaming data sources and integrates with various data processing and storage
    systems. Feast uses BigQuery for offline features and BigTable or Redis for online
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tecton:** This is a managed feature platform that provides a complete solution
    for building, deploying, and using features for machine learning. Tecton allows
    users to define features in code, version control them, and deploy them to production
    with best practices. Furthermore, it integrates with existing data infrastructure
    and ML platforms like SageMaker and Kubeflow, and it uses Spark for feature transformations
    and DynamoDB for online feature serving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Featureform:** This is a virtual feature store that transforms existing data
    infrastructure into a feature store. Featureform allows users to create, store,
    and access features using standard feature definitions and a Python SDK. It orchestrates
    and manages the data pipelines required for feature engineering and materialization,
    and it is compatible with a wide range of data systems, such as Snowflake, Redis,
    Spark, and Cassandra.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureML Managed Feature Store:** This is a new type of workspace that lets
    users discover, create, and operationalize features. This service integrates with
    existing data stores, feature pipelines, and ML platforms like Azure Databricks
    and Kubeflow. Plus, it uses SQL, PySpark, SnowPark, or Python for feature transformations
    and Parquet/S3 or Cosmos DB for feature storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can read more about LangChain’s integration with features at [https://blog.langchain.dev/feature-stores-and-llms/](https://blog.langchain.dev/feature-stores-and-llms/).
  prefs: []
  type: TYPE_NORMAL
- en: Developing the front-end with Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen the logic behind an LLM-powered recommendation system,
    it is time to give a GUI to our MovieHarbor. To do so, we will once again leverage
    Streamlit, and we will assume the cold-start scenario. As always, you can find
    the whole Python code in the GitHub book repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: As per the Globebotter application in *Chapter 6*, in this case also you need
    to create a `.py` file to run in your terminal via `streamlit run file.py`. In
    our case, the file will be named `movieharbor.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now summarize the key steps to build the app with the front-end:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure the application webpage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the credentials and establish the connection to LanceDB:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create some widgets for the user to define their features and movies preferences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the parametrized prompt chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the `RetrievalQA` chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Insert the search bar for the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And that’s it! You can run the final result in your terminal with `streamlit
    run movieharbor.py`. It looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Sample front-end for Movieharbor with Streamlit'
  prefs: []
  type: TYPE_NORMAL
- en: So, you can see, in just few lines of code we were able to set up a webapp for
    our MovieHarbor. Starting from this template, you can customize your layout with
    Streamlit’s components, as well as tailor it to content-based scenarios. Plus,
    you can customize your prompts in such a way that the recommender acts as you
    prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how LLMs could change the way we approach a recommendation
    system task. We started from the analysis of the current strategies and algorithms
    for building recommendation applications, differentiating between various scenarios
    (collaborative filtering, content-based, cold start, etc.) as well as different
    techniques (KNN, matrix factorization, and NNs).
  prefs: []
  type: TYPE_NORMAL
- en: We then moved to the new, emerging field of research into how to apply the power
    of LLMs to this field, and explored the various experiments that have been done
    in recent months.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging this knowledge, we built a movie recommender application powered
    by LLMs, using LangChain as the AI orchestrator and Streamlit as the front-end,
    showing how LLMs can revolutionize this field thanks to their reasoning capabilities
    as well as their generalization. This was just one example of how LLMs not only
    can open new frontiers, but can also enhance existing fields of research.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see what these powerful models can do when working
    with structured data.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recommendation as Language Processing** (**RLP**): A Unified **Pretrain,
    Personalized Prompt & Predict Paradigm** (**P5**). [https://arxiv.org/abs/2203.13366](https://arxiv.org/abs/2203.13366)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain’s blog about featurestores. [https://blog.langchain.dev/feature-stores-and-llms/](https://blog.langchain.dev/feature-stores-and-llms/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feast. [https://docs.feast.dev/](https://docs.feast.dev/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tecton. [https://www.tecton.ai/](https://www.tecton.ai/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FeatureForm. [https://www.featureform.com/](https://www.featureform.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Machine Learning feature store. [https://learn.microsoft.com/en-us/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2](https://learn.microsoft.com/en-us/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/llm](https://packt.link/llm )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code214329708533108046.png)'
  prefs: []
  type: TYPE_IMG
