- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Search and Recommendation Engines with LLMs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM的搜索和推荐引擎
- en: In the previous chapter, we covered the core steps involved in building conversational
    applications. We started with a plain vanilla chatbot, then added more complex
    components, such as memory, non-parametric knowledge, and external tools. All
    of this was made straightforward with the pre-built components of LangChain, as
    well as Streamlit for UI rendering. Even though conversational applications are
    often seen as the “comfort zone” of generative AI and LLMs, those models do embrace
    a wider spectrum of applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了构建对话应用的核心步骤。我们从一个简单的聊天机器人开始，然后添加了更复杂的组件，如记忆、非参数知识库和外部工具。所有这些都可以通过LangChain的预构建组件以及Streamlit的UI渲染来实现。尽管对话应用通常被视为生成AI和LLM的“舒适区”，但这些模型确实拥抱了更广泛的应用范围。
- en: In this chapter, we are going to cover how LLMs can enhance recommendation systems,
    using both embeddings and generative models. We will learn how to create our own
    recommendation system application leveraging state-of-the-art LLMs using LangChain
    as the framework.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何利用嵌入和生成模型来增强推荐系统，我们将学习如何利用LangChain作为框架，利用最先进的LLM（大型语言模型）来创建自己的推荐系统应用。
- en: 'Throughout this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Definition and evolutions of recommendation systems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统的定义和演变
- en: How LLMs are impacting this field of research
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM如何影响这个研究领域
- en: Building recommendation systems with LangChain
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangChain构建推荐系统
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete the tasks in this book, you will need the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本书中的任务，你需要以下内容：
- en: Hugging Face account and a user access token.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face账户和用户访问令牌。
- en: OpenAI account and a user access token.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI账户和用户访问令牌。
- en: Python version 3.7.1 or later.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python版本3.7.1或更高版本。
- en: 'Make sure to have the following Python packages installed: `langchain`, `python-dotenv`,
    `huggingface_hub`, `streamlit`, `lancedb`, `openai`, and `tiktoken`. These can
    be easily installed via `pip install` in your terminal.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保安装以下Python包：`langchain`、`python-dotenv`、`huggingface_hub`、`streamlit`、`lancedb`、`openai`和`tiktoken`。这些包可以通过在终端中使用`pip
    install`命令轻松安装。
- en: You’ll find the code for this chapter in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到本章的代码：[https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml)。
- en: Introduction to recommendation systems
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统简介
- en: A recommendation system is a computer program that recommends items for users
    of digital platforms such as e-commerce websites and social networks. It uses
    large datasets to develop models of users’ likes and interests, and then recommends
    similar items to individual users.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是一种计算机程序，为电子商务网站和社交网络等数字平台上的用户提供推荐项目。它使用大量数据集来开发用户喜好和兴趣的模型，然后为单个用户提供类似项目的推荐。
- en: 'There are different types of recommendation systems, depending on the methods
    and data they use. Some of the common types are:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的方法和数据，推荐系统有多种类型。其中一些常见的类型包括：
- en: '**Collaborative filtering**: This type of recommendation system uses the ratings
    or feedback of other users who have similar preferences to the target user. It
    assumes that users who liked certain items in the past will like similar items
    in the future. For example, if user A and user B both liked movies X and Y, then
    the algorithm may recommend movie Z to user A if user B also liked it.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**：这种类型的推荐系统使用具有相似偏好的其他用户的评分或反馈。它假设过去喜欢某些项目的用户将来也会喜欢类似的项目。例如，如果用户A和用户B都喜欢电影X和Y，那么如果用户B也喜欢电影Z，算法可能会向用户A推荐电影Z。'
- en: 'Collaborative filtering can be further divided into two subtypes: user-based
    and item-based:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤可以进一步分为两种子类型：基于用户和基于项目：
- en: '**User-based collaborative filtering** finds similar users to the target user
    and recommends items that they liked.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于用户的协同过滤**寻找与目标用户相似的用户，并推荐他们喜欢的项目。'
- en: '**Item-based collaborative filtering** finds similar items to the ones that
    the target user liked and recommends them.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于项目的协同过滤**寻找与目标用户喜欢的项目相似的项目，并推荐它们。'
- en: '**Content-based filtering**: This type of recommendation system uses the features
    or attributes of the items themselves to recommend items that are similar to the
    ones that the target user has liked or interacted with before. It assumes that
    users who liked certain features of an item will like other items with similar
    features. The main difference with item-based collaborative filtering is that,
    while this latter item-based uses patterns of user behavior to make recommendations,
    content-based filtering uses information about the items themselves. For example,
    if user A liked movie X, which is a comedy with actor Y, then the algorithm may
    recommend movie Z, which is also a comedy with actor Y.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于内容的过滤**：这种类型的推荐系统使用项目本身的特征或属性来推荐与目标用户之前喜欢或互动过的项目相似的项目。它假设喜欢某个项目特定特征的用户也会喜欢具有相似特征的其它项目。与基于项目的协同过滤的主要区别在于，后者使用用户行为模式来做出推荐，而基于内容的过滤则使用关于项目的自身信息。例如，如果用户A喜欢电影X，这是一部由演员Y主演的喜剧，那么算法可能会推荐电影Z，这也是一部由演员Y主演的喜剧。'
- en: '**Hybrid filtering**: This type of recommendation system combines both collaborative
    and content-based filtering methods to overcome some of their limitations and
    provide more accurate and diverse recommendations. For example, YouTube uses hybrid
    filtering to recommend videos based on both the ratings and views of other users
    who have watched similar videos, and the features and categories of the videos
    themselves.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合过滤**：这种推荐系统结合了协同过滤和基于内容的过滤方法，以克服它们的一些局限性，并提供更准确和多样化的推荐。例如，YouTube 使用混合过滤根据观看过类似视频的其他用户的评分和观看次数，以及视频本身的特征和类别来推荐视频。'
- en: '**Knowledge-based filtering**: This type of recommendation system uses explicit
    knowledge or rules about the domain and the user’s needs or preferences to recommend
    items that satisfy certain criteria or constraints. It does not rely on ratings
    or feedback from other users, but rather on the user’s input or query. For example,
    if user A wants to buy a laptop with certain specifications and budget, then the
    algorithm may recommend a laptop that satisfies those criteria. Knowledge-based
    recommender systems work well when there is no or little rating history available,
    or when the items are complex and customizable.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于知识的过滤**：这种类型的推荐系统使用关于领域和用户需求或偏好的显式知识或规则来推荐满足某些标准或约束的项目。它不依赖于其他用户的评分或反馈，而是依赖于用户的输入或查询。例如，如果用户A想购买一款具有特定规格和预算的笔记本电脑，那么算法可能会推荐一款满足这些标准的笔记本电脑。基于知识的推荐系统在没有或很少的评分历史记录的情况下，或者当项目复杂且可定制时，效果很好。'
- en: Within the above frameworks, there are then various machine learning techniques
    that can be used, which we will cover in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述框架内，然后有各种机器学习技术可以使用，我们将在下一节中介绍。
- en: Existing recommendation systems
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有的推荐系统
- en: 'Modern recommendation systems use **machine learning** (**ML**) techniques
    to make better predictions about users’ preferences, based on the available data
    such as the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现代推荐系统使用**机器学习**（**ML**）技术来根据如下可用数据（例如）做出关于用户偏好的更好预测：
- en: '**User behavior data**:Insights about user interaction with a product. This
    data can be acquired from factors like user ratings, clicks, and purchase records.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户行为数据**：关于用户与产品互动的见解。这些数据可以从用户评分、点击和购买记录等因素中获得。'
- en: '**User demographic data**: This refers to personal information about users,
    including details like age, educational background, income level, and geographical
    location.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户人口统计数据**：这指的是关于用户个人的信息，包括诸如年龄、教育背景、收入水平和地理位置等细节。'
- en: '**Product attribute data**: This involves information about the characteristics
    of a product, such as genres of books, casts of movies, or specific cuisines in
    the context of food.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产品属性数据**：这涉及关于产品特征的信息，例如书籍的流派、电影的演员阵容，或食物背景下的特定菜系。'
- en: As of today, some of the most popular ML techniques are K-nearest neighbors,
    dimensionality reduction, and neural networks. Let’s look at these methods in
    detail.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一些最受欢迎的机器学习技术包括K-最近邻、降维和神经网络。让我们详细看看这些方法。
- en: K-nearest neighbors
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-最近邻
- en: '**K-nearest neighbors** (**KNN**) is an ML algorithm that can be used for both
    classification and regression problems. It works by finding the *k* closest data
    points (where *k* refers to the number of nearest data point you want to find,
    and is set by the user before initializing the algorithm) to a new data point
    and using their labels or values to make a prediction. KNN is based on the assumption
    that similar data points are likely to have similar labels or values.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**K近邻算法（KNN**）是一种可以用于分类和回归问题的机器学习算法。它通过找到到新数据点最近的k个数据点（其中k表示要找到的最近数据点的数量，由用户在初始化算法之前设置）来工作，并使用它们的标签或值进行预测。KNN基于相似数据点可能具有相似标签或值的假设。'
- en: 'KNN can be applied to recommendation systems in the context of collaborative
    filtering, both user-based and item-based:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: KNN可以应用于协同过滤的上下文中的推荐系统，包括基于用户和基于物品的：
- en: User-based KNN is a type of collaborative filtering, which uses the ratings
    or feedback of other users who have similar tastes or preferences to the target
    user.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于用户的KNN是一种协同过滤类型，它使用与目标用户有相似口味或偏好的其他用户的评分或反馈。
- en: 'For example, let’s say we have three users: Alice, Bob, and Charlie. They all
    buy books online and rate them. Alice and Bob both liked (rated highly) the series,
    *Harry Potter*, and the book, *The Hobbit*. The system sees this pattern and considers
    Alice and Bob to be similar.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有三个用户：Alice、Bob和Charlie。他们都在线购买书籍并对其进行评分。Alice和Bob都喜欢（高度评价）系列作品《哈利·波特》和书籍《霍比特人》。系统看到这个模式，并认为Alice和Bob是相似的。
- en: Now, if Bob also liked the book *A Game of Thrones*, which Alice hasn’t read
    yet, the system will recommend *A Game of Thrones* to Alice. This is because it
    assumes that since Alice and Bob have similar tastes, Alice might also like *A
    Game of Thrones*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果Bob也喜欢Alice尚未阅读的书籍《权力的游戏》，系统将向Alice推荐《权力的游戏》。这是因为它假设由于Alice和Bob有相似的口味，Alice也可能喜欢《权力的游戏》。
- en: Item-based KNN is another type of collaborative filtering, which uses the attributes
    or features of the items to recommend similar items to the target user.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于物品的KNN是另一种协同过滤类型，它使用物品的属性或特征来向目标用户推荐相似物品。
- en: For example, let’s consider the same users and their ratings for the books.
    The system notices that the *Harry Potter* series and the book, *The Hobbit* are
    both liked by Alice and Bob. So, it considers these two books to be similar.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑相同的用户及其对书籍的评分。系统注意到《哈利·波特》系列和书籍《霍比特人》都受到Alice和Bob的喜爱。因此，它认为这两本书是相似的。
- en: Now, if Charlie reads and likes *Harry Potter*, the system will recommend *The
    Hobbit* to Charlie. This is because it assumes that since *Harry Potter* and *The
    Hobbit* are similar (both liked by the same users), Charlie might also like *The
    Hobbit*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果Charlie阅读并喜欢《哈利·波特》，系统将向Charlie推荐《霍比特人》。这是因为它假设由于《哈利·波特》和《霍比特人》是相似的（被相同用户喜欢），Charlie也可能喜欢《霍比特人》。
- en: 'KNN is a popular technique in recommendation systems, but it has some pitfalls:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是推荐系统中的流行技术，但它有一些缺陷：
- en: '**Scalability**: KNN can become computationally expensive and slow when dealing
    with large datasets, as it requires calculating distances between all pairs of
    items or users.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：当处理大型数据集时，KNN可能会变得计算成本高昂且速度缓慢，因为它需要计算所有物品或用户对之间的距离。'
- en: '**Cold-start problem**: KNN struggles with new items or users that have limited
    or no interaction history, as it relies on finding neighbors based on historical
    data.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冷启动问题**：KNN在处理新物品或用户时遇到困难，这些物品或用户具有有限的或没有交互历史，因为它依赖于基于历史数据找到邻居。'
- en: '**Data sparsity**: KNN performance can degrade in sparse datasets where there
    are many missing values, making it challenging to find meaningful neighbors.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据稀疏性**：在存在许多缺失值的稀疏数据集中，KNN的性能可能会下降，这使得找到有意义的邻居变得具有挑战性。'
- en: '**Feature relevance**: KNN treats all features equally and assumes that all
    features contribute equally to similarity calculations. This may not hold true
    in scenarios where some features are more relevant than others.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征相关性**：KNN将所有特征视为同等重要，并假设所有特征对相似度计算的贡献是相等的。但在某些特征比其他特征更相关的情况下，这可能并不成立。'
- en: '**Choice of K**: Selecting the appropriate value of K (number of neighbors)
    can be subjective and impact the quality of recommendations. A small K may result
    in noise, while a large K may lead to overly broad recommendations.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K的选择**：选择适当的K值（邻居数量）可能是主观的，并影响推荐的质量。较小的K值可能导致噪声，而较大的K值可能导致过于宽泛的推荐。'
- en: Generally speaking, KNN is recommended in scenarios with small datasets with
    minimal noise (so that outliers, missing values and other noises do not impact
    the distance metric) and dynamic data (KNN is an instance-based method that doesn’t
    require retraining and can adapt to changes quickly).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，在数据集小且噪声最小（这样异常值、缺失值和其他噪声不会影响距离度量）以及动态数据（KNN是一种基于实例的方法，不需要重新训练，可以快速适应变化）的场景下推荐使用KNN。
- en: Additionally, further techniques are widely used in the file of recommendation
    systems, such as matrix factorization.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在推荐系统文件中广泛使用了其他技术，例如矩阵分解。
- en: Matrix factorization
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: Matrix factorization is a technique used in recommendation systems to analyze
    and predict user preferences or behaviors based on historical data. It involves
    decomposing a large matrix into two or more smaller matrices to uncover latent
    features that contribute to the observed data patterns and address the so-called
    “curse of dimensionality.”
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解是一种在推荐系统中使用的技术，用于根据历史数据分析和预测用户偏好或行为。它涉及将一个大矩阵分解成两个或更多较小的矩阵，以揭示导致观察到的数据模式的潜在特征，并解决所谓的“维度诅咒”问题。
- en: '**Definition**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: The curse of dimensionality refers to challenges that arise when dealing with
    high-dimensional data. It leads to increased complexity, sparse data, and difficulties
    in analysis and modeling due to the exponential growth of data requirements and
    potential overfitting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒指的是处理高维数据时出现的挑战。它导致复杂性增加、数据稀疏，以及由于数据需求指数增长和潜在的过拟合，分析和建模困难。
- en: In the context of recommendation systems, this technique is employed to predict
    missing values in the user-item interaction matrix, which represents users’ interactions
    with various items (such as movies, products, or books).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统的背景下，这项技术被用来预测用户-项目交互矩阵中的缺失值，该矩阵表示用户与各种项目（如电影、产品或书籍）的交互。
- en: 'Let’s consider the following example. Imagine you have a matrix where rows
    represent users, columns represent movies, and the cells contain ratings (from
    1 as lowest to 5 as highest). However, not all users have rated all movies, resulting
    in a matrix with many missing entries:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下示例。想象你有一个矩阵，其中行代表用户，列代表电影，单元格包含评分（从1作为最低到5作为最高）。然而，并非所有用户都对所有电影进行了评分，导致矩阵中有许多缺失项：
- en: '|  | Movie 1 | Movie 2 | Movie 3 | Movie 4 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | 电影1 | 电影2 | 电影3 | 电影4 |'
- en: '| User 1 | 4 | - | 5 | - |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 用户1 | 4 | - | 5 | - |'
- en: '| User 2 | - | 3 | - | 2 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 用户2 | - | 3 | - | 2 |'
- en: '| User 3 | 5 | 4 | - | 3 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 用户3 | 5 | 4 | - | 3 |'
- en: 'Table 7.1: Example of a dataset with missing data'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1：具有缺失数据的示例数据集
- en: 'Matrix factorization aims to break down this matrix into two matrices: one
    for users and another for movies, with a reduced number of dimensions (latent
    factors). These latent factors could represent attributes like genre preferences
    or specific movie characteristics. By multiplying these matrices, you can predict
    the missing ratings and recommend movies that the users might enjoy.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解旨在将这个矩阵分解成两个矩阵：一个用于用户，另一个用于电影，具有减少的维度（潜在因子）。这些潜在因子可能代表诸如类型偏好或特定电影特征等属性。通过乘以这些矩阵，可以预测缺失的评分并推荐用户可能喜欢的电影。
- en: 'There are different algorithms for matrix factorization, including the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解有不同的算法，包括以下几种：
- en: '**Singular value decomposition** (**SVD**) decomposes a matrix into three separate
    matrices, where the middle matrix contains singular values that represent the
    importance of different components in the data. It’s widely used in data compression,
    dimensionality reduction, and collaborative filtering in recommendation systems.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）将矩阵分解成三个独立的矩阵，其中中间矩阵包含奇异值，这些奇异值代表了数据中不同组件的重要性。它在数据压缩、降维和推荐系统中的协同过滤中得到广泛应用。'
- en: '**Principal component analysis** (**PCA**) is a technique to reduce the dimensionality
    of data by transforming it into a new coordinate system aligned with the principal
    components. These components capture the most significant variability in the data,
    allowing efficient analysis and visualization.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是一种通过将其转换到一个与主成分对齐的新坐标系来降低数据维度的技术。这些成分捕捉数据中最显著的变化，允许有效的分析和可视化。'
- en: '**Non-negative matrix factorization** (**NMF**) decomposes a matrix into two
    matrices with non-negative values. It’s often used for topic modeling, image processing,
    and feature extraction, where the components represent non-negative attributes.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非负矩阵分解**（**NMF**）将矩阵分解为两个具有非负值的矩阵。它通常用于主题建模、图像处理和特征提取，其中组件代表非负属性。'
- en: 'In the context of recommendation systems, probably the most popular technique
    is SVD (thanks to its interpretability, flexibility, and ability to handle missing
    values and performance), so let’s use this one to go on with our example. We will
    use the Python `numpy` module to apply SVD as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统的背景下，最流行的技术可能是 SVD（归功于其可解释性、灵活性和处理缺失值和性能的能力），因此让我们使用这个技术继续我们的例子。我们将使用 Python
    的 `numpy` 模块来应用 SVD，如下所示：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following is the output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为输出结果：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, the `U` matrix contains user-related information, the `s` matrix
    contains singular values, and the `V` matrix contains movie-related information.
    By selecting a certain number of latent factors (`num_latent_factors`), you can
    reconstruct the original matrix with reduced dimensions, while setting the `full_matrices=False`
    parameter in the `np.linalg.svd` function ensures that the decomposed matrices
    are truncated to have dimensions consistent with the selected number of latent
    factors.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`U` 矩阵包含与用户相关的信息，`s` 矩阵包含奇异值，而 `V` 矩阵包含与电影相关的信息。通过选择一定数量的潜在因子（`num_latent_factors`），你可以用降低维度的原始矩阵重建，同时将
    `np.linalg.svd` 函数中的 `full_matrices=False` 参数设置为 `False` 确保分解的矩阵截断到与所选潜在因子数量一致的维度。
- en: These predicted ratings can then be used to recommend movies with higher predicted
    ratings to users. Matrix factorization enables recommendation systems to uncover
    hidden patterns in user preferences and make personalized recommendations based
    on those patterns.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测评分可以用来向用户推荐预测评分更高的电影。矩阵分解使推荐系统能够揭示用户偏好中的隐藏模式，并根据这些模式进行个性化推荐。
- en: 'Matrix factorization has been a widely used technique in recommendation systems,
    especially when dealing with large datasets containing a substantial number of
    users and items, since it efficiently captures latent factors even in such scenarios;
    or when you want personalized recommendations based on latent factors, since it
    learns unique latent representations for each user and item. However, it has some
    pitfalls (some similar to the KNN’s technique):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解在推荐系统中被广泛使用，尤其是在处理包含大量用户和项目的庞大数据集时，因为它能够有效地捕捉到这些场景中的潜在因子；或者当你想要基于潜在因子进行个性化推荐时，因为它为每个用户和项目学习独特的潜在表示。然而，它也有一些缺陷（一些与
    KNN 技术相似）：
- en: '**Cold-start problem**: Similar to KNN, matrix factorization struggles with
    new items or users that have limited or no interaction history. Since it relies
    on historical data, it can’t effectively provide recommendations for new items
    or users.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冷启动问题**：与 KNN 类似，矩阵分解在处理新项目或用户时遇到困难，这些新项目或用户具有有限或没有交互历史。由于它依赖于历史数据，因此它无法有效地为新项目或用户提供推荐。'
- en: '**Data sparsity**: As the number of users and items grows, the user-item interaction
    matrix becomes increasingly sparse, leading to challenges in accurately predicting
    missing values.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据稀疏性**：随着用户和项目数量的增加，用户-项目交互矩阵变得越来越稀疏，导致准确预测缺失值具有挑战性。'
- en: '**Scalability**: For large datasets, performing matrix factorization can be
    computationally expensive and time-consuming.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：对于大型数据集，执行矩阵分解可能计算成本高且耗时。'
- en: '**Limited context**: Matrix factorization typically only considers user-item
    interactions, ignoring contextual information like time, location, or additional
    user attributes.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限上下文**：矩阵分解通常只考虑用户-项目交互，忽略了时间、位置或额外用户属性等上下文信息。'
- en: Hence, **neural networks** (**NNs**) have been explored as an alternative to
    mitigate these pitfalls in recent years.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，近年来，**神经网络**（**NNs**）被探索作为缓解这些缺陷的替代方案。
- en: Neural networks
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'NNs are used in recommendation systems to improve the accuracy and personalization
    of recommendations by learning intricate patterns from data. Here’s how neural
    networks are commonly applied in this context:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NNs）在推荐系统中用于通过从数据中学习复杂模式来提高推荐的准确性和个性化。以下是神经网络在这个背景下通常的应用方式：
- en: '**Collaborative filtering with neural networks**: Neural networks can model
    user-item interactions by embedding users and items into continuous vector spaces.
    These embeddings capture latent features that represent user preferences and item
    characteristics. Neural collaborative filtering models combine these embeddings
    with neural network architectures to predict ratings or interactions between users
    and items.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于神经网络的协同过滤**：神经网络可以通过将用户和物品嵌入到连续向量空间中来模拟用户-物品交互。这些嵌入捕捉了代表用户偏好和物品特征的潜在特征。基于神经网络的协同过滤模型将这些嵌入与神经网络架构相结合，以预测用户和物品之间的评分或交互。'
- en: '**Content-based recommendations**: In content-based recommendation systems,
    neural networks can learn representations of item content, such as text, images,
    or audio. These representations capture item characteristics and user preferences.
    Neural networks like **convolutional neural networks** (**CNNs**) and **recurrent
    neural networks** (**RNNs**) are used to process and learn from item content,
    enabling personalized content-based recommendations.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于内容的推荐**：在基于内容的推荐系统中，神经网络可以学习物品内容的表示，如文本、图像或音频。这些表示捕捉了物品特征和用户偏好。像**卷积神经网络**（**CNNs**）和**循环神经网络**（**RNNs**）这样的神经网络被用来处理和从物品内容中学习，从而实现个性化的基于内容的推荐。'
- en: '**Sequential models**: In scenarios where user interactions have a temporal
    sequence, such as clickstreams or browsing history, RNNs or variants such as **long
    short-term memory** (**LSTM**) networks can capture temporal dependencies in the
    user behavior and make sequential recommendations.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列模型**：在用户交互具有时间序列的场景中，例如点击流或浏览历史，RNNs或其变体如**长短期记忆**（**LSTM**）网络可以捕捉用户行为中的时间依赖性，并做出序列推荐。'
- en: '**Autoencoders and variational autoencoders** (**VAEs**) can be used to learn
    low-dimensional representations of users and items.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器和变分自动编码器**（**VAEs**）可以用来学习用户和物品的低维表示。'
- en: '**Definition**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Autoencoders are a type of neural network architecture used for unsupervised
    learning and dimensionality reduction. They consist of an encoder and a decoder.
    The encoder maps the input data into a lower-dimensional latent space representation,
    while the decoder attempts to reconstruct the original input data from the encoded
    representation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是一种用于无监督学习和降维的神经网络架构。它由编码器和解码器组成。编码器将输入数据映射到低维潜在空间表示，而解码器则试图从编码表示中重建原始输入数据。
- en: VAEs are an extension of traditional autoencoders that introduce probabilistic
    elements. VAEs not only learn to encode the input data into a latent space but
    also model the distribution of this latent space using probabilistic methods.
    This allows for the generation of new data samples from the learned latent space.
    VAEs are used for generative tasks like image synthesis, anomaly detection, and
    data imputation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs是传统自动编码器的一种扩展，它引入了概率元素。VAEs不仅学习将输入数据编码到潜在空间中，还使用概率方法来建模这个潜在空间的分布。这允许从学习到的潜在空间中生成新的数据样本。VAEs用于生成任务，如图像合成、异常检测和数据插补。
- en: In both autoencoders and VAEs, the idea is to learn a compressed and meaningful
    representation of the input data in the latent space, which can be useful for
    various tasks including feature extraction, data generation, and dimensionality
    reduction.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动编码器和VAEs中，想法是在潜在空间中学习输入数据的压缩和有意义的表示，这对于包括特征提取、数据生成和降维在内的各种任务都是有用的。
- en: 'These representations can then be used to make recommendations by identifying
    similar users and items in the latent space. In fact, the unique architecture
    that features NNs allows for the following techniques:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示可以用来通过在潜在空间中识别相似的用户和物品来做出推荐。事实上，具有神经网络独特架构的这种独特架构允许以下技术：
- en: '**Side information integration**: NNs can incorporate additional user and item
    attributes, such as demographic information, location, or social connections,
    to improve recommendations by learning from diverse data sources.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**侧信息集成**：NNs可以结合额外的用户和物品属性，如人口统计信息、位置或社交关系，通过从多样化的数据源中学习来改善推荐。'
- en: '**Deep reinforcement learning**: In certain scenarios, deep reinforcement learning
    can be used to optimize recommendations over time, learning from user feedback
    to suggest actions that maximize long-term rewards.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度强化学习**：在特定场景下，深度强化学习可以用来随着时间的推移优化推荐，通过学习用户反馈来建议最大化长期奖励的行动。'
- en: 'NNs offer flexibility and the ability to capture complex patterns in data,
    making them well suited for recommendation systems. However, they also require
    careful design, training, and tuning to achieve optimal performance. NNs also
    bring their own challenges, including the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NNs）提供了灵活性，能够捕捉数据中的复杂模式，这使得它们非常适合用于推荐系统。然而，它们也需要精心设计、训练和调整以达到最佳性能。神经网络也带来了自己的挑战，包括以下内容：
- en: '**Increased complexity**: NNs, especially **deep neural networks** (**DNNs**),
    can become incredibly complex due to their layered architecture. As we add more
    hidden layers and neurons, the model’s capacity to learn intricate patterns increases.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加复杂性**：神经网络，尤其是**深度神经网络**（**DNNs**），由于其分层架构，可能会变得极其复杂。随着我们添加更多的隐藏层和神经元，模型学习复杂模式的能力增加。'
- en: '**Training requirements**: NNs are heavy models whose training requires special
    hardware requirements including GPUs, which might be very expensive.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练需求**：神经网络是重量级的模型，其训练需要特殊的硬件要求，包括GPU，这可能会非常昂贵。'
- en: '**Potential overfitting**: Overfitting occurs when an ANN learns to perform
    exceptionally well on the training data but fails to generalize to unseen data'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在的过拟合**：当人工神经网络（ANN）在训练数据上表现出色，但无法推广到未见数据时，就会发生过拟合。'
- en: Selecting appropriate architectures, handling large datasets, and tuning hyperparameters
    are essential to effectively use NNs in recommendation systems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的架构、处理大型数据集和调整超参数对于在推荐系统中有效使用神经网络至关重要。
- en: 'Even though relevant advancements have been made in recent years, the aforementioned
    techniques still suffer from some pitfalls, primarily their being task-specific.
    For example, a rating-prediction recommendation system will not be able to tackle
    a task where we need to recommend the top *k* items that likely match the user’s
    taste. Actually, if we extend this limitation to other “pre-LLMs” AI solutions,
    we might see some similarities: it is indeed the task-specific situation that
    LLMs and, more generally, Large Foundation Models are revolutionizing, being highly
    generalized and adaptable to various tasks, depending on user’s prompts and instructions.
    Henceforth, extensive research in the field of recommendation systems is being
    done into what extent LLMs can enhance the current models. In the following sections,
    we will cover the theory behind these new approaches referring to recent papers
    and blogs about this emerging domain.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来已经取得了一些相关进展，但上述技术仍然存在一些缺陷，主要是它们具有任务特定性。例如，一个评分预测推荐系统将无法处理我们需要推荐可能符合用户口味的顶级
    *k* 项的任务。实际上，如果我们将这种限制扩展到其他“预LLMs”人工智能解决方案，我们可能会看到一些相似之处：确实，是任务特定的情况使得LLMs以及更一般的大型基础模型正在实现革命，它们高度通用且能够适应各种任务，这取决于用户的提示和指令。因此，在推荐系统领域进行了广泛的研究，以探讨LLMs可以提升当前模型到何种程度。在接下来的章节中，我们将介绍这些新方法背后的理论，并参考关于这个新兴领域的最新论文和博客。
- en: How LLMs are changing recommendation systems
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs如何改变推荐系统
- en: 'We saw in previous chapters how LLMs can be customized in three main ways:
    pre-training, fine-tuning, and prompting. According to the paper *Recommender
    systems in the Era of Large Language Models (LLMs)* from Wenqi Fan et al., these
    techniques can also be used to tailor an LLM to be a recommender system:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中看到了LLMs可以通过三种主要方式定制：预训练、微调和提示。根据Wenqi Fan等人撰写的论文《大型语言模型时代的推荐系统》，这些技术也可以用来定制LLM以成为推荐系统：
- en: '**Pre-training**: Pre-training LLMs for recommender systems is an important
    step to enable LLMs to acquire extensive world knowledge and user preferences,
    and to adapt to different recommendation tasks with zero or few shots.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练**：为推荐系统预训练LLMs是一个重要步骤，使LLMs能够获得广泛的世界知识和用户偏好，并适应不同的推荐任务，无需或仅需少量示例。'
- en: '**Note**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: 'An example of a recommendation system LLM is P5, introduced by Shijie Gang
    et al. in their paper *Recommendation as Language Processing (RLP): A Unified
    Pretrain, Personalized Prompt & Predict Paradigm (P5)*.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Shijie Gang等人在其论文《推荐作为语言处理（RLP）：统一预训练、个性化提示与预测范式（P5）》中介绍了一个推荐系统LLM的例子。
- en: 'P5 is a unified text-to-text paradigm for building recommender systems using
    **large language models** (**LLMs**). It consists of three steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: P5是一个使用**大型语言模型**（**LLMs**）构建推荐系统的统一文本到文本范式。它包括三个步骤：
- en: 'Pretrain: A foundation language model based on T5 architecture is pretrained
    on a large-scale web corpus and fine-tuned on recommendation tasks.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练：基于T5架构的基础语言模型在大规模网络语料库上预训练，并在推荐任务上进行微调。
- en: 'Personalized prompt: A personalized prompt is generated for each user based
    on their behavior data and contextual features.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个性化提示：根据每个用户的行为数据和上下文特征生成个性化的提示。
- en: 'Predict: The personalized prompt is fed into the pretrained language model
    to generate recommendations.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测：将个性化提示输入到预训练的语言模型中，以生成推荐。
- en: P5 is based on the idea that LLMs can encode extensive world knowledge and user
    preferences and can be adapted to different recommendation tasks with zero or
    few shots.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: P5基于LLM可以编码广泛的世界知识和用户偏好，并且可以适应不同的推荐任务，无需或仅需少量示例。
- en: '**Fine-tuning**: Training an LLM from scratch is a highly computational-intensive
    activity. An alternative and less intrusive approach to customize an LLM for recommendation
    systems might be fine-tuning.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：从头开始训练一个LLM是一项计算密集型活动。为推荐系统定制LLM的另一种替代方法可能是微调。'
- en: 'More specifically, the authors of the paper review two main strategies for
    fine-tuning LLMs:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，论文的作者回顾了微调LLM的两种主要策略：
- en: '**Full-model fine-tuning** involves changing the entire model’s weights based
    on task-specific recommendation datasets.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全模型微调**涉及根据特定任务的推荐数据集更改整个模型的权重。'
- en: '**Parameter-efficient fine-tuning** aims to change only a small part of weights
    or develop trainable adapters to fit specific tasks.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数高效微调**旨在仅改变一小部分权重或开发可训练的适配器以适应特定任务。'
- en: '**Prompting**: The third and “lightest” way of tailoring LLMs to be recommender
    systems is prompting. According to the authors, there are three main techniques
    for prompting LLMs:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：将LLM调整为推荐系统的第三种和“最轻量级”的方式是提示。根据作者的说法，调整LLM的主要技术有三种：'
- en: '**Conventional prompting** aims to unify downstream tasks into language generation
    tasks by designing text templates or providing a few input-output examples.'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传统提示**旨在通过设计文本模板或提供一些输入输出示例，将下游任务统一到语言生成任务中。'
- en: '**In-context learning** enables LLMs to learn new tasks based on contextual
    information without fine-tuning.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文学习**使LLM能够在不进行微调的情况下，根据上下文信息学习新任务。'
- en: '**Chain-of-thought** enhances the reasoning abilities of LLMs by providing
    multiple demonstrations to describe the chain of thought as examples within the
    prompt. The authors also discuss the advantages and challenges of each technique
    and provide some examples of existing methods that adopt them.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思维链**通过在提示中提供多个示例来描述思维链，从而增强LLM的推理能力。作者还讨论了每种技术的优缺点，并提供了一些采用这些技术的现有方法的例子。'
- en: Regardless of the typology, prompting is the fastest way to test whether a general-purpose
    LLM can tackle recommendation systems’ tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 无论类型如何，提示是测试通用LLM是否能够处理推荐系统任务的最快方式。
- en: The application of LLMs within the recommendation system domain is raising interest
    in the research field, and there is already some interesting evidence of the results
    as seen above.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统领域内LLM的应用正在引起研究领域的兴趣，并且已经有一些有趣的结果证据，如上述所见。
- en: In the next section, we are going to implement our own recommendation application
    using the prompting approach and leveraging the capabilities of LangChain as an
    AI orchestrator.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用提示方法并利用LangChain作为AI编排器的能力来实现我们自己的推荐应用。
- en: Implementing an LLM-powered recommendation system
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个由LLM驱动的推荐系统
- en: Now that we have covered some theory about recommendation systems and emerging
    research on how LLMs can enhance them, let’s start building our recommendation
    app, which will be a movie recommender system called MovieHarbor. The goal will
    be to make it as general as possible, meaning that we want our app to be able
    to address various recommendations tasks with a conversational interface. The
    scenario we are going to simulate will be that of the so-called “cold start,”
    concerning the first interaction of a user with the recommendation system where
    we do not have the user’s preference history. We will leverage a movie database
    with textual descriptions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些关于推荐系统以及LLMs如何增强它们的研究的理论，让我们开始构建我们的推荐应用程序，这将是一个名为MovieHarbor的电影推荐系统。目标是使其尽可能通用，这意味着我们希望我们的应用程序能够通过对话界面处理各种推荐任务。我们将模拟的是所谓的“冷启动”场景，即用户与推荐系统的第一次交互，我们不知道用户的偏好历史。我们将利用具有文本描述的电影数据库。
- en: For this purpose, we will use the *Movie recommendation data* dataset, available
    on Kaggle at [https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data](https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们将使用可在Kaggle上找到的*电影推荐数据*数据集，链接为[https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data](https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data)。
- en: The reason for using a dataset with a textual description of each movie (alongside
    information such as ratings and movie titles) is so that we can get the embeddings
    of the text. So let’s start building our MovieHarbor application.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包含每部电影文本描述（以及如评分和电影标题等信息）的数据集的原因是为了我们可以获取文本的嵌入。那么，让我们开始构建我们的MovieHarbor应用程序。
- en: Data preprocessing
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'In order to apply LLMs to our dataset, we first need to preprocess the data.
    The initial dataset included several columns; however, the ones we are interested
    in are the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将LLMs应用到我们的数据集中，我们首先需要对数据进行预处理。初始数据集包括几个列；然而，我们感兴趣的是以下列：
- en: '**Genres**: A list of applicable genres for the movie.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型**：适用于电影的适用类型列表。'
- en: '**Title**: The movie’s title.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标题**：电影的标题。'
- en: '**Overview**: Textual description of the plot.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述**：剧情的文本描述。'
- en: '**Vote_average**: A rating from 1 to 10 for a given movie'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均评分**：给定电影的1到10的评分。'
- en: '**Vote_count**: The number of votes for a given movie.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投票数**：给定电影的投票数。'
- en: 'I won’t report here the whole code (you can find it in the GitHub repo of this
    book at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml)),
    however, I will share the main steps of data preprocessing:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里报告完整的代码（你可以在本书的GitHub仓库中找到它，链接为[https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml)），然而，我会分享数据预处理的主要步骤：
- en: 'First, we format the `genres` column into a `numpy` array, which is easier
    to handle than the original dictionary format in the dataset:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将`genres`列格式化为一个`numpy`数组，这比数据集中的原始字典格式更容易处理：
- en: '[PRE2]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we merge the `vote_average` and `vote_count` columns into a single column,
    which is the weighted ratings with respect to the number of votes. I’ve also limited
    the rows to the 95^(th) percentile of the number of votes, so that we can get
    rid of minimum vote counts to prevent skewed results:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将`vote_average`和`vote_count`列合并为一个单列，这个单列是关于投票数量的加权评分。我还将行限制在投票数量的95^(th)百分位，这样我们就可以去除最低的投票数，以防止结果偏差：
- en: '[PRE3]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we create a new column called `combined_info` where we are going to merge
    all the elements that will be provided as context to the LLMs. Those elements
    are the movie title, overview, genres, and ratings:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个名为`combined_info`的新列，我们将合并所有将作为LLMs上下文提供的元素。这些元素包括电影标题、概述、类型和评分：
- en: '[PRE4]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We tokenize the movie `combined_info` so that we will get better results while
    embedding:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对电影`combined_info`进行分词，以便在嵌入时获得更好的结果：
- en: '[PRE5]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Definition**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: '`cl100k_base` is the name of a tokenizer used by OpenAI’s embeddings API. A
    tokenizer is a tool that splits a text string into units called tokens, which
    can then be processed by a neural network. Different tokenizers have different
    rules and vocabularies for how to split the text and what tokens to use.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`cl100k_base`是OpenAI嵌入API使用的分词器的名称。分词器是一个工具，它将文本字符串分割成称为标记的单位，然后可以被神经网络处理。不同的分词器有不同的规则和词汇表，用于如何分割文本以及使用哪些标记。'
- en: The `cl100k_base` tokenizer is based on the **byte pair encoding** (**BPE**)
    algorithm, which learns a vocabulary of subword units from a large corpus of text.
    The `cl100k_base` tokenizer has a vocabulary of 100,000 tokens, which are mostly
    common words and word pieces, but also include some special tokens for punctuation,
    formatting, and control. It can handle texts in multiple languages and domains,
    and can encode up to 8,191 tokens per input.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`cl100k_base`分词器基于**字节对编码**（**BPE**）算法，从大量文本语料库中学习子词单元的词汇表。`cl100k_base`分词器有10万个标记，其中大部分是常见单词和词组，但也包括一些用于标点、格式化和控制的特殊标记。它可以处理多种语言和领域的文本，并且可以编码每个输入高达8,191个标记。'
- en: 'We embed the text with `text-embedding-ada-002`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`text-embedding-ada-002`将文本嵌入：
- en: '[PRE6]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After changing some columns’ names and dropping unnecessary columns, the final
    dataset looks as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在更改一些列名和删除不必要的列之后，最终的数据集如下所示：
- en: '![](img/B21714_07_01.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_07_01.png)'
- en: 'Figure 7.1: Sample of the final movies dataset'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：最终电影数据集的样本
- en: 'Let’s have a look at a random row of text:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看文本的一个随机行：
- en: '[PRE7]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following output is obtained:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们得到的结果：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The last change we will make is modifying some naming conventions and data
    types as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要做的最后一个修改是修改一些命名约定和数据类型，如下所示：
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that we have our final dataset, we need to store it in a VectorDB. For
    this purpose, we are going to leverage **LanceDB**, an open-source database for
    vector-search built with persistent storage, which greatly simplifies the retrieval,
    filtering, and management of embeddings and also offers a native integration with
    LangChain. You can easily install LanceDB via `pip install lancedb`:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了最终的数据集，我们需要将其存储在VectorDB中。为此，我们将利用**LanceDB**，这是一个使用持久存储构建的开源向量搜索数据库，它极大地简化了嵌入的检索、过滤和管理，同时也提供了与LangChain的原生集成。您可以通过`pip
    install lancedb`轻松安装LanceDB：
- en: '[PRE10]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have all our ingredients, we can start working with those embeddings
    and start building our recommendation system. We will start with a simple task
    in a cold-start scenario, adding progressive layers of complexity with LangChain
    components. Afterwards, we will also try a content-based scenario to challenge
    our LLMs with diverse tasks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有原料，我们可以开始使用这些嵌入并开始构建我们的推荐系统。我们将从一个简单的冷启动场景任务开始，使用LangChain组件逐步增加复杂度。之后，我们还将尝试一个基于内容的场景，以挑战我们的LLMs（大型语言模型）进行多样化的任务。
- en: Building a QA recommendation chatbot in a cold-start scenario
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在冷启动场景中构建QA推荐聊天机器人
- en: In previous sections, we saw how the cold-start scenario – that means interacting
    with a user for the first time without their backstory – is a problem often encountered
    by recommendation systems. The less information we have about a user, the harder
    it is to match the recommendations to their preferences.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了冷启动场景——这意味着在没有用户背景的情况下首次与用户互动——是推荐系统经常遇到的问题。我们对用户了解得越少，就越难将推荐与他们的偏好匹配。
- en: 'In this section, we are going to simulate a cold-start scenario with LangChain
    and OpenAI’s LLMs with the following high-level architecture:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用以下高级架构模拟LangChain和OpenAI的LLMs的冷启动场景：
- en: '![A diagram of a computer  Description automatically generated](img/B21714_07_02.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![计算机图解  描述自动生成](img/B21714_07_02.png)'
- en: 'Figure 7.2: High-level architecture of recommendation system in a cold-start
    scenario'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：冷启动场景中推荐系统的高级架构
- en: In the previous section, we’ve already saved our embeddings in LanceDB. Now,
    we are going to build a LangChain RetrievalQA retriever, a chain component designed
    for question-answering against an index. In our case, we will use the vector store
    as our index retriever. The idea is that the chain returns the top *k* most similar
    movies upon the user’s query, using cosine similarity as the distance metric (which
    is the default).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们已经将我们的嵌入保存到了LanceDB中。现在，我们将构建一个LangChain RetrievalQA检索器，这是一个为针对索引进行问答而设计的链组件。在我们的案例中，我们将使用向量存储作为我们的索引检索器。想法是链返回用户查询时最相似的
    *k* 部分电影，使用余弦相似度作为距离度量（这是默认的）。
- en: 'So, let’s start building the chain:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始构建链：
- en: 'We are using only the movie overview as information input:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只使用电影概述作为信息输入：
- en: '[PRE11]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is the corresponding output (I will display a truncated version
    of the output, showing only the first out of four document sources):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对应的输出（我将显示输出的一部分，只显示四个文档来源中的第一个）：
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, alongside each `Document`, all variables are reported as metadata,
    plus the distance is also reported as a score. The lower the distance, the greater
    the proximity between the user’s query and the movie’s text embedding.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，每个`Document`旁边都报告了所有变量作为元数据，此外，距离也被报告为分数。距离越低，用户查询与电影文本嵌入之间的接近度就越大。
- en: 'Once we have gathered the most similar documents, we want a conversational
    response. For this goal, in addition to the embedding models, we will also use
    OpenAI’s completion model GPT-3 and combine it in RetrievalQA:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们收集了最相似的文档，我们希望得到一个对话式回答。为了这个目标，除了嵌入模型外，我们还将使用OpenAI的完成模型GPT-3并将其与RetrievalQA结合使用：
- en: '[PRE13]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s look at the output:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看输出：
- en: '[PRE14]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Since we set the `return_source_documents=True` parameter, we can also retrieve
    the document sources:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们设置了`return_source_documents=True`参数，我们还可以检索文档来源：
- en: '[PRE15]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following is the output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that the first document reported is not the one the model suggested. This
    occurred probably because of the rating, which is lower than Transformers (which
    was only the third result). This is a great example of how the LLM was able to
    consider multiple factors, on top of similarity, to suggest a movie to the user.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，报告的第一个文档并不是模型建议的。这可能是由于评分较低，低于Transformers（仅第三结果）。这是一个很好的例子，说明了LLM如何能够考虑多个因素，而不仅仅是相似性，来向用户建议一部电影。
- en: 'The model was able to generate a conversational answer, however, it is still
    using only a part of the available information – the textual overview. What if
    we want our MovieHarbor system to also leverage the other variables? We can approach
    the task in two ways:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型能够生成对话式回答，然而，它仍然只使用了可用信息的一部分——文本概述。如果我们想让我们的MovieHarbor系统也能利用其他变量呢？我们可以以两种方式来处理这个任务：
- en: '**The “filter” way**: This approach consists of adding some filters as **kwargs**
    to our retriever, which might be required by the application before responding
    to the user. Those questions might be, for example, about the genre of a movie.'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“过滤”方式**：这种方法包括向我们的检索器添加一些作为**kwargs**的过滤器，这些过滤器可能在响应用户之前由应用程序所需要。这些问题可能，例如，是关于电影类型的。'
- en: 'For example, let’s say we want to provide results featuring only those movies
    for which the genre is tagged as comedy. You can achieve this with the following
    code:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，假设我们只想提供那些类型被标记为喜剧的电影的结果。您可以使用以下代码实现这一点：
- en: '[PRE17]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The filter can also operate at the metadata level, as shown in the following
    example, where we want to filter only results with a rating above 7:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过滤器也可以在元数据级别上操作，如下面的例子所示，我们只想过滤评分高于7的结果：
- en: '[PRE18]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**The “agentic” way**: This is probably the most innovative way to approach
    the problem. Making our chain agentic means converting the retriever to a tool
    that the agent can leverage if needed, including the additional variables. By
    doing so, it would be sufficient for the user to provide their preferences in
    natural language so that the agent can retrieve the most promising recommendation
    if needed.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“代理”方式**：这可能是解决这个问题的最创新的方法。使我们的链成为代理意味着将检索器转换为一个代理在需要时可以利用的工具，包括额外的变量。通过这样做，用户只需以自然语言提供他们的偏好，这样代理就可以在需要时检索最有希望的推荐。'
- en: 'Let’s see how to implement this with code, asking specifically for an action
    movie (thus filtering on the `genre` variable):'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看如何用代码实现这一点，具体要求是一部动作电影（因此对`genre`变量进行过滤）：
- en: '[PRE19]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s see a glimpse of the chain of thoughts and the output produced (always
    based on the four most similar movies according to cosine similarity):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看思维链的片段和产生的输出（始终基于根据余弦相似度计算出的最相似的四个电影）：
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Finally, we might also want to make our application more tailored toward its
    goal of being a recommender system. To do so, we need to do some prompt engineering.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可能还想让我们的应用程序更符合其作为推荐系统的目标。为此，我们需要进行一些提示工程。
- en: '**Note**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: One of the advantages of using LangChain’s pre-built components, such as the
    RetrievalQA chain, is that they come with a pre-configured, well-curated prompt
    template. Before overriding the existing prompt, it’s a good practice to inspect
    it, so that you can also see which variables (within `{}`) are already expected
    from the component.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LangChain预构建组件的优点之一，例如RetrievalQA链，是它们附带预配置的、精心制作的提示模板。在覆盖现有提示之前，检查它是很好的做法，这样您也可以看到哪些变量（在`{}`内）已由组件预期。
- en: 'To explore the existing prompt, you can run the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索现有的提示，您可以运行以下代码：
- en: '[PRE21]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the output:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s say, for example, that we want our system to return three suggestions
    for each user’s request, with a short description of the plot and the reason why
    the user might like it. The following is a sample prompt that could match this
    goal:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，例如，我们希望我们的系统为每个用户的请求返回三个建议，包括剧情简短描述以及用户可能喜欢它的原因。以下是一个可能符合这一目标的样本提示：
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we need to pass it into our chain:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要将其传递到我们的链中：
- en: '[PRE24]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following output is obtained:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是获得的：
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Another thing that we might want to implement in our prompt is the information
    gathered with the conversational preliminary questions that we might want to set
    as a welcome page. For example, before letting the user input their natural language
    question, we might want to ask their age, gender, and favorite movie genre. To
    do so, we can insert in our prompt a section where we can format the input variables
    with those shared by the user, and then combine this prompt chunk in the final
    prompt we are going to pass to the chain. Below you can find an example (for simplicity,
    we are going to set the variables without asking the user):'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的提示中，我们可能还想实现通过对话初步问题收集的信息，我们可能希望将其设置为欢迎页面。例如，在让用户输入他们的自然语言问题之前，我们可能想询问他们的年龄、性别和最喜欢的电影类型。为此，我们可以在提示中插入一个部分，其中我们可以格式化与用户共享的输入变量，然后将这个提示块组合到我们将要传递给链的最终提示中。以下是一个示例（为了简单起见，我们将设置变量而不询问用户）：
- en: '[PRE26]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is the output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now let’s format the prompt and pass it into our chain:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们格式化提示并将其传递到我们的链中：
- en: '[PRE28]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We receive the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了以下输出：
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see, the system considered the user’s information provided. When
    we build the front-end of MovieHarbor, we will make this information dynamic as
    preliminary questions proposed to the user.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '如您所见，系统考虑了提供的用户信息。当我们构建MovieHarbor的前端时，我们将使这些信息动态化，作为向用户提出的初步问题。 '
- en: Building a content-based system
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建基于内容的系统
- en: In the previous section, we covered the cold-start scenario where the system
    knew nothing about the user. Sometimes, recommender systems already have some
    backstory about users, and it is extremely useful to embed this knowledge in our
    application. Let’s imagine, for example, that we have a users database where the
    system has stored all the registered user’s information (such as age, gender,
    country, etc.) as well as the movies the user has already watched alongside their
    rating.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了冷启动场景，即系统对用户一无所知的情况。有时，推荐系统已经对用户有一些背景信息，将这种知识嵌入我们的应用中非常有用。让我们设想，例如，我们有一个用户数据库，其中系统存储了所有已注册用户的信息（如年龄、性别、国家等），以及用户已经观看的电影及其评分。
- en: 'To do so, we will need to set a custom prompt that is able to retrieve this
    information from a source. For simplicity, we will create a sample dataset with
    users’ information with just two records, corresponding to two users. Each user
    will exhibit the following variables: username, age, gender, and a dictionary
    containing movies already watched alongside with the rating they gave to them.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要设置一个自定义提示，能够从源中检索这些信息。为了简单起见，我们将创建一个包含用户信息的样本数据集，只有两条记录，对应两个用户。每个用户将展示以下变量：用户名、年龄、性别，以及一个包含已观看电影及其评分的字典。
- en: 'The high-level architecture is represented by the following diagram:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 高级架构由以下图表示：
- en: '![A diagram of a computer flowchart  Description automatically generated](img/B21714_07_03.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![计算机流程图示意图  自动生成描述](img/B21714_07_03.png)'
- en: 'Figure 7.3: High-level architecture of a content-based recommendation system'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：基于内容的推荐系统的高级架构
- en: 'Let’s break down this architecture and examine each step to build the final
    chat for this content-based system, starting from the available users’ data:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个架构并检查每个步骤，以构建这个基于内容的系统的最终聊天，从可用的用户数据开始：
- en: 'As discussed earlier, we now have a bit of information about our users’ preferences.
    More specifically, imagine we have a dataset containing users’ attributes (name,
    age, gender) along with their reviews (a score from 1 to 10) of some movies. The
    following is the code used to create the dataset:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，我们现在对我们的用户偏好有一些了解。更具体地说，想象我们有一个包含用户属性（姓名、年龄、性别）以及他们对一些电影评论（1到10分的评分）的数据集。以下是用以创建数据集的代码：
- en: '[PRE30]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following output is obtained:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是获得的：
- en: '![A black and white screen with white text  Description automatically generated](img/B21714_07_04.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![黑白屏幕，白色文字，描述自动生成](img/B21714_07_04.png)'
- en: 'Figure 7.4: Sample users dataset'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：样本用户数据集
- en: 'What we want to do now is apply the same logic of the prompt of the cold start
    with the formatting with variables. The difference here is that, rather than asking
    the user to provide the values for those variables, we will directly collect them
    from our user dataset. So, we first define our prompt chunks:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在想要应用与变量格式化相同的冷启动提示逻辑。这里的区别在于，我们不会要求用户提供这些变量的值，而是直接从我们的用户数据集中收集它们。因此，我们首先定义我们的提示块：
- en: '[PRE31]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We then format the `user_info` chunk as follows (assuming that the user interacting
    with the system is `Alice`):'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们然后将`user_info`块格式化为以下形式（假设与系统交互的用户是`Alice`）：
- en: '[PRE32]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is the output:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE33]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s now use this prompt within our chain:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们现在在我们的链中使用这个提示：
- en: '[PRE34]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then obtain the following output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后获得以下输出：
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As you can see, the model is now able to recommend a list of movies to Alice
    based on the user’s information about past preferences, retrieved as context within
    the model’s metaprompt.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，该模型现在能够根据用户关于过去偏好的信息推荐一系列电影给Alice，这些信息作为模型元提示中的上下文检索。
- en: Note that, in this scenario, we used as dataset a simple pandas dataframe. In
    production scenarios, a best practice for storing variables related to a task
    to be addressed (such as a recommendation task) is that of using a feature store.
    Feature stores are data systems that are designed to support machine learning
    workflows. They allow data teams to store, manage, and access features that are
    used for training and deploying machine learning models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个场景中，我们使用了一个简单的pandas数据框作为数据集。在生产场景中，存储与待解决任务（如推荐任务）相关的变量的最佳实践是使用特征存储。特征存储是设计来支持机器学习工作流程的数据系统。它们允许数据团队存储、管理和访问用于训练和部署机器学习模型的特征。
- en: 'Furthermore, LangChain offers native integrations towards some of the most
    popular features stores:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LangChain提供了对一些最受欢迎的特征存储的原生集成：
- en: '**Feast:** This is an open-source feature store for machine learning. It allows
    teams to define, manage, discover, and serve features. Feast supports batch and
    streaming data sources and integrates with various data processing and storage
    systems. Feast uses BigQuery for offline features and BigTable or Redis for online
    features.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Feast**：这是一个开源的机器学习特征存储。它允许团队定义、管理、发现和提供特征。Feast支持批量和流数据源，并与各种数据处理和存储系统集成。Feast使用BigQuery进行离线特征，使用BigTable或Redis进行在线特征。'
- en: '**Tecton:** This is a managed feature platform that provides a complete solution
    for building, deploying, and using features for machine learning. Tecton allows
    users to define features in code, version control them, and deploy them to production
    with best practices. Furthermore, it integrates with existing data infrastructure
    and ML platforms like SageMaker and Kubeflow, and it uses Spark for feature transformations
    and DynamoDB for online feature serving.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tecton**：这是一个托管特征平台，提供构建、部署和使用机器学习特征的完整解决方案。Tecton允许用户在代码中定义特征，进行版本控制，并按照最佳实践将它们部署到生产环境中。此外，它集成了现有的数据基础设施和ML平台，如SageMaker和Kubeflow，并使用Spark进行特征转换，使用DynamoDB进行在线特征服务。'
- en: '**Featureform:** This is a virtual feature store that transforms existing data
    infrastructure into a feature store. Featureform allows users to create, store,
    and access features using standard feature definitions and a Python SDK. It orchestrates
    and manages the data pipelines required for feature engineering and materialization,
    and it is compatible with a wide range of data systems, such as Snowflake, Redis,
    Spark, and Cassandra.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Featureform**：这是一个虚拟特征存储，将现有的数据基础设施转换为特征存储。Featureform允许用户使用标准的特征定义和Python
    SDK创建、存储和访问特征。它协调和管理特征工程和物化所需的数据管道，并且与广泛的数据库系统兼容，如Snowflake、Redis、Spark和Cassandra。'
- en: '**AzureML Managed Feature Store:** This is a new type of workspace that lets
    users discover, create, and operationalize features. This service integrates with
    existing data stores, feature pipelines, and ML platforms like Azure Databricks
    and Kubeflow. Plus, it uses SQL, PySpark, SnowPark, or Python for feature transformations
    and Parquet/S3 or Cosmos DB for feature storage.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AzureML托管特征存储**：这是一种新的工作空间类型，允许用户发现、创建和操作特征。此服务与现有的数据存储、特征管道和ML平台（如Azure
    Databricks和Kubeflow）集成。此外，它使用SQL、PySpark、SnowPark或Python进行特征转换，并使用Parquet/S3或Cosmos
    DB进行特征存储。'
- en: You can read more about LangChain’s integration with features at [https://blog.langchain.dev/feature-stores-and-llms/](https://blog.langchain.dev/feature-stores-and-llms/).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://blog.langchain.dev/feature-stores-and-llms/](https://blog.langchain.dev/feature-stores-and-llms/)了解更多关于LangChain与特征集成的信息。
- en: Developing the front-end with Streamlit
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Streamlit开发前端
- en: Now that we have seen the logic behind an LLM-powered recommendation system,
    it is time to give a GUI to our MovieHarbor. To do so, we will once again leverage
    Streamlit, and we will assume the cold-start scenario. As always, you can find
    the whole Python code in the GitHub book repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了一个由LLM驱动的推荐系统背后的逻辑，那么是时候为我们的MovieHarbor提供一个图形用户界面了。为此，我们将再次利用Streamlit，并假设冷启动场景。一如既往，你可以在GitHub书籍仓库中找到整个Python代码，网址为[https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_07.xhtml)。
- en: As per the Globebotter application in *Chapter 6*, in this case also you need
    to create a `.py` file to run in your terminal via `streamlit run file.py`. In
    our case, the file will be named `movieharbor.py`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*第6章*中的Globebotter应用程序，在这种情况下，你也需要创建一个`.py`文件，通过`streamlit run file.py`在你的终端中运行。在我们的例子中，文件将被命名为`movieharbor.py`。
- en: 'Let’s now summarize the key steps to build the app with the front-end:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们总结一下使用前端构建应用程序的关键步骤：
- en: 'Configure the application webpage:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置应用程序网页：
- en: '[PRE36]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Import the credentials and establish the connection to LanceDB:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入凭证并建立与LanceDB的连接：
- en: '[PRE37]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create some widgets for the user to define their features and movies preferences:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为用户创建一些小部件来定义他们的特征和电影偏好：
- en: '[PRE38]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the parametrized prompt chunks:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义参数化的提示块：
- en: '[PRE39]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Set up the `RetrievalQA` chain:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`RetrievalQA`链：
- en: '[PRE40]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Insert the search bar for the user:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入用户搜索栏：
- en: '[PRE41]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And that’s it! You can run the final result in your terminal with `streamlit
    run movieharbor.py`. It looks like the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你可以在终端中使用`streamlit run movieharbor.py`运行最终结果。它看起来如下所示：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_07_05.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B21714_07_05.png)'
- en: 'Figure 7.5: Sample front-end for Movieharbor with Streamlit'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：使用Streamlit的Movieharbor示例前端
- en: So, you can see, in just few lines of code we were able to set up a webapp for
    our MovieHarbor. Starting from this template, you can customize your layout with
    Streamlit’s components, as well as tailor it to content-based scenarios. Plus,
    you can customize your prompts in such a way that the recommender acts as you
    prefer.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以看到，仅仅几行代码，我们就能够为我们的MovieHarbor设置一个webapp。从这个模板开始，你可以使用Streamlit的组件自定义你的布局，以及根据基于内容的场景进行定制。此外，你可以以这种方式自定义你的提示，使得推荐器按照你的偏好行事。
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored how LLMs could change the way we approach a recommendation
    system task. We started from the analysis of the current strategies and algorithms
    for building recommendation applications, differentiating between various scenarios
    (collaborative filtering, content-based, cold start, etc.) as well as different
    techniques (KNN, matrix factorization, and NNs).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了LLM如何改变我们处理推荐系统任务的方式。我们从分析构建推荐应用程序的当前策略和算法开始，区分了各种场景（协同过滤、基于内容的、冷启动等）以及不同的技术（KNN、矩阵分解和NNs）。
- en: We then moved to the new, emerging field of research into how to apply the power
    of LLMs to this field, and explored the various experiments that have been done
    in recent months.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后转向了研究的新兴领域，即如何将LLM的力量应用于这个领域，并探讨了最近几个月所进行的各种实验。
- en: Leveraging this knowledge, we built a movie recommender application powered
    by LLMs, using LangChain as the AI orchestrator and Streamlit as the front-end,
    showing how LLMs can revolutionize this field thanks to their reasoning capabilities
    as well as their generalization. This was just one example of how LLMs not only
    can open new frontiers, but can also enhance existing fields of research.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些知识，我们构建了一个由LLM（大型语言模型）驱动的电影推荐应用，使用LangChain作为AI编排器，Streamlit作为前端，展示了LLM如何凭借其推理能力和泛化能力彻底改变这一领域。这只是LLM不仅能够开辟新的前沿，还能增强现有研究领域的一个例子。
- en: In the next chapter, we will see what these powerful models can do when working
    with structured data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到这些强大的模型在处理结构化数据时能做什么。
- en: References
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**Recommendation as Language Processing** (**RLP**): A Unified **Pretrain,
    Personalized Prompt & Predict Paradigm** (**P5**). [https://arxiv.org/abs/2203.13366](https://arxiv.org/abs/2203.13366)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐作为语言处理**（**RLP**）：一个统一的**预训练、个性化提示与预测范式**（**P5**）。[https://arxiv.org/abs/2203.13366](https://arxiv.org/abs/2203.13366)'
- en: LangChain’s blog about featurestores. [https://blog.langchain.dev/feature-stores-and-llms/](https://blog.langchain.dev/feature-stores-and-llms/)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain关于特征存储的博客。[https://blog.langchain.dev/feature-stores-and-llms/](https://blog.langchain.dev/feature-stores-and-llms/)
- en: Feast. [https://docs.feast.dev/](https://docs.feast.dev/)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feast. [https://docs.feast.dev/](https://docs.feast.dev/)
- en: Tecton. [https://www.tecton.ai/](https://www.tecton.ai/)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tecton. [https://www.tecton.ai/](https://www.tecton.ai/)
- en: FeatureForm. [https://www.featureform.com/](https://www.featureform.com/)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FeatureForm. [https://www.featureform.com/](https://www.featureform.com/)
- en: Azure Machine Learning feature store. [https://learn.microsoft.com/en-us/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2](https://learn.microsoft.com/en-us/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Machine Learning特征存储。 [https://learn.microsoft.com/en-us/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2](https://learn.microsoft.com/en-us/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2)
- en: Join our community on Discord
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm](https://packt.link/llm )'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm )'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code214329708533108046.png)'
