<html><head></head><body><div><div><div><h1 id="_idParaDest-230" class="chapter-number"><a id="_idTextAnchor295"/>19</h1>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor296"/>Reinforcement Learning from Human Feedback</h1>
			<p>In this chapter, we’ll dive into <strong class="bold">Reinforcement Learning from Human Feedback</strong> (<strong class="bold">RLHF</strong>), a<a id="_idIndexMarker914"/> powerful technique for aligning LLMs with human preferences. RLHF combines reinforcement learning with human feedback to fine-tune language models. It aims to align the model’s outputs with human preferences, improving the quality and safety of generated text.</p>
			<p>RLHF differs from standard supervised fine-tuning by optimizing for human preferences rather than predefined correct answers. While supervised learning minimizes loss against labeled examples, RLHF creates a reward model from human comparisons between model outputs and then <a id="_idIndexMarker915"/>uses this reward function (typically with <strong class="bold">proximal policy optimization</strong> (<strong class="bold">PPO</strong>)) to update the model’s policy. The process typically employs a divergence penalty to prevent excessive drift from the initial model distribution.</p>
			<p>The key<a id="_idIndexMarker916"/> benefits of RLHF are as follows:</p>
			<ul>
				<li>Improved alignment of models with human values and preferences</li>
				<li>Enhanced control over model outputs</li>
				<li>Reduction of harmful or biased content</li>
				<li>Ability to optimize for specific task performance</li>
			</ul>
			<p>By the end of this chapter, you’ll be able to implement RLHF techniques to improve the alignment and output quality of your LLMs.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Components of RLHF systems</li>
				<li>Scaling RLHF</li>
				<li>Limitations of RLHF in language modeling</li>
				<li>Applications of RLHF</li>
			</ul>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor297"/>Components of RLHF systems</h1>
			<p>A typical RLHF system for LLMs consists of three main components:</p>
			<ul>
				<li><strong class="bold">Base language model</strong>: The<a id="_idIndexMarker917"/> pre-trained LLM to be fine-tuned</li>
				<li><strong class="bold">Reward model</strong>: A <a id="_idIndexMarker918"/>model trained on human preferences to provide feedback</li>
				<li><strong class="bold">Policy optimization</strong>: The<a id="_idIndexMarker919"/> process of updating the base model using the reward signal</li>
			</ul>
			<p>The base language model<a id="_idIndexMarker920"/> serves as the starting point. This is the general-purpose large language model that has already undergone extensive pre-training on large-scale corpora using self-supervised objectives such as next-token prediction. At this stage, the model is capable of generating coherent language and demonstrating broad linguistic competence. However, it lacks alignment with human preferences, task-specific objectives, or context-dependent behavior expected in real-world deployment. This pre-trained model is the substrate upon which subsequent tuning is performed. Its architecture, training regime, and scaling have already been well-documented in literature, and since RLHF builds upon it without altering its fundamental structure, further detailing it is unnecessary here.</p>
			<p>Instead, let us focus on the reward model and the policy optimization component, which work together to guide and reshape the output distribution of the base model based on human-aligned criteria. These two parts introduce the core mechanisms of feedback-driven adaptation and reinforcement tuning and will be examined in the following sections.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor298"/>Reward model</h2>
			<p>Let’s implement a basic structure for the <a id="_idIndexMarker921"/>reward model:</p>
			<pre class="source-code">
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
class RLHFSystem:
    def __init__(self, base_model_name, reward_model_name):
        self.base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name)
        self.reward_model = \
            AutoModelForSequenceClassification.from_pretrained(
            reward_model_name
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_name)
    def generate_text(self, prompt):
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.base_model.generate(inputs, max_length=100)
        return self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
    def get_reward(self, text):
        inputs = self.tokenizer(text, return_tensors="pt")
        with torch.no_grad():
            outputs = self.reward_model(inputs)
        return outputs.logits.item()</pre>			<p>This class sets up the <a id="_idIndexMarker922"/>basic structure for an RLHF system, including the base language model and the reward model. The <code>generate_text</code> method produces text from a given prompt, while <code>get_reward</code> estimates the reward for a given text using the reward model.</p>
			<p>The reward model is central to the RLHF process, as it translates human preferences into a learnable signal. Trained on datasets consisting of human comparisons between model outputs—where evaluators choose the better of two responses—it learns to predict how a human might rate any given response. During the reinforcement learning phase, this reward model serves as an automated proxy for human judgment, allowing the base model to receive immediate feedback on thousands of generated outputs. The policy model (the language model being optimized) then learns to maximize these predicted reward scores through techniques such as PPO, gradually shifting its behavior toward generating responses that better align with human preferences while maintaining coherence and capabilities through divergence constraints. This creates a powerful feedback loop that enables continuous alignment with human values, something that would be impossible with static supervised datasets.</p>
			<p>Here’s a simple implementation of reward model training:</p>
			<pre class="source-code">
from torch.utils.data import DataLoader, Dataset
from transformers import Trainer, TrainingArguments
class FeedbackDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        return {"text": self.texts[idx], "label": self.labels[idx]}
    def train_reward_model(model, tokenizer, texts, labels):
    dataset = FeedbackDataset(texts, labels)
    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length",
            truncation=True)
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=8,
        learning_rate=2e-5,
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    trainer.train()
    return model</pre>			<p>This code sets up a dataset of<a id="_idIndexMarker923"/> human feedback and trains the reward model using the Hugging Face Trainer API. The reward model learns to predict human preferences based on the provided labels.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor299"/>Policy optimization</h2>
			<p>Policy<a id="_idIndexMarker924"/> optimization is the process of updating the base language model using the rewards from the reward model. A common approach is PPO, which strikes a balance between ease of implementation, sample efficiency, and reliable performance. The term “proximal” in PPO refers to its key innovation: limiting how much the policy can change in each training step to prevent harmful large updates. It does this by using a “clipped” objective function that discourages updates that would move the policy too far from its previous version. PPO has become especially popular in AI alignment and RLHF because it’s more stable than other policy gradient methods – it helps with avoiding the problem where model updates become too aggressive and destroy previously learned good behaviors. When used in language models, PPO helps gradually shift the model’s outputs to better match human preferences while maintaining coherent and fluent text generation.</p>
			<p>Here’s a simplified implementation of PPO for LLMs:</p>
			<pre class="source-code">
def ppo_step(
    base_model, reward_model, optimizer, prompt, num_iterations=5
):
    for _ in range(num_iterations):
        # Generate text
        outputs = base_model.generate(prompt, max_length=100,
            return_dict_in_generate=True, output_scores=True
        )
        generated_text = tokenizer.decode(
            outputs.sequences[0], skip_special_tokens=True
        )
        # Get reward
        reward = reward_model(generated_text)
        # Compute policy loss
        log_probs = outputs.scores[0].log_softmax(dim=-1)
        policy_loss = -log_probs * reward
        # Update model
        optimizer.zero_grad()
        policy_loss.mean().backward()
        optimizer.step()
    return base_model</pre>			<p>This function<a id="_idIndexMarker925"/> performs a single step of PPO, generating text, computing rewards, and updating the base model’s parameters to maximize the expected reward. Keep in mind that this PPO code is illustrative; actual implementations may require more around rewards and safety checks.</p>
			<p><strong class="bold">Direct preference optimization</strong> (<strong class="bold">DPO</strong>) is <a id="_idIndexMarker926"/>another approach in RLHF that focuses on aligning models with human preferences by directly optimizing for preferred outcomes. Unlike traditional RL methods, which often rely on reward models to guide learning, DPO simplifies the process by using pairs of preferred and dispreferred outputs to adjust the model’s behavior. This method enhances efficiency and effectiveness in training models so that they generate outputs that align more closely with human expectations.</p>
			<p>DPO <a id="_idIndexMarker927"/>might be preferred over PPO when computational efficiency and implementation simplicity are priorities. This is because <a id="_idIndexMarker928"/>DPO eliminates the need for separate reward model training and complex reinforcement learning optimization loops. It offers a more streamlined approach by directly updating policy parameters from preference data, which can be particularly valuable in scenarios with limited resources or when PPO training exhibits instability or reward hacking. DPO can also make better use of limited human preference datasets without the intermediate step of reward modeling. Additionally, it provides a cleaner experimental setup for studying how preferences directly impact model behavior without the confounding factors introduced by separate reward models and reinforcement learning optimization.</p>
			<p>Here’s a short code example demonstrating how to implement DPO using Python:</p>
			<pre class="source-code">
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer
# Load a pre-trained language model and tokenizer
model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Define the dataset containing human preference pairs
# Each entry in the dataset is a tuple (prompt, preferred_completion, dispreferred_completion)
dataset = [
    ("Prompt 1", "Preferred Completion 1", "Dispreferred Completion 1"),
    ("Prompt 2", "Preferred Completion 2", "Dispreferred Completion 2"),
    # Add more data as needed
]
# Initialize the DPO Trainer
trainer = DPOTrainer(
    model=model,
    tokenizer=tokenizer,
    dataset=dataset,
    beta=0.1  # Hyperparameter controlling the strength of preference optimization
)
# Train the model using DPO
trainer.train()
# Save the fine-tuned model
model.save_pretrained("fine-tuned-model")
tokenizer.save_pretrained("fine-tuned-model")</pre>			<p>This code snippet<a id="_idIndexMarker929"/> demonstrates how to set up and train a language model using DPO, allowing it to better align with human feedback by directly optimizing for preferred completions.</p>
			<p>Having discussed PPO and DPO, next, we’ll examine scaling strategies for RLHF regarding large-scale models.</p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor300"/>Scaling RLHF</h1>
			<p>Scaling <a id="_idIndexMarker930"/>RLHF to large models presents challenges due to computational requirements. Here are some strategies that can be implemented:</p>
			<ul>
				<li><strong class="bold">Distributed training</strong>: This<a id="_idIndexMarker931"/> involves partitioning the training workload across multiple devices – typically GPUs or TPUs – by employing data parallelism, model parallelism, or pipeline parallelism. In data parallelism, the same model is replicated across devices, and each replica processes a different mini-batch of data. Gradients are averaged and synchronized after each step. On the other hand, model parallelism splits the model itself across multiple devices, enabling the training of architectures that are too large to fit on a single device. Finally, pipeline parallelism further divides the model into sequential stages across devices, which are then trained in a pipelined fashion to improve throughput. Frameworks such as DeepSpeed and Megatron-LM provide infrastructure for managing these complex parallelization schemes and optimizing communication overheads.</li>
				<li><code>torch.utils.checkpoint</code> or TensorFlow’s recomputation wrappers make it possible to apply this technique without having to rewrite model architectures.</li>
				<li><strong class="bold">Mixed-precision training</strong>: This<a id="_idIndexMarker933"/> utilizes 16-bit floating-point (FP16 or BF16) formats instead of the standard 32-bit (FP32) for most computations. This reduces memory footprint and increases throughput due to faster arithmetic and lower memory bandwidth usage. To maintain model accuracy and numerical <a id="_idIndexMarker934"/>stability, a master copy of weights is maintained in FP32, and dynamic loss scaling is often used to prevent underflow in gradients. Libraries such as NVIDIA’s Apex or native support in PyTorch and TensorFlow enable automatic mixed-precision training. This method is especially effective on modern hardware such as NVIDIA’s Tensor Cores or Google’s TPUs, which are optimized for low-precision computation.</li>
			</ul>
			<p><em class="italic">Figure 19</em><em class="italic">.1</em> summarizes these strategies:</p>
			<div><div><img src="img/B31249_19_01.jpg" alt="Figure 19.1 – Strategies for scaling RLHF" width="495" height="418"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.1 – Strategies for scaling RLHF</p>
			<p>Here’s an<a id="_idIndexMarker935"/> example of how to implement gradient checkpointing:</p>
			<pre class="source-code">
from transformers import GPT2LMHeadModel
def enable_gradient_checkpointing(model):
    if hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()
    else:
        model.base_model.gradient_checkpointing_enable()
    return model
base_model = GPT2LMHeadModel.from_pretrained("gpt2-large")
base_model = enable_gradient_checkpointing(base_model)</pre>			<p>This function enables gradient checkpointing for the model, which can significantly reduce memory usage during training, allowing for larger batch sizes or model sizes.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor301"/>Limitations of RLHF in language modeling</h1>
			<p>While <a id="_idIndexMarker936"/>RLHF is powerful, it faces several challenges:</p>
			<ul>
				<li><strong class="bold">Reward hacking</strong>: Models may exploit loopholes in the reward function</li>
				<li><strong class="bold">Limited feedback</strong>: Human feedback may not cover all possible scenarios</li>
				<li><strong class="bold">Suboptimal local optima</strong>: The optimization process may get stuck in suboptimal solutions</li>
				<li><strong class="bold">Scaling issues</strong>: Obtaining high-quality human feedback at scale is challenging</li>
			</ul>
			<p>To address reward hacking, consider implementing a constrained optimization approach:</p>
			<pre class="source-code">
def constrained_ppo_step(
    base_model, reward_model, constraint_model,
    optimizer, prompt, constraint_threshold=0.5
):
    outputs = base_model.generate(prompt, max_length=100,
        return_dict_in_generate=True, output_scores=True
    )
    generated_text = tokenizer.decode(
        outputs.sequences[0], skip_special_tokens=True
    )
    reward = reward_model(generated_text)
    constraint_value = constraint_model(generated_text)
    if constraint_value &gt; constraint_threshold:
        return base_model  # Skip update if constraint is violated
    # Compute and apply policy update (similar to previous ppo_step)
    # ...
    return base_model</pre>			<p>This function adds a constraint check before updating the model, helping to prevent reward hacking by ensuring the generated text meets certain criteria.</p>
			<p>This method modifies the standard training flow by evaluating a generated output not just for reward alignment but also for compliance with an external constraint model. The process begins with a response being generated from the base model using a given prompt. The resulting text is passed through both a reward model and a constraint model. The reward model assigns a scalar reward value based on its alignment with desired behaviors or objectives. In parallel, the constraint model evaluates whether the output satisfies specified limitations, such as avoiding harmful content, staying within factual bounds, or respecting legal or ethical filters.</p>
			<p>The<a id="_idIndexMarker937"/> constraint model returns a scalar value that quantifies the degree of constraint violation. This value is compared against a predefined threshold. If the value exceeds the threshold, indicating that the output violates the constraint, the training step is aborted for this sample. No gradient is calculated, and the model parameters remain unchanged. This selective update mechanism ensures that only outputs that both align with human preferences and satisfy safety or policy constraints contribute to learning. This design decouples the constraint signal from the reward function, maintaining clear<a id="_idIndexMarker938"/> boundaries between learning objectives and constraint enforcement. As a result, it preserves the integrity of both components and makes the system more interpretable and modular.</p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor302"/>Applications of RLHF</h1>
			<p>RLHF can be applied to<a id="_idIndexMarker939"/> various LLM tasks, including the following:</p>
			<ul>
				<li>Open-ended text generation</li>
				<li>Dialogue systems</li>
				<li>Content moderation</li>
				<li>Summarization</li>
				<li>Code generation</li>
			</ul>
			<p>Here’s an example of applying RLHF to a summarization task:</p>
			<pre class="source-code">
def rlhf_summarization(
    base_model, reward_model, text, num_iterations=5
):
    prompt = f"Summarize the following text:\n{text}\n\nSummary:"
    for _ in range(num_iterations):
        summary = base_model.generate(prompt, max_length=100)
        reward = reward_model(summary)
        # Update base_model using PPO or another RL algorithm
        # ...
    return summary
# Example usage
long_text = "..."  # Long text to summarize
summary = rlhf_summarization(base_model, reward_model, long_text)
print(summary)</pre>			<p>This function applies<a id="_idIndexMarker940"/> RLHF to the task of text summarization, iteratively improving the summary based on rewards from the reward model.</p>
			<p>The key steps involve generating a summary using a base model, receiving feedback from a reward model, and updating the base model iteratively to improve the summarization over time.</p>
			<p>Here’s a breakdown of how summarization works in this code:</p>
			<ol>
				<li><code>Summarize the following text:\n{text}\n\nSummary:</code>. This prompt is sent to the base model so that a summary can be generated.</li>
				<li><code>base_model.generate </code>function is used to generate a<a id="_idIndexMarker942"/> summary from the prompt. The generated summary is limited to a maximum length of 100 tokens (<code>max_length=100</code>). The summary is based on the input text and is the first attempt at summarization.</li>
				<li><strong class="bold">Reward model feedback</strong>: After the<a id="_idIndexMarker943"/> base model generates a summary, the reward model evaluates the quality of the summary. The reward model is a separate model that measures how well the generated summary aligns with desired qualities (such as being accurate, concise, or coherent). The reward function assigns a score to the summary, which reflects its quality based on the model’s internal criteria.</li>
				<li><code>num_iterations</code> times (in this case, five times by default). Each iteration involves generating a new summary, receiving feedback from the reward model, and potentially updating the base model to improve the summary in future iterations.</li>
				<li><code># Update base_model using PPO or another RL algorithm</code>, indicates that after each iteration, the base model should be updated using a reinforcement learning algorithm, such as PPO. This update will adjust the parameters of the base model to generate better summaries based on feedback from the reward model. However, the actual code for model updating isn’t provided here and would typically involve reinforcement learning techniques to fine-tune the base model based on the rewards it receives.</li>
				<li><strong class="bold">Final output</strong>: After<a id="_idIndexMarker946"/> completing the specified number of iterations, the function returns the final summary generated by the base model. This summary is expected to be the <a id="_idIndexMarker947"/>result of multiple improvements made based on the feedback that’s received from the reward model during the iterative process.</li>
			</ol>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor303"/>Summary</h1>
			<p>RLHF is a powerful technique used by many frontier model providers, such as OpenAI and Anthropic, in fine-tuning pre-trained models. This chapter discussed some basic ideas behind this pattern. RLHF still has its limitations since humans are involved in the process of training a reward model, and as such, it doesn’t scale well. Recently, some more generic reinforcement learning without human feedback has been tested by companies such as DeepSeek. However, this is beyond the scope of this book. You can refer to the following research paper by DeepSeek for more information: <a href="https://arxiv.org/pdf/2501.12948">https://arxiv.org/pdf/2501.12948</a>.</p>
			<p>As we move forward, we’ll explore advanced prompt engineering techniques for LLMs. In the next chapter, we’ll delve into sophisticated methods for guiding LLM behavior and outputs through carefully crafted prompts, building on the alignment techniques we’ve discussed here. These advanced prompting strategies will enable you to leverage the full potential of your LLMs while maintaining fine-grained control over their outputs.</p>
		</div>
	</div></div>
<div><div><p>&#13;
			<h1 id="_idParaDest-239" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor304"/>Part 4: Advanced Prompt Engineering Techniques</h1>&#13;
			<p>In this part, we explore advanced techniques that enhance the capabilities of LLMs through innovative prompting strategies and reasoning methods. You will learn how to use chain-of-thought and tree-of-thoughts prompting to guide models through complex reasoning processes. We also cover techniques for reasoning without direct observation, enabling LLMs to tackle hypothetical scenarios and abstract problems. Reflection techniques will show you how to prompt LLMs for iterative self-improvement, while methods for automatic multi-step reasoning and tool use will teach you how to extend LLMs into sophisticated, multi-functional systems. By mastering these advanced approaches, you will gain the ability to unlock the full potential of LLMs, allowing them to address even the most challenging problems.</p>&#13;
			<p>This part has the following chapters:</p>&#13;
			<ul>&#13;
				<li><a href="B31249_20.xhtml#_idTextAnchor305"><em class="italic">Chapter 20</em></a>, <em class="italic">Chain-of-Thought Prompting</em></li>&#13;
				<li><a href="B31249_21.xhtml#_idTextAnchor315"><em class="italic">Chapter 21</em></a>, <em class="italic">Tree-of-Thoughts Prompting</em></li>&#13;
				<li><a href="B31249_22.xhtml#_idTextAnchor325"><em class="italic">Chapter 22</em></a>, <em class="italic">Reasoning and Acting</em></li>&#13;
				<li><a href="B31249_23.xhtml#_idTextAnchor339"><em class="italic">Chapter 23</em></a>, <em class="italic">Reasoning </em><em class="italic">WithOut</em><em class="italic"> Observation</em></li>&#13;
				<li><a href="B31249_24.xhtml#_idTextAnchor346"><em class="italic">Chapter 24</em></a>, <em class="italic">Reflection Techniques</em></li>&#13;
				<li><a href="B31249_25.xhtml#_idTextAnchor355"><em class="italic">Chapter 25</em></a>, <em class="italic">Automatic Multi-Step Reasoning and Tool Use</em></li>&#13;
			</ul>&#13;
		</p>&#13;
		<p>&#13;
			<div>&#13;
			</p>&#13;
		</div>&#13;
		<p>&#13;
			<div>&#13;
			</p>&#13;
		</div>&#13;
	</div></div></body></html>