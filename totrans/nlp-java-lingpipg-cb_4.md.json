["```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter4.InterestingPhrases\n\n    ```", "```py\n    Score 42768.0 : Crayola Color \n    Score 42768.0 : Bing Rewards \n    Score 42768.0 : PassPorter Moms \n    Score 42768.0 : PRINCESS BATMAN \n    Score 42768.0 : Vinylmation NIB \n    Score 42768.0 : York City \n    Score 42768.0 : eternal damnation \n    Score 42768.0 : ncipes azules \n    Score 42768.0 : diventare realt \n    Score 42768.0 : possono diventare \n    ….\n    Score 42768.0 : Pictures Releases \n    Score 42768.0 : SPACE MOUNTAIN \n    Score 42768.0 : DEVANT MOI \n    Score 42768.0 : QUOI DEVANT \n    Score 42768.0 : Lindsay Lohan \n    Score 42768.0 : EPISODE VII \n    Score 42768.0 : STAR WARS \n    Score 42768.0 : Indiana Jones \n    Score 42768.0 : Steve Jobs \n    Score 42768.0 : Smash Mouth\n\n    ```", "```py\npackage com.lingpipe.cookbook.chapter4;\n\nimport java.io.FileReader;\nimport java.io.IOException;\nimport java.util.List;\nimport java.util.SortedSet;\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.aliasi.lm.TokenizedLM;\nimport com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;\nimport com.aliasi.util.ScoredObject;\n\npublic class InterestingPhrases {\n  static int TEXT_INDEX = 3;\n  public static void main(String[] args) throws IOException {\n    String inputCsv = args.length > 0 ? args[0] : \"data/disney.csv\";\n```", "```py\nList<String[]> lines = Util.readCsv(new File(inputCsv));\nint ngramSize = 3;\nTokenizedLM languageModel = new TokenizedLM(IndoEuropeanTokenizerFactory.INSTANCE, ngramSize);\n```", "```py\nfor (String [] line: lines) {\n  languageModel.train(line[TEXT_INDEX]);\n}\n```", "```py\nint phraseLength = 2;\nint minCount = 2;\nint maxReturned = 100;\nSortedSet<ScoredObject<String[]>> collocations = languageModel.collocationSet(phraseLength, minCount, maxReturned);\n```", "```py\nfor (ScoredObject<String[]> scoredTokens : collocations) {\n  double score = scoredTokens.score();\n  StringBuilder sb = new StringBuilder();\n  for (String token : scoredTokens.getObject()) {\n    sb.append(token + \" \");\n  }\n  System.out.printf(\"Score %.1f : \", score);\n  System.out.println(sb);\n}\n```", "```py\n    java -cp  lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter4.InterestingPhrasesForegroundBackground\n\n    ```", "```py\n    Score 989.621859 : [sleeping, beauty]\n    Score 989.621859 : [california, adventure]\n    Score 521.568529 : [winter, dreams]\n    Score 367.309361 : [disneyland, resort]\n    Score 339.429700 : [talking, about]\n    Score 256.473825 : [disneyland, during]\n\n    ```", "```py\nTokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;\ntokenizerFactory = new LowerCaseTokenizerFactory(tokenizerFactory);\nint minLength = 5;\ntokenizerFactory = new LengthFilterTokenizerFactoryPreserveToken(tokenizerFactory, minLength);\n```", "```py\nChapter 2, *Finding and Working with Words*, but the third one is a customized factory that bears some examination. The intent behind the LengthFilterTokenizerFactoryPreserveToken class is to filter short tokens but at the same time not lose adjacency information. The goal is to take the phrase, \"Disney is my favorite resort\", and produce tokens (disney, _234, _235, favorite, resort), because we don't want short words in our interesting phrases—they tend to sneak past simple statistical models and mess up the output. Please refer to src/come/lingpipe/cookbook/chapter4/LengthFilterTokenizerFactoryPreserveToken.java for the source of the third tokenizer. Also, refer to Chapter 2, *Finding and Working with Words* for exposition. Next is the background model:\n```", "```py\nint nGramOrder = 3;\nTokenizedLM backgroundLanguageModel = new TokenizedLM(tokenizerFactory, nGramOrder);\nfor (String [] line: backgroundData) {\n  backgroundLanguageModel.train(line[Util.TEXT_OFFSET]);\n}\n```", "```py\nTokenizedLM foregroundLanguageModel = new TokenizedLM(tokenizerFactory,nGramOrder);\nfor (String [] line: foregroundData) {\n  foregroundLanguageModel.train(line[Util.TEXT_OFFSET]);\n}\n```", "```py\nint phraseSize = 2;\nint minCount = 3;\nint maxReturned = 100;\nSortedSet<ScoredObject<String[]>> suprisinglyNewPhrases\n    = foregroundLanguageModel.newTermSet(phraseSize, minCount, maxReturned,backgroundLanguageModel);\nfor (ScoredObject<String[]> scoredTokens : suprisinglyNewPhrases) {\n    double score = scoredTokens.score();\n    String[] tokens = scoredTokens.getObject();\n    System.out.printf(\"Score %f : \", score);\n    System.out.println(java.util.Arrays.asList(tokens));\n}\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter9.PosTagger \n\n    ```", "```py\n    INPUT> Reality is not always probable, or likely.\n\n    ```", "```py\n    Reality_nn is_bez not_* always_rb probable_jj ,_, or_cc likely_jj ._. \n\n    ```", "```py\npublic static void main(String[] args) throws ClassNotFoundException, IOException {\n  TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;\n  String hmmModelPath = args.length > 0 ? args[0] : \"models/pos-en-general-brown.HiddenMarkovModel\";\n  HiddenMarkovModel hmm = (HiddenMarkovModel) AbstractExternalizable.readObject(new File(hmmModelPath));\n  HmmDecoder decoder = new HmmDecoder(hmm);\n  BufferedReader bufReader = new BufferedReader(new InputStreamReader(System.in));\n  while (true) {\n    System.out.print(\"\\n\\nINPUT> \");\n    System.out.flush();\n    String input = bufReader.readLine();\n    Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),0,input.length());\n    String[] tokens = tokenizer.tokenize();\n    List<String> tokenList = Arrays.asList(tokens);\n    firstBest(tokenList,decoder);\n  }\n}\n```", "```py\nstatic void firstBest(List<String> tokenList, HmmDecoder decoder) {\n  Tagging<String> tagging = decoder.tag(tokenList);\n    System.out.println(\"\\nFIRST BEST\");\n    for (int i = 0; i < tagging.size(); ++i){\n      System.out.print(tagging.token(i) + \"_\" + tagging.tag(i) + \" \");\n    }\n  System.out.println();\n}\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter4.NbestPosTagger \n\n    ```", "```py\n    INPUT> Colorless green ideas sleep furiously.\n\n    ```", "```py\n    N BEST\n    #   JointLogProb         Analysis\n    0     -91.141  Colorless_jj   green_jj   ideas_nns  sleep_vb   furiously_rb   ._. \n    1     -93.916  Colorless_jj   green_nn   ideas_nns  sleep_vb   furiously_rb   ._. \n    2     -95.494  Colorless_jj   green_jj   ideas_nns  sleep_rb   furiously_rb   ._. \n    3     -96.266  Colorless_jj   green_jj   ideas_nns  sleep_nn   furiously_rb   ._. \n    4     -98.268  Colorless_jj   green_nn   ideas_nns  sleep_rb   furiously_rb   ._.\n\n    ```", "```py\nstatic void nBest(List<String> tokenList, HmmDecoder decoder, int maxNBest) {\n  System.out.println(\"\\nN BEST\");\n  System.out.println(\"#   JointLogProb         Analysis\");\n  Iterator<ScoredTagging<String>> nBestIt = decoder.tagNBest(tokenList,maxNBest);\n  for (int n = 0; nBestIt.hasNext(); ++n) {\n    ScoredTagging<String> scoredTagging = nBestIt.next();\n    System.out.printf(n + \"   %9.3f  \",scoredTagging.score());\n    for (int i = 0; i < tokenList.size(); ++i){\n      System.out.print(scoredTagging.token(i) + \"_\" + pad(scoredTagging.tag(i),5));\n    }\n    System.out.println();\n  }\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter4.ConfidenceBasedTagger\n\n    ```", "```py\n    INPUT> Colorless green ideas sleep furiously.\n\n    ```", "```py\n    CONFIDENCE\n    #   Token          (Prob:Tag)*\n    0   Colorless           0.991:jj       0.006:np$      0.002:np \n    1   green               0.788:jj       0.208:nn       0.002:nns \n    2   ideas               1.000:nns      0.000:rb       0.000:jj \n    3   sleep               0.821:vb       0.101:rb       0.070:nn \n    4   furiously           1.000:rb       0.000:ql       0.000:jjr \n    5   .                   1.000:.        0.000:np       0.000:nn \n\n    ```", "```py\nstatic void confidence(List<String> tokenList, HmmDecoder decoder) {\n  System.out.println(\"\\nCONFIDENCE\");\n  System.out.println(\"#   Token          (Prob:Tag)*\");\n  TagLattice<String> lattice = decoder.tagMarginal(tokenList);\n\n  for (int tokenIndex = 0; tokenIndex < tokenList.size(); ++tokenIndex) {\n    ConditionalClassification tagScores = lattice.tokenClassification(tokenIndex);\n    System.out.print(pad(Integer.toString(tokenIndex),4));\n    System.out.print(pad(tokenList.get(tokenIndex),15));\n\n    for (int i = 0; i < 3; ++i) {\n      double conditionalProb = tagScores.score(i);\n      String tag = tagScores.category(i);\n      System.out.printf(\" %9.3f:\" + pad(tag,4),conditionalProb);\n\n    }\n    System.out.println();\n  }\n}\n```", "```py\n    [The ungentle laws and customs touched upon in this tale are\n    historical, and the episodes which are used to illustrate them\n    are also historical.] [It is not pretended that these laws and\n    customs existed in England in the sixth century; no, it is only\n    pretended that inasmuch as they existed in the English and other\n    civilizations of far later times, it is safe to consider that it is\n    no libel upon the sixth century to suppose them to have been in\n    practice in that day also.] [One is quite justified in inferring\n    that whatever one of these laws or customs was lacking in that\n    remote time, its place was competently filled by a worse one.]\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter4.HmmTrainer\n\n    ```", "```py\n    Training The/BOS ungentle/WORD laws/WORD and/WORD customs/WORD touched/WORD…\n    done training, token count: 123\n    Enter text followed by new line\n    > The cat in the hat. The dog in a bog.\n    The/BOS cat/WORD in/WORD the/WORD hat/WORD ./EOS The/BOS dog/WORD in/WORD a/WORD bog/WORD ./EOS\n\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n  String inputFile = args.length > 0 ? args[0] : \"data/connecticut_yankee_EOS.txt\";\n  char[] text = Files.readCharsFromFile(new File(inputFile), Strings.UTF8);\n  TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;\n  List<Tagging<String>> taggingList = new ArrayList<Tagging<String>>();\n  addTagging(tokenizerFactory,taggingList,text);\n```", "```py\nstatic void addTagging(TokenizerFactory tokenizerFactory, List<Tagging<String>> taggingList, char[] text) {\n  Tokenizer tokenizer = tokenizerFactory.tokenizer(text, 0, text.length);\n  List<String> tokens = new ArrayList<String>();\n  List<String> tags = new ArrayList<String>();\n  boolean bosFound = false;\n  for (String token : tokenizer.tokenize()) {\n    if (token.equals(\"[\")) {\n      bosFound = true;\n    }\n    else if (token.equals(\"]\")) {\n      tags.set(tags.size() - 1,\"EOS\");\n    }\n    else {\n      tokens.add(token);\n      if (bosFound) {\n        tags.add(\"BOS\");\n        bosFound = false;\n      }\n      else {\n        tags.add(\"WORD\");\n      }\n    }\n  }\n  if (tokens.size() > 0) {\n    taggingList.add(new Tagging<String>(tokens,tags));\n  }\n}\n```", "```py\nListCorpus<Tagging<String>> corpus = new ListCorpus<Tagging<String>> ();\nfor (Tagging<String> tagging : taggingList) {\n  System.out.println(\"Training \" + tagging);\n  corpus.addTrain(tagging);\n}\n```", "```py\nHmmCharLmEstimator estimator = new HmmCharLmEstimator();\ncorpus.visitTrain(estimator);\nSystem.out.println(\"done training, token count: \" + estimator.numTrainingTokens());\nHmmDecoder decoder = new HmmDecoder(estimator);\n```", "```py\nNote that there is no requirement that the training tokenizer be the same as the production tokenizer, but one must be careful to not tokenize in a radically different way; otherwise, the HMM will not be seeing the tokens it was trained with. The back-off model will then be used, which will likely degrade performance. Have a look at the following code snippet:\n```", "```py\nBufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\nwhile (true) {\n  System.out.print(\"Enter text followed by new line\\n>\");\n  String evalText = reader.readLine();\n  Tokenizer tokenizer = tokenizerFactory.tokenizer(evalText.toCharArray(),0,evalText.length());\n  List<String> evalTokens = Arrays.asList(tokenizer.tokenize());\n  Tagging<String> evalTagging = decoder.tag(evalTokens);\n  System.out.println(evalTagging);\n}\n```", "```py\npublic class TinyPosCorpus extends Corpus<ObjectHandler<Tagging<String>>> {\n\n  public void visitTrain(ObjectHandler<Tagging<String>> handler) {\n    for (String[][] wordsTags : WORDS_TAGSS) {\n      String[] words = wordsTags[0];\n      String[] tags = wordsTags[1];\n      Tagging<String> tagging = new Tagging<String>(Arrays.asList(words),Arrays.asList(tags));\n      handler.handle(tagging);\n    }\n  }\n\n  public void visitTest(ObjectHandler<Tagging<String>> handler) {\n    /* no op */\n  }\n\n  static final String[][][] WORDS_TAGSS = new String[][][] {\n    { { \"John\", \"ran\", \".\" },{ \"PN\", \"IV\", \"EOS\" } },\n    { { \"Mary\", \"ran\", \".\" },{ \"PN\", \"IV\", \"EOS\" } },\n    { { \"John\", \"jumped\", \"!\" },{ \"PN\", \"IV\", \"EOS\" } },\n    { { \"The\", \"dog\", \"jumped\", \"!\" },{ \"DET\", \"N\", \"IV\", \"EOS\" } },\n    { { \"The\", \"dog\", \"sat\", \".\" },{ \"DET\", \"N\", \"IV\", \"EOS\" } },\n    { { \"Mary\", \"sat\", \"!\" },{ \"PN\", \"IV\", \"EOS\" } },\n    { { \"Mary\", \"likes\", \"John\", \".\" },{ \"PN\", \"TV\", \"PN\", \"EOS\" } },\n    { { \"The\", \"dog\", \"likes\", \"Mary\", \".\" }, { \"DET\", \"N\", \"TV\", \"PN\", \"EOS\" } },\n    { { \"John\", \"likes\", \"the\", \"dog\", \".\" }, { \"PN\", \"TV\", \"DET\", \"N\", \"EOS\" } },\n    { { \"The\", \"dog\", \"ran\", \".\" },{ \"DET\", \"N\", \"IV\", \"EOS\", } },\n    { { \"The\", \"dog\", \"ran\", \".\" },{ \"DET\", \"N\", \"IV\", \"EOS\", } }\n  };\n```", "```py\n/*\nList<Tagging<String>> taggingList = new ArrayList<Tagging<String>>();\naddTagging(tokenizerFactory,taggingList,text);\nListCorpus<Tagging<String>> corpus = new ListCorpus<Tagging<String>> ();\nfor (Tagging<String> tagging : taggingList) {\n  System.out.println(\"Training \" + tagging);\n  corpus.addTrain(tagging);\n}\n*/\n\nCorpus<ObjectHandler<Tagging<String>>> corpus = new TinyPosCorpus();\nHmmCharLmEstimator estimator = new HmmCharLmEstimator();\ncorpus.visitTrain(estimator);\n```", "```py\njava -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar \ndone training, token count: 42\nEnter text followed by new line\n> The cat in the hat is back.\nThe/DET cat/N in/TV the/DET hat/N is/TV back/PN ./EOS\n\n```", "```py\npublic class TagEvaluator {\n  public static void main(String[] args) throws ClassNotFoundException, IOException {\n    HmmDecoder decoder = null;\n    boolean storeTokens = true;\n    TaggerEvaluator<String> evaluator = new TaggerEvaluator<String>(decoder,storeTokens);\n    Corpus<ObjectHandler<Tagging<String>>> smallCorpus = new TinyPosCorpus();\n    int numFolds = 10;\n    XValidatingObjectCorpus<Tagging<String>> xValCorpus = new XValidatingObjectCorpus<Tagging<String>>(numFolds);\n    smallCorpus.visitCorpus(xValCorpus);\n    for (int i = 0; i < numFolds; ++i) {\n      xValCorpus.setFold(i);\n      HmmCharLmEstimator estimator = new HmmCharLmEstimator();\n      xValCorpus.visitTrain(estimator);\n      System.out.println(\"done training \" + estimator.numTrainingTokens());\n      decoder = new HmmDecoder(estimator);\n      evaluator.setTagger(decoder);\n      xValCorpus.visitTest(evaluator);\n    }\n    BaseClassifierEvaluator<String> classifierEval = evaluator.tokenEval();\n    System.out.println(classifierEval);\n  }\n}\n```", "```py\n    HmmDecoder decoder = null;\n    boolean storeTokens = true;\n    TaggerEvaluator<String> evaluator = new TaggerEvaluator<String>(decoder,storeTokens);\n    ```", "```py\n    Corpus<ObjectHandler<Tagging<String>>> smallCorpus = new TinyPosCorpus();\n    int numFolds = 10;\n    XValidatingObjectCorpus<Tagging<String>> xValCorpus = new XValidatingObjectCorpus<Tagging<String>>(numFolds);\n    smallCorpus.visitCorpus(xValCorpus);\n    ```", "```py\n    for (int i = 0; i < numFolds; ++i) {\n      xValCorpus.setFold(i);\n      HmmCharLmEstimator estimator = new HmmCharLmEstimator();\n      xValCorpus.visitTrain(estimator);\n      System.out.println(\"done training \" + estimator.numTrainingTokens());\n    ```", "```py\n    decoder = new HmmDecoder(estimator);\n    evaluator.setTagger(decoder);\n    xValCorpus.visitTest(evaluator);\n    ```", "```py\n    BaseClassifierEvaluator<String> classifierEval = evaluator.tokenEval();\n    System.out.println(classifierEval);\n    ```", "```py\n    Confusion Matrix\n    reference \\ response\n      ,DET,PN,N,IV,TV,EOS\n      DET,4,2,0,0,0,0\n      PN,0,7,0,1,0,0\n      N,0,0,4,1,1,0\n      IV,0,0,0,8,0,0\n      TV,0,1,0,0,2,0\n      EOS,0,0,0,0,0,11\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter4.CRFTagger\n\n    ```", "```py\n    Enter text followed by new line\n    >The rain in Spain falls mainly on the plain.\n\n    ```", "```py\n    The/DET rain/N in/TV Spain/PN falls/IV mainly/EOS on/DET the/N plain/IV ./EOS\n\n    ```", "```py\nCorpus<ObjectHandler<Tagging<String>>> corpus = new TinyPosCorpus();\n```", "```py\nfinal ChainCrfFeatureExtractor<String> featureExtractor\n  = new SimpleCrfFeatureExtractor();\n```", "```py\nboolean addIntercept = true;\nint minFeatureCount = 1;\nboolean cacheFeatures = false;\nboolean allowUnseenTransitions = true;\ndouble priorVariance = 4.0;\nboolean uninformativeIntercept = true;\nRegressionPrior prior = RegressionPrior.gaussian(priorVariance, uninformativeIntercept);\nint priorBlockSize = 3;\ndouble initialLearningRate = 0.05;\ndouble learningRateDecay = 0.995;\nAnnealingSchedule annealingSchedule = AnnealingSchedule.exponential(initialLearningRate,\n  learningRateDecay);\ndouble minImprovement = 0.00001;\nint minEpochs = 2;\nint maxEpochs = 2000;\nReporter reporter = Reporters.stdOut().setLevel(LogLevel.INFO);\n```", "```py\nSystem.out.println(\"\\nEstimating\");\nChainCrf<String> crf = ChainCrf.estimate(corpus,featureExtractor,addIntercept,minFeatureCount,cacheFeatures,allowUnseenTransitions,prior,priorBlockSize,annealingSchedule,minImprovement,minEpochs,maxEpochs,reporter);\n```", "```py\nTokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;\nBufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\nwhile (true) {\n  System.out.print(\"Enter text followed by new line\\n>\");\n  System.out.flush();\n  String text = reader.readLine();\n  Tokenizer tokenizer = tokenizerFactory.tokenizer(text.toCharArray(),0,text.length());\n  List<String> evalTokens = Arrays.asList(tokenizer.tokenize());\n  Tagging<String> evalTagging = crf.tag(evalTokens);\n  System.out.println(evalTagging);\n```", "```py\npublic class SimpleCrfFeatureExtractor implements ChainCrfFeatureExtractor<String> {\n  public ChainCrfFeatures<String> extract(List<String> tokens, List<String> tags) {\n    return new SimpleChainCrfFeatures(tokens,tags);\n  }\n```", "```py\nstatic class SimpleChainCrfFeatures extends ChainCrfFeatures<String> {\n```", "```py\npublic SimpleChainCrfFeatures(List<String> tokens, List<String> tags) {\n  super(tokens,tags);\n}\n```", "```py\npublic Map<String,Double> nodeFeatures(int n) {\n  ObjectToDoubleMap<String> features = new ObjectToDoubleMap<String>();\n  features.increment(\"TOK_\" + token(n),1.0);\n  return features;\n}\n```", "```py\npublic Map<String,Double> edgeFeatures(int n, int k) {\n  ObjectToDoubleMap<String> features = new ObjectToDoubleMap<String>();\n  features.increment(\"TAG_\" + tag(k),1.0);\n  return features;\n}\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter4.ModifiedCrfFeatureExtractor\n\n    ```", "```py\n    -------------------\n    Tagging:  John/PN\n\n    ```", "```py\n    Node Feats:{nps=2.0251355582754984E-4, np=0.9994337160349874, nn=2.994165140854113E-4, TOK_John=1.0}\n    ```", "```py\n    Edge Feats:{TOKEN_SHAPE_LET-CAP=1.0, TAG_PN=1.0}\n    Edge Feats:{TAG_IV=1.0, TOKEN_SHAPE_LET-CAP=1.0}\n    Edge Feats:{TOKEN_SHAPE_LET-CAP=1.0, TAG_EOS=1.0}\n    ```", "```py\npublic static void main(String[] args) throws IOException, ClassNotFoundException {\n\n  Corpus <ObjectHandler<Tagging<String>>> corpus = new TinyPosCorpus();\n  final ChainCrfFeatureExtractor<String> featureExtractor = new ModifiedCrfFeatureExtractor();\n```", "```py\ncorpus.visitCorpus(new ObjectHandler<Tagging<String>>() {\n  @Override\n  public void handle(Tagging<String> tagging) {\n    ChainCrfFeatures<String> features = featureExtractor.extract(tagging.tokens(), tagging.tags());\n```", "```py\nfor (int i = 0; i < tagging.size(); ++i) {\n  System.out.println(\"---------\");\n  System.out.println(\"Tagging:  \" + tagging.token(i) + \"/\" + tagging.tag(i));\n```", "```py\nSystem.out.println(\"Node Feats:\" + features.nodeFeatures(i));\n```", "```py\nfor (int j = 0; j < tagging.size(); ++j) {\n  System.out.println(\"Edge Feats:\" \n        + features.edgeFeatures(i, j));\n}\n```", "```py\nHmmDecoder mDecoder;\n\npublic ModifiedCrfFeatureExtractor() throws IOException, ClassNotFoundException {\n  File hmmFile = new File(\"models/pos-en-general-\" + \"brown.HiddenMarkovModel\");\n  HiddenMarkovModel hmm = (HiddenMarkovModel)AbstractExternalizable.readObject(hmmFile);\n  mDecoder = new HmmDecoder(hmm);\n}\n```", "```py\npublic ChainCrfFeatures<String> extract(List<String> tokens, List<String> tags) {\n  return new ModChainCrfFeatures(tokens,tags);\n}\n```", "```py\nclass ModChainCrfFeatures extends ChainCrfFeatures<String> {\n\n  TagLattice<String> mBrownTaggingLattice;\n\n  public ModChainCrfFeatures(List<String> tokens, List<String> tags) {\n    super(tokens,tags);\n    mBrownTaggingLattice = mDecoder.tagMarginal(tokens);\n  }\n```", "```py\npublic Map<String,? extends Number> edgeFeatures(int n, int k) {\n  ObjectToDoubleMap<String> features = newObjectToDoubleMap<String>();\n  features.set(\"TAG_\" + tag(k), 1.0d);\n  String category = IndoEuropeanTokenCategorizer.CATEGORIZER.categorize(token(n));\n  features.set(\"TOKEN_SHAPE_\" + category,1.0d);\n  return features;\n}\n```", "```py\npublic Map<String,? extends Number> nodeFeatures(int n) {\n  ObjectToDoubleMap<String> features = new ObjectToDoubleMap<String>();\n  features.set(\"TOK_\" + token(n), 1);\n  ConditionalClassification tagScores = mBrownTaggingLattice.tokenClassification(n);\n  for (int i = 0; i < 3; ++ i) {\n    double conditionalProb = tagScores.score(i);\n    String tag = tagScores.category(i);\n    features.increment(tag, conditionalProb);\n  }\n  return features;\n}\n```", "```py\nfeatures.set(\"TOK_\" + token(n), 1); \n```"]