- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Constructing a Recommendation Graph with H&M Personalization Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Neo4j is great for building knowledge graphs, it would be prudent to look
    at how we model the data. A good data persistence model can make data retrieval
    optimal and handle large loads better. In this chapter, we will take a step back
    to look at what constitutes a **knowledge graph** and how a different look at
    data modeling with a Neo4j data persistence approach can help build more powerful
    knowledge graphs. You might need to revisit the approaches defined in [*Chapter
    3*](Preface.xhtml#_idTextAnchor012), which will enable you to build a knowledge
    graph with Personalized Fashion Recommendations (H&M personalization) data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover these topics in this chapter as we tackle data modeling evolution:'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling a recommendation graph with the H&M Personalization dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimizing for recommendations: Best practices in graph modeling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need to be familiar with SQL and Cypher. We will be using SQLite and
    Neo4j to understand the various aspects of data modeling. We will use the following
    tools in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Neo4j Desktop ([https://neo4j.com/docs/desktop-manual/current/](https://neo4j.com/docs/desktop-manual/current/))
    or Neo4j Aura ([https://neo4j.com/docs/aura/](https://neo4j.com/docs/aura/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The H&M dataset to create the recommendation system: This dataset is available
    at [https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/overview](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/overview)
    (Carlos García Ling, ElizabethHMGroup, FridaRim, inversion, Jaime Ferrando, Maggie,
    neuraloverflow, and xlsrln. H&M Personalized Fashion Recommendations. 2022\. Kaggle)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember from [*Chapter 3*](Preface.xhtml#_idTextAnchor012) that a good graph
    data model makes the *retrieval* part of RAG flow more effective. It makes retrieving
    relevant data faster and easier. You may revisit [*Chapter 3*](Preface.xhtml#_idTextAnchor012)
    for a quick recap of graph data modeling. In this chapter, we model the data with
    time as a dimension. The chain of transactions with the time as a dimension makes
    data retrieval very efficient and performant.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the recommendation graph with the H&M personalization dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a graph data model with the real-life large-scale
    H&M Personalization dataset. This graph data model will enable us to power up
    the recommendation engine that we will create in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In 2022, H&M posted customer transaction data along with other metadata related
    to customers, products, and so on, as part of a competition to build a recommendation
    engine. This dataset contains data from previous transactions, as well as from
    customer and product metadata. The available metadata spans simple data, such
    as garment type and customer age, to text data from product descriptions, to image
    data from garment images.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss the dataset’s characteristics and load the data into a knowledge
    graph as we go, step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take a look at the data available in this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`images/:` This contains the images for a given `article_id.` Not all articles
    in the dataset may have images associated with them. We will not be using this
    data to build the graph. Storing the images in a graph would not only be inefficient,
    but it is not necessary for the graph flow we are building.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`articles.csv:` This file contains the metadata for each article available
    for purchase. Each row represents one unique article with metadata such as the
    product family, color, style, section the article belongs to, and department.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`customers.csv:` This file contains the metadata for each customer in the dataset,
    including customer ID, age, fashion news frequency, active flag, H&M club member
    status, and postal code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transactions_train.csv:` This file contains the transactions made by customers.
    If a customer made multiple purchases of the same item, that data might come as
    multiple rows – one row for each item purchased, with the transaction date, article
    ID, customer ID, price, and sales channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will take a look at the **graph data model** of this data in the next section
    and load the data for that model. When we build the knowledge graph for the H&M
    personalization dataset recommendations, we will have a list of transactions made
    by customers, and representing these as a chain of transactions with time as a
    dimension might work very well for us. By adding our understanding of the data
    into the graph data model can make our recommendations more valuable. For instance,
    the transactions are a sequence of events; hence modeling them as a sequence makes
    more sense. Unlike traditional databases, Neo4j makes it possible to store these
    transactions as a graph that is connected sequentially using relationships.
  prefs: []
  type: TYPE_NORMAL
- en: We can say, we are persisting our knowledge of data into the graph, thus creating
    a knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: Building your recommendation graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build the recommendation model graph, we will take a look at the data within
    each of the files in the dataset and how they contribute to the graph. We will
    apply the process we discussed previously, in [*Chapter 3*](Preface.xhtml#_idTextAnchor012)*,*
    to build the graph. Before loading the data, we need to use Neo4j Desktop and
    perform these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a local database. You can follow the instructions at [https://neo4j.com/docs/desktop-manual/current/operations/create-dbms/](https://neo4j.com/docs/desktop-manual/current/operations/create-dbms/)
    to perform this operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the CSV files from the H&M recommendation dataset to the `import` directory
    of this database. If you are not sure how to do this, please visit [https://community.neo4j.com/t/where-is-neo4j-home/6488/5](https://community.neo4j.com/t/where-is-neo4j-home/6488/5)
    for reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let us load the data into the graph database.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the customer data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The customer data contains these elements: customer ID, age, fashion news frequency,
    active flag, H&M club member status, and postal code.'
  prefs: []
  type: TYPE_NORMAL
- en: The customer ID is the unique ID of the customer. To make sure we have unique
    nodes representing the customer, we need to have a `UNIQUE` constraint. Also,
    we will make the postal code a node, as we might want to segregate customers by
    postal code easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before loading this data, we need to create these unique constraints, by connecting
    to the Neo4j database we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once unique constraints are created, we can use this Cypher to load the data
    into the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: For the `LOAD` `CSV` queries, we need to prefix them with `auto` to be able
    to run them in Neo4j Browser.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This script loads the customer data into the database, using 1,000 rows as
    one batch to commit. In this script, we can notice a couple of things:'
  prefs: []
  type: TYPE_NORMAL
- en: We have only one property, named `age`, on the `Customer` node, apart from the
    unique ID, `customer_id`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We map the other properties of the customer data as labels on the `Customer`
    node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach follows the *consumption-based approach* to data modeling we discussed
    previously. Say we want to understand how the customers who are regular fashion
    news subscribers behave – this gives us an easy way to retrieve this information.
    Neo4j optimizes this type of retrieval using a label-based approach. We could
    make this customer behavior (fashion news subscription) a property and create
    an index to retrieve this data, but that would require more storage, as well as
    having an index lookup cost. Say we want to use the customers who are active club
    members and are regular fashion news consumers – this label-based approach gives
    us an edge to retrieve this information more effectively when compared to storing
    it as a property. Also, when we display this information as a graph, users can
    easily see the information in the labels, rather than looking for a property.
    It feels more natural to consume the data in this manner and queries also look
    more natural.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will load the article data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the article data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The article data contains other categories that describe the article, apart
    from the unique article ID and description. We will make other attributes that
    describe articles nodes themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this purpose, we need to create these unique constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we have converted most of the attributes of the articles into
    nodes. This sort of normalizes the data represented in the graph. This Cypher
    will load the data into the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For each row, create an article, product, and product group and associate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now add the graphical appearance and colors associated with the article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us connect the department associated with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let us connect the section the article belongs to and the garment
    group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: From the Cypher query, we can see that, in the graph, we are persisting the
    normalized data, without duplicating values for various aspects that describe
    the article.
  prefs: []
  type: TYPE_NORMAL
- en: We will load the transactions next.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the transaction data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `transaction_train.csv` data is in the order transactions have occurred.
    This makes it possible to load the data and preserve the sequence in the graph
    in an easy manner. We have this data in each row for transactions: transaction
    date, article ID, customer ID, price, and sales channel.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: We don’t have a unique ID for each of the transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this Cypher to load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: From this Cypher, we can see that we take the first transaction we find for
    a given customer and connect it to the customer using a *START_TRANSACTION* relationship.
    We use a *LATEST* relationship to track the last transaction the customer made.
    As we keep getting more transactions for the customer, we keep moving the *LATEST*
    relationship to the newest transaction. We connect the earlier transaction that
    was connected using a *LATEST* relationship and the new transaction with a *NEXT*
    relationship. So, in this graph, we are representing the transactions made by
    customers as a transaction train, true to the name of the dataset file `transaction_train.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Final graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After loading all the data, our graph model will look as shown in *Figure 8.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 — Graph data model after loading the H&M data](img/B31107_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 — Graph data model after loading the H&M data
  prefs: []
  type: TYPE_NORMAL
- en: We can see from this graph that the article attributes are fanned out into various
    individual nodes. The **Customer** node is connected to the postal code and first
    and last transactions. **Transaction** is associated with **Article**. The **Transaction**
    nodes are also connected to any next transactions available for a given customer.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have loaded the data, let us explore how we can further enhance
    the graph from this data, to add our own understanding of the data and ideas into
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimizing for recommendations: best practices in graph modeling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have a graph now, with data loaded the way we want to consume it and representing
    the context of the data. Still, the graph represents only the original context
    provided. Say we want to consume the data by season and year – we still need to
    build queries to retrieve it. Since Neo4j is schema optional, maybe we can do
    some post-processing and add extra relationships to consume the data in that way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this Cypher script, we are creating seasonal relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each customer, iterate through the transactions and assign a season value
    based on month and year:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For example, if the month is `1` and the year is `2020`, we assign `WINTER_2019`
    as the season name for that transaction as the context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Collect transactions for each season value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the first record of the collection for each season value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a relationship between the customer and that transaction with the season
    value as the relationship name. We are using the `apoc` method to create the relationship
    as the relationship name is dynamic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Do note that this is a very basic approach. This shows we can create extra context
    in the graph based on our understanding of the data. These approaches make Neo4j
    very suitable for building knowledge graphs. When we make it easy to access data
    in this way, it can open up more ideas on how we can look at the same data differently
    to extract more *intelligence* in a simple manner that’s traceable and understandable
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not want to load the data manually, you can download the database
    snapshot from the following URL:  [https://packt-neo4j-powered-applications.s3.us-east-1.amazonaws.com/Building+Neo4j-Powered+Applications+with+LLMs+Database+Dump+files.zip](https://packt-neo4j-powered-applications.s3.us-east-1.amazonaws.com/Building+Neo4j-Powered+Applications+with+LLMs+Database+Dump+files.zip)
  prefs: []
  type: TYPE_NORMAL
- en: We have now added more context to the data. Let’s look at the graph data model
    next.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 — H&M graph data model after enhancing with seasonal relationships](img/B31107_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 — H&M graph data model after enhancing with seasonal relationships
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us use our understanding of the data to write a query to get articles bought
    by a random customer in the summer of 2019:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With this query, we find customers who bought items both in the summer and fall
    of 2019, pick one customer from that list, and retrieve the article descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the query will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 — Cypher query to retrieve SUMMER_2019 purchases for a customer](img/B31107_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 — Cypher query to retrieve SUMMER_2019 purchases for a customer
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the query, it is easy to understand what the query is doing. We
    use `SUMMER_2019` as the starting point and a transaction before `FALL_2019` relationship
    as the endpoint, traverse from the start point to the endpoint, and retrieve the
    articles of those transactions.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that we are completely relying on the graph traversals instead of
    property-based filters, which makes executing this query very efficient. Neo4j
    is built to execute these kinds of queries very efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to look at a graph data model and how building
    a model based on how we consume it makes it easier to retrieve the data efficiently.
    We looked at the H&M recommendations dataset and loaded it using those principles,
    and also augmented it using the properties and our understanding of that data.
    This added more context to the graph and also made it simple to query the data
    – queries are more readable and explainable to others in a simpler way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build on this data, using an LLM to enhance it
    further, and will see how LLMs can provide us with more capable knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
