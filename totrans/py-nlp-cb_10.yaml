- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI and Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore recipes that use the generative aspect of the
    transformer models to generate text. As we touched upon the same in [*Chapter
    8*](B18411_08.xhtml#_idTextAnchor205), *Transformers and Their Applications*,
    the generative aspect of the transformer models uses the decoder component of
    the transformer network. The decoder component is responsible for generating text
    based on the provided context.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of the **General Purpose Transformers** (**GPT**) family of
    **Large Language Models** (**LLMs**), these have only grown in size and capability
    with each new version. LLMs such as GPT-4 have been trained on large corpora of
    text and can match or beat their state-of-the-art counterparts in many NLP tasks.
    These LLMs have also built upon their generational capability and they can be
    instructed to generate text based on human prompting.
  prefs: []
  type: TYPE_NORMAL
- en: We will use generative models based on the transformer architecture for our
    recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Running an LLM locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running an LLM to follow instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting an LLM with external data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting the LLM with external content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a chatbot using an LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating code using an LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a SQL query using human-defined requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents – making an LLM to reason and act
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is in a folder named `Chapter10` in the GitHub repository
    of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter10)).
  prefs: []
  type: TYPE_NORMAL
- en: As in previous chapters, the packages required for this chapter are part of
    the `poetry`/`pip` requirements configuration file that is present in the repository.
    We recommend that the reader set up the environment beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Model access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will use models from Hugging Face and OpenAI. The following
    are the instructions to enable model access for the various models that will be
    used for the recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hugging Face Mistral model**: Create the necessary credentials on the Hugging
    Face site to ensure that the model is available to be used or downloaded via the
    code. Please visit the Mistral model details at [https://huggingface.co/mistralai/Mistral-7B-v0.3](https://huggingface.co/mistralai/Mistral-7B-v0.3).
    You will need to request access to the model on the site before running the recipe
    that uses this model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hugging Face Llama model**: Create the necessary credentials on the Hugging
    Face site to ensure that the model is available to be used or downloaded via the
    code. Please visit the Llama 3.1 model details at [https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).
    You will have to request for the model access on the site before you run the recipe
    that uses this model.'
  prefs: []
  type: TYPE_NORMAL
- en: In the code snippets, we are using Jupyter as an environment for execution.
    If you are using the same, you will something like the screenshot shown here.
    You can enter the token in the text field and let the recipe make progress. The
    recipe will wait for the token to be entered the first time. Subsequent runs of
    the recipe will use the cached token that the Hugging Face library creates for
    the user locally.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18411_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Copying a token from Hugging Face
  prefs: []
  type: TYPE_NORMAL
- en: '`api-token`. In the code snippets, we are using Jupyter as an environment for
    execution. If you are using the same, you will see a text box where you will need
    to enter the `api-token`. You can enter the token in the text field and let the
    recipe make progress. The recipe will wait for the token to be entered.'
  prefs: []
  type: TYPE_NORMAL
- en: Running an LLM locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to load an LLM locally using the CPU or GPU
    and generate text from it after giving it a starting text as seed input. An LLM
    running locally can be instructed to generate text based on prompting. This new
    paradigm of generation of text via instruction prompting has brought the LLM to
    recent prominence. Learning to do this allows for control over hardware resources
    and environment setup, optimizing performance and enabling rapid experimentation
    or prototyping with text generation from seed inputs. This enhances data privacy
    and security, along with a reduced reliance on cloud services, and facilitates
    cost-effective deployment for educational and practical applications. As we run
    an LLM locally as part of the recipe, we will use instruction prompting to make
    it generate text based on a simple instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We recommend that you use a system with at least 16 GB of RAM or a system with
    a GPU that has at least 8 GB of VRAM. These examples were created on a system
    with 8 GB of RAM and an nVidia RTX 2070 GPU with 8 GB of VRAM. These examples
    will work without a GPU as long as there is 16 GB of system RAM. In this recipe,
    we will load the **Mistral-7B** model using the Hugging Face ([https://huggingface.co/docs](https://huggingface.co/docs))
    libraries. This model has a smaller size compared to other language models in
    its class but can outperform them on several NLP tasks. The Mistral-7B model with
    7 billion network parameters can outperform the **Llama2** model, which has over
    13 billion parameters.
  prefs: []
  type: TYPE_NORMAL
- en: It is required that the user create the necessary credentials on the Hugging
    Face site to ensure that the model is available to be used or downloaded via the
    code. Please refer to *Model access* under the *Technical requirements* section
    to complete the step to access the Mistral model. Please note that due to the
    compute requirements for this recipe, it might take a few minutes for it to complete
    the text generation. If the required compute capacity is unavailable, we recommend
    that the reader refer to the *Using OpenAI models instead of local ones* section
    at the end of this chapter and use the method described there to use an OpenAI
    model for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we set up the login for Hugging Face. Though we can set the token
    directly in the code, we recommend setting the token in an environment variable
    and then reading from it in the notebook. Calling the **login** method with the
    token authorizes the call to Hugging Face and allows the code to download the
    model locally and use it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the device, the **mistralai/Mistral-7B-v0.3** model,
    and the tokenizer, respectively. We set the **device_map** parameter to **auto**,
    which lets the pipeline pick the available device to use. We set the **load_in_4bit**
    parameter to **True**. This lets us load the quantized model for the inference
    (or generation) step. Using a quantized model consumes less memory and lets us
    load the model locally on systems with limited memory. The loading of the quantized
    model is handled by the **AutoModelForCausalLM** module, and it downloads a model
    from the Hugging Face hub that has been quantized to the bit size specified in
    the parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a generation config. This generation config is
    passed to the model, instructing it on how to generate the text. We set the **num_beams**
    parameter to **4**. This parameter results in the generated text being more coherent
    and grammatically correct as the number of beams is increased. However, a greater
    number of beams also results in decoding (or text-generation) time. We set the
    **early_stopping** parameter to **True** as the generation of the next word is
    concluded as soon as the number of beams reaches the value specified in the **num_beams**
    parameter. The **eos_token_id** (e.g., **50256** for GPT models) and **pad_token_id**
    (e.g., **0** for GPT models) are defaulted to use the model’s token IDs. These
    token IDs are used to specify the end-of-sentence and padding tokens that will
    be used by the model. The **max_new_tokens** parameter specifies the maximum number
    of tokens that will be generated. There are more parameters that can be specified
    for generating the text and we encourage you to play around with different values
    of the previously specified parameters, as well as any additional parameters for
    customizing the text generation. For more information, please refer to the transformer
    documentation on the **GenerationConfig** class at [https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a seed sentence. This seed sentence acts as a prompt
    to the model asking it to generate a step-by-step way to make an apple pie:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we tokenize the seed sentence to transform the text into the
    corresponding embedded representation and pass it to the model to generate the
    text. We also pass the **generation_config** instance to it. The model generates
    the token IDs as part of its generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, we decode the token IDs that were generated from the previous
    step. The transformer model uses special tokens such as **CLS** or **MASK** and
    to generate the text as part of the training. We set the value of **skip_special_tokens**
    to **True**. This allows us to omit these special tokens and generate pure text
    as part of our output. We print the decoded (or generated) text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output would look like the following. We have shortened the output for
    brevity. You might see a longer result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Running an LLM to follow instructions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to get an LLM to follow instructions via prompting.
    An LLM can be provided some context and asked to generate text based on that context.
    This is a very novel feature of an LLM. The LLM can be specifically instructed
    to generate text based on explicit user requirements. Using this feature expands
    the breadth of use cases and applications that can be developed. The context and
    the question to be answered can be generated dynamically and used in various use
    cases ranging from answering simple math problems to sophisticated data extraction
    from knowledge bases.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `meta-llama/Meta-Llama-3.1-8B-Instruct` model for this recipe.
    This model is built on top of the `meta-llama/Meta-Llama-3.1-8B` model and has
    been tuned to follow instructions via prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is required that the user create the necessary credentials on the Hugging
    Face site to ensure that the model is available to be used or downloaded via the
    code. Please refer to *Model access* under the *Technical requirements* section
    to complete the step to access the Llama model.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the **10.2_instruct_llm.ipynb** notebook from the code site if you
    want to work from an existing notebook. Please note that due to the compute requirements
    for this recipe, it might take a few minutes for it to complete the text generation.
    If the required compute capacity is unavailable, we recommend that the reader
    refer to the *Using OpenAI models instead of local ones* section at the end of
    this chapter and use the method described there to use an OpenAI model for this
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes an LLM model to be loaded into memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It initializes a prompt to instruct the LLM to perform a task. This task is
    that of answering a question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It sends the prompt to the LLM and asks it to generate an answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the login for Hugging Face. Set the **HuggingFace** token in an environment
    variable and read from it into a local variable. Calling the **login** method
    with the token authorizes the call to **HuggingFace** and allows the code to download
    the model locally and use it. You will see a similar login window as the one shown
    in the *Running an LLM locally* recipe in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we specify the model name. We also define the quantization configuration.
    Quantization is a technique to reduce the size of the internal LLM network weights
    to a lower precision. This allows us to load the model on systems with limited
    CPU or GPU memory. Loading an LLM with its default precision requires a large
    amount of CPU/GPU memory. In this case, we load the network weights in four bits
    using the **load_in_4bit** parameter of the **BitsAndBytesConfig** class. The
    other parameters used are described as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**bnb_4bit_compute_dtype**: This parameter specifies the data type that is
    used during the computation. Though the network weights are stored in four bits,
    the computation still happens in 16 or 32 bits as defined by this parameter. Setting
    this to **torch.float16** results in speed improvements in certain scenarios.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bnb_4bit_use_double_quant**: This parameter specifies that nested quantization
    should be used. This means that a second quantization is performed which saves
    an additional 0.4 bits per parameter in the network. This helps us save the memory
    needed for the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**bnb_4bit_quant_type**: This **nf4** parameter value initializes the weights
    of the network using a normal distribution, which is useful during the training
    of the model. However, it does not have any impact on inference, such as for this
    recipe. We will still be setting this to **nf4** to keep it consistent with the
    model weights.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For quantization concepts, we recommend referring to the blog post at [https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes),
    where this is explained in greater detail. Please note that in order to load the
    model in 4-bit, it is required that a GPU is used:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we load the **meta-llama/Meta-Llama-3.1-8B-Instruct** model and
    the corresponding tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a pipeline that weaves the model and tokenizer
    together with some additional parameters. We covered the description of these
    parameters in the *Running an LLM locally* recipe in this chapter. We recommend
    referring to that recipe for more details on these parameters. We are adding an
    additional parameter named **repetition_penalty** here. This ensures that the
    LLM does not go into a state where it starts repeating itself or parts of the
    text that were generated before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create a prompt that sets up an instruction context that can
    be passed to the LLM. The LLM acts as per the instructions set up in the prompt.
    In this case, we start our instruction with a conversation between the user and
    the agent. The conversation starts with the question **What is your favourite
    country?**. This question is followed by the model answer in the form of **Well,
    I am quite fascinated with Peru.**. We then follow it up with another instruction
    by asking the question **What can you tell me about Peru?**. This methodology
    serves as a template for the LLM to learn our intent and generate an answer for
    the follow-up question based on the pattern we specified in our instruction prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we execute the pipeline with the prompt and execute it. We additionally
    specify the maximum number of tokens that should be generated as part of the output.
    This explicitly instructs the LLM to stop generation once the specific length
    is reached:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have seen a way to instruct a model to generate text, we can just
    change the prompt and get the model to generate text for a completely different
    kind of question. Let us change the prompt text to the following and use the same
    recipe to generate text based on the updated prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the preceding output, the model is able to understand the
    instructions quite clearly. It is able to reason well and answer the question
    correctly. This recipe only used the context that was stored within the LLM. More
    specifically, the LLM used its internal knowledge to answer this question. LLMs
    are trained on huge corpora of text and can generate answers based on that large
    corpus. In the next recipe, we will learn how to augment the knowledge of an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting an LLM with external data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following recipes, we will learn how to get an LLM to answer questions
    on which it has not been trained. These could include information that was created
    after the LLM was trained. New content keeps getting added to the World Wide Web
    daily. There is no one LLM that can be trained on that context every day. The
    **Retriever Augmented Generation** (**RAG**) frameworks allow us to augment the
    LLM with additional content that can be sent as input to it for generating content
    for downstream tasks. This allows us to save on costs too since we do not have
    to spend time and compute costs on retraining a model based on updated content.
    As a basic introduction to RAG, we will augment an LLM with some content from
    a few web pages and ask some questions pertaining to the content contained in
    those pages. For this recipe, we will first load the LLM and ask it a few questions
    without providing it any context. We will then augment this LLM with additional
    context and ask the same questions. We will compare the answers, which will demonstrate
    the power of the LLM when coupled with augmented content.
  prefs: []
  type: TYPE_NORMAL
- en: Executing a simple prompt-to-LLM chain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will create a simple prompt that can be used to instruct
    an LLM. A prompt is a template with placeholder values that can be populated at
    runtime. The LangChain framework allows us to weave a prompt and an LLM together,
    along with other components in the mix, to generate text. We will explore these
    techniques in this and some of the recipes that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We must create the necessary credentials on the Hugging Face site to ensure
    that the model is available to be used or downloaded via the code. Please refer
    to *Model access* under the *Technical requirements* section to complete the step
    to access the Llama model.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the LangChain framework ([https://www.langchain.com/](https://www.langchain.com/))
    to demonstrate the LangChain framework and its capabilities with an example based
    on **LangChain Expression Language** (**LCEL**). Let us start with a simple recipe
    based on the LangChain framework and extend it in the recipes that follow from
    there on. The first part of this recipe is very similar to the previous one. The
    only difference is the use of the LangChain framework.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `10.3_langchain_prompt_with_llm.ipynb` notebook from the code
    site if you want to work from an existing notebook. Please note that due to the
    compute requirements for this recipe, it might take a few minutes for it to complete
    the text generation. If the required compute capacity is unavailable, we recommend
    that you refer to the *Using OpenAI models instead of local ones* section at the
    end of this chapter and use the method described there to use an OpenAI model
    for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes an LLM model to be loaded into memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It initializes a prompt to instruct the LLM perform a task. This task is that
    of answering a question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It sends the prompt to the LLM and asks it to generate an answer. This is all
    done via the LangChain framework.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with doing the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the model name and the quantization configuration.
    We have expanded upon quantization in the *Running an LLM to follow instructions*
    recipe; please check there for more details. We will use the **meta-llama/Meta-Llama-3.1-8B-Instruct**
    model that was released by Meta in July of 2024\. It has outperformed models of
    bigger size on many NLP tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the model. We have elaborated on the methodology
    for loading the model and the tokenizer using the **Transformers** library in
    detail in [*Chapter 8*](B18411_08.xhtml#_idTextAnchor205). To avoid repeating
    the same information here, please refer to that chapter for more details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the pipeline. We have elaborated on the pipeline
    construct from the transformers library in detail in [*Chapter 8*](B18411_08.xhtml#_idTextAnchor205).
    To avoid repeating the same information here, please refer to that chapter for
    more details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a chat prompt template, which is of the defined
    **ChatPromptTemplate** type. The **from_messages** method takes a series of (**message
    type**, **template**) tuples. The second tuple in the messages array has the **{input}**
    template. This signifies that this value will be passed later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize an output parser that is of the **StrOutputParser**
    type. It converts a chat message returned by an LLM instance to a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize an instance of a chain next. The chain pipes the output of one
    component to the next. In this instance, the prompt is sent to the LLM and it
    operates on the prompt instance. The output of this operation is a chat message.
    The chat message is then sent to the **output_parser**, which converts it into
    a string. In this step, we only set up the various components of the chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we invoke the chain and print the results. We pass the input
    argument in a dictionary. We set up the prompt template as a message that had
    the **{input}** placeholder defined there. As part of the chain invocation, the
    input argument is passed through to the template. The chain invokes the command.
    The chain is instructing the LLM to generate the answer to the question it asked
    via the prompt that we set up previously. As we can see from the output, the advice
    presented in this example is good. We have clipped the output for brevity and
    you might see a longer output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we change the prompt a bit and make it answer a simple question
    about the 2024 Paris Olympics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output is generated for the question. We can see that the answer
    to the question of the number of volunteers is inaccurate by comparing the answer
    to the Wikipedia source. We have omitted a large part of the text that was returned
    in the result. However, to show an example, the Llama 3.1 model generated more
    text than we asked it to and started answering more questions that it was never
    asked. In the next recipe, we will provide a web page source to an LLM and compare
    the returned results with this one for the same question:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Augmenting the LLM with external content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will expand upon the previous example and build a chain that
    passes external content to the LLM and helps it answer questions based on that
    augmented content. The technique learned as part of this recipe will help us understand
    a simple framework for how to extract content from a source and store that in
    a medium that is conducive to fast semantic searches based on context. Once we
    learn how to store the content in a searchable format, we can use that store to
    extract answers to questions that are in open form. This approach can be scaled
    for production as well using the right tools and approaches. Our goal here is
    to demonstrate the basic framework to extract an answer to a question, given a
    content source.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use a model from OpenAI in this recipe. Please refer to *Model access*
    under the *Technical requirements* section to complete the step to access the
    OpenAI model. You can use the `10.4_rag_with_llm.ipynb` notebook from the code
    site if you want to work off an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes the ChatGPT LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It scrapes content from a webpage and breaks it into chunks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text in the document chunks is vectorized and stored in a vector store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A chain is created that wires the LLM, the vector store, and a prompt with a
    question to answer questions based on the content present on the web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the **gpt-4o-mini** model from OpenAI using the
    ChatOpenAI initializer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we load the Wikipedia entry on the 2024 Summer Olympics. We initialize
    a **WebBaseLoader** object and pass it the Wikipedia URL for the 2024 Summer Olympics.
    It extracts the HTML content and the main content on each HTML page that is parsed.
    The **load** method on the loader instance triggers the extraction of the content
    from the URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the text splitter instance and call the **split_documents**
    method on it. This splitting of the document is a needed step as an LLM can only
    operate on a context of a limited length. For some large documents, the length
    of the document exceeds the maximum context length supported by the LLM. Breaking
    a document into chunks and using those to match the query text allows us to retrieve
    more relevant parts from the document. The **RecursiveCharacterTextSplitter**
    splits the document based on newline, spaces, and double-newline characters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a vector store. We initialize the vector store
    with the document chunks and the embedding provider. The vector store creates
    embeddings of the document chunks and stores them along with the document metadata.
    For production-grade applications, we recommend visiting the following URL: [https://python.langchain.com/docs/integrations/vectorstores/](https://python.langchain.com/docs/integrations/vectorstores/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There, you can select a vector store based on your requirements. The LangChain
    framework is versatile and works with a host of prominent vector stores.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we initialize a retriever by making a call to the `as_retriever` method
    of the vector-store instance. The retriever returned by the method is used to
    retrieve the content from the vector store. The `as_retriever` method is passed
    a `search_type` argument with the `similarity` value, which is also the default
    option. This means that the vector store will be searched against the question
    text based on similarity. The other options supported are `mmr`, which penalizes
    search results of the same type and returns diverse results, and `similarity_score_threshold`,
    which operates in the same way as the `similarity` search type, but can filter
    out the results based on a threshold. These options also support an accompanying
    dictionary argument that can be used to tweak the search parameters. We recommend
    that the readers refer to the LangChain documentation and tweak the parameters
    based on their requirements and empirical findings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We also define a helper method, `format_docs`, that appends the content of
    all the repository docs separated by two newline characters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define a chat template and create an instance of **ChatPromptTemplate**
    from it. This prompt template instructs the LLM to answer the question for the
    given context. This context is provided by the augmentation step via the vector
    store search results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we set up the chain. The chain sequence sets up the retriever
    as a context provider. The **question** argument is assumed to be passed later
    by the chain. The next component is the prompt, which supplies the context value.
    The populated prompt is sent to the LLM. The LLM pipes or forwards the results
    to the **StrOutputParser()** string, which is designed to return the string contained
    in the output of the LLM. There is no execution in this step. We are only setting
    up the chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we invoke the chain and print the results. For each invocation,
    the question text is matched by similarity against the vector store. Then, the
    relevant document chunks are returned, followed by the LLM using these document
    chunks as context and using that context to answer the respective questions. As
    we can see in this case, the answers returned by the chain are accurate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the chain with another question and print the results. As we can see
    in this case, the answers returned by the chain are accurate, though I am skeptical
    about whether **Breaking** is indeed a sport, as returned in the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the chain with another question and print the results. As we can see
    in this case, the answers returned by the chain are accurate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: If we compare these results with the last step of the previous recipe, we can
    see that the LLM returned accurate information as per the content on the Wikipedia
    page. This is an effective use case for RAG where the LLM uses the context to
    answer the question, instead of making up information as it did in the previous
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a chatbot using an LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will create a chatbot using the LangChain framework. In the
    previous recipe, we learned how to ask questions to an LLM based on a piece of
    content. Though the LLM was able to answer questions accurately, the interaction
    with the LLM was completely stateless. The LLM looks at each question in isolation
    and ignores any previous interactions or questions that it was asked. In this
    recipe, we will use an LLM to create a chat interaction, wherein the LLM will
    be aware of the previous conversations and use the context from them to answer
    subsequent questions. Applications of such a framework would be to converse with
    document sources and get to the right answer by asking a series of questions.
    These document sources could be of a wide variety of types, from internal company
    knowledge bases to customer contact center troubleshooting guides. Our goal here
    is to present a basic step-by-step framework to demonstrate the essential components
    working together to achieve the end goal.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use a model from OpenAI in this recipe. Please refer to *Model access*
    under the *Technical requirements* section to complete the step to access the
    OpenAI model. You can use the `10.5_chatbot_with_llm.ipynb` notebook from the
    code site if you want to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes the ChatGPT LLM and an embedding provider. The embedding provider
    is used to vectorize the document content so that a vector-based similarity search
    can be performed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It scrapes content from a webpage and breaks it into chunks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text in the document chunks is vectorized and stored in a vector store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A conversation is started with the LLM via some curated prompts and a follow-up
    question is asked based on the answer provided by the LLM in the previous context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the **gpt-4o-mini** model from OpenAI using the
    ChatOpenAI initializer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we load the embedding provider. The content from the webpage
    is vectorized via the embedding provider. We use the pre-trained **sentence-transformers/all-mpnet-base-v2**
    model using the call to the **HuggingFaceEmbeddings** constructor call. This model
    is a good one for encoding short sentences or a paragraph. The encoded vector
    representation captures the semantic context well. Please refer to the model card
    at [https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
    for more details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will load a web page that has content based on which we want
    to ask questions. You are free to choose any webpage of your choice. We initialize
    a **WebBaseLoader** object and pass it the URL. We call the **load** method for
    the loader instance. Feel free to change the link to any other webpage that you
    might want to use as the chat knowledge base:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the text splitter instance of the **RecursiveCharacterTextSplitter**
    type. Use the text splitter instance to split the documents into chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize the vector or embedding store from the document chunks that we
    created in the previous step. We pass it the document chunks and the embedding
    provider. We also initialize the vector store retriever and the output parser.
    The retriever will provide the augmented content to the chain via the vector store.
    We provided more details in the *Augmenting the LLM with external content* recipe
    from this chapter. To avoid repetition, we recommend referring to that recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a contextualized system prompt. A system prompt
    defines the persona and the instruction that is to be followed by the LLM. In
    this case, we use the system prompt to contain the instruction that the LLM has
    to use the chat history to formulate a standalone question. We initialize the
    prompt instance with the system prompt definition and set it up with the expectation
    that it will have access to the **chat_history** variable that will be passed
    to it at run time. We also set it up with the question template that will also
    be passed at run time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the contextualized chain. As you can see in the
    previous code snippet, we are setting up the prompt with the context and the chat
    history. This chain uses the chat history and a given follow-up question from
    the user and sets up the context for it as part of the prompt. The populated prompt
    template is sent to the LLM. The idea here is that the subsequent question will
    not provide any context and ask the question based on the chat history generated
    so far:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a system prompt, much like in the previous recipe,
    based on RAG. This prompt just sets up a prompt template. However, we pass this
    prompt a contextualized question as the chat history grows. This prompt always
    answers a contextualized question, barring the first one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize two helper methods. The **contextualized_question** method returns
    the contextualized chain if a chat history exists; otherwise, it returns the input
    question. This is the typical scenario for the first question. Once the **chat_history**
    is present, it returns the contextualized chain. The **format_docs** method concatenates
    the page content for each document separated by two newline characters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we set up a chain. We use the **RunnablePassthrough** class to
    set up the context. The **RunnablePassthrough** class allows us to pass the input
    or add additional data to the input via dictionary values. The **assign** method
    will take a key and will assign the value to this key. In this case, the key is
    **context** and the assigned value for it is the result of the chained evaluation
    of the contextualized question, the retriever, and the **format_docs**. Putting
    that into the context of the entire recipe, for the first question, the context
    will use the set of matched records for the question. For the second question,
    the context will use the contextualized question from the chat history, retrieve
    a set of matching records, and pass that as the context. The LangChain framework
    uses a deferred execution model here. We set up the chain here with the necessary
    constructs such as **context**, **qa_prompt**, and the LLM. This is just setting
    the expectation with the chain that all these components will pipe their input
    to the next component when the chain is invoked. Any placeholder arguments that
    were set as part of the prompts will be populated and used during invocation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a chat history array. We ask a simple question
    to the chain by invoking it. What happens internally is the question is essentially
    just the first question since there is no chat history present at this point.
    The **rag_chain** just answers the question simply and prints the answer. We also
    extend the **chat_history** with the returned message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we invoke the chain again with a subsequent question, without
    providing many contextual cues. We provide the chain with the chat history and
    print the answer to the second question. Internally, the **rag_chain** and the
    **contextualize_q_chain** work in tandem to answer this question. The **contextualize_q_chain**
    uses the chat history to add more context to the follow-up question, retrieves
    matched records, and sends that as context to the **rag_chain**. The **rag_chain**
    used the context and the contextualized question to answer the subsequent question.
    As we observe from the output, the LLM was able to decipher what **it** means
    in this context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Note:'
  prefs: []
  type: TYPE_NORMAL
- en: We provided a basic workflow for how to execute RAG-based flows. We recommend
    referring to the LangChain documentation and using the necessary components to
    run solutions in production. Some of these would include evaluating other vector
    DB stores, using concrete types such as **BaseChatMessageHistory** and **RunnableWithMessageHistory**
    to better manage chat histories. Also, use LangServe to expose endpoints to serve
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: Generating code using an LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore how an LLM can be used to generate code. We
    will use two separate examples to check the breadth of coverage for the generation.
    We will also compare the output from two LLMs to observe how the generation varies
    across two different models. Applications of such methods are already incorporated
    in popular **Integrated Development Environments** (**IDEs**). Our goal here is
    to demonstrate a basic framework for how to use a pre-trained LLM to generate
    code snipped based on simple human-defined requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use a model from Hugging Face as well as OpenAI in this recipe. Please
    refer to *Model access* under the *Technical requirements* section to complete
    the step to access the Llama and OpenAI models. You can use the `10.6_code_generation_with_llm.ipynb`
    notebook from the code site if you want to work from an existing notebook. Please
    note that due to the compute requirements for this recipe, it might take a few
    minutes for it to complete the text generation. If the required compute capacity
    is unavailable, we recommend referring to the *Using OpenAI models instead of
    local ones section* at the end of this chapter and using the method described
    there to use an OpenAI model for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes a prompt template that instructs the LLM to generate code for
    a given problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It initializes an LLM model and a tokenizer and wires them together in a pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It creates a chain that connects the prompt, LLM and string post-processor to
    generate a code snippet based on a given instruction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We additionally show the result of the same instructions when executed via an
    OpenAI model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, we define a template. This template defines the instruction or
    the system prompt that is sent to the model as the task description. In this case,
    the template defines an instruction to generate Python code based on users’ requirements.
    We use this template to initialize a prompt object. The initialized object is
    of the **ChatPromptTemplate** type. This object lets us send requirements to the
    model in an interactive way. We can converse with the model based on our instructions
    to generate several code snippets without having to load the model each time.
    Note the **{input}** placeholder in the prompt. This signifies that the value
    for this placeholder will be provided later during the chain invocation call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]python'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '....'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the parameters for the model. *Steps 3-5* have been explained in more
    detail in the *Executing a simple prompt-to-LLM chain* recipe earlier in this
    chapter. Please refer to that recipe for more details. We also initialize a configuration
    for quantization. This has been described in more detail in the *Running an LLM
    to follow instructions* recipe in this chapter. To avoid repetition, we recommend
    referring to *step 3* of that recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the model. In this instance, as we are working to generate code,
    we use the **Meta-Llama-3.1-8B-Instruct** model. This model also has the ability
    to generate code. For a model of this size, it has demonstrated very good performance
    for code generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize the pipeline with the model and the tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize the chain with the prompt and the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We invoke the chain and print the result. As we can see from the output, the
    generated code is reasonably good, with the Node **class** having a constructor
    along with the **inorder_traversal** helper method. It also prints out the instructions
    to use the class. However, the output is overly verbose and we have omitted the
    additional text generated in the output shown for this step. The output contains
    code for preorder traversal too, which we did not instruct the LLM to generate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]python'
  prefs: []
  type: TYPE_NORMAL
- en: '....'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]python'
  prefs: []
  type: TYPE_NORMAL
- en: 'class Node:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __init__(self, value):'
  prefs: []
  type: TYPE_NORMAL
- en: self.value = value
  prefs: []
  type: TYPE_NORMAL
- en: self.left = None
  prefs: []
  type: TYPE_NORMAL
- en: self.right = None
  prefs: []
  type: TYPE_NORMAL
- en: 'class BinaryTree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __init__(self):'
  prefs: []
  type: TYPE_NORMAL
- en: self.root = None
  prefs: []
  type: TYPE_NORMAL
- en: 'def insert(self, value):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if self.root is None:'
  prefs: []
  type: TYPE_NORMAL
- en: self.root = Node(value)
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: self._insert(self.root, value)
  prefs: []
  type: TYPE_NORMAL
- en: 'def _insert(self, node, value):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if value < node.value:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if node.left is None:'
  prefs: []
  type: TYPE_NORMAL
- en: node.left = Node(value)
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: self._insert(node.left, value)
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if node.right is None:'
  prefs: []
  type: TYPE_NORMAL
- en: node.right = Node(value)
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: self._insert(node.right, value)
  prefs: []
  type: TYPE_NORMAL
- en: 'def inorder(self):'
  prefs: []
  type: TYPE_NORMAL
- en: result = []
  prefs: []
  type: TYPE_NORMAL
- en: self._inorder(self.root, result)
  prefs: []
  type: TYPE_NORMAL
- en: return result
  prefs: []
  type: TYPE_NORMAL
- en: 'def _inorder(self, node, result):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if node is not None:'
  prefs: []
  type: TYPE_NORMAL
- en: self._inorder(node.left, result)
  prefs: []
  type: TYPE_NORMAL
- en: result.append(node.value)
  prefs: []
  type: TYPE_NORMAL
- en: self._inorder(node.right, result)
  prefs: []
  type: TYPE_NORMAL
- en: tree = BinaryTree()
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(8)
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(3)
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(10)
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(1)
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(6)
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(14)
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(4)
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(7)
  prefs: []
  type: TYPE_NORMAL
- en: tree.insert(13)
  prefs: []
  type: TYPE_NORMAL
- en: 'print(tree.inorder())  # Output: [1, 3, 4, 6, 7, 8, 10, 13, 14]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'result = chain.invoke({"input": "write a program to generate a 512-bit SHA3
    hash"})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print(result)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'System: Write some python code to solve the user''s problem. Keep the answer
    as brief as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Return only python code in Markdown format, e.g.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Human: write a program to generate a 512-bit SHA3 hash'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_openai import ChatOpenAI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: api_key = os.environ.get('OPENAI_API_KEY')
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: llm = ChatOpenAI(openai_api_key=api_key)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'result = chain.invoke({"input": " write a program to generate a 512-bit SHA3
    hash"})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print(result)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'result = chain.invoke({"input": "write a program to generate a 512-bit AES
    hash"})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print(result)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_core.prompts import ChatPromptTemplate
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain_core.output_parsers import StrOutputParser
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain_core.runnables import RunnablePassthrough
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain_community.utilities import SQLDatabase
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain_openai import ChatOpenAI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: import os
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'template = """You are a SQL expert. Based on the table schema below, write
    just the SQL query without the results that would answer the user''s question.:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{schema}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: {question}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SQL Query:"""
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: prompt = ChatPromptTemplate.from_template(template)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: db = SQLDatabase.from_uri(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"sqlite:///db/northwind-SQLite3/dist/northwind.db")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def get_schema(_):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return db.get_table_info()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def run_query(query):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return db.run(query)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: api_key = os.environ.get('OPENAI_API_KEY')
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: model = ChatOpenAI(openai_api_key=api_key)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: sql_response = (
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RunnablePassthrough.assign(schema=get_schema)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| prompt'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| model.bind(stop=["\nSQLResult:"])'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| StrOutputParser()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'sql_response.invoke({"question": "How many employees are there?"})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '''SELECT COUNT(*) FROM Employees'''
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'sql_response.invoke({"question": "How many employees have been tenured for
    more than 11 years?"})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '"SELECT COUNT(*) \nFROM Employees \nWHERE HireDate <= DATE(''now'', ''-5 years'')"'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'template = """Based on the table schema below, question, sql query, and sql
    response, write a natural language response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{schema}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Question: {question}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'SQL Query: {query}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'SQL Response: {response}"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: prompt_response = ChatPromptTemplate.from_template(template)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: full_chain = (
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RunnablePassthrough.assign(query=sql_response).assign(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: schema=get_schema,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'response=lambda x: run_query(x["query"]),'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| prompt_response'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'result = full_chain.invoke({"question": "How many employees are there?"})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print(result)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: content='There are 9 employees in the database.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'result = full_chain.invoke({"question": "Give me the name of employees who
    have been tenured for more than 11 years?"})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print(result)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: content='The employees who have been tenured for more than 11 years are Nancy
    Davolio, Andrew Fuller, and Janet Leverling.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: from langchain.agents import AgentType, initialize_agent, load_tools
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain.agents.tools import Tool
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain.chains import LLMMathChain
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain_experimental.plan_and_execute import (
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: PlanAndExecute, load_agent_executor, load_chat_planner)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain.utilities import SerpAPIWrapper
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from langchain_openai import OpenAI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'api_key = ''OPEN_API_KEY'' # set your OPENAI API key'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'serp_api_key=''SERP API KEY'' # set your SERPAPI key'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: llm = OpenAI(api_key=api_key, temperature=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: search_helper = SerpAPIWrapper(serpapi_api_key=serp_api_key)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: math_helper = LLMMathChain.from_llm(llm=llm, verbose=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: search_tool = Tool(name='Search', func=search_helper.run,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: description="use this tool to search for information")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: math_tool = Tool(name='Calculator', func=math_helper.run,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: description="use this tool for mathematical calculations")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: tools = [search_tool, math_tool]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: action_planner = load_chat_planner(llm)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: agent_executor = load_agent_executor(llm, tools, verbose=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: agent = PlanAndExecute(planner=action_planner,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: executor=agent_executor, verbose=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: agent.invoke("How many more FIFA world cup wins does Brazil have compared to
    France?")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Entering new PlanAndExecute chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: steps=[Step(value='Gather data on the number of FIFA world cup wins for Brazil
    and France.'), Step(value='Calculate the difference between the two numbers.'),
    Step(value='Output the difference as the answer.\n')]
  prefs: []
  type: TYPE_NORMAL
- en: Entering new AgentExecutor chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Search",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "Number of FIFA world cup wins for Brazil and France"'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*****'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step: Gather data on the number of FIFA world cup wins for Brazil and France.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response: Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Search",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "Number of FIFA world cup wins for Brazil and France"'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Entering new AgentExecutor chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Thought: I can use the Calculator tool to subtract the number of wins for France
    from the number of wins for Brazil.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Entering new LLMMathChain chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5 - 2[PRE107]
  prefs: []
  type: TYPE_NORMAL
- en: '...numexpr.evaluate("5 - 2")...'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: 3'
  prefs: []
  type: TYPE_NORMAL
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Observation: Answer: 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thought: I have the final answer now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*****'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step: Calculate the difference between the two numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response: The difference between the number of FIFA world cup wins for Brazil
    and France is 3.'
  prefs: []
  type: TYPE_NORMAL
- en: Entering new AgentExecutor chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "The difference between the number of FIFA world cup wins for
    Brazil and France is 3."'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*****'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step: Output the difference as the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response: Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "The difference between the number of FIFA world cup wins for
    Brazil and France is 3."'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{''input'': ''How many more FIFA world cup wins does Brazil have compared to
    France?'','
  prefs: []
  type: TYPE_NORMAL
- en: '''output'': ''Action:\n{\n  "action": "Final Answer",\n  "action_input": "The
    difference between the number of FIFA world cup wins for Brazil and France is
    3."\n}\n\n''}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: import getpass
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_openai import ChatOpenAI
  prefs: []
  type: TYPE_NORMAL
- en: os.environ["OPENAI_API_KEY"] = getpass.getpass()
  prefs: []
  type: TYPE_NORMAL
- en: llm = ChatOpenAI(model="gpt-4o-mini")
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: This completes our chapter on generative AI and LLMs. We have just scratched
    the surface of what is possible via generative AI; we hope that the examples presented
    in this chapter help illuminate the capabilities of LLMs and their relation to
    generative AI. We recommend exploring the LangChain site for updates and new tools
    and agents for their use cases and applying them in production scenarios following
    the established best practices. New models are frequently added on the Hugging
    Face site and we recommend staying up to date with the latest model updates and
    their related use cases. This makes it easier to become effective NLP practitioners.
  prefs: []
  type: TYPE_NORMAL
