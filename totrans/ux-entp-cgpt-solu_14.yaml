- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not all companies are built the same way. The development process is running
    full steam, and time and resources were given to do the design steps at the right
    time and scale. Perfect. It would be best if you had written this chapter. Enterprise-scale
    problems come with enterprise-sized issues. The one that makes my head spin the
    most is the time it takes to incorporate design solutions into a live environment.
    *Time is the enemy of good*. And with each evolution of technology, the time to
    market and the iterative cycle have to be faster. Today’s user is not what it
    once was.
  prefs: []
  type: TYPE_NORMAL
- en: Provide the best solution as soon as possible or risk losing customers. Thus,
    even in an Agile world (let’s include Scrum, Lean, and another iterative modern
    approach to software development when we mention Agile), its goal is to deliver
    quality promptly. Design and the efforts that go into it can cause headaches for
    engineering teams wanting to move quickly. Remember our discussion on cheap, fast,
    and good, choose two? Fast is required not to alienate customers, and customers
    no longer accept poor quality from enterprise solutions, so it must be good. Hence,
    it might not be cheap. Well, *cheap* here means expending resources to ensure
    that it is good. The previous chapters talked about how to make it good, but we
    need to apply these methods, practices, guidelines, and heuristics are applied
    in a way conducive to bringing quality work to market.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll discuss two areas to include conversational design in
    a software development organization:'
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating design thinking into development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a content improvement life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating design thinking into development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of this book is about using the steps needed to solve problems, considering
    user needs, refining problems, and creating and testing solutions. This is the
    essence of **design thinking**. You were probably doing this even without knowing
    its name. This hands-on approach emphasizes user empathy through the research
    methods covered in the book. However, in practical terms, a design thinker remembers
    the user goals at every process step. Because Agile aligns so well with the iterative
    nature of Generative AI, this chapter exposes tricks to make generative AI successful
    in an enterprise software development organization. This works for companies who
    deliver enterprise tools and those who use enterprise tools to work with a mass
    audience. It should be clear that both have compelling use cases and challenges
    with design thinking in an agile AI world.
  prefs: []
  type: TYPE_NORMAL
- en: This will sound harsh. If an organization is not using Agile or a form of iterative
    development, successful Generative AI will be out of reach. We live in a new,
    fast-paced world that needs a robust *iterative* approach to support it. I strongly
    encourage its consideration. There is a wealth of resources, from the Scaled Agile
    Framework discussed early on to the Agile Alliance and many more. AI tools are
    becoming fundamental in many development processes. Indeed, they should be deployed
    to customers as well. Tools for prompt engineering and fine-tuning were covered,
    but a wealth of AI add-ons can support backlog management, write better issues,
    or enrich development processes. Over 50 tools are marked as *AI apps* in the
    Atlassian Marketplace for Jira and Confluence. I can’t speak to how good they
    are, but certainly, in the end, some will be timesavers and workflow boons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Website: [AI marketplace at Atlassian](https://marketplace.atlassian.com/categories/artificial-intelligence)
    ([https://marketplace.atlassian.com/categories/artificial-intelligence](https://marketplace.atlassian.com/categories/artificial-intelligence))'
  prefs: []
  type: TYPE_NORMAL
- en: These tools are outside the scope of this book, but they help teams move towards
    more efficient processes. The key is that this is an ever-changing world. Adapt
    and learn how our content, customers, and AI work together, and then be able to
    make improvements to production quickly.
  prefs: []
  type: TYPE_NORMAL
- en: I’d be surprised if anyone needs to watch this video. If you are new to Agile,
    take one of the three-day Agile scrum classes. They can be life-changing, and
    there is a lot to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Introduction to Agile](https://www.agilealliance.org/agile101/agile-basics/introduction-to-agile/)
    ([https://www.agilealliance.org/agile101/agile-basics/introduction-to-agile/](https://www.agilealliance.org/agile101/agile-basics/introduction-to-agile/))'
  prefs: []
  type: TYPE_NORMAL
- en: The software industry is able to transition to an AI-centric approach. The results
    from the 17th Annual State of Agile survey (2024 based on 2023 results), with
    788 respondents, are compelling. Because the industry is overwhelmingly Agile,
    and all our recommendations can apply to any method, the focus is to reference
    Agile and how to address concerns along the way. 71% of surveyed companies use
    Agile. Some teams use other modern approaches, such as **DevOps** (**Development
    and Operations** working as one team), **Iterative**, **Lean** (which works only
    on the most critical items, with no multitasking), or **Spiral** (a four-phase
    repetitive process). Even the **waterfall** approach (a complete phase with no
    going back) is found in 28% of respondents.
  prefs: []
  type: TYPE_NORMAL
- en: There are fans of Agile, but there are critics as well. The biggest complaints
    concern the relentless pace and story points. Story points are for sizing work,
    but some argue for the well-understood measurement in hours. Every significant
    change in someone’s lifestyle will be met with complaints. *Change is hard. Change
    is inevitable.* The pace of change in the technology world is constantly speeding
    up. And we don’t need to argue about story points. Do research on your own. The
    chapter focuses on how design assists an LLM solution come to life. And there
    is value in aligning with Agile. The most significant value is the alignment around
    iterative design. It is mandatory with Generative AI to have an iterative life
    cycle. But doing it well has challenges. From the scoring discussion in [*Chapter
    4*](B21964_04.xhtml#_idTextAnchor085), *Scoring Stories*, it is clear that design-related
    efforts have a place in Agile, but designer efforts do not fit nicely into a single
    sprint. Research and studies typically take time to write and run a plan but don’t
    fit in a week’s sprint. It turns out that is ok. There are answers. So, let me
    summarize some tips to make design a successful part of a Generative AI iterative
    *Agile* process. Even without a strict Agile method, the principles and coaching
    can be applied within most organizations. Even our first suggestion about finding
    a sponsor is broader than just Agile.
  prefs: []
  type: TYPE_NORMAL
- en: Find a sponsor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Agile approach is perfect for an iterative AI care and feeding approach.
    Because of this, a business’s issues with Agile become issues with AI adoption.
    Look at *Figure 11**.1* from Digital AI’s 17th annual survey of Agile.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – The 17th Annual State of Agile survey – business issues with Agile
    (openart.ai)
  prefs: []
  type: TYPE_NORMAL
- en: The survey covers a lot of ground, but organizational support is critical for
    Agile success. By extension, a Generative AI solution needs organizational support.
    The three top issues (and others) relate to managerial issues. This is the latest
    survey.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [State of Agile Report](https://digital.ai/resource-center/analyst-reports/state-of-agile-report/)
    ([https://digital.ai/resource-center/analyst-reports/state-of-agile-report/](https://digital.ai/resource-center/analyst-reports/state-of-agile-report/))'
  prefs: []
  type: TYPE_NORMAL
- en: The iterative care and feeding approach is typically radically different than
    a quarterly release schedule. It is a cultural shift. Even in an Agile organization,
    many ship products on a multiple of the Agile schedule. It is common to see two-week
    sprints but quarterly releases. And combining a slow cadence with a lack of leadership
    participation and support creates a problem. Get a sponsor who recognizes that
    the speed of change is critical to AI success and can help create a process conducive
    to rapid changes. Leadership support is essential as an organization recognizes
    the effort it takes to do AI well. Of course, people are critical, but tools help,
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Find the right tools and integrate Generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unsurprisingly, it is a massive undertaking to keep up with this level of change,
    manage huge suites of test cases, and deal with an entire suite of knowledge.
    It doesn’t involve a single tool but dozens, but at least start with a tool to
    manage the process and change. As discussed in [*Chapter 4*](B21964_04.xhtml#_idTextAnchor085),
    *Scoring Stories* using Agile tracking tools, incorporating scoring, and working
    from a WSJF backlog manages change. It would be challenging to find a large enterprise
    that doesn’t have a knowledge management tool. Still, adaption might be necessary
    to feed the RAG solution accurately – that is, recognize that knowledge and data
    will need to adapt to improve the results generated by ChatGPT. It isn’t magic
    but hard work that fills in the gaps. This might mean articles tuned to support
    the LLM, reworking existing knowledge, adapting APIs to include more context with
    calls, and using intermediate tools to improve the flow of information to the
    LLM for processing. And let’s not forget to mention using smaller models to do
    bespoke tasks a few times, feeding those results to other models in a chain to
    improve overall performance. It reminds me of a quote: “*Stick to the plan, stick
    to the plan, stick to the plan.*” This means making last-minute changes to processes
    can sometimes have spurious consequences. Another similar concept is “*Be religious...*
    *at first.*”'
  prefs: []
  type: TYPE_NORMAL
- en: Be religious… at first
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a reason the Agile manifesto works for millions of people, even as
    they grumble about its shortcomings. Sure, 15% of respondents in the Agile survey
    were “*not at all satisfied*” with Agile, but given our understanding of what
    Agile, Scrum, and the shared concept, experience suggests most of this is about
    not knowing who or what to blame for failures. Consider these and the accompanying
    principles as they can be applied. Once you become an expert, *then* adapt. At
    an organization, I went into one Wednesday team meeting where the VP of a group
    of about 800 people dictated, “*On Monday, we will start doing Agile.*” Then they
    proceeded to choose which elements to do or not do (including deciding that self-organizing
    teams weren’t something they could do, not knowing the meaning of “self-organizing”).
    Don’t let gross incompetence derail quality goals. Try to understand the intent
    of the principles of Agile before discarding them. Let me quote a few of the 12
    basic principles of Agile:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Build projects around motivated individuals. Give them the environment and
    support they need, and trust them to get the* *job done.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The most efficient and effective method of conveying information to and within
    a development team is* *face-to-face conversation.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Working software is the primary measure* *of progress.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Agile processes promote* *sustainable development.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [The Agile Manifesto](http://agilemanifesto.org/) ([http://agilemanifesto.org/](http://agilemanifesto.org/))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our processes support building stuff: that is how we learn. Even in a world
    of video conferencing, people operate better with face-to-face meetings. And with
    the rapid pace of AI changes, sustainability needs to be built into our life cycle
    to avoid burning people out. Organizations love to reduce risks. Reduce risk by
    following the process religiously until there is confidence that the unknowns
    are known and change is required. Risks create unknowns, especially in solutions
    such as an LLM, where what it will say at every turn is not predictable.'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid “unknown unknowns”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is so good it is worth repeating:'
  prefs: []
  type: TYPE_NORMAL
- en: “Reports that say that something hasn’t happened are always interesting to me,
    because as we know, there are known knowns; there are things we know we know.
    We also know there are known unknowns; that is to say, we know there are some
    things we do not know. But there are also unknown unknowns – the ones we don’t
    know we don’t know.”
  prefs: []
  type: TYPE_NORMAL
- en: – Donald Rumsfeld, US Secretary of Defense (February 12, 2002)
  prefs: []
  type: TYPE_NORMAL
- en: Dig into understanding and learning. Research, uncover the root causes of issues,
    and learn *how to learn* in a Generative AI world. See why design is so well positioned
    to uncover issues; design methods are intended to solve these issues. And always
    try to evolve and improve.
  prefs: []
  type: TYPE_NORMAL
- en: Always evolve and improve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrospectives are not to be missed. They allow a content team to learn and
    give feedback on engineering issues that cause concern. Evolve to use better metrics
    (value and effort, not bugs) to drive improvement (measure the right things).
    Some metrics discussed in [*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216),
    *Monitoring and Evaluation*, are confusing and still evolving, but we have learned
    to understand how metrics change and work with them as they grow. Design thinking
    is fundamentally about listening and learning, so designers should be best positioned
    to help a team develop and improve. And improving includes knowing how to define
    what needs to be done. This means having requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Agile does not mean “no requirements”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Specify what *others* need; don’t over-specify what *you* need (inter-module
    documentation, yes!). Agile is right:'
  prefs: []
  type: TYPE_NORMAL
- en: “Working software over comprehensive documentation… we value the items on the
    left more.”
  prefs: []
  type: TYPE_NORMAL
- en: This doesn’t mean you don’t create test cases; that is a requirement to take
    steps forward without going backward, but that might be all that is needed. Consider
    the goals and let data science and engineering work to meet those goals. If an
    analysis identifies that a system needs better context recall, define new realistic
    goals. No one works in isolation on these projects. Focus on what others need.
    It doesn’t preclude writing a test plan for user research. Do what is required,
    but if you know what to do, consider whether it needs to be documented. This brings
    us to discussing others and how to work with a team.
  prefs: []
  type: TYPE_NORMAL
- en: Team composition and location matters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Face-to-face conversation is the most efficient and effective method of conveying
    information to and within a development team. This is a big challenge for remote-only
    companies or any company these days. In the State of Agile survey, 91% of respondents
    said their teams were fully remote. However, some successful companies recognize
    a partial solution – they have teams whose members work in similar time zones.
    Having writers in Romania, QA in China, and PMs and designers in California would
    be wrong. Work to create content teams that can work together in real time to
    help them stay aligned on the work they have in progress.
  prefs: []
  type: TYPE_NORMAL
- en: Manage Work in Progress (WIP) and technical debt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is so much work and so little time. Even with Generative AI, only so much
    gets done. Focus resources on the most valuable items addressed in [*Chapter 4*](B21964_04.xhtml#_idTextAnchor085),
    *Scoring Stories*. Break up work into smaller pieces and deliver that value sooner.
    *Figure 11**.2* explains this with an analogy of how homes are built.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_11_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Do not juggle excessive work in progress (Photoshop AI)
  prefs: []
  type: TYPE_NORMAL
- en: It is straightforward – most people would rather be 100% done on 80% of the
    work than be 80% complete on 100% of the work. Work that is done has value. A
    realtor can sell four homes and then build on the next lot instead of having five
    homes that still are a work in progress and can’t be sold. This is why builders
    have phases. They build a set of homes (for efficiency), sell them, and then build
    the next set of homes. This reduces risk and increases value, and customers get
    the benefits sooner. It’s the same with content changes. Make incremental progress.
    This incremental progress usually means more extensive work is broken down into
    manageable pieces. *Figure 11**.3* jokingly represents organizing to create manageable,
    well-understood sets of efforts to address in order.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_11_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Break down large projects into manageable pieces (openart.ai)
  prefs: []
  type: TYPE_NORMAL
- en: The prioritization tools discussed and an Agile process (or any methodology
    with backlog grooming) help focus on priorities. Do work that is worth doing.
    This work should provide the most customer value.
  prefs: []
  type: TYPE_NORMAL
- en: Focus on customer value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There was an entire chapter on scoring user stories. By now, giving customers
    value by prioritizing the most essential work should be well understood. The more
    stories resonate with customers, the better their experience will be, and the
    more they will say good things about it (think Net Promoter Score), increasing
    users and usage. Then, invest to create even more value. One way to focus on customer
    value is to ensure the design process is part of the overall development process.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporate the design process into the dev process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are few enterprise companies with a design-first approach. Design-first
    means more than just doing the design first; everyone has to do some design *first*.
    It is more about leading with a design mentality to create customer success, not
    driving solutions because engineering thinks it is cool or marketing wants something
    flashy. It has to deliver a user need and drive value. Also, as mentioned earlier,
    it should be **functional, usable, necessary, and engaging** (**FUN-E** or *funny*,
    which is a good mnemonic). However, most product design people live in reality,
    so flexibility is required. The products built with Generative AI are not mature
    enough to survive on a three-to-six-month release schedule. Look for places to
    go faster and improve incrementally. In addition, designers, writers, linguists,
    and product managers will need to work with engineering, QA, and data science
    because many frameworks and processes need to be implemented. But do it with a
    design mindset so that new processes are also *FUN-E*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When incorporating GenAI or ChatGPT into processes, consider a content sprint
    focused on knowledge bases (documents and data) and prompt engineering improvements.
    This Agile life cycle can coincide with traditional **development** (**dev**)
    sprints. The concept of a content sprint will be introduced. Here are some suggestions
    for key individuals with responsibilities around designing Generative AI solutions
    in an Agile organization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Designers, writers,** **and linguists**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agile practices address design. However, most design efforts are done before
    entering a *dev* sprint. For example, user research won’t happen inside a one-week
    development sprint. Agile, Scaled Agile, and many frameworks have accommodations
    for the work needed to enter a sprint. The design team should have *design* sprints
    and processes to create the results so the content team or the dev team can do
    its job during their sprints. The concept of a content sprint is explained in
    the next section.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deliver the design when it is 90% ready before a team starts a dev sprint. Reserve
    content and prompt changes for the content sprint. Items that span content and
    development are placed in the development sprint backlog. This allows more technical
    changes to be thoroughly tested.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use GenAI tools to build GenAI products. Like the tools discussed for classification,
    synthetic data, and fine-tuning, there is an ever-increasing number of GenAI tools
    that can make work more efficient. GenAI tools also help with general practices
    for professionals, editing, brainstorming, thinking through problems, word choices,
    translation issues, and understanding components and interaction design best practices.
    Even a generic foundational model like ChatGPT 4o should be considered your intern
    for getting tasks done better.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Designers**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get more involved in prioritization (to focus end-to-end and ensure that customer-critical
    items are completed in priority order).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn to identify the differences between prompt issues, fine-tuning gaps, knowledge
    or RAG issues, integration data issues, or context issues.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Actively resolve in-sprint issues promptly (i.e., to be responsive and to allocate
    time to be in the sprint). Being responsive to the dev team encourages them to
    become dependent on product people. This is important because designers (writers,
    PMs, and even linguists) are better positioned to judge these content issues.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design owners**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When entering the dev sprint, freeze the UI design that is expected for that
    sprint. Post the document, move additional significant changes to the next version,
    and only deal with the remaining 10% of detailed design changes within the sprint.
    Give the dev team design goals that are not moving.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Content sprints have a schedule for changes; *when in doubt, throw it out*.
    Keep what is trusted to work, and move the rest to the next sprint. Content sprints
    are like working with live wires; don’t be shocked at the results.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Developers and** **data scientists**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t do it alone; this is a team sport.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn and listen to feedback; some changes have unintended consequences. As
    the team starts to get a feel for the content, the data, and the context size,
    prototype and experiment with other releases or different models, to improve quality.
    Rely on the content people, such as designers, PMs, and writers, who are better
    positioned to judge the appropriateness and significance of issues with language
    and conversational interaction.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scrum master**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invite the designer to participate in *all* sprint scrum activities with active
    UI work. It is better to have teams focused on only UI and content work.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Design-related tasks in dev sprints should be assigned daily to ensure a fast
    turnaround for UI review/feedback/in-sprint design changes. No one wants surprises
    during the review at the end of the sprint.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product owners**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have the requirements handed to the designer before starting work. Work with
    designers during design sprints to deliver the “end-to-end” story for later dev
    sprints. Some changes require testing and research and will not fit into the same
    sprint as the dev. Scaled Agile is okay with this.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the runway clean and in sync with the expected design needs so that the
    design can be provided before the dev sprint starts. Even content sprint teams
    might need infrastructure or want to try new models, and this would fall back
    on the development sprints to provide.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale and scope stories correctly. Break up epics into smaller epics, stories,
    and tasks. Adopting a new LLM, for example, will span multiple phases and sprints
    to test, fine-tune, and validate.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Get content experts involved sooner rather than later. Before entering the sprint,
    agree on the language issues, support for product slang, outdated terms for products,
    discontinued product names, and synonyms for product names. Build this dictionary
    of terms to go beyond official terms. Remember, customers will use the language
    they are comfortable with.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scrum masters/design owners**: Any designer can be on up to three teams,
    typically two. Designers should “mostly” work ahead of dev sprints. In the dev
    track sprints, effort should be limited since a story should enter a sprint at
    least 90% ready. Content designers won’t have that luxury, as we are about to
    explain. They might have a 10% to 20% backlog from the week before, but this week’s
    work will be figured out *during* the sprint, and almost all the work will be
    done within the content update sprint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, it’s time to explain the a content improvement life cycle and a content
    sprint.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a content improvement life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031), *Conducting Effective
    User Research*, discussed extensively monitoring log files to improve the care
    and feeding life cycle. I can’t emphasize strongly enough how important it is
    to understand the metrics associated with the solution’s performance and adapt
    the solution to improve it. Although the data might differ for chat solutions
    versus recommender UIs, the process is the same for both. For behind-the-scenes
    work, figure out the feedback needed so that a continuous improvement process
    can be built.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a vision of the content design team’s week once the product
    is in production. *Figure 11**.4* shows what a production content improvement
    team might do. Call the team a *production content team* or a *content/prompt
    sprint team*. Find a name that works so it is clear this is a slightly different
    beast from the more robust and complex *dev* sprint team.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_11_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – A weekly cadence for a content team’s continuous improvement lifecycle
  prefs: []
  type: TYPE_NORMAL
- en: This life cycle may not be weekly early in the development phase; it could be
    a two- or three-week cadence. A week is excellent for an immature production system.
    It takes time to achieve this cadence. If an organization uses a two-week Agile
    development life cycle, start by matching that and improve the cadence over time.
    Alternatively, consider returning to a two-week approach when the product is more
    mature and changes slowly. It is not written in stone that this process needs
    to match the development cadence. There are some advantages, but it is worth considering
    the tradeoffs. This might be the place to break that rule of thumb.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now describe in detail the typical work week for a conversational analysis
    team. This can also work for recommender solutions, even without the same inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs for conversational AIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logs are the primary input source for analysis but can be supplemented by surveys,
    bugs, or feedback. Since the logs represent what did happen, I would always check
    the logs against any anecdotal feedback. A friend told me the other day that the
    most significant missing feature from Google Sheets was the ability to drag and
    drop rows. And I said, “You can drag and drop columns, so why can’t you drag and
    drop rows?” We went to Google Sheets, and drag and drop worked fine for rows,
    even on the mobile version. Unsurprisingly, this is an affordance issue, as discussed
    in the heuristic evaluation discussion in [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190),
    *Guidelines and Heuristics*. It is hard to know what can or can’t be done when
    there is no clear visual indicator of its support. Always check what people report.
    It is easy to be annoyed by something and only be confused by its real cause.
    Root-cause analysis can uncover the cause. Be willing to dig into the valid reason
    for a problem. Our user-centered design methods are based on the concepts of **root-cause
    analysis**. This book follows this approach (define the problem, gather data,
    identify possible causes, develop and deploy solutions, and then monitor and verify).
    You can apply this to any AI solution, including recommender UIs.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs for recommender UIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recommender UIs don’t have log data like our conversational cousins. Earlier
    chapters covered feedback methods, but downstream metrics can proxy for usage
    data. Access and monitor this data to judge value and quality. For example, suppose
    a recommendation engine suggests call follow-ups, email announcements, or product
    discounting. Analytics from the phone system, outbound email logs, or sales discount
    tools can be correlated with recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: In places where direct correlations cannot be created, secondary methods, such
    as user feedback, can be used to determine a user’s perception and gather written
    or verbal commentary. At least there is some data. Backend AIs are less lucky.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs for backend AIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since there is no UI, consider this a more challenging task to gather feedback.
    There are two suggestions to consider. The first would be to build feedback mechanisms
    pointing to issues in the results. The customer will need to find out where the
    problem is. They will see something they don’t like, feel it is a bit off, or
    identify it as wrong. Let them do that, then determine the root cause. However,
    it is possible to locate the issue if enough data is gathered with the feedback
    (such as their usage history, tag-specific datasets, or screenshots they would
    see).
  prefs: []
  type: TYPE_NORMAL
- en: The second relates to working out an AI monitoring system. Using ground truth
    examples, a secondary AI (not the same engine used for the solution) can check
    the work of the primary system. It might find results that don’t match what is
    expected or appear more than an order of magnitude outside of what is typical.
    This feeds into a continuous improvement system that starts on the first day of
    the week. The same foundational model could be used if fine-tuned for specific
    criteria, such as the checkpoint testing cycle in *Figure 11**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_11_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – A checkpoint system for validating results to share
  prefs: []
  type: TYPE_NORMAL
- en: This example allows us to fine-tune checkpoint models to pass judgments for
    specific criteria. It supports direct user input processes, or for a back-end
    solution, it would be a prompt generated on the customer’s behalf.
  prefs: []
  type: TYPE_NORMAL
- en: Did the answer pass an ethical test (**Judgment 1**)? Do the facts match the
    product discussed (**Judgment 2**)? And in **Judgement 3**, does that answer fall
    within the company’s capabilities? If the results are promising, pass it on to
    the customer. If the answer is not good, gather more context or have them ask
    questions differently. In any case, the results of each of these models support
    the primary model. Each of those failures creates an opportunity to learn. The
    output from models is part of what goes into the process for improvement each
    week. The idea is to work toward doing all of this on a weekly cadence. Let’s
    look at the work needed on the first day of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Monday
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031)*, Conducting Effective
    User Research*, covers log analysis. This is a valuable way of learning what is
    happening with customers. Manual tools were provided to start, and methods were
    shared to prioritize the outcomes. Focus on the analysis on Monday or whatever
    day of the week is the start.'
  prefs: []
  type: TYPE_NORMAL
- en: Is Monday the first day of the week?
  prefs: []
  type: TYPE_NORMAL
- en: Maybe your country starts on Sunday. However, this won’t give you the alliteration
    of *Monitoring Monday*. This alliteration only works in some languages; Israel,
    Japan, and Saudi Arabia can start with *Surveying Sunday*. Come up with names
    that work in your language. I used the translation spreadsheet from the earlier
    chapter to translate Monitoring Mondays. Only one of the 30 languages happens
    even to be close to alliteration. If you are a word geek, it was Dutch, with the
    translation of “Maandag monitoren.” Consider making it fun and not feel like a
    structured march that people will oppose (a typical Agile complaint).
  prefs: []
  type: TYPE_NORMAL
- en: There is only so much a team can do to review logs, also look at customer surveys,
    feedback messages, bug reports, failures from intermediate steps of multi-step
    validation models, and sales or service feedback. Filter for the issues to triage
    them over the next few days. Use this time to work on projects that carried over
    from last week.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis Tuesday (and Wednesday’s workup)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understand, classify, and chunk results. For extensive collections of common
    issues, don’t write 20 stories; write one. This is where classification methods
    and third-party tools come into play, as discussed in [*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031),
    *Conducting Effective User Research,* and [*Chapter 3*](B21964_03.xhtml#_idTextAnchor058),
    *Identifying Optimal Use Cases for ChatGPT*. Work with the dev team to understand
    and segment issues into those that can be fixed this week and those that need
    devs’ help for improvements. Of course, investing in tools to make improvements
    is better without investing in development resources at every turn. This is the
    adage – *if you give someone a fish, you feed them for a day. If you teach them
    to fish, you feed them for a lifetime*. Dev can teach us to fish by providing
    tools that allow us to make improvements. Solutions like editing knowledge, prompt
    changes, fine-tuning examples, adding test cases, or other content changes can
    be considered through analysis. The analysis will also create work for researchers
    and ample items for dev teams to address around fine-tuning, new models, better
    reflection or model-to-model integrations, and API work. All this is done to decide
    what the content team can deliver in the next few days and plan for what the dev
    team can provide in future sprints.
  prefs: []
  type: TYPE_NORMAL
- en: Treatment Thursday and fault-finding Friday
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I would love to have these on different days, but figuring out solutions and
    testing to identify faults is an iterative process. Some readers will be laughing
    at the attempt to do all of this in a week. I get it; it is the goal, work towards
    it. Fixing and tests should be done over two days. Conflicts will be expected.
    Unsurprisingly, asking a model to do one thing can break something else, or two
    new solutions can conflict. And prompts can then get unwieldy. All hands are on
    deck for solutions: writing or editing knowledge, updating prompts, improving
    fine-tuning, or incorporating a new API (and as discussed, this is likely to come
    from a previous sprint, as APIs don’t magically appear). Automation is needed
    to quickly rerun test cases and analyze results and metrics for this to work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Only some things can be fixed in days or hours, but some improvements are well
    within reason. Rapid improvements can happen for *content*:'
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting instructions and prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training to improve grammar or to include company-specific language and initialisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Editing knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating knowledge (it’s hard to develop technical documentation quickly, but
    maybe FAQs are within reason)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding learning examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying context elements to include in instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding test cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be a challenge. The content team can’t make one change, test it and
    then make more changes when a testing run can take minutes or hours. It is also
    hard to debug issues with a batch of new changes. It is a trade-off. Sometimes,
    failed tests can prevent releasing a solution, while some solutions fail to materialize
    and require more or different work. Work can carry over to next week or need to
    be referred to the dev team. Let’s explore carry-over work in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: What doesn’t fit into a week is still important
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “If I had one hour to save the world, I would spend 55 minutes defining the
    problem and only five minutes finding the solution.”
  prefs: []
  type: TYPE_NORMAL
- en: –Albert Einstein (not really)
  prefs: []
  type: TYPE_NORMAL
- en: The “famous” quote from Albert Einstein isn’t from him (Google knows!), but
    the sentiment is still valid. Not all solutions are easy. However, understanding
    the solution is generally manageable if time is spent understanding the problems.
  prefs: []
  type: TYPE_NORMAL
- en: When identifying problems that clearly won’t fit into a week or a sprint, evaluating
    their value is still important, as is getting a cost estimate and moving it into
    a backlog based on the Weighted Shortest Job First approach. By doing this on
    an ongoing basis, there will be stories from previous weeks (or months) now being
    integrated into this week’s changes. Thus, these changes will impact the solution
    as tests are built. In true Agile fashion, if something is too big and disruptive
    to the process, it should not be picked up alongside other changes; use the traditional
    development and testing lifecycle for the more significant, more technically complex
    stories.
  prefs: []
  type: TYPE_NORMAL
- en: For example, new test cases will be needed as a new API or integration becomes
    available. Because these former out-of-domain cases become in-domain solutions,
    this can cause new conflicts. For example, in an internal business solution that
    supports sales, inventory, and team staffing reports, new APIs that support marketing
    reports need to work with existing requests for other *reports*. Add training
    data to assist the system in differentiating these new marketing reports, rerun
    fine-tuning, update prompts, and refine FAQs to account for this feature. The
    previous out-of-domain test cases must move to valid in-domain test cases.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.6* shows adapting content sprint cadence based on the product’s
    stage. This is offered to encourage thinking about the level of change at each
    phase of product development and the value of that change.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_11_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Cadence can change based on the stage of the product
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that the content sprint schedule adapts over time. This is a radical
    idea for Agile, but it might provide some value to consider. Since a team is self-organizing,
    they can make this schedule decision. Most enterprises do not believe a team can
    make this call, as they think self-organizing is a no-no. But there should be
    some flexibility. This isn’t suggesting changing often, just as the process matures.
    That is the Agile way.
  prefs: []
  type: TYPE_NORMAL
- en: Notice a generic *quality* metric for the vertical axis. This could be any measure
    or all measures. Any specific metric can see dips when building and testing. In
    [*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216), *Monitoring and Evaluation*,
    not all of OpenAI’s efforts to improve their enterprise use case were successful.
    Remember that even when a product is in production, efforts in dev will eventually
    come online and give opportunities for more significant improvements in quality
    than just what can be done from incremental improvements from a content perspective.
    Only so much can be done by editing prompts, improving tuning, adding new APIs,
    and working with our knowledge integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down each phase and the approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Early development**: There are no checks and balances at this stage; try
    to test changes regularly. Testing frameworks and metrics will still be in development
    and need to be available to ensure progress (try not to take one step forward
    and two steps back). Manual testing will be typical. Significant changes will
    come from major investments, and it is okay if the sprint cycle for content doesn’t
    reflect development. Writing content, editing and reviewing updates, and bringing
    testing online takes time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Later development**: As releases approach, changes will likely tighten and
    be on the same schedule as dev Agile teams. Two weeks are typical; some teams
    will be already on a one-week cadence. However, be aware that the Agile sprint
    length doesn’t always equate to a release schedule, which is okay. Only when some
    of these significant investments pay off will there be major jumps in quality.
    **Minimum Viable Product** (**MVP**) expectations are shown on the preceding chart
    at **80%**. An astute reader will notice we talked about 97% in other graphs.
    97% is the *goal*; 80% is a release minimum. It would be prudent to show some
    repeated ability to maintain that level before going live. Some changes will hurt
    some quality measures. Significant changes in the product scope could reduce the
    quality of each interaction but increase the number of interactions that can be
    handled; learn to live with these bumps in the road.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early production**: The goal is a complete update cycle in one week. By categorizing
    changes, there can be opportunities to make faster changes with some adjustments,
    while more significant changes, such as access to a new API, take longer. Customers
    need to feel improvements. I use ChatGPT all day and see improvements monthly.
    One week is possible for some changes. Do what is best, but always with the focus
    on providing value to customers sooner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mature production**: As the solution matures, less value will be gained by
    incremental improvements. It might take the same effort to raise quality 10% from
    80% to 90% as it does to go up 1% from 95% to 96%. This diminishing return is
    an indication to reconsider how time is spent. The cadence of changes can decrease
    because the value of those changes is small. It may be time to reduce the story
    points the team can do in a content team and allocate those resources to new opportunities
    or the next-generation solution. Of course, this might be years, so cross that
    bridge when you come to it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No one is perfect, and we don’t need perfection. The cost of aiming for perfection
    can drive you out of business. Recall the failed Ford Pinto example from [*Chapter
    6*](B21964_06_split_000.xhtml#_idTextAnchor134)*, Gathering Data—Content is King*.
    It would be best to make choices; don’t make bad choices. Use guardrails for high-value,
    high-risk answers. Value ethics, reduction in bias, and accuracy over feature
    expansion.
  prefs: []
  type: TYPE_NORMAL
- en: “Perfection is not attainable. But if we chase perfection, we can catch excellence.”
  prefs: []
  type: TYPE_NORMAL
- en: – Vince Lombardi
  prefs: []
  type: TYPE_NORMAL
- en: As new models emerge, quality changes significantly, so consider how to re-adapt
    to a faster cadence. It is not uncommon to see quality hiccups as a function of
    new models, as with ChatGPT releases in the last two years, it is primarily a
    solid upward trajectory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway is to implement practices to improve quality quickly, keeping
    customers on the happy path. It is a labor of love to monitor regularly, gather
    feedback, learn about issues, classify to bundle improvements, research issues,
    test enhancements, and to start the process over again. Here are a few tips to
    overcome typical challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring log files can be monotonous, so time-box the work. Reviewing 1,000
    rows of logs is time-consuming and tedious. No one should do this every day, so
    do it for only a day. Alternatively, assign review and summarization to an LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over time, monitoring will get faster. It is much faster (10x) to review 1,000
    log files with 50 issues than with 200 issues since a human can quickly review
    conversations without issues but needs time to understand a single problem. ChatGPT
    models can also do some of this work, such as seeing what is available for automation,
    but do some monitoring manually to understand customers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixing only some things in one content iteration or sprint is okay. Some items
    are not fixed by prompt engineering, editing knowledge, or creating new content.
    Sometimes, new APIs are needed; in some organizations, they can take a long time,
    if ever. Classifying issues helps because if multiple instances of the same problem
    appear, it is possible to advocate to address them. Put in placeholders to acknowledge
    a customer’s needs. A placeholder may include a link to a third-party resource
    or an explanation if nothing else is available. Good AI classifiers are available;
    a model can be created to classify issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enlist resources. Designers, analysts, content authors, product managers, engineers,
    quality assurance, and data scientists can understand and diagnose issues. Use
    these resources collaboratively to learn how to address a situation best. There
    could be more than one approach to something. Communicate, collaborate, and conclude
    based on the information gathered. Also, there can be more than one solution.
    A short-term solution with prompt engineering might be better addressed by improving
    the knowledge base in the long term. Implement one solution while waiting for
    the latter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release management won’t want to make changes quickly. This is a tough one.
    Work within a system or find advocates to change it. It would be best to have
    a sponsor. There are different kinds of changes; work with a sponsor on the types
    of changes release management would approve. Build trust and expand this list
    over time. Give examples. If an online news organization had an article that said,
    “*Each new plane for the military will cost 200 million dollars,*” but it was
    20 million dollars, no one would expect that *content* change to take a week to
    fix. And much of what we are working with is content. Consider classifying changes
    so that some refinements can be done weekly while others require more effort,
    testing, and time. All should be run through automated and manual testing as part
    of the process, so maybe that is enough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garbage in, garbage out. Don’t let lousy knowledge throw off the RAG model.
    Don’t feed the model only with generative data; use that sparingly if it can be
    helped. Inject new, high-value content and look for opportunities for improvement.
    No one will suggest editing or improving all articles every week; pick the right
    battles. Let data lead the charge. The articles with the most hits take priority.
    Use analytics to tailor editorial investment. Recognize and test for interactions
    between similar documents for different products. Look for places where databases
    have good contextual data that can add value to prompts, giving customers more
    customized and accurate experiences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invest time in the process of handling supervised fine-tuning. With technical
    material, add well-known samples to the training. When inspecting logs, recognize
    that some human variety in interaction should be better understood. Consider whether
    FAQs already serve this purpose. Remember how prompts should look for these examples.
    A wealth of fine-tuning datasets are excellent examples of scope and scale, even
    if generic data is not of value to enterprise use cases. Here is one good example,
    with 200,000 robust prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [Ultrachat 200,000-row data set](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)
    ([https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Build custom collections of tuned material, test cases, and templates to provide
    high-quality RAG results. The Generative AI revolution is not without cost. Over
    time, model costs will be less significant than overall life cycle costs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let’s conclude with some key takeaways from the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Without continual growth and progress, such words as improvement, achievement,
    and success have no meaning.”
  prefs: []
  type: TYPE_NORMAL
- en: – Benjamin Franklin
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to conversational solutions, no truer words were spoken. Software
    changes decades ago were measured in years and then, months or quarters, and now,
    weeks and days. This is expected, needed, and sometimes valuable. With processes
    to support continuous improvement and even delivery, it is a win-win. Tools and
    methods will improve quickly, allowing more time to tackle new projects, while
    customers will benefit from better solutions sooner.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, we have explored the entire life cycle of applying UX design
    thinking and processes to create exceptional enterprise ChatGPT solutions. Sometimes,
    we went too deep and left you with some homework. The technology is moving so
    fast; the hope is that 96% of the concepts discussed in this book will apply to
    any conversational AI platform or collection of tools. Models improve, and the
    tools become more robust. The methods and practices should still work until we
    to the mythical general-purpose artificial intelligence (GPAI), strong AI, or
    artificial general intelligence (AGI). This book will be relevant until an LLM
    doesn’t need a significant investment in creating and tuning, with a healthy dose
    of care and feeding. As such, we will wrap up our journey in the next chapter
    to send you on your way to conquer the next AI project.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| ![](img/Image98994.jpg) | The links, book recommendations, and GitHub files
    in this chapter are posted on the reference page.Web page: [Chapter 11 References](https://uxdforai.com/references#C11)
    ([https://uxdforai.com/references#C11](https://uxdforai.com/references#C11)) |'
  prefs: []
  type: TYPE_TB
