- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enhancing NLP Tasks Using LLMs with spacy-llm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build upon the knowledge gained in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087)
    , and explore how to integrate **large language models** ( **LLMs** ) into spaCy
    pipelines using the **spacy-llm** library. We will start by understanding the
    basics of LLMs and prompt engineering, and how these powerful models can be leveraged
    to perform a wide range of NLP tasks within spaCy. We’ll demonstrate how to configure
    and use pre-built LLM tasks such as text summarization, and then take a step further
    by creating a custom task to extract contextual information from text. This will
    involve using Jinja templates for prompt creation and writing custom spaCy components
    that can efficiently handle complex NLP tasks. By the end of this chapter, you
    will have a deeper understanding of how to enhance traditional NLP pipelines with
    the flexibility and power of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs and prompt engineering basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization with LLMs and spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating custom LLM tasks with Jinja template
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be working with spaCy and the **spacy-llm** library
    to create and run our pipelines. You can find the code used in this chapter at
    [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  prefs: []
  type: TYPE_NORMAL
- en: LLMs and prompt engineering basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) , **language modeling**
    is the task of predicting the next token given the sequence of previous tokens.
    The example we used was that given the sequence of words **Yesterday I visited
    a** , a language model can predict the next token to be something such as **church**
    , **hospital** , **school** , and so on. Conventional language models are usually
    trained in a supervised manner to perform a specific task. **Pre-trained language
    models** ( **PLM** ) are trained in a self-supervised manner, with the aim of
    learning a generic representation of the language. These PLM models are then fine-tuned
    to perform a specific downstream task. This self-supervised pre-training made
    PLM models much more powerful than regular language models.
  prefs: []
  type: TYPE_NORMAL
- en: The LLMs are an evolution of PLMs that have many more model parameters and larger
    training datasets. The GPT-3 model, for example, has 175B parameters. Its successor,
    GPT3.5, was the base for the ChatGPT model released in November 2022. LLMs can
    serve as general-purpose tools, capable of tasks from language translation to
    coding assistance. Their ability to understand and generate human-like text has
    led to impactful applications in medicine, education, science, math, law, and
    more. In medicine, LLMs support doctors with evidence-based recommendations and
    enhance patient interactions. In education, they customize learning experiences
    and assist teachers in creating content. In science, LLMs speed up research and
    scientific writing. In law, they analyze legal documents and clarify complex terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use LLMs for regular NLP tasks such as **Named Entity Recognition**
    ( **NER** ), text categorization, and text summarization. Basically, these models
    can do almost anything we ask them to. But this doesn’t come for free since training
    them requires extensive computational resources, and the large number of layers
    and parameters make them produce answers much more slowly than non-LLM models.
    LLMs can also **hallucinate** : produce responses that at first seem plausible
    but are in fact incorrect or not aligned with the facts or context. This phenomenon
    occurs because the models generate text based on patterns learned from their training
    data, rather than verifying information against an external source. As a result,
    they might create statements that sound reasonable but are misleading, inaccurate,
    or entirely fictional. Given all that, LLMs are useful but we should always analyze
    if they are the best solution to the project at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To interact with LLMs, we use prompts. **Prompts** should guide the models
    to generate answers or make the model take action. Prompts usually have these
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instruction** : The task you want the model to execute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context** : External information or additional context that should be useful
    to produce better answers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input data** : The input/question we want to be answered'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output indicator** : The type of format we want to see as the model output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With **spacy-llm** , we define prompts as tasks. When building spaCy pipelines
    with LLMs, each LLM component is defined using a **task** and a **model** . The
    **task** defines the prompt and functionality to parse the resulting response.
    The **model** defines the LLM model and how to connect to it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know what LLMs are and how to interact with them, let’s use a **spacy-llm**
    component in a pipeline. In the next section, we’re going to create a pipeline
    to summarize texts using an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization with LLMs and spacy-llm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each **spacy-llm** component has a task definition. spaCy has some pre-defined
    tasks, and we can also create own tasks. In this section, we’re going to use the
    **spacy.Summarization.v1** task. Each task is defined using a prompt. Here is
    the prompt for this task, available at [https://github.com/explosion/spacy-llm/blob/main/spacy_llm/tasks/templates/summarization.v1.jinja](https://github.com/explosion/spacy-llm/blob/main/spacy_llm/tasks/templates/summarization.v1.jinja)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**spacy-llm** uses **Jinja** templates to define the instructions and examples.
    Jinja uses placeholders to insert data dynamically in the templates. The most
    common placeholders are **{{ }}** , **{% %}** , and **{# #}** . **{{ }}** is used
    to add variables or expressions, **{% %}** is used with flow control statements,
    and **{# #}** is used to add comments. Let’s see how some of these placeholders
    are used in the **spacy.Summarization.v1** template.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can ask the model to output a summary with a certain maximum number of words.
    The default value for **max_n_words** is **null** . If this parameter is set in
    our config, the template will include this number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Few-shot prompting** is a technique that consists of providing some examples
    (usually 1 to 5) of desirable inputs and outputs to show the model how we want
    the results. These examples help the model better understand the patterns without
    the need for fine-tuning with lots of examples. The **spacy.Summarization.v1**
    task has an **examples** parameter to generate few-shot examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how the summarization template task works it’s time to work
    on the other element of **spacy-llm** , the **model** . We’r e going to use Anthropic’s
    **Claude 2** model. To make sure the credentials to connect to this model are
    available, you can open the console on your computer and run **export ANTHROPIC_API_KEY="..."**
    . Now let’s install the package with the **python3 -m pip install spacy-llm==0.7.2**
    command. We will load the pipeline using a **config.cfg** file (if you need a
    refresher, you can go back to the *Working with spaCy config.cfg files* section
    in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) ). Let’s build the configuration
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll define the **nlp** section, where we should define the language
    and the components of our pipeline. We’re only using the **llm** component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it’s time to specify the **components** section. To initialize the **llm**
    component, we’ll write **factory = "llm"** ; then, we will specify the task and
    the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To load this pipeline, we’ll pass the path to this config file to the **assemble**
    method from **spacy_llm.util** . Let’s ask the model to summarize the *LLMs and
    prompt engineering basics* section of this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **spacy.Summarization.v1** task adds the summary to the **._.summary**
    extension attribute by default. Here is the response from the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Nice, right? You can check the other parameters to customize the task here [https://spacy.io/api/large-language-models#summarization-v1](https://spacy.io/api/large-language-models#summarization-v1)
    . Some of the other available **spacy-llm** tasks include **spacy.EntityLinker.v1**
    , **spacy.NER.v3** , **spacy.SpanCat.v3** , **spacy.TextCat.v3** , and **spacy.Sentiment.v1**
    . But beyond using these pre-built tasks, you can also create your own, which
    not only enhances the power of **spacy-llm** but also provides an organized way
    to follow best practices when building NLP pipelines. Let’s learn how to do that
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom spacy-llm tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’re going to create a task where, given a quote from [https://dummyjson.com/docs/quotes](https://dummyjson.com/docs/quotes)
    , the model should provide the context of the quote. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step of creating a custom **spacy-llm** task is to create the prompt
    and save it as a Jinja template. Here is the template for this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll save this in a file named **templates/quote_context_extract.jinja** .
    The next step is to create the class for the task. This class should implement
    two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**generate_prompts(docs: Iterable[Doc]) -> Iterable[str]** : This function
    converts a list of spaCy **Doc** objects into a list of prompts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parse_responses(docs: Iterable[Doc], responses: Iterable[str]) -> Iterable[Doc]**
    : This function parses LLM outputs into a spaCy **Doc** object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **generate_prompts()** method will use our Jinja template and **parse_responses()**
    method will receive the model’s response and add the context extension attribution
    to our **Doc** . Let’s create the **QuoteContextExtractTask** class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import all the methods we’ll need and set the directory for the templates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create a method to read the text of the Jinja template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can start to create the **QuoteContextExtractTask** class. Let’s
    start creating the **__init__()** method. The class should be initiated with the
    name of the Jinja template and a field string to set the name of the extension
    attribute the task will add to the **Doc** object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will create a method to build the prompt. Jinja uses an **Environment**
    object to load the templates from the file. We will use the **from_string()**
    method to build the template from text and yield it. Each time this method runs
    internally, it will render the template replacing the **{{text}}** variable of
    the template with the value of **doc.text** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now write the last method of the class. The **parse_responses()** method
    will add the response of the model to the **Doc** object. First, we create a helper
    method to add the extension attribute if it doesn’t exist. To set the extension
    attribute, we will use Python’s **setattr ()** method so we can dynamically set
    the attribute using the class field variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To use this class in our **config.cfg** file, we need to add the task to the
    **spacy-llm** **llm_tasks** register:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And we’re done! Save this to a **quote.py** class. Here is the full code that
    should be in this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s test our custom task by first creating the **config_custom_task.cfg**
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can assemble the **nlp** object using this file and print out the
    context for a quote. Don’t forget to import the **QuoteContextExtractTask** from
    **quote.py** so spaCy knows where to load this from:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, you’ve created a custom **spacy-llm** task to extract the context
    from a given quote. This approach not only allows you to craft highly specific
    NLP tasks tailored to your needs but also provides a structured way to integrate
    best practices in software engineering, such as modularity and reusability, into
    your NLP pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to leverage **large language models** ( **LLMs**
    ) within spaCy pipelines using the **spacy-llm** library. We reviewed the basics
    of LLMs and prompt engineering, emphasizing their role as versatile tools capable
    of performing a wide range of NLP tasks, from text categorization to summarization.
    However, we also noted the limitations of LLMs, such as their high computational
    cost and the tendency to hallucinate. We then demonstrated how to integrate LLMs
    into spaCy pipelines by defining tasks and models. Specifically, we implemented
    a summarization task and, subsequently, created a custom task to extract the context
    from a quote. This process involved creating Jinja templates for prompts and defining
    methods for generating and parsing responses.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going back to more traditional machine learning and
    learn how to annotate data and train a pipeline component from scratch using spaCy.
  prefs: []
  type: TYPE_NORMAL
