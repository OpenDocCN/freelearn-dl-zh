- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: RAG Feature Pipeline
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG功能管道
- en: '**Retrieval-augmented generation** (**RAG**) is fundamental in most generative
    AI applications. RAG’s core responsibility is to inject custom data into the **large
    language model** (**LLM**) to perform a given action (e.g., summarize, reformulate,
    and extract the injected data). You often want to use the LLM on data it wasn’t
    trained on (e.g., private or new data). As fine-tuning an LLM is a highly costly
    operation, RAG is a compelling strategy that bypasses the need for constant fine-tuning
    to access that new data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索增强生成**（**RAG**）在大多数生成式AI应用中是基本的。RAG的核心责任是将自定义数据注入**大型语言模型**（**LLM**）以执行给定的操作（例如，总结、重新表述和提取注入的数据）。您通常希望将LLM应用于它未训练过的数据（例如，私有或新数据）。由于微调LLM是一项高度昂贵的操作，因此RAG是一种有吸引力的策略，可以绕过访问新数据时需要不断微调的需求。'
- en: 'We will start this chapter with a theoretical part that focuses on the fundamentals
    of RAG and how it works. We will then walk you through all the components of a
    naïve RAG system: chunking, embedding, and vector DBs. Ultimately, we will present
    various optimizations used for an advanced RAG system. Then, we will continue
    exploring LLM Twin’s RAG feature pipeline architecture. At this step, we will
    apply all the theoretical aspects we discussed at the beginning of the chapter.
    Finally, we will go through a practical example by implementing the LLM Twin’s
    RAG feature pipeline based on the system design described throughout the book.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节的理论部分开始，重点关注RAG的基本原理及其工作方式。然后，我们将向您介绍一个天真RAG系统的所有组件：分块、嵌入和向量数据库。最终，我们将展示用于高级RAG系统的各种优化。然后，我们将继续探索LLM双胞胎的RAG功能管道架构。在这一步，我们将应用我们在章节开头讨论的所有理论方面。最后，我们将通过实现基于全书所述的系统设计的LLM双胞胎的RAG功能管道来通过一个实际例子。
- en: 'The main sections of this chapter are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要部分包括：
- en: Understanding RAG
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解RAG
- en: An overview of advanced RAG
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级RAG概述
- en: Exploring the LLM Twin’s RAG feature pipeline architecture
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索LLM双胞胎的RAG功能管道架构
- en: Implementing the LLM Twin’s RAG feature pipeline
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现LLM双胞胎的RAG功能管道
- en: By the end of this chapter, you will have a clear and comprehensive understanding
    of what RAG is and how it is applied to our LLM Twin use case.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将清楚地全面了解RAG是什么以及它是如何应用于我们的LLM双胞胎用例的。
- en: Understanding RAG
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解RAG
- en: 'RAG enhances the accuracy and reliability of generative AI models with information
    fetched from external sources. It is a technique complementary to the internal
    knowledge of the LLMs. Before going into the details, let’s understand what RAG
    stands for:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RAG通过从外部来源获取的信息增强了生成式AI模型的准确性和可靠性。它是一种与LLM内部知识互补的技术。在深入细节之前，让我们了解RAG代表什么：
- en: '**Retrieval**: Search for relevant data'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**：搜索相关数据'
- en: '**Augmented**: Add the data as context to the prompt'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强**：将数据作为上下文添加到提示中'
- en: '**Generation**: Use the augmented prompt with an LLM for generation'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**：使用增强的提示和LLM进行生成'
- en: Any LLM is bound to understand the data it was trained on, sometimes called
    parameterized knowledge. Thus, even if the LLM can perfectly answer what happened
    in the past, it won’t have access to the newest data or any other external sources
    on which it wasn’t trained.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 任何大型语言模型（LLM）都必然理解其训练的数据，这有时被称为参数化知识。因此，即使LLM可以完美地回答过去发生的事情，它也无法访问最新数据或任何其他它未训练过的外部来源。
- en: Let’s take the most powerful model from OpenAI as an example, which, in the
    summer of 2024, is GPT-4o. The model is trained on data up to October 2023\. Thus,
    if we ask what happened during the 2020 pandemic, it can be answered perfectly
    due to its parametrized knowledge. However, it will not know the answer if we
    ask about the 2024 European Football Championship results due to its bounded parametrized
    knowledge. Another scenario is that it will start confidently hallucinating and
    provide a faulty answer.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以OpenAI最强大的模型为例，2024年夏季是GPT-4o。该模型在截至2023年10月的数据上进行了训练。因此，如果我们询问2020年大流行期间发生了什么，它可以完美回答，因为它具有参数化知识。然而，如果我们询问2024年欧洲足球锦标赛的结果，由于其有限的参数化知识，它将不知道答案。另一个场景是，它将自信地产生幻觉并提供错误的答案。
- en: RAG overcomes these two limitations of LLMs. It provides access to external
    or latest data and prevents hallucinations, enhancing generative AI models’ accuracy
    and reliability.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RAG克服了LLM的这两个限制。它提供了访问外部或最新数据的能力，并防止了幻觉，增强了生成式AI模型的准确性和可靠性。
- en: Why use RAG?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用RAG？
- en: We briefly explained the importance of using RAG in generative AI applications
    earlier. Now, we will dig deeper into the “why,” following which we will focus
    on what a naïve RAG framework looks like.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前简要解释了在生成式AI应用中使用RAG的重要性。现在，我们将深入探讨“为什么”，之后我们将关注一个简单的RAG框架是什么样的。
- en: For now, to get an intuition about RAG, you have to know that when using RAG,
    we inject the necessary information into the prompt to answer the initial user
    question. After that, we pass the augmented prompt to the LLM for the final answer.
    Now, the LLM will use the additional context to answer the user question.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，为了对RAG有一个直观的了解，你必须知道在使用RAG时，我们将必要的信息注入提示中，以回答初始用户的问题。之后，我们将增强后的提示传递给LLM以获得最终答案。现在，LLM将使用额外的上下文来回答用户的问题。
- en: 'There are two fundamental problems that RAG solves:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RAG解决了两个基本问题：
- en: Hallucinations
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉
- en: Old or private information
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过时或私有信息
- en: Hallucinations
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 幻觉
- en: 'If a chatbot without RAG is asked a question about something it wasn’t trained
    on, there is a high chance that it will give you a confident answer about something
    that isn’t true. Let’s take the 2024 European Football Championship as an example.
    If the model is trained up to October 2023 and we ask it something about the tournament,
    it will most likely come up with a random answer that is hard to differentiate
    between reality and truth. Even if the LLM doesn’t hallucinate all the time, it
    raises concerns about the trustworthiness of its answers. Thus, we must ask ourselves:
    “When can we trust the LLM’s answers?” and “How can we evaluate if the answers
    are correct?”.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个没有RAG的聊天机器人被问及它没有训练过的某个问题，它有很大可能会给出一个关于不真实事物的自信回答。以2024年欧洲足球锦标赛为例。如果模型训练到2023年10月，我们问它关于锦标赛的问题，它很可能会给出一个难以区分现实与真相的随机答案。即使LLM并不总是产生幻觉，这也引发了对其答案可信度的担忧。因此，我们必须问自己：“我们何时可以信任LLM的答案？”以及“我们如何评估答案是否正确？”。
- en: By introducing RAG, we enforce the LLM to always answer solely based on the
    introduced context. The LLM will act as the reasoning engine, while the additional
    information added through RAG will act as the single source of truth for the generated
    answer. By doing so, we can quickly evaluate if the LLM’s answer is based on the
    external data or not.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入RAG，我们强制LLM始终仅基于引入的上下文来回答。LLM将作为推理引擎，而通过RAG添加的额外信息将作为生成答案的唯一真实来源。通过这样做，我们可以快速评估LLM的答案是否基于外部数据。
- en: Old information
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过时信息
- en: 'Any LLM is trained or fine-tuned on a subset of the total world knowledge dataset.
    This is due to three main issues:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 任何LLM都是在总世界知识数据集的一个子集上训练或微调的。这主要归因于三个主要问题：
- en: '**Private data**: You cannot train your model on data you don’t own or have
    the right to use.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**私有数据**：你不能在你不拥有或没有使用权的资料上训练你的模型。'
- en: '**New data**: New data is generated every second. Thus, you would have to constantly
    train your LLM to keep up.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新数据**：每秒都会生成新的数据。因此，你必须不断地训练你的LLM以保持同步。'
- en: '**Costs**: Training or fine-tuning an LLM is an extremely costly operation.
    Hence, it is not feasible to do it on an hourly or daily basis.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：训练或微调LLM是一项极其昂贵的操作。因此，按小时或按天进行是不可行的。'
- en: RAG solves these issues, as you no longer have to constantly fine-tune your
    LLM on new data (or even private data). Directly injecting the necessary data
    to respond to user questions into the prompts that are fed to the LLM is enough
    to generate correct and valuable answers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RAG解决了这些问题，因为你不再需要不断地在新数据（甚至私有数据）上微调你的LLM。直接将必要的数据注入到LLM接收的提示中，就足以生成正确且有价值的信息。
- en: To conclude, RAG is key for a robust and flexible generative AI system. But
    how do we inject the right data into the prompt based on the user’s questions?
    We will dig into the technical aspects of RAG in the next sections.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，RAG对于构建强大和灵活的生成式AI系统至关重要。但我们是怎样根据用户的问题将正确的数据注入提示中的呢？我们将在下一节深入探讨RAG的技术细节。
- en: The vanilla RAG framework
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础RAG框架
- en: Every RAG system is similar at its roots. We will first focus on understanding
    RAG in its simplest form. Later, we will gradually introduce more advanced RAG
    techniques to improve the system’s accuracy. Note that we will use vanilla and
    naive RAG interchangeably to avoid repetition.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个RAG系统在其根源上都是相似的。我们首先将专注于理解RAG在其最简单形式下的情况。随后，我们将逐步介绍更多高级的RAG技术来提高系统的准确性。请注意，我们将交替使用vanilla和naive
    RAG以避免重复。
- en: 'A RAG system is composed of three main modules independent of each other:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RAG系统由三个相互独立的主要模块组成：
- en: '**Ingestion pipeline**: A batch or streaming pipeline used to populate the
    vector DB'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄取管道**：用于填充向量数据库的批量或流式管道'
- en: '**Retrieval pipeline**: A module that queries the vector DB and retrieves relevant
    entries to the user’s input'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索管道**：一个查询向量数据库并检索与用户输入相关的条目的模块'
- en: '**Generation pipeline**: The layer that uses the retrieved data to augment
    the prompt and an LLM to generate answers'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成管道**：使用检索到的数据来增强提示并使用LLM生成答案的层'
- en: 'As these three components are classes or services of their own, we will dig
    into each separately. But for now, let’s try to answer the question “How are these
    three modules connected?”. Here is a very simplistic overview:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这三个组件都是各自独立的类或服务，我们将分别深入探讨。但就目前而言，让我们尝试回答“这三个模块是如何连接的？”这个问题。以下是一个非常简化的概述：
- en: On the backend side, the ingestion pipeline runs either on a schedule or constantly
    to populate the vector DB with external data.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在后端，摄取管道按照计划或持续运行，以将外部数据填充到向量数据库中。
- en: On the client side, the user asks a question.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户端，用户提出问题。
- en: The question is passed to the retrieval module, which preprocesses the user’s
    input and queries the vector DB.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题被传递到检索模块，该模块预处理用户的输入并查询向量数据库。
- en: The generation pipelines use a prompt template, user input, and retrieved context
    to create the prompt.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成管道使用提示模板、用户输入和检索到的上下文来创建提示。
- en: The prompt is passed to an LLM to generate the answer.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示信息被传递给大型语言模型（LLM）以生成答案。
- en: The answer is shown to the user.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案被展示给用户。
- en: '![](img/B31105_04_01.png)Figure 4.1: Vanilla RAG architecture'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B31105_04_01.png)图4.1：纯RAG架构'
- en: You must implement RAG in your generative AI application when you need access
    to any type of external information. For example, when implementing a financial
    assistant, you most likely need access to the latest news, reports, and prices
    before providing valuable answers. Or, if you build a traveling recommender, you
    must retrieve and parse a list of potential attractions, restaurants, and activities.
    At training time, LLMs don’t have access to your specific data, so you will often
    have to implement a RAG strategy in your generative AI project. Now, let’s dig
    into the ingestion, retrieval, and generation pipelines.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要访问任何类型的外部信息时，必须在您的生成式AI应用程序中实现RAG。例如，在实现财务助手时，您很可能需要在提供有价值的答案之前访问最新的新闻、报告和价格。或者，如果您构建一个旅行推荐系统，您必须检索并解析潜在景点、餐厅和活动的列表。在训练时间，LLM无法访问您的特定数据，因此您通常必须在您的生成式AI项目中实现RAG策略。现在，让我们深入探讨摄取、检索和生成管道。
- en: Ingestion pipeline
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摄取管道
- en: The RAG ingestion pipeline extracts raw documents from various data sources
    (e.g., data warehouse, data lake, web pages, etc.). Then, it cleans, chunks (splits
    into smaller sections), and embeds the documents. Ultimately, it loads the embedded
    chunks into a vector DB (or other similar vector storage).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: RAG摄取管道从各种数据源（例如数据仓库、数据湖、网页等）提取原始文档。然后，它进行清洗、分块（分割成更小的部分）和嵌入文档。最终，它将嵌入的分块加载到向量数据库（或其他类似的向量存储）中。
- en: 'Thus, the RAG ingestion pipeline is split into the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RAG摄取管道被拆分为以下部分：
- en: The **data extraction module** gathers all the necessary data from various sources
    such as DBs, APIs, or web pages. This module is highly dependent on your data.
    It can be as easy as querying your data warehouse or something more complex such
    as crawling Wikipedia.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据提取模块**从各种来源（如数据库、API或网页）收集所有必要的数据。此模块高度依赖于您的数据。它可以像查询您的数据仓库那样简单，也可以像爬取维基百科那样复杂。'
- en: A **cleaning layer** standardizes and removes unwanted characters from the extracted
    data. For example, you must remove all invalid characters from your input text,
    such as non-ASCII and bold and italic characters. Another popular cleaning strategy
    is to replace URLs with placeholders. However, your cleaning strategy will vary
    depending on your data source and embedding model.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清洗层**将标准化并从提取的数据中移除不需要的字符。例如，您必须从输入文本中移除所有无效字符，如非ASCII字符、粗体和斜体字符。另一种流行的清洗策略是将URL替换为占位符。然而，您的清洗策略将根据您的数据源和嵌入模型而有所不同。'
- en: The **chunking module** splits the cleaned documents into smaller ones. As we
    want to pass the document’s content to an embedding model, this is necessary to
    ensure it doesn’t exceed the model’s input maximum size. Also, chunking is required
    to separate specific regions that are semantically related. For example, when
    chunking a book’s chapter, the most optimal way is to group similar paragraphs
    into the same section or chunk. By doing so, at the retrieval time, you will add
    only the essential data to the prompt.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分块模块**将清洗后的文档分割成更小的部分。由于我们希望将文档内容传递给嵌入模型，这是必要的，以确保它不超过模型的输入最大大小。此外，分块是必要的，以分离语义相关的特定区域。例如，当分块一本书的章节时，最理想的方式是将相似的段落分组到同一个部分或分块中。通过这样做，在检索时，你将只添加必要的数据到提示中。'
- en: The **embedding component** uses anembedding model to take the chunk’s content
    (text, images, audio, etc.) and project it into a dense vector packed with semantic
    value—more on embeddings in the *What are embeddings?* section below.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入组件**使用嵌入模型将分块的内容（文本、图像、音频等）投影到一个密集的向量中，该向量包含语义值——更多关于嵌入的内容在下面的*什么是嵌入？*部分。'
- en: The **loading module** takes the embedded chunks along with a metadata document.
    The metadata will contain essential information such as the embedded content,
    the URL to the source of the chunk, and when the content was published on the
    web. The embedding is used as an index to query similar chunks, while the metadata
    is used to access the information added to augment the prompt.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载模块**接收嵌入的分块以及元数据文档。元数据将包含诸如嵌入内容、分块来源的URL以及内容在网络上发布的时间等关键信息。嵌入用作索引以查询相似的块，而元数据用于访问添加到增强提示的信息。'
- en: At this point, we have a RAG ingestion pipeline that takes raw documents as
    input, processes them, and populates a vector DB. The next step is to retrieve
    relevant data from the vector store correctly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个RAG摄取管道，它以原始文档为输入，处理它们，并填充向量数据库。下一步是正确地从向量存储中检索相关数据。
- en: Retrieval pipeline
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索管道
- en: The retrieval components take the user’s input (text, image, audio, etc.), embed
    it, and query the vector DB for similar vectors to the user’s input.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 检索组件接收用户的输入（文本、图像、音频等），将其嵌入，并查询向量数据库以找到与用户输入相似的向量。
- en: The primary function of the retrieval step is to project the user’s input into
    the same vector space as the embeddings used as an index in the vector DB. This
    allows us to find the top K’s most similar entries by comparing the embeddings
    from the vector storage with the user’s input vector. These entries then serve
    as content to augment the prompt that is passed to the LLM to generate the answer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 检索步骤的主要功能是将用户的输入投影到与向量数据库中用作索引的嵌入相同的向量空间。这使得我们能够通过比较向量存储中的嵌入与用户的输入向量来找到最相似的K个条目。然后，这些条目作为内容来增强传递给LLM以生成答案的提示。
- en: 'You must use a distance metric to compare two vectors, such as the Euclidean
    or Manhattan distance. But the most popular one is the cosine distance, which
    is equal to 1 minus the cosine of the angle between two vectors, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须使用距离度量来比较两个向量，例如欧几里得距离或曼哈顿距离。但最流行的是余弦距离，它等于两个向量之间角度的余弦值的1减去，如下所示：
- en: '![](img/B31105_04_001.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_001.png)'
- en: It ranges from `-1` to `1`, with a value of `-1` when vectors **A** and **B**
    are in opposite directions, `0` if they are orthogonal, and `1` if they point
    in the same direction.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它的范围从`-1`到`1`，当向量**A**和**B**方向相反时值为`-1`，如果它们是正交的，则值为`0`，如果它们指向同一方向，则值为`1`。
- en: Most of the time, the cosine distance works well in non-linear complex vector
    spaces. However, it is essential to notice that choosing the proper distance between
    two vectors depends on your data and the embedding model you use.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，余弦距离在非线性复杂向量空间中表现良好。然而，重要的是要注意，选择两个向量之间的适当距离取决于你的数据和所使用的嵌入模型。
- en: One critical factor to highlight is that the user’s input and embeddings must
    be in the same vector space. Otherwise, you cannot compute the distance between
    them. To do so, it is essential to preprocess the user input in the same way you
    processed the raw documents in the RAG ingestion pipeline. This means you must
    clean, chunk (if necessary), and embed the user’s input using the same functions,
    models, and hyperparameters. This is similar to how you have to preprocess the
    data into features in the same way between training and inference; otherwise,
    the inference will yield inaccurate results—a phenomenon also known as the training-serving
    skew.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要强调的关键因素是用户的输入和嵌入必须在同一个向量空间中。否则，您无法计算它们之间的距离。要做到这一点，您必须以与RAG摄取管道中处理原始文档相同的方式预处理用户输入。这意味着您必须使用相同的函数、模型和超参数来清理、分块（如果需要）和嵌入用户的输入。这类似于您必须在训练和推理之间以相同的方式将数据预处理为特征；否则，推理将产生不准确的结果——这种现象也称为训练-服务偏差。
- en: Generation pipeline
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成管道
- en: The last step of the RAG system is to take the user’s input, retrieve data,
    pass it to an LLM, and generate a valuable answer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统的最后一步是获取用户的输入，检索数据，将其传递给LLM，并生成一个有价值的答案。
- en: The final prompt results from a system and prompt template populated with the
    user’s query and retrieved context. You might have a single prompt template or
    multiple prompt templates, depending on your application. Usually, all the prompt
    engineering is done at the prompt template level.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的提示结果来自一个系统和提示模板，其中包含用户的查询和检索到的上下文。您可能有一个提示模板或多个提示模板，具体取决于您的应用。通常，所有的提示工程都是在提示模板级别完成的。
- en: 'Below, you can see a dummy example of what a generic system and prompt template
    look like and how they are used together with the retrieval logic and the LLM
    to generate the final answer:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，您可以看到一个通用系统和提示模板的示例，以及它们如何与检索逻辑和LLM一起使用以生成最终答案：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As the prompt templates evolve, each change should be tracked and versioned
    using **machine learning operations** (**MLOps**) best practices. Thus, during
    training or inference time, you always know that a given answer was generated
    by a specific version of the LLM and prompt template(s). You can do this through
    Git, store the prompt templates in a DB, or use specific prompt management tools
    such as LangFuse.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 随着提示模板的演变，每个变更都应该使用**机器学习操作**（**MLOps**）最佳实践进行跟踪和版本控制。因此，在训练或推理时间，您始终知道给定的答案是由特定的LLM版本和提示模板生成的。您可以通过Git完成此操作，将提示模板存储在数据库中，或使用诸如LangFuse之类的特定提示管理工具。
- en: As we’ve seen in the retrieval pipeline, some critical aspects that directly
    impact the accuracy of your RAG system are the embeddings of the external data,
    usually stored in vector DBs, the embedding of the user’s query, and how we can
    find similarities between the two using functions such as the cosine distance.
    To better understand this part of the RAG algorithm, let’s zoom in on what embeddings
    are and how they are computed.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在检索管道中看到的，一些直接影响您RAG系统准确性的关键方面是外部数据的嵌入，通常存储在向量数据库中，用户的查询嵌入，以及我们如何使用诸如余弦距离之类的函数在两者之间找到相似性。为了更好地理解RAG算法的这一部分，让我们深入探讨嵌入是什么以及它们是如何计算的。
- en: What are embeddings?
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入是什么？
- en: Imagine you’re trying to teach a computer to understand the world. Embeddings
    are like a particular translator that turns these things into a numerical code.
    This code isn’t random, though, because similar words or items end up with codes
    that are close to each other. It’s like a map where words with similar meanings
    are clustered together.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在尝试教一台计算机理解世界。嵌入就像一个特定的翻译器，将这些事物转换成数值代码。然而，这个代码并非随机，因为相似的单词或项目最终会得到彼此接近的代码。这就像一张地图，其中具有相似意义的单词聚集在一起。
- en: With that in mind, a more theoretical definition is that embeddings are dense
    numerical representations of objects encoded as vectors in a continuous vector
    space, such as words, images, or items in a recommendation system. This transformation
    helps capture the semantic meaning and relationships between the objects. For
    instance, in **natural language processing** (**NLP**), embeddings translate words
    into vectors where semantically similar words are positioned closely together
    in the vector space.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，一个更理论化的定义是，嵌入是对象在连续向量空间中编码为向量的密集数值表示，例如单词、图像或推荐系统中的项目。这种转换有助于捕捉对象之间的语义意义和关系。例如，在**自然语言处理**（**NLP**）中，嵌入将单词转换为向量，其中语义相似的单词在向量空间中位置靠近。
- en: '![](img/B31105_04_02.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_04_02.png)'
- en: 'Figure 4.2: What are embeddings?'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：嵌入是什么？
- en: A popular method is visualizing the embeddings to understand and evaluate their
    geometrical relationship. As the embeddings often have more than 2 or 3 dimensions,
    usually between 64 and 2048, you must project them again to 2D or 3D.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的方法是可视化嵌入以理解和评估它们的几何关系。由于嵌入通常具有超过2或3个维度，通常在64到2048之间，你必须将它们再次投影到2D或3D。
- en: For example, you can use UMAP ([https://umap-learn.readthedocs.io/en/latest/index.html](https://umap-learn.readthedocs.io/en/latest/index.html)),
    a dimensionality reduction method well known for keeping the geometrical properties
    between the points when projecting the embeddings to 2D or 3D. Another popular
    algorithm for dimensionality reduction when visualizing vectors is t-SNE ([https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)).
    However, compared to UMAP, it is more stochastic and doesn’t preserve the topological
    relationships between the points.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以使用UMAP（[https://umap-learn.readthedocs.io/en/latest/index.html](https://umap-learn.readthedocs.io/en/latest/index.html)），这是一种在将嵌入投影到2D或3D时，能够很好地保持点之间几何属性的降维方法。在可视化向量时，另一个流行的降维算法是t-SNE（[https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)）。然而，与UMAP相比，t-SNE更加随机，并且不保留点之间的拓扑关系。
- en: A dimensionality reduction algorithm, such as PCA, UMAP, and t-SNE, is a mathematical
    technique used to reduce the number of input variables or features in a dataset
    while preserving the data’s essential patterns, structure, and relationships.
    The goal is to transform high-dimensional data into a lower-dimensional form,
    making it easier to visualize, interpret, and process while minimizing the loss
    of important information. These methods help to address the “curse of dimensionality,”
    improve computational efficiency, and often enhance the performance of ML algorithms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 降维算法，如PCA、UMAP和t-SNE，是一种数学技术，用于在保留数据的基本模式、结构和关系的同时，减少数据集中的输入变量或特征数量。目标是把高维数据转换成低维形式，使其更容易可视化、解释和处理，同时最大限度地减少重要信息的损失。这些方法有助于解决“维度诅咒”，提高计算效率，并通常增强机器学习算法的性能。
- en: '![](img/B31105_04_03.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_04_03.png)'
- en: 'Figure 4.3: Visualize embeddings using UMAP (Source: UMAP’s documentation)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：使用UMAP可视化嵌入（来源：UMAP文档）
- en: Why embeddings are so powerful
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么嵌入如此强大
- en: Firstly, ML models work only with numerical values. This is not a problem when
    working with tabular data, as the data is often in numerical form or can easily
    be processed into numbers. Embeddings come in handy when we want to feed words,
    images, or audio data into models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，机器学习模型只处理数值。当处理表格数据时，这并不是问题，因为数据通常以数值形式存在或可以轻松转换为数值。当我们想要将单词、图像或音频数据输入模型时，嵌入就派上用场了。
- en: For instance, when working with transformer models, you tokenize all your text
    input, where each token has an embedding associated with it. The beauty of this
    process lies in its simplicity; the input to the transformer is a sequence of
    embeddings, which can be easily and confidently interpreted by the dense layers
    of the neural network.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当使用转换器模型时，你需要对所有的文本输入进行分词，每个标记都与一个嵌入相关联。这个过程的美妙之处在于其简单性；转换器的输入是一个嵌入序列，这可以被神经网络密集层轻松且自信地解释。
- en: Based on this example, you can use embeddings to encode any categorical variable
    and feed it to an ML model. But why not use other simple methods, such as **one-hot
    encoding?** When working with categorical variables with high cardinality, such
    as language vocabularies, you will suffer from the curse of dimensionality when
    using other classical methods. For example, if your vocabulary has 10,000 tokens,
    then only one token will have a length of 10,000 after applying one-hot encoding.
    If the input sequence has N tokens, that will become N * 10,000 input parameters.
    If N >= 100, often, when inputting text, the input is too large to be usable.
    Another issue with other classical methods that don’t suffer from the curse of
    dimensionality, such as **hashing**, is that you lose the semantic relationships
    between the vectors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个例子，你可以使用嵌入来编码任何分类变量并将其输入到机器学习模型中。但为什么不使用其他简单的方法，例如**独热编码**呢？当处理具有高基数（例如语言词汇）的分类变量时，使用其他经典方法会遇到维度灾难。例如，如果你的词汇有10,000个标记，那么在应用独热编码后，只有一个标记的长度为10,000。如果输入序列有N个标记，那么这将变成N
    * 10,000个输入参数。如果N >= 100，通常在输入文本时，输入数据太大而无法使用。其他经典方法（如**哈希**）不遭受维度灾难的问题，但你可能会丢失向量之间的语义关系。
- en: '**One-hot encoding** is a technique that converts categorical variables into
    a binary matrix representation. Each category is represented as a unique binary
    vector. For each categorical variable, a binary vector is created with a length
    equal to the number of unique categories, where all values are zero except for
    the index corresponding to the specific category, which is set to one. The method
    preserves all information about the categories. It is simple and interpretable.
    However, a significant disadvantage is that it can lead to a high-dimensional
    feature space if the categorical variable has many unique values, making the method
    impractical.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**独热编码**是一种将分类变量转换为二进制矩阵表示的技术。每个类别都表示为一个唯一的二进制向量。对于每个分类变量，创建一个长度等于唯一类别数量的二进制向量，其中所有值都是零，除了对应特定类别的索引，该索引被设置为1。该方法保留了关于类别的所有信息。它简单且可解释。然而，一个显著的缺点是，如果分类变量有大量唯一值，它可能导致高维特征空间，使得该方法不实用。'
- en: '**Feature hashing**, also known as hashing encoding or the “hash trick,” is
    a technique used to convert categorical variables into numerical features by applying
    a hash function to the category values. Compared to one-hot encoding, the method
    is not bound to the number of unique categories, but it reduces the dimensionality
    of the feature space by mapping categories into a fixed number of bins or buckets.
    Thus, it reduces the dimensionality of the feature space, which is particularly
    useful when dealing with high-cardinality categorical variables. This makes it
    efficient in terms of memory usage and computational time. However, there is a
    risk of collisions, where different categories might map to the same bin, leading
    to a loss of information. The mapping makes the method uninterpretable. Also,
    it is difficult to understand the relationship between the original categories
    and the hashed features.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征哈希**，也称为哈希编码或“哈希技巧”，是一种通过将类别值应用哈希函数来将分类变量转换为数值特征的技术。与独热编码相比，该方法不受唯一类别数量的限制，但它通过将类别映射到固定数量的桶或桶中，减少了特征空间的维度。因此，它减少了特征空间的维度，这在处理高基数分类变量时特别有用。这使得它在内存使用和计算时间方面效率很高。然而，存在碰撞的风险，即不同的类别可能映射到同一个桶，导致信息丢失。这种映射使得该方法不可解释。此外，很难理解原始类别与哈希特征之间的关系。'
- en: Embeddings help us encode categorical variables while controlling the output
    vector’s dimension. They also use ingenious ways to condense information into
    a lower dimension space than naive hashing tricks.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入帮助我们编码分类变量，同时控制输出向量的维度。它们还使用巧妙的方法将信息压缩到比原始哈希技巧低得多的维度空间中。
- en: Secondly, embedding your input reduces the size of its dimension and condenses
    all of its semantic meaning into a dense vector. This is an extremely popular
    technique when working with images, where a CNN encoder module maps the high-dimensional
    meaning into an embedding, which is later processed by a CNN decoder that performs
    the classification or regression steps.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，嵌入输入减少了其维度的大小，并将所有语义意义压缩到一个密集的向量中。这在处理图像时是一个非常流行的技术，其中CNN编码模块将高维意义映射到嵌入中，随后由CNN解码器处理分类或回归步骤。
- en: 'The following image shows a typical CNN layout. Imagine tiny squares within
    each layer. Those are the “receptive fields.” Each square feeds information to
    a single neuron in the previous layer. As you move through the network, two key
    things are happening:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个典型的CNN布局。想象一下每一层中的小方块。这些是“感受野”。每个方块将信息传递给前一层的一个神经元。随着你通过网络移动，有两个关键的事情正在发生：
- en: '**Shrinking the picture**: Special “subsampling” operations make the layers
    smaller, focusing on essential details.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩小图片**：特殊的“子采样”操作使层变得更小，专注于关键细节。'
- en: '**Learning features**: “Convolution” operations, on the other hand, actually
    increase the layer size as the network learns more complex features from the image.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习特征**：另一方面，“卷积”操作实际上随着网络从图像中学习更复杂的特征而增加层的大小。'
- en: Finally, a fully connected layer at the end takes all this processed information
    and transforms it into the final vector embedding, a numerical image representation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个完全连接的层在最后将所有这些处理过的信息转换成最终的向量嵌入，这是一个数值图像表示。
- en: '![](img/B31105_04_04.png)Figure 4.4: Creating embeddings from an image using
    a CNN (Image source)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_04.png)图4.4：使用CNN从图像中创建嵌入（图片来源）'
- en: 'The preceding image is sourced from *Wikimedia Commons* ([https://commons.wikimedia.org/wiki/File:Typical_cnn.png](https://commons.wikimedia.org/wiki/File:Typical_cnn.png))
    and licensed under the Creative Commons Attribution-ShareAlike 4.0 International
    License (CC BY-SA 4.0: [https://creativecommons.org/licenses/by-sa/4.0/deed.en](https://creativecommons.org/licenses/by-sa/4.0/deed.en)).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图片来源于*维基共享资源*([https://commons.wikimedia.org/wiki/File:Typical_cnn.png](https://commons.wikimedia.org/wiki/File:Typical_cnn.png))，并授权于Creative
    Commons Attribution-ShareAlike 4.0国际许可协议（CC BY-SA 4.0：[https://creativecommons.org/licenses/by-sa/4.0/deed.en](https://creativecommons.org/licenses/by-sa/4.0/deed.en))。
- en: How are embeddings created?
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入是如何创建的？
- en: Embeddings are created by deep learning models that understand the context and
    semantics of your input and project it into a continuous vector space.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是由理解输入的上下文和语义的深度学习模型创建的，并将它投影到一个连续的向量空间中。
- en: Various deep learning models can be used to create embeddings, varying by the
    data input type. Thus, it is fundamental to understand your data and what you
    need from it before picking an embedding model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用各种深度学习模型来创建嵌入，具体取决于数据输入类型。因此，在挑选嵌入模型之前，理解你的数据以及你需要从数据中获得什么是非常基本的。
- en: For example, when working with text data, one of the early methods used to create
    embeddings for your vocabulary is Word2Vec and GloVe. These are still popular
    methods used today for simpler applications.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当处理文本数据时，用于创建词汇嵌入的早期方法之一是Word2Vec和GloVe。这些方法至今仍然是用于简单应用中流行的方法。
- en: Another popular method is to use encoder-only transformers, such as BERT, or
    other methods from its family, such as RoBERTa. These models leverage the encoder
    of the transformer architecture to smartly project your input into a dense vector
    space that can later be used as embeddings.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的方法是使用仅编码器的transformer，如BERT，或其家族中的其他方法，如RoBERTa。这些模型利用transformer架构的编码器，将你的输入智能地投影到一个密集的向量空间中，该空间可以稍后用作嵌入。
- en: To quickly compute the embeddings in Python, you can conveniently leverage the
    Sentence Transformers Python package (also available in Hugging Face’s transformer
    package). This tool provides a user-friendly interface, making the embedding process
    straightforward and efficient.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速在Python中计算嵌入，你可以方便地利用Sentence Transformers Python包（也可在Hugging Face的transformer包中找到）。这个工具提供了一个用户友好的界面，使得嵌入过程变得简单高效。
- en: 'In the code snippet below, you can see how we loaded a model from SentenceTransformer,
    computed the embeddings for three sentences, and, ultimately, computed the cosine
    similarity between them. The similarity between one sentence and itself is always
    1\. Also, the similarity between the first and second sentences is approximately
    0, as the sentences have nothing in common. In contrast, the value between the
    first and third one is higher as there is some overlapping context:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，你可以看到我们如何从SentenceTransformer加载一个模型，计算三个句子的嵌入，并最终计算它们之间的余弦相似度。一个句子与自身的相似度总是1。同样，第一句和第二句之间的相似度大约为0，因为这两个句子没有共同点。相比之下，第一句和第三句之间的值更高，因为它们有一些重叠的上下文：
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The examples in the embeddings section can be run within the virtual environment
    used across the book, as it contains all the required dependencies.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入部分中的示例可以在本书使用的虚拟环境中运行，因为它包含了所有必需的依赖项。
- en: The best-performing embedding model can change with time and your specific use
    case. You can find particular models on the **Massive Text Embedding Benchmark**
    (**MTEB**) on Hugging Face. Depending on your needs, you can consider the best-performing
    model, the one with the best accuracy, or the one with the smallest memory footprint.
    This decision is solely based on your requirements (e.g., accuracy and hardware).
    However, Hugging Face and SentenceTransformer make switching between different
    models straightforward. Thus, you can always experiment with various options.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳嵌入模型可能会随着时间和你的特定用例而变化。你可以在Hugging Face上的**大规模文本嵌入基准**（**MTEB**）中找到特定模型。根据你的需求，你可以考虑性能最佳模型、准确度最高的模型或内存占用最小的模型。这个决定完全基于你的要求（例如，准确度和硬件）。然而，Hugging
    Face和SentenceTransformer使得在不同模型之间切换变得简单。因此，你可以始终尝试不同的选项。
- en: When working with images, you can embed them using **convolutional neural networks**
    (**CNNs**). Popular CNN networks are based on the ResNet architecture. However,
    we can’t directly use image embedding techniques for audio recordings. Instead,
    we can create a visual representation of the audio, such as a spectrogram, and
    then apply image embedding models to those visuals. This allows us to capture
    the essence of images and sounds in a way computers can understand.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理图像时，你可以使用**卷积神经网络**（**CNNs**）来嵌入它们。基于ResNet架构的CNN网络很受欢迎。然而，我们无法直接使用图像嵌入技术来处理音频记录。相反，我们可以创建音频的视觉表示，例如频谱图，然后应用图像嵌入模型到这些视觉上。这使我们能够以计算机可以理解的方式捕捉图像和声音的本质。
- en: By leveraging models like CLIP, you can practically embed a piece of text and
    an image in the same vector space. This allows you to find similar images using
    a sentence as input, or the other way around, demonstrating the practicality of
    CLIP.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用CLIP等模型，你可以实际上将一段文本和一张图片嵌入到同一个向量空间中。这允许你使用句子作为输入来找到相似图片，或者反过来，展示了CLIP的实用性。
- en: 'In the following code snippet, we use CLIP to encode a crazy cat image and
    three sentences. Ultimately, we use cosine similarity to compute the resemblance
    between the picture and the sentences:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们使用CLIP对一张疯狂猫的图片和三个句子进行编码。最终，我们使用余弦相似度来计算图片和句子之间的相似度：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The source code can be found at [https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_image_embeddings.py](https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_image_embeddings.py).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码可以在[https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_image_embeddings.py](https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_text_image_embeddings.py)找到。
- en: Here, we provided a small introduction to how embeddings can be computed. The
    realm of specific implementations is vast, but what is important to know is that
    embeddings can be computed for most digital data categories, such as words, sentences,
    documents, images, videos, and graphs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简要介绍了如何计算嵌入。具体实现的领域非常广泛，但重要的是要知道，大多数数字数据类别都可以计算嵌入，例如单词、句子、文档、图像、视频和图。
- en: It’s crucial to grasp that you must use specialized models when you need to
    compute the distance between two different data categories, such as the distance
    between the vector of a sentence and of an image. These models are designed to
    project both data types into the same vector space, such as CLIP, ensuring accurate
    distance computation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一点至关重要：当你需要计算两个不同数据类别之间的距离时，例如句子向量和图像向量之间的距离，你必须使用专门的模型。这些模型旨在将两种数据类型投影到同一个向量空间，例如CLIP，以确保准确的距离计算。
- en: Applications of embeddings
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入的应用
- en: 'Due to the generative AI revolution, which uses RAG, embeddings have become
    extremely popular in information retrieval tasks, such as semantic search for
    text, code, images, and audio, and long-term memory of agents. But before generative
    AI, embeddings were already heavily used in:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用RAG的生成式AI革命，嵌入在信息检索任务中变得极其流行，例如文本、代码、图像和音频的语义搜索以及代理的长时记忆。但在生成式AI之前，嵌入已经在以下方面被大量使用：
- en: Representing categorical variables (e.g., vocabulary tokens) that are fed to
    an ML model
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示传递给ML模型的分类变量（例如，词汇标记）
- en: Recommender systems by encoding the users and items and finding their relationship
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过编码用户和项目并找到它们之间的关系来构建推荐系统
- en: Clustering and outlier detection
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类和异常值检测
- en: Data visualization by using algorithms such as UMAP
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用UMAP等算法进行数据可视化
- en: Classification by using the embeddings as features
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用嵌入作为特征进行分类
- en: Zero-shot classification by comparing the embedding of each class and picking
    the most similar one
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过比较每个类的嵌入并选择最相似的一个进行零样本分类
- en: The last step to fully understanding how RAG works is to examine vector DBs
    and how they leverage embeddings to retrieve data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 完全理解RAG（检索增强生成）工作原理的最后一步是检查向量数据库以及它们如何利用嵌入来检索数据。
- en: More on vector DBs
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于向量数据库的信息
- en: Vector DBs are specialized DBs designed to efficiently store, index, and retrieve
    vector embeddings. Traditional scalar-based DBs struggle with the complexity of
    vector data, making vector DBs crucial for tasks like real-time semantic search.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库是专门设计的数据库，用于高效地存储、索引和检索向量嵌入。传统的基于标量的数据库在处理向量数据的复杂性方面存在困难，这使得向量数据库对于实时语义搜索等任务至关重要。
- en: While standalone vector indices like FAISS are effective for similarity search,
    they lack vector DBs’ comprehensive data management capabilities. Vector DBs support
    CRUD operations, metadata filtering, scalability, real-time updates, backups,
    ecosystem integration, and robust data security, making them more suited for production
    environments than standalone indices.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像FAISS这样的独立向量索引对于相似度搜索是有效的，但它们缺乏向量数据库全面的数据管理功能。向量数据库支持CRUD操作、元数据过滤、可伸缩性、实时更新、备份、生态系统集成和强大的数据安全，这使得它们比独立索引更适合生产环境。
- en: How does a vector DB work?
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量数据库是如何工作的？
- en: Think of how you usually search a DB. You type in something specific, and the
    system spits out the exact match. That’s how traditional DBs work. Vector DBs
    are different. Instead of perfect matches, we look for the closest neighbors of
    the query vector. Under the hood, a vector DB uses **approximate nearest neighbor**
    (**ANN**) algorithms to find these close neighbors.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你通常是如何搜索数据库的。你输入一些特定的内容，系统就会输出精确匹配的结果。这就是传统数据库的工作方式。向量数据库则不同。我们寻找的是查询向量的最近邻，而不是完美匹配。在底层，向量数据库使用**近似最近邻**（**ANN**）算法来找到这些邻近的邻居。
- en: While ANN algorithms don’t return the top matches for a given search, standard
    nearest neighbor algorithms are too slow to work in practice. Also, it is shown
    empirically that using only approximations of the top matches for a given input
    query works well enough. Thus, the trade-off between accuracy and latency ultimately
    favors ANN algorithms.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ANN算法不会为给定搜索返回顶部匹配项，但标准最近邻算法在实践中的速度太慢。此外，经验表明，仅使用给定输入查询的顶部匹配项的近似值就足够好了。因此，准确性和延迟之间的权衡最终有利于ANN算法。
- en: 'This is a typical workflow of a vector DB:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个向量数据库的典型工作流程：
- en: '**Indexing vectors**: Vectors are indexed using data structures optimized for
    high-dimensional data. Common indexing techniques include **hierarchical navigable
    small world** (**HNSW**), random projection, **product quantization** (**PQ**),
    and **locality-sensitive hashing** (**LSH**).'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**索引向量**：向量使用针对高维数据优化的数据结构进行索引。常见的索引技术包括**层次可导航小世界**（**HNSW**）、随机投影、**产品量化**（**PQ**）和**局部敏感哈希**（**LSH**）。'
- en: '**Querying for similarity**: During a search, the DB queries the indexed vectors
    to find those most similar to the input vector. This process involves comparing
    vectors based on similarity measures such as cosine similarity, Euclidean distance,
    or dot product. Each has unique advantages and is suitable for different use cases.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询相似度**：在搜索过程中，数据库查询索引向量以找到与输入向量最相似的向量。这个过程涉及根据相似度度量（如余弦相似度、欧几里得距离或点积）比较向量。每种方法都有其独特的优势，适用于不同的用例。'
- en: '**Post-processing results**: After identifying potential matches, the results
    undergo post-processing to refine accuracy. This step ensures that the most relevant
    vectors are returned to the user.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**后处理结果**：在识别出潜在匹配项后，结果将进行后处理以精炼准确性。这一步骤确保返回给用户的是最相关的向量。'
- en: Vector DBs can filter results based on metadata before or after the vector search.
    Both approaches have trade-offs in terms of performance and accuracy. The query
    also depends on the metadata (along with the vector index), so it contains a metadata
    index user for filtering operations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库可以在向量搜索之前或之后根据元数据过滤结果。这两种方法在性能和准确性方面都有权衡。查询还依赖于元数据（以及向量索引），因此它包含一个用于过滤操作的元数据索引用户。
- en: Algorithms for creating the vector index
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建向量索引的算法
- en: 'Vector DBs use various algorithms to create the vector index and manage searching
    data efficiently:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库使用各种算法来创建向量索引并有效地管理搜索数据：
- en: '**Random projection**: Random projection reduces the dimensionality of vectors
    by projecting them into a lower-dimensional space using a random matrix. This
    technique preserves the relative distances between vectors, facilitating faster
    searches.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机投影**：随机投影通过使用随机矩阵将向量投影到低维空间来降低向量的维度。这项技术保留了向量之间的相对距离，从而促进了更快的搜索。'
- en: '**PQ**: PQ compresses vectors by dividing them into smaller sub-vectors and
    then quantizing these sub-vectors into representative codes. This reduces memory
    usage and speeds up similarity searches.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PQ**：PQ通过将向量分成更小的子向量，然后将这些子向量量化为代表性代码来压缩向量。这减少了内存使用并加快了相似性搜索。'
- en: '**LSH**: LSH maps similar vectors into buckets. This method enables fast approximate
    nearest neighbor searches by focusing on a subset of the data, reducing the computational
    complexity.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSH**：LSH将相似的向量映射到桶中。这种方法通过关注数据的一个子集，实现了快速近似最近邻搜索，从而降低了计算复杂度。'
- en: '**HNSW**: HNSW constructs a multi-layer graph where each node represents a
    set of vectors. Similar nodes are connected, allowing the algorithm to navigate
    the graph and find the nearest neighbors efficiently.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HNSW**：HNSW构建了一个多层图，其中每个节点代表一组向量。相似节点相连，允许算法在图中导航并有效地找到最近邻。'
- en: These algorithms enable vector DBs to efficiently handle complex and large-scale
    data, making them a perfect fit for a variety of AI and ML applications.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法使向量数据库能够高效地处理复杂和大规模数据，使它们成为各种人工智能和机器学习应用的完美选择。
- en: DB operations
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据库操作
- en: 'Vector DBs also share common characteristics with standard DBs to ensure high
    performance, fault tolerance, and ease of management in production environments.
    Key operations include:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库也具有与标准数据库的共同特征，以确保在生产环境中具有高性能、容错性和易于管理。关键操作包括：
- en: '**Sharding and replication**: Data is partitioned (sharded) across multiple
    nodes to ensure scalability and high availability. Data replication across nodes
    helps maintain data integrity and availability in case of node failures.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分片和复制**：数据在多个节点之间分区（分片），以确保可扩展性和高可用性。节点间的数据复制有助于在节点故障的情况下保持数据完整性和可用性。'
- en: '**Monitoring**: Continuous monitoring of DB performance, including query latency
    and resource usage (RAM, CPU, disk), helps maintain optimal operations and identify
    potential issues before they impact the system.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：持续监控数据库性能，包括查询延迟和资源使用情况（RAM、CPU、磁盘），有助于保持最佳运行状态并在问题影响系统之前发现潜在问题。'
- en: '**Access control**: Implementing robust access control mechanisms ensures that
    only authorized users can access and modify data. This includes role-based access
    controls and other security protocols to protect sensitive information.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问控制**：实施强大的访问控制机制确保只有授权用户才能访问和修改数据。这包括基于角色的访问控制和其它安全协议，以保护敏感信息。'
- en: '**Backups**: Regular DB backups are critical for disaster recovery. Automated
    backup processes ensure that data can be restored to a previous state in case
    of corruption or loss.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**备份**：定期的数据库备份对于灾难恢复至关重要。自动备份过程确保在数据损坏或丢失的情况下，数据可以恢复到之前的状态。'
- en: An overview of advanced RAG
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级RAG概述
- en: 'The vanilla RAG framework we just presented doesn’t address many fundamental
    aspects that impact the quality of the retrieval and answer generation, such as:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才提出的标准RAG框架没有解决影响检索和答案生成质量的一些基本方面，例如：
- en: Are the retrieved documents relevant to the user’s question?
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索的文档与用户的问题相关吗？
- en: Is the retrieved context enough to answer the user’s question?
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索到的上下文是否足够回答用户的问题？
- en: Is there any redundant information that only adds noise to the augmented prompt?
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否存在任何冗余信息，这些信息只会增加增强提示的噪声？
- en: Does the latency of the retrieval step match our requirements?
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索步骤的延迟是否符合我们的要求？
- en: What do we do if we can’t generate a valid answer using the retrieved information?
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们无法使用检索到的信息生成有效的答案，我们该怎么办？
- en: From the questions above, we can draw two conclusions. The first one is that
    we need a robust evaluation module for our RAG system that can quantify and measure
    the quality of the retrieved data and generate answers relative to the user’s
    question. We will discuss this topic in more detail in *Chapter 9*. The second
    conclusion is that we must improve our RAG framework to address the retrieval
    limitations directly in the algorithm. These improvements are known as advanced
    RAG.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述问题中，我们可以得出两个结论。第一个结论是我们需要一个强大的评估模块来评估我们的RAG系统，它可以量化并衡量检索数据的质量，并针对用户的问题生成答案。我们将在第9章中更详细地讨论这个话题。第二个结论是我们必须改进我们的RAG框架，以直接在算法中解决检索限制。这些改进被称为高级RAG。
- en: 'The vanilla RAG design can be optimized at three different stages:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的RAG设计可以在三个不同的阶段进行优化：
- en: '**Pre-retrieval**: This stage focuses on how to structure and preprocess your
    data for data indexing optimizations as well as query optimizations.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预检索**：这个阶段侧重于如何对数据进行结构和预处理，以优化数据索引和查询优化。'
- en: '**Retrieval**: This stage revolves around improving the embedding models and
    metadata filtering to improve the vector search step.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**：这个阶段主要围绕改进嵌入模型和元数据过滤，以提高向量搜索步骤。'
- en: '**Post-retrieval**: This stage mainly targets different ways to filter out
    noise from the retrieved documents and compress the prompt before feeding it to
    an LLM for answer generation.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后检索**：这个阶段主要针对从检索到的文档中过滤噪声和压缩提示的不同方法，在将其馈送到LLM进行答案生成之前。'
- en: '![](img/B31105_04_05.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_04_05.png)'
- en: 'Figure 4.5: The three stages of advanced RAG'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：高级RAG的三个阶段
- en: This section is not meant to be an exhaustive list of all the advanced RAG methods
    available. The goal is to build an intuition about what can be optimized. We will
    use only examples based on text data, but the principles of advanced RAG remain
    the same regardless of the data category. Now, let’s zoom in on all three components.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的不是列出所有可用的先进RAG方法，而是建立对可以优化内容的直觉。我们将仅使用基于文本数据的示例，但无论数据类别如何，高级RAG的原则都是相同的。现在，让我们深入探讨所有三个组成部分。
- en: Pre-retrieval
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预检索
- en: 'The pre-retrieval steps are performed in two different ways:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 预检索步骤以两种不同的方式进行：
- en: '**Data indexing**: It is part of the RAG ingestion pipeline. It is mainly implemented
    within the cleaning or chunking modules to preprocess the data for better indexing.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据索引**：它是RAG摄取管道的一部分。它主要在清理或分块模块中实现，以预处理数据以更好地索引。'
- en: '**Query optimization**: The algorithm is performed directly on the user’s query
    before embedding it and retrieving the chunks from the vector DB.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询优化**：算法在嵌入和从向量数据库检索块之前直接在用户的查询上执行。'
- en: 'As we index our data using embeddings that semantically represent the content
    of a chunked document, most of the **data indexing** techniques focus on better
    preprocessing and structuring the data to improve retrieval efficiency, such as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用语义上表示分块文档内容的嵌入来索引我们的数据，大多数**数据索引**技术都集中在更好的数据预处理和结构化上，以提高检索效率，例如：
- en: '**Sliding window**: The sliding window technique introduces overlap between
    text chunks, ensuring that important context near chunk boundaries is retained,
    which enhances retrieval accuracy. This is particularly beneficial in domains
    like legal documents, scientific papers, customer support logs, and medical records,
    where critical information often spans multiple sections. The embedding is computed
    on the chunk along with the overlapping portion. Hence, the sliding window improves
    the system’s ability to retrieve relevant and coherent information by maintaining
    context across boundaries.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滑动窗口**：滑动窗口技术引入了文本块之间的重叠，确保了重要上下文在块边界附近被保留，这提高了检索的准确性。这在法律文件、科学论文、客户支持日志和医疗记录等领域特别有益，在这些领域，关键信息经常跨越多个部分。嵌入计算在块及其重叠部分上。因此，滑动窗口通过保持上下文跨边界来提高系统检索相关和连贯信息的能力。'
- en: '**Enhancing data granularity**: This involves data cleaning techniques like
    removing irrelevant details, verifying factual accuracy, and updating outdated
    information. A clean and accurate dataset allows for sharper retrieval.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强数据粒度**：这涉及数据清理技术，如删除无关细节、验证事实准确性以及更新过时信息。一个干净且准确的数据集允许更精确的检索。'
- en: '**Metadata**: Adding metadata tags like dates, URLs, external IDs, or chapter
    markers helps filter results efficiently during retrieval.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据**: 添加日期、URL、外部ID或章节标记等元数据标签有助于在检索过程中有效地过滤结果。'
- en: '**Optimizing index structures**: It is based on different data index methods,
    such as various chunk sizes and multi-indexing strategies.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化索引结构**: 它基于不同的数据索引方法，如各种块大小和多索引策略。'
- en: '**Small-to-big**: The algorithm decouples the chunks used for retrieval and
    the context used in the prompt for the final answer generation. The algorithm
    uses a small sequence of text to compute the embedding while preserving the sequence
    itself and a wider window around it in the metadata. Thus, using smaller chunks
    enhances the retrieval’s accuracy, while the larger context adds more contextual
    information to the LLM.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从小到大**: 该算法将用于检索的块和用于最终答案生成的提示中的上下文解耦。算法使用一小段文本来计算嵌入，同时保留该序列本身及其周围的元数据更宽的窗口。因此，使用较小的块可以提高检索的准确性，而较大的上下文则为LLM添加更多上下文信息。'
- en: The intuition behind this is that if we use the whole text for computing the
    embedding, we might introduce too much noise, or the text could contain multiple
    topics, which results in a poor overall semantic representation of the embedding.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种直觉在于，如果我们使用整个文本来计算嵌入，可能会引入过多的噪声，或者文本可能包含多个主题，这会导致嵌入的整体语义表示不佳。
- en: '![](img/B31105_04_06.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_04_06.png)'
- en: 'Figure 4.6: Query routing'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：查询路由
- en: 'On the **query optimization** side, we can leverage techniques such as query
    routing, query rewriting, and query expansion to refine the retrieved information
    for the LLM further:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在**查询优化**方面，我们可以利用查询路由、查询重写和查询扩展等技术来进一步细化LLM检索的信息：
- en: '**Query routing**: Based on the user’s input, we might have to interact with
    different categories of data and query each category differently. Query rooting
    is used to decide what action to take based on the user’s input, similar to if/else
    statements. Still, the decisions are made solely using natural language instead
    of logical statements.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询路由**: 根据用户输入，我们可能需要与不同类别的数据交互，并对每个类别进行不同的查询。查询路由用于根据用户输入决定采取什么行动，类似于if/else语句。然而，决策完全使用自然语言而不是逻辑语句。'
- en: As illustrated in Figure 4.6, let’s assume that, based on the user’s input,
    to do RAG, we can retrieve additional context from a vector DB using vector search
    queries, a standard SQL DB by translating the user query to an SQL command, or
    the internet by leveraging REST API calls. The query router can also detect whether
    a context is required, helping us avoid making redundant calls to external data
    storage. Also, a query router can be used to pick the best prompt template for
    a given input. For example, in the LLM Twin use case, depending on whether the
    user wants an article paragraph, a post, or a code snippet, you need different
    prompt templates to optimize the creation process. The routing usually uses an
    LLM to decide what route to take or embeddings by picking the path with the most
    similar vectors. To summarize, query routing is identical to an if/else statement
    but much more versatile as it works directly with natural language.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如图4.6所示，假设根据用户输入，进行RAG操作时，我们可以使用向量搜索查询从向量数据库中检索额外的上下文，通过将用户查询转换为SQL命令使用标准SQL数据库，或通过利用REST
    API调用从互联网上检索。查询路由器还可以检测是否需要上下文，帮助我们避免对外部数据存储进行冗余调用。此外，查询路由器可以用于为给定输入选择最佳的提示模板。例如，在LLM
    Twin用例中，根据用户是否想要文章段落、帖子或代码片段，需要不同的提示模板来优化创建过程。路由通常使用LLM来决定采取哪种路线或通过选择具有最相似向量的路径来选择嵌入。总之，查询路由类似于if/else语句，但更加灵活，因为它直接与自然语言工作。
- en: '**Query rewriting**: Sometimes, the user’s initial query might not perfectly
    align with the way your data is structured. Query rewriting tackles this by reformulating
    the question to match the indexed information better. This can involve techniques
    like:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询重写**: 有时，用户的初始查询可能无法完美地与您的数据结构对齐。查询重写通过重新表述问题以更好地匹配索引信息来解决这个问题。这可能涉及以下技术：'
- en: '**Paraphrasing**: Rephrasing the user’s query while preserving its meaning
    (e.g., “What are the causes of climate change?” could be rewritten as “Factors
    contributing to global warming”).'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**释义**: 在保留其意义的同时重新表述用户的查询（例如，“气候变化的原因是什么？”可以改写为“导致全球变暖的因素”）。'
- en: '**Synonym substitution**: Replacing less common words with synonyms to broaden
    the search scope (e.g., “ joyful” could be rewritten as “happy”).'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同义词替换**：用同义词替换不太常见的词以扩大搜索范围（例如，“愉快”可以改写为“快乐”）。'
- en: '**Sub-queries**: For longer queries, we can break them down into multiple shorter
    and more focused sub-queries. This can help the retrieval stage identify relevant
    documents more precisely.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子查询**：对于较长的查询，我们可以将其分解为多个更短、更专注的子查询。这有助于检索阶段更精确地识别相关文档。'
- en: '**Hypothetical document embeddings** (**HyDE**): This technique involves having
    an LLM create a hypothetical response to the query. Then, both the original query
    and the LLM’s response are fed into the retrieval stage.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设文档嵌入**（**HyDE**）：这种技术涉及让一个LLM对查询创建一个假设性响应。然后，将原始查询和LLM的响应都输入到检索阶段。'
- en: '**Query expansion**: This approach aims to enrich the user’s question by adding
    additional terms or concepts, resulting in different perspectives of the same
    initial question. For example, when searching for “disease,” you can leverage
    synonyms and related terms associated with the original query words and also include
    “illnesses” or “ailments.”'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询扩展**：这种方法旨在通过添加额外的术语或概念来丰富用户的问题，从而产生相同初始问题的不同视角。例如，当搜索“疾病”时，可以利用与原始查询词相关的同义词和相关术语，并包括“疾病”或“不适”。'
- en: '**Self-query**: The core idea is to map unstructured queries into structured
    ones. An LLM identifies key entities, events, and relationships within the input
    text. These identities are used as filtering parameters to reduce the vector search
    space (e.g., identify cities within the query, for example, “Paris,” and add it
    to your filter to reduce your vector search space).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自查询**：核心思想是将非结构化查询映射为结构化查询。一个大型语言模型（LLM）在输入文本中识别关键实体、事件和关系。这些身份被用作过滤参数以减少向量搜索空间（例如，识别查询中的城市，例如“巴黎”，并将其添加到过滤器中以减少向量搜索空间）。'
- en: Both data indexing and query optimization pre-retrieval optimization techniques
    depend highly on your data type, structure, and source. Thus, as with any data
    processing pipeline, no method always works, as every use case has its own particularities
    and gotchas. Optimizing your pre-retrieval RAG layer is experimental. Thus, what
    is essential is to try multiple methods (such as the ones enumerated in this section),
    reiterate, and observe what works best.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 数据索引和查询优化（检索前优化技术）高度依赖于您的数据类型、结构和来源。因此，与任何数据处理管道一样，没有一种方法总是有效，因为每个用例都有其特定的特点和难点。优化您的检索前RAG层是实验性的。因此，最重要的是尝试多种方法（如本节中列举的方法），反复试验，并观察哪种方法最有效。
- en: Retrieval
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: 'The retrieval step can be optimized in two fundamental ways:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 检索步骤可以通过两种基本方式优化：
- en: '**Improving the embedding models** used in the RAG ingestion pipeline to encode
    the chunked documents and, at inference time, transform the user’s input.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进RAG摄入管道中使用的嵌入模型**，以编码分块文档，并在推理时转换用户的输入。'
- en: '**Leveraging the DB’s filter and search features.** Thisstep will be used solely
    at inference time when you have to retrieve the most similar chunks based on user
    input.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用数据库的过滤和搜索功能**。此步骤仅在推理时使用，当您需要根据用户输入检索最相似的块时。'
- en: 'Both strategies are aligned with our ultimate goal: to enhance the vector search
    step by leveraging the semantic similarity between the query and the indexed data.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 两种策略都与我们的最终目标一致：通过利用查询与索引数据之间的语义相似性来增强向量搜索步骤。
- en: When improving the embedding models, you usually have to fine-tune the pre-trained
    embedding models to tailor them to specific jargon and nuances of your domain,
    especially for areas with evolving terminology or rare terms.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在改进嵌入模型时，通常需要微调预训练的嵌入模型，以使其适应特定领域的术语和细微差别，特别是对于术语不断演变或术语罕见的领域。
- en: Instead of fine-tuning the embedding model, you can leverage instructor models
    ([https://huggingface.co/hkunlp/instructor-xl](https://huggingface.co/hkunlp/instructor-xl))
    to guide the embedding generation process with an instruction/prompt aimed at
    your domain. Tailoring your embedding network to your data using such a model
    can be a good option, as fine-tuning a model consumes more computing and human
    resources.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与微调嵌入模型不同，您可以使用指导模型（[https://huggingface.co/hkunlp/instructor-xl](https://huggingface.co/hkunlp/instructor-xl)）通过针对您领域的指令/提示来引导嵌入生成过程。使用此类模型将嵌入网络定制到您的数据中可能是一个好选择，因为微调模型需要更多的计算和人力资源。
- en: 'In the code snippet below, you can see an example of an Instructor model that
    embeds article titles about AI:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，您可以看到一个嵌入关于人工智能文章标题的指导模型示例：
- en: '[PRE3]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The source code can be found at [https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_instructor_embeddings.py](https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_instructor_embeddings.py).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码可在[https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_instructor_embeddings.py](https://github.com/PacktPublishing/LLM-Engineering/blob/main/code_snippets/08_instructor_embeddings.py)找到。
- en: 'To run the instructor code, you have to create a different virtual environment
    and activate it:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行指导代码，您必须创建一个不同的虚拟环境并激活它：
- en: '[PRE4]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And install the required Python dependencies:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 安装所需的 Python 依赖项：
- en: '[PRE5]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On the other side of the spectrum, here is how you can improve your retrieval
    by leveraging classic filter and search DB features:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在光谱的另一端，以下是您如何通过利用经典的过滤和搜索数据库功能来提高检索效果的方法：
- en: '**Hybrid search**: This is a vector and keyword-based search blend. Keyword-based
    search excels at identifying documents containing specific keywords. When your
    task demands pinpoint accuracy and the retrieved information must include exact
    keyword matches, hybrid search shines. Vector search, while powerful, can sometimes
    struggle with finding exact matches, but it excels at finding more general semantic
    similarities. You leverage both keyword matching and semantic similarities by
    combining the two methods. You have a parameter, usually called alpha, that controls
    the weight between the two methods. The algorithm has two independent searches,
    which are later normalized and unified.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合搜索**：这是一种基于向量和关键词的搜索混合。基于关键词的搜索在识别包含特定关键词的文档方面表现出色。当任务需要精确度时，检索的信息必须包含确切的匹配关键词，混合搜索就非常出色。虽然向量搜索功能强大，但有时在寻找确切的匹配项时可能会遇到困难，但它擅长找到更一般的语义相似性。您通过结合两种方法利用关键词匹配和语义相似性。您有一个参数，通常称为
    alpha，它控制两种方法之间的权重。算法有两个独立的搜索，这些搜索随后被归一化和统一。'
- en: '**Filtered vector search**: This type of search leverages the metadata index
    to filter for specific keywords within the metadata. It differs from a hybrid
    search in that you retrieve the data once using only the vector index and perform
    the filtering step before or after the vector search to reduce your search space.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤向量搜索**：此类搜索利用元数据索引来过滤元数据中的特定关键词。它与混合搜索不同，因为它一次仅使用向量索引检索数据，并在向量搜索之前或之后执行过滤步骤以减少搜索空间。'
- en: In practice, on the retrieval side, you usually start with filtered vector search
    or hybrid search, as they are fairly quick to implement. This approach gives you
    the flexibility to adjust your strategy based on performance. If the results are
    not as expected, you can always fine-tune your embedding model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，在检索方面，您通常从过滤向量搜索或混合搜索开始，因为它们相对容易实现。这种方法使您可以根据性能调整策略。如果结果不符合预期，您始终可以微调您的嵌入模型。
- en: Post-retrieval
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后检索
- en: The post-retrieval optimizations are solely performed on the retrieved data
    to ensure that the LLM’s performance is not compromised by issues such as limited
    context windows or noisy data. This is because the retrieved context can sometimes
    be too large or contain irrelevant information, both of which can distract the
    LLM.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 后检索优化仅针对检索到的数据进行，以确保 LLM 的性能不会因诸如有限上下文窗口或噪声数据等问题而受到影响。这是因为检索到的上下文有时可能太大或包含不相关信息，这两者都可能分散
    LLM 的注意力。
- en: 'Two popular methods performed at the post-retrieval step are:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索步骤中执行的两个流行方法包括：
- en: '**Prompt compression**: Eliminate unnecessary details while keeping the essence
    of the data.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示压缩**：在保留数据本质的同时消除不必要的细节。'
- en: '**Re-ranking**: Use a cross-encoder ML model to give a matching score between
    the user’s input and every retrieved chunk. The retrieved items are sorted based
    on this score. Only the top N results are kept as the most relevant. As you can
    see in *Figure 4.7*, this works because the re-ranking model can find more complex
    relationships between the user input and some content than a simple similarity
    search. However, we can’t apply this model at the initial retrieval step because
    it is costly. That is why a popular strategy is to retrieve the data using a similarity
    distance between the embeddings and refine the retrieved information using a re-raking
    model, as illustrated in Figure 4.8.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重排序**：使用交叉编码器机器学习模型为用户输入和每个检索到的片段之间提供匹配分数。检索到的项目根据此分数排序。仅保留前 N 个结果作为最相关的结果。如图
    4.7 所示，这是因为重排序模型可以找到用户输入和某些内容之间比简单相似度搜索更复杂的关系。然而，我们无法在初始检索步骤中应用此模型，因为它成本高昂。这就是为什么一种流行的策略是使用嵌入之间的相似度距离检索数据，并使用重排序模型细化检索到的信息，如图
    4.8 所示。'
- en: '![](img/B31105_04_07.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7](img/B31105_04_07.png)'
- en: 'Figure 4.7: Bi-encoder (the standard embedding model) versus cross-encoder'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：双编码器（标准嵌入模型）与交叉编码器
- en: The abovementioned techniques are far from an exhaustive list of all potential
    solutions. We used them as examples to get an intuition on what you can (and should)
    optimize at each step in your RAG workflow. The truth is that these techniques
    can vary tremendously by the type of data you work with.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 上述技术远非所有潜在解决方案的详尽列表。我们使用它们作为例子，以获得对你在 RAG 工作流程的每个步骤中可以（和应该）优化的内容的直觉。事实是，这些技术可以根据你处理的数据类型而有很大差异。
- en: For example, if you work with multi-modal data such as text and images, most
    of the techniques from earlier won’t work as they are designed for text only.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你处理的是多模态数据，如文本和图像，那么之前的大部分技术将无法正常工作，因为它们是为文本设计的。
- en: '![](img/B31105_04_08.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8](img/B31105_04_08.png)'
- en: 'Figure 4.8: The re-ranking algorithm'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：重排序算法
- en: 'To summarize, the primary goal of these optimizations is to enhance the RAG
    algorithm at three key stages: pre-retrieval, retrieval, and post-retrieval. This
    involves preprocessing data for improved vector indexing, adjusting user queries
    for more accurate searches, enhancing the embedding model, utilizing classic filtering
    DB operations, and removing noisy data. By keeping these goals in mind, you can
    effectively optimize your RAG workflow for data processing and retrieval'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这些优化的主要目标是增强 RAG 算法在三个关键阶段：预检索、检索和后检索。这包括为改进向量索引预处理数据，调整用户查询以进行更精确的搜索，增强嵌入模型，利用经典的过滤数据库操作，以及移除噪声数据。通过牢记这些目标，你可以有效地优化你的
    RAG 工作流程以进行数据处理和检索。
- en: Exploring the LLM Twin’s RAG feature pipeline architecture
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 LLM Twin 的 RAG 功能管道架构
- en: Now that you have a strong intuition and understanding of RAG and its workings,
    we will continue exploring our particular LLM Twin use case. The goal is to provide
    a hands-on end-to-end example to solidify the theory presented in this chapter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你对 RAG 及其工作原理有了强烈的直觉和深入的理解，我们将继续探索我们特定的 LLM Twin 用例。目标是提供一个端到端的实战示例，以巩固本章中提出的理论。
- en: 'Any RAG system is split into two independent components:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 RAG 系统都分为两个独立的组件：
- en: The **ingestion pipeline** takes in raw data, cleans, chunks, embeds, and loads
    it into a vector DB.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄取管道**接收原始数据，清理、分块、嵌入，并将其加载到向量数据库中。'
- en: The **inference pipeline** queries the vector DB for relevant context and ultimately
    generates an answer by levering an LLM.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理管道**查询向量数据库以获取相关上下文，并最终通过利用大型语言模型生成答案。'
- en: In this chapter, we will focus on implementing the RAG ingestion pipeline, and
    in *Chapter 9*, we will continue developing the inference pipeline.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于实现 RAG 摄取管道，而在第 9 章中，我们将继续开发推理管道。
- en: With that in mind, let’s have a quick refresher on the problem we are trying
    to solve and where we get our raw data. Remember that we are building an end-to-end
    ML system. Thus, all the components talk to each other through an interface (or
    a contract), and each pipeline has a single responsibility. In our case, we ingest
    raw documents, preprocess them, and load them into a vector DB.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们快速回顾一下我们试图解决的问题以及我们获取原始数据的地方。记住，我们正在构建一个端到端的机器学习系统。因此，所有组件都通过接口（或合约）相互通信，每个管道只有一个职责。在我们的案例中，我们摄取原始文档，预处理它们，并将它们加载到向量数据库中。
- en: The problem we are solving
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们正在解决的问题
- en: As presented in the previous chapter, this book aims to show you how to build
    a production-ready LLM Twin backed by an end-to-end ML system. In this chapter
    specifically, we want to design a RAG feature pipeline that takes raw social media
    data (e.g., articles, code repositories, and posts) from our MongoDB data warehouse.
    The text of the raw documents will be cleaned, chunked, embedded, and ultimately
    loaded to a feature store. As discussed in *Chapter 1*, we will implement a logical
    feature store using ZenML artifacts and a Qdrant vector DB.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，本书旨在向您展示如何构建一个由端到端机器学习系统支持的、生产就绪的LLM Twin。在本章中，我们特别想要设计一个RAG特征管道，该管道从我们的MongoDB数据仓库中提取原始社交媒体数据（例如，文章、代码存储库和帖子）。原始文档的文本将被清理、分块、嵌入，并最终加载到特征存储中。如*第1章*中所述，我们将使用ZenML工件和Qdrant向量数据库实现一个逻辑特征存储。
- en: As we want to build a fully automated feature pipeline, we want to sync the
    data warehouse and logical feature store. Remember that, at inference time, the
    context used to generate the answer is retrieved from the vector DB. Thus, the
    speed of synchronization between the data warehouse and the feature store will
    directly impact the accuracy of our RAG algorithm.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建一个完全自动化的特征管道，因此我们需要同步数据仓库和逻辑特征存储。记住，在推理时间，用于生成答案的上下文是从向量数据库中检索的。因此，数据仓库和特征存储之间的同步速度将直接影响我们RAG算法的准确性。
- en: Another key consideration is how to automate the feature pipeline and integrate
    it with the rest of our ML system. Our goal is to minimize any desynchronization
    between the two data storages, as this could potentially compromise the integrity
    of our system.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键考虑因素是如何自动化特征管道并将其集成到我们的机器学习系统的其余部分。我们的目标是最大限度地减少两个数据存储之间的任何不同步，因为这可能会损害我们系统的完整性。
- en: To conclude, we must design a feature pipeline that constantly syncs the data
    warehouse and logical feature store while processing the data accordingly. Having
    the data in a feature store is critical for a production-ready ML system. The
    LLM Twin inference pipeline will query it for RAG, while the training pipeline
    will consume tracked and versioned fine-tuning datasets from it.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们必须设计一个特征管道，它不断地同步数据仓库和逻辑特征存储，同时相应地处理数据。在特征存储中拥有数据对于生产就绪的机器学习系统至关重要。LLM
    Twin推理管道将查询它以进行RAG，而训练管道将从中消费跟踪和版本化的微调数据集。
- en: The feature store
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征存储
- en: The **feature store** will be the **central access point** for all the features
    used within the training and inference pipelines. The training pipeline will use
    the cleaned data from the feature store (stored as artifacts) to fine-tune LLMs.
    The inference pipeline will query the vector DB for chunked documents for RAG.
    That is why we are designing a feature pipeline and not only a RAG ingestion pipeline.
    In practice, the feature pipeline contains multiple subcomponents, one of which
    is the RAG logic.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征存储**将是训练和推理管道中所有使用的特征的**中央访问点**。训练管道将使用特征存储中的清洗数据（存储为工件）来微调LLMs。推理管道将查询向量数据库以获取用于RAG的块状文档。这就是为什么我们设计的是一个特征管道，而不仅仅是RAG摄取管道。在实践中，特征管道包含多个子组件，其中之一是RAG逻辑。'
- en: Remember that the feature pipeline is mainly used as a mind map to navigate
    the complexity of ML systems. It clearly states that it takes raw data as input
    and then outputs features and optional labels, which are stored in the feature
    store. Thus, a good intuition is to consider that all the logic between the data
    warehouse and the feature store goes into the feature pipeline namespace, consisting
    of one or more sub-pipelines. For example, we will implement another pipeline
    that takes in cleaned data, processes it into instruct datasets, and stores it
    in artifacts; this also sits under the feature pipeline umbrella as the artifacts
    are part of the logical feature store. Another example would be implementing a
    data validation pipeline on top of the raw data or computed features.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，特征管道主要用于作为思维导图来导航机器学习系统的复杂性。它清楚地表明，它以原始数据作为输入，然后输出特征和可选标签，这些特征和标签存储在特征存储中。因此，一个很好的直觉是考虑所有在数据仓库和特征存储之间的逻辑都进入特征管道命名空间，包括一个或多个子管道。例如，我们将实现另一个管道，它接受清洗数据，将其处理成指令数据集，并将其存储在工件中；这也位于特征管道的伞下，因为工件是逻辑特征存储的一部分。另一个例子是在原始数据或计算特征之上实现数据验证管道。
- en: Another important observation to make is that text data stored as strings are
    not considered features if you follow the standard conventions. A feature is something
    that is fed directly into the model. For example, we would have to tokenize the
    instruct datasets or chunked documents to be considered features. Why? Because
    the tokens are fed directly to the model and not the sentences as strings. Unfortunately,
    this makes the system more complex and unflexible. Thus, we will do the tokenization
    at runtime. But this observation is important to understand as it’s a clear example
    that you don’t have to be too rigid about the feature/training/inference (FTI)
    architecture. You have to take it and adapt it to your own use case.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的观察点是，按照标准惯例，存储为字符串的文本数据如果不被视为特征。特征是直接输入到模型中的东西。例如，我们必须对指令数据集或分块文档进行标记化，才能被视为特征。为什么？因为标记直接输入到模型中，而不是作为字符串的句子。不幸的是，这使得系统更加复杂和不灵活。因此，我们将在运行时进行标记化。但这个观察结果很重要，因为它是一个明确的例子，说明你不必对特征/训练/推理（FTI）架构过于僵化。你必须根据自己的用例对其进行调整。
- en: Where does the raw data come from?
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始数据从哪里来？
- en: As a quick reminder, all the raw documents are stored in a MongoDB data warehouse.
    The data warehouse is populated by the data collection ETL pipeline presented
    in *Chapter 3*. The ETL pipeline crawls various platforms such as Medium and Substack,
    standardizes the data, and loads it into MongoDB. Check out *Chapter 3* for more
    details on this topic.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 作为快速提醒，所有原始文档都存储在 MongoDB 数据仓库中。数据仓库由第 3 章中介绍的 ETL 数据收集管道填充。ETL 管道爬取各种平台，如 Medium
    和 Substack，标准化数据，并将其加载到 MongoDB 中。有关此主题的更多详细信息，请参阅第 3 章。
- en: Designing the architecture of the RAG feature pipeline
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计 RAG 特征管道的架构
- en: The last step is to architect and go through the design of the RAG feature pipeline
    of the LLM Twin application. We will use a batch design scheduled to poll data
    from the MongoDB data warehouse, process it, and load it to a Qdrant vector DB.
    The first question to ask ourselves is, “Why a batch pipeline?”
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是设计和实施 LLM Twin 应用程序的 RAG 特征管道。我们将使用一个批量设计，该设计计划从 MongoDB 数据仓库中轮询数据，处理它，并将其加载到
    Qdrant 向量数据库中。我们首先要问自己的问题是，“为什么是批量管道？”
- en: But before answering that, let’s quickly understand how a batch architecture
    works and behaves relative to a streaming design.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 但在回答这个问题之前，让我们快速了解批量架构是如何工作以及相对于流式设计的表现的。
- en: '![](img/B31105_04_09.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_04_09.png)'
- en: 'Figure 4.9: The architecture of the LLM Twin’s RAG feature pipeline'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：LLM Twin 的 RAG 特征管道架构
- en: Batch pipelines
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量管道
- en: 'A batch pipeline in data systems refers to a data processing method where data
    is collected, processed, and stored in predefined intervals and larger volumes,
    also known as “batches”. This approach differs from real-time or streaming data
    processing, where data is processed continuously as it arrives. This is what happens
    in a batch pipeline:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 数据系统中的批量管道是指一种数据处理方法，其中数据按预定义的间隔和较大体积收集、处理和存储，也称为“批量”。这种方法与实时或流式数据处理不同，后者数据到达时连续处理。这就是批量管道中的情况：
- en: '**Data collection**: Data is collected from various sources and stored until
    sufficient amounts are accumulated for processing. This can include data from
    DBs, logs, files, and other sources.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集**：数据从各种来源收集并存储，直到积累到足够多的量以进行处理。这可以包括来自数据库、日志、文件和其他来源的数据。'
- en: '**Scheduled processing**: Data processing is scheduled at regular intervals,
    for example, hourly or daily. During this time, the collected data is processed
    in bulk. This can involve data cleansing, transformation, aggregation, and other
    operations.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计划处理**：数据处理按固定时间间隔进行，例如每小时或每天。在这段时间内，收集到的数据将批量处理。这可能包括数据清理、转换、聚合和其他操作。'
- en: '**Data loading**: After processing, the data is loaded into the target system,
    such as a DB, data warehouse, data lake, or feature store. This processed data
    is then available for analysis, querying, or further processing.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据加载**：处理完毕后，数据被加载到目标系统中，例如数据库、数据仓库、数据湖或特征存储。然后，这些处理后的数据可用于分析、查询或进一步处理。'
- en: 'Batch pipelines are particularly useful when dealing with large volumes of
    data that do not require immediate processing. They offer several advantages,
    including:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 批量管道在处理大量数据且不需要立即处理的情况下特别有用。它们提供了几个优点，包括：
- en: '**Efficiency**: Batch processing can handle large volumes of data more efficiently
    than real-time processing, allowing for optimized resource allocation and parallel
    processing.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：批量处理比实时处理更高效地处理大量数据，允许优化资源分配和并行处理。'
- en: '**Complex processing**: Batch pipelines can perform complex data transformations
    and aggregations that might be too resource-intensive for real-time processing.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂处理**：批量管道可以执行可能对实时处理来说过于资源密集的复杂数据转换和聚合。'
- en: '**Simplicity**: Batch processing systems’ architectures are often simpler than
    those of real-time systems, making them easier to implement and maintain.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单性**：批量处理系统的架构通常比实时系统简单，这使得它们更容易实现和维护。'
- en: Batch versus streaming pipelines
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量与流管道对比
- en: 'When implementing feature pipelines, you have two main design choices: batch
    and streaming. Thus, it is worthwhile to see the difference between the two and
    understand why we chose a batch architecture over a streaming one for our LLM
    Twin use case.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当实现特征管道时，你有两个主要的设计选择：批量处理和流处理。因此，了解这两种方法之间的差异以及为什么我们选择批量架构而不是流架构来处理我们的LLM Twin用例是值得的。
- en: You can effortlessly write a dedicated chapter on streaming pipelines, which
    suggests its complexity over a batch design. However, as streaming architectures
    become increasingly popular, one must have an intuition of how they work to choose
    the best option for your application.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地写一个关于流管道的专门章节，这表明其复杂性超过批量设计。然而，随着流架构变得越来越流行，人们必须对它们的工作方式有直观的了解，以便为你的应用程序选择最佳选项。
- en: 'The core elements of streaming applications are a distributed event streaming
    platform such as Apache Kafka or Redpanda to store events from multiple clients
    and a streaming engine such as Apache Flink or Bytewax to process the events.
    To simplify your architecture, you can swap your event streaming platform with
    queues, such as RabbitMQ, to store the events until processed. *Table 4.1* compares
    batch and streaming pipelines based on multiple criteria such as processing schedule
    and complexity:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 流应用的核心元素是一个分布式事件流平台，如Apache Kafka或Redpanda，用于存储来自多个客户端的事件，以及一个流引擎，如Apache Flink或Bytewax，用于处理这些事件。为了简化你的架构，你可以用队列，如RabbitMQ，来替换你的事件流平台，以存储事件直到处理。*表4.1*根据多个标准比较了批量处理和流处理管道，如处理调度和复杂性：
- en: '| **Aspect** | **Batch pipeline** | **Streaming pipeline** |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **批量管道** | **流管道** |'
- en: '| **Processing schedule** | Processes data at regular intervals (e.g., every
    minute, hourly, daily). | Processes data continuously, with minimal latency. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| **处理调度** | 定期间隔处理数据（例如，每分钟、每小时、每天）。 | 持续处理数据，延迟最小。 |'
- en: '| **Efficiency** | Handles large volumes of data more efficiently, optimizing
    resource allocation and parallel processing. | Handles single data points, providing
    immediate insights and updates, allowing for rapid response to changes. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| **效率** | 更高效地处理大量数据，优化资源分配和并行处理。 | 处理单个数据点，提供即时洞察和更新，允许快速响应变化。 |'
- en: '| **Processing complexity** | Capable of performing complex data transformations
    and aggregations. | Designed to handle high-velocity data streams with low latency.
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| **处理复杂性** | 能够执行复杂的数据转换和聚合。 | 设计用于以低延迟处理高速数据流。 |'
- en: '| **Use cases** | Suitable for scenarios where immediate data processing is
    not critical. Commonly used in data warehousing, reporting, ETL processes, and
    feature pipelines. | Ideal for applications requiring real-time analytics, features,
    monitoring, and event-driven architectures. |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| **用例** | 适用于不需要即时数据处理的场景。常用于数据仓库、报告、ETL流程和特征管道。 | 适用于需要实时分析、功能、监控和事件驱动架构的应用。
    |'
- en: '| **System complexity** | Compared to streaming pipelines, systems are generally
    simpler to implement and maintain. | More complex to implement and maintain due
    to the need for low-latency processing, fault tolerance, and scalability. The
    tooling is also more advanced and complicated. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| **系统复杂性** | 与流管道相比，系统通常更容易实现和维护。 | 由于需要低延迟处理、容错性和可伸缩性，实现和维护更复杂。工具也更为先进和复杂。
    |'
- en: 'Table 4.1: Batch versus streaming pipelines'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1：批量与流管道对比
- en: For example, streaming pipelines are extremely powerful in social media recommender
    systems like TikTok. When using social media, user behavior changes frequently.
    A typical scenario is that you want to relax at a certain point in time and mostly
    look at videos of puppies. Still, after 15 minutes, you get bored and want something
    more serious, such as educative content or news. This means the recommender system
    has to capture these behavior changes without delay to keep you engaged. As the
    transition between interests is cyclical and not predictable, you can’t use a
    batch pipeline that runs every 30 minutes or every hour to generate more content.
    You can run it every minute to create new content, but, at the same time, it will
    result in unnecessary costs, as most predictions will not be consumed. By implementing
    a streaming pipeline, you update the features of specific users in real time,
    which are then passed to a chain of models that predict the new recommendations.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，流式管道在像 TikTok 这样的社交媒体推荐系统中非常强大。在使用社交媒体时，用户行为经常变化。一个典型的场景是，你希望在某个时间点放松，主要观看小狗的视频。然而，15
    分钟后，你会感到无聊，想要一些更严肃的内容，比如教育内容或新闻。这意味着推荐系统必须及时捕捉这些行为变化，以保持你的兴趣。由于兴趣之间的转换是循环的且不可预测的，你不能使用每
    30 分钟或每小时运行一次的批处理管道来生成更多内容。你可以每分钟运行一次以创建新内容，但与此同时，它将导致不必要的成本，因为大多数预测将不会被消费。通过实现流式管道，你可以实时更新特定用户的特征，然后这些特征被传递到一系列模型中，以预测新的推荐内容。
- en: Streaming architectures are also the backbone of real-time fraud detection algorithms,
    such as those used at Stripe or PayPal. In this context, it’s critical to identify
    potentially fraudulent transactions as they occur, not after a few minutes or
    hours as a batch pipeline would process them. The same urgency applies to high-frequency
    trading platforms that make stock predictions based on the constant influx of
    market data, enabling traders to make decisions within milliseconds.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 流式架构也是实时欺诈检测算法（如 Stripe 或 PayPal 中使用的算法）的骨架。在这种情况下，关键是要在交易发生时识别潜在的欺诈交易，而不是在几分钟或几小时之后，正如批处理管道会处理的那样。同样的紧迫性也适用于基于不断涌入的市场数据进行股票预测的高频交易平台，使交易者能够在毫秒内做出决策。
- en: On the other hand, you can use a batch architecture for an offline recommender
    system. For example, when implementing one for an e-commerce or streaming platform,
    you don’t need the system to be so reactive, as the user’s behavior rarely changes.
    Thus, updating the recommendations periodically, such as every night, based on
    historical user behavior data using a batch pipeline is easier to implement and
    cheaper.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你可以为离线推荐系统使用批处理架构。例如，在为电子商务或流媒体平台实现时，你不需要系统如此反应灵敏，因为用户的行为很少改变。因此，基于历史用户行为数据，定期（如每晚）使用批处理管道更新推荐内容更容易实现且成本更低。
- en: Another popular example of batch pipelines is the ETL design used to extract,
    transform, and load data for different use cases. The ETL design is widespread
    in data pipelines used to move data from one DB to another. Some practical use
    cases include aggregating data for analytics, where you have to extract data from
    multiple sources, aggregate it, and load it to a data warehouse connected to a
    dashboard. The analytics domains can be widespread, from e-commerce and marketing
    to finance and research.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理管道的另一个流行例子是用于提取、转换和加载数据以用于不同用例的 ETL 设计。ETL 设计在用于将数据从一个数据库移动到另一个数据库的数据管道中很常见。一些实际用例包括用于分析的聚合数据，其中你必须从多个来源提取数据，对其进行聚合，并将其加载到连接到仪表板的数据库中。分析领域可以非常广泛，从电子商务和营销到金融和研究。
- en: The data collection pipeline used in the LLM Twin use case is another example
    of an ETL pipeline that extracts data from the internet, structures it, and loads
    it into a data warehouse for future processing.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM Twin 用例中使用的数据收集管道是另一个提取数据来自互联网、对其进行结构化并将其加载到数据仓库以供未来处理的 ETL 管道的例子。
- en: Along with prediction or feature freshness, another disadvantage of batch pipelines
    over streaming ones is that you usually make redundant predictions. Let’s take
    the example of a recommender system for a streaming platform like Netflix. Every
    night, you make the predictions for all users. There is a significant chance that
    a large chunk of users won’t log in that day. Also, users usually don’t browse
    all the recommendations but stick to the first ones. Thus, only a portion of predictions
    are used, wasting computing power on all the others.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预测或特征新鲜度之外，批量管道相对于流式管道的另一个缺点是，你通常会做出冗余的预测。以Netflix等流媒体平台的推荐系统为例。每晚，你为所有用户做出预测。有很大可能性，一大群用户那天不会登录。此外，用户通常不会浏览所有推荐，而是坚持查看前几个。因此，只有一部分预测被使用，而其他所有预测都浪费了计算能力。
- en: That’s why a popular strategy is to start with a batch architecture, as it’s
    faster and easier to implement. After the product is in place, you gradually move
    to a streaming design to reduce costs and improve the user experience.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，一种流行的策略是从批量架构开始，因为它更快、更容易实现。产品到位后，你逐渐过渡到流式设计，以降低成本并提高用户体验。
- en: 'To conclude, we have used a batch architecture (and not a streaming one) to
    implement the LLM Twin’s feature pipeline for the following reasons:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们之所以选择批量架构（而非流式架构）来实现LLM Twin的特征管道，有以下原因：
- en: '**Does not require immediate data processing**: Even if syncing the data warehouse
    and feature store is critical for an accurate RAG system, a delay of a few minutes
    is acceptable. Thus, we can schedule the batch pipeline to run every minute, constantly
    syncing the two data storages. This technique works because the data volume is
    small. The whole data warehouse will have only thousands of records, not millions
    or billions. Hence, we can quickly iterate through them and sync the two DBs.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不需要立即数据处理**：即使同步数据仓库和特征存储对于准确的RAG系统至关重要，几分钟的延迟是可以接受的。因此，我们可以安排批量管道每分钟运行一次，持续同步两个数据存储。这种技术之所以有效，是因为数据量小。整个数据仓库将只有几千条记录，而不是数百万或数十亿。因此，我们可以快速迭代它们并同步两个数据库。'
- en: '**Simplicity**: As stated earlier, implementing a streaming pipeline is two
    times more complex. In the real world, you want to keep your system as simple
    as possible, making it easier to understand, debug, and maintain. Also, simplicity
    usually translates to lower infrastructure and development costs.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单性**：如前所述，实现流式管道是两倍复杂。在现实世界中，你希望尽可能简化你的系统，使其更容易理解、调试和维护。此外，简单性通常意味着更低的基础设施和开发成本。'
- en: In *Figure 8.10*, we compare what tools you can use based on your architecture
    (streaming versus batch) and the quantity of data you have to process (small versus
    big data). In our use case, we are in the smaller data and batch quadrant, where
    we picked a combination of vanilla Python and generative AI tools such as LangChain,
    Sentence Transformers, and Unstructured.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8.10*中，我们比较了根据你的架构（流式处理与批量处理）以及你要处理的数据量（小数据与大数据）你可以使用哪些工具。在我们的用例中，我们处于小数据和批量象限，我们选择了纯Python和生成式AI工具的组合，如LangChain、Sentence
    Transformers和Unstructured。
- en: '![](img/B31105_04_10.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_10.png)'
- en: 'Figure 4.10: Tools on the streaming versus batch and smaller versus bigger
    data spectrum'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10：流式处理与批量处理以及小数据与大数据谱系中的工具
- en: 'In the *Change data capture: syncing the data warehouse and feature store*section
    later in this chapter, we will discuss when switching from a batch architecture
    to a streaming one makes sense.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面的*“变更数据捕获：同步数据仓库和特征存储”*部分，我们将讨论何时从批量架构切换到流式架构是有意义的。
- en: Core steps
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心步骤
- en: 'Most of the RAG feature pipelines are composed of five core steps. The one
    implemented in the LLM Twin architecture makes no exception. Thus, you can quickly
    adapt this pattern for other RAG applications, but here is what the LLM Twin’s
    RAG feature pipeline looks like:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数RAG特征管道由五个核心步骤组成。在LLM Twin架构中实现的也不例外。因此，你可以快速地将这种模式应用于其他RAG应用，但以下是LLM Twin的RAG特征管道的样子：
- en: '**Data extraction**: Extract the latest articles, code repositories, and posts
    from the MongoDB data warehouse. At the extraction step, you usually aggregate
    all the data you need for processing.'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据提取**：从MongoDB数据仓库中提取最新的文章、代码仓库和帖子。在提取步骤中，你通常会聚合所有需要处理的数据。'
- en: '**Cleaning**: The data from the data warehouse is standardized and partially
    clean, but we have to ensure that the text contains only useful information, is
    not duplicated, and can be interpreted by the embedding model. For example, we
    must clean and normalize all non-ASCII characters before passing the text to the
    embedding model. Also, to keep the information semantically dense, we decided
    to replace all the URLs with placeholders and remove all emojis. The cleaning
    step is more art than **science**. Hence, after you have the first iteration with
    an evaluation mechanism in place, you will probably reiterate and improve it.'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**清洗**: 数据仓库中的数据已经标准化和部分清洗，但我们必须确保文本只包含有用的信息，不重复，并且可以被嵌入模型解释。例如，在将文本传递给嵌入模型之前，我们必须清理和标准化所有非ASCII字符。此外，为了保持信息在语义上密集，我们决定用占位符替换所有URL，并移除所有表情符号。清洗步骤更多的是艺术而非**科学**。因此，在你有了带有评估机制的第一次迭代之后，你可能会反复迭代并改进它。'
- en: '**Chunking**: You must adopt various chunking strategies based on each data
    category and embedding model. For example, when working with code repositories,
    you want the chunks broader, whereas when working with articles, you want them
    narrower or scoped at the paragraph level. Depending on your data, you must decide
    if you split your document based on the chapter, section, paragraph, sentence,
    or just a fixed window size. Also, you have to ensure that the chunk size doesn’t
    exceed the maximum input size of the embedding model. That is why you usually
    chunk a document based on your data structure and the maximum input size of the
    model.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分块**: 你必须根据每个数据类别和嵌入模型采用各种分块策略。例如，当与代码仓库一起工作时，你希望分块更宽泛，而当与文章一起工作时，你希望它们更窄或定位在段落级别。根据你的数据，你必须决定是否根据章节、部分、段落、句子或只是一个固定的窗口大小来分割你的文档。此外，你必须确保分块大小不超过嵌入模型的最大输入大小。这就是为什么你通常根据你的数据结构和模型的最大输入大小来分块文档。'
- en: '**Embedding**: You pass each chunk individually to an embedding model of your
    choice. Implementation-wise, this step is usually the simplest, as tools such
    as SentenceTransformer and Hugging Face provide high-level interfaces for most
    embedding models. As explained in the *What are embeddings?* section of this chapter,
    at this step, the most critical decisions are to decide what model to use and
    whether to fine-tune it or not. For example, we used an `"all-mpnet-base-v2"`
    embedding model from *SentenceTransformer*, which is relatively tiny and runs
    on most machines. However, we provide a configuration file where you can quickly
    configure the embedding model with something more powerful based on the state
    of the art when reading this book. You can quickly find other options on the MTEB
    on Hugging Face ([https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入**: 你将每个分块单独传递到你选择的嵌入模型。在实现方面，这一步通常是最简单的，因为SentenceTransformer和Hugging
    Face等工具为大多数嵌入模型提供了高级接口。正如本章“什么是嵌入？”部分所解释的，在这一步，最重要的决定是决定使用什么模型以及是否对其进行微调。例如，我们使用了来自SentenceTransformer的`"all-mpnet-base-v2"`嵌入模型，它相对较小，可以在大多数机器上运行。然而，我们提供了一个配置文件，你可以根据阅读本书时的最新技术状态快速配置嵌入模型。你可以在Hugging
    Face的MTEB上快速找到其他选项（[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)）。'
- en: '**Data loading**: The final step combines the embedding of a chunked document
    and its metadata, such as the author and the document ID, content, URL, platform,
    and creation date. Ultimately, we wrap the vector and the metadata into a structure
    compatible with Qdrant and push it to the vector DB. As we want to use Qdrant
    as the single source of truth for the features, we also push the cleaned documents
    (before chunking) to Qdrant. We can push data without vectors, as the metadata
    index of Qdrant behaves like a NoSQL DB. Thus, pushing metadata without a vector
    attached to it is like using a standard NoSQL engine.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据加载**: 最后一步是将分块文档的嵌入及其元数据（如作者、文档ID、内容、URL、平台和创建日期）结合起来。最终，我们将向量和元数据包装成一个与Qdrant兼容的结构，并将其推送到向量数据库。由于我们希望将Qdrant作为特征的单一事实来源，我们也把清洗过的文档（在分块之前）推送到Qdrant。我们可以推送没有向量的数据，因为Qdrant的元数据索引就像一个NoSQL数据库。因此，推送没有附加向量的元数据就像使用标准的NoSQL引擎一样。'
- en: 'Change data capture: syncing the data warehouse and feature store'
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据捕获变更：同步数据仓库和特征存储
- en: As highlighted a few times in this chapter, data is constantly changing, which
    can result in DBs, data lakes, data warehouses, and feature stores getting out
    of sync. **Change data capture** (**CDC**) is a strategy that allows you to optimally
    keep two or more data storage types in sync without computing and I/O overhead.
    It captures any CRUD operation done on the source DB and replicates it on a target
    DB. Optionally, you can add preprocessing steps in between the replication.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章多次强调的那样，数据是不断变化的，这可能导致数据库（DBs）、数据湖、数据仓库和特征存储失去同步。**变更数据捕获**（**CDC**）是一种策略，允许您在不进行计算和I/O开销的情况下，最优地保持两种或多种数据存储类型同步。它捕获对源数据库执行的任何CRUD操作，并将其复制到目标数据库。可选地，您可以在复制之间添加预处理步骤。
- en: The syncing issues also apply when building a feature pipeline. One key design
    choice concerns how to sync the data warehouse with the feature store to have
    data fresh enough for your particular use case.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 同步问题也适用于构建特征管道。一个关键的设计选择是关于如何同步数据仓库与特征存储，以便为您的特定用例提供足够新鲜的数据。
- en: 'In our LLM Twin use case, we chose a naïve approach out of simplicity. We implemented
    a batch pipeline that is triggered periodically or manually. It reads all the
    raw data from the data warehouse, processes it in batches, and inserts new records
    or updates old ones from the Qdrant vector DB. This works fine when you are working
    with a small number of records, at the order of thousands or tens of thousands.
    But our naïve approach raises the following questions:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的LLM Twin用例中，我们出于简单起见选择了一种天真方法。我们实现了一个周期性或手动触发的批量管道。它从数据仓库中读取所有原始数据，批量处理它们，并将新记录或更新插入到Qdrant向量数据库中。当您处理数千或数万条记录时，这种方法效果良好。但我们的天真方法提出了以下问题：
- en: What happens if the data suddenly grows to millions of records (or higher)?
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据突然增长到数百万条记录（或更多）怎么办？
- en: What happens if a record is deleted from the data warehouse? How is this reflected
    in the feature store?
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果从数据仓库中删除了一条记录？这在特征存储中如何反映？
- en: What if we want to process only the new or updated items from the data warehouse
    and not all of them?
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们只想处理数据仓库中的新或更新项，而不是所有项怎么办？
- en: 'Fortunately, the CDC pattern can solve all of these issues. When implementing
    CDC, you can take multiple approaches, but all of them use either a push or pull
    strategy:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，CDC模式可以解决所有这些问题。在实施CDC时，您可以采取多种方法，但所有这些方法都使用推送或拉取策略：
- en: '**Push:** The source DB is the primary driver in the push approach. It actively
    identifies and transmits data modifications to target systems for processing.
    This method ensures near-instantaneous updates at the target, but data loss can
    occur if target systems are inaccessible. To mitigate this, a messaging system
    is typically employed as a buffer.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推送**：在推送方法中，源数据库是主要驱动因素。它积极识别并向目标系统传输数据修改以进行处理。此方法确保在目标处几乎瞬间更新，但如果目标系统不可访问，则可能会发生数据丢失。为了减轻这种情况，通常使用消息系统作为缓冲。'
- en: '**Pull:** The pull method assigns a more passive role to the source DB, which
    only records data changes. Target systems periodically request these changes and
    handle updates accordingly. While this approach lightens the load on the source,
    it introduces a delay in data propagation. A messaging system is again essential
    to prevent data loss during periods of target system unavailability.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拉取**：拉取方法赋予源数据库更被动的角色，它仅记录数据更改。目标系统定期请求这些更改并相应地处理更新。虽然这种方法减轻了源系统的负担，但引入了数据传播的延迟。在目标系统不可用期间，消息系统再次是防止数据丢失的关键。'
- en: 'In summary, the push method is ideal for applications demanding immediate data
    access, whereas the pull method is better suited for large-scale data transfers
    where real-time updates aren’t critical. With that in mind, there are different
    methods to detect changes in data. Thus, let’s list the main CDC patterns that
    are used in the industry:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，推送方法适用于需要即时数据访问的应用程序，而拉取方法更适合大规模数据传输，其中实时更新不是关键。考虑到这一点，有不同方法来检测数据变化。因此，让我们列出在行业中使用的主要CDC模式：
- en: '**Timestamp-based**: The approach involves adding a modification time column
    to DB tables, usually called `LAST_MODIFIED` or `LAST_UPDATED`. Downstream systems
    can query this column to identify records that have been updated since their last
    check. While simple to implement, this method is limited to tracking changes,
    not deletions, and imposes performance overhead due to the need to scan entire
    tables.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于时间戳的**: 这种方法涉及在数据库表中添加一个修改时间列，通常称为 `LAST_MODIFIED` 或 `LAST_UPDATED`。下游系统可以查询此列以识别自上次检查以来已更新的记录。虽然实现简单，但此方法仅限于跟踪更改，而不是删除，并且由于需要扫描整个表而增加了性能开销。'
- en: '**Trigger-based**: The trigger-based approach utilizes DB triggers to automatically
    record data modifications in a separate table upon INSERT, UPDATE, or DELETE operations,
    often known as the event table. This method provides comprehensive change tracking
    but can impact the DB performance due to the additional write operations involved
    for each event.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于触发器的**: 基于触发器的方法利用数据库触发器在 INSERT、UPDATE 或 DELETE 操作时自动记录数据修改到一个单独的表中，通常称为事件表。此方法提供了全面的变化跟踪，但由于每个事件都涉及额外的写入操作，可能会影响数据库性能。'
- en: '**Log-based**: DBs maintain transaction logs to record all data modifications,
    including timestamps. Primarily used for recovery, these logs can also be leveraged
    to propagate changes to target systems in real time. This approach minimizes the
    performance impact on the source DB. As a huge advantage, it avoids additional
    processing overhead on the source DB, captures all data changes, and requires
    no schema modification. But on the opposite side, it lacks standardized log formats,
    leading to vendor-specific implementations.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于日志的**: 数据库维护事务日志以记录所有数据修改，包括时间戳。主要用于恢复，这些日志还可以用于实时传播到目标系统中的更改。这种方法最小化了源数据库的性能影响。作为一个巨大的优势，它避免了源数据库上的额外处理开销，捕获了所有数据更改，并且不需要对模式进行修改。但另一方面，它缺乏标准化的日志格式，导致供应商特定的实现。'
- en: 'For more details on CDC, I recommend *What is Change Data Capture?* from Confluent’s
    blog: [https://www.confluent.io/en-gb/learn/change-data-capture/](https://www.confluent.io/en-gb/learn/change-data-capture/).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 CDC 的更多详细信息，我推荐阅读 Confluent 的博客中的 *什么是变更数据捕获？*：[https://www.confluent.io/en-gb/learn/change-data-capture/](https://www.confluent.io/en-gb/learn/change-data-capture/)。
- en: With these CDC techniques in mind, we could quickly implement a pull timestamp-based
    strategy in our RAG feature pipeline to sync the data warehouse and feature store
    more optimally when the data grows. Our implementation is still pull-based but
    doesn’t check any last updated field in the source DB; it just pulls everything
    from the data warehouse.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些 CDC 技术，我们可以在我们的 RAG 功能管道中快速实现一个基于拉取时间戳的策略，以在数据增长时更优化地同步数据仓库和特征存储库。我们的实现仍然是基于拉取的，但不会检查源数据库中的任何最后更新字段；它只是从数据仓库中拉取所有内容。
- en: However, the most popular and optimal technique in the industry is the log-based
    one. It doesn’t add any I/O overhead to the source DB, has low latency, and supports
    all CRUD operations. The biggest downside is its development complexity, which
    requires a queue to capture all the CRUD events and a streaming pipeline to process
    them.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在行业中最为流行和最优的技术是日志基础技术。它不会给源数据库增加任何 I/O 开销，具有低延迟，并支持所有 CRUD 操作。最大的缺点是其开发复杂性，需要队列来捕获所有
    CRUD 事件，并需要一个流式处理管道来处理它们。
- en: As this is an LLM book and not a data engineering one, we wanted to keep things
    simple, but it’s important to know that these techniques exist, and you can always
    upgrade your current implementation when it doesn’t fit your application requirements
    anymore.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一本关于大型语言模型的书，而不是数据工程的书，我们希望保持简单，但重要的是要知道这些技术存在，并且当你的当前实现不再满足应用程序需求时，你总是可以升级你的实现。
- en: Why is the data stored in two snapshots?
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么数据存储在两个快照中？
- en: 'We store two snapshots of our data in the logical feature store:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在逻辑特征存储库中存储了我们数据的两个快照：
- en: '**After the data is cleaned**: For fine-tuning LLMs'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清理后**: 用于微调大型语言模型'
- en: '**After the documents are chunked and embedded**: For RAG'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在文档分块和嵌入后**: 用于 RAG'
- en: '*Why did we design it this way?* Remember that the features should be accessed
    solely from the feature store for training and inference. Thus, this adds consistency
    to our design and makes it cleaner.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们为什么这样设计？* 记住，特征应该仅从特征存储库中访问用于训练和推理。因此，这增加了我们设计的连贯性，并使其更简洁。'
- en: Also, storing the data cleaned specifically for our fine-tuning and embedding
    use case in the MongoDB data warehouse would have been an antipattern. The data
    from the warehouse is shared all across the company. Thus, processing it for a
    specific use case is not good practice. Imagine another summarization use case
    where we must clean and preprocess the data differently. We must create a new
    “Cleaned Data” table prefixed with the use case name. We have to repeat that for
    every new use case. Therefore, to avoid having a spaghetti data warehouse, the
    data from the data warehouse is generic and is modeled to specific applications
    only in downstream components, which, in our case, is the feature store.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将专门用于我们的微调和嵌入用例的数据清洗存储在 MongoDB 数据仓库中将会是一个反模式。仓库中的数据在整个公司中共享。因此，为特定用例处理它不是好的做法。想象一下另一个摘要用例，我们必须以不同的方式清洗和预处理数据。我们必须为每个新的用例创建一个新的“清洗数据”表，并以前缀使用例名称命名。我们必须为每个新的用例重复此操作。因此，为了避免有一个混乱的数据仓库，数据仓库中的数据是通用的，并且仅在下游组件中针对特定应用进行建模，在我们的情况下，是特征存储。
- en: Ultimately, as we mentioned in the *Core steps* section, you can leverage the
    metadata index of a vector DB as a NoSQL DB. Based on these factors, we decided
    to keep the cleaned data in Qdrant, along with the chunked and embedded versions
    of the documents.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，正如我们在 *核心步骤* 部分提到的，你可以利用向量数据库的元数据索引作为 NoSQL 数据库。基于这些因素，我们决定将清洗后的数据以及文档的块化和嵌入版本都保存在
    Qdrant 中。
- en: As a quick reminder, when operationalizing our LLM Twin system, the create instruct
    dataset pipeline, explained in *Chapter 5*, will read the cleaned documents from
    Qdrant, process them, and save them under a versioned ZenML artifact. The training
    pipeline requires a dataset and not plain documents. This is a reminder that our
    logical feature store comprises the Qdrant vector DB for online serving and ZenML
    artifacts for offline training.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 作为快速提醒，当将我们的 LLM Twin 系统投入运营时，*第五章* 中解释的创建指令数据集管道将从 Qdrant 读取清洗后的文档，处理它们，并将它们保存为版本化的
    ZenML 艺术品。训练管道需要一个数据集而不是普通文档。这是一个提醒，我们的逻辑特征存储包括 Qdrant 向量数据库用于在线服务以及 ZenML 艺术品用于离线训练。
- en: Orchestration
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编排
- en: ZenML will orchestrate the batch RAG feature pipeline. Using ZenML, we can schedule
    it to run on a schedule, for example, every hour, or quickly manually trigger
    it. Another option is to trigger it after the ETL data collection pipeline finishes.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ZenML 将编排批处理 RAG 特征管道。使用 ZenML，我们可以安排它按计划运行，例如每小时运行一次，或者快速手动触发它。另一个选项是在 ETL
    数据收集管道完成后触发它。
- en: By orchestrating the feature pipeline and integrating it into ZenML (or any
    other orchestration tool), we can operationalize the feature pipeline with the
    end goal of **continuous training** (**CT**).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通过编排特征管道并将其集成到 ZenML（或任何其他编排工具）中，我们可以将特征管道投入运营，最终目标是**持续训练**（**CT**）。
- en: We will go into all the details of orchestration, scheduling, and CT in *Chapter
    11*.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 *第 11 章* 中详细介绍编排、调度和 CT。
- en: Implementing the LLM Twin’s RAG feature pipeline
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 LLM Twin 的 RAG 特征管道
- en: 'The last step is to review the LLM Twin’s RAG feature pipeline code to see
    how we applied everything we discussed in this chapter. We will walk you through
    the following:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是审查 LLM Twin 的 RAG 特征管道代码，看看我们如何应用本章中讨论的所有内容。我们将向您介绍以下内容：
- en: ZenML code
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZenML 代码
- en: Pydantic domain objects
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pydantic 领域对象
- en: A custom **object-vector mapping** (**OVM**) implementation
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个定制的**对象-向量映射**（**OVM**）实现
- en: The cleaning, chunking, and embedding logic for all our data categories
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们所有数据类别的清洗、分块和嵌入逻辑
- en: We will take a top-down approach. Thus, let’s start with the Settings class
    and ZenML pipeline.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取自上而下的方法。因此，让我们从 Settings 类和 ZenML 管道开始。
- en: Settings
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: We use Pydantic Settings ([https://docs.pydantic.dev/latest/concepts/pydantic_settings/](https://docs.pydantic.dev/latest/concepts/pydantic_settings/))
    to define a global Settings class that loads sensitive or non-sensitive variables
    from a `.env` file. This approach also gives us all the benefits of Pydantic,
    such as type validation. For example, if we provide a string for the `QDRANT_DATABASE_PORT`
    variable instead of an integer, the program will crash. This behavior makes the
    whole application more deterministic and reliable.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Pydantic Settings ([https://docs.pydantic.dev/latest/concepts/pydantic_settings/](https://docs.pydantic.dev/latest/concepts/pydantic_settings/))
    来定义一个全局的 Settings 类，该类从 `.env` 文件中加载敏感或非敏感变量。这种方法也为我们带来了 Pydantic 的所有好处，例如类型验证。例如，如果我们为
    `QDRANT_DATABASE_PORT` 变量提供一个字符串而不是整数，程序将会崩溃。这种行为使得整个应用程序更加确定性和可靠。
- en: 'Here is what the `Settings` class looks like with all the variables necessary
    to build the RAG feature pipeline:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `Settings` 类的样子，其中包含构建 RAG 特征管道所需的所有变量：
- en: '[PRE6]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As stated in the internal Config class, all the variables have default values
    or can be overridden by providing a `.env` file.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如内部 Config 类所述，所有变量都有默认值，或者可以通过提供 `.env` 文件来覆盖。
- en: ZenML pipeline and steps
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ZenML 管道和步骤
- en: 'The ZenML pipeline is the entry point for the RAG feature engineering pipeline.
    It reflects the five core phases of RAG ingestion code: extracting raw documents,
    cleaning, chunking, embedding, and loading them to the logical feature store.
    The calls within the `feature_engineering()` function are ZenML steps, representing
    a single execution unit performing the five phases of RAG. The code is available
    in the GitHub repository at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/pipelines/feature_engineering.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/pipelines/feature_engineering.py):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ZenML 管道是 RAG 特征工程管道的入口点。它反映了 RAG 吸收代码的五个核心阶段：提取原始文档、清理、分块、嵌入并将它们加载到逻辑特征存储中。`feature_engineering()`
    函数内的调用是 ZenML 步骤，代表执行五个阶段的单个执行单元。代码可在 GitHub 仓库中找到：[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/pipelines/feature_engineering.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/pipelines/feature_engineering.py)
- en: '[PRE7]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Figure 4.11* shows how multiple feature engineering pipeline runs look in
    ZenML’s dashboard.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.11* 展示了在 ZenML 仪表板中多个特征工程管道运行的外观。'
- en: '![](img/B31105_04_11.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_11.png)'
- en: 'Figure 4.11: Feature pipeline runs in the ZenML dashboard'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：ZenML 仪表板中的特征管道运行
- en: '*Figure 8.12* shows the DAG of the RAG feature pipeline, where you can follow
    all the pipeline steps and their output artifacts. Remember that whatever is returned
    from a ZenML step is automatically saved as an artifact, stored in ZenML’s artifact
    registry, versioned, and shareable across the application.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.12* 展示了 RAG 特征管道的 DAG，你可以跟踪所有管道步骤及其输出工件。请记住，从 ZenML 步骤返回的任何内容都会自动保存为工件，存储在
    ZenML 的工件注册表中，进行版本控制，并在应用程序中共享。'
- en: '![](img/B31105_04_12.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_12.png)'
- en: 'Figure 4.12: Feature pipeline DAG in the ZenML dashboard'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：ZenML 仪表板中的特征管道 DAG
- en: 'The final puzzle piece is understanding how to configure the RAG feature pipeline
    dynamically. All its available settings are exposed as function parameters. Here,
    we need only a list of author’s names, as seen in the function’s signature: `feature_engineering(author_full_names:
    list[str])`. We inject a YAML configuration file at runtime that contains all
    the necessary values based on different use cases. For example, the following
    configuration includes a list of all the authors of this book as we want to populate
    the feature store with data from all of us (available in the GitHub repository
    at `configs/feature_engineering.yaml`):'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '最后一个拼图是理解如何动态配置 RAG 特征管道。所有可用设置都作为函数参数公开。在这里，我们只需要一个作者名字的列表，正如函数签名中所示：`feature_engineering(author_full_names:
    list[str])`。我们注入一个运行时 YAML 配置文件，其中包含基于不同用例的所有必要值。例如，以下配置包括本书所有作者的名字，因为我们希望用我们所有人的数据填充特征存储（可在
    GitHub 仓库的 `configs/feature_engineering.yaml` 中找到）：'
- en: '[PRE8]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The beauty of this approach is that you don’t have to modify the code to configure
    the feature pipeline with different input values. You have to provide a different
    configuration file when running it, as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是，你不需要修改代码来使用不同的输入值配置特征管道。你需要在运行时提供不同的配置文件，如下所示：
- en: '[PRE9]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can either hardcode the path to the config file or provide the `config_path`
    from the CLI, which allows you to modify the pipeline’s configuration between
    different runs. Out of simplicity, we hard-coded the configuration file. Thus,
    we can call the feature engineering pipeline calling the `run.py` script as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以硬编码配置文件的路径，或者从 CLI 提供的 `config_path`，这允许你在不同运行之间修改管道的配置。出于简单起见，我们硬编码了配置文件。因此，我们可以通过以下方式调用特征工程管道，即调用
    `run.py` 脚本：
- en: '[PRE10]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'However, you can easily add another CLI argument to pass the `config_path`
    variable. Also, you can run the feature pipeline using the following `poe` command:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以轻松地添加另一个 CLI 参数来传递 `config_path` 变量。你也可以使用以下 `poe` 命令运行特征管道：
- en: '[PRE11]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let’s move forward to the ZenML steps and sequentially zoom in on all of them.
    The source code for all the feature engineering pipeline steps is available on
    GitHub at `"steps/feature_engineering"`. We will begin with the first step, which
    involves querying the data warehouse for new content to process into features.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前进到ZenML步骤，并依次聚焦于所有这些步骤。所有特征工程管道步骤的源代码都可在GitHub上的`"steps/feature_engineering"`获取。我们将从第一个步骤开始，该步骤涉及查询数据仓库以获取新内容进行处理成特征。
- en: Querying the data warehouse
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询数据仓库
- en: 'The first thing to notice is that a step is a Python function decorated with
    `@step`, similar to how a ZenML pipeline works. The function below takes as input
    a list of authors’ full names and performs the following core steps:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，一个步骤是一个用`@step`装饰的Python函数，类似于ZenML管道的工作方式。下面的函数接受作者全名列表作为输入，并执行以下核心步骤：
- en: It attempts to get or create a `UserDocument` instance using the first and last
    names, appending this instance to the authors list. If the user doesn’t exist,
    it throws an error.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它尝试使用姓名和姓氏获取或创建一个`UserDocument`实例，并将此实例追加到作者列表中。如果用户不存在，它将抛出一个错误。
- en: It fetches all the raw data for the user from the data warehouse and extends
    the `documents` list to include these user documents.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从数据仓库中获取所有原始数据供用户使用，并将`documents`列表扩展以包括这些用户文档。
- en: Ultimately, it computes a descriptive metadata dictionary logged and tracked
    in ZenML.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，它计算一个描述性元数据字典，该字典在ZenML中记录和跟踪。
- en: '[PRE12]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The fetch function leverages a thread pool that runs each query on a different
    thread. As we have multiple data categories, we have to make a different query
    for the articles, posts, and repositories, as they are stored in different collections.
    Each query calls the data warehouse, which is bounded by the network I/O and data
    warehouse latency, not by the machine’s CPU. Thus, by moving each query to a different
    thread, we can parallelize them. Ultimately, instead of adding the latency of
    each query as the total timing, the time to run this fetch function will be the
    max between all the calls.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 获取函数利用线程池，在各个不同的线程上运行每个查询。由于我们有多个数据类别，我们必须为文章、帖子存储在不同的集合中，因此必须为它们进行不同的查询。每个查询都调用数据仓库，这受限于网络I/O和数据仓库延迟，而不是机器的CPU。因此，通过将每个查询移动到不同的线程，我们可以并行化它们。最终，运行此获取函数的时间将是所有调用中的最大值。
- en: Using threads to parallelize I/O-bounded calls is good practice in Python, as
    they are not locked by the Python **Global Interpreter Lock** (**GIL**). In contrast,
    adding each call to a different process would add too much overhead, as a process
    takes longer to spin off than a thread.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中使用线程并行化I/O受限的调用是良好的实践，因为它们不受Python全局解释器锁（GIL）的锁定。相比之下，将每个调用添加到不同的进程中会添加过多的开销，因为进程启动比线程慢。
- en: In Python, you want to parallelize things with processes only when the operations
    are CPU or memory-bound because the GIL affects them. Each process has a different
    GIL. Thus, parallelizing your computing logic, such as processing a batch of documents
    or images already loaded in memory, isn’t affected by Python’s GIL limitations.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，您只想在操作受CPU或内存限制时使用进程来并行化事物，因为全局解释器锁（GIL）会影响它们。每个进程都有自己的GIL。因此，并行化您的计算逻辑，例如处理已加载到内存中的文档或图像批处理，不会受到Python
    GIL限制的影响。
- en: '[PRE13]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `_get_metadata()` function takes the list of queried documents and authors
    and counts the number of them relative to each data category:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '`_get_metadata()`函数接受查询到的文档和作者列表，并计算它们相对于每个数据类别的数量：'
- en: '[PRE14]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We will expose this metadata in the ZenML dashboard to quickly see some statistics
    on the loaded data. For example, in *Figure 4.13*, we accessed the metadata tab
    of the `query_data_warehouse()` step, where you can see that, within that particular
    run of the feature pipeline, we loaded 76 documents from three authors. This is
    extremely powerful for monitoring and debugging batch pipelines.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在ZenML仪表板上公开此元数据，以便快速查看加载数据的某些统计数据。例如，在*图4.13*中，我们访问了`query_data_warehouse()`步骤的元数据标签，您可以看到，在特征管道的特定运行中，我们加载了来自三位作者的76个文档。这对于监控和调试批处理管道来说非常强大。
- en: You can always extend it with anything that makes sense for your use case.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以始终根据您的用例扩展任何有意义的操作。
- en: '![](img/B31105_04_13.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_13.png)'
- en: 'Figure 4.13: Metadata of the “query the data warehouse” ZenML step'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13：“查询数据仓库”ZenML步骤的元数据
- en: Cleaning the documents
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清理文档
- en: In the cleaning step, we iterate through all the documents and delegate all
    the logic to a `CleaningDispatcher` who knows what cleaning logic to apply based
    on the data category. Remember that we want to apply, or have the ability to apply
    in the future, different cleaning techniques on articles, posts, and code repositories.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在清理步骤中，我们遍历所有文档，并将所有逻辑委托给一个`CleaningDispatcher`，该调度器知道根据数据类别应用哪种清理逻辑。请记住，我们希望应用，或者在未来有应用不同清理技术的能力，对文章、帖子以及代码库进行清理。
- en: '[PRE15]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The computed metadata is similar to what we logged in the `query_data_warehouse()`
    step. Thus, let’s move on to chunking and embedding.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的元数据与我们在`query_data_warehouse()`步骤中记录的类似。因此，让我们继续进行分块和嵌入。
- en: Chunk and embed the cleaned documents
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分块并嵌入清洗后的文档
- en: Similar to how we cleaned the documents, we delegate the chunking and embedding
    logic to a dispatcher who knows how to handle each data category. Note that the
    chunking dispatcher returns a list instead of a single object, which makes sense
    as the document is split into multiple chunks. We will dig into the dispatcher
    in the “The dispatcher layer” section of this chapter.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们清理文档的方式类似，我们将分块和嵌入逻辑委托给一个知道如何处理每个数据类别的调度器。请注意，分块调度器返回一个列表而不是单个对象，这是有意义的，因为文档被分割成多个块。我们将在本章的“调度器层”部分深入探讨调度器。
- en: '[PRE16]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In *Figure 4.14*, you can see the metadata of the chunking and embedding ZenML
    step. For example, you can quickly understand that we transformed 76 documents
    into 2,373 chunks, or the properties we used for chunking articles, such as a
    `chunk_size` of 500 and a `chunk_overlap` of **50**.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.14*中，您可以查看分块和嵌入ZenML步骤的元数据。例如，您可以快速了解我们将76篇文档转换成了2,373个块，或者我们用于分块文章的属性，例如`chunk_size`为500和`chunk_overlap`为**50**。
- en: '![](img/B31105_04_14.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_14.png)'
- en: 'Figure 4.14: Metadata of the embedding and chunking ZenML step, detailing the
    uncategorized and chunking dropdowns'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14：嵌入和分块ZenML步骤的元数据，详细说明了未分类和分块下拉菜单
- en: In Figure 4.15, the rest of the ZenML metadata from the embedding and chunking
    step details the embedding model and its properties used to compute the vectors.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.15中，嵌入和分块步骤的其余ZenML元数据详细说明了嵌入模型及其用于计算向量的属性。
- en: '![](img/B31105_04_15.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_15.png)'
- en: 'Figure 4.15: Metadata of the embedding and chunking ZenML step, detailing the
    embedding dropdown'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15：嵌入和分块ZenML步骤的元数据，详细说明了嵌入下拉菜单
- en: As ML systems can break at any time while in production due to drifts or untreated
    use cases, leveraging the metadata section to monitor the ingested data can be
    a powerful tool that will save debugging days, translating to tens of thousands
    of dollars or more for your business.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习系统在生产过程中可能会因为漂移或未处理的用例而随时崩溃，利用元数据部分来监控摄取的数据可以是一个强大的工具，这将节省调试天数，对于您的业务来说可能意味着数万美元或更多。
- en: Loading the documents to the vector DB
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将文档加载到向量数据库
- en: 'As each article, post, or code repository sits in a different collection inside
    the vector DB, we have to group all the documents based on their data category.
    Then, we load each group in bulk in the Qdrant vector DB:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每篇文章、帖子或代码库都位于向量数据库中的不同集合中，我们必须根据其数据类别对所有文档进行分组。然后，我们将每个组批量加载到Qdrant向量数据库中：
- en: '[PRE17]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Pydantic domain entities
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pydantic领域实体
- en: Before investigating the dispatchers, we must understand the domain objects
    we work with. To some extent, in implementing the LLM Twin, we are following the
    **domain-driven design** (**DDD**) principles, which state that domain entities
    are the core of your application. Thus, before proceeding, it’s important to understand
    the hierarchy of the domain classes we are working with.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查调度器之前，我们必须了解我们正在处理的领域对象。在某种程度上，在实现LLM Twin时，我们遵循**领域驱动设计**（**DDD**）原则，该原则指出领域实体是应用程序的核心。因此，在继续之前，了解我们正在处理的领域类别的层次结构非常重要。
- en: The code for the domain entities is available on GitHub at [https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/domain](https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/domain).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 领域实体的代码可在GitHub上找到，链接为[https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/domain](https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/domain)。
- en: We used Pydantic to model all our domain entities. When we wrote the book, choosing
    Pydantic was a no-brainer, as it is the go-to Python package for writing data
    structures with out-of-the-box type validation. As Python is a dynamically typed
    language, using Pydantic for type validation at runtime makes your system order
    of times more robust, as you can be sure that you are always working with the
    right type of data.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Pydantic 来建模所有我们的领域实体。当我们编写这本书时，选择 Pydantic 是不言而喻的，因为它是编写具有开箱即用类型验证的数据结构的
    Python 包的首选。由于 Python 是一种动态类型语言，使用 Pydantic 在运行时进行类型验证可以使你的系统更稳健，因为你可以确信你总是在处理正确的数据类型。
- en: 'The domain of our LLM Twin application is split into two dimensions:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 LLM Twin 应用程序的领域被分为两个维度：
- en: '**The data category**: Post, article, and repository'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据类别**：帖子、文章和库'
- en: '**The state of the data**: Cleaned, chunked, and embedded'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据状态**：清洗、分块和嵌入'
- en: 'We decided to create a base class for each state of the document, resulting
    in having the following base abstract classes:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定为文档的每个状态创建一个基类，从而得到以下基抽象类：
- en: '`class CleanedDocument(VectorBaseDocument, ABC)`'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class CleanedDocument(VectorBaseDocument, ABC)`'
- en: '`class Chunk(VectorBaseDocument, ABC)`'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class Chunk(VectorBaseDocument, ABC)`'
- en: '`class EmbeddedChunk(VectorBaseDocument, ABC)`'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class EmbeddedChunk(VectorBaseDocument, ABC)`'
- en: Note that all of them inherit the `VectorBaseDocument` class, which is our custom
    **OVM** implementation, which we will explain in the next section of this chapter.
    Also, it inherits from ABC, which makes the class abstract. Thus, you cannot initialize
    an object out of these classes; you may only inherit from them. That is why base
    classes are always marked as abstract.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有这些类都继承自 `VectorBaseDocument` 类，这是我们自定义的 **OVM** 实现，我们将在本章下一节中解释。它还继承自 ABC，这使得类成为抽象类。因此，你不能从这些类中初始化对象；你只能从它们继承。这就是为什么基类总是被标记为抽象的。
- en: 'Each base abstract class from above (which models the state) will have a subclass
    that adds the data category dimension. For example, the `CleanedDocument` class
    will have the following subclasses:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的每个基本抽象类（它表示状态）都将有一个子类，该子类添加数据类别维度。例如，`CleanedDocument` 类将具有以下子类：
- en: '`class CleanedPostDocument(CleanedDocument)`'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class CleanedPostDocument(CleanedDocument)`'
- en: '`class CleanedArticleDocument(CleanedDocument)`'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class CleanedArticleDocument(CleanedDocument)`'
- en: '`class CleanedRepositoryDocument(CleanedDocument)`'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class CleanedRepositoryDocument(CleanedDocument)`'
- en: As we can see in *Figure 8.16*, we will repeat the same logic for the `Chunk`
    and `EmbeddedChunk` base abstract classes. We will implement a specific document
    class for each data category and state combination, resulting in nine types of
    domain entities. For example, when ingesting a raw document, the cleaning step
    will yield a `CleanedArticleDocument` instance, the chunking step will return
    a list of `ArticleChunk` objects, and the embedding operation will return `EmbeddedArticleChunk`
    instances that encapsulate the embedding and all the necessary metadata to ingest
    in the vector DB.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *图 8.16* 中所看到的，我们将对 `Chunk` 和 `EmbeddedChunk` 基抽象类重复相同的逻辑。我们将为每个数据类别和状态组合实现特定的文档类，从而产生九种类型的领域实体。例如，当摄取原始文档时，清洗步骤将产生一个
    `CleanedArticleDocument` 实例，分块步骤将返回一个 `ArticleChunk` 对象列表，嵌入操作将返回封装嵌入和所有必要元数据的
    `EmbeddedArticleChunk` 实例，以便在向量数据库中摄取。
- en: The same will happen for the posts and repositories.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 对于帖子和解压库，也会发生同样的事情。
- en: '![](img/B31105_04_16.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_16.png)'
- en: 'Figure 4.16: Domain entities class hierarchy and their interaction'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16：领域实体类层次结构及其交互
- en: We chose this design because the list of states will rarely change, and we want
    to extend the list of data categories. Thus, structuring the classes after the
    state allows us to plug another data category by inheriting these base abstract
    classes.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这种设计是因为状态列表很少改变，我们希望扩展数据类别列表。因此，根据状态结构化类允许我们通过继承这些基抽象类来插入另一个数据类别。
- en: Let’s see the complete code for the hierarchy of the cleaned document. All the
    attributes of a cleaned document will be saved within the metadata of the vector
    DB. For example, the metadata of a cleaned article document will always contain
    the content, platform, author ID, author full name, and link of the article.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看清洗文档的完整代码。清洗文档的所有属性都将保存在向量数据库的元数据中。例如，清洗文章文档的元数据将始终包含内容、平台、作者 ID、作者全名和文章链接。
- en: 'Another fundamental aspect is the `Config` internal class, which defines the
    name of the collection within the vector DB, the data category of the entity,
    and whether to leverage the vector index when creating the collection:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基本方面是`Config`内部类，它定义了向量数据库中集合的名称、实体的数据类别以及是否在创建集合时利用向量索引：
- en: '[PRE18]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To conclude this section, let’s also take a look at the base abstract class
    of the chunk and embedded chunk:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本节内容，让我们也看一下块和嵌入块的基础抽象类：
- en: '[PRE19]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We also defined an enum that aggregates all our data categories in a single
    structure of constants:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个枚举，它将所有数据类别聚合到一个常量结构中：
- en: '[PRE20]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The last step to fully understand how the domain objects work is to zoom into
    the `VectorBaseDocument` OVM class.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 完全理解域对象如何工作的最后一步是放大查看`VectorBaseDocument` OVM类。
- en: OVM
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OVM
- en: The term OVM is inspired by the **object-relational mapping** (**ORM**) pattern
    we discussed in *Chapter 3*. We called it OVM because we work with embedding and
    vector DBs instead of structured data and SQL tables. Otherwise, it follows the
    same principles as an ORM pattern.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 术语OVM是从我们在*第3章*中讨论的**对象关系映射**（**ORM**）模式中获得的灵感。我们称之为OVM，因为我们与嵌入和向量数据库而不是结构化数据和SQL表一起工作。否则，它遵循与ORM模式相同的原理。
- en: Similar to what we did in *Chapter 3*, we will implement our own OVM version.
    Even if our custom example is simple, it’s a powerful example of how to write
    modular and extendable classes by leveraging OOP best practices and principles.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在*第3章*中所做的一样，我们将实现自己的OVM版本。即使我们的自定义示例很简单，它也是一个强大的示例，展示了如何通过利用面向对象的最佳实践和原则来编写模块化和可扩展的类。
- en: The full implementation of the `VectorBaseDocument` class is available on GitHub
    at [https://github.com/PacktPublishing/LLM-Engineering/blob/main/llm_engineering/domain/base/vector.py](https://github.com/PacktPublishing/LLM-Engineering/blob/main/llm_engineering/domain/base/vector.py).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorBaseDocument`类的完整实现可在GitHub上找到：[https://github.com/PacktPublishing/LLM-Engineering/blob/main/llm_engineering/domain/base/vector.py](https://github.com/PacktPublishing/LLM-Engineering/blob/main/llm_engineering/domain/base/vector.py)。'
- en: Our OVM base class is called `VectorBaseDocument`. It will support CRUD operations
    on top of Qdrant. Based on our application’s demands, we limited it only to create
    and read operations, but it can easily be extended to update and delete functions.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的OVM基础类被称为`VectorBaseDocument`。它将在Qdrant之上支持CRUD操作。根据我们应用的需求，我们仅将其限制为创建和读取操作，但它可以轻松扩展到更新和删除功能。
- en: 'Let’s take a look at the definition of the `VectorBaseDocument` class:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`VectorBaseDocument`类的定义：
- en: '[PRE21]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `VectorBaseDocument` class inherits from Pydantic’s `BaseModel` and helps
    us structure a single record’s attributes from the vector DB. Every OVM will be
    initialized by default with UUID4 as its unique identifier. Using generics—more
    precisely, by inheriting from `Generic[T]`—the signatures of all the subclasses
    of the `VectorBaseDocument` class will adapt to that given class. For example,
    the `from_record()` method of the `Chunk()` class, which inherits `VectorBaseDocument`,
    will return the Chunk type, which drastically helps the static analyzer and type
    checkers such as `mypy` ([https://mypy.readthedocs.io/en/stable/](https://mypy.readthedocs.io/en/stable/)).
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VectorBaseDocument`类继承自Pydantic的`BaseModel`，帮助我们从向量数据库中结构化单个记录的属性。每个OVM默认都会初始化为UUID4作为其唯一标识符。使用泛型——更确切地说，通过继承自`Generic[T]`——`VectorBaseDocument`类的所有子类的签名都将适应给定的类。例如，继承自`VectorBaseDocument`的`Chunk()`类的`from_record()`方法将返回块类型，这极大地帮助了静态分析器和类型检查器，如`mypy`
    ([https://mypy.readthedocs.io/en/stable/](https://mypy.readthedocs.io/en/stable/))。'
- en: The `from_record()` method adapts a data point from Qdrant’s format to our internal
    structure based on Pydantic. On the other hand, the `to_point()` method takes
    the attributes of the current instance and adapts them to Qdrant’s `PointStruct()`
    format. We will leverage these two methods for our create and read operations.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '`from_record()`方法将来自Qdrant的数据点格式适配到基于Pydantic的内部结构。另一方面，`to_point()`方法将当前实例的属性适配到Qdrant的`PointStruct()`格式。我们将利用这两个方法来进行创建和读取操作。'
- en: Ultimately, all operations made to Qdrant will be done through the `connection`
    instance, which is instantiated in the application’s infrastructure layer.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，所有对Qdrant的操作都将通过`connection`实例来完成，该实例在应用的基础设施层中实例化。
- en: The `bulk_insert()` method maps each document to a point. Then, it uses the
    Qdrant `connection` instance to load all the points to a given collection in Qdrant.
    If the insertion fails once, it tries to create the collection and do the insertion
    again. Often, it is good practice to split your logic into two functions. One
    private function contains the logic, in our case `_bulk_insert()`, and one public
    function handles all the errors and failure scenarios.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '`bulk_insert()`方法将每个文档映射到一个点。然后，它使用Qdrant `connection`实例将所有点加载到Qdrant中给定集合。如果插入失败一次，它将尝试创建集合并再次进行插入。通常，将逻辑分成两个函数是一个好的做法。一个私有函数包含逻辑，在我们的例子中是`_bulk_insert()`，而一个公共函数处理所有错误和失败场景。'
- en: '[PRE22]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The collection name is inferred from the `Config` class defined in the subclasses
    inheriting the OVM:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 集合名称是从继承OVM的子类中定义的`Config`类推断出来的：
- en: '[PRE23]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, we must define a method that lets us read all the records from the vector
    DB (without using vector similarity search logic). The `bulk_find()` method enables
    us to scroll (or list) all the records from a collection. The function below scrolls
    the Qdrant vector DB, which returns a list of data points, which are ultimately
    mapped to our internal structure using the `from_record()` method.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须定义一个方法，使我们能够从向量数据库（不使用向量相似性搜索逻辑）中读取所有记录。`bulk_find()`方法使我们能够滚动（或列出）集合中的所有记录。下面的函数滚动Qdrant向量数据库，它返回一个数据点列表，这些数据点最终通过`from_record()`方法映射到我们的内部结构。
- en: The limit parameters control how many items we return at once, and the offset
    signals the ID of the point from which Qdrant starts returning records.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 限制参数控制我们一次返回多少项，而偏移量指示Qdrant开始返回记录的点ID。
- en: '[PRE24]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The last piece of the puzzle is to define a method that performs a vector similarity
    search on a provided query embedding. Like before, we defined a public `search()`
    and private `_search()` method. The search is performed by Qdrant when calling
    the `connection.search()` function.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个拼图是要定义一个方法，该方法在提供的查询嵌入上执行向量相似性搜索。就像之前一样，我们定义了一个公共的`search()`方法和一个私有的`_search()`方法。搜索是通过调用`connection.search()`函数时由Qdrant执行的。
- en: '[PRE25]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that we understand what our domain entities look like and how the OVM works,
    let’s move on to the dispatchers who clean, chunk, and embed the documents.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了我们的领域实体看起来像什么以及OVM是如何工作的，让我们继续到清理、分块和嵌入文档的分发器。
- en: The dispatcher layer
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分发器层
- en: A dispatcher inputs a document and applies dedicated handlers based on its data
    category (article, post, or repository). A handler can either clean, chunk, or
    embed a document.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 分发器输入一个文档，并根据其数据类别（文章、帖子或存储库）应用特定的处理程序。处理程序可以清理、分块或嵌入文档。
- en: 'Let’s start by zooming in on the `CleaningDispatcher`. It mainly implements
    a `dispatch()` method that inputs a raw document. Based on its data category,
    it instantiates and calls a handler that applies the cleaning logic specific to
    that data point:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从关注`CleaningDispatcher`开始。它主要实现了一个`dispatch()`方法，该方法输入一个原始文档。根据其数据类别，它实例化并调用一个处理程序，该处理程序应用针对该数据点的特定清理逻辑：
- en: '[PRE26]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The key in the dispatcher logic is the `CleaningHandlerFactory()`, which instantiates
    a different cleaning handler based on the document’s data category:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 分发器逻辑中的关键是`CleaningHandlerFactory()`，它根据文档的数据类别实例化不同的清理处理程序：
- en: '[PRE27]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The Dispatcher or Factory classes are nothing fancy, but they offer an intuitive
    and simple interface for applying various operations to your documents. When manipulating
    documents, instead of worrying about their data category and polluting your business
    logic with if-else statements, you have a class dedicated to handling that. You
    have a single class that cleans any document, which respects the DRY (don’t repeat
    yourself) principles from software engineering. By respecting DRY, you have a
    single point of failure, and the code can easily be extended. For example, if
    we add an extra type, we must extend only the Factory class instead of multiple
    occurrences in the code.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 分发器或工厂类并不复杂，但它们提供了一个直观且简单的接口，用于将各种操作应用于您的文档。在处理文档时，您无需担心它们的数据类别，也不必用if-else语句污染业务逻辑，您有一个专门处理这一点的类。您有一个单独的类可以清理任何文档，这遵循了软件工程中的DRY（不要重复自己）原则。通过遵循DRY原则，您有一个单一的错误点，代码可以轻松扩展。例如，如果我们添加一个额外的类型，我们只需扩展工厂类，而不是在代码中的多个位置进行扩展。
- en: The `ChunkingDispatcher` and `EmbeddingDispatcher` follow the same pattern.
    They use a `ChunkingHandlerFactory` and, respectively, an `EmbeddingHandlerFactory`
    that initializes the correct handler based on the data category of the input document.
    Afterward, they call the handler and return the result.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChunkingDispatcher`和`EmbeddingDispatcher`遵循相同的模式。它们使用`ChunkingHandlerFactory`和分别的`EmbeddingHandlerFactory`，根据输入文档的数据类别初始化正确的处理器。之后，它们调用处理器并返回结果。'
- en: The source code of all the dispatchers and factories can be found on GitHub
    at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/preprocessing/dispatchers.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/preprocessing/dispatchers.py)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 所有调度器和工厂的源代码可以在GitHub上找到：[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/preprocessing/dispatchers.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/preprocessing/dispatchers.py)
- en: The Factory class leverages theabstract factory creational pattern ([https://refactoring.guru/design-patterns/abstract-factory](https://refactoring.guru/design-patterns/abstract-factory)),
    which instantiates a family of classes implementing the same interface. In our
    case, these handlers implement the `clean()` method regardless of the handler
    type.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 工厂类利用了抽象工厂创建模式（[https://refactoring.guru/design-patterns/abstract-factory](https://refactoring.guru/design-patterns/abstract-factory)），该模式实例化一组实现相同接口的类。在我们的情况下，这些处理器无论处理器类型如何都实现了`clean()`方法。
- en: Also, the Handler class family leverages the strategy behavioral pattern ([https://refactoring.guru/design-patterns/strategy](https://refactoring.guru/design-patterns/strategy))
    used to instantiate when you want to use different variants of an algorithm within
    an object and be able to switch from one algorithm to another during runtime.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`Handler`类家族利用了策略行为模式（[https://refactoring.guru/design-patterns/strategy](https://refactoring.guru/design-patterns/strategy)），在需要在使用对象内使用算法的不同变体，并在运行时能够从一种算法切换到另一种算法时使用。
- en: 'Intuitively, in our dispatcher layer, the combination of the factory and strategy
    patterns works as follows:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，在我们的调度层中，工厂模式和策略模式的组合工作如下：
- en: Initially, we knew we wanted to clean the data, but as we knew the data category
    only at runtime, we couldn’t decide on what strategy to apply.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始时，我们知道我们想要清理数据，但由于我们只在运行时知道数据类别，我们无法决定应用哪种策略。
- en: We can write the whole code around the cleaning code and abstract away the logic
    under a `Handler()` interface, which will represent our strategy.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以围绕清理代码编写整个代码，并将逻辑抽象化在`Handler()`接口下，该接口将代表我们的策略。
- en: When we get a data point, we apply the abstract factory pattern and create the
    correct cleaning handler for its data type.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们得到一个数据点时，我们应用抽象工厂模式并为其数据类型创建正确的清理处理器。
- en: Ultimately, the dispatcher layer uses the handler and executes the right strategy.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，调度层使用处理器并执行正确的策略。
- en: 'By doing so, we:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们：
- en: Isolate the logic for a given data category.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特定数据类别的逻辑隔离。
- en: Leverage polymorphism to avoid filling up the code with hundreds of `if-else`
    statements.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用多态性来避免在代码中填充数百个`if-else`语句。
- en: Make the code modular and extendable. When a new data category arrives, we must
    implement a new handler and modify the Factory class without touching any other
    part of the code.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使代码模块化和可扩展。当新的数据类别到来时，我们必须实现一个新的处理器并修改工厂类，而不需要触及代码的任何其他部分。
- en: Until now, we have just modeled our entities and how the data flows in our application.
    We haven’t written a single piece of cleaning, chunking, or embedding code. That
    is one big difference between a quick demo and a production-ready application.
    In a demo, you don’t care about software engineering best practices and structuring
    your code to make it future-proof. However, writing clean, modular, and scalable
    code is critical for its longevity when building a real-world application.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们只是对实体进行了建模以及数据在我们应用程序中的流动方式。我们没有编写任何清理、分块或嵌入代码。这是快速演示和可投入生产的应用程序之间的一大区别。在演示中，你不需要关心软件工程的最佳实践和构建未来兼容的代码结构。然而，编写干净、模块化和可扩展的代码对于构建现实世界应用程序的长期寿命至关重要。
- en: The last component of the RAG feature pipeline is the implementation of the
    cleaning, chunking, and embedding handlers.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: RAG功能管道的最后一个组件是实现清理、分块和嵌入处理器。
- en: The handlers
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理器
- en: 'The handler has a one-on-one structure with our domain, meaning that every
    entity has its own handler, as shown in Figure 8.17\. In total, we will have nine
    Handler classes that follow the next base interfaces:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器与我们的领域具有一对一的结构，这意味着每个实体都有自己的处理器，如图 8.17 所示。总共，我们将有九个处理器类，它们遵循以下基本接口：
- en: '`class CleaningDataHandler()`'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class CleaningDataHandler()`'
- en: '`class ChunkingDataHandler()`'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class ChunkingDataHandler()`'
- en: '`class EmbeddingDataHandler()`'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class EmbeddingDataHandler()`'
- en: '![](img/B31105_04_17.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_04_17.png)'
- en: 'Figure 4.17: Handler class hierarchy and their interaction'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17：处理器类层次结构和它们的交互
- en: The code for all the handlers is available on GitHub at [https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing](https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 所有处理器的代码可在 GitHub 上找到：[https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing](https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing)。
- en: Let’s examine each handler family and see how it is implemented.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查每个处理器家族，看看它们是如何实现的。
- en: The cleaning handlers
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清理处理器
- en: 'The `CleaningDataHandler()` strategy interface looks as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '`CleaningDataHandler()` 策略接口看起来如下：'
- en: '[PRE28]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, for every post, article and repository, we have to implement a different
    handler, as follows:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于每个帖子、文章和存储库，我们必须实现不同的处理器，如下所示：
- en: '[PRE29]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The handlers input a raw document domain entity, clean the content, and return
    a cleaned document. All the handlers use the `clean_text()` function to clean
    the text. Out of simplicity, we used the same cleaning technique for all the data
    categories. Still, in a real-world setup, we would have to further optimize and
    create a different cleaning function for each data category. The strategy pattern
    makes this a breeze, as we swap the cleaning function in the handlers, and that’s
    it.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器输入原始文档领域实体，清理内容，并返回一个清理后的文档。所有处理器都使用 `clean_text()` 函数来清理文本。出于简化的考虑，我们为所有数据类别使用了相同的清理技术。然而，在实际的设置中，我们不得不进一步优化并为每个数据类别创建不同的清理函数。策略模式使得这一点变得非常简单，因为我们只需在处理器中交换清理函数即可。
- en: The cleaning steps applied in the `clean_text()` function are the same ones
    discussed in *Chapter* *5* in the *Creating an instruction dataset* section. We
    don’t want to repeat ourselves. Thus, for a refresher, check out that chapter.
    At this point, we mostly care about automating and integrating the whole logic
    into the RAG feature pipeline. Thus, after operationalizing the ML system, all
    the cleaned data used for fine-tuning will be accessed from the logical feature
    store, making it the single source of truth for accessing data.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `clean_text()` 函数中应用的清理步骤与在 *第 5 章* 的 *创建指令数据集* 部分中讨论的相同。我们不希望重复自己。因此，为了复习，请查看该章节。在此阶段，我们主要关注自动化并将整个逻辑集成到
    RAG 功能管道中。因此，在将 ML 系统投入运行后，所有用于微调的清理数据都将从逻辑特征存储中访问，使其成为访问数据的单一真相来源。
- en: The chunking handlers
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分块处理器
- en: First, let’s examine the `ChunkingDataHandler()` strategy handler. We exposed
    the `metadata` dictionary as a property to aggregate all the necessary properties
    required for chunking in a single structure. By structuring it like this, we can
    easily log everything to ZenML to track and debug our chunking logic. The handler
    takes cleaned documents as input and returns chunk entities. All the handlers
    can be found on GitHub at [https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing](https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查 `ChunkingDataHandler()` 策略处理器。我们将 `metadata` 字典作为属性公开，以便在单个结构中聚合所有必要的分块属性。通过这种方式结构化，我们可以轻松地将所有内容记录到
    ZenML 以跟踪和调试我们的分块逻辑。处理器以清理后的文档为输入，并返回分块实体。所有处理器都可以在 GitHub 上找到：[https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing](https://github.com/PacktPublishing/LLM-Engineering/tree/main/llm_engineering/application/preprocessing)。
- en: '[PRE30]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s understand how the `ArticleChunkingHandler()` class is implemented. The
    first step is to override the metadata property and customize the type of properties
    the chunking logic requires. For example, when working with articles, we are interested
    in the chunk’s minimum and maximum length.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解 `ArticleChunkingHandler()` 类是如何实现的。第一步是重写元数据属性并自定义分块逻辑所需的属性类型。例如，当处理文章时，我们关注分块的最小和最大长度。
- en: The handler’s `chunk()` method inputs cleaned article documents and returns
    a list of article chunk entities. It uses the `chunk_text()` function to split
    the cleaned content into chunks. The chunking function is customized based on
    the `min_length` and `max_length` metadata fields. The chunk_id is computed as
    the MD5 hash of the chunk’s content. Thus, if the two chunks have precisely the
    same content, they will have the same ID, and we can easily deduplicate them.
    Lastly, we create a list of chunk entities and return them.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 处理程序的`chunk()`方法输入清洗后的文章文档，并返回一个文章文本块实体列表。它使用`chunk_text()`函数将清洗后的内容分割成文本块。文本块分割函数是根据`min_length`和`max_length`元数据字段定制的。文本块ID是文本块内容的MD5哈希值。因此，如果两个文本块的内容完全相同，它们将具有相同的ID，我们可以轻松地去除重复项。最后，我们创建一个文本块实体列表并返回它们。
- en: '[PRE31]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The last step is to dig into the `chunk_article()` function, which mainly does
    two things:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是深入研究`chunk_article()`函数，该函数主要做两件事：
- en: It uses a regex to find all the sentences within the given text by looking for
    periods, question marks, or exclamation points followed by a space. However, it
    avoids splitting into cases where the punctuation is part of an abbreviation or
    initialism (like “`e.g.`" or “`Dr.`")
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用正则表达式通过查找后面跟着空格的句号、问号或感叹号来在给定文本中找到所有句子。然而，它避免将标点符号作为缩写或首字母缩略词（如“`e.g.`”或“`Dr.`”）的一部分的情况分割开。
- en: It groups sentences into a single chunk until the `max_length` limit is reached.
    When the maximum size is reached, and the chunk size is bigger than the minimum
    allowed value, it is added to the final list the function returns.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会将句子组合成一个单独的文本块，直到达到`max_length`限制。当达到最大尺寸时，如果文本块的大小大于允许的最小值，它就会被添加到函数返回的最终列表中。
- en: '[PRE32]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `PostChunkingHandler` and `RepositoryChunkingHandler`, available on GitHub
    at `llm_engineering/application/preprocessing/chunking_data_handlers.py`, have
    a similar structure to the ArticleChunkingHandler. However, they use a more generic
    chunking function called `chunk_text()`, worth looking into. The `chunk_text()`
    function is a two-step process that has the following logic:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上可用的`PostChunkingHandler`和`RepositoryChunkingHandler`与`ArticleChunkingHandler`具有类似的结构。然而，它们使用一个更通用的文本块分割函数`chunk_text()`，值得深入研究。`chunk_text()`函数是一个两步过程，具有以下逻辑：
- en: It uses a `RecursiveCharacterTextSplitter()` from LangChain to split the text
    based on a given separator or chunk size. Using the separator, we first try to
    find paragraphs in the given text, but if there are no paragraphs or they are
    too long, we cut it at a given chunk size.
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用LangChain的`RecursiveCharacterTextSplitter()`根据给定的分隔符或文本块大小来分割文本。使用分隔符，我们首先尝试在给定文本中找到段落，但如果没有段落或它们太长，我们就在给定的文本块大小处将其分割。
- en: Notice that we want to ensure that the chunk doesn’t exceed the maximum input
    length of the embedding model. Thus, we pass all the chunks created above into
    a `SenteceTransformersTokenTextSplitter()`, which considers the maximum input
    length of the model. At this point, we also apply the `chunk_overlap` logic, as
    we want to do it only after we validate that the chunk is small enough.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，我们希望确保文本块不超过嵌入模型的输入长度最大值。因此，我们将上面创建的所有文本块传递给一个`SenteceTransformersTokenTextSplitter()`，该函数会考虑模型的输入长度最大值。在此阶段，我们还应用了`chunk_overlap`逻辑，因为我们希望在验证文本块足够小之后才执行此操作。
- en: '[PRE33]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: To conclude, the function above returns a list of chunks that respect both the
    provided chunk parameters and the embedding model’s max input length.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，上述函数返回一个列表，其中的文本块既符合提供的文本块参数，也符合嵌入模型的最大输入长度。
- en: The embedding handlers
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入处理程序
- en: The embedding handlers differ slightly from the others as the `EmbeddingDataHandler()`
    interface contains most of the logic. We took this approach because, when calling
    the embedding model, we want to batch as many samples as possible to optimize
    the inference process. When running the model on a GPU, the batched samples are
    processed independently and in parallel. Thus, by batching the chunks, we can
    optimize the inference process by 10x or more, depending on the batch size and
    hardware we use.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入处理程序与其他处理程序略有不同，因为`EmbeddingDataHandler()`接口包含大部分逻辑。我们采取这种方法是因为，在调用嵌入模型时，我们希望尽可能多地批量处理样本以优化推理过程。当在GPU上运行模型时，批量样本是独立且并行处理的。因此，通过批量处理文本块，我们可以根据批量大小和硬件优化推理过程，提高10倍或更多。
- en: We implemented an `embed()` method, in case you want to run the inference on
    a single data point, and an `embed_batch()` method. The `embed_batch()` method
    takes chunked documents as input, gathers their content into a list, passes them
    to the embedding model, and maps the results to an embedded chunk domain entity.
    The mapping is done through the `map_model()` abstract method, which has to be
    customized for every data category.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了一个 `embed()` 方法，以防你想在单个数据点上运行推理，以及一个 `embed_batch()` 方法。`embed_batch()`
    方法接收分块文档作为输入，将它们的内容收集到一个列表中，然后将它们传递给嵌入模型，并将结果映射到嵌入块域实体。映射是通过 `map_model()` 抽象方法完成的，该方法必须针对每个数据类别进行定制。
- en: '[PRE34]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Let’s look only at the implementation of the `ArticleEmbeddingHandler()`, as
    the other handlers are highly similar. As you can see, we only have to implement
    the `map_model()` method, which takes a chunk of input and computes the embeddings
    in batch mode. Its scope is to map this information to an `EmbeddedArticleChunk`
    Pydantic entity.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们只看看 `ArticleEmbeddingHandler()` 的实现，因为其他处理程序高度相似。正如你所见，我们只需要实现 `map_model()`
    方法，该方法接收输入块并批量计算嵌入。它的作用是将这些信息映射到 `EmbeddedArticleChunk` Pydantic 实体。
- en: '[PRE35]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The last step is to understand how the `EmbeddingModelSingleton()` works. It
    is a wrapper over the `SentenceTransformer()` class from Sentence Transformers
    that initializes the embedding model. Writing a wrapper over external packages
    is often good practice. Thus, when you want to change the third-party tool, you
    have to modify only the internal logic of the wrapper instead of the whole code
    base.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是理解 `EmbeddingModelSingleton()` 的工作原理。它是 Sentence Transformers 中 `SentenceTransformer()`
    类的包装器，用于初始化嵌入模型。在外部包上编写包装器通常是很好的实践。因此，当你想要更改第三方工具时，你只需修改包装器的内部逻辑，而不是整个代码库。
- en: The `SentenceTransformer()` class is initialized with the `model_id` defined
    in the `Settings` class, allowing us to quickly test multiple embedding models
    just by changing the configuration file and not the code. That is why I am not
    insisting at all on what embedding model to use. This differs constantly based
    on your use case, data, hardware, and latency. But by writing a generic class,
    which can quickly be configured, you can experiment with multiple embedding models
    until you find the best one for you.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '`SentenceTransformer()` 类使用在 `Settings` 类中定义的 `model_id` 进行初始化，这使得我们能够通过更改配置文件而不是代码来快速测试多个嵌入模型。这就是为什么我根本不坚持使用哪种嵌入模型。这始终根据你的用例、数据、硬件和延迟而变化。但是，通过编写一个可以快速配置的通用类，你可以尝试多个嵌入模型，直到找到最适合你的模型。'
- en: '[PRE36]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The embedding model class implements the singleton pattern ([https://refactoring.guru/design-patterns/singleton](https://refactoring.guru/design-patterns/singleton)),
    a creational design pattern that ensures a class has only one instance while providing
    a global access point to this instance. The `EmbeddingModelSingleton()` class
    inherits from the `SingletonMeta` class, which ensures that whenever an `EmbeddingModelSingleton()`
    is instantiated, it returns the same instance. This works well with ML models,
    as you load them once in memory through the singleton pattern, and afterward,
    you can use them anywhere in the code base. Otherwise, you risk loading the model
    in memory every time you use it or loading it multiple times, resulting in memory
    issues. Also, this makes it very convenient to access properties such as `embedding_size`,
    where you have to make a dummy forward pass into the embedding model to find the
    size of its output. As a singleton, you do this forward pass only once, and then
    you have it accessible all the time during the program’s execution.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型类实现了单例模式（[https://refactoring.guru/design-patterns/singleton](https://refactoring.guru/design-patterns/singleton)），这是一种创建型设计模式，确保一个类只有一个实例，同时提供一个全局访问点来访问这个实例。`EmbeddingModelSingleton()`
    类继承自 `SingletonMeta` 类，确保每次实例化 `EmbeddingModelSingleton()` 时，都返回相同的实例。这对于机器学习模型来说效果很好，因为你可以通过单例模式在内存中一次性加载它们，之后你可以在代码库的任何地方使用它们。否则，你每次使用模型时都可能在内存中加载它，或者加载多次，从而导致内存问题。此外，这使得访问诸如
    `embedding_size` 这样的属性变得非常方便，因为你必须对嵌入模型进行一次虚拟前向传递以找到其输出的大小。作为单例，你只需进行一次前向传递，然后在程序执行期间始终可以访问它。
- en: Summary
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter began with a soft introduction to RAG and why and when you should
    use it. We also understood how embeddings and vector DBs work, representing the
    cornerstone of any RAG system. Then, we looked into advanced RAG and why we need
    it in the first place. We built a strong understanding of what parts of the RAG
    can be optimized and proposed some popular advanced RAG techniques for working
    with textual data. Next, we applied everything we learned about RAG to designing
    the architecture of LLM Twin’s RAG feature pipeline. We also understood the difference
    between a batch and streaming pipeline and presented a short introduction to the
    CDC pattern, which helps sync two DBs.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以对RAG（关系型数据库）的软性介绍开始，解释了何时以及为什么应该使用它。我们还了解了嵌入和向量数据库是如何工作的，这是任何RAG系统的基石。然后，我们探讨了高级RAG及其为何最初需要它。我们深入理解了RAG哪些部分可以被优化，并提出了针对文本数据的一些流行的先进RAG技术。接下来，我们将所学关于RAG的知识应用于设计LLM
    Twin的RAG功能管道架构。我们还理解了批处理和流式管道之间的区别，并简要介绍了CDC模式，该模式有助于同步两个数据库。
- en: Ultimately, we went step-by-step into the implementation of the LLM Twin’s RAG
    feature pipeline, where we saw how to integrate ZenML as an orchestrator, how
    to design the domain entities of the application, and how to implement an OVM
    module. Also, we understood how to apply some software engineering best practices,
    such as the abstract factory and strategy software patterns, to implement a modular
    and extendable layer that applies different cleaning, chunking, and embedding
    techniques based on the data category of each document.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们逐步深入到LLM Twin的RAG功能管道实现中，我们看到了如何将ZenML作为协调器进行集成，如何设计应用程序的领域实体，以及如何实现一个OVM模块。我们还理解了如何应用一些软件工程最佳实践，例如抽象工厂和策略软件模式，以实现一个模块化和可扩展的层，该层根据每个文档的数据类别应用不同的清理、分块和嵌入技术。
- en: This chapter focused only on implementing the ingestion pipeline, which is just
    one component of a standard RAG application. In *Chapter 9*, we will conclude
    the RAG system by implementing the retrieval and generation components and integrating
    them into the inference pipeline. But first, in the next chapter, we will explore
    how to generate a custom dataset using the data we collected and fine-tune an
    LLM with it.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 本章仅关注实现摄取管道，这只是一个标准RAG应用的一个组成部分。在*第9章*中，我们将通过实现检索和生成组件并将它们集成到推理管道中来完成RAG系统的构建。但首先，在下一章中，我们将探讨如何使用收集到的数据生成自定义数据集，并使用它微调LLM。
- en: References
  id: totrans-489
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Kenton, J.D.M.W.C. and Toutanova, L.K., 2019, June. Bert: Pre-training of deep
    bidirectional transformers for language understanding. In *Proceedings of naacL-HLT*
    (Vol. 1, p. 2).'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kenton, J.D.M.W.C. 和 Toutanova, L.K., 2019年6月。BERT：用于语言理解的双向变换器预训练。在*naacL-HLT会议论文集*（第1卷，第2页）。
- en: 'Liu, Y., 2019\. Roberta: A robustly optimized bert pretraining approach. *arXiv
    preprint arXiv:1907.11692*.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu, Y., 2019\. Roberta：一种稳健优化的BERT预训练方法。*arXiv预印本arXiv:1907.11692*。
- en: Mikolov, T., 2013\. Efficient estimation of word representations in vector space.
    *arXiv preprint arXiv:1301.3781*.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov, T., 2013\. 高效估计向量空间中的词表示。*arXiv预印本arXiv:1301.3781*。
- en: 'Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014\. *GloVe:
    Global Vectors for Word Representation*. In *Proceedings of the 2014 Conference
    on Empirical Methods in Natural Language Processing* (*EMNLP*), pages 1532–1543,
    Doha, Qatar. Association for Computational Linguistics.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeffrey Pennington, Richard Socher, 和 Christopher Manning。2014\. *GloVe：全局词表示向量*。在*2014年自然语言处理实证方法会议*（EMNLP），第1532-1543页，多哈，卡塔尔。计算语言学协会。
- en: He, K., Zhang, X., Ren, S. and Sun, J., 2016\. Deep residual learning for image
    recognition. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition* (pp. 770-778).
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, K., Zhang, X., Ren, S. 和 Sun, J., 2016\. 用于图像识别的深度残差学习。在*IEEE计算机视觉与模式识别会议论文集*（第770-778页）。
- en: Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
    G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July. Learning transferable
    visual models from natural language supervision. In *International conference
    on machine learning* (pp. 8748-8763). PMLR.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
    G., Askell, A., Mishkin, P., Clark, J. 和 Krueger, G., 2021年7月。从自然语言监督中学习可迁移的视觉模型。在*国际机器学习会议*（第8748-8763页）。PMLR。
- en: '*What is Change Data Capture (CDC)? | Confluent*. (n.d.). Confluent. [https://www.confluent.io/en-gb/learn/change-data-capture/](https://www.confluent.io/en-gb/learn/change-data-capture/
    )'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是变更数据捕获 (CDC)？| Confluent*. (n.d.). Confluent. [https://www.confluent.io/en-gb/learn/change-data-capture/](https://www.confluent.io/en-gb/learn/change-data-capture/)'
- en: Refactoring.Guru. (2024, January 1). *Singleton*. [https://refactoring.guru/design-patterns/singleton](https://refactoring.guru/design-patterns/singleton
    )
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Refactoring.Guru. (2024, January 1). *Singleton*. [https://refactoring.guru/design-patterns/singleton](https://refactoring.guru/design-patterns/singleton)
- en: Refactoring.Guru. (2024b, January 1). *Strategy*. [https://refactoring.guru/design-patterns/strategy](https://refactoring.guru/design-patterns/strategy
    )
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Refactoring.Guru. (2024b, January 1). *策略*. [https://refactoring.guru/design-patterns/strategy](https://refactoring.guru/design-patterns/strategy)
- en: Refactoring.Guru. (2024a, January 1). *Abstract Factory*. [https://refactoring.guru/design-patterns/abstract-factory](https://refactoring.guru/design-patterns/abstract-factory
    )
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Refactoring.Guru. (2024a, January 1). *抽象工厂*. [https://refactoring.guru/design-patterns/abstract-factory](https://refactoring.guru/design-patterns/abstract-factory)
- en: Schwaber-Cohen, R. (n.d.). *What is a Vector Database & How Does it Work? Use
    Cases + Examples*. Pinecone. [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/
    )
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwaber-Cohen, R. (n.d.). *什么是向量数据库？它如何工作？用例 + 示例*. Pinecone. [https://www.pinecone.io/learn/vector-database/](https://www.pinecone.io/learn/vector-database/)
- en: 'Monigatti, L. (2024, February 19). *Advanced Retrieval-Augmented Generation:
    From Theory to LlaMaIndex Implementatio*n. *Medium*. [https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930](https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930
    )'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monigatti, L. (2024, February 19). *高级检索增强生成：从理论到 LlaMaIndex 实现*. *Medium*.
    [https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930](https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930)
- en: Monigatti, L. (2023, December 6). A guide on 12 tuning Strategies for Production-Ready
    RAG applications. *Medium*. [https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439
    )
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monigatti, L. (2023, December 6). 生产就绪 RAG 应用程序的 12 种调优策略指南。*Medium*. [https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439)
- en: 'Monigatti, L. (2024b, February 19). *Advanced Retrieval-Augmented Generation:
    From Theory to LlaMaIndex Implementation*. *Medium*. [https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930](https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930
    )'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monigatti, L. (2024b, February 19). *高级检索增强生成：从理论到 LlaMaIndex 实现*. *Medium*.
    [https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930](https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930)
- en: Maameri, S. (2024, May 10). Routing in RAG-Driven applications - towards data
    science. *Medium*. [https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220](https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220  )
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maameri, S. (2024, May 10). RAG-驱动应用中的路由 - 走向数据科学。*Medium*. [https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220](https://towardsdatascience.com/routing-in-rag-driven-applications-a685460a7220)
- en: Join our book’s Discord space
  id: totrans-505
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code79969828252392890.png)'
