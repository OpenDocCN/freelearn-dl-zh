- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Getting Started with Information Extraction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息提取入门
- en: In this chapter, we will cover the basics of **information extraction**. Information
    extraction is the task of pulling very specific information from text. For example,
    you might want to know the companies mentioned in a news article. Instead of spending
    time reading the whole article, you can use information extraction techniques
    to access the companies almost instantly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍**信息提取**的基础知识。信息提取是从文本中提取非常具体信息的任务。例如，您可能想知道新闻文章中提到的公司。您不必花时间阅读整篇文章，可以使用信息提取技术几乎立即访问这些公司。
- en: We will start with extracting emails addresses and URLs from job announcements.
    Then, we will use an algorithm called **Levenshtein distance** to find similar
    strings. Next, we will extract important keywords from text. After that, we will
    use **spaCy** to find named entities in text, and later, we will train our own
    named entity recognition model in spaCy. We will then do basic sentiment analysis,
    and, finally, we will train two custom sentiment analysis models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从从工作公告中提取电子邮件地址和URL开始。然后，我们将使用称为**Levenshtein距离**的算法来查找相似字符串。接下来，我们将从文本中提取重要关键词。之后，我们将使用**spaCy**在文本中查找命名实体，稍后，我们将在spaCy中训练自己的命名实体识别模型。然后，我们将进行基本的情感分析，最后，我们将训练两个自定义情感分析模型。
- en: You will learn how to use existing tools and train your own models for information
    extraction tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您将学习如何使用现有工具和训练自己的模型进行信息提取任务。
- en: 'We will cover the following recipes in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下食谱：
- en: Using regular expressions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式
- en: Finding similar strings – Levenshtein distance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找相似字符串 – Levenshtein距离
- en: Extracting keywords
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取关键词
- en: Performing named entity recognition using spaCy
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用spaCy进行命名实体识别
- en: Training your own NER model with spaCy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用spaCy训练自己的NER模型
- en: Fine-tuning BERT for NER
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调BERT进行NER
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is in a folder named `Chapter05` in the GitHub repository
    of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter05)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于本书GitHub仓库中名为`Chapter05`的文件夹中（[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter05)）。
- en: As in previous chapters, the packages required for this chapter are part of
    the Poetry environment. Alternatively, you can install all the packages using
    the `requirements.txt` file.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，本章所需的包是Poetry环境的一部分。或者，您可以使用`requirements.txt`文件安装所有包。
- en: Using regular expressions
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式
- en: In this recipe, we will use regular expressions to find email addresses and
    URLs in text. Regular expressions are special character sequences that define
    search patterns and can be created and used via the Python `re` package. We will
    use a job descriptions dataset and write two regular expressions, one for emails
    and one for URLs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将使用正则表达式在文本中查找电子邮件地址和URL。正则表达式是特殊的字符序列，用于定义搜索模式，可以通过Python的`re`包创建和使用。我们将使用工作描述数据集并编写两个正则表达式，一个用于电子邮件，一个用于URL。
- en: Getting ready
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Download the job descriptions dataset here: [https://www.kaggle.com/andrewmvd/data-scientist-jobs](https://www.kaggle.com/andrewmvd/data-scientist-jobs).
    It is also available in the book’s GitHub repository at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/DataScientist.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/DataScientist.csv).
    Save it into the `/``data` folder.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里下载工作描述数据集：[https://www.kaggle.com/andrewmvd/data-scientist-jobs](https://www.kaggle.com/andrewmvd/data-scientist-jobs)。它也可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/DataScientist.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/DataScientist.csv)。将其保存到`/data`文件夹中。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.1_regex.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.1_regex.ipynb).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.1_regex.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.1_regex.ipynb)。
- en: How to do it…
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will read the data from the CSV file into a `pandas` DataFrame and will
    use the Python `re` package to create regular expressions and search the text.
    The steps are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从CSV文件中读取数据到`pandas` DataFrame中，并使用Python的`re`包来创建正则表达式并搜索文本。步骤如下：
- en: 'Import the **re** and **pandas** packages:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入**re**和**pandas**包：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Read in the data and check the contents inside it:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据并检查其内容：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output will be long and should start like this:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将会很长，应该像这样开始：
- en: '![](img/B18411_05_1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18411_05_1.jpg)'
- en: Figure 5.1 – DataFrame output
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – DataFrame输出
- en: 'The **get_list_of_items** helper function takes a DataFrame as input and turns
    one of its columns into a list. It accepts the DataFrame and the column name as
    inputs. First, it gets the column values, which is a list of lists, and then flattens
    that list. It then removes duplicates by turning the list into a set and casts
    it back to a list:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**get_list_of_items**辅助函数接受一个DataFrame作为输入，并将其某一列转换为列表。它接受DataFrame和列名作为输入。首先，它获取列值，这是一个列表的列表，然后将其展平。然后，它通过将列表转换为集合来删除重复项，并将其转换回列表：'
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this step, we define the **get_emails** function to get all the emails that
    appear in the **Job Description** column. The regular expression consists of three
    parts that appear in square brackets followed by quantifiers:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们定义了**get_emails**函数来获取**Job Description**列中出现的所有电子邮件。正则表达式由三个部分组成，这些部分出现在方括号中，后面跟着量词：
- en: '**[^\s:|()\'']+** is the username part of the regular expression, followed
    by the **@** sign. It consists of one group of characters, which is shown in square
    brackets. Any characters from this group may appear in the username one or more
    times. This is shown using the **+** quantifier. The characters in the username
    can be anything but a space (**\s**), colon (**:**), pipe (**|**), and apostrophe
    (**''**). The **^** character shows the negation of the character class. An apostrophe
    is a special character in regular expressions and has to be escaped with a backward
    slash in order to invoke the regular meaning of the character.'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[^\s:|()\'']+**是正则表达式的用户名部分，后面跟着一个**@**符号。它由一个字符组组成，显示在方括号中。这个组中的任何字符都可以在用户名中出现一次或多次。这使用**+**量词来表示。用户名中的字符可以是任何字符，但不能是空格（**\s**）、冒号（**:**）、竖线（**|**）和引号（**''**）。**^**字符表示字符类的否定。引号在正则表达式中是一个特殊字符，必须使用反斜杠转义才能调用字符的常规意义。'
- en: '**[a-zA-Z0-9\.]+** is the first part of the domain name, followed by a dot.
    This part is simply alphanumeric characters, lowercase or uppercase, and a dot
    appearing one or more times. Since a dot is a special character, we escape it
    with a backward slash. The **a-z** expression signifies a range of characters
    from *a* to *z*.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[a-zA-Z0-9\.]+**是域名的一部分，后面跟着一个点。这部分是简单的字母数字字符，小写或大写，并且点出现一次或多次。由于点是特殊字符，我们使用反斜杠来转义它。**a-z**表达式表示从*a*到*z*的字符范围。'
- en: '**[a-zA-Z]+** is the last part of the domain name, which is the top-level domain,
    such as **.com**, **.org**, and so on. Usually, no digits are allowed in these
    top-level domains, and the regular expression matches lowercase or uppercase characters
    that appear one or more times.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[a-zA-Z]+**是域名的一部分，即顶级域名，如**.com**、**.org**等。通常，这些顶级域名不允许出现数字，正则表达式匹配出现一次或多次的小写或大写字母。'
- en: 'This regular expression is sufficient to parse all emails in the dataset and
    not present any false positives. You might find that, in your data, there are
    additional adjustments that need to be made to the regular expression:'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个正则表达式足以解析数据集中的所有电子邮件，并且不会出现任何假阳性。你可能会发现，在你的数据中，需要对正则表达式进行一些额外的调整：
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will now get the emails from the DataFrame using the previous functions:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将使用之前的功能从DataFrame中获取电子邮件：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `finditer` function from the `re` package. It finds all matches in a text
    and returns them as `Match` objects. We can find the start and end of the match
    by using the `span()`object method. It returns a tuple, where the first element
    is the start and the second element is the end of the match:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`re`包中的`finditer`函数。它在一个文本中找到所有匹配项，并将它们作为`Match`对象返回。我们可以通过使用`span()`对象方法来找到匹配的开始和结束位置。它返回一个元组，其中第一个元素是匹配的开始，第二个元素是匹配的结束：'
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will get the URLs in a similar fashion:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将以类似的方式获取URL：
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Part of the result will look like this:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果的一部分可能看起来像这样：
- en: '[PRE7]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There’s more…
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Writing regular expressions can quickly turn into a messy affair. I use regular
    expression testing websites to enter the text in which I expect a match and the
    regular expression. One example of such a site is [https://regex101.com/](https://regex101.com/).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 编写正则表达式可能会迅速变成一件杂乱无章的事情。我使用正则表达式测试网站来输入我期望匹配的文本和正则表达式。此类网站的一个例子是[https://regex101.com/](https://regex101.com/)。
- en: Finding similar strings – Levenshtein distance
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找相似字符串 – Levenshtein距离
- en: 'When doing information extraction, in many cases, we deal with misspellings,
    which can bring complications to the task. To get around this problem, several
    methods are available, including Levenshtein distance. This algorithm finds the
    number of edits/additions/deletions needed to change one string into another.
    For example, to change the word *put* into *pat*, you need to substitute *u* for
    *a*, and that is one change. To change the word *kitten* into *smitten*, you need
    to do two edits: change *k* into *m* and add an *s* at the start.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行信息提取时，在许多情况下，我们处理拼写错误，这可能会给任务带来复杂性。为了解决这个问题，有几种方法可用，包括Levenshtein距离。此算法找到将一个字符串更改为另一个字符串所需的编辑/添加/删除的数量。例如，要将单词*put*更改为*pat*，需要将*u*替换为*a*，这是一个更改。要将单词*kitten*更改为*smitten*，需要进行两个编辑：将*k*更改为*m*并在开头添加一个*s*。
- en: In this recipe, you will be able to use this technique to find a match to a
    misspelled email.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，你将能够使用这种技术来找到与拼写错误的电子邮件的匹配项。
- en: Getting ready
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the same packages and the data scientist job description dataset
    that we used in the previous recipe, and the `python-Levenshtein` package, which
    is part of the Poetry environment and is included in the `requirements.txt` file.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前配方中相同的包和数据科学家职位描述数据集，以及`python-Levenshtein`包，它是Poetry环境的一部分，并包含在`requirements.txt`文件中。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.2_similar_strings.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.2_similar_strings.ipynb).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.2_similar_strings.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.2_similar_strings.ipynb)。
- en: How to do it…
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will read the dataset into a `pandas` DataFrame and use the emails extracted
    from it to search for a misspelled email. Your steps should be formatted like
    so:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集读入一个`pandas` DataFrame，并使用从中提取的电子邮件来搜索拼写错误的电子邮件。你的步骤应该格式化如下：
- en: 'Run the language utilities file. This file contains the **get_emails** function
    we created in the previous recipe:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行语言工具文件。此文件包含我们在之前的配方中创建的**get_emails**函数：
- en: '[PRE8]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Do the necessary imports:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE9]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Read the data into a **pandas** DataFrame object:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据读入一个**pandas** DataFrame对象：
- en: '[PRE10]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Filter out all emails from the DataFrame using the **get_emails** function,
    which is explained in more detail in the previous recipe, *Using* *regular expressions*:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**get_emails**函数从DataFrame中过滤掉所有电子邮件，该函数在之前的配方中有更详细的解释，*使用正则表达式*：
- en: '[PRE11]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The **find_levenshtein** function takes in a DataFrame and an input string
    and computes the Levenshtein distance between it and each string in the emails
    column. It takes in an input string and a DataFrame with emails and creates a
    new column in which the value is the Levenshtein distance between the input and
    the email address in the DataFrame. The column name is **distance_to_[input_string]**:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**find_levenshtein**函数接收一个DataFrame和一个输入字符串，并计算它与电子邮件列中每个字符串之间的Levenshtein距离。它接收一个输入字符串和一个包含电子邮件的DataFrame，并在其中创建一个新列，该列的值是输入与DataFrame中的电子邮件地址之间的Levenshtein距离。该列名为**distance_to_[input_string]**：'
- en: '[PRE12]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this step, we define the **get_closest_email_lev** function, which takes
    in a DataFrame with emails and an email to match and returns the email in the
    DataFrame that is closest to the input. We accomplish this by using the **find_levenshtein**
    function to create a new column with distances to the input email and then using
    the **idxmin()** function from **pandas** to find the index of the minimum value.
    We use the minimum index to find the closest email:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们定义了**get_closest_email_lev**函数，它接收一个包含电子邮件的DataFrame和一个要匹配的电子邮件，并返回DataFrame中与输入最接近的电子邮件。我们通过使用**find_levenshtein**函数创建一个包含到输入电子邮件距离的新列，然后使用**pandas**中的**idxmin()**函数找到最小值的索引。我们使用最小索引来找到最近的电子邮件：
- en: '[PRE13]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we load the emails into a new DataFrame and use the misspelled email
    address **rohitt.macdonald@prelim.com** to find a match in the new **email** DataFrame:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将电子邮件加载到新的 DataFrame 中，并使用拼写错误的电子邮件地址 **rohitt.macdonald@prelim.com**
    在新的 **email** DataFrame 中查找匹配项：
- en: '[PRE14]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The function returns `rohit.mcdonald@prolim.com`, the correct spelling of the
    email address:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数返回正确的电子邮件地址拼写 `rohit.mcdonald@prolim.com`：
- en: '[PRE15]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There’s more…
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The Levenshtein package includes other string similarity measuring methods,
    which you can explore at [https://rapidfuzz.github.io/Levenshtein/](https://rapidfuzz.github.io/Levenshtein/).
    In this section, we look at the **Jaro distance**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Levenshtein 包包括其他字符串相似度测量方法，您可以在 [https://rapidfuzz.github.io/Levenshtein/](https://rapidfuzz.github.io/Levenshtein/)
    中探索。在本节中，我们查看 **Jaro 距离**。
- en: 'We can use another function, the Jaro similarity, which outputs similarity
    between two strings as a number between `0` and `1`, where `1` means that two
    strings are the same. The process is similar, but we need the index with the maximum
    value instead of the minimum since the Jaro similarity function returns a higher
    value for more similar strings. Let’s go through the steps:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用另一个函数，即 Jaro 相似度，它将两个字符串之间的相似度输出为一个介于 `0` 和 `1` 之间的数字，其中 `1` 表示两个字符串完全相同。过程类似，但我们需要具有最大值的索引而不是最小值，因为
    Jaro 相似度函数对更相似的字符串返回更高的值。让我们一步步来：
- en: 'The **find_jaro** function takes in a DataFrame and an input string and computes
    the Jaro similarity between it and each string in the email column:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**find_jaro** 函数接收一个 DataFrame 和一个输入字符串，并计算它与电子邮件列中每个字符串之间的 Jaro 相似度：'
- en: '[PRE16]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The **get_closest_email_jaro** function uses the function we defined in the
    previous step to find the email address that is closest to the one input:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**get_closest_email_jaro** 函数使用我们在上一步中定义的函数来找到与输入最接近的电子邮件地址：'
- en: '[PRE17]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we use the misspelled email address **rohitt.macdonald@prelim.com** to
    find a match in the new email DataFrame:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用拼写错误的电子邮件地址 **rohitt.macdonald@prelim.com** 在新的电子邮件 DataFrame 中查找匹配项：
- en: '[PRE18]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE19]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'An extension of the Jaro similarity function is the **Jaro-Winkler function**,
    which attaches a weight to the end of the word, and that weight lowers the importance
    of misspellings toward the end. For example, let’s look at the following function:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jaro 相似度函数的一个扩展是 **Jaro-Winkler 函数**，它给单词的末尾附加一个权重，并且这个权重降低了末尾拼写错误的重要性。例如，让我们看看以下函数：
- en: '[PRE20]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This outputs the following:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE21]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Extracting keywords
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取关键词
- en: 'In this recipe, we will extract keywords from a text. We will be working with
    the BBC news dataset that contains news articles. You can learn more about the
    dataset in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106), in the recipe titled
    *Clustering sentences using K-Means: unsupervised* *text classification*.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将从文本中提取关键词。我们将使用包含新闻文章的 BBC 新闻数据集。您可以在[*第 4 章*](B18411_04.xhtml#_idTextAnchor106)中了解更多关于数据集的信息，该章节的标题为
    *使用 K-Means 进行句子聚类：无监督 *文本分类*。
- en: Extracting keywords from text can give us a quick idea about what the article
    is about and can also serve as a basis for a tagging system, for example, on a
    website.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取关键词可以快速了解文章的主题，也可以作为标签系统的依据，例如，在网站上。
- en: For the extraction to work correctly, we need to train a TF-IDF vectorizer that
    we will use during the extraction phase.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确提取，我们需要训练一个 TF-IDF 向量化器，我们将在提取阶段使用它。
- en: Getting ready
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the `sklearn` package. It is part of the Poetry
    environment. You can also install it together with other packages by installing
    the `requirements.txt` file.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用 `sklearn` 包。它是 Poetry 环境的一部分。您也可以通过安装 `requirements.txt` 文件来与其他包一起安装它。
- en: The BBC news dataset is available on Hugging Face at [https://huggingface.co/datasets/SetFit/bbc-news](https://huggingface.co/datasets/SetFit/bbc-news).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: BBC 新闻数据集可在 Hugging Face 上获得 [https://huggingface.co/datasets/SetFit/bbc-news](https://huggingface.co/datasets/SetFit/bbc-news)。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.3_keyword_extraction.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.3_keyword_extraction.ipynb).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.3_keyword_extraction.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.3_keyword_extraction.ipynb)。
- en: How to do it…
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'To extract keywords from a given text, we first need a corpus of text that
    we will fit the vectorizer on. Once that is done, we can use it to extract keywords
    from a text that is similar to the processed corpus. Here are the steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要从给定文本中提取关键词，我们首先需要一个我们将拟合向量器的文本语料库。一旦完成，我们就可以使用它从与处理语料库相似的文本中提取关键词。以下是步骤：
- en: 'Run the language utilities notebook:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行语言工具笔记本：
- en: '[PRE22]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Import the necessary packages and functions:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包和函数：
- en: '[PRE23]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Load the training and testing datasets, convert them to **pandas** DataFrame
    objects, and print out the training DataFrame to discover how it looks. The DataFrame
    has three columns, one for the news article text, one for the label in numeric
    format, and one for the label text:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练和测试数据集，将它们转换为**pandas** DataFrame对象，并打印出训练DataFrame以查看其外观。DataFrame有三个列，一列用于新闻文章文本，一列用于数字格式的标签，另一列用于标签文本：
- en: '[PRE24]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The result should look similar to this:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应类似于以下内容：
- en: '[PRE25]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create the vectorizer and fit it on the training data text. To learn more about
    vectorizers, see [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067). The TF-IDF vectorizer
    is discussed in the *Representing texts with TF-IDF* recipe. We use English stopwords,
    a minimum document frequency of **2**, and a maximum document frequency of 95%
    (to learn more about stopwords, see the *Removing stopwords* recipe in [*Chapter
    1*](B18411_01.xhtml#_idTextAnchor013)):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建向量器并将其拟合到训练数据文本上。要了解更多关于向量器的信息，请参阅[*第3章*](B18411_03.xhtml#_idTextAnchor067)。在*使用TF-IDF表示文本*的配方中讨论了TF-IDF向量器。我们使用英语停用词，最小文档频率为**2**，最大文档频率为95%（要了解更多关于停用词的信息，请参阅[*第1章*](B18411_01.xhtml#_idTextAnchor013)中的*移除停用词*配方）：
- en: '[PRE26]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we will define a few helper functions. The first one will sort a coordinate
    matrix by the TF-IDF score. It takes the coordinate matrix that is converted from
    the vector created by the vectorizer. This coordinate matrix’s **col** attribute
    provides the word indices and the **data** attribute provides the TF-IDF scores
    for each word. The function creates a list of tuples from this data, where the
    first value in the tuple is the index and the second value is the TF-IDF score.
    It then sorts the tuple list by the TF-IDF score and returns the sorted result.
    This will give us words that have the maximum TF-IDF score or the ones that are
    most characteristic of this particular news piece:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义一些辅助函数。第一个函数将按TF-IDF分数对坐标矩阵进行排序。它接受由向量器创建的向量转换成的坐标矩阵。该坐标矩阵的**col**属性提供单词索引，**data**属性提供每个单词的TF-IDF分数。该函数从这个数据创建一个元组列表，其中元组的第一个值是索引，第二个值是TF-IDF分数。然后它按TF-IDF分数对元组列表进行排序并返回排序结果。这将给我们具有最大TF-IDF分数的单词或最具有该特定新闻稿特征的单词：
- en: '[PRE27]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The next function, **get_keyword_strings**, will get the keywords for a given
    vector. It returns the extracted keywords for a given vector. It takes as input
    the fitted vectorizer, the number of keywords to extract, and the sorted vector
    of the input text. The function first defines the **index_dict** variable as the
    dictionary with word indices as keys and corresponding words as values. It then
    iterates through the sorted vector and appends the words from the dictionary to
    the **words** list variable. It stops when it reaches the desired number of words.
    Since the function iterates through the sorted vector, it will give us the words
    with the highest TF-IDF scores. These words will be the ones most used in this
    document but not used in other documents, thus giving us an idea about the topic
    of the article:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个函数**get_keyword_strings**将获取给定向量的关键词。它返回给定向量的提取关键词。它接受拟合的向量器、要提取的关键词数和输入文本的排序向量作为输入。该函数首先将**index_dict**变量定义为键为单词索引、值为相应单词的字典。然后它遍历排序向量并将字典中的单词追加到**words**列表变量中。它达到所需单词数时停止。由于该函数遍历排序向量，它将给出具有最高TF-IDF分数的单词。这些单词将是该文档中最常使用但在其他文档中未使用的单词，从而给我们一个关于文章主题的想法：
- en: '[PRE28]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The **get_keywords_simple** function will return a list of keywords for a given
    text. It takes in the input text, the fitted vectorizer, and the desired number
    of words. It creates a vector for the input text by using the vectorizer, then
    sorts the vector by using the **sort_data_tfidf_score** function, and finally,
    gets the top words using the **get_keyword_strings** function:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**get_keywords_simple**函数将返回给定文本的关键词列表。它接受输入文本、拟合的向量器和所需单词数。它使用向量器为输入文本创建一个向量，然后使用**sort_data_tfidf_score**函数对向量进行排序，并最终使用**get_keyword_strings**函数获取顶级单词：'
- en: '[PRE29]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We use the previous function on the first text from the test DataFrame. We
    take the first article text in the test data and create a list of keywords using
    the **get_keywords_simple** function. We see that some of the keywords fit the
    summary, and some are less suitable:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用之前的函数处理测试数据集中的第一篇文本。我们从测试数据中的第一篇文章中提取关键词列表，使用**get_keywords_simple**函数。我们发现一些关键词适合摘要，而一些则不太合适：
- en: '[PRE30]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The result will be as follows:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE31]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: There’s more…
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Now, we will use a more sophisticated approach to extracting keywords from news
    summaries. We will use a vectorizer that scores not just individual words but
    also bigrams and trigrams. We will also use the spaCy noun chunks to make sure
    that the bigrams and trigrams that are output make sense. To learn more about
    noun chunks, see the *Extracting noun chunks* recipe in [*Chapter 2*](B18411_02.xhtml#_idTextAnchor042).
    The advantage of this method is that we get not only individual words as output
    but also phrases, such as *Saturday morning* instead of just *Saturday* and *morning*
    individually.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用一种更复杂的方法来从新闻摘要中提取关键词。我们将使用一个向量器，它不仅对单个单词进行评分，还对双词和三词进行评分。我们还将使用spaCy名词短语来确保输出的双词和三词是有意义的。要了解更多关于名词短语的信息，请参阅[*第2章*](B18411_02.xhtml#_idTextAnchor042)中的*提取名词短语*配方。这种方法的优势在于，我们不仅得到单个单词作为输出，还有短语，例如*周六早晨*而不是单独的*周六*和*早晨*。
- en: 'Create the new vectorizer and fit it on the training summaries. We exclude
    the word **the** from the stopwords list since spaCy entities might contain it:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建新的向量器并将其拟合到训练摘要中。由于spaCy实体可能包含它，我们从停用词列表中排除了单词**the**：
- en: '[PRE32]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, define the **get_keyword_strings_all** function. It will get all the keywords
    from the sorted vector. It has no restriction on how many words it gets:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义**get_keyword_strings_all**函数。它将从排序向量中获取所有关键词，它对获取多少个单词没有限制：
- en: '[PRE33]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we define the **get_keywords_complex** function that outputs main keywords
    and phrases up to three words long:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义**get_keywords_complex**函数，该函数输出主要关键词和最多三个单词长的短语：
- en: '[PRE34]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we use the previous function on the first test summary:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将在第一个测试摘要上使用之前的函数：
- en: '[PRE35]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The result will look like this:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE36]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Performing named entity recognition using spaCy
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用spaCy进行命名实体识别
- en: '**Named entity recognition** (**NER**) is the task of parsing the names of
    places, people, organizations, and so on, out of text. This can be useful in many
    downstream tasks. For example, you could imagine a situation where you would like
    to sort an article set by the people that are mentioned in it, for example, when
    carrying out research about a certain person.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）是从文本中解析地点、人物、组织等名称的任务。这在许多下游任务中可能很有用。例如，你可以想象一个场景，你希望根据文章中提到的人物对文章集进行排序，例如，在研究某个特定人物时。'
- en: In this recipe, we will use NER to parse out named entities from article texts
    in the BBC dataset. We will load the package and the parsing engine and loop through
    the NER results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用NER从BBC数据集中的文章文本中解析出命名实体。我们将加载包和解析引擎，并遍历NER结果。
- en: Getting ready
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will use spaCy. To run it correctly, you will need to download
    a language model. We will download the small and large models. These models take
    up significant disk space:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用spaCy。要正确运行它，你需要下载一个语言模型。我们将下载小型和大型模型。这些模型占用大量的磁盘空间：
- en: '[PRE37]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.4_named_entity_extraction.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.4_named_entity_extraction.ipynb).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.4_named_entity_extraction.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.4_named_entity_extraction.ipynb)。
- en: How to do it…
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'NER happens automatically with the processing that spaCy does for an input
    text. Accessing the entities happens through the `doc.ents` variable. We will
    input an article about Apple’s iPhone and see which entities will get parsed from
    it. Let’s see the steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: NER在spaCy对输入文本的处理过程中自动发生。通过`doc.ents`变量访问实体。我们将输入一篇关于苹果iPhone的文章，并查看哪些实体将从其中解析出来。让我们看看步骤：
- en: 'Run the language utilities file. This will import the necessary packages and
    functions and initialize the spaCy engine:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行语言工具文件。这将导入必要的包和函数并初始化spaCy引擎：
- en: '[PRE38]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Initialize the article text. This is an article from [https://www.globalsmt.net/social-media-news/iphone-12-apple-makes-jump-to-5g/:](https://www.globalsmt.net/social-media-news/iphone-12-apple-makes-jump-to-5g/:)
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化文章文本。这是一篇来自 [https://www.globalsmt.net/social-media-news/iphone-12-apple-makes-jump-to-5g/:](https://www.globalsmt.net/social-media-news/iphone-12-apple-makes-jump-to-5g/:)
- en: '[PRE39]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here, we create the spaCy **Doc** object and use it to extract the entities.
    The **Doc** object is created by using the small spaCy model on the text. The
    model extracts different attributes, including named entities. We print the length
    of the parsed entities and the entities themselves, together with start and end
    character information and the entity type (the meaning of the named entity labels
    can be found in the spaCy documentation at [https://spacy.io/models/en](https://spacy.io/models/en)):'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们创建 spaCy **Doc** 对象并使用它来提取实体。**Doc** 对象是通过在文本上使用小 spaCy 模型创建的。该模型提取不同的属性，包括命名实体。我们打印出解析出的实体的长度以及实体本身，包括起始和结束字符信息以及实体类型（命名实体标签的含义可以在
    spaCy 文档的 [https://spacy.io/models/en](https://spacy.io/models/en) 找到）：
- en: '[PRE40]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'When we print out the result, we see different types of entities, including
    cardinal numbers, percentages, names of people, dates, organizations, and a `NORP`
    entity, which stands for **Nationalities or Religious or** **Political groups**:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们打印出结果时，我们可以看到不同类型的实体，包括基数词、百分比、人名、日期、组织以及一个 `NORP` 实体，代表 **国籍或宗教或** **政治团体**：
- en: '[PRE41]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: There’s more…
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We can compare the performance of the small and large models with the following
    steps:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下步骤比较小模型和大模型的表现：
- en: 'Run the same step as *step 3* from the *How to do it…* section but with the
    large model:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *How to do it…* 部分的 *步骤 3* 开始运行相同的步骤，但使用大型模型：
- en: '[PRE42]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The result will be as follows:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE43]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'There are more entities parsed by the large model, and we can take a look at
    the differences. We print out two lists; one list contains entities that the small
    model recognizes, and the other list contains entities that the large model recognizes
    but not the small:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大型模型解析出的实体更多，我们可以看看它们之间的差异。我们打印出两个列表；一个列表包含小模型识别的实体，另一个列表包含大模型识别但小模型没有识别的实体：
- en: '[PRE44]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The result will be as follows:'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE45]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: You can see that there are some differences between the results provided by
    the two models.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，两个模型提供的结果之间存在一些差异。
- en: Training your own NER model with spaCy
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 spaCy 训练自己的 NER 模型
- en: In the previous recipe, we used the pretrained spaCy model to extract named
    entities. This NER model can suffice in many cases. There might be other times,
    however, when we would like to create a new one from scratch. In this recipe,
    we will train a new NER model to parse out the names of musicians and their works
    of art.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个菜谱中，我们使用了预训练的 spaCy 模型来提取命名实体。这个 NER 模型在很多情况下已经足够使用。然而，可能会有其他时候，我们希望从头开始创建一个新的模型。在这个菜谱中，我们将训练一个新的
    NER 模型来解析音乐家和他们的艺术作品的名字。
- en: Getting ready
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the spaCy package to train a new NER model. You do not need any
    other packages other than `spacy`. The data we are going to use is from [https://github.com/deezer/music-ner-eacl2023](https://github.com/deezer/music-ner-eacl2023).
    The data file is preloaded in the data folder ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/music_ner.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/music_ner.csv))
    and you will need to download it from the book’s GitHub repository into the `data`
    directory.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 spaCy 包来训练一个新的 NER 模型。除了 `spacy` 之外，你不需要其他任何包。我们将使用的数据来自 [https://github.com/deezer/music-ner-eacl2023](https://github.com/deezer/music-ner-eacl2023)。数据文件已预先加载在数据文件夹中（[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/music_ner.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/music_ner.csv)），你需要从本书的
    GitHub 仓库将其下载到 `data` 目录中。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.5_training_own_spacy_model.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.5_training_own_spacy_model.ipynb).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.5_training_own_spacy_model.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.5_training_own_spacy_model.ipynb)。
- en: How to do it…
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will define our training data and then use it to train a new model. We will
    then test the model and save it to disk. The steps are as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义我们的训练数据，然后使用它来训练一个新的模型。然后我们将测试该模型并将其保存到磁盘。步骤如下：
- en: 'Run the language utilities file:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行语言工具文件：
- en: '[PRE46]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Import other functions and packages:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入其他函数和包：
- en: '[PRE47]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In this step, we load the data and print it out:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们加载数据并打印出来：
- en: '[PRE48]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The data has five columns: `id`, `start offset`, `end offset`, `text`, and
    `label`. Sentences repeat if there is more than one entity per sentence, as there
    is one row per named entity. There are 428 entries in the data.'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据有五个列：`id`、`start offset`、`end offset`、`text`和`label`。如果句子中有一个以上的实体，则句子会重复，因为每个命名实体有一行。数据中有428个条目。
- en: '![](img/B18411_05_2.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18411_05_2.jpg)'
- en: Figure 5.2 – DataFrame output
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – DataFrame输出
- en: 'Here, we remove **_deduced** from the labels so the labels are now **Artist**,
    **WoA (work of** **art)**, **Artist_or_WoA**:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将**_deduced**从标签中删除，因此标签现在是**Artist**、**WoA (work of art)**、**Artist_or_WoA**：
- en: '[PRE49]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The result will look like this:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '![](img/B18411_05_3.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18411_05_3.jpg)'
- en: Figure 5.3 – DataFrame output
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – DataFrame输出
- en: 'In this step, we create the **DocBin** objects that will store the processed
    data. **DocBin** objects are required for input data for spaCy models (to learn
    more about them, see the *Training a spaCy textcat model* recipe in [*Chapter
    4*](B18411_04.xhtml#_idTextAnchor106)):'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们创建存储处理数据的**DocBin**对象。**DocBin**对象是spaCy模型输入数据所必需的（要了解更多信息，请参阅[*第4章*](B18411_04.xhtml#_idTextAnchor106)中的*训练spaCy
    textcat模型*配方）：
- en: '[PRE50]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here, we create a list of unique IDs and split it into training and test data.
    The reason we would like to get the unique IDs is because sentences repeat through
    the dataset. There are 227 unique IDs (or sentences), and there are 170 sentences
    in the training data and 57 sentences in the test data:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们创建一个唯一的ID列表并将其拆分为训练和测试数据。我们想要获取唯一ID的原因是因为句子在数据集中重复。有227个唯一的ID（或句子），训练数据中有170个句子，测试数据中有57个句子：
- en: '[PRE51]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The result will be as follows:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE52]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Here, we create and save training and test data in the **DocBin** objects.
    We loop through IDs, and for each ID, we get the sentence. We process the sentence
    using the small model and then have a spaCy **Doc** object. Then, we loop through
    the entities in the sentence and add them to the **ents** attribute of the **Doc**
    object. The processed **Doc** object then goes into one of the **DocBin** objects:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们创建并保存训练和测试数据到**DocBin**对象中。我们遍历ID，对于每个ID，我们获取句子。我们使用小型模型处理句子，然后得到一个spaCy
    **Doc**对象。然后，我们遍历句子中的实体并将它们添加到**Doc**对象的**ents**属性中。处理后的**Doc**对象然后进入一个**DocBin**对象：
- en: '[PRE53]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In this step, we train the model. We use the **spacy_config_ner.cfg** configuration
    file in the **data** folder. You can create your own customized configuration
    file at [https://spacy.io/usage/training/#quickstart](https://spacy.io/usage/training/#quickstart).
    The output shows the loss, accuracy, precision, recall, F1 score, and other metrics
    for every epoch. Finally, it saves the model to the specified directory:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们训练模型。我们使用**data**文件夹中的**spacy_config_ner.cfg**配置文件。您可以在[https://spacy.io/usage/training/#quickstart](https://spacy.io/usage/training/#quickstart)创建自己的定制配置文件。输出显示了每个epoch的损失、准确率、精确率、召回率、F1分数和其他指标。最后，它将模型保存到指定的目录：
- en: '[PRE54]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output will look like this:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![](img/B18411_05_4.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18411_05_4.jpg)'
- en: Figure 5.4 – Model training output
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 模型训练输出
- en: 'In this step, we load the trained model and use it on data not seen during
    training. We get an ID from the test set, get all the rows from the test data
    with that ID, and load the sentence. We then print out the sentence and the annotated
    entities. Then, we process the sentence using our model (in exactly the same way
    as other pretrained spaCy models) and print out the entities it parsed:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们加载训练好的模型并使用它对训练期间未见过的数据进行处理。我们从测试集中获取一个ID，获取具有该ID的所有测试数据行，并加载句子。然后我们打印出句子和标注的实体。然后，我们使用我们的模型（与其他预训练的spaCy模型完全相同的方式）处理句子并打印出它解析的实体：
- en: '[PRE55]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We see that the resulting entities are quite good (output results may vary):'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到生成的实体相当不错（输出结果可能有所不同）：
- en: '[PRE56]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here, we evaluate the model using spaCy’s **evaluate** function. We see that
    the **WoA** and **Artist** tags have metrics that are low but in the double digits,
    while the **Artist_or_WoA** tag has an F1 score of about 10%. This is due to the
    fact that it has much less data than the other two tags. Overall, the performance
    of the model according to the statistics is not very good, and that is because
    we have a very small amount of data overall:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们使用 spaCy 的 **evaluate** 函数评估模型。我们看到 **WoA** 和 **Artist** 标签的指标低但为两位数，而
    **Artist_or_WoA** 标签的 F1 分数约为 10%。这是因为它比其他两个标签的数据量少得多。总体而言，根据统计数据，模型的性能并不很好，这是因为我们整体的数据量非常小：
- en: '[PRE57]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The statistics might vary, but here is the output I got (output condensed):'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 统计数据可能会有所不同，但以下是我得到的结果（输出已压缩）：
- en: '[PRE58]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: See also
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'The spaCy NER model is a neural network model. You can learn more about its
    architecture from the spaCy documentation: [https://spacy.io/models#architecture](https://spacy.io/models#architecture).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 命名实体识别模型是一个神经网络模型。你可以从 spaCy 文档中了解更多关于其架构的信息：[https://spacy.io/models#architecture](https://spacy.io/models#architecture)。
- en: Fine-tuning BERT for NER
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调 BERT 用于命名实体识别
- en: In this recipe, we will fine-tune the pretrained BERT model for the NER task.
    The difference between training a model from scratch and fine-tuning it is as
    follows. Fine-tuning an NLP model, such as BERT, involves taking a pretrained
    model and modifying it for your specific task, such as NER in this case. The pretrained
    model already has lots of knowledge stored in it and the results are likely to
    be better than when training a model from scratch.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将微调预训练的 BERT 模型以用于命名实体识别任务。从头开始训练模型和微调模型之间的区别如下。微调 NLP 模型，如 BERT，涉及取一个预训练模型并对其进行修改以适应你的特定任务，例如本例中的
    NER。预训练模型已经存储了大量的知识，结果可能比从头开始训练模型要好。
- en: We will use similar data as in the previous recipe, creating a model that can
    tag entities as `Artist` or `WoA`. The data comes from the same dataset but it
    is labeled using the IOB format, which is required for the `transformers` packages
    we are going to use. We also only use the `Artist` and `WoA` tags, removing the
    `Artist_or_WoA` tag, since there is not enough data for that tag.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与上一个配方类似的数据，创建一个可以将实体标记为 `Artist` 或 `WoA` 的模型。数据来自同一个数据集，但使用 IOB 格式进行标记，这是我们将要使用的
    `transformers` 包所必需的。我们还只使用了 `Artist` 和 `WoA` 标签，移除了 `Artist_or_WoA` 标签，因为该标签的数据量不足。
- en: For this recipe, we will use the Hugging Face `Trainer` class, although it is
    also possible to train Hugging Face models using PyTorch or Tensorflow. See more
    at [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将使用 Hugging Face 的 `Trainer` 类，尽管也可以使用 PyTorch 或 Tensorflow 训练 Hugging
    Face 模型。更多信息请参阅 [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)。
- en: Getting ready
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the `transformers` package from Hugging Face. It is preloaded in
    the Poetry environment. You can also install the package from the `requirements.txt`
    file.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Hugging Face 的 `transformers` 包。它在 Poetry 环境中已预加载。你也可以从 `requirements.txt`
    文件中安装此包。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.6_fine_tune_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.6_fine_tune_bert.ipynb).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.6_fine_tune_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.6_fine_tune_bert.ipynb)。
- en: How to do it…
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We will load and preprocess the data, train the model, and evaluate it, and
    then we will use it on unseen data. Your steps should be formatted like so:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载数据并进行预处理，训练模型，然后评估它，最后我们将使用它对未见过的数据进行处理。你的步骤应该格式化如下：
- en: 'Run the language utilities notebook:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行语言工具笔记本：
- en: '[PRE59]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Import other packages and functions:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入其他包和函数：
- en: '[PRE60]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In this step, we load the music NER dataset using the pandas **read_csv** function.
    We then define a function that takes a label, splits it on the underscore, and
    removes the last part (**_deduced**). We then apply this function to the **label**
    column. We also substitute the **|** character in case it could interfere with
    our code:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们使用 pandas 的 **read_csv** 函数加载音乐命名实体识别数据集。然后我们定义一个函数，它接受一个标签，将其在下划线处分割，并移除最后一部分（**_deduced**）。然后我们将此函数应用于
    **label** 列。我们还替换了 **|** 字符，以防它可能干扰我们的代码：
- en: '[PRE61]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output will look similar to this:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '![](img/B18411_05_5.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18411_05_5.jpg)'
- en: Figure 5.5 – Dataset DataFrame
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 数据集 DataFrame
- en: 'Here, we start our data preprocessing. We get the list of unique IDs from the
    **id** column. We loop through this list and get the sentence that corresponds
    to the given ID. We then process the text using the spaCy small model and add
    the entities from the DataFrame into the **Doc** object. We then store each of
    these sentences in a dictionary in which the keys are the sentence text strings
    and the values are the **Doc** objects:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们开始我们的数据预处理。我们从 **id** 列中获取唯一 ID 的列表。我们遍历这个列表，获取与给定 ID 对应的句子。然后，我们使用 spaCy
    小型模型处理文本，并将 DataFrame 中的实体添加到 **Doc** 对象中。然后，我们将每个句子存储在一个字典中，其中键是句子文本字符串，值是 **Doc**
    对象：
- en: '[PRE62]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, we load the data in IOB format. This format is required to fine-tune BERT,
    as opposed to the format used by spaCy. For that, we load a separate data file,
    **../data/music_ner_bio.bio**. We create a dictionary of tag mappings and initialize
    empty lists for tokens, NER tags, and spans. We then loop through the sentence
    data we read from the data file. For each sentence, each line is a pair of a word
    and its label. We append the words to the **words** list and the numbers corresponding
    to the labels to the **tags** list. We also get the spans from the dictionary
    of **Doc** objects we created in the previous step and append those to the **spans**
    list:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们以 IOB 格式加载数据。这种格式是微调 BERT 所必需的，与 spaCy 使用的格式不同。为此，我们加载一个单独的数据文件，**../data/music_ner_bio.bio**。我们创建一个标签映射字典，并为标记、NER
    标签和跨度初始化空列表。然后，我们遍历从数据文件中读取的句子数据。对于每个句子，每行都是一个词及其标签的配对。我们将单词添加到 **words** 列表，并将与标签对应的数字添加到
    **tags** 列表。我们还从之前步骤中创建的 **Doc** 对象字典中获取跨度，并将其添加到 **spans** 列表：
- en: '[PRE63]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Here, we split the data into training and testing. For that, we split the indices
    of the **spans** list. Then, we create separate tokens, NER tags, and **spans**
    lists for training and test data:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将数据分为训练集和测试集。为此，我们拆分 **spans** 列表的索引。然后，我们为训练数据和测试数据创建单独的标记、NER 标签和 **spans**
    列表：
- en: '[PRE64]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output will be as follows:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE65]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In this step, we create new DataFrames from the training and test lists we
    compiled in *step 6*. We then join the contents of the **tokens** column with
    spaces to get a sentence string instead of a list of words. We then drop empty
    data using the **dropna()** function and print the contents of the test DataFrame:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们从在 *步骤 6* 中编译的训练和测试列表中创建新的 DataFrames。然后，我们将 **tokens** 列的内容与空格连接起来，以获取句子字符串而不是单词列表。然后，我们使用
    **dropna()** 函数删除空数据，并打印测试 DataFrame 的内容：
- en: '[PRE66]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The result will look like this:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE67]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Here, we load the pretrained model and tokenizer and initialize the **Dataset**
    objects. The **Features** object describes the data and its properties. We create
    one training and one test **Dataset** object. We use the **Features** object we
    created, and the DataFrames we initialized in the previous step. We then add these
    newly created **Dataset** objects to **DatasetDict**, with one entry for the training
    dataset and one for the test data. We then print out the resulting object:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们加载预训练模型和分词器，并初始化 **Dataset** 对象。**Features** 对象描述了数据和其属性。我们创建一个训练集和一个测试集
    **Dataset** 对象。我们使用之前创建的 **Features** 对象和之前步骤中初始化的 DataFrames。然后，我们将这些新创建的 **Dataset**
    对象添加到 **DatasetDict** 中，一个条目用于训练数据集，另一个用于测试数据。然后，我们打印出结果对象：
- en: '[PRE68]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The result will be as follows:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE69]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'In this step, we create the **tokenize_adjust_labels** function that will assign
    the correct labels to word parts. We define the **tokenize_adjust_labels** function.
    The BERT tokenizer splits some words into components, and we need to make sure
    that the same label is assigned to each word part. The function first tokenizes
    all the text samples using the preloaded tokenizer. It then loops through the
    input IDs of the tokenized samples and adjusts the labels according to the word
    parts:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们创建了一个名为 **tokenize_adjust_labels** 的函数，该函数将为词的部分分配正确的标签。我们定义了 **tokenize_adjust_labels**
    函数。BERT 分词器将一些词拆分成组件，我们需要确保每个词的部分都被分配相同的标签。该函数首先使用预加载的分词器对所有文本样本进行分词。然后，它遍历分词样本的输入
    ID，并根据词的部分调整标签：
- en: '[PRE70]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Use the previous function on the dataset:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据集上使用之前的函数：
- en: '[PRE71]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Here, we initialize the data collator object. Data collators simplify the handling
    of data for training, for example, padding and truncating the input for all inputs
    to be of the same length:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们初始化数据合并对象。数据合并器简化了训练数据的手动处理，例如，对所有的输入进行填充和截断，以确保它们具有相同的长度：
- en: '[PRE72]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now, we create the **compute_metrics** function, which calculates evaluation
    metrics, including precision, recall, F1 score, and accuracy. In the function,
    we delete all the tokens that have the label **-100**, which are the special tokens.
    This function uses the **seqeval** evaluation method commonly used to evaluate
    NER tasks:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建**compute_metrics**函数，该函数计算评估指标，包括精确度、召回率、F1分数和准确率。在函数中，我们删除所有带有标签**-100**的标记，这些是特殊标记。此函数使用**seqeval**评估方法，这是常用的用于评估NER任务的评估方法：
- en: '[PRE73]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Here, we load the pretrained BERT model (the uncased version since our input
    is in lowercase). We then specify the training arguments by initializing the **TrainingArguments**
    object. This object contains the model hyperparameters. We then initialize the
    **Trainer** object by providing the training arguments, dataset, tokenizer, data
    collator, and **metrics** function. We then start the training process:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们加载预训练的BERT模型（由于我们的输入是小写，所以使用未分词版本）。然后通过初始化**TrainingArguments**对象来指定训练参数。此对象包含模型超参数。然后通过提供训练参数、数据集、分词器、数据收集器和**metrics**函数来初始化**Trainer**对象。然后开始训练过程：
- en: '[PRE74]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output will include different information, including the following:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将包括不同的信息，包括以下内容：
- en: '[PRE75]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'In this step, we evaluate the fine-tuned model:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们评估微调后的模型：
- en: '[PRE76]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'For the `Artist` label, it achieves an F1 score of 76%, and for the `WoA` label,
    it achieves an F1 score of 52%:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于**Artist**标签，它达到了76%的F1分数，而对于**WoA**标签，它达到了52%的F1分数：
- en: '[PRE77]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Save the model:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存模型：
- en: '[PRE78]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Now, load the trained model:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，加载训练好的模型：
- en: '[PRE79]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Here, we test the fine-tuned model on an unseen text. We initialize the **text**
    variable. We then load the **pipeline** package to create a pipeline we will use.
    A text-processing pipeline takes the text to its final output value processed
    by the model. This particular pipeline specifies the task as **token-classification**,
    which fine-tuned model to use, the corresponding tokenizer, and the aggregation
    strategy. The aggregation strategy parameter specifies how to combine the results
    of several models when several models are used. We then run the pipeline on the
    text:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们在一个未见过的文本上测试微调后的模型。我们初始化**text**变量。然后加载**pipeline**包以创建我们将使用的管道。一个文本处理管道将文本传递给模型，并得到最终的处理输出值。这个特定的管道指定了任务为**token-classification**，即使用哪个微调模型，相应的分词器，以及聚合策略。聚合策略参数指定了当使用多个模型时如何组合多个模型的结果。然后我们在文本上运行管道：
- en: '[PRE80]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output will vary. The sample output identifies the music artist, *Morphine*
    *Robocobra Quartet*:'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将会有所不同。以下是一个示例输出，它识别了音乐艺术家，*Morphine* *Robocobra Quartet*：
- en: '[PRE81]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: We can see that the labels assigned by the model are correct.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型分配的标签是正确的。
