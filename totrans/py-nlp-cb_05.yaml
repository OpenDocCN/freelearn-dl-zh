- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Started with Information Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover the basics of **information extraction**. Information
    extraction is the task of pulling very specific information from text. For example,
    you might want to know the companies mentioned in a news article. Instead of spending
    time reading the whole article, you can use information extraction techniques
    to access the companies almost instantly.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with extracting emails addresses and URLs from job announcements.
    Then, we will use an algorithm called **Levenshtein distance** to find similar
    strings. Next, we will extract important keywords from text. After that, we will
    use **spaCy** to find named entities in text, and later, we will train our own
    named entity recognition model in spaCy. We will then do basic sentiment analysis,
    and, finally, we will train two custom sentiment analysis models.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn how to use existing tools and train your own models for information
    extraction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding similar strings – Levenshtein distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting keywords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing named entity recognition using spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training your own NER model with spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning BERT for NER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is in a folder named `Chapter05` in the GitHub repository
    of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter05)).
  prefs: []
  type: TYPE_NORMAL
- en: As in previous chapters, the packages required for this chapter are part of
    the Poetry environment. Alternatively, you can install all the packages using
    the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: Using regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use regular expressions to find email addresses and
    URLs in text. Regular expressions are special character sequences that define
    search patterns and can be created and used via the Python `re` package. We will
    use a job descriptions dataset and write two regular expressions, one for emails
    and one for URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Download the job descriptions dataset here: [https://www.kaggle.com/andrewmvd/data-scientist-jobs](https://www.kaggle.com/andrewmvd/data-scientist-jobs).
    It is also available in the book’s GitHub repository at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/DataScientist.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/DataScientist.csv).
    Save it into the `/``data` folder.'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.1_regex.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.1_regex.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will read the data from the CSV file into a `pandas` DataFrame and will
    use the Python `re` package to create regular expressions and search the text.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the **re** and **pandas** packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the data and check the contents inside it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be long and should start like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – DataFrame output
  prefs: []
  type: TYPE_NORMAL
- en: 'The **get_list_of_items** helper function takes a DataFrame as input and turns
    one of its columns into a list. It accepts the DataFrame and the column name as
    inputs. First, it gets the column values, which is a list of lists, and then flattens
    that list. It then removes duplicates by turning the list into a set and casts
    it back to a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define the **get_emails** function to get all the emails that
    appear in the **Job Description** column. The regular expression consists of three
    parts that appear in square brackets followed by quantifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[^\s:|()\'']+** is the username part of the regular expression, followed
    by the **@** sign. It consists of one group of characters, which is shown in square
    brackets. Any characters from this group may appear in the username one or more
    times. This is shown using the **+** quantifier. The characters in the username
    can be anything but a space (**\s**), colon (**:**), pipe (**|**), and apostrophe
    (**''**). The **^** character shows the negation of the character class. An apostrophe
    is a special character in regular expressions and has to be escaped with a backward
    slash in order to invoke the regular meaning of the character.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[a-zA-Z0-9\.]+** is the first part of the domain name, followed by a dot.
    This part is simply alphanumeric characters, lowercase or uppercase, and a dot
    appearing one or more times. Since a dot is a special character, we escape it
    with a backward slash. The **a-z** expression signifies a range of characters
    from *a* to *z*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[a-zA-Z]+** is the last part of the domain name, which is the top-level domain,
    such as **.com**, **.org**, and so on. Usually, no digits are allowed in these
    top-level domains, and the regular expression matches lowercase or uppercase characters
    that appear one or more times.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This regular expression is sufficient to parse all emails in the dataset and
    not present any false positives. You might find that, in your data, there are
    additional adjustments that need to be made to the regular expression:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now get the emails from the DataFrame using the previous functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `finditer` function from the `re` package. It finds all matches in a text
    and returns them as `Match` objects. We can find the start and end of the match
    by using the `span()`object method. It returns a tuple, where the first element
    is the start and the second element is the end of the match:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will get the URLs in a similar fashion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Part of the result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Writing regular expressions can quickly turn into a messy affair. I use regular
    expression testing websites to enter the text in which I expect a match and the
    regular expression. One example of such a site is [https://regex101.com/](https://regex101.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Finding similar strings – Levenshtein distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When doing information extraction, in many cases, we deal with misspellings,
    which can bring complications to the task. To get around this problem, several
    methods are available, including Levenshtein distance. This algorithm finds the
    number of edits/additions/deletions needed to change one string into another.
    For example, to change the word *put* into *pat*, you need to substitute *u* for
    *a*, and that is one change. To change the word *kitten* into *smitten*, you need
    to do two edits: change *k* into *m* and add an *s* at the start.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will be able to use this technique to find a match to a
    misspelled email.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the same packages and the data scientist job description dataset
    that we used in the previous recipe, and the `python-Levenshtein` package, which
    is part of the Poetry environment and is included in the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.2_similar_strings.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.2_similar_strings.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will read the dataset into a `pandas` DataFrame and use the emails extracted
    from it to search for a misspelled email. Your steps should be formatted like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the language utilities file. This file contains the **get_emails** function
    we created in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the data into a **pandas** DataFrame object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Filter out all emails from the DataFrame using the **get_emails** function,
    which is explained in more detail in the previous recipe, *Using* *regular expressions*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **find_levenshtein** function takes in a DataFrame and an input string
    and computes the Levenshtein distance between it and each string in the emails
    column. It takes in an input string and a DataFrame with emails and creates a
    new column in which the value is the Levenshtein distance between the input and
    the email address in the DataFrame. The column name is **distance_to_[input_string]**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define the **get_closest_email_lev** function, which takes
    in a DataFrame with emails and an email to match and returns the email in the
    DataFrame that is closest to the input. We accomplish this by using the **find_levenshtein**
    function to create a new column with distances to the input email and then using
    the **idxmin()** function from **pandas** to find the index of the minimum value.
    We use the minimum index to find the closest email:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load the emails into a new DataFrame and use the misspelled email
    address **rohitt.macdonald@prelim.com** to find a match in the new **email** DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The function returns `rohit.mcdonald@prolim.com`, the correct spelling of the
    email address:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Levenshtein package includes other string similarity measuring methods,
    which you can explore at [https://rapidfuzz.github.io/Levenshtein/](https://rapidfuzz.github.io/Levenshtein/).
    In this section, we look at the **Jaro distance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use another function, the Jaro similarity, which outputs similarity
    between two strings as a number between `0` and `1`, where `1` means that two
    strings are the same. The process is similar, but we need the index with the maximum
    value instead of the minimum since the Jaro similarity function returns a higher
    value for more similar strings. Let’s go through the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **find_jaro** function takes in a DataFrame and an input string and computes
    the Jaro similarity between it and each string in the email column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **get_closest_email_jaro** function uses the function we defined in the
    previous step to find the email address that is closest to the one input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we use the misspelled email address **rohitt.macdonald@prelim.com** to
    find a match in the new email DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An extension of the Jaro similarity function is the **Jaro-Winkler function**,
    which attaches a weight to the end of the word, and that weight lowers the importance
    of misspellings toward the end. For example, let’s look at the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Extracting keywords
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will extract keywords from a text. We will be working with
    the BBC news dataset that contains news articles. You can learn more about the
    dataset in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106), in the recipe titled
    *Clustering sentences using K-Means: unsupervised* *text classification*.'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting keywords from text can give us a quick idea about what the article
    is about and can also serve as a basis for a tagging system, for example, on a
    website.
  prefs: []
  type: TYPE_NORMAL
- en: For the extraction to work correctly, we need to train a TF-IDF vectorizer that
    we will use during the extraction phase.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the `sklearn` package. It is part of the Poetry
    environment. You can also install it together with other packages by installing
    the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: The BBC news dataset is available on Hugging Face at [https://huggingface.co/datasets/SetFit/bbc-news](https://huggingface.co/datasets/SetFit/bbc-news).
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.3_keyword_extraction.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.3_keyword_extraction.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To extract keywords from a given text, we first need a corpus of text that
    we will fit the vectorizer on. Once that is done, we can use it to extract keywords
    from a text that is similar to the processed corpus. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the language utilities notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the necessary packages and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the training and testing datasets, convert them to **pandas** DataFrame
    objects, and print out the training DataFrame to discover how it looks. The DataFrame
    has three columns, one for the news article text, one for the label in numeric
    format, and one for the label text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the vectorizer and fit it on the training data text. To learn more about
    vectorizers, see [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067). The TF-IDF vectorizer
    is discussed in the *Representing texts with TF-IDF* recipe. We use English stopwords,
    a minimum document frequency of **2**, and a maximum document frequency of 95%
    (to learn more about stopwords, see the *Removing stopwords* recipe in [*Chapter
    1*](B18411_01.xhtml#_idTextAnchor013)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define a few helper functions. The first one will sort a coordinate
    matrix by the TF-IDF score. It takes the coordinate matrix that is converted from
    the vector created by the vectorizer. This coordinate matrix’s **col** attribute
    provides the word indices and the **data** attribute provides the TF-IDF scores
    for each word. The function creates a list of tuples from this data, where the
    first value in the tuple is the index and the second value is the TF-IDF score.
    It then sorts the tuple list by the TF-IDF score and returns the sorted result.
    This will give us words that have the maximum TF-IDF score or the ones that are
    most characteristic of this particular news piece:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next function, **get_keyword_strings**, will get the keywords for a given
    vector. It returns the extracted keywords for a given vector. It takes as input
    the fitted vectorizer, the number of keywords to extract, and the sorted vector
    of the input text. The function first defines the **index_dict** variable as the
    dictionary with word indices as keys and corresponding words as values. It then
    iterates through the sorted vector and appends the words from the dictionary to
    the **words** list variable. It stops when it reaches the desired number of words.
    Since the function iterates through the sorted vector, it will give us the words
    with the highest TF-IDF scores. These words will be the ones most used in this
    document but not used in other documents, thus giving us an idea about the topic
    of the article:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **get_keywords_simple** function will return a list of keywords for a given
    text. It takes in the input text, the fitted vectorizer, and the desired number
    of words. It creates a vector for the input text by using the vectorizer, then
    sorts the vector by using the **sort_data_tfidf_score** function, and finally,
    gets the top words using the **get_keyword_strings** function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the previous function on the first text from the test DataFrame. We
    take the first article text in the test data and create a list of keywords using
    the **get_keywords_simple** function. We see that some of the keywords fit the
    summary, and some are less suitable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will use a more sophisticated approach to extracting keywords from news
    summaries. We will use a vectorizer that scores not just individual words but
    also bigrams and trigrams. We will also use the spaCy noun chunks to make sure
    that the bigrams and trigrams that are output make sense. To learn more about
    noun chunks, see the *Extracting noun chunks* recipe in [*Chapter 2*](B18411_02.xhtml#_idTextAnchor042).
    The advantage of this method is that we get not only individual words as output
    but also phrases, such as *Saturday morning* instead of just *Saturday* and *morning*
    individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the new vectorizer and fit it on the training summaries. We exclude
    the word **the** from the stopwords list since spaCy entities might contain it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define the **get_keyword_strings_all** function. It will get all the keywords
    from the sorted vector. It has no restriction on how many words it gets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the **get_keywords_complex** function that outputs main keywords
    and phrases up to three words long:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we use the previous function on the first test summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performing named entity recognition using spaCy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Named entity recognition** (**NER**) is the task of parsing the names of
    places, people, organizations, and so on, out of text. This can be useful in many
    downstream tasks. For example, you could imagine a situation where you would like
    to sort an article set by the people that are mentioned in it, for example, when
    carrying out research about a certain person.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use NER to parse out named entities from article texts
    in the BBC dataset. We will load the package and the parsing engine and loop through
    the NER results.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will use spaCy. To run it correctly, you will need to download
    a language model. We will download the small and large models. These models take
    up significant disk space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.4_named_entity_extraction.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.4_named_entity_extraction.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NER happens automatically with the processing that spaCy does for an input
    text. Accessing the entities happens through the `doc.ents` variable. We will
    input an article about Apple’s iPhone and see which entities will get parsed from
    it. Let’s see the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the language utilities file. This will import the necessary packages and
    functions and initialize the spaCy engine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Initialize the article text. This is an article from [https://www.globalsmt.net/social-media-news/iphone-12-apple-makes-jump-to-5g/:](https://www.globalsmt.net/social-media-news/iphone-12-apple-makes-jump-to-5g/:)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we create the spaCy **Doc** object and use it to extract the entities.
    The **Doc** object is created by using the small spaCy model on the text. The
    model extracts different attributes, including named entities. We print the length
    of the parsed entities and the entities themselves, together with start and end
    character information and the entity type (the meaning of the named entity labels
    can be found in the spaCy documentation at [https://spacy.io/models/en](https://spacy.io/models/en)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we print out the result, we see different types of entities, including
    cardinal numbers, percentages, names of people, dates, organizations, and a `NORP`
    entity, which stands for **Nationalities or Religious or** **Political groups**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can compare the performance of the small and large models with the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the same step as *step 3* from the *How to do it…* section but with the
    large model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are more entities parsed by the large model, and we can take a look at
    the differences. We print out two lists; one list contains entities that the small
    model recognizes, and the other list contains entities that the large model recognizes
    but not the small:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see that there are some differences between the results provided by
    the two models.
  prefs: []
  type: TYPE_NORMAL
- en: Training your own NER model with spaCy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we used the pretrained spaCy model to extract named
    entities. This NER model can suffice in many cases. There might be other times,
    however, when we would like to create a new one from scratch. In this recipe,
    we will train a new NER model to parse out the names of musicians and their works
    of art.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the spaCy package to train a new NER model. You do not need any
    other packages other than `spacy`. The data we are going to use is from [https://github.com/deezer/music-ner-eacl2023](https://github.com/deezer/music-ner-eacl2023).
    The data file is preloaded in the data folder ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/music_ner.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/music_ner.csv))
    and you will need to download it from the book’s GitHub repository into the `data`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.5_training_own_spacy_model.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.5_training_own_spacy_model.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will define our training data and then use it to train a new model. We will
    then test the model and save it to disk. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the language utilities file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import other functions and packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we load the data and print it out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The data has five columns: `id`, `start offset`, `end offset`, `text`, and
    `label`. Sentences repeat if there is more than one entity per sentence, as there
    is one row per named entity. There are 428 entries in the data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – DataFrame output
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we remove **_deduced** from the labels so the labels are now **Artist**,
    **WoA (work of** **art)**, **Artist_or_WoA**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – DataFrame output
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we create the **DocBin** objects that will store the processed
    data. **DocBin** objects are required for input data for spaCy models (to learn
    more about them, see the *Training a spaCy textcat model* recipe in [*Chapter
    4*](B18411_04.xhtml#_idTextAnchor106)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we create a list of unique IDs and split it into training and test data.
    The reason we would like to get the unique IDs is because sentences repeat through
    the dataset. There are 227 unique IDs (or sentences), and there are 170 sentences
    in the training data and 57 sentences in the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we create and save training and test data in the **DocBin** objects.
    We loop through IDs, and for each ID, we get the sentence. We process the sentence
    using the small model and then have a spaCy **Doc** object. Then, we loop through
    the entities in the sentence and add them to the **ents** attribute of the **Doc**
    object. The processed **Doc** object then goes into one of the **DocBin** objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we train the model. We use the **spacy_config_ner.cfg** configuration
    file in the **data** folder. You can create your own customized configuration
    file at [https://spacy.io/usage/training/#quickstart](https://spacy.io/usage/training/#quickstart).
    The output shows the loss, accuracy, precision, recall, F1 score, and other metrics
    for every epoch. Finally, it saves the model to the specified directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Model training output
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we load the trained model and use it on data not seen during
    training. We get an ID from the test set, get all the rows from the test data
    with that ID, and load the sentence. We then print out the sentence and the annotated
    entities. Then, we process the sentence using our model (in exactly the same way
    as other pretrained spaCy models) and print out the entities it parsed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see that the resulting entities are quite good (output results may vary):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we evaluate the model using spaCy’s **evaluate** function. We see that
    the **WoA** and **Artist** tags have metrics that are low but in the double digits,
    while the **Artist_or_WoA** tag has an F1 score of about 10%. This is due to the
    fact that it has much less data than the other two tags. Overall, the performance
    of the model according to the statistics is not very good, and that is because
    we have a very small amount of data overall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The statistics might vary, but here is the output I got (output condensed):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The spaCy NER model is a neural network model. You can learn more about its
    architecture from the spaCy documentation: [https://spacy.io/models#architecture](https://spacy.io/models#architecture).'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning BERT for NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will fine-tune the pretrained BERT model for the NER task.
    The difference between training a model from scratch and fine-tuning it is as
    follows. Fine-tuning an NLP model, such as BERT, involves taking a pretrained
    model and modifying it for your specific task, such as NER in this case. The pretrained
    model already has lots of knowledge stored in it and the results are likely to
    be better than when training a model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: We will use similar data as in the previous recipe, creating a model that can
    tag entities as `Artist` or `WoA`. The data comes from the same dataset but it
    is labeled using the IOB format, which is required for the `transformers` packages
    we are going to use. We also only use the `Artist` and `WoA` tags, removing the
    `Artist_or_WoA` tag, since there is not enough data for that tag.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will use the Hugging Face `Trainer` class, although it is
    also possible to train Hugging Face models using PyTorch or Tensorflow. See more
    at [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `transformers` package from Hugging Face. It is preloaded in
    the Poetry environment. You can also install the package from the `requirements.txt`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.6_fine_tune_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.6_fine_tune_bert.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load and preprocess the data, train the model, and evaluate it, and
    then we will use it on unseen data. Your steps should be formatted like so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the language utilities notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import other packages and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we load the music NER dataset using the pandas **read_csv** function.
    We then define a function that takes a label, splits it on the underscore, and
    removes the last part (**_deduced**). We then apply this function to the **label**
    column. We also substitute the **|** character in case it could interfere with
    our code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Dataset DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we start our data preprocessing. We get the list of unique IDs from the
    **id** column. We loop through this list and get the sentence that corresponds
    to the given ID. We then process the text using the spaCy small model and add
    the entities from the DataFrame into the **Doc** object. We then store each of
    these sentences in a dictionary in which the keys are the sentence text strings
    and the values are the **Doc** objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we load the data in IOB format. This format is required to fine-tune BERT,
    as opposed to the format used by spaCy. For that, we load a separate data file,
    **../data/music_ner_bio.bio**. We create a dictionary of tag mappings and initialize
    empty lists for tokens, NER tags, and spans. We then loop through the sentence
    data we read from the data file. For each sentence, each line is a pair of a word
    and its label. We append the words to the **words** list and the numbers corresponding
    to the labels to the **tags** list. We also get the spans from the dictionary
    of **Doc** objects we created in the previous step and append those to the **spans**
    list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we split the data into training and testing. For that, we split the indices
    of the **spans** list. Then, we create separate tokens, NER tags, and **spans**
    lists for training and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create new DataFrames from the training and test lists we
    compiled in *step 6*. We then join the contents of the **tokens** column with
    spaces to get a sentence string instead of a list of words. We then drop empty
    data using the **dropna()** function and print the contents of the test DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we load the pretrained model and tokenizer and initialize the **Dataset**
    objects. The **Features** object describes the data and its properties. We create
    one training and one test **Dataset** object. We use the **Features** object we
    created, and the DataFrames we initialized in the previous step. We then add these
    newly created **Dataset** objects to **DatasetDict**, with one entry for the training
    dataset and one for the test data. We then print out the resulting object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create the **tokenize_adjust_labels** function that will assign
    the correct labels to word parts. We define the **tokenize_adjust_labels** function.
    The BERT tokenizer splits some words into components, and we need to make sure
    that the same label is assigned to each word part. The function first tokenizes
    all the text samples using the preloaded tokenizer. It then loops through the
    input IDs of the tokenized samples and adjusts the labels according to the word
    parts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the previous function on the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we initialize the data collator object. Data collators simplify the handling
    of data for training, for example, padding and truncating the input for all inputs
    to be of the same length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we create the **compute_metrics** function, which calculates evaluation
    metrics, including precision, recall, F1 score, and accuracy. In the function,
    we delete all the tokens that have the label **-100**, which are the special tokens.
    This function uses the **seqeval** evaluation method commonly used to evaluate
    NER tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we load the pretrained BERT model (the uncased version since our input
    is in lowercase). We then specify the training arguments by initializing the **TrainingArguments**
    object. This object contains the model hyperparameters. We then initialize the
    **Trainer** object by providing the training arguments, dataset, tokenizer, data
    collator, and **metrics** function. We then start the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will include different information, including the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we evaluate the fine-tuned model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the `Artist` label, it achieves an F1 score of 76%, and for the `WoA` label,
    it achieves an F1 score of 52%:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, load the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we test the fine-tuned model on an unseen text. We initialize the **text**
    variable. We then load the **pipeline** package to create a pipeline we will use.
    A text-processing pipeline takes the text to its final output value processed
    by the model. This particular pipeline specifies the task as **token-classification**,
    which fine-tuned model to use, the corresponding tokenizer, and the aggregation
    strategy. The aggregation strategy parameter specifies how to combine the results
    of several models when several models are used. We then run the pipeline on the
    text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will vary. The sample output identifies the music artist, *Morphine*
    *Robocobra Quartet*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the labels assigned by the model are correct.
  prefs: []
  type: TYPE_NORMAL
