["```py\nfrom langchain_openai import OpenAIEmbeddings\n# Initialize the embeddings model\nembeddings_model = OpenAIEmbeddings()\n# Create embeddings for the original example sentences\ntext1 = \"The cat sat on the mat\"\ntext2 = \"A feline rested on the carpet\"\ntext3 = \"Python is a programming language\"\n# Get embeddings using LangChain\nembeddings = embeddings_model.embed_documents([text1, text2, text3])\n# These similar sentences will have similar embeddings\nembedding1 = embeddings[0] # Embedding for \"The cat sat on the mat\"\nembedding2 = embeddings[1] # Embedding for \"A feline rested on the\ncarpet\"\nembedding3 = embeddings[2] # Embedding for \"Python is a programming\nlanguage\"\n# Output shows 3 documents with their embedding dimensions\nprint(f\"Number of documents: {len(embeddings)}\")\nprint(f\"Dimensions per embedding: {len(embeddings[0])}\")\n# Typically 1536 dimensions with OpenAI's embeddings\n```", "```py\n# Example of data that needs efficient storage in a vector store\ndocument_data = {\n \"id\": \"doc_42\",\n \"text\": \"LangChain is a framework for developing applications powered by language models.\",\n \"embedding\": [0.123, -0.456, 0.789, ...],  # 1536 dimensions for OpenAI embeddings\n \"metadata\": {\n \"source\": \"documentation.pdf\",\n \"page\": 7,\n \"created_at\": \"2023-06-15\"\n    }\n}\n```", "```py\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_chroma import Chroma\n# Initialize with an embedding model\nembeddings = OpenAIEmbeddings()\nvector_store = Chroma(embedding_function=embeddings)\n```", "```py\n    docs = [Document(page_content=\"Content 1\"), Document(page_\n    content=\"Content 2\")]\n    ids = vector_store.add_documents(docs)\n    ```", "```py\n    results = vector_store.similarity_search(\"How does LangChain work?\", k=3)\n    ```", "```py\n    vector_store.delete(ids=[\"doc_1\", \"doc_2\"])\n    ```", "```py\n    # Find relevant BUT diverse documents (reduce redundancy)\n    results = vector_store.max_marginal_relevance_search(\n     \"How does LangChain work?\",\n     k=3,\n     fetch_k=10,\n     lambda_mult=0.5  # Controls diversity (0=max diversity, 1=max relevance)\n    )\n    ```", "```py\nimport numpy as np\nimport faiss\nimport time\n# Create sample data - 10,000 vectors with 128 dimensions\ndimension = 128\nnum_vectors = 10000\nvectors = np.random.random((num_vectors, dimension)).astype('float32')\nquery = np.random.random((1, dimension)).astype('float32')\n```", "```py\n# Exact search index\nexact_index = faiss.IndexFlatL2(dimension)\nexact_index.add(vectors)\n# HNSW index (approximate but faster)\nhnsw_index = faiss.IndexHNSWFlat(dimension, 32)  # 32 connections per node\nhnsw_index.add(vectors)\n# Compare search times\nstart_time = time.time()\nexact_D, exact_I = exact_index.search(query, k=10)  # Search for 10 nearest neighbors\nexact_time = time.time() - start_time\nstart_time = time.time()\nhnsw_D, hnsw_I = hnsw_index.search(query, k=10)\nhnsw_time = time.time() - start_time\n# Calculate overlap (how many of the same results were found)\noverlap = len(set(exact_I[0]).intersection(set(hnsw_I[0])))\noverlap_percentage = overlap * 100 / 10\nprint(f\"Exact search time: {exact_time:.6f} seconds\")\nprint(f\"HNSW search time: {hnsw_time:.6f} seconds\")\nprint(f\"Speed improvement: {exact_time/hnsw_time:.2f}x faster\")\nprint(f\"Result overlap: {overlap_percentage:.1f}%\")\nRunning this code typically produces results like:\nExact search time: 0.003210 seconds\nHNSW search time: 0.000412 seconds\nSpeed improvement: 7.79x faster\nResult overlap: 90.0%\n```", "```py\n# For query transformation\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n# For basic RAG implementation\nfrom langchain_community.document_loaders import JSONLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\n# 1\\. Load documents\nloader = JSONLoader(\n    file_path=\"knowledge_base.json\",\n    jq_schema=\".[].content\",  # This extracts the content field from each array item\n    text_content=True\n)\ndocuments = loader.load()\n# 2\\. Convert to vectors\nembedder = OpenAIEmbeddings()\nembeddings = embedder.embed_documents([doc.page_content for doc in documents])\n# 3\\. Store in vector database\nvector_db = FAISS.from_documents(documents, embedder)\n# 4\\. Retrieve similar docs\nquery = \"What are the effects of climate change?\"\n```", "```py\nfrom langchain_community.document_loaders import JSONLoader\n# Load a json file\nloader = JSONLoader(\n file_path=\"knowledge_base.json\",\n jq_schema=\".[].content\",  # This extracts the content field from each array item\n text_content=True\n)\ndocuments = loader.load()\nprint(documents)\n```", "```py\nfrom langchain_text_splitters import CharacterTextSplitter\ntext_splitter = CharacterTextSplitter(\n separator=\" \",   # Split on spaces to avoid breaking words\n chunk_size=200,\n chunk_overlap=20\n)\nchunks = text_splitter.split_documents(documents)\nprint(f\"Generated {len(chunks)} chunks from document\")\n```", "```py\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n    chunk_size=150,\n    chunk_overlap=20\n)\ndocument = \"\"\"\ndocument = \"\"\"# Introduction to RAG\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.\nIt helps address hallucinations by grounding responses in retrieved information.\n## Key Components\nRAG consists of several components:\n1\\. Document processing\n2\\. Vector embedding\n3\\. Retrieval\n4\\. Augmentation\n5\\. Generation\n### Document Processing\nThis step involves loading and chunking documents appropriately.\n\"\"\"\nchunks = text_splitter.split_text(document)\nprint(chunks)\n```", "```py\n['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.', 'It helps address hallucinations by grounding responses in retrieved information.', '## Key Components\\nRAG consists of several components:\\n1\\. Document processing\\n2\\. Vector embedding\\n3\\. Retrieval\\n4\\. Augmentation\\n5\\. Generation', '### Document Processing\\nThis step involves loading and chunking documents appropriately.']\n```", "```py\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings()\ntext_splitter = SemanticChunker(\n    embeddings=embeddings,\n    add_start_index=True  # Include position metadata\n)\nchunks = text_splitter.split_text(document)\n```", "```py\n['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models. It helps address hallucinations by grounding responses in retrieved information. ## Key Components\\nRAG consists of several components:\\n1\\. Document processing\\n2\\. Vector embedding\\n3\\. Retrieval\\n4.',\n 'Augmentation\\n5\\. Generation\\n\\n### Document Processing\\nThis step involves loading and chunking documents appropriately. ']\n```", "```py\n# Basic retriever interaction\ndocs = retriever.invoke(\"What is machine learning?\")\n```", "```py\nfrom langchain_community.retrievers import KNNRetriever\nfrom langchain_openai import OpenAIEmbeddings\nretriever = KNNRetriever.from_documents(documents, OpenAIEmbeddings())\nresults = retriever.invoke(\"query\")\n```", "```py\n    from langchain_community.retrievers.pubmed import PubMedRetriever\n    retriever = PubMedRetriever()\n    results = retriever.invoke(\"COVID research\")\n    ```", "```py\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_community.retrievers import BM25Retriever\nfrom langchain.vectorstores import FAISS\n# Setup semantic retriever\nvector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n# Setup lexical retriever\nbm25_retriever = BM25Retriever.from_documents(documents)\nbm25_retriever.k = 5\n# Combine retrievers\nhybrid_retriever = EnsembleRetriever(\n    retrievers=[vector_retriever, bm25_retriever],\n    weights=[0.7, 0.3]  # Weight semantic search higher than keyword search\n)\nresults = hybrid_retriever.get_relevant_documents(\"climate change impacts\")\n```", "```py\n    # Complete document compressor example\n    from langchain.retrievers.document_compressors import CohereRerank\n    from langchain.retrievers import ContextualCompressionRetriever\n    # Initialize the compressor\n    compressor = CohereRerank(top_n=3)\n    # Create a compression retriever\n    compression_retriever = ContextualCompressionRetriever(\n     base_compressor=compressor,\n     base_retriever=base_retriever\n    )\n    # Original documents\n    print(\"Original documents:\")\n    original_docs = base_retriever.get_relevant_documents(\"How do transformers work?\")\n    for i, doc in enumerate(original_docs):\n     print(f\"Doc {i}: {doc.page_content[:100]}...\")\n    # Compressed documents\n    print(\"\\nCompressed documents:\")\n    compressed_docs = compression_retriever.get_relevant_documents(\"How do transformers work?\")\n    for i, doc in enumerate(compressed_docs):\n     print(f\"Doc {i}: {doc.page_content[:100]}...\")\n    ```", "```py\n    from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank\n    compressor = RankLLMRerank(top_n=3, model=\"zephyr\")\n    ```", "```py\n    # Simplified example - LangChain provides more streamlined implementations\n    relevance_score_chain = ChatPromptTemplate.from_template(\n     \"Rate relevance of document to query on scale of 1-10: {document}\"\n    ) | llm | StrOutputParser()\n    ```", "```py\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nexpansion_template = \"\"\"Given the user question: {question}\n```", "```py\n1.\"\"\"\nexpansion_prompt = PromptTemplate(\n    input_variables=[\"question\"],\n    template=expansion_template\n)\nllm = ChatOpenAI(temperature=0.7)\nexpansion_chain = expansion_prompt | llm | StrOutputParser()\n```", "```py\n# Generate expanded queries\noriginal_query = \"What are the effects of climate change?\"\nexpanded_queries = expansion_chain.invoke(original_query)\nprint(expanded_queries)\n```", "```py\nWhat impacts does climate change have?\n2\\. How does climate change affect the environment?\n3\\. What are the consequences of climate change?\n```", "```py\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n# Create prompt for generating hypothetical document\nhyde_template = \"\"\"Based on the question: {question}\nWrite a passage that could contain the answer to this question:\"\"\"\nhyde_prompt = PromptTemplate(\n    input_variables=[\"question\"],\n    template=hyde_template\n)\nllm = ChatOpenAI(temperature=0.2)\nhyde_chain = hyde_prompt | llm | StrOutputParser()\n# Generate hypothetical document\nquery = \"What dietary changes can reduce carbon footprint?\"\nhypothetical_doc = hyde_chain.invoke(query)\n# Use the hypothetical document for retrieval\nembeddings = OpenAIEmbeddings()\nembedded_query = embeddings.embed_query(hypothetical_doc)\nresults = vector_db.similarity_search_by_vector(embedded_query, k=3)\n```", "```py\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(temperature=0)\ncompressor = LLMChainExtractor.from_llm(llm)\n# Create a basic retriever from the vector store\nbase_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\ncompression_retriever = ContextualCompressionRetriever(\n base_compressor=compressor,\n base_retriever=base_retriever\n)\ncompressed_docs = compression_retriever.invoke(\"How do transformers work?\")\n```", "```py\n[Document(metadata={'source': 'Neural Network Review 2021', 'page': 42}, page_content=\"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\"),\n Document(metadata={'source': 'Large Language Models Survey', 'page': 89}, page_content='GPT models are autoregressive transformers that predict the next token based on previous tokens.')]\n```", "```py\nfrom langchain_community.vectorstores import FAISS\nvector_store = FAISS.from_documents(documents, embeddings)\nmmr_results = vector_store.max_marginal_relevance_search(\n query=\"What are transformer models?\",\n k=5,            # Number of documents to return\n fetch_k=20,     # Number of documents to initially fetch\n lambda_mult=0.5  # Diversity parameter (0 = max diversity, 1 = max relevance)\n)\n```", "```py\nfrom langchain_core.documents import Document\n# Example documents\ndocuments = [\n    Document(\n page_content=\"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\",\n```", "```py\n        metadata={\"source\": \"Neural Network Review 2021\", \"page\": 42}\n    ),\n    Document(\n page_content=\"BERT uses bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks.\",\n        metadata={\"source\": \"Introduction to NLP\", \"page\": 137}\n    ),\n    Document(\n page_content=\"GPT models are autoregressive transformers that predict the next token based on previous tokens.\",\n        metadata={\"source\": \"Large Language Models Survey\", \"page\": 89}\n    )\n]\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import OpenAIEmbeddings\n# Create a vector store and retriever\nembeddings = OpenAIEmbeddings()\nvector_store = FAISS.from_documents(documents, embeddings)\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n# Source attribution prompt template\nattribution_prompt = ChatPromptTemplate.from_template(\"\"\"\nYou are a precise AI assistant that provides well-sourced information.\nAnswer the following question based ONLY on the provided sources. For each fact or claim in your answer,\n```", "```py\ninclude a citation using [1], [2], etc. that refers to the source. Include a numbered reference list at the end.\nQuestion: {question}\nSources:\n{sources}\nYour answer:\n\"\"\")\n```", "```py\n# Create a source-formatted string from documents\ndef format_sources_with_citations(docs):\n    formatted_sources = []\n for i, doc in enumerate(docs, 1):\n        source_info = f\"[{i}] {doc.metadata.get('source', 'Unknown source')}\"\n if doc.metadata.get('page'):\n            source_info += f\", page {doc.metadata['page']}\"\n        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n return \"\\n\\n\".join(formatted_sources)\n# Build the RAG chain with source attribution\ndef generate_attributed_response(question):\n # Retrieve relevant documents\n    retrieved_docs = retriever.invoke(question)\n\n # Format sources with citation numbers\n    sources_formatted = format_sources_with_citations(retrieved_docs)\n\n # Create the attribution chain using LCEL\n    attribution_chain = (\n        attribution_prompt\n        | ChatOpenAI(temperature=0)\n        | StrOutputParser()\n```", "```py\n    )\n\n # Generate the response with citations\n    response = attribution_chain.invoke({\n \"question\": question,\n \"sources\": sources_formatted\n    })\n\n return response\n```", "```py\n# Example usage\nquestion = \"How do transformer models work and what are some examples?\"\nattributed_answer = generate_attributed_response(question)\nattributed_answer\nWe should be getting a response like this:\nTransformer models work by utilizing self-attention mechanisms to weigh the importance of different input tokens when making predictions. This architecture was first introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017 [1].\nOne example of a transformer model is BERT, which employs bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks [2]. Another example is GPT (Generative Pre-trained Transformer) models, which are autoregressive transformers that predict the next token based on previous tokens [3].\nReference List:\n```", "```py\n[1] Neural Network Review 2021, page 42\n[2] Introduction to NLP, page 137\n[3] Large Language Models Survey, page 89\n```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\nfrom typing import List, Dict\nfrom langchain_core.documents import Document\ndef verify_response_accuracy(\n    retrieved_docs: List[Document],\n    generated_answer: str,\n    llm: ChatOpenAI = None\n) -> Dict:\n \"\"\"\n    Verify if a generated answer is fully supported by the retrieved documents.\n    Args:\n        retrieved_docs: List of documents used to generate the answer\n        generated_answer: The answer produced by the RAG system\n        llm: Language model to use for verification\n    Returns:\n        Dictionary containing verification results and any identified issues\n    \"\"\"\n if llm is None:\n        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n\n # Create context from retrieved documents\n```", "```py\n    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n\n```", "```py\n # Define verification prompt - fixed to avoid JSON formatting issues in the template\n    verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n    As a fact-checking assistant, verify whether the following answer is fully supported\n    by the provided context. Identify any statements that are not supported or contradict the context.\n\n    Context:\n    {context}\n\n    Answer to verify:\n    {answer}\n\n    Perform a detailed analysis with the following structure:\n    1\\. List any factual claims in the answer\n    2\\. For each claim, indicate whether it is:\n       - Fully supported (provide the supporting text from context)\n       - Partially supported (explain what parts lack support)\n       - Contradicted (identify the contradiction)\n       - Not mentioned in context\n    3\\. Overall assessment: Is the answer fully grounded in the context?\n\n    Return your analysis in JSON format with the following structure:\n    {{\n      \"claims\": [\n        {{\n          \"claim\": \"The factual claim\",\n          \"status\": \"fully_supported|partially_supported|contradicted|not_mentioned\",\n          \"evidence\": \"Supporting or contradicting text from context\",\n```", "```py\n          \"explanation\": \"Your explanation\"\n        }}\n      ],\n      \"fully_grounded\": true|false,\n      \"issues_identified\": [\"List any specific issues\"]\n    }}\n    \"\"\")\n```", "```py\n    # Create verification chain using LCEL\n    verification_chain = (\n        verification_prompt\n        | llm\n        | StrOutputParser()\n    )\n\n    # Run verification\n    result = verification_chain.invoke({\n \"context\": context,\n \"answer\": generated_answer\n    })\n\n    return result\n# Example usage\nretrieved_docs = [\n    Document(page_content=\"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017\\. It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\"),\n    Document(page_content=\"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\")\n]\n```", "```py\ngenerated_answer = \"The transformer architecture was introduced by OpenAI in 2018 and uses recurrent neural networks. BERT is a transformer model developed by Google.\"\nverification_result = verify_response_accuracy(retrieved_docs, generated_answer)\nprint(verification_result)\n```", "```py\n{\n    \"claims\": [\n        {\n            \"claim\": \"The transformer architecture was introduced by OpenAI in 2018\",\n            \"status\": \"contradicted\",\n            \"evidence\": \"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\",\n            \"explanation\": \"The claim is contradicted by the fact that the transformer architecture was introduced in 2017 by Vaswani et al., not by OpenAI in 2018.\"\n        },\n        {\n            \"claim\": \"The transformer architecture uses recurrent neural networks\",\n            \"status\": \"contradicted\",\n            \"evidence\": \"It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\",\n            \"explanation\": \"The claim is contradicted by the fact that the transformer architecture does not use recurrent neural networks but relies on self-attention mechanisms.\"\n        },\n        {\n            \"claim\": \"BERT is a transformer model developed by Google\",\n            \"status\": \"fully_supported\",\n            \"evidence\": \"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\",\n```", "```py\n            \"explanation\": \"This claim is fully supported by the provided context.\"\n        }\n    ],\n    \"fully_grounded\": false,\n    \"issues_identified\": [\"The answer contains incorrect information about the introduction of the transformer architecture and its use of recurrent neural networks.\"]\n}\n```", "```py\nfrom pydantic import BaseModel, Field\nclass DocumentRelevanceScore(BaseModel):\n \"\"\"Binary relevance score for document evaluation.\"\"\"\n is_relevant: bool = Field(description=\"Whether the document contains information relevant to the query\")\n    reasoning: str = Field(description=\"Explanation for the relevance decision\")\ndef evaluate_document(document, query, llm):\n \"\"\"Evaluate if a document is relevant to a query.\"\"\"\n    prompt = f\"\"\" You are an expert document evaluator. Your task is to determine if the following document contains information relevant to the given query.\nQuery: {query}\nDocument content:\n{document.page_content}\nAnalyze whether this document contains information that helps answer the query.\n\"\"\"\n    Evaluation = llm.with_structured_output(DocumentRelevanceScore).invoke(prompt)\n return evaluation\n```", "```py\nDocument(page_content=\"Hello, world!\", metadata={\"source\": \"https://example.com\"})\n```", "```py\nimport logging\nimport os\nimport pathlib\nimport tempfile\nfrom typing import Any\nfrom langchain_community.document_loaders.epub import UnstructuredEPubLoader\nfrom langchain_community.document_loaders.pdf import PyPDFLoader\nfrom langchain_community.document_loaders.text import TextLoader\nfrom langchain_community.document_loaders.word_document import (\n UnstructuredWordDocumentLoader\n)\nfrom langchain_core.documents import Document\nfrom streamlit.logger import get_logger\nlogging.basicConfig(encoding=\"utf-8\", level=logging.INFO)\nLOGGER = get_logger(__name__)\n```", "```py\nclass EpubReader(UnstructuredEPubLoader):\n def __init__(self, file_path: str | list[str], **unstructured_kwargs: Any):\n super().__init__(file_path, **unstructured_kwargs, mode=\"elements\", strategy=\"fast\")\nclass DocumentLoaderException(Exception):\n pass\nclass DocumentLoader(object):\n \"\"\"Loads in a document with a supported extension.\"\"\"\n    supported_extensions = {\n \".pdf\": PyPDFLoader,\n \".txt\": TextLoader,\n \".epub\": EpubReader,\n \".docx\": UnstructuredWordDocumentLoader,\n \".doc\": UnstructuredWordDocumentLoader,\n    }\n```", "```py\ndef load_document(temp_filepath: str) -> list[Document]:\n \"\"\"Load a file and return it as a list of documents.\"\"\"\n    ext = pathlib.Path(temp_filepath).suffix\n    loader = DocumentLoader.supported_extensions.get(ext)\n if not loader:\n raise DocumentLoaderException(\n f\"Invalid extension type {ext}, cannot load this type of file\"\n        )\n    loaded = loader(temp_filepath)\n    docs = loaded.load()\n```", "```py\n    logging.info(docs)\n return docs\n```", "```py\nfrom langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import LocalFileStore\nfrom langchain_groq import ChatGroq\nfrom langchain_openai import OpenAIEmbeddings\nfrom config import set_environment\nset_environment()\n```", "```py\nchat_model = ChatGroq(\n model=\"deepseek-r1-distill-llama-70b\",\n temperature=0,\n max_tokens=None,\n timeout=None,\n max_retries=2,\n)\n```", "```py\nstore = LocalFileStore(\"./cache/\")\nunderlying_embeddings = OpenAIEmbeddings(\n```", "```py\n model=\"text-embedding-3-large\",\n)\n# Avoiding unnecessary costs by caching the embeddings.\nEMBEDDINGS = CacheBackedEmbeddings.from_bytes_store(\n    underlying_embeddings, store, namespace=underlying_embeddings.model\n)\n```", "```py\nimport os\nimport tempfile\nfrom typing import List, Any\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\nfrom langchain_core.documents import Document\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom chapter4.document_loader import load_document\nfrom chapter4.llms import EMBEDDINGS\n```", "```py\nVECTOR_STORE = InMemoryVectorStore(embedding=EMBEDDINGS)\n```", "```py\ndef split_documents(docs: List[Document]) -> list[Document]:\n \"\"\"Split each document.\"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1500, chunk_overlap=200\n    )\n return text_splitter.split_documents(docs)\n```", "```py\nclass DocumentRetriever(BaseRetriever):\n \"\"\"A retriever that contains the top k documents that contain the user query.\"\"\"\n    documents: List[Document] = []\n    k: int = 5\n def model_post_init(self, ctx: Any) -> None:\n self.store_documents(self.documents)\n    @staticmethod\n def store_documents(docs: List[Document]) -> None:\n \"\"\"Add documents to the vector store.\"\"\"\n        splits = split_documents(docs)\n        VECTOR_STORE.add_documents(splits)\n def add_uploaded_docs(self, uploaded_files):\n \"\"\"Add uploaded documents.\"\"\"\n        docs = []\n        temp_dir = tempfile.TemporaryDirectory()\n for file in uploaded_files:\n            temp_filepath = os.path.join(temp_dir.name, file.name)\n with open(temp_filepath, \"wb\") as f:\n                f.write(file.getvalue())\n                docs.extend(load_document(temp_filepath))\n self.documents.extend(docs)\n self.store_documents(docs)\n def _get_relevant_documents(\n            self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n```", "```py\n ) -> List[Document]:\n \"\"\"Sync implementations for retriever.\"\"\"\n if len(self.documents) == 0:\n return []\n return VECTOR_STORE.similarity_search(query=\"\", k=self.k)\n```", "```py\nfrom typing import Annotated\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.constants import END\nfrom langgraph.graph import START, StateGraph, add_messages\nfrom typing_extensions import List, TypedDict\nfrom chapter4.llms import chat_model\nfrom chapter4.retriever import DocumentRetriever\n```", "```py\nsystem_prompt = (\n \"You're a helpful AI assistant. Given a user question \"\n \"and some corporate document snippets, write documentation.\"\n \"If none of the documents is relevant to the question, \"\n \"mention that there's no relevant document, and then \"\n \"answer the question to the best of your knowledge.\"\n \"\\n\\nHere are the corporate documents: \"\n \"{context}\"\n)\n```", "```py\nretriever = DocumentRetriever()\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        (\"human\", \"{question}\"),\n    ]\n)\n```", "```py\nclass State(TypedDict):\n    question: str\n    context: List[Document]\n    answer: str\n    issues_report: str\n    issues_detected: bool\n    messages: Annotated[list, add_messages]\n```", "```py\ndef retrieve(state: State):\n    retrieved_docs = retriever.invoke(state[\"messages\"][-1].content)\n print(retrieved_docs)\n return {\"context\": retrieved_docs}\ndef generate(state: State):\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n    messages = prompt.invoke(\n        {\"question\": state[\"messages\"][-1].content, \"context\": docs_content}\n    )\n    response = chat_model.invoke(messages)\n print(response.content)\n return {\"answer\": response.content}\n```", "```py\ndef double_check(state: State):\n    result = chat_model.invoke(\n        [{\n \"role\": \"user\",\n \"content\": (\n f\"Review the following project documentation for compliance with our corporate standards. \"\n f\"Return 'ISSUES FOUND' followed by any issues detected or 'NO ISSUES': {state['answer']}\"\n            )\n        }]\n    )\n if \"ISSUES FOUND\" in result.content:\n print(\"issues detected\")\n return {\n \"issues_report\": result.split(\"ISSUES FOUND\", 1)[1].strip(),\n \"issues_detected\": True\n        }\n print(\"no issues detected\")\n return {\n \"issues_report\": \"\",\n \"issues_detected\": False\n    }\n```", "```py\ndef doc_finalizer(state: State):\n \"\"\"Finalize documentation by integrating feedback.\"\"\"\n if \"issues_detected\" in state and state[\"issues_detected\"]:\n        response = chat_model.invoke(\n            messages=[{\n \"role\": \"user\",\n \"content\": (\n```", "```py\n f\"Revise the following documentation to address these feedback points: {state['issues_report']}\\n\"\n f\"Original Document: {state['answer']}\\n\"\n f\"Always return the full revised document, even if no changes are needed.\"\n                )\n            }]\n        )\n return {\n \"messages\": [AIMessage(response.content)]\n        }\n return {\n \"messages\": [AIMessage(state[\"answer\"])]\n    }\n```", "```py\ngraph_builder = StateGraph(State).add_sequence(\n [retrieve, generate, double_check, doc_finalizer]\n)\ngraph_builder.add_edge(START, \"retrieve\")\ngraph_builder.add_edge(\"doc_finalizer\", END)\nmemory = MemorySaver()\ngraph = graph_builder.compile(checkpointer=memory)\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\nWe can visualize this graph from a Jupyter notebook:\nfrom IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "```py\nfrom langchain_core.messages import HumanMessage\ninput_messages = [HumanMessage(\"What's the square root of 10?\")]\nresponse = graph.invoke({\"messages\": input_messages}, config=config\n```", "```py\nprint(response[\"messages\"][-1].content)\n```", "```py\nimport streamlit as st\nfrom langchain_core.messages import HumanMessage\nfrom chapter4.document_loader import DocumentLoader\nfrom chapter4.rag import graph, config, retriever\n```", "```py\nst.set_page_config(page_title=\"Corporate Documentation Manager\", layout=\"wide\")\n```", "```py\nif \"chat_history\" not in st.session_state:\n    st.session_state.chat_history = []\nif 'uploaded_files' not in st.session_state:\n    st.session_state.uploaded_files = []\n```", "```py\nfor message in st.session_state.chat_history:\n print(f\"message: {message}\")\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n```", "```py\ndocs = retriever.add_uploaded_docs(st.session_state.uploaded_files)\n```", "```py\ndef process_message(message):\n \"\"\"Assistant response.\"\"\"\n    response = graph.invoke({\"messages\": HumanMessage(message)}, config=config)\n return response[\"messages\"][-1].content\n```", "```py\nst.markdown(\"\"\"\n#  Corporate Documentation Manager with Citations\n\"\"\")\n```", "```py\ncol1, col2 = st.columns([2, 1])\n```", "```py\nwith col1:\n st.subheader(\"Chat Interface\")\n    # React to user input\n if user_message := st.chat_input(\"Enter your message:\"):\n        # Display user message in chat message container\n        with st.chat_message(\"User\"):\n st.markdown(user_message)\n        # Add user message to chat history\n st.session_state.chat_history.append({\"role\": \"User\", \"content\": user_message})\n        response = process_message(user_message)\n```", "```py\n        with st.chat_message(\"Assistant\"):\n st.markdown(response)\n        # Add response to chat history\n st.session_state.chat_history.append(\n            {\"role\": \"Assistant\", \"content\": response}\n        )\n```", "```py\nwith col2:\n st.subheader(\"Document Management\")\n    # File uploader\n    uploaded_files = st.file_uploader(\n \"Upload Documents\",\n type=list(DocumentLoader.supported_extensions),\n        accept_multiple_files=True\n    )\n if uploaded_files:\n for file in uploaded_files:\n if file.name not in st.session_state.uploaded_files:\n st.session_state.uploaded_files.append(file)\n```", "```py\n    PYTHONPATH=. streamlit run chapter4/streamlit_app.py\n    ```"]