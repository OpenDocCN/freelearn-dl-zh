- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Natural Language Understanding
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言理解
- en: In this chapter, we will explore recipes that will allow us to interpret and
    understand the text contained in short as well as long passages. **Natural language
    understanding** (**NLU**) is a very broad term and the various systems developed
    as part of NLU do not interpret or understand a passage of text the same way a
    human reader would. However, based on the specificity of the task, we can create
    some applications that can be combined to generate an interpretation or understanding
    that can be used to solve a given problem related to text processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一些食谱，这些食谱将使我们能够解释和理解包含在短篇和长篇段落中的文本。**自然语言理解**（**NLU**）是一个非常宽泛的术语，作为NLU一部分开发的各种系统并不以与人类读者相同的方式解释或理解一段文本。然而，基于任务的特定性，我们可以创建一些应用，这些应用可以组合起来生成一种解释或理解，用于解决与文本处理相关的特定问题。
- en: Organizations that have a huge document corpus need a seamless way to search
    through documents. More specifically, what users really need is an answer to a
    specific question without having to glean through a list of documents that are
    returned as part of a document search. Users would prefer the query to be formulated
    as a question in natural language and the answer to be emitted in the same manner.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有大量文档语料库的组织需要一个无缝的方式来搜索文档。更具体地说，用户真正需要的是一个针对特定问题的答案，而无需浏览作为文档搜索结果返回的文档列表。用户更愿意将查询以自然语言问题的形式提出，并以相同的方式输出答案。
- en: Another set of applications is that of document summarization and text entailment.
    While processing a large set of documents, it is helpful if the document length
    can be shortened without the loss of meaning or context. Additionally, it’s important
    to determine whether the information contained in the document at the sentence
    level entails itself.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 另一套应用是文档摘要和文本蕴涵。在处理大量文档时，如果能够缩短文档长度而不丢失意义或上下文，那就很有帮助。此外，确定文档中包含的信息在句子层面上是否蕴涵自身也很重要。
- en: While we work on processing and classifying the documents, there are always
    challenges in understanding why or how the model assigns a label to a piece of
    text – more specifically, what parts of the text contribute to the different labels.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理和分类文档时，总会有理解为什么或如何模型将标签分配给文本片段的挑战——更具体地说，文本的哪些部分有助于不同的标签。
- en: This chapter will cover different techniques to explore the various aspects
    previously described. We will follow recipes that will allow us to perform these
    tasks and understand the underlying building blocks that help us achieve the end
    goals.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖探索先前描述的各种技术的不同方法。我们将遵循食谱，使我们能够执行这些任务，并理解帮助我们实现最终目标的底层构建块。
- en: 'As part of this chapter, we will build recipes for the following tasks:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的一部分，我们将为以下任务构建食谱：
- en: Answering questions from a short text passage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从短文本段落中回答问题
- en: Answering questions from a long text passage
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从长文本段落中回答问题
- en: Answering questions from a document corpus in an extractive manner
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以提取方式从文档语料库中回答问题
- en: Answering questions from a document corpus in an abstractive manner
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以抽象方式从文档语料库中回答问题
- en: Summarizing text using pretrained models based on Transformers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于Transformers的预训练模型总结文本
- en: Detecting sentence entailment
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测句子蕴涵
- en: Enhancing explainability via a classifier-invariant approach
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分类器不变方法增强可解释性
- en: Enhancing explainability via text generation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过文本生成增强可解释性
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is in a folder named `Chapter9` in the GitHub repository
    of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于GitHub书籍仓库中名为`Chapter9`的文件夹中（[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09)）。
- en: As in previous chapters, the packages required for this chapter are part of
    the `poetry` environment. Alternatively, you can install all the packages using
    the `requirements.txt` file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，本章所需的包是`poetry`环境的一部分。或者，您可以使用`requirements.txt`文件安装所有包。
- en: Answering questions from a short text passage
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从短文本段落中回答问题
- en: To get started with question answering, we will start with a simple recipe that
    can answer a question from a short passage.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始问答，我们将从一个简单的配方开始，这个配方可以回答来自简短段落的问题。
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As part of this chapter, we will use the libraries from Hugging Face ([huggingface.co](http://huggingface.co)).
    For this recipe, we will use the `BertForQuestionAnswering` and `BertTokenizer`
    modules from the Transformers package. The `BertForQuestionAnswering` model uses
    the base BERT large uncased model that was trained on the `SQuAD` dataset and
    fine-tuned for the question-answering task. This pre-trained model can be used
    to load a text passage and answer questions based on the contents of the passage.
    You can use the `9.1_question_answering.ipynb` notebook from the code site if
    you need to work from an existing notebook.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的一部分，我们将使用来自 Hugging Face 的库 ([huggingface.co](http://huggingface.co))。对于这个配方，我们将使用
    Transformers 包中的 `BertForQuestionAnswering` 和 `BertTokenizer` 模块。`BertForQuestionAnswering`
    模型使用的是在 `SQuAD` 数据集上训练的基于 BERT 的基础大模型，并针对问答任务进行了微调。这个预训练模型可以用来加载一段文本并基于段落内容回答问题。如果你需要从一个现有的笔记本开始工作，可以使用代码网站上的
    `9.1_question_answering.ipynb` 笔记本。
- en: How to do it...
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this recipe, we will load a pretrained model that has been trained on the
    SQuAD dataset ([https://huggingface.co/datasets/squad](https://huggingface.co/datasets/squad)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将加载一个在 SQuAD 数据集 ([https://huggingface.co/datasets/squad](https://huggingface.co/datasets/squad))
    上训练的预训练模型。
- en: 'The recipe does the following things:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 配方执行以下操作：
- en: It initializes a question-answering pipeline based on the pre-trained **BertForQuestionAnswering**
    model and **BertTokenizer** tokenizer.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它初始化一个基于预训练的 **BertForQuestionAnswering** 模型和 **BertTokenizer** 分词器的问答管道。
- en: It further initializes a context passage and a question and emits the output
    of the answer based on these two parameters. It also prints the exact text of
    the answer.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它进一步初始化一个上下文段落和一个问题，并基于这两个参数输出答案。它还打印出答案的确切文本。
- en: It asks a follow-up question to the same pipeline by just changing the question
    text, and prints the exact text answer to the question.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过只更改问题文本来向同一个管道提出后续问题，并打印出问题的确切文本答案。
- en: 'The steps for the recipe are as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 配方的步骤如下：
- en: 'Do the necessary imports to import the necessary types and functions from the
    **datasets** package:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入，从 **datasets** 包中导入必要的类型和函数：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this step, we initialize the model and tokenizer, respectively, using the
    pre-trained **bert-large-uncased-whole-word-masking-finetuned-squad** artifacts.
    These will be downloaded from the Hugging Face website if they are not present
    locally on the machine as part of these calls. We have chosen the specific model
    and tokenizer for our recipe, but feel free to explore other models on the Hugging
    Face site that might suit your needs. As a generic step for this and the following
    recipe, we discover whether there are any GPU devices in the system and attempt
    to use them. If a GPU is not detected, we use the CPU instead:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们分别使用预训练的 **bert-large-uncased-whole-word-masking-finetuned-squad**
    艺术品初始化模型和分词器。如果这些艺术品没有在本地机器上作为这些调用的一部分存在，它们将从 Hugging Face 网站下载。我们已经为我们的配方选择了特定的模型和分词器，但你可以自由探索
    Hugging Face 网站上可能适合你需求的其它模型。作为这个和下一个配方的通用步骤，我们检查系统中是否有任何 GPU 设备，并尝试使用它们。如果没有检测到
    GPU，我们将使用 CPU：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this step, we initialize a question-answering pipeline with the model and
    tokenizer. The task type for this pipeline is set to **question-answering**:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们使用模型和分词器初始化一个问答管道。这个管道的任务类型设置为 **问答**：
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this step, we initialize a **context** passage. This passage was generated
    as part of our *Text Generation via Transformers* example in [*Chapter 8*](B18411_08.xhtml#_idTextAnchor205).
    It’s entirely acceptable if you want to use a different passage:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们初始化一个 **上下文** 段落。这个段落是我们 *通过 Transformers 生成文本* 的例子中的一部分，在 [*第 8 章*](B18411_08.xhtml#_idTextAnchor205)。如果你想要使用不同的段落，那是完全可以接受的：
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this step, we initialize a question text, invoke the pipeline with the context
    and question, and store the result in a variable. The type of the result is a
    Python **dict** object:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们初始化一个问题文本，使用上下文和问题调用管道，并将结果存储在一个变量中。结果类型是一个 Python **dict** 对象：
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this step, we print the value of the result. The **score** value shows the
    probability of the answer. The **start** and **end** values denote the start and
    end character indices in the **context** passage that constitute the answer. The
    **answer** value denotes the actual text of the answer:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们打印结果值。**score**值显示了答案的概率。**start**和**end**值表示构成答案的上下文段落中的起始和结束字符索引。**answer**值表示答案的实际文本：
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this step, we print the exact text answer. This is present in the **answer**
    key in the **result** dictionary:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，我们打印出确切的文本答案。这个答案在**result**字典的**answer**键中：
- en: '[PRE7]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this step, we ask another question using the same context and print the
    result:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们使用相同上下文提出另一个问题并打印结果：
- en: '[PRE9]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Answering questions from a long text passage
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从长文本段落中回答问题
- en: In the previous recipe, we learned an approach to extract the answer to a question,
    given a context. This pattern involves the model retrieving the answer from the
    given context. The model cannot answer a question that is not contained in the
    context. This does serve a purpose where we want an answer from a given context.
    This type of question-answering system is defined as **Closed Domain Question**
    **Answering** (**CDQA**).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个菜谱中，我们学习了一种在给定上下文的情况下提取问题答案的方法。这种模式涉及模型从给定的上下文中检索答案。模型不能回答不在上下文中的问题。这在我们需要从给定上下文中获取答案的情况下是有用的。这种问答系统被定义为**封闭域问答**（**CDQA**）。
- en: There is another system of question answering that can answer questions that
    are general in nature. These systems are trained on larger corpora. This training
    provides them with the ability to answer questions that are open in nature. These
    systems are called **Open Domain Question Answering** (**ODQA**) systems.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另有一种问答系统可以回答本质上是普遍性的问题。这些系统在更大的语料库上进行了训练。这种训练使它们能够回答本质上是开放性的问题。这些系统被称为**开放域问答**（**ODQA**）系统。
- en: Getting ready
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As part of this recipe, we will use the `deeppavlov` library along with the
    **Knowledge Base Question Answering** (**KBQA**) model. This model has been trained
    on English wiki data as a knowledge base. It uses various NLP techniques such
    as entity linking and disambiguation, knowledge graphs, and so on to extract the
    exact answer to the question.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这个菜谱的一部分，我们将使用`deeppavlov`库以及**知识库问答**（**KBQA**）模型。这个模型已经在英语维基数据上作为知识库进行了训练。它使用各种NLP技术，如实体链接和消歧，知识图谱等，以提取问题的确切答案。
- en: 'This recipe needs a few steps to set up the right environment for its execution.
    The `poetry` file for this recipe is in the `9.2_QA_on_long_passages` folder.
    We will also need to install and download the document corpus by performing the
    following command:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱需要几个步骤来设置正确的执行环境。这个菜谱的`poetry`文件位于`9.2_QA_on_long_passages`文件夹中。我们还需要通过执行以下命令来安装和下载文档语料库：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can also use the `9.2_QA_on_long_passages.ipynb` notebook, which is contained
    within the same folder.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用包含在同一文件夹中的`9.2_QA_on_long_passages.ipynb`笔记本。
- en: How to do it...
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will initialize the KBQA model based on the `DeepPavlov`
    library and use it to answer an open question. The steps for the recipe are as
    follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将基于`DeepPavlov`库初始化KBQA模型，并使用它来回答一个开放性问题。菜谱的步骤如下：
- en: 'Do the necessary imports:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE12]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this step, we initialize the KBQA model, **kbqa_cq_en**, which is passed
    to the **build_model** method as an argument. We also set the **download** argument
    to **True** so that the model is downloaded as well in case it is missing locally:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们初始化KBQA模型，**kbqa_cq_en**，并将其作为参数传递给**build_model**方法。我们还设置**download**参数为**True**，以便在本地缺失的情况下也下载模型：
- en: '[PRE13]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We use the initialized model and pass it a couple of questions that we want
    to be answered:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用初始化后的模型并传递我们想要回答的几个问题：
- en: '[PRE14]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We print the result as returned by the model. The result contains three arrays.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印出模型返回的结果。结果包含三个数组。
- en: The first array contains the exact answers to the question ordered in the same
    way as the original input. In this case, the answers `Cairo` and `Hillary Clinton`
    are in the same order as the questions they pertain to.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个数组包含按与原始输入相同顺序排列的精确答案。在这种情况下，答案`Cairo`和`Hillary Clinton`与它们相关的问题顺序相同。
- en: 'You might observe some additional artifacts in the output. These are internal
    identifiers that are generated by the library. We have omitted them for brevity:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能会在输出中观察到一些额外的工件。这些是由库生成的内部标识符。为了简洁起见，我们已省略它们：
- en: '[PRE15]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: See also
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more information on the internal details of the working of DeepPavlov, please
    refer to [https://deeppavlov.ai](https://deeppavlov.ai).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有关DeepPavlov工作内部细节的更多信息，请参阅[https://deeppavlov.ai](https://deeppavlov.ai)。
- en: Answering questions from a document corpus in an extractive manner
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以提取方式从文档语料库回答问题
- en: For the use cases where we have a document corpus that contains a large number
    of documents, it’s not feasible to load the document content at runtime to answer
    a question. Such an approach would lead to long query times and would not be suitable
    for production-grade systems.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含大量文档的文档语料库的使用案例，在运行时加载文档内容以回答问题是不切实际的。这种方法会导致查询时间过长，并且不适合生产级系统。
- en: In this recipe, we will learn how to preprocess the documents and transform
    them into a form for faster reading, indexing, and retrieval that allows the system
    to extract the answer for a given question with short query times.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将学习如何预处理文档并将它们转换成一种更快阅读、索引和检索的形式，这样系统就可以在短时间内查询到给定问题的答案。
- en: Getting ready
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'As part of this recipe, we will use the **Haystack** ([https://haystack.deepset.ai/](https://haystack.deepset.ai/))
    framework to build a **QA system** that can answer questions from a document corpus.
    We will download a dataset based on *Game of Thrones* and index it. For our QA
    system to be performant, we will need to index the documents beforehand. Once
    the documents are indexed, answering a question follows a two-step process:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这个食谱的一部分，我们将使用**Haystack**([https://haystack.deepset.ai/](https://haystack.deepset.ai/))框架来构建一个**问答系统**，该系统能够从文档语料库中回答问题。我们将下载一个基于《权力的游戏》的数据集并进行索引。为了使我们的问答系统性能良好，我们需要事先对文档进行索引。一旦文档被索引，回答问题将遵循两步过程：
- en: '**Retriever**: Since we have many documents, scanning each document to fetch
    an answer is not a feasible approach. We will first retrieve a set of candidate
    documents that can possibly contain an answer to our question. This step is performed
    using a **Retriever** component. This searches through the pre-created index to
    filter the number of documents that we will need to scan to retrieve the exact
    answer.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Retriever**：由于我们有很多文档，扫描每个文档以获取答案不是一个可行的方法。我们将首先使用**Retriever**组件检索一组可能包含我们问题答案的候选文档。这一步是通过**Retriever**组件执行的。它搜索预先创建的索引，以过滤出我们需要扫描以检索确切答案的文档数量。'
- en: '**Reader**: Once we have a candidate set of documents that could contain the
    answer, we will search these documents to retrieve the exact answer to our question.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Reader**：一旦我们有一个可能包含答案的候选文档集，我们将搜索这些文档以检索我们问题的确切答案。'
- en: We will discuss the details of these components throughout this recipe. You
    can use the `9.3_QA_on_document_corpus.ipynb` notebook from the code site if you
    need to work from an existing notebook. To start with, let’s set up the prerequisites.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将讨论这些组件的详细信息。如果您需要从一个现有的笔记本开始工作，可以使用代码网站上的`9.3_QA_on_document_corpus.ipynb`笔记本。首先，让我们设置先决条件。
- en: How to do it...
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In this step, we do the necessary imports:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们进行必要的导入：
- en: '[PRE16]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this step, we specify a folder that will be used to save our dataset. Then,
    we retrieve the dataset from the source. The second parameter to the **fetch_archive_from_http**
    method is the folder in which the dataset will be downloaded. We set the parameter
    to the folder that we defined in the first line. The **fetch_archive_from_http**
    method decompresses the archive **.zip** file and extracts all files into the
    same folder. We then read from the folder and create a list of files contained
    in the folder. We also print the number of files that are present:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们指定一个文件夹，用于保存我们的数据集。然后，我们从源中检索数据集。**fetch_archive_from_http**方法的第二个参数是数据集将被下载到的文件夹。我们将该参数设置为第一行中定义的文件夹。**fetch_archive_from_http**方法解压缩**.zip**存档文件，并将所有文件提取到同一个文件夹中。然后我们从文件夹中读取并创建文件夹中包含的文件列表。我们还打印了现有文件的数量：
- en: '[PRE17]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We initialize a document store based on the files. We create an indexing pipeline
    based on the document store and execute the indexing operation. To achieve this,
    we initialize an **InMemoryDocumentStore** instance. In this method call, we set
    the **use_bm25** argument as **True**. The document store uses **Best Match 25**
    (**bm25**) as the algorithm for the retriever step. The **bm25** algorithm is
    a simple bag-of-words-based algorithm that uses a scoring function. This function
    utilizes the number of times a term is present in the document and the length
    of the document. [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067) covers the **bm25**
    algorithm in more detail and we recommend you refer to that chapter for better
    understanding. Note that there are various other **DocumentStore** options such
    as **ElasticSearch**, **OpenSearch**, and so on. We used an **InMemoryDocumentStore**
    document store to keep the recipe simple and focus on the retriever and reader
    concepts:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们基于文件初始化一个文档存储。我们基于文档存储创建一个索引管道并执行索引操作。为了实现这一点，我们初始化一个 **InMemoryDocumentStore**
    实例。在这个方法调用中，我们将 **use_bm25** 参数设置为 **True**。文档存储使用 **最佳匹配 25** （**bm25**）作为检索步骤的算法。**bm25**
    算法是一个基于词袋的简单算法，它使用一个评分函数。这个函数利用了术语在文档中出现的次数和文档的长度。[*第 3 章*](B18411_03.xhtml#_idTextAnchor067)
    更详细地介绍了 **bm25** 算法，我们建议您参考该章节以获得更好的理解。请注意，还有各种其他 **DocumentStore** 选项，例如 **ElasticSearch**、**OpenSearch**
    等。我们使用 **InMemoryDocumentStore** 文档存储来简化配方并专注于检索器和阅读器概念：
- en: '[PRE18]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once we have loaded the documents, we initialize our retriever and reader instances.
    To achieve this, we initialize the retriever and the reader components. **BM25Retriever**
    uses the **bm25** scoring function to retrieve the initial set of documents. For
    the reader, we initialize the **FARMReader** object. This is based on deepset’s
    FARM framework, which can utilize the QA models from Hugging Face. In our case,
    we use the **deepset/roberta-base-squad2** model as a reader. The **use_gpu**
    argument can be set appropriately based on whether your device has a GPU or not:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们加载了文档，我们就初始化我们的检索器和阅读器实例。为了实现这一点，我们初始化检索器和阅读器组件。**BM25Retriever** 使用 **bm25**
    分数函数检索初始文档集。对于阅读器，我们初始化 **FARMReader** 对象。这是基于 deepset 的 FARM 框架，可以利用 Hugging
    Face 的 QA 模型。在我们的情况下，我们使用 **deepset/roberta-base-squad2** 模型作为阅读器。**use_gpu**
    参数可以根据您的设备是否有 GPU 适当设置：
- en: '[PRE19]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We now create a pipeline that we can use to answer questions. After having
    initialized the retriever and reader in the previous step, we want to combine
    them for querying. The **pipeline** abstraction from the Haystack framework allows
    us to integrate the reader and retriever together using a series of pipelines
    that address different use cases. In this instance, we will use **ExtractiveQAPipeline**
    for our QA system. After the initialization of the pipeline, we generate the answer
    to a question from the *Game of Thrones* series. The **run** method takes the
    question as the query. The second argument, **params**, dictates how the results
    from the retriever and reader are combined to present the answer:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在创建一个可以用来回答问题的管道。在前一个步骤中初始化检索器和阅读器之后，我们希望将它们结合起来进行查询。Haystack 框架的 **pipeline**
    抽象允许我们使用一系列针对不同用例的管道来集成阅读器和检索器。在这个例子中，我们将使用 **ExtractiveQAPipeline** 作为我们的问答系统。在初始化管道后，我们从
    *权力的游戏* 系列中生成一个问题答案。**run** 方法将问题作为查询。第二个参数，**params**，决定了检索器和阅读器结果如何组合以呈现答案：
- en: '**"Retriever": {"top_k": 10}**: The **top_k** keyword argument specifies that
    the top-k (in this case, **10**) results from the retriever are used by the reader
    to search for the exact answer'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"Retriever": {"top_k": 10}**: The **top_k** keyword argument specifies that
    the top-k (in this case, **10**) results from the retriever are used by the reader
    to search for the exact answer'
- en: '**"Reader": {"top_k": 5}**: The **top_k** keyword argument specifies that the
    top-k (in this case, **5**) results from the reader are presented as the output
    of the method:'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**"Reader": {"top_k": 5}**: The **top_k** keyword argument specifies that the
    top-k (in this case, **5**) results from the reader are presented as the output
    of the method:'
- en: '[PRE20]'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We print the answer to our question. The system prints out the exact answer
    along with the associated context that it used to extract the answer from. Note
    that we use the value of **all** for the **details** argument. Using the **all**
    value for the same argument prints out **start** and **end** spans for the answer
    along with all the auxiliary information. Setting the value of **medium** for
    the **details** argument provides the relative score of each answer. This score
    can be used to filter out the results further based on the accuracy requirements
    of the system. Using the argument of **medium** presents only the answer and the
    context. We encourage you to make a suitable choice based on your requirements:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印出问题的答案。系统会打印出确切的答案以及它用来提取答案的相关上下文。注意，我们使用**all**值作为**details**参数的值。使用**all**值作为相同参数会打印出答案的**start**和**end**范围以及所有辅助信息。将**details**参数的值设置为**medium**会提供每个答案的相对分数。这个分数可以用来根据系统的准确性要求进一步过滤结果。使用**medium**参数只显示答案和上下文。我们鼓励您根据自己的需求做出合适的选择：
- en: '[PRE21]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: See also
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For a QA system to work in a high-performance production system, it is recommended
    to use a different document store from an in-memory one. We recommend you refer
    to [https://docs.haystack.deepset.ai/docs/document_store](https://docs.haystack.deepset.ai/docs/document_store)
    and use an appropriate document store based on your production-grade requirements.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使QA系统在高性能的生产系统中运行，建议使用不同于内存的文档存储。我们建议您参考[https://docs.haystack.deepset.ai/docs/document_store](https://docs.haystack.deepset.ai/docs/document_store)，并根据您生产级的要求使用适当的文档存储。
- en: Answering questions from a document corpus in an abstractive manner
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以抽象方式从文档语料库回答问题
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Getting ready
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we will build a QA system that will provide answers that are
    abstractive in nature. We will load the `bilgeyucel/seven-wonders` dataset from
    the Hugging Face site and initialize a retriever from it. This dataset has content
    about the seven wonders of the ancient world. To generate the answers, we will
    use the `PromptNode` component from the Haystack framework to set up a pipeline
    that can generate answers in an abstractive fashion. You can use the `9.4_abstractive_qa_on_document_corpus.ipynb`
    notebook from the code site if you need to work from an existing notebook. Let’s
    get started.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将构建一个提供抽象性答案的QA系统。我们将从Hugging Face网站加载`bilgeyucel/seven-wonders`数据集，并从中初始化一个检索器。这个数据集包含关于古代世界七大奇迹的内容。为了生成答案，我们将使用Haystack框架的`PromptNode`组件来设置一个可以以抽象方式生成答案的管道。如果您需要从一个现有的笔记本开始工作，可以使用代码网站的`9.4_abstractive_qa_on_document_corpus.ipynb`笔记本。让我们开始吧。
- en: How to do it
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'The steps are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'In this step, we do the necessary imports:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们进行必要的导入：
- en: '[PRE23]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As part of this step, we load the **bilgeyucel/seven-wonders** dataset into
    an in-memory document store. This dataset has been created out of the Wikipedia
    pages of *Seven Wonders of the Ancient World* ([https://en.wikipedia.org/wiki/Wonders_of_the_World](https://en.wikipedia.org/wiki/Wonders_of_the_World)).
    This dataset has been preprocessed and uploaded to the Hugging Face site, and
    can be easily downloaded by using the **datasets** module from Hugging Face. We
    use **InMemoryDocumentStore** as our document store, with **bm25** as the search
    algorithm. We write the documents from the dataset into the document store. To
    have a performant query time performance, the **write_documents** method automatically
    optimizes how the documents are written. Once the documents are written into,
    we initialize the retriever based on **bm25**, similar to our previous recipe:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为这一步骤的一部分，我们将**bilgeyucel/seven-wonders**数据集加载到内存文档存储中。这个数据集是从**古代世界七大奇迹**的维基百科页面创建的([https://en.wikipedia.org/wiki/Wonders_of_the_World](https://en.wikipedia.org/wiki/Wonders_of_the_World))。这个数据集已经预处理并上传到Hugging
    Face网站，可以通过使用Hugging Face的**datasets**模块轻松下载。我们使用**InMemoryDocumentStore**作为我们的文档存储，使用**bm25**作为搜索算法。我们将数据集中的文档写入文档存储。为了获得高性能的查询时间，**write_documents**方法会自动优化文档的写入方式。一旦文档写入完成，我们就基于**bm25**初始化检索器，类似于我们之前的配方：
- en: '[PRE24]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As part of this step, we initialize a prompt template. We can define the task
    we want the model to perform as a simple instruction in English using the `document`
    and **query**. These arguments are expected to be in the execution context at
    runtime. The second argument, **output_parser**, takes an **AnswerParser** object.
    This object instructs the **PromptNode** object to store the results in the **answers**
    element. After defining the prompt, we initialize a **PromptNode** object with
    a model and the prompt template. We use the **google/flan-t5-large** model as
    the answer generator. This model is based on the Google T5 language model and
    has been fine-tuned (**flan** stands for **fine-tuning language models**). Fine-tuning
    a language model with an instruction dataset allows the language model to perform
    tasks following simple instructions and generating text based on the given context
    and instruction. One of the fine-tuning steps as part of this model training was
    to operate on human written instructions as tasks. This allowed the model to perform
    different downstream tasks on instructions alone and reduced the need for any
    few-shot examples to be trained on.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为这一步骤的一部分，我们初始化一个提示模板。我们可以使用`document`和**query**来定义模型要执行的任务，将其作为英语中的简单指令。这些参数在运行时预期存在于执行上下文中。第二个参数**output_parser**接受一个**AnswerParser**对象。此对象指示**PromptNode**对象将结果存储在**answers**元素中。在定义提示后，我们使用模型和提示模板初始化一个**PromptNode**对象。我们使用**google/flan-t5-large**模型作为答案生成器。该模型基于Google
    T5语言模型，并经过微调（**flan**代表**微调语言模型**）。使用指令数据集微调语言模型允许语言模型根据简单指令执行任务，并基于给定上下文和指令生成文本。模型训练过程中的一个微调步骤是操作人类编写的指令作为任务。这使得模型仅通过指令即可执行不同的下游任务，并减少了训练时对任何少样本示例的需求。
- en: '[PRE25]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now create a pipeline and add the **retriever** and **prompt_node** components
    that we initialized in the previous steps. The **retriever** component operates
    on the query supplied by the user and generates a set of results. These results
    are passed to the prompt node, which uses the configured **flan-t5-model** to
    generate the answer:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建一个管道，并添加我们在上一步中初始化的**retriever**和**prompt_node**组件。**retriever**组件操作用户提供的查询并生成一组结果。这些结果传递给提示节点，该节点使用配置的**flan-t5-model**生成答案：
- en: '[PRE26]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once the pipeline is set up, we use it to answer questions on the content based
    on the dataset:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦设置好管道，我们就用它来根据数据集回答关于内容的问题：
- en: '[PRE27]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: See also
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Please refer to the prompt engineering guide on Haystack on how to generate
    prompts for your use cases ([https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines](https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅Haystack上的提示工程指南，了解如何为您的用例生成提示（[https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines](https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines)）。
- en: Summarizing text using pre-trained models based on Transformers
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于Transformer的预训练模型进行文本摘要
- en: We will now explore techniques for performing text summarization. Generating
    a summary for a long passage of text allows NLP practitioners to extract the relevant
    information for their use cases and use these summaries for other downstream tasks.
    As part of the summarization, we will explore recipes that use Transformer models
    to generate the summaries.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨执行文本摘要的技术。为长篇文本生成摘要允许NLP从业者提取其用例中的相关信息，并使用这些摘要进行其他下游任务。在摘要过程中，我们将探讨使用Transformer模型生成摘要的配方。
- en: Getting ready
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Our first recipe for summarization will use the Google `9.5_summarization.ipynb`
    notebook from the code site if you need to work from an existing notebook.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一种摘要配方将使用来自代码网站的Google `9.5_summarization.ipynb`笔记本，如果您需要从一个现有的笔记本开始工作。
- en: How to do it
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'Let’s get started:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'Do the necessary imports:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE29]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As part of this step, we initialize the input passage that we need to summarize
    along with the pipeline. We also calculate the length of the passage since this
    will be used as an argument to be passed to the pipeline during the task execution
    in the next step. Since we have defined the task as **summarization**, the object
    returned by the pipeline module is of the **SummarizationPipeline** type. We also
    pass **t5-large** as the model parameter for the pipeline. This model is based
    on the **Encoder-Decoder** Transformer model and acts as a pure sequence-to-sequence
    model. That means the input and output to/from the model are text sequences. This
    model was pre-trained using the denoising objective of finding masked words in
    a sentence followed by fine-tuning on specific downstream tasks such as summarization,
    textual entailment, language translation, and so on:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为这一步骤的一部分，我们初始化了需要总结的输入段落以及管道。我们还计算了段落的长度，因为这将作为下一个步骤中任务执行时传递给管道的参数。由于我们将任务定义为**总结**，管道模块返回的对象是**SummarizationPipeline**类型。我们还传递**t5-large**作为管道的模型参数。这个模型基于**编码器-解码器**Transformer模型，并作为一个纯序列到序列模型。这意味着模型的输入和输出都是文本序列。这个模型使用寻找句子中掩码词的降噪目标进行预训练，随后在总结、文本蕴涵、语言翻译等特定下游任务上进行微调：
- en: '[PRE30]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We now use the **pipeline_instance** initialized in the previous step and pass
    the text passage to it to perform the **summarization** step. A string array can
    be passed as well if multiple sequences are to be summarized. We pass **max_length=512**
    as the second argument. The T5 model is memory-intensive and the compute requirements
    grow quadratically with the increase in the input text length. This step might
    take a few minutes to complete based on the compute capability of the environment
    you are executing this on:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在使用之前步骤中初始化的**pipeline_instance**并将文本段落传递给它以执行**总结**步骤。如果需要总结多个序列，也可以传递一个字符串数组。我们将**max_length=512**作为第二个参数传递。T5模型内存密集，计算需求随着输入文本长度的增加而呈二次增长。根据您执行此操作的环境的计算能力，这一步骤可能需要几分钟才能完成：
- en: '[PRE31]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once the **summarization** step is complete, we extract the result from the
    output and print it. The pipeline returns a list of dictionaries. Each list item
    corresponds to the input argument. In this case, since we passed only one string
    as input, the first item in the list is the output dictionary that contains our
    summary. The summary can be retrieved by indexing the dictionary on the **summary_text**
    element:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦**总结**步骤完成，我们就从输出中提取结果并打印出来。管道返回一个字典列表。列表中的每个项目对应于输入参数。在这种情况下，由于我们只传递了一个字符串作为输入，列表中的第一个项目是包含我们的摘要的输出字典。可以通过在**summary_text**元素上索引字典来检索摘要：
- en: '[PRE32]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: There’s more…
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Now that we have seen how we can generate a summary using the T5 model, we can
    use the same code framework and tweak it slightly to use other models to generate
    summaries.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何使用T5模型生成摘要，我们可以使用相同的代码框架并稍作调整，以使用其他模型生成摘要。
- en: 'The following lines would be common for the other summarization recipes that
    we are using. We added an extra variable named `device`, which we will use in
    our pipelines. We set this variable to the value of the device that we will use
    to generate the summary. If a GPU is present and configured in the system, it
    will be used; otherwise, the summarization will be performed using the CPU:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行对于我们将使用的其他总结食谱来说是常见的。我们添加了一个名为`device`的额外变量，我们将在我们的管道中使用它。我们将此变量设置为生成摘要时将使用的设备值。如果系统中存在并配置了GPU，它将被使用；否则，将使用CPU进行总结：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the following example, we use the **BART** model ([https://huggingface.co/facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn))
    from Facebook. This model was trained using a denoising objective. A function
    adds some random piece of text to an input sequence. The model is trained based
    on the objective to denoise or remove the noisy text from the input sequence.
    The model was further fine-tuned using the **CNN DailyMail** dataset ([https://huggingface.co/datasets/abisee/cnn_dailymail](https://huggingface.co/datasets/abisee/cnn_dailymail))
    for summarization:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用了Facebook的**BART**模型([https://huggingface.co/facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn))。这个模型使用降噪目标进行训练。一个函数会在输入序列中添加一些随机的文本片段。模型基于降噪或从输入序列中移除噪声文本的目标进行训练。该模型进一步使用**CNN
    DailyMail**数据集([https://huggingface.co/datasets/abisee/cnn_dailymail](https://huggingface.co/datasets/abisee/cnn_dailymail))进行微调，用于总结：
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we observe from the generated summary, it is verbose and extractive in nature.
    Let’s try generating a summary with another model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成的摘要中我们可以观察到，它具有冗长和提取性的特点。让我们尝试使用另一个模型来生成摘要。
- en: 'In the following example, we use the **PEGASUS** model from Google ([https://huggingface.co/google/pegasus-large](https://huggingface.co/google/pegasus-large))
    for summarization. This model is a Transformer-based Encoder-Decoder model that
    was pre-trained with a large news and web page corpus – C4 ([https://huggingface.co/datasets/allenai/c4](https://huggingface.co/datasets/allenai/c4))
    and the HugeNews dataset – on a training objective of detecting important sentences.
    HugeNews is a dataset of 1.5 billion articles curated from news and news-like
    websites from 2013–2019\. This model was further fine-tuned for summarization
    using the subset of the same dataset. The training objective for the fine-tuning
    involved masking important sentences and making the model generate a summary that
    has these important sentences. This model generates abstract summaries:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用 Google 的 **PEGASUS** 模型（[https://huggingface.co/google/pegasus-large](https://huggingface.co/google/pegasus-large)）进行摘要。这是一个基于
    Transformer 的编码器-解码器模型，它使用大型新闻和网页语料库 C4（[https://huggingface.co/datasets/allenai/c4](https://huggingface.co/datasets/allenai/c4)）和巨大的新闻数据集进行预训练，训练目标是检测重要句子。巨大的新闻数据集是从
    2013 年至 2019 年从新闻和类似新闻网站上精心挑选的 15 亿篇文章的数据集。该模型进一步使用相同数据集的子集进行摘要微调。微调的训练目标涉及屏蔽重要句子，并使模型生成包含这些重要句子的摘要。该模型生成抽象摘要：
- en: '[PRE36]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As we observe from the generated summary, it is concise and abstractive.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成的摘要中我们可以观察到，它简洁且具有抽象性。
- en: See also
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考阅读
- en: As many new and improved models for summarization are always in the works, we
    recommend that you refer to the models on the Hugging Face site ([https://huggingface.co/models?pipeline_tag=summarization](https://huggingface.co/models?pipeline_tag=summarization))
    and make the respective choice based on your requirements.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于总是有许多新的和改进的摘要模型正在开发中，我们建议您参考 Hugging Face 网站上的模型（[https://huggingface.co/models?pipeline_tag=summarization](https://huggingface.co/models?pipeline_tag=summarization)），并根据您的需求做出相应的选择。
- en: Detecting sentence entailment
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测句子蕴涵
- en: 'In this recipe, we will explore techniques to detect `premise`, which sets
    up a context. The second sentence is the `hypothesis`, which serves as the claim.
    Textual entailment identifies the contextual relationship between the `premise`
    and the `hypothesis`. These relationships can be of three types, defined as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将探讨检测 `前提` 的技术，它设定了上下文。第二句是 `假设`，它作为主张。文本蕴涵确定了 `前提` 和 `假设` 之间的上下文关系。这些关系可以分为三种类型，如下定义：
- en: '**Entailment** – The hypothesis supports the premise'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蕴涵** – 假设支持前提'
- en: '**Contradiction** – The hypothesis contradicts the premise'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矛盾** – 假设与前提矛盾'
- en: '**Neutral** – The hypothesis neither supports nor contradicts the premise'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中立** – 假设既不支持也不反驳前提'
- en: Getting ready
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the Transformers library to detect text entailment. You can use
    the `9.6_textual_entailment.ipynb` notebook from the code site if you need to
    work from an existing notebook.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Transformers 库来检测文本蕴涵。如果您需要从一个现有的笔记本开始工作，可以使用代码网站上的 `9.6_textual_entailment.ipynb`
    笔记本。
- en: How to do it...
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will initialize different sets of sentences that are related
    through each of the previously defined relationships and explore methods to detect
    these relationships. Let’s get started:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将初始化与之前定义的每种关系相关的不同句子集，并探讨检测这些关系的方法。让我们开始吧：
- en: 'Do the necessary imports:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE37]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Initialize the device, the tokenizer, and the model. In this case, we are using
    Google’s **t5-small** model. We set the **legacy** flag to **False** since we
    don’t need to use the legacy behavior of the model. We set the **device** value
    based on whatever device we have available in our execution environment. Similarly,
    for the model, we set the **model** name and **device** parameter similar to the
    tokenizer. We set the **return_dict** parameter as **True** so that we get the
    model results as a dictionary instead of a tuple:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化设备、分词器和模型。在这种情况下，我们使用 Google 的 **t5-small** 模型。我们将 **legacy** 标志设置为 **False**，因为我们不需要使用模型的旧行为。我们根据执行环境中可用的任何设备设置
    **device** 值。同样，对于模型，我们设置与分词器相似的 **model** 名称和 **device** 参数。我们将 **return_dict**
    参数设置为 **True**，以便我们以字典的形式获取模型结果，而不是元组：
- en: '[PRE38]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We initialize the **premise** and **hypothesis** sentences. In this case, the
    hypothesis supports the premise:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化**premise**和**hypothesis**句子。在这种情况下，假设支持前提：
- en: '[PRE39]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In this step, we call the tokenizer with the **mnli premise** and **hypothesis**
    values. This is a simple text concatenation step to set up the tokenizer for the
    **entailment** task. We read the **input_ids** property to get the token identifiers
    for the concatenated string. Once we have the token IDs, we use the model to generate
    the entailment prediction. This returns a list of tensors with the predictions,
    which we use in the next step:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们使用**mnli premise**和**hypothesis**值调用分词器。这是一个简单的文本连接步骤，用于设置分词器以进行**entailment**任务。我们读取**input_ids**属性以获取连接字符串的标记标识符。一旦我们有了标记ID，我们就使用模型来生成蕴涵预测。这返回一个包含预测的张量列表，我们在下一步中使用这些张量：
- en: '[PRE40]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In this step, we call the **decode** method of the tokenizer and pass it the
    first tensor (or vector) of the tensors that were returned by the **generate**
    call of the model. We also instruct the tokenizer to skip the special tokens that
    are used by the tokenizer internally. The tokenizer generates the string label
    from the vector that is passed in. We print the prediction result. In this case,
    the generated prediction by the model is **entailment**:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们调用分词器的**decode**方法，并传递模型**generate**调用返回的张量中的第一个张量（或向量）。我们还指示分词器跳过分词器内部使用的特殊标记。分词器从传入的向量中生成字符串标签。我们打印预测结果。在这种情况下，模型生成的预测是**entailment**：
- en: '[PRE41]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: There’s more...
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Now that we have shown an example in the case of entailment with a single sentence,
    the same framework can be used to process a batch of sentences to generate entailment
    predictions. We will tailor *steps 3*,*4*, and *5* from the previous recipe for
    this example. We initialize an array of two sentences for both `premise` and `hypothesis`,
    respectively. Both the `premise` sentences are the same, while the `hypothesis`
    sentences are of `entailment` and `contradiction`, respectively:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经展示了使用单个句子进行蕴涵的例子，相同的框架可以用来处理句子批处理以生成蕴涵预测。我们将根据这个例子调整之前食谱中的*步骤3*、*步骤4*和*步骤5*。我们分别为`premise`和`hypothesis`初始化一个包含两个句子的数组。两个`premise`句子相同，而`hypothesis`句子分别是`entailment`和`contradiction`：
- en: '[PRE43]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Since we have an array of sentences for both `premises` and `hypothesis`, we
    create an array of concatenated inputs that combine the `tokenizer` instruction.
    This array is used to pass to the tokenizer and we use the token IDs returned
    by `tokenizer` in the next step:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们既有`premises`也有`hypothesis`的句子数组，我们创建一个包含连接输入的数组，这些输入结合了`tokenizer`指令。这个数组用于传递给分词器，我们在下一步中使用分词器返回的标记ID：
- en: '[PRE44]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We now generate the predictions using the same methodology that we used earlier.
    However, in this step, we generate the inference label by iterating through the
    tensors returned by the model output and printing the prediction:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用之前使用的方法生成预测。然而，在这一步中，我们通过迭代模型输出返回的张量来生成推理标签：
- en: '[PRE45]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Enhancing explainability via a classifier-invariant approach
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过分类器不变方法增强可解释性
- en: Now, we will explore recipes that will allow us to understand the decisions
    made by text classifiers. We will explore techniques that will use a sentiment
    classifier and NLP explainability libraries to interpret the classification labels
    and their relation to the input text, especially in the aspect of individual words
    in the text.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探索一些食谱，这些食谱将使我们能够理解文本分类器所做的决策。我们将探讨使用情感分类器和NLP可解释性库来解释分类标签及其与输入文本关系的技巧，特别是在文本中单个单词的方面。
- en: Though a lot of the current models for text classification in NLP are based
    on deep neural networks, it is difficult to interpret the results of classification
    via the network weights or parameters. It is equally challenging to map these
    network parameters to the individual components or words in the input. However,
    there are still a few techniques in the NLP space to help us understand the decisions
    made by the classifier. We will explore these techniques in the current recipe
    and the following one.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前许多NLP中的文本分类模型都是基于深度神经网络，但很难通过网络权重或参数来解释分类结果。将这些网络参数映射到输入的各个组件或单词同样具有挑战性。然而，在NLP领域仍有一些技术可以帮助我们理解分类器的决策。我们将在当前食谱和下一个食谱中探讨这些技术。
- en: In this recipe, we will learn how to interpret the feature importance of each
    word in a text passage while being invariant of the classifier model. This technique
    can be used for any text classifier as we treat the classifier as a black box
    and use the results of the predictions to infer the results from an explainability
    perspective.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将学习如何解释文本段落中每个单词的特征重要性，同时保持对分类器模型的无关性。这项技术可以用于任何文本分类器，因为我们把分类器视为黑盒，并使用预测结果从可解释性的角度推断结果。
- en: Getting ready
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the `lime` library for explainability. You can use the `9.7_explanability_via_classifier.ipynb`
    notebook from the code site if you want to work from an existing notebook.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`lime`库来进行可解释性。如果您想从一个现有的笔记本开始工作，可以使用代码网站上的`9.7_explanability_via_classifier.ipynb`笔记本。
- en: How to do it...
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In this recipe, we will repurpose a classifier that we built in the *Transformers*
    chapter and use it to generate a sentiment prediction. We will call this classifier
    multiple times with a perturbation of the input to understand the contribution
    of each word to the sentiment. Let’s get started:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将重新利用我们在*Transformers*章节中构建的分类器，并使用它来生成情感预测。我们将多次调用此分类器，并对输入进行扰动，以了解每个单词对情感的贡献。让我们开始吧：
- en: 'Do the necessary imports:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE46]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In this step, we initialize the device and the pipeline for sentiment classification.
    For more details on this step, please refer to chapter-8\.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们初始化设备以及情感分类的流水线。有关此步骤的更多详细信息，请参阅第8章。
- en: '[PRE47]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In this step, we initialize a sample text passage along with setting the print
    options. Setting the print options allows us to print the outputs in the later
    steps in an easy-to-read format:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们初始化一个样本文本段落，并设置打印选项。设置打印选项允许我们在后续步骤中以易于阅读的格式打印输出：
- en: '[PRE48]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In this step, we create a wrapper function for sentiment classification. This
    method is used by the explainer to invoke the classification pipeline multiple
    times to gauge the contribution of each word in the passage:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们为情感分类创建一个包装函数。此方法由解释器用于多次调用分类管道，以衡量段落中每个单词的贡献：
- en: '[PRE49]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In this step, we instantiate the **LimeTextExplainer** class and call the **explain_instance**
    method for it. This method takes the sample text along with the **classifier**
    wrapper function. The wrapper function passed to this method expects it to take
    a single instance of a string and return the probabilities of the target classes.
    In this case, our wrapper function accepts a simple string and returns the probabilities
    for the **NEGATIVE** and **POSITIVE** classes, respectively, and in that order:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们实例化**LimeTextExplainer**类，并为其调用**explain_instance**方法。此方法需要样本文本以及**分类器**包装函数。传递给此方法的包装函数期望它接受一个字符串的单个实例，并返回目标类别的概率。在这种情况下，我们的包装函数接受一个简单的字符串，并按顺序返回**负面**和**正面**类别的概率：
- en: '[PRE50]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In this step, we print the class probabilities for the sample text. As we observe,
    the sample text has been assigned a **POSITIVE** sentiment as per the classifier:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们打印样本文本的类别概率。正如我们所观察到的，样本文本被分配了分类器的**正面**情感：
- en: '[PRE51]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In this step, we print the explanations. As we observe from the probabilities
    for each word, the words **entertaining** and **liked** contributed the most to
    the **POSITIVE** class. There are some words that contribute negatively to the
    positive sentiment, but overall, the sentence is classified as positive:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们打印解释。正如我们从每个单词的概率中观察到的那样，单词**娱乐**和**喜欢**对**正面**类别的贡献最大。有一些单词对正面情绪有负面影响，但总体而言，句子被分类为正面：
- en: '[PRE53]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let’s initialize another text to something with a negative sentiment:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们初始化另一个具有负面情绪的文本：
- en: '[PRE55]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Get the class probability as predicted by the classifier for the new text and
    print it:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取分类器对新文本预测的类别概率并打印出来：
- en: '[PRE56]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Use the **explainer** instance to evaluate the text and print the contribution
    of each word to the negative sentiment. We observe that the words **boring** and
    **slow** contributed most to the negative sentiment:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**解释器**实例来评估文本并打印每个单词对负面情绪的贡献。我们观察到单词**无聊**和**缓慢**对负面情绪的贡献最大：
- en: '[PRE58]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: There’s more...
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Now that we have seen how to interpret the word contributions for the sentiment
    classification, we want to further improve our recipe to provide a visual representation
    of the explainability:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何解释情感分类中的单词贡献，我们希望进一步改进我们的配方，以提供可解释性的可视化表示：
- en: 'Continuing on from *step 5* in the recipe, we can also print the explanations
    using **pyplot**:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从食谱中的*步骤5*继续，我们还可以使用**pyplot**打印解释：
- en: '[PRE60]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![](img/B18411_09_1.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18411_09_1.jpg)'
- en: Figure 9.1 – Probability contribution of each word in the sentence to the final
    class
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 句中每个单词对最终类别的概率贡献
- en: 'We can also highlight the exact words in the text. The contribution of each
    word is also highlighted using a light or dark shade of the assigned class, which,
    in this case, is orange. The words with the blue highlights are the ones that
    contribute against the **POSITIVE** class:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以突出显示文本中的确切单词。每个单词的贡献也使用分配的类别的浅色或深色阴影突出显示，在这种情况下，是橙色。带有蓝色高亮的单词是那些对**POSITIVE**类有贡献的单词：
- en: '[PRE61]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](img/B18411_09_2.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18411_09_2.jpg)'
- en: Figure 9.2 – The highlighted class association for each word
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 每个单词的高亮类别关联
- en: Enhancing explainability via text generation
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过文本生成增强可解释性
- en: In this recipe, we will learn how to understand the inference emitted by the
    classifier using text generation. We will use the same classifier that we used
    in the *Explainability via a classifier invariant approach* recipe. To better
    understand the behavior of the classifier in a random setting, we will replace
    the words in the input sentence with different tokens.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将学习如何通过文本生成理解分类器发出的推理。我们将使用与我们在*通过分类器不变方法进行可解释性*食谱中使用的相同的分类器。为了更好地理解分类器在随机设置中的行为，我们将用不同的标记替换输入句子中的单词。
- en: Getting ready
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will need to install a `spacy` artifact for this recipe. Please use the following
    command in your environment before starting this recipe.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个食谱，我们需要安装一个`spacy`工件。请在开始此食谱之前，在您的环境中使用以下命令。
- en: 'Now that we have installed `spacy`, we will need to download the `en_core_web_sm`
    pipeline using the following step beforehand:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了`spacy`，我们还需要在以下步骤之前下载`en_core_web_sm`管道：
- en: '[PRE62]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: You can use the `9.8_explanability_via_generation.ipynb` notebook from the code
    site if you need to work from an existing notebook.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要从一个现有的笔记本开始工作，可以使用代码网站上的`9.8_explanability_via_generation.ipynb`笔记本。
- en: How to do it
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作
- en: 'Let’s get started:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'Do the necessary imports:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE63]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'In this step, we initialize the **spacy** pipeline with the **en_core_web_sm**
    model. This pipeline contains the components for **tok2vec**, **tagger**, **parser**,
    **ner**, **lemmatizer**, and so on, and is optimized for the CPU:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们使用**en_core_web_sm**模型初始化**spacy**管道。这个管道包含**tok2vec**、**tagger**、**parser**、**ner**、**lemmatizer**等组件，并针对CPU进行了优化：
- en: '[PRE64]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In this step, we initialize the device and our classifier. We use the same
    sentence classifier that we used in the *Explainability via a classifier invariant
    approach* recipe. The idea is to understand the same classifier and observe how
    its classification behaves for different inputs, as generated by the **anchor**
    explainability library:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化设备和我们的分类器。我们使用与我们在*通过分类器不变方法进行可解释性*食谱中使用的相同的句子分类器。想法是理解相同的分类器，并观察其分类在不同输入下的行为，这些输入由**锚点**可解释性库生成：
- en: '[PRE65]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In this step, we define a function that takes a list of sentences and emits
    a list of **POSITIVE** or **NEGATIVE** labels for them. This method internally
    calls the classifier that was initialized in the previous step:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们定义一个函数，它接受一个句子列表并为其发出**POSITIVE**或**NEGATIVE**标签列表。此方法内部调用之前步骤中初始化的分类器：
- en: '[PRE66]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In this step, we initialize a passage of text. We use that text sentence to
    predict its class probability by using the **predict_prob** method and print the
    prediction:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们初始化一段文本。我们使用该文本句子通过使用**predict_prob**方法预测其类别概率，并打印预测结果：
- en: '[PRE67]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In this step, we call the `explain_instance` method for the explainer instance.
    We pass it the input sentence, the `predict_prob` method, and a `threshold`. The
    explainer instance uses the `predict_prob` method to invoke the classifier for
    different variations of the input sentence to explain what words contribute the
    most. It also identifies what class labels are emitted when some words in the
    input sentence are replaced by the `UNK` token. The `threshold` parameter defines
    the minimum probability for a given class under which all the generated samples
    are to be ignored. This effectively means that all the sentences generated by
    the explainer will have the probability greater than the threshold, for a given
    class.exp = explainer.explain_instance(text, predict_prob, threshold=0.95)
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这一步，我们为解释器实例调用`explain_instance`方法。我们传递给它输入句子、`predict_prob`方法和一个`threshold`参数。解释器实例使用`predict_prob`方法来调用分类器，对输入句子的不同变体进行解释，以确定哪些单词对贡献最大。它还识别当输入句子中的某些单词被`UNK`标记替换时发出的类标签。`threshold`参数定义了给定类别的最小概率，低于此概率的所有生成的样本都将被忽略。这意味着解释器生成的所有句子对于给定类别都将具有大于阈值的概率.exp
    = explainer.explain_instance(text, predict_prob, threshold=0.95)
- en: 'We print the **anchor** words that contribute the most to the **POSITIVE**
    label in this case. We also print the precision as measured by the explainer.
    We observe that it identifies the words **good**, **a**, and **is** as contributing
    the most to the **POSITIVE** classification:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印出在这种情况下对**积极**标签贡献最大的**锚点**单词。我们还打印出解释器测量的精度。我们观察到它将单词**good**、**a**和**is**识别为对**积极**分类贡献最大：
- en: '[PRE68]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We print some of the possible sentences that the explainer believes would result
    in a **POSITIVE** classification. The explainer perturbs the input sentence by
    replacing one or more of the words with the **UNK** token and invokes the classifier
    method on the perturbed sentence. There are some interesting observations on how
    the classifier behaves. For example, the sentence **The UNK UNK is a good story
    UNK** has been labeled as **POSITIVE**. This indicates that the title of the story
    is irrelevant to the classification. Another interesting example is the sentence
    **The UNK mermaid is a good UNK UNK**. In this sentence, we observe that the classifier
    is invariant to the object in context, which, in this case, is a story:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印出解释器认为可能导致**积极**分类的一些可能句子。解释器通过将一个或多个单词替换为**UNK**标记来扰动输入句子，并在扰动后的句子上调用分类器方法。关于分类器行为的一些有趣观察。例如，句子**UNK
    UNK 是一个好故事 UNK**被标记为**积极**。这表明故事标题与分类无关。另一个有趣的例子是句子**UNK 美人鱼是一个好 UNK UNK**。在这个句子中，我们观察到分类器对上下文中的对象（在这种情况下是故事）是不变的：
- en: '[PRE70]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Similar to the previous step, we now ask the explainer to print sentences that
    would result in a **NEGATIVE** classification. In this particular case, the explainer
    was unable to generate any negative examples by just replacing the words. The
    explainer is unable to generate any **NEGATIVE** examples. This happens because
    the explainer can only use the **UNK** token to perturb the input sentence. And
    since the **UNK** token is not associated with any **POSITIVE** or **NEGATIVE**
    sentiment, using just that token does not provide a way to affect the classifier
    to generate a **NEGATIVE** classification. We get no output from this step:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与上一步类似，我们现在要求解释器打印出可能导致**消极**分类的句子。在这种情况下，解释器无法仅通过替换单词来生成任何负面示例。解释器无法生成任何**消极**示例。这是因为解释器只能使用**UNK**标记来扰动输入句子。由于**UNK**标记与任何**积极**或**消极**情感无关，仅使用该标记无法提供影响分类器以生成**消极**分类的方法。这一步没有输出：
- en: '[PRE72]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'So far, we used the **UNK** token to vary or perturb the input to the classifier.
    The presence of the **UNK** token in the text makes it unnatural. To understand
    the classifier better, it would be useful to enumerate natural sentences and understand
    how those affect the classification. We will use **BERT** to perturb the input
    and get the explainer to generate natural sentences. This will help us better
    understand how the results differ in the context of sentences that are natural:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用**UNK**标记来改变或扰动输入到分类器的数据。文本中**UNK**标记的存在使其显得不自然。为了更好地理解分类器，列举自然句子并了解它们如何影响分类将是有用的。我们将使用**BERT**来扰动输入，并让解释器生成自然句子。这将帮助我们更好地理解在自然句子上下文中结果如何不同：
- en: '[PRE73]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We now print some sentences for which the classifier thinks the label would
    be **POSITIVE**. In this instance, we observe that the explainer generates sentences
    that are natural. For example, the generated sentence **my little mermaid tells
    a good story** replaced the word **the** in the original sentence with **my**.
    This word was generated via BERT. BERT uses the encoder part of the Transformer
    architecture and has been trained to predict missing words in a sentence by masking
    them. The explainer in this case masks the individual words in the input sentence
    and uses BERT to generate the replacement word. Since the underlying model to
    generate text is a probabilistic model, your output might differ from the following
    and also vary between runs:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在打印出一些句子，分类器认为这些句子的标签应该是**正面的**。在这种情况下，我们观察到解释器生成的句子是自然的。例如，生成的句子**我的小美人鱼讲述了一个好故事**，将原句中的**the**一词替换成了**my**。这个单词是通过BERT生成的。BERT使用了Transformer架构的编码器部分，并且经过训练，可以通过遮蔽句子中的缺失单词来预测这些单词。在这种情况下，解释器遮蔽了输入句子中的单个单词，并使用BERT生成替换单词。由于生成文本的底层模型是一个概率模型，你的输出可能与以下内容不同，并且在不同的运行中也可能有所变化：
- en: '[PRE74]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We now print some sentences for which the classifier thinks the label would
    be **NEGATIVE**. Though not all the sentences appear to have a **NEGATIVE** sentiment,
    there are quite a few of them with such a sentiment:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在打印出一些句子，分类器认为这些句子的标签应该是**负面的**。尽管并非所有句子都表现出**负面**的情感，但其中相当多的是这样的情感：
- en: '[PRE76]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
