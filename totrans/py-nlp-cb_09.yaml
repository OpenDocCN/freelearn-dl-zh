- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural Language Understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore recipes that will allow us to interpret and
    understand the text contained in short as well as long passages. **Natural language
    understanding** (**NLU**) is a very broad term and the various systems developed
    as part of NLU do not interpret or understand a passage of text the same way a
    human reader would. However, based on the specificity of the task, we can create
    some applications that can be combined to generate an interpretation or understanding
    that can be used to solve a given problem related to text processing.
  prefs: []
  type: TYPE_NORMAL
- en: Organizations that have a huge document corpus need a seamless way to search
    through documents. More specifically, what users really need is an answer to a
    specific question without having to glean through a list of documents that are
    returned as part of a document search. Users would prefer the query to be formulated
    as a question in natural language and the answer to be emitted in the same manner.
  prefs: []
  type: TYPE_NORMAL
- en: Another set of applications is that of document summarization and text entailment.
    While processing a large set of documents, it is helpful if the document length
    can be shortened without the loss of meaning or context. Additionally, it’s important
    to determine whether the information contained in the document at the sentence
    level entails itself.
  prefs: []
  type: TYPE_NORMAL
- en: While we work on processing and classifying the documents, there are always
    challenges in understanding why or how the model assigns a label to a piece of
    text – more specifically, what parts of the text contribute to the different labels.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will cover different techniques to explore the various aspects
    previously described. We will follow recipes that will allow us to perform these
    tasks and understand the underlying building blocks that help us achieve the end
    goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'As part of this chapter, we will build recipes for the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Answering questions from a short text passage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering questions from a long text passage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering questions from a document corpus in an extractive manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering questions from a document corpus in an abstractive manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing text using pretrained models based on Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting sentence entailment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing explainability via a classifier-invariant approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing explainability via text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is in a folder named `Chapter9` in the GitHub repository
    of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09)).
  prefs: []
  type: TYPE_NORMAL
- en: As in previous chapters, the packages required for this chapter are part of
    the `poetry` environment. Alternatively, you can install all the packages using
    the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: Answering questions from a short text passage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started with question answering, we will start with a simple recipe that
    can answer a question from a short passage.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of this chapter, we will use the libraries from Hugging Face ([huggingface.co](http://huggingface.co)).
    For this recipe, we will use the `BertForQuestionAnswering` and `BertTokenizer`
    modules from the Transformers package. The `BertForQuestionAnswering` model uses
    the base BERT large uncased model that was trained on the `SQuAD` dataset and
    fine-tuned for the question-answering task. This pre-trained model can be used
    to load a text passage and answer questions based on the contents of the passage.
    You can use the `9.1_question_answering.ipynb` notebook from the code site if
    you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will load a pretrained model that has been trained on the
    SQuAD dataset ([https://huggingface.co/datasets/squad](https://huggingface.co/datasets/squad)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipe does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: It initializes a question-answering pipeline based on the pre-trained **BertForQuestionAnswering**
    model and **BertTokenizer** tokenizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It further initializes a context passage and a question and emits the output
    of the answer based on these two parameters. It also prints the exact text of
    the answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It asks a follow-up question to the same pipeline by just changing the question
    text, and prints the exact text answer to the question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps for the recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports to import the necessary types and functions from the
    **datasets** package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the model and tokenizer, respectively, using the
    pre-trained **bert-large-uncased-whole-word-masking-finetuned-squad** artifacts.
    These will be downloaded from the Hugging Face website if they are not present
    locally on the machine as part of these calls. We have chosen the specific model
    and tokenizer for our recipe, but feel free to explore other models on the Hugging
    Face site that might suit your needs. As a generic step for this and the following
    recipe, we discover whether there are any GPU devices in the system and attempt
    to use them. If a GPU is not detected, we use the CPU instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a question-answering pipeline with the model and
    tokenizer. The task type for this pipeline is set to **question-answering**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a **context** passage. This passage was generated
    as part of our *Text Generation via Transformers* example in [*Chapter 8*](B18411_08.xhtml#_idTextAnchor205).
    It’s entirely acceptable if you want to use a different passage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a question text, invoke the pipeline with the context
    and question, and store the result in a variable. The type of the result is a
    Python **dict** object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we print the value of the result. The **score** value shows the
    probability of the answer. The **start** and **end** values denote the start and
    end character indices in the **context** passage that constitute the answer. The
    **answer** value denotes the actual text of the answer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we print the exact text answer. This is present in the **answer**
    key in the **result** dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we ask another question using the same context and print the
    result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Answering questions from a long text passage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we learned an approach to extract the answer to a question,
    given a context. This pattern involves the model retrieving the answer from the
    given context. The model cannot answer a question that is not contained in the
    context. This does serve a purpose where we want an answer from a given context.
    This type of question-answering system is defined as **Closed Domain Question**
    **Answering** (**CDQA**).
  prefs: []
  type: TYPE_NORMAL
- en: There is another system of question answering that can answer questions that
    are general in nature. These systems are trained on larger corpora. This training
    provides them with the ability to answer questions that are open in nature. These
    systems are called **Open Domain Question Answering** (**ODQA**) systems.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of this recipe, we will use the `deeppavlov` library along with the
    **Knowledge Base Question Answering** (**KBQA**) model. This model has been trained
    on English wiki data as a knowledge base. It uses various NLP techniques such
    as entity linking and disambiguation, knowledge graphs, and so on to extract the
    exact answer to the question.
  prefs: []
  type: TYPE_NORMAL
- en: 'This recipe needs a few steps to set up the right environment for its execution.
    The `poetry` file for this recipe is in the `9.2_QA_on_long_passages` folder.
    We will also need to install and download the document corpus by performing the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can also use the `9.2_QA_on_long_passages.ipynb` notebook, which is contained
    within the same folder.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will initialize the KBQA model based on the `DeepPavlov`
    library and use it to answer an open question. The steps for the recipe are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the KBQA model, **kbqa_cq_en**, which is passed
    to the **build_model** method as an argument. We also set the **download** argument
    to **True** so that the model is downloaded as well in case it is missing locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the initialized model and pass it a couple of questions that we want
    to be answered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We print the result as returned by the model. The result contains three arrays.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first array contains the exact answers to the question ordered in the same
    way as the original input. In this case, the answers `Cairo` and `Hillary Clinton`
    are in the same order as the questions they pertain to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You might observe some additional artifacts in the output. These are internal
    identifiers that are generated by the library. We have omitted them for brevity:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on the internal details of the working of DeepPavlov, please
    refer to [https://deeppavlov.ai](https://deeppavlov.ai).
  prefs: []
  type: TYPE_NORMAL
- en: Answering questions from a document corpus in an extractive manner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the use cases where we have a document corpus that contains a large number
    of documents, it’s not feasible to load the document content at runtime to answer
    a question. Such an approach would lead to long query times and would not be suitable
    for production-grade systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to preprocess the documents and transform
    them into a form for faster reading, indexing, and retrieval that allows the system
    to extract the answer for a given question with short query times.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As part of this recipe, we will use the **Haystack** ([https://haystack.deepset.ai/](https://haystack.deepset.ai/))
    framework to build a **QA system** that can answer questions from a document corpus.
    We will download a dataset based on *Game of Thrones* and index it. For our QA
    system to be performant, we will need to index the documents beforehand. Once
    the documents are indexed, answering a question follows a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retriever**: Since we have many documents, scanning each document to fetch
    an answer is not a feasible approach. We will first retrieve a set of candidate
    documents that can possibly contain an answer to our question. This step is performed
    using a **Retriever** component. This searches through the pre-created index to
    filter the number of documents that we will need to scan to retrieve the exact
    answer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reader**: Once we have a candidate set of documents that could contain the
    answer, we will search these documents to retrieve the exact answer to our question.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will discuss the details of these components throughout this recipe. You
    can use the `9.3_QA_on_document_corpus.ipynb` notebook from the code site if you
    need to work from an existing notebook. To start with, let’s set up the prerequisites.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this step, we do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we specify a folder that will be used to save our dataset. Then,
    we retrieve the dataset from the source. The second parameter to the **fetch_archive_from_http**
    method is the folder in which the dataset will be downloaded. We set the parameter
    to the folder that we defined in the first line. The **fetch_archive_from_http**
    method decompresses the archive **.zip** file and extracts all files into the
    same folder. We then read from the folder and create a list of files contained
    in the folder. We also print the number of files that are present:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize a document store based on the files. We create an indexing pipeline
    based on the document store and execute the indexing operation. To achieve this,
    we initialize an **InMemoryDocumentStore** instance. In this method call, we set
    the **use_bm25** argument as **True**. The document store uses **Best Match 25**
    (**bm25**) as the algorithm for the retriever step. The **bm25** algorithm is
    a simple bag-of-words-based algorithm that uses a scoring function. This function
    utilizes the number of times a term is present in the document and the length
    of the document. [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067) covers the **bm25**
    algorithm in more detail and we recommend you refer to that chapter for better
    understanding. Note that there are various other **DocumentStore** options such
    as **ElasticSearch**, **OpenSearch**, and so on. We used an **InMemoryDocumentStore**
    document store to keep the recipe simple and focus on the retriever and reader
    concepts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have loaded the documents, we initialize our retriever and reader instances.
    To achieve this, we initialize the retriever and the reader components. **BM25Retriever**
    uses the **bm25** scoring function to retrieve the initial set of documents. For
    the reader, we initialize the **FARMReader** object. This is based on deepset’s
    FARM framework, which can utilize the QA models from Hugging Face. In our case,
    we use the **deepset/roberta-base-squad2** model as a reader. The **use_gpu**
    argument can be set appropriately based on whether your device has a GPU or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now create a pipeline that we can use to answer questions. After having
    initialized the retriever and reader in the previous step, we want to combine
    them for querying. The **pipeline** abstraction from the Haystack framework allows
    us to integrate the reader and retriever together using a series of pipelines
    that address different use cases. In this instance, we will use **ExtractiveQAPipeline**
    for our QA system. After the initialization of the pipeline, we generate the answer
    to a question from the *Game of Thrones* series. The **run** method takes the
    question as the query. The second argument, **params**, dictates how the results
    from the retriever and reader are combined to present the answer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**"Retriever": {"top_k": 10}**: The **top_k** keyword argument specifies that
    the top-k (in this case, **10**) results from the retriever are used by the reader
    to search for the exact answer'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"Reader": {"top_k": 5}**: The **top_k** keyword argument specifies that the
    top-k (in this case, **5**) results from the reader are presented as the output
    of the method:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'We print the answer to our question. The system prints out the exact answer
    along with the associated context that it used to extract the answer from. Note
    that we use the value of **all** for the **details** argument. Using the **all**
    value for the same argument prints out **start** and **end** spans for the answer
    along with all the auxiliary information. Setting the value of **medium** for
    the **details** argument provides the relative score of each answer. This score
    can be used to filter out the results further based on the accuracy requirements
    of the system. Using the argument of **medium** presents only the answer and the
    context. We encourage you to make a suitable choice based on your requirements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a QA system to work in a high-performance production system, it is recommended
    to use a different document store from an in-memory one. We recommend you refer
    to [https://docs.haystack.deepset.ai/docs/document_store](https://docs.haystack.deepset.ai/docs/document_store)
    and use an appropriate document store based on your production-grade requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Answering questions from a document corpus in an abstractive manner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will build a QA system that will provide answers that are
    abstractive in nature. We will load the `bilgeyucel/seven-wonders` dataset from
    the Hugging Face site and initialize a retriever from it. This dataset has content
    about the seven wonders of the ancient world. To generate the answers, we will
    use the `PromptNode` component from the Haystack framework to set up a pipeline
    that can generate answers in an abstractive fashion. You can use the `9.4_abstractive_qa_on_document_corpus.ipynb`
    notebook from the code site if you need to work from an existing notebook. Let’s
    get started.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As part of this step, we load the **bilgeyucel/seven-wonders** dataset into
    an in-memory document store. This dataset has been created out of the Wikipedia
    pages of *Seven Wonders of the Ancient World* ([https://en.wikipedia.org/wiki/Wonders_of_the_World](https://en.wikipedia.org/wiki/Wonders_of_the_World)).
    This dataset has been preprocessed and uploaded to the Hugging Face site, and
    can be easily downloaded by using the **datasets** module from Hugging Face. We
    use **InMemoryDocumentStore** as our document store, with **bm25** as the search
    algorithm. We write the documents from the dataset into the document store. To
    have a performant query time performance, the **write_documents** method automatically
    optimizes how the documents are written. Once the documents are written into,
    we initialize the retriever based on **bm25**, similar to our previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As part of this step, we initialize a prompt template. We can define the task
    we want the model to perform as a simple instruction in English using the `document`
    and **query**. These arguments are expected to be in the execution context at
    runtime. The second argument, **output_parser**, takes an **AnswerParser** object.
    This object instructs the **PromptNode** object to store the results in the **answers**
    element. After defining the prompt, we initialize a **PromptNode** object with
    a model and the prompt template. We use the **google/flan-t5-large** model as
    the answer generator. This model is based on the Google T5 language model and
    has been fine-tuned (**flan** stands for **fine-tuning language models**). Fine-tuning
    a language model with an instruction dataset allows the language model to perform
    tasks following simple instructions and generating text based on the given context
    and instruction. One of the fine-tuning steps as part of this model training was
    to operate on human written instructions as tasks. This allowed the model to perform
    different downstream tasks on instructions alone and reduced the need for any
    few-shot examples to be trained on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now create a pipeline and add the **retriever** and **prompt_node** components
    that we initialized in the previous steps. The **retriever** component operates
    on the query supplied by the user and generates a set of results. These results
    are passed to the prompt node, which uses the configured **flan-t5-model** to
    generate the answer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the pipeline is set up, we use it to answer questions on the content based
    on the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please refer to the prompt engineering guide on Haystack on how to generate
    prompts for your use cases ([https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines](https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines)).
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing text using pre-trained models based on Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now explore techniques for performing text summarization. Generating
    a summary for a long passage of text allows NLP practitioners to extract the relevant
    information for their use cases and use these summaries for other downstream tasks.
    As part of the summarization, we will explore recipes that use Transformer models
    to generate the summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first recipe for summarization will use the Google `9.5_summarization.ipynb`
    notebook from the code site if you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As part of this step, we initialize the input passage that we need to summarize
    along with the pipeline. We also calculate the length of the passage since this
    will be used as an argument to be passed to the pipeline during the task execution
    in the next step. Since we have defined the task as **summarization**, the object
    returned by the pipeline module is of the **SummarizationPipeline** type. We also
    pass **t5-large** as the model parameter for the pipeline. This model is based
    on the **Encoder-Decoder** Transformer model and acts as a pure sequence-to-sequence
    model. That means the input and output to/from the model are text sequences. This
    model was pre-trained using the denoising objective of finding masked words in
    a sentence followed by fine-tuning on specific downstream tasks such as summarization,
    textual entailment, language translation, and so on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now use the **pipeline_instance** initialized in the previous step and pass
    the text passage to it to perform the **summarization** step. A string array can
    be passed as well if multiple sequences are to be summarized. We pass **max_length=512**
    as the second argument. The T5 model is memory-intensive and the compute requirements
    grow quadratically with the increase in the input text length. This step might
    take a few minutes to complete based on the compute capability of the environment
    you are executing this on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the **summarization** step is complete, we extract the result from the
    output and print it. The pipeline returns a list of dictionaries. Each list item
    corresponds to the input argument. In this case, since we passed only one string
    as input, the first item in the list is the output dictionary that contains our
    summary. The summary can be retrieved by indexing the dictionary on the **summary_text**
    element:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have seen how we can generate a summary using the T5 model, we can
    use the same code framework and tweak it slightly to use other models to generate
    summaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines would be common for the other summarization recipes that
    we are using. We added an extra variable named `device`, which we will use in
    our pipelines. We set this variable to the value of the device that we will use
    to generate the summary. If a GPU is present and configured in the system, it
    will be used; otherwise, the summarization will be performed using the CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following example, we use the **BART** model ([https://huggingface.co/facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn))
    from Facebook. This model was trained using a denoising objective. A function
    adds some random piece of text to an input sequence. The model is trained based
    on the objective to denoise or remove the noisy text from the input sequence.
    The model was further fine-tuned using the **CNN DailyMail** dataset ([https://huggingface.co/datasets/abisee/cnn_dailymail](https://huggingface.co/datasets/abisee/cnn_dailymail))
    for summarization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As we observe from the generated summary, it is verbose and extractive in nature.
    Let’s try generating a summary with another model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we use the **PEGASUS** model from Google ([https://huggingface.co/google/pegasus-large](https://huggingface.co/google/pegasus-large))
    for summarization. This model is a Transformer-based Encoder-Decoder model that
    was pre-trained with a large news and web page corpus – C4 ([https://huggingface.co/datasets/allenai/c4](https://huggingface.co/datasets/allenai/c4))
    and the HugeNews dataset – on a training objective of detecting important sentences.
    HugeNews is a dataset of 1.5 billion articles curated from news and news-like
    websites from 2013–2019\. This model was further fine-tuned for summarization
    using the subset of the same dataset. The training objective for the fine-tuning
    involved masking important sentences and making the model generate a summary that
    has these important sentences. This model generates abstract summaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As we observe from the generated summary, it is concise and abstractive.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As many new and improved models for summarization are always in the works, we
    recommend that you refer to the models on the Hugging Face site ([https://huggingface.co/models?pipeline_tag=summarization](https://huggingface.co/models?pipeline_tag=summarization))
    and make the respective choice based on your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting sentence entailment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will explore techniques to detect `premise`, which sets
    up a context. The second sentence is the `hypothesis`, which serves as the claim.
    Textual entailment identifies the contextual relationship between the `premise`
    and the `hypothesis`. These relationships can be of three types, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entailment** – The hypothesis supports the premise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contradiction** – The hypothesis contradicts the premise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neutral** – The hypothesis neither supports nor contradicts the premise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the Transformers library to detect text entailment. You can use
    the `9.6_textual_entailment.ipynb` notebook from the code site if you need to
    work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will initialize different sets of sentences that are related
    through each of the previously defined relationships and explore methods to detect
    these relationships. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the device, the tokenizer, and the model. In this case, we are using
    Google’s **t5-small** model. We set the **legacy** flag to **False** since we
    don’t need to use the legacy behavior of the model. We set the **device** value
    based on whatever device we have available in our execution environment. Similarly,
    for the model, we set the **model** name and **device** parameter similar to the
    tokenizer. We set the **return_dict** parameter as **True** so that we get the
    model results as a dictionary instead of a tuple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize the **premise** and **hypothesis** sentences. In this case, the
    hypothesis supports the premise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we call the tokenizer with the **mnli premise** and **hypothesis**
    values. This is a simple text concatenation step to set up the tokenizer for the
    **entailment** task. We read the **input_ids** property to get the token identifiers
    for the concatenated string. Once we have the token IDs, we use the model to generate
    the entailment prediction. This returns a list of tensors with the predictions,
    which we use in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we call the **decode** method of the tokenizer and pass it the
    first tensor (or vector) of the tensors that were returned by the **generate**
    call of the model. We also instruct the tokenizer to skip the special tokens that
    are used by the tokenizer internally. The tokenizer generates the string label
    from the vector that is passed in. We print the prediction result. In this case,
    the generated prediction by the model is **entailment**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have shown an example in the case of entailment with a single sentence,
    the same framework can be used to process a batch of sentences to generate entailment
    predictions. We will tailor *steps 3*,*4*, and *5* from the previous recipe for
    this example. We initialize an array of two sentences for both `premise` and `hypothesis`,
    respectively. Both the `premise` sentences are the same, while the `hypothesis`
    sentences are of `entailment` and `contradiction`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have an array of sentences for both `premises` and `hypothesis`, we
    create an array of concatenated inputs that combine the `tokenizer` instruction.
    This array is used to pass to the tokenizer and we use the token IDs returned
    by `tokenizer` in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We now generate the predictions using the same methodology that we used earlier.
    However, in this step, we generate the inference label by iterating through the
    tensors returned by the model output and printing the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Enhancing explainability via a classifier-invariant approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will explore recipes that will allow us to understand the decisions
    made by text classifiers. We will explore techniques that will use a sentiment
    classifier and NLP explainability libraries to interpret the classification labels
    and their relation to the input text, especially in the aspect of individual words
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: Though a lot of the current models for text classification in NLP are based
    on deep neural networks, it is difficult to interpret the results of classification
    via the network weights or parameters. It is equally challenging to map these
    network parameters to the individual components or words in the input. However,
    there are still a few techniques in the NLP space to help us understand the decisions
    made by the classifier. We will explore these techniques in the current recipe
    and the following one.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to interpret the feature importance of each
    word in a text passage while being invariant of the classifier model. This technique
    can be used for any text classifier as we treat the classifier as a black box
    and use the results of the predictions to infer the results from an explainability
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `lime` library for explainability. You can use the `9.7_explanability_via_classifier.ipynb`
    notebook from the code site if you want to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will repurpose a classifier that we built in the *Transformers*
    chapter and use it to generate a sentiment prediction. We will call this classifier
    multiple times with a perturbation of the input to understand the contribution
    of each word to the sentiment. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, we initialize the device and the pipeline for sentiment classification.
    For more details on this step, please refer to chapter-8\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a sample text passage along with setting the print
    options. Setting the print options allows us to print the outputs in the later
    steps in an easy-to-read format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create a wrapper function for sentiment classification. This
    method is used by the explainer to invoke the classification pipeline multiple
    times to gauge the contribution of each word in the passage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we instantiate the **LimeTextExplainer** class and call the **explain_instance**
    method for it. This method takes the sample text along with the **classifier**
    wrapper function. The wrapper function passed to this method expects it to take
    a single instance of a string and return the probabilities of the target classes.
    In this case, our wrapper function accepts a simple string and returns the probabilities
    for the **NEGATIVE** and **POSITIVE** classes, respectively, and in that order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we print the class probabilities for the sample text. As we observe,
    the sample text has been assigned a **POSITIVE** sentiment as per the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we print the explanations. As we observe from the probabilities
    for each word, the words **entertaining** and **liked** contributed the most to
    the **POSITIVE** class. There are some words that contribute negatively to the
    positive sentiment, but overall, the sentence is classified as positive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s initialize another text to something with a negative sentiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the class probability as predicted by the classifier for the new text and
    print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the **explainer** instance to evaluate the text and print the contribution
    of each word to the negative sentiment. We observe that the words **boring** and
    **slow** contributed most to the negative sentiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have seen how to interpret the word contributions for the sentiment
    classification, we want to further improve our recipe to provide a visual representation
    of the explainability:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing on from *step 5* in the recipe, we can also print the explanations
    using **pyplot**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/B18411_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Probability contribution of each word in the sentence to the final
    class
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also highlight the exact words in the text. The contribution of each
    word is also highlighted using a light or dark shade of the assigned class, which,
    in this case, is orange. The words with the blue highlights are the ones that
    contribute against the **POSITIVE** class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/B18411_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – The highlighted class association for each word
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing explainability via text generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to understand the inference emitted by the
    classifier using text generation. We will use the same classifier that we used
    in the *Explainability via a classifier invariant approach* recipe. To better
    understand the behavior of the classifier in a random setting, we will replace
    the words in the input sentence with different tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need to install a `spacy` artifact for this recipe. Please use the following
    command in your environment before starting this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have installed `spacy`, we will need to download the `en_core_web_sm`
    pipeline using the following step beforehand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: You can use the `9.8_explanability_via_generation.ipynb` notebook from the code
    site if you need to work from an existing notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the **spacy** pipeline with the **en_core_web_sm**
    model. This pipeline contains the components for **tok2vec**, **tagger**, **parser**,
    **ner**, **lemmatizer**, and so on, and is optimized for the CPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize the device and our classifier. We use the same
    sentence classifier that we used in the *Explainability via a classifier invariant
    approach* recipe. The idea is to understand the same classifier and observe how
    its classification behaves for different inputs, as generated by the **anchor**
    explainability library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define a function that takes a list of sentences and emits
    a list of **POSITIVE** or **NEGATIVE** labels for them. This method internally
    calls the classifier that was initialized in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we initialize a passage of text. We use that text sentence to
    predict its class probability by using the **predict_prob** method and print the
    prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, we call the `explain_instance` method for the explainer instance.
    We pass it the input sentence, the `predict_prob` method, and a `threshold`. The
    explainer instance uses the `predict_prob` method to invoke the classifier for
    different variations of the input sentence to explain what words contribute the
    most. It also identifies what class labels are emitted when some words in the
    input sentence are replaced by the `UNK` token. The `threshold` parameter defines
    the minimum probability for a given class under which all the generated samples
    are to be ignored. This effectively means that all the sentences generated by
    the explainer will have the probability greater than the threshold, for a given
    class.exp = explainer.explain_instance(text, predict_prob, threshold=0.95)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We print the **anchor** words that contribute the most to the **POSITIVE**
    label in this case. We also print the precision as measured by the explainer.
    We observe that it identifies the words **good**, **a**, and **is** as contributing
    the most to the **POSITIVE** classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We print some of the possible sentences that the explainer believes would result
    in a **POSITIVE** classification. The explainer perturbs the input sentence by
    replacing one or more of the words with the **UNK** token and invokes the classifier
    method on the perturbed sentence. There are some interesting observations on how
    the classifier behaves. For example, the sentence **The UNK UNK is a good story
    UNK** has been labeled as **POSITIVE**. This indicates that the title of the story
    is irrelevant to the classification. Another interesting example is the sentence
    **The UNK mermaid is a good UNK UNK**. In this sentence, we observe that the classifier
    is invariant to the object in context, which, in this case, is a story:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the previous step, we now ask the explainer to print sentences that
    would result in a **NEGATIVE** classification. In this particular case, the explainer
    was unable to generate any negative examples by just replacing the words. The
    explainer is unable to generate any **NEGATIVE** examples. This happens because
    the explainer can only use the **UNK** token to perturb the input sentence. And
    since the **UNK** token is not associated with any **POSITIVE** or **NEGATIVE**
    sentiment, using just that token does not provide a way to affect the classifier
    to generate a **NEGATIVE** classification. We get no output from this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So far, we used the **UNK** token to vary or perturb the input to the classifier.
    The presence of the **UNK** token in the text makes it unnatural. To understand
    the classifier better, it would be useful to enumerate natural sentences and understand
    how those affect the classification. We will use **BERT** to perturb the input
    and get the explainer to generate natural sentences. This will help us better
    understand how the results differ in the context of sentences that are natural:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now print some sentences for which the classifier thinks the label would
    be **POSITIVE**. In this instance, we observe that the explainer generates sentences
    that are natural. For example, the generated sentence **my little mermaid tells
    a good story** replaced the word **the** in the original sentence with **my**.
    This word was generated via BERT. BERT uses the encoder part of the Transformer
    architecture and has been trained to predict missing words in a sentence by masking
    them. The explainer in this case masks the individual words in the input sentence
    and uses BERT to generate the replacement word. Since the underlying model to
    generate text is a probabilistic model, your output might differ from the following
    and also vary between runs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We now print some sentences for which the classifier thinks the label would
    be **NEGATIVE**. Though not all the sentences appear to have a **NEGATIVE** sentiment,
    there are quite a few of them with such a sentiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
