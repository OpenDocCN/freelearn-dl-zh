- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Your Neo4j Graph with Movies Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how knowledge graphs have emerged as a
    transformative tool, offering a structured way to connect diverse data points,
    enabling smarter search, recommendations, and inference capabilities across a
    wide range of domains.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs excel at capturing complex relationships between entities,
    making them indispensable for applications that require deep contextual understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Neo4j, with its state-of-the-art graph database technology, stands out as a
    leading platform for building and managing knowledge graphs. As we saw in the
    previous chapter, unlike traditional relational databases, Neo4j is designed to
    handle highly connected data with ease, allowing for more intuitive querying and
    faster retrieval of insights. This makes it an ideal choice for developers and
    data scientists looking to transform raw, unstructured data into meaningful insights
    that can drive AI-powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Design considerations for a Neo4j graph for an efficient search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing a movies dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building your movie knowledge graph with code examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beyond the basics: advanced Cypher techniques for complex graph structures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To successfully work through the exercises in this chapter, you will need the
    following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Neo4j AuraDB**: You can use Neo4j AuraDB, the cloud version of Neo4j, available
    at [https://neo4j.com/aura](https://neo4j.com/aura).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cypher query language**: Familiarity with the Cypher query language is essential,
    as we will be using Cypher extensively to create and query the graph. You can
    find out more about Cypher syntax in the Cypher query language documentation:
    [https://neo4j.com/docs/cypher/](https://neo4j.com/docs/cypher/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python**: You will need Python 3.x installed on your system. Python is used
    for scripting and interacting with the Neo4j database. You can download Python
    from the official Python website: [https://www.python.org/downloads/](https://www.python.org/downloads/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python libraries**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neo4j Driver for Python**: Install the Neo4j Python driver to connect to
    the Neo4j database using Python. You can install it via `pip`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '**pandas**: This library will be used for data manipulation and analysis. Install
    it via `pip`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '**Integrated Development Environment (IDE)**: An IDE such as PyCharm, VS Code,
    or Jupyter Notebook is recommended for writing and managing your Python code efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Git and GitHub**: Basic knowledge of Git is required for version control.
    You will also need a GitHub account to access the code repository for this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Movies dataset**: **The Movie Database** (**TMDb**) is required, available
    on Kaggle: [https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This dataset is a derivative of the **Movie Lens Datasets** (F. Maxwell Harper
    and Joseph A. Konstan. 2015\. The MovieLens Datasets: History and Context. ACM
    Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19.[https://doi.org/10.1145/2827872](https://doi.org/10.1145/2827872)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the data files, such as `credits.csv` and `ratings.csv`, may not be
    available on GitHub due to storage constraints. However, you can access all the
    raw data files from a GCS bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the code for this chapter is available in the following GitHub repository:
    [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch4](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch4).'
  prefs: []
  type: TYPE_NORMAL
- en: The folder includes all the necessary files and scripts to help you build your
    Neo4j graph using the movies dataset and Cypher code.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to clone or download the repository to follow along with the code
    examples provided in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The GitHub repository contains the path to GCS to access the raw data files.
  prefs: []
  type: TYPE_NORMAL
- en: Design considerations for a Neo4j graph for an efficient search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A well-designed Neo4j graph ensures that your search functionality is not only
    accurate but also efficient, enabling the quick retrieval of relevant information.
    The way data is organized in a graph directly impacts the performance and relevance
    of search results, making it crucial to understand the principles of effective
    graph modeling.
  prefs: []
  type: TYPE_NORMAL
- en: This section will delve into the importance of structuring your Neo4j graph
    correctly, how it influences the search process, and the key considerations you
    need to keep in mind while designing your graph model.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations while defining node and relationship types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from [*Chapter 3*](Preface.xhtml#_idTextAnchor012) that the foundation
    of any Neo4j graph is built upon **nodes** and **relationships**. Nodes represent
    entities, such as movies or people (e.g., actors or directors), while relationships
    define how these entities are connected. The types of nodes and relationships
    you choose play a crucial role in determining the effectiveness of your search
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: In a movies dataset, nodes could traditionally represent distinct entities such
    as `Movies`, `Actors`, `Directors`, and `Genres`. Relationships would then define
    how these nodes interact, such as `ACTED_IN`, `DIRECTED`, or `BELONGS_TO`. However,
    there is an alternative and often more efficient approach—consolidating similar
    entities under a single node type.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of creating separate nodes for `Actors` and `Directors`, you can create
    a single `Person` node. The characteristic of each `Person` node—whether they
    are an actor, a director, or both—is then defined by the type of relationship
    it has with the `Movie` node. For example, a `Person` node connected to a `Movie`
    node by an `ACTED_IN` relationship signifies that the person is an actor in that
    movie. Similarly, a `DIRECTED` relationship indicates that the person directed
    the movie. We will be creating the complete graph in upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, let’s talk about why this approach is better. As we demonstrated
    in [*Chapter 3*](Preface.xhtml#_idTextAnchor012), this approach results in the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplified data model**: By using a single `Person` node to represent both
    actors and directors, your data model becomes more streamlined. This reduces the
    complexity of the graph and makes it easier to understand and manage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced query performance**: With fewer node types, the graph database can
    more efficiently traverse relationships during queries. This is because the database
    engine has fewer distinct entities to differentiate between, leading to faster
    query execution times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced redundancy**: A unified `Person` node eliminates the need to duplicate
    information. In cases where a person is both an actor and a director, you avoid
    creating two separate nodes with overlapping data, thus minimizing redundancy,
    and saving storage space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible relationship definitions**: This approach allows for more flexible
    and granular relationship definitions. If a person has multiple roles in different
    movies (e.g., acting in one and directing another), the relationships can clearly
    distinguish these roles without needing to create multiple nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easier maintenance and scalability**: As your dataset grows, maintaining
    a simpler node structure becomes increasingly important. Adding new roles or relationships
    becomes more straightforward when you are working with a unified node type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By carefully selecting and defining these types and relationships, you create
    a graph structure that mirrors real-world connections. This makes your search
    queries more intuitive, the results more meaningful, and the overall system more
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Applying indexing and constraints on search performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As your Neo4j graph grows, the importance of **indexing** and applying **constraints**
    becomes paramount. **Indexes** allow Neo4j to quickly locate the starting points
    for queries, drastically improving search performance, especially in large datasets.
    Constraints, however, ensure data integrity by preventing the creation of duplicate
    nodes or invalid relationships.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of our movies dataset, where we use a unified `Person` node for
    both actors and directors, indexing becomes even more crucial. You might **index**
    nodes based on properties such as `person_name` or `role`, ensuring that searches
    for specific people or their roles in movies return results swiftly. For example,
    you could index the role property on the relationships (e.g., `ACTED_IN` or `DIRECTED`)
    to quickly filter people by their involvement in a particular movie.
  prefs: []
  type: TYPE_NORMAL
- en: Constraints are also essential to maintaining the integrity of your graph. Let’s
    look at some of these constraints. The constraints should be carefully designed
    based on the nature of your dataset and application requirements—they are not
    a one-size-fits-all solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some example statements that demonstrate how to create constraints
    and indexes tailored for a movie dataset. These examples include common scenarios
    such as ensuring the uniqueness of person identifiers and optimizing search performance
    across node and relationship properties. Depending on your specific use case and
    data quality, you can adapt these patterns to enforce data integrity and improve
    query speed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unique constraint on `person_name` (for a simplified use case). In many cases—such
    as our movie dataset, where we assume each person has a unique name—you might
    enforce a unique constraint on the `person_name` property to ensure that each
    individual is represented by a single node, even if they take on multiple roles
    (e.g., actor and director) across different movies. Here is how you can do this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This helps prevent the accidental creation of duplicate nodes and keeps your
    graph clean and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Unique constraint on a more reliable ID (e.g., `person_id`). The uniqueness
    constraint in the previous scenarios is based on assumptions about your data.
    In real-world scenarios, it is common to encounter different individuals with
    the same name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In such cases, you should use a more reliable identifier, such as a `person_id`
    value from an external source (e.g., **Internet Movie Database** (**IMDb**) or
    **TMDb**) to enforce uniqueness. The following Cypher code shows how to achieve
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Index on `person_name` (for faster lookup if uniqueness is not enforced). If
    you’re not enforcing uniqueness but still frequently search for people by name,
    an index on the `person_name` property can significantly improve query performance.
    This allows Neo4j to quickly locate `Person` nodes based on their names:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Index on the `title` property of `Movie`. Movies are often queried by title—especially
    in recommendation systems or search functionalities. Indexing the `title` property
    ensures quick lookups when users search for specific movies:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Index on the `role` property in the `ACTED_IN` relationship. If your application
    requires filtering actors by their specific roles in movies (e.g., lead or cameo),
    indexing the `role` property on the `ACTED_IN` relationship helps speed up those
    queries by avoiding full scans of all relationships:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Note**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Neo4j only supports relationship property indexes in version `5.x` and above.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Properly implemented indexing and constraints make your graph more resilient
    and your search processes faster and more reliable. This not only enhances the
    user experience but also reduces the computational load on your system, allowing
    for more scalable solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore how to harness the power of open data by
    utilizing a movies dataset to build your graph.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing a movies dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will focus on utilizing **TMDb**, a comprehensive collection
    of metadata made available on Kaggle: [https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/).
    This dataset includes a wide range of information about movies, such as titles,
    genres, cast, crew, release dates, and ratings. With over 45,000 movies and detailed
    information about the people involved in their creation, this dataset provides
    a robust foundation for building a Neo4j graph that captures the complex relationships
    within the film industry.'
  prefs: []
  type: TYPE_NORMAL
- en: You will use this dataset to model the data as a knowledge graph, learning about
    data integration in a practical context. You will learn how to source, prepare,
    and import this data into Neo4j.
  prefs: []
  type: TYPE_NORMAL
- en: When working with large datasets such as TMDb, it is crucial to ensure that
    the data is clean, consistent, and properly structured before integrating it into
    your Neo4j graph. Raw data, while rich in information, often contains inconsistencies,
    redundancies, and complex structures that can hinder the performance and accuracy
    of your knowledge graph. This is where data normalization and cleaning come into
    play.
  prefs: []
  type: TYPE_NORMAL
- en: Why normalize and clean data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Maintaining a clean and normalized dataset is crucial when building a Neo4j
    graph, as it directly impacts the quality and performance of your application.
    By normalizing and cleaning your data, you ensure consistency, improve efficiency,
    and create a scalable foundation for analysis. Here is why each of these steps
    matters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: Raw data can come with variations in how similar information
    is recorded. For example, movie genres might be listed in different formats or
    contain duplicates. Normalizing data ensures that similar data points are recorded
    in a consistent format, making it easier to query and analyze. However, tackling
    these issues in real-world datasets can be challenging. Neo4j helps address problems
    such as entity linkage and deduplication through powerful features such as Cypher
    pattern matching, APOC procedures for merging nodes and cleaning up duplicates,
    and the Graph Data Science library, which includes node similarity algorithms
    to identify and consolidate related entities. These capabilities enable you to
    build a clean, reliable graph that reflects the true structure of your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: Normalizing data reduces redundancy, which can improve the
    efficiency of your Neo4j graph. By organizing data into a standardized format,
    you minimize the storage requirements and optimize the performance of your queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy**: Cleaning data involves removing or correcting inaccurate records.
    This step is essential to ensure that the insights derived from your graph are
    based on accurate and reliable data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: A clean and normalized dataset is easier to scale. As your
    dataset grows, maintaining a standardized structure ensures that the graph remains
    manageable and performs well under increasing loads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us move on to cleaning and normalizing CSV files next.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and normalizing the CSV files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will clean and normalize each CSV file included in TMDb. The available
    CSV files in our dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`credits.csv`: This file contains detailed information about the cast and crew
    for each movie in our dataset, presented as a stringified JSON object. For our
    purposes, we will focus specifically on extracting the relevant details related
    to characters, actors, directors, and producers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`keywords.csv`: This file contains the movie plot keywords for each movie in
    the dataset. The keywords are essential for categorizing and identifying thematic
    elements within the movies, which can be used for various purposes, such as search,
    recommendation, and content analysis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`links.csv`: This file contains essential metadata that links each movie in
    the full **MovieLens dataset** to its corresponding entries in both TMDb and IMDB.
    This file serves as a crucial bridge for connecting the MovieLens dataset with
    external movie databases, enabling enriched data integration and further analysis.
    However, for this use case, we skipped processing the `links.csv` file, as it
    is not essential to our current analysis. Our focus will remain on other CSV files
    more directly relevant to our project’s objectives. The data contained in `links.csv`
    can still be useful for future projects that require integration with external
    databases, but it will not be utilized in this instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`links_small.csv`: This file contains the TMDb and IMDb IDs for a small subset
    of 9,000 movies from the full MovieLens dataset. While this file provides a streamlined
    version of the links for a smaller selection of movies, we will not be using this
    file, as we are already utilizing the full dataset from Kaggle, which includes
    all available movies. This file is typically useful for scenarios where a more
    manageable, smaller dataset is needed, but for our purposes, the full data set
    is preferred for comprehensive analysis and integration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movies_metadata.csv`: This file is a comprehensive dataset containing detailed
    information on 45,000 movies featured in the full MovieLens dataset. This file
    includes various features such as posters, backdrops, budgets, revenue, release
    dates, languages, production countries, and companies, among others. To efficiently
    organize and analyze this data, we will normalize the `movies_metadata.csv` file
    into multiple CSV files, each representing a relevant node in our dataset. These
    nodes include genres, production companies, production countries, and spoken languages.
    By breaking down the data into these separate files, we can more easily manage
    and utilize the rich information contained within this dataset. Let’s see how.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Begin with necessary imports.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Extract and normalize genres, production companies, countries, and spoken languages.
    We will demonstrate this step for genres and production companies. The rest of
    the code is available on [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch4](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch4)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Apply the extraction functions.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Normalize the exploded data. Let’s do this for genres.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Next up, extract the collection name.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`ratings.csv`: This file is the full MovieLens dataset, consisting of 26 million
    ratings and 750,000 tag applications from 270,000 users on all 45,000 movies in
    this dataset. This comprehensive dataset provides detailed user interaction data,
    which we will use directly without the need for normalization. However, for this
    use case, we have decided to skip processing the `ratings.csv` file. While it
    provides extensive user interaction data, it is not essential to our current analysis
    and objectives. We are focusing on other CSV files that are more directly relevant
    to our project. The data in `ratings.csv` can still be valuable for future projects
    that require a deep dive into user ratings and interactions, but it will not be
    utilized in this instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ratings_small.csv`: This file is a smaller subset of the `ratings.csv` file,
    containing 100,000 ratings from 700 users on 9,000 movies. We will be using `ratings_small.csv`
    instead of focusing on the full dataset provided in `ratings.csv`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through this process, we have learned how to transform raw, unstructured data
    into clean, normalized datasets that are now primed for integration into your
    Neo4j graph. This preparation paves the way for constructing a robust, efficient,
    and effective AI-powered search and recommendation system. In the next section,
    we will take these normalized CSV files and use Cypher code to build a knowledge
    graph, unlocking the full potential of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Building your movie knowledge graph with code examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will import your normalized datasets into Neo4j and transform
    them into a fully functional knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your AuraDB free instance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start building your knowledge graph with Neo4j, you will first need to set
    up an AuraDB Free instance. AuraDB Free is a cloud-hosted Neo4j database that
    allows you to quickly get started without worrying about local installations or
    infrastructure management.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create your instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://console.neo4j.io](https://console.neo4j.io).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in with your Google account or with email.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create Free Instance**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the instance is being provisioned, a pop-up window will appear showing
    the connection credentials for your database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure to download and securely save the following details from the popup—these
    are essential for connecting your application to Neo4j:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With your AuraDB Free instance set up, you are now ready to import your normalized
    datasets and start building your knowledge graph using Cypher code. In the following
    section, we will guide you through importing data and constructing relationships
    within your graph.
  prefs: []
  type: TYPE_NORMAL
- en: Importing your data into AuraDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that your AuraDB Free instance is up and running, it is time to import
    your normalized datasets and build your knowledge graph. In this section, we will
    walk through preparing your CSV files, setting up indexes and constraints, importing
    data, and creating relationships—all through a Python script:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your CSV files for import.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that the CSV files you generated (e.g., `normalized_movies.csv`, `normalized_genres.csv`,
    etc.) are ready for import. These files should be clean, well structured, and
    hosted at accessible URLs. In this case, the `graph_build.py` script fetches files
    from public cloud storage (for example, [https://storage.googleapis.com/movies-packt/normalized_movies.csv](https://storage.googleapis.com/movies-packt/normalized_movies.csv)),
    so you do not need to upload them manually anywhere.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add indexes and constraints to optimize graph query retrieval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before loading data, it is critical to create unique constraints and indexes
    to ensure integrity and optimize query performance. The script includes Cypher
    commands to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure uniqueness on IDs such as `tmdbId`, `movieId`, and `company_id`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create indexes on properties such as `actor_id`, `crew_id`, and `user_id`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is how you can create indexes and constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Import data and create nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After adding constraints and indexes, the script loads the nodes from their
    respective CSVs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`load_movies()` adds all movie metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_genres()`, `load_production_companies()`, `load_countries()`, and others
    create related nodes, such as `Genre`, `ProductionCompany`, `Country`, and `SpokenLanguage`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Person-related data is added using `load_person_actors()` and `load_person_crew()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional properties are added via `load_links()`, `load_keywords()`, and `load_ratings()`
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Create relationships.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As each loader function runs, it not only creates nodes but also establishes
    meaningful relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HAS_GENRE` between `Movie` and `Genre`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PRODUCED_BY` between `Movie` and `ProductionCompany`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HAS_LANGUAGE` between `Movie and SpokenLanguage`, `PRODUCED_IN` between `Movie`
    and `Country`, `ACTED_IN`, `DIRECTED`, `PRODUCED` between `Movie` and `Person`,
    and `RATED` between `Movie` and `User`, among others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the full script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before running the script, ensure you have the Neo4j Python driver installed.
    You can install it using `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the entire graph-building process, simply execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This script performs the following, in order:'
  prefs: []
  type: TYPE_NORMAL
- en: Connects to your AuraDB instance using credentials from the `.env` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleans up the database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adds indexes and constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads all node data and relationships in bulk using hosted CSVs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please refer to the complete script available here: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch4/graph_build.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch4/graph_build.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once complete, verify your import using Neo4j Browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 4.1* illustrates a connected movie graph with over 90K nodes and 320K+
    relationships. Nodes such as `Movie`, `Genre`, `Person`, and `ProductionCompany`
    are represented with distinct colors, while relationships such as `ACTED_IN`,
    `HAS_GENRE`, and `PRODUCED_BY` showcase the web of interconnected metadata.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 — Neo4j graph of movies dataset](img/B31107_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 — Neo4j graph of movies dataset
  prefs: []
  type: TYPE_NORMAL
- en: With your data successfully imported and your knowledge graph fully constructed
    using Python and Cypher, you are now ready to dive into building a GenAI-powered
    search application in the next chapter. In the following section, we will dive
    into advanced Cypher techniques that empower you to handle intricate relationships
    and derive deeper insights from your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the basics: advanced Cypher techniques for complex graph structures'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As your knowledge graph grows in size and complexity, so do the demands on your
    querying and data management capabilities. Cypher, Neo4j’s powerful query language,
    offers a range of advanced features designed to handle complex graph structures
    and enable more sophisticated data analysis. In this section, we will explore
    these advanced Cypher techniques, including **path patterns**, **variable-length
    relationships**, subqueries, and graph algorithms. Understanding these techniques,
    will help you efficiently manage intricate relationships, perform deeper analyses,
    and unlock the full potential of your knowledge graph for advanced use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us explore these key advanced Cypher techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Variable-length relationships**:  Variable-length relationships in Cypher
    allow you to match paths of varying lengths between nodes. This is particularly
    useful when exploring hierarchical structures or networks with multiple degrees
    of separation. An example is finding all movies connected to a specific actor
    within three degrees of separation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `*1..3` specifies that the relationship path can be between 1 and 3 steps
    long.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use cases**: Variable-length relationships are ideal for scenarios such as
    social network analysis, where you want to find all people within a certain degree
    of connection, or in hierarchical datasets where you want to explore parent-child
    relationships across multiple levels.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pattern matching with path patterns**: You can create **named path patterns**
    as well as chain the paths in Neo4j.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defining path patterns**: Cypher allows you to define named path patterns
    that can be reused throughout your queries. This makes your queries more readable
    and allows you to encapsulate complex relationships in a single pattern. Take
    the following example:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Here, `path` is a named path pattern that can be reused in subsequent operations
    or subqueries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Chaining path patterns**: Cypher allows you to combine multiple path patterns
    to perform complex traversals within the graph. This is especially useful when
    trying to uncover indirect relationships or discover multiple paths that satisfy
    specific criteria.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An example is exploring collaborations in the movies dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s say we want to find movies where an actor has worked with a director
    they’ve previously collaborated with, possibly through another movie. This involves
    chaining paths from an actor to a movie, then to a director, and seeing whether
    there’s another movie connecting the same actor-director pair:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This kind of pattern chaining is extremely helpful in identifying professional
    relationships, recurring collaborations, or analyzing indirect influence in networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Subqueries** and **procedural logic**: You can use subqueries and procedures
    to process complex queries. Here is how:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using subqueries for modular queries**: Subqueries in Cypher allow you to
    break down complex queries into modular, reusable components. This is particularly
    helpful when dealing with large graphs or when you need to perform multiple operations
    on the same dataset. Take the following example:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Here, the subquery retrieves all action movies, and the outer query matches
    these movies to their directors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Procedural logic with** **CALL**: The `CALL` clause in Cypher allows you
    to invoke procedures and use the results in further queries. This is essential
    for advanced data processing, such as running graph algorithms or invoking custom
    procedures.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve already applied this in our own implementation in the `graph_build.py`
    file, specifically in the `load_ratings()` function. Here, we use the `CALL {
    ... } IN TRANSACTIONS` pattern to efficiently load large datasets by processing
    them in chunks of 50,000 rows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This approach allows us to handle massive CSV imports while maintaining performance
    and transactional integrity—just one of the many powerful use cases for `CALL`
    in real-world graph applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Working with** **nested queries:** In complex graph structures, you might
    need to **combine results from multiple queries**. Cypher allows you to nest queries,
    passing results from one query into another, which is useful for filtering or
    refining results based on multiple criteria. Take the following example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the nested query refines the results by filtering movies based on revenue
    and then finding their associated genres.
  prefs: []
  type: TYPE_NORMAL
- en: These Cypher techniques empower you to tackle complex graph structures, enabling
    deeper insights and more sophisticated analyses. You can refer to [https://neo4j.com/docs/cypher-manual/current/appendix/tutorials/advanced-query-tuning/](https://neo4j.com/docs/cypher-manual/current/appendix/tutorials/advanced-query-tuning/)
    to explore some of these techniques further.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked on transforming raw, semi-structured data into clean,
    normalized datasets, ready for integration into our knowledge graph. We then explored
    the best practices in graph modeling, focusing on how to structure your nodes
    and relationships to enhance search efficiency and ensure your graph remains scalable
    and performant. Following this, we tackled other Cypher techniques, equipping
    you with the skills to handle variable-length relationships, pattern matching,
    subqueries, and graph algorithms. You are now well prepared to build a knowledge
    graph-driven search that can handle even the most intricate data relationships.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a step further by exploring how to integrate
    Haystack into Neo4j. This practical guide will show you how to build powerful
    search functionalities within your knowledge graph, allowing you to leverage the
    full potential of both Neo4j and Haystack for intelligent search solutions.
  prefs: []
  type: TYPE_NORMAL
