- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-Tuning Generative Models for Specific Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our narrative with StyleSprint, we described using a pre-trained generative
    AI model for creating engaging product descriptions. While this model showed adeptness
    in generating diverse content, StyleSprint’s evolving needs require a shift in
    focus. The new challenge is not just about producing content but also about engaging
    in specific, task-oriented interactions such as automatically answering customer’s
    specific questions about the products described.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we introduce the concept of fine-tuning, a vital step in adapting
    a pre-trained model to perform specific downstream tasks. For StyleSprint, this
    means transforming the model from a versatile content generator to a specialized
    tool capable of providing accurate and detailed responses to customer questions.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore and define a range of scalable fine-tuning techniques, comparing
    them with other approaches such as in-context learning. We will demonstrate advanced
    fine-tuning methods, including parameter-efficient fine-tuning and prompt tuning,
    to demonstrate how they can fine-tune a model’s abilities for specific tasks such
    as Q&A.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have trained a language model to answer
    questions and do so in a way that aligns with StyleSprint’s brand guidelines.
    However, before we explore the mechanics of fine-tuning and its importance in
    our application, we will revisit the history of fine-tuning in the context of
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation and relevance – an introduction to fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning is the process of leveraging a model pre-trained on a large dataset
    and continuing the training process on a smaller, task-specific dataset to improve
    its performance on that task. It may also involve additional training that adapts
    a model to the nuances of a new domain. The latter is known as domain adaptation,
    which we will cover in [*Chapter 6*](B21773_06.xhtml#_idTextAnchor211). The former
    is typically referred to as task-specific fine-tuning, and it can be performed
    to accomplish several tasks, including Q&A, summarization, classification, and
    many others. For this chapter, we will focus on task-specific fine-tuning to improve
    a general-purpose model’s performance when answering questions.
  prefs: []
  type: TYPE_NORMAL
- en: For StyleSprint, fine-tuning a model to handle a specific task such as answering
    customer inquiries about products introduces unique challenges. Unlike generating
    product descriptions, which primarily involves language generation using an out-of-the-box
    pre-trained model, answering customer questions requires the model to have an
    extensive understanding of product-specific data and should have a brand-aware
    voice. Specifically, the model must accurately interpret and respond to questions
    about product features, sizes, availability, user reviews, and many other details.
    It should also produce answers consistent with StyleSprint’s distinct brand tone.
    This task requires both generalized natural language proficiency (from pre-training)
    and robust knowledge of product metadata and customer feedback, accomplished through
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Models such as GPT initially learn to predict text through an unsupervised learning
    process that involves being trained on wide-ranging and vast datasets. This pre-training
    phase exposes the model to a diverse array of texts, enabling it to gain a broad
    understanding of language, including syntax, grammar, and context, without any
    specific task-oriented guidance. However, fine-tuning applies task-oriented, supervised
    learning to refine the model’s capabilities to accomplish the specified task –
    specifically, semi-supervised learning, which, as described by Radford et al.
    (2018), involves adapting the model to a specific supervised task by exposing
    it to a dataset comprising input sequences (`x1`, ..., `xm`) and corresponding
    labels (`y`).
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, we will detail the fine-tuning process, including how
    to selectively train the model on a curated dataset of product-related information
    and customer interactions, enabling it to respond with the informed, brand-aligned
    precision that customers expect. However, fine-tuning an LLM, which could have
    billions of parameters, would typically require an enormous number of resources
    and time. This is where advanced techniques such as **Parameter-Efficient Fine-Tuning**
    (**PEFT**) become particularly valuable in making fine-tuning accessible.
  prefs: []
  type: TYPE_NORMAL
- en: PEFT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional fine-tuning methods become increasingly impractical as the model
    size grows due to the immense computational resources and time required to train
    and update all model parameters. For most businesses, including larger organizations,
    a classical approach to fine-tuning is cost-prohibitive and, effectively, a non-starter.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, PEFT methods modify only a small subset of a model’s parameters,
    reducing the computational burden while still achieving state-of-the-art performance.
    This method is advantageous for adapting large models to specific tasks without
    extensive retraining.
  prefs: []
  type: TYPE_NORMAL
- en: One such PEFT method is the **Low-Rank Adaptation** (**LoRA**) methodology,
    developed by Hu et al. (2021).
  prefs: []
  type: TYPE_NORMAL
- en: LoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The LoRA method focuses on selectively fine-tuning specific components within
    the Transformer architecture to enhance efficiency and effectiveness in LLMS.
    LoRA targets the weight matrices found in the self-attention module of the Transformer,
    which, as discussed in [*Chapter 3*](B21773_03.xhtml#_idTextAnchor081), are key
    to its functionality and include four matrices: wq (query), wk (key), wv (value),
    and wo (output). Although these matrices can be divided into multiple heads in
    a multi-head attention setting – where each *head* represents one of several parallel
    attention mechanisms that process inputs independently – LoRA treats them as singular
    matrices, simplifying the adaptation process.'
  prefs: []
  type: TYPE_NORMAL
- en: LoRA’s approach involves adapting only the attention weights for downstream
    tasks, while the weights in the other component of the Transformer, the **feed-forward
    network** (**FFN**), are unchanged. This decision to focus exclusively on the
    attention weights and freeze the FFN is made for simplicity and parameter efficiency.
    By doing so, LoRA ensures a more manageable and resource-efficient fine-tuning
    process, avoiding the complexities and demands of retraining the entire network.
  prefs: []
  type: TYPE_NORMAL
- en: This selective fine-tuning strategy enables LoRA to effectively tailor the model
    for specific tasks while maintaining the overall structure and strengths of the
    pre-trained model. This makes LoRA a practical solution for adapting LLMs to new
    tasks with a reduced computational burden without requiring comprehensive parameter
    updates across the entire model (Liu et al., 2021).
  prefs: []
  type: TYPE_NORMAL
- en: Building upon the foundation of LoRA, **Adaptive Low-Rank Adaptation** (**AdaLoRA**),
    as introduced in a study by Liu et al. (2022), represents a further advancement
    in PEFT methods. The key difference between LoRA and AdaLoRA lies in (as the name
    suggests) its adaptiveness. While LoRA applies a consistent, low-rank approach
    to fine-tuning across the model, AdaLoRA tailors the updates to the needs of each
    layer, offering a more flexible and potentially more effective way to fine-tune
    large models for specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: AdaLoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AdaLoRA’s key innovation lies in its adaptive allocation of the **parameter
    budget** among the weight matrices of the pre-trained model. Many PEFT methods
    tend to distribute the parameter budget evenly across all pre-trained weight matrices,
    potentially neglecting the varying importance of different weight parameters.
    AdaLoRA overcomes this by assigning importance scores to these weight matrices
    and allocating the parameter budget accordingly. **Importance scores** in the
    context of AdaLoRA are metrics used to determine the significance (or importance)
    of different weight parameters in a model, guiding the allocation of the parameter
    budget more effectively during fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Parameter budget* refers to the predefined limit on the number of additional
    parameters that can be introduced during the fine-tuning of a pre-trained model.
    This budget is set to ensure that the model’s complexity does not increase significantly,
    which can lead to challenges such as overfitting, increased computational costs,
    and longer training times.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, AdaLoRA applies **singular value decomposition** (**SVD**) to
    efficiently organize the incremental updates made during the model’s fine-tuning
    process. SVD allows for the effective pruning of singular values associated with
    less critical updates, reducing the overall parameter budget required for fine-tuning.
    It is important to note that this method also avoids the need for computationally
    intensive exact computations, making the fine-tuning process more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: AdaLoRA has been empirically tested across various domains, including natural
    language processing, question-answering, and natural language generation. Extensive
    experiments have demonstrated its effectiveness in improving model performance,
    particularly in question-answering tasks. The adaptability and efficiency of AdaLoRA
    make it an ideal choice for applications requiring precise and efficient model
    adjustments for complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of StyleSprint, AdaLoRA presents an opportunity to fine-tune its
    language model for answering customer questions without the considerable overhead
    that would be incurred by traditional fine-tuning, which would require adjusting
    all of the model parameters. By adopting AdaLoRA, StyleSprint can efficiently
    adapt its model to handle nuanced customer inquiries by adjusting significantly
    fewer parameters. Specifically, AdaLoRA’s adaptive allocation of parameter budgets
    means that StyleSprint can optimize its model for the specific nuances of customer
    queries without using extensive computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have fine-tuned an LLM using AdaLoRA for
    our Q&A task. However, we should first decide whether fine-tuning is truly the
    right approach. Prompt-based LLMs offer a viable alternative known as in-context
    learning, where the model can learn from examples given in the prompt, meaning
    that the prompt would contain the customer’s question paired with a few key historical
    examples of how other questions were answered. The model can infer from the examples
    how to answer the question at hand in a way that is consistent with the examples.
    In the next section, we will explore the benefits and drawbacks of in-context
    learning to help us determine whether fine-tuning is the best approach to enable
    a model to answer very specific questions.
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In-context learning is a technique where the model generates responses based
    on a few examples provided in the input prompt. This method leverages the model’s
    pre-trained knowledge and the specific context or examples included in the prompt
    to perform tasks without the need for parameter updates or retraining. The general
    approach, detailed in *Language Models are Few-Shot Learners* by Brown et al.
    (2020), describes how the extensive pre-training of these models enables them
    to perform tasks and generate responses based on a limited set of examples paired
    with instructions embedded within prompts. Unlike traditional methods that require
    fine-tuning for each specific task, in-context learning allows the model to adapt
    and respond based on the additional context provided at inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Central to in-context learning is the concept of few-shot prompting, which
    is critical for enabling models to adapt to and perform tasks without additional
    training data, relying instead on their pre-trained knowledge and the context
    provided within input prompts. For context, we’ll describe how an LLM typically
    works, which is known as the zero-shot approach, and contrast it to in-context
    learning, which uses the few-shot approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`. The model calculates the likelihood of a potential output sequence, `y`,
    expressed as `P(y|x)`. This computation is performed without prior examples specific
    to the task, relying entirely on the model’s general pre-training. Meaning, the
    zero-shot approach has no specific context apart from its general knowledge. For
    example, if we were to ask *Are winter coats available in children’s sizes?*,
    the model could not provide a specific answer about StyleSprint’s inventory. It
    could only provide some generic answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x`) to form an extended input sequence. So, our question *Are winter coats
    available in children’s sizes?* might be paired with a few examples such as the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Do you sell anything in` `children’s sizes?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Any items for children are specifically listed on the “StyleSprint for` `Kids”
    page`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`What do you offer` `for kids?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StyleSprint offers a variety of children’s fashions` `on its “StyleSprint
    for` `Kids” page`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The LLM then computes the probability of generating a specific output sequence,
    `y`, given this extended input sequence, `x`. Mathematically, this can be conceptualized
    as the model estimating the joint probability distribution of `y` and `x` (where
    `x` includes both the prompt and the few-shot examples, as demonstrated previously).
    The model uses this joint probability distribution to generate a response consistent
    with the instructions paired with the examples given in the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, the model’s ability to adapt its output based on the given context,
    whether with zero examples or a few, demonstrates the flexibility and sophistication
    of its underlying architecture and training. However, the few-shot approach allows
    the LLM to learn from the very specific examples provided.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider how StyleSprint could apply in-context learning to answer customer
    queries. Performance using in-context learning (or the few-shot approach) consistently
    reflects significant gains over zero-shot behavior (Brown et al., 2020). We can
    expand our prior example to where a customer asks about the availability of a
    specific product. Again, the StyleSprint team could systematically append a few
    examples to each prompt as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the prompt: `Respond to the following {question} about` `product availability.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Do you carry black` `leather handbags?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Give me a moment while I retrieve information about that` `particular item.`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 2:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Do you have the silk scarves` `in blue?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Let me search our inventory for blue` `silk scarves.`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: StyleSprint can provide examples that effectively help the model understand
    the nature of the inquiry and generate a response that is informative and aligned
    with the company’s policies and product offerings. In this example, we see that
    the responses are intended to be paired with a search component. This is a common
    approach and can be accomplished using a technique called **Retrieval Augmented
    Generation** (**RAG**), which is a component that facilitates retrieval of real-time
    data to inform the generated response. Combining a few-shot in-context learning
    approach with RAG could ensure that the system provides a logical and specific
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning using a few-shot approach allows the model to rapidly adapt
    to various customer queries using a limited set of examples. When augmented with
    RAG, StyleSprint could potentially satisfy their use case and reduce the time
    and resources needed to fine-tune. However, this approach must be weighed against
    the depth of specialization and consistency of task-specific fine-tuning, which,
    as described, could also produce highly accurate answers that fit the brand tone.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will formulate metrics that help us draw a direct comparison
    to guide StyleSprint in making an informed decision that best suits its customer
    service objectives and operational framework.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning versus in-context learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how in-context learning could allow StyleSprint’s model to handle
    a diverse range of customer queries without requiring extensive retraining. Specifically,
    a few-shot approach combined with RAG could facilitate quick adaptation to new
    inquiries, as the model can generate responses based on a few examples. However,
    the effectiveness of in-context learning heavily relies on the quality and relevance
    of the examples provided in the prompts. Its success would also rely on the implementation
    of RAG. Moreover, without fine-tuning, responses may lack consistency or may not
    adhere as strictly to StyleSprint’s brand tone and customer service policies.
    Finally, depending entirely on a generative model without fine-tuning may inadvertently
    introduce bias, as discussed in [*Chapter 4*](B21773_04.xhtml#_idTextAnchor123).
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we have two very comparable and viable approaches. However, to
    make an informed decision, we should first perform a more in-depth comparison
    using quantitative methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'To impartially assess the efficacy of in-context learning compared to fine-tuning,
    we can measure the quality and consistency of the generated responses. We can
    accomplish this using established and reliable metrics to compare outcomes from
    each of the approaches. Like prior evaluations, we will want to apply quantitative
    and qualitative methods applied across the following key dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Alignment with human judgment**: We can again apply semantic similarity to
    provide a quantitative measure of how often the model’s responses are correct
    or relevant based on a reference answer written by a human.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StyleSprint’s brand communication experts can review a subset of the responses
    to provide a qualitative evaluation of the response accuracy and alignment with
    brand tone and voice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Consistency and stability**: It is important to measure the degree to which
    questions are answered consistently each time despite minor variations in how
    the question is posed. Again, we can leverage semantic similarity to compare each
    new output to the prior when the input is held constant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to evaluating the quality of model responses for each approach,
    we can also directly compare the operational and computational overhead required
    for each.
  prefs: []
  type: TYPE_NORMAL
- en: For fine-tuning, we will need to understand the overhead involved in training
    the model. While the PEFT method will significantly reduce the training effort,
    there could be considerably more infrastructure-related costs compared to in-context
    learning, which requires no additional training. Alternatively, for in-context
    learning, commoditized models such as OpenAI’s GPT-4 have a per-token cost model.
    StyleSprint must also consider the cost of tokens required to embed a sufficient
    number of few-shot examples in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, StyleSprint will incur some operational costs to create best-in-class
    examples written by humans that can be used as a “gold standard” in either the
    few-shot approach or for additional model training.
  prefs: []
  type: TYPE_NORMAL
- en: By conducting these comparative tests and analyzing the results, StyleSprint
    will gain valuable insights into which approach – in-context learning or fine-tuning
    – best aligns with its operational goals and customer service standards. This
    data-driven evaluation will inform the decision on the optimal AI strategy for
    enhancing their customer service experience. We will implement these comparisons
    in the practice project that follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Practice project: Fine-tuning for Q&A using PEFT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our practice project, we will experiment with AdaLoRA to efficiently fine-tune
    a model for a customer query and compare it directly to the output of a **state-of-the-art**
    (**SOTA**) model using in-context learning. Like the previous chapter, we can
    rely on a prototyping environment such as Google Colab to complete the evaluation
    and comparison of the two approaches. We will demonstrate how to configure model
    training to use AdaLoRA as our PEFT method.
  prefs: []
  type: TYPE_NORMAL
- en: Background regarding question-answering fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our project utilizes the Hugging Face training pipeline library, a widely recognized
    resource in the machine learning community. This library offers a variety of pre-built
    pipelines, including one for question-answering, which allows us to fine-tune
    pre-trained models with minimal setup. Hugging Face pipelines abstract much of
    the complexity involved in model training, making it accessible for developers
    to implement advanced natural language processing tasks directly and efficiently
    In particular, this pipeline behaves as an interface to a transformer model with
    a specific head for question-answering tasks. Recall that when we fine-tune a
    transformer model, we keep the architecture of the model – including the self-attention
    mechanism and the transformer layers – but we train the model’s parameters on
    a specific task, which, in this case, results in a model refined specifically
    to answer questions. Recall our practice project in [*Chapter 3*](B21773_03.xhtml#_idTextAnchor081)
    where the resulting model was a translator; we used a translator head to accomplish
    translation from English to French. For this project, the “head” is aligned to
    learn patterns in question-answering data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when using a question-answer training pipeline, it is important to
    understand that the model does not simply memorize question-answer pairs, it learns
    the connection between questions and answers. Moreover, to answer appropriately,
    the model cannot rely entirely on training. It also requires additional context
    as input to compose a relevant answer. To understand this further, we decompose
    the model inferencing step as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When feeding a question to a model, we must also include context relevant to
    the topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model then determines the most relevant part of the context that answers
    the question. It does this by assigning probability scores to each token (word
    or sub-word) in the context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The model “thinks” of the context as a potential source for the answer and
    assigns each token two scores: one score for being the **start** of the answer,
    and another for being the **end** of the answer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The token with the highest “start” score and “end” score is then chosen to form
    the answer **span**. The span is what is presented to the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To provide a concrete example, if we ask the model, `Does StyleSprint have any
    leather jackets?` and provide a context of `StyleSprint sells a variety of coats,
    jackets and outerwear`, the model will process this context and identify that
    the most likely answer is something like `Yes, StyleSprint sells a variety of
    outerwear`. However, if the answer to a question is not included in the provided
    context, the model cannot generate a reliable answer. Additionally, if the context
    is too unspecific, the model may provide a more generic answer. Like in-context
    learning, the fine-tuned approach for question-answering requires relevant context.
    This means that, in practice, the model must be integrated with a search component
    that can retrieve additional context to pair with each question.
  prefs: []
  type: TYPE_NORMAL
- en: Consider our leather jacket example. When a question is received, the system
    could perform a search of its knowledge base and retrieve any contextual information
    relevant to a leather jacket (e.g., a paragraph about outerwear). Again, since
    the model was trained to answer questions in a way that aligns with the brand
    tone, it will extract the relevant information from the context provided to formulate
    an appropriate answer. Not only will integration with search provide the model
    with the context it needs but it will also allow the model to have up-to-date
    and real-time information.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we might incorporate a confidence threshold, where the model only
    gives an answer if it assigns a high enough probability to the start and end tokens.
    If the highest probability is below this threshold, we might say the model does
    not know, or request more information. Overall, the model efficacy relies heavily
    on the quality and size of the training data as well as the relevance of the context
    with regard to the questions posed.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better understanding of how fine-tuning for question-answering
    works and what to expect when using the question-answering pipeline from Hugging
    Face, we can begin to write our implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First and foremost, we install the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we import the question-answering modules from the transformers library.
    For our project, we will use Google’s **Flan T5 (small)**, which is considered
    a SOTA alternative to GPT 3.5\. As one of our goals continues to be to measure
    the performance versus efficiency trade-off, we begin with the smallest version
    of Flan T5, which has 80M parameters. This will enable faster training and more
    rapid iteration. However, please note that even a small model trained over a small
    number of epochs will require a high-RAM runtime environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With the pre-trained model instantiated, we can now configure the model to
    adapt its training process to use AdaLoRA, which, as we’ve learned, is specifically
    designed to allocate the parameter budget efficiently during the fine-tuning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed, fine-tuning relies heavily on the quality and size of the training
    data. In the StyleSprint scenario, the company could aggregate question-answer
    pairs from its FAQ page, social media, and customer service transcripts. For this
    exercise, we will construct a simple dataset that looks similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'However, in order to integrate our dataset with the question-answer pipeline,
    we should first understand the `Trainer` class. The `Trainer` class in the Hugging
    Face transformers library expects the training and evaluation datasets to be in
    a specific format, usually as a PyTorch `Dataset` object, not just as simple lists
    of dictionaries. Further, each entry in the dataset needs to be tokenized and
    structured with the necessary fields such as `input_ids`, `attention_mask`, and,
    for question-answering tasks, `start_positions` and `end_positions`. Let us explore
    these in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids`: This is a sequence of integers that represent the input sentence
    in the model. Each word or sub-word in the sentence is converted into a unique
    integer or ID. Recall from earlier chapters that this process is known as `[101,`
    `354, 2459]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask`: An attention mask is a sequence of binary values where 1s
    indicate real tokens and 0s indicate padding tokens. In other words, in the places
    where 1s are present, the model will understand that those places need attention
    and the places with 0s will be ignored by the model. This is crucial when dealing
    with sentences of varying lengths and dealing with batches of sentences in training
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` and `end_positions`: These are for question-answering tasks.
    They represent the indices of the start and end tokens of the answer in the tokenized
    form of the context. For example, in the context *Paris is the capital of France*,
    if the question is *What is the capital of France?* and the answer given is *Paris*,
    after tokenization, `start_position` and `end_position` will correspond to the
    index of *Paris* in the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With that understanding, we can create a class that adapts our dataset to meet
    the expectations of the trainer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For the complete custom dataset class code, visit this book’s GitHub repository
    at [https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the training set prepared and our pipeline configured to apply the AdaLoRA
    method, we can finally move to the training step. For this project, we will configure
    the training to run for just a few epochs, but in the StyleSprint scenario, a
    much more robust training process would be required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For our simple experiment, we do not expect a highly performant model; however,
    we can learn how to interpret the training output, which describes how well the
    model performed on the evaluation samples. The `Trainer` class will output a training
    summary that includes the loss metric.
  prefs: []
  type: TYPE_NORMAL
- en: Training loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training loss is a measure of how well the model is performing; a lower loss
    indicates better performance. In many deep learning models, especially those dealing
    with complex tasks such as language understanding, it’s common to start with a
    relatively high loss. The expectation is that this value should decrease as training
    progresses.
  prefs: []
  type: TYPE_NORMAL
- en: In the early stages of training, a high loss isn’t a cause for alarm as it commonly
    decreases as the model continues to learn. However, if the loss remains high,
    this signals that additional training may be needed. If the loss continues to
    be high after prolonged training, the learning rate and other hyperparameters
    may require adjustment, as an inappropriate learning rate can impact the model’s
    learning effectiveness. Moreover, the quality and quantity of your training data
    should be evaluated as insufficient data can hinder the training. For example,
    as we only use a few examples for the experiment, we expect a relatively high
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to use our newly fine-tuned model to infer or predict. We
    should also secure our trained model parameters so we can reuse it without retraining:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As discussed, we introduce context along with a question to the model, so that
    it can identify which fragment of the context responds most appropriately to the
    query. Consequently, we may want to consider integrating a vector search system
    (such as RAG) to automatically identify relevant documents from large datasets
    based on semantic similarities to a query. These search results may not provide
    specific answers, but the trained QA model can extract more precise answers from
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: With this hybrid approach, the vector search system first retrieves documents
    or text segments that are semantically related to the query. The QA model then
    analyzes this context to identify the precise answer that aligns with StyleSprint’s
    guidelines and expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate our model outcomes, StyleSprint might apply the qualitative and
    quantitative approaches we have discussed in the chapter already. For the purpose
    of our experiment, we can measure the output of the model to a golden standard
    response using a simple measure for semantic similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of our evaluation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | PEFT Flan T5 | GPT 3.5T |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fine-tuned | In-context |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic Similarity | 0.543 | 0.91 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1: Semantic similarity scores for fine-tuned Flan and GPT 3.5 Turbo,
    respectively'
  prefs: []
  type: TYPE_NORMAL
- en: Undoubtedly, the in-context learning arrived at an answer that was much closer
    to our gold standard reference. However, the fine-tuned model was not far behind.
    This tells us that with a more robust training dataset and considerably more epochs,
    the fine-tuned model could be comparable to GPT 3.5\. With more iteration and
    experimentation, StyleSprint could have a very robust fine-tuned model to answer
    very specific questions for its customers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on the strategic decision-making process between
    fine-tuning and in-context learning for StyleSprint’s AI-driven customer service
    system. While in-context learning, particularly few-shot learning, offers adaptability
    and resource efficiency, it may not consistently align with StyleSprint’s brand
    tone and customer service guidelines. This method relies heavily on the quality
    and relevance of the examples provided in the prompts, requiring careful crafting
    to ensure optimal outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, PEFT methods such as AdaLoRA, offer a more focused approach
    to adapt a pre-trained model to the specific demands of customer service queries.
    PEFT methods modify only a small subset of a model’s parameters, reducing the
    computational burden while still achieving high performance. This efficiency is
    crucial for real-world applications where computational resources and response
    accuracy are both key considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the choice between in-context learning and fine-tuning is not just
    a technical decision but also a strategic one, deeply intertwined with the company’s
    operational goals, resource allocation, and the desired customer experience. The
    chapter suggests conducting comparative tests to assess the efficacy of both approaches,
    evaluating outcomes at scale through reliable metrics. This data-driven evaluation
    will inform StyleSprint’s decision on the optimal AI strategy for enhancing their
    customer service experience.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we now have a more complete understanding of the implications of
    fine-tuning versus in-context learning in LLMs, specifically in the context of
    customer service. It highlights the need for a company like StyleSprint to make
    a well-informed strategic decision, balancing the depth of specialization and
    consistency offered by fine-tuning against the adaptability and efficiency of
    in-context learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore PEFT for domain adaptation where the outcome
    of our training is a general-purpose model refined to understand a highly specific
    domain like finance or law.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  prefs: []
  type: TYPE_NORMAL
- en: Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). *Improving
    language understanding by generative* *pre-training*. OpenAI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu, E. J., Shen, Y., Wallis, P., Li, Y., Wang, S., Wang, L., and Chen, W. (2021).
    *LoRA: Low-Rank Adaptation of Large Language Models*. ArXiv. /abs/2106.09685'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T.
    (2023). *Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning*. ArXiv.
    /abs/2303.10512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown TB, Mann B, Ryder N, et al. 2020\. *Language Models are Few-Shot* *Learners*.
    ArXiv:2005.14165.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
