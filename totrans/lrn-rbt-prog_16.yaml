- en: '*Chapter 13*: Robot Vision – Using a Pi Camera and OpenCV'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Giving a robot the ability to see things allows it to behave in ways to which
    humans relate well. Computer vision is still actively being researched, but some
    of the basics are already available for use in our code, with a Pi Camera and
    a little work.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use the robot and camera to drive to objects and follow
    faces with our pan-and-tilt mechanism. We'll be using the PID algorithm some more
    and streaming camera output to a web page, giving you a way to see what your robot
    is seeing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Raspberry Pi camera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up computer vision software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Raspberry Pi camera stream app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running background tasks when streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following colored objects with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking faces with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The robot with the pan-and-tilt mechanism from [*Chapter 11*](B15660_11_Final_ASB_ePub.xhtml#_idTextAnchor219),
    *Programming Encoders with Python*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code for the robot up to [*Chapter 11*](B15660_11_Final_ASB_ePub.xhtml#_idTextAnchor219),
    *Programming Encoders with Python*, which you can download from GitHub at [https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter11](https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter11).
    We will be extending and modifying this for new functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Raspberry Pi camera.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 300 mm-long Pi Camera cable, as the cable included with the camera is too
    short. Be sure that the cable is not for a Pi Zero (which has different connectors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two M2 bolts and an M2 nut.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A small square of thin cardboard—a cereal box will do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A small jeweler's screwdriver.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pencil.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A kids' bowling set—the type with differently colored pins (plain, with no pictures).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A well-lit space for the robot to drive in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internet access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter is on GitHub, available at [https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter13](https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter13).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/39xfDJ9](https://bit.ly/39xfDJ9)'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Raspberry Pi camera
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can get into computer vision, we need to prepare the camera on your
    robot. There is hardware installation and software installation involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have completed this installation, our robot block diagram will look
    like *Figure 13.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Our robot block diagram with the camera added
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13.1* continues the block diagrams we have shown throughout the book,
    with the camera''s addition and its connection to the Raspberry Pi highlighted
    on the left.'
  prefs: []
  type: TYPE_NORMAL
- en: We will first attach the camera to the pan-and-tilt assembly. We can then use
    a longer cable to wire the camera into the Pi. Let's start preparing the camera
    to be attached.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching the camera to the pan-and-tilt mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B15660_10_Final_ASB_ePub.xhtml#_idTextAnchor192), *Using Python
    to Control Servo Motors*, you added a pan-and-tilt mechanism to your robot. You
    will mount the camera onto the front plate of this mechanism. There are brackets
    and kits, but they are not universally available. Feel free to use one of these
    if you can adapt it to the pan-and-tilt mechanism; if not, I have a few plans.
  prefs: []
  type: TYPE_NORMAL
- en: Building a robot requires creative thinking and being adaptable, as well as
    the necessary technical skills. I frequently look through the materials I have
    for possible solutions before I go and buy something. Sometimes, the first thing
    you attempt will not work, and you'll need a plan B. My plan A was to use a hook-and-loop
    fastener (such as Velcro) stuck directly to the camera, but it does not adhere
    well to the back of the camera. So I had to move to plan B, that is, using a square
    of cardboard, making holes for 2 mm screws in it, bolting the camera to the cardboard,
    and then using the hook-and-loop fastener to attach the camera assembly to the
    Pi. Another possibility is to drill additional holes in the pan-and-tilt mechanism
    to line up with the camera screw holes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Could I glue this? Yes, like most of our robot build, some glue—even crazy glue—could
    be used to adhere the camera to the pan and tilt. It would probably be an easier
    build. However, I can easily foresee that you would need to replace or remove
    the camera at some point. Reasons for that might be to reverse the camera cable
    or swap the camera out for another sensor, or even a newer camera with better
    features. It is for this reason that I generally avoid glue in my robot builds,
    looking for modular and replaceable solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parts needed are shown in *Figure 13.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – The parts needed for our plan to fit the camera module
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13.2* shows the tools and materials laid out: some thin card, 2 mm
    bolts and screws, the Pi Camera module, some scissors, a small spanner (or pliers),
    hook-and-loop tape, and a small screwdriver. You will also need a pencil.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While making this, please try not to touch the camera''s lens. So let''s begin.
    The following figure shows you the steps to attach the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Fitting the camera, steps 1–2
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how to use these parts to mount the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: First, cut a small amount for one side of the hook-and-loop fastener and adhere
    it to the pan-and-tilt mechanism, as shown in *Figure 13.3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mark and cut out a small square of cardboard a little larger than the camera:![](img/B15660_13_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 13.4 – Using a pen to mark the screw positions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then use a pen or pencil to poke through the camera screw holes to mark a dot,
    as shown in *Figure 13.4*. Then take a pointed tool (such as the point of a cross-headed
    jeweler's screwdriver or a math set compass), and on a firm surface, punch a hole
    where you made the mark:![](img/B15660_13_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 13.5 – Bolting the camera to the cardboard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use a couple of M2 bolts and nuts to fasten the camera onto the cardboard carrier,
    as shown in *Figure 13.5*. Note that the bolt-facing side is at the back—this
    is so any protruding threads won't interfere with the hook and loop:![](img/B15660_13_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 13.6 – The back of the cardboard/camera assembly with our hook-and-loop
    fastener
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now cut a small amount of the hook-and-loop fabric, to which the material on
    the pan-and-tilt mechanism will fasten, and stick it to the back of the cardboard,
    as shown in *Figure 13.6*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the camera may have a film covering the lens—please remove this.
  prefs: []
  type: TYPE_NORMAL
- en: The camera is ready to be stuck to the robot. Don't attach the camera just yet,
    as we need to wire in the cable first. Let's see how in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Wiring in the camera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the camera ready to attach, we'll need to use the Raspberry Pi camera cable
    to connect it to the Pi. We'll need to move some parts to get to the Raspberry
    Pi connector and thread the ribbon connector through.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of images in *Figure 13.7* shows how we will wire this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – The camera connector slot and the motor board
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps in *Figure 13.7* show how we''ll prepare the cable connector:'
  prefs: []
  type: TYPE_NORMAL
- en: The Raspberry Pi has a slot specifically for the camera—the camera cable fits
    into this. We will be wiring our camera into this slot, but the motor board covers
    the slot on our robot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get around the slot being covered, we will need to lift the other boards
    above the Pi. You'll temporarily need to unbolt the **Inertial Measurement Unit**
    (**IMU**), so the motor board isn't covered by it. Loosen the nut on top of the
    IMU; then you can turn the lower spacer post by hand to remove the IMU, leaving
    the IMU and standoff assembly complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Disconnect the motor wires (note how you'd previously connected them, or take
    a photo for later reference).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now gently lift the motor board off the Raspberry Pi.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you connect the camera to the Pi, the long cable will need to pass through
    the motor board. Keep this in mind as you perform the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I recommend following *Connect ribbon cable to camera* in *The Official Raspberry
    Pi Camera Guide* ([https://magpi.raspberrypi.org/books/camera-guide](https://magpi.raspberrypi.org/books/camera-guide))
    for attaching the camera using the long 300 mm cable. After following the guide,
    you should have the ribbon installed the correct way around in the camera, then
    going through the slot in the motor board and into the port the right way around
    on the Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: Double-checking that your connections are the right way around before replacing
    the motor board will save you a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete the reassembly, take a look at *Figure 13.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Completing the camera interface
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps, using *Figure 13.8* as a reference:'
  prefs: []
  type: TYPE_NORMAL
- en: Gently replace the motor board, pushing its header down onto the Raspberry Pi
    GPIO header and the holes onto spacers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bolt the IMU back in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reconnect the motor cables based on your reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Push the camera onto the hook-and-loop attachment on the pan-and-tilt head,
    with the cable facing upward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You've seen how to wire in Raspberry Pi cameras. This camera is now wired and
    ready to use. Next, we will start preparing the software to get images from the
    camera.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up computer vision software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can start writing code, we'll need to set up drivers, tools, and libraries
    to interact with the camera and software to assist with computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will activate the camera in Raspberry Pi OS Raspberry Pi
    OS and get a test picture. Then we will add the libraries to start interacting
    with the camera for visual processing.
  prefs: []
  type: TYPE_NORMAL
- en: We will then build our first app with the tool to demonstrate that the parts
    are in place and give us a starting point for the behaviors. Let's get into setting
    up the software.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Pi Camera software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So that the camera is ready to use, we need to enable it:'
  prefs: []
  type: TYPE_NORMAL
- en: Power up the Pi on external power (that is, plugged into a USB wall adapter)
    for this operation, leaving the motors powered down for now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log in via SSH. At the terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should now see `raspi-config`. Select the **Interfacing Options** menu item
    by using the cursor keys and *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `raspi-config` will then ask whether you would like the camera interface
    to be enabled. Select **Yes** and **Ok**, then **Finish**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will need to reboot for this to take effect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To verify that we can get pictures, we'll need the `picamera` package. At the
    time of writing, there is a copy of `picamera` already installed in Raspberry
    Pi OS.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the camera is enabled, let's try getting our first picture.
  prefs: []
  type: TYPE_NORMAL
- en: Getting a picture from the Raspberry Pi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we need to do, to confirm that our setup was successful, is
    to ask the Pi Camera to take a picture. If the camera isn''t detected, please
    go back and check that the cable connection is correct, that you have installed
    `picamera`, and that you have enabled the Raspberry Pi camera in `raspi-config`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reconnect to the Raspberry Pi and type the following to get a picture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can then use your SFTP client (which we set up in [*Chapter 4*](B15660_04_Final_ASB_ePub.xhtml#_idTextAnchor063),
    *Preparing a Headless Raspberry Pi for a Robot*) to download this image and verify
    it on your computer. You will notice that the picture is upside down, due to how
    the camera is mounted. Don't worry—we will correct this with our software.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With a picture taken, you know that the camera works. Now we can install the
    rest of the software needed to use the camera in visual processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenCV and support libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need a few helper libraries to do the heavy lifting of visual processing
    and display the output in a useful way. **Open Computer Vision** (**OpenCV**)
    is a library with a collection of tools for manipulating pictures and extracting
    information. Code can use these OpenCV tools together to make useful behaviors
    and pipelines for processing images.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run our code on the Raspberry Pi, we will need to install the Python OpenCV
    library there:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV has some dependencies that are needed first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Raspberry Pi OS requires a library to be identified for OpenCV to work. This
    line identifies the library every time you log in to the Pi. We should also prepare
    it for this session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Flask** is a library for creating web servers that we''ll use to stream the
    video data to a browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**NumPy**, the numeric Python library, is excellent for the manipulation of
    large blocks of numbers. An image stored on a computer is essentially a large
    block of numbers, with each tiny dot having similar content to the three-color
    numbers we sent to the LEDs in [*Chapter 9*](B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171),
    *Programming RGB LED Strips in Python*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will need to install the large array extension for `picamera`. This will
    help us convert it''s data for use in NumPy and OpenCV:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will continue testing on external power for the next few operations.
  prefs: []
  type: TYPE_NORMAL
- en: You've now prepared the software libraries and verified that the camera can
    take pictures. Next, we'll build an app to stream video from the camera to your
    browser.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Raspberry Pi camera stream app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Downloading one picture at a time is fine, but we need to do things with those
    pictures on our robot. We also need a handy way to see what the robot is doing
    with the camera data. For that, we will learn how to use a Flask web server to
    serve up our pictures so we can view the output on a phone or laptop. We can use
    the core of this app to make a few different behaviors. We'll keep the base app
    around for them.
  prefs: []
  type: TYPE_NORMAL
- en: A video or video stream is a sequence of images, usually known as **frames**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's design our streaming server.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the OpenCV camera server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The diagram in *Figure 13.9* shows an image data pipeline, going from the camera,
    through the processing, and out to our web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – The image server app
  prefs: []
  type: TYPE_NORMAL
- en: The image server app in *Figure 13.9* starts with the camera. The camera feeds
    image data to a **convert to OpenCV** step, with the raw photo given. Image data
    needs some processing for OpenCV to be able to manipulate it.
  prefs: []
  type: TYPE_NORMAL
- en: '**convert to OpenCV** feeds data to **process a frame**, which can be anything
    we require; for this example, we''ll apply a color mask, which we explore in more
    depth in the next section. Above the **process a frame** step is an example of
    an image after using a red color mask.'
  prefs: []
  type: TYPE_NORMAL
- en: The raw frame and processed frame go into the next step, **join with original**,
    which creates a compound image with both images. Above the step are the two images
    joined into a single longer frame.
  prefs: []
  type: TYPE_NORMAL
- en: The joined images go into the `jpeg`, an image encoding that a browser can show,
    and importantly, display as a sequence of frames, a streaming movie.
  prefs: []
  type: TYPE_NORMAL
- en: The encoded data goes to **serve over HTTP**, getting the data into a system
    you can view with a web browser. It uses a template (some layout and text for
    the browser) to serve this.
  prefs: []
  type: TYPE_NORMAL
- en: The image output then goes from **serve over HTTP**, via the network, to the
    users, browser. Finally, the browser shows the image to the user. The browser
    could be on a laptop or a phone.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s time to start building the code. We''ll break it down into two major
    parts: first, a `CameraStream` object, which will send our frames to the second
    part of our code project, an `image_server.py` script.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing the CameraStream object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As part of our system, we will create a helper library to set up the camera
    and get data streams from it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the `camera_stream.py` file with the following imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next few lines set up parameters for the capture size and image quality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a function to set up the camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After initializing the camera, we set its resolution to the size. I mentioned
    that the camera is the wrong way up, so we set its rotation to 180 degrees to
    turn the pictures around.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will need a function to start capturing a stream of images (a video, but
    a frame at a time):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can loop through `cam_stream` for frames until we choose to stop. Python
    has a concept of `for` loop is a generator. Every cycle will yield the raw `.array`
    from the frame that the stream captured. What this means is that a loop can use
    the output of the `start_stream` function, so when looped over, the code in this
    `for` loop will run just enough to produce one raw frame, then the next, and so
    on. Python generators are a way to construct processing pipelines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last line of the loop calls `truncate` to reset `image_storage` ready to
    hold another image. `PiRGBArray` can store many images in sequence, but we only
    want the latest one. More than one image may have arrived while we were processing
    a frame, so we must throw them away.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final thing we add to the `camera_stream.py` script is a function to encode
    an image as `jpeg` and then into bytes for sending, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will use the `camera_stream` library for a few of our behaviors, giving us
    the ability to fetch and encode camera frames, both ready for input and encoded
    for display. With that ready, let's use it in a test app to serve frames in a
    browser.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the image server main app
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This part of the app will set up Flask, start our camera stream, and link them
    together. We will put this in a new script named `image_server.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to import all of these components and set up a Flask app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Flask works in routes, which are links between an address you hit a web server
    at and a registered handler function. A matching address asked for at our server
    app will run the corresponding function. Let''s set up the most basic route:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we get to the tricky bit, the video feed. Although `camera_stream` does
    some of the encoding, we need to turn the frames into an HTTP stream of data,
    that is, data that your browser expects to be continuous. I''ll put this in a
    `frame_generator` function, which we''ll need to break down a little. Let''s start
    by setting up the camera stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to loop over the frames from `camera_stream`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To send the encoded frame bytes back to the browser, we use another generator
    with `yield`, so Flask considers this a multipart stream—a response made of multiple
    chunks of data, with parts deferred for later—which many frames of the same video
    would be. Note that HTTP content declarations prefix the encoded bytes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next function, named `display`, routes from Flask to a loopable stream
    of HTTP frames from `frame_generator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can just add the code to start Flask. I''ve put this app on port `5001`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The app is nearly ready, but we mentioned a template—let's use this to describe
    what will go on the web page with the camera stream.
  prefs: []
  type: TYPE_NORMAL
- en: Building a template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flask makes web pages using HTML templates, which route functions render into
    the output, replacing some elements at runtime if necessary. Create a `templates`
    folder, then make a file in that folder named `image_server.html`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our template starts with the HTML tag, with a title and a level 1 heading:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we add the image link that will display the output of our server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we just close the tags in the template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can serve this template up in our main server app.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can upload all three of these parts, ensuring that you upload the template
    into the `templates` directory on the Pi.
  prefs: []
  type: TYPE_NORMAL
- en: With the server code and templates ready, you should be able to run the image
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Running the server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start the app with `python3 image_server.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Point your browser at the app by going to `http://myrobot.local:5001` (or your
    robot''s address), and you should see a video served, as shown in *Figure 13.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – Screen capture of the robot image server
  prefs: []
  type: TYPE_NORMAL
- en: The screenshot in *Figure 13.10* shows our robot image server output in a browser.
    The top shows the browser search bar, with the `myrobot.local:5001` address in
    it. Below this is the **Robot Image Server** heading from the template. Below
    the heading is an image capture of a kids' red bowling pin taken from a robot
    camera—served up with the video stream code.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have problems running the server and seeing the picture, try the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see errors while running the code, do the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) Ensure you can capture images with raspistill.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Ensure you have installed all the required dependencies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) If it's about `libatomic`, please ensure that you have performed the previous
    `LD_PRELOAD` exports.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Check that the code is correct.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the image is black, check your lighting. The Raspberry Pi camera is susceptible
    to light conditions and needs a well-lit space to operate. Note that none of the
    following tracking will work if the camera is not getting enough light.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expect the rate to be slow—this is not a fast or high-quality capture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now you can stream images from a Raspberry Pi into a browser. Next, we will
    add a background worker task and control mechanism to the app, as this whole server
    depends on the slow browser request cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Running background tasks when streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our image service works but has a significant flaw. Currently it will wait between
    requests before taking each action, but what if we want our robot to be doing
    something? To do this, we need to be able to run a behavior in parallel with the
    server. That behavior and the server both need access to the image data.
  prefs: []
  type: TYPE_NORMAL
- en: We will approach this by making the Flask web app a secondary process, with
    the behavior as the primary process for the robot when it is running. Python has
    a handy tool for precisely this kind of structure, called multiprocessing. Find
    out more at [https://docs.python.org/3/library/multiprocessing.html](https://docs.python.org/3/library/multiprocessing.html).
  prefs: []
  type: TYPE_NORMAL
- en: Communicating between multiple processes is tricky. If two processes try to
    access (read or write) the same data simultaneously, the results can be unpredictable
    and cause strange behavior. So, to save them trying to access data simultaneously,
    we will use the multiprocessing queue object. A queue allows one process to put
    data in at one end and another process to consume it safely at the other—it is
    a one-way flow of information. We will use one queue to send images to the server
    and another to get control data from user interactions in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram in *Figure 13.11* shows the way data will flow through these behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – Data flow between a browser, server process, and robot behavior
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 13.11*, we abridge some of the sections in *Figure 13.9*. First,
    there is data from the camera going into a visual processing behavior (for example,
    tracking an object). This behavior will output image frames to an image queue.
    The output will be the fully processed and joined image.
  prefs: []
  type: TYPE_NORMAL
- en: A server process, the web app, will take the images from the image queue to
    serve them to a browser via the network. However, the web app will also handle
    commands from user interaction in the browser. The app puts them in the control
    queue as messages. The visual processing behavior will read any messages from
    the control queue and act on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few caveats: the visual processing behavior will only place images in the
    image queue when it''s empty, so the queue will only ever contain one image. Allowing
    only one prevents the visual processing behavior from trying to overwrite an image
    in shared memory when a server tries to output it. The control queue has no such
    restriction; we''ll just expect that user interactions will not produce control
    messages faster than the behavior loop can consume them.'
  prefs: []
  type: TYPE_NORMAL
- en: We will separate the web app as a core and then write a behavior based on it.
    We can use the web app core multiple times. Let's write this code.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a web app core
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this design, the web app core will handle setting up the queues, running
    the server process, and the Flask-based routing. We will write the library in
    Flask style, using plain Python functions in a module.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an interface to the core, our other behaviors will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`start_server_process(template_name)` will start the web app server, using
    the named template.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`put_output_image(encoded_bytes)` will put images into the display queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_control_instruction()` is used to check and return instructions from the
    control queue. This function returns a dictionary of instruction data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Flask/web server part of the app is slightly independent of the behavior,
    allowing the user to *tune in* to see its display, but it should not stop the
    app running when a user is not present or a browser stalls:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with some imports. We''ll put this code in `image_app_core.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define our Flask app and the queues. We only really want one frame
    queued, but we put in one in case of hiccups while transmitting—although we can
    check whether a `Queue` instance is empty, this is not 100% reliable, and we don''t
    want one part of the app waiting for the other:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will also define a global `display_template` here, in which we''ll store
    the main app template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we add routes for this Flask app. The index route is only different in
    that it uses `display_template`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create the loop for getting frames: a modified version of `frame_generator`.
    This function is our main video feed. So that it doesn''t *spin* (that is, run
    very quickly in a tight loop), we put in a sleep of 0.05 to limit the frame rate
    to 20 frames per second:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the sleep, we should try to get data from `display_queue` (we''ll put
    frames into the queue later). Like we did in `image_server`, this loop also turns
    our data into multi-part data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now make that available through a display block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need a way to post control messages to our app. The `control` route accepts
    these, takes their form data (a dictionary with instructions), and uses `control_queue.put`
    to pass that along to the robot behavior:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That gives us all the core internals, but we also need to start the server
    process. The part of the app from earlier that started our server, we''ve now
    put into a function named `start_server_process`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next interface task is putting an image into the queue we created in *step
    1*. To ensure that we don''t run up a lot of memory, we only intend the queue
    to have a length of one. That means that the first frame will be stale, but the
    next frame will arrive soon enough for it not to affect the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, for this interface, we need a function to get the control messages
    out. This function will not wait and will return a message if there is one or
    `None` for *no message*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `image_app_core.py` file establishes a controllable base for us to build
    visual processing robot behaviors with, or indeed any behavior with a web interface,
    control instructions, an output stream, and background process. Next, let's test
    this core with a simple behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Making a behavior controllable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can try out our core with a behavior that sends images to the web service
    and accepts a simple `exit` control message:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make a new file called `control_image_behavior.py`, starting with imports
    for the `image_app_core` interface and `camera_stream`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then add a function that runs our simple behavior with the main loop. I''ve
    broken this function down as it''s a little complicated. First, we''ll set up
    the camera and use a sleep to give the camera warm-up time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we get frames from a camera stream in a `for` loop and put those as encoded
    bytes on the output queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While still in the loop, we will try accepting a control instruction to exit.
    Normally the instruction will be `None`, signalling there are no control instructions
    waiting. But if we have a message, we should match the command in it to exit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then need to start the server and start our behavior. We always want to
    stop the web server process. By surrounding the behavior with `try` and `finally`,
    it will *always* run anything in the `finally` part, in this case, making sure
    the process is terminated (stopped):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We now have a simple controllable behavior; however, it mentions the `control_image_behavior.html`
    template. We need to provide that.
  prefs: []
  type: TYPE_NORMAL
- en: Making the control template
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This template, in `templates/control_image_behavior.html`, is the same as the
    one before, but with two important differences, shown here in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The differences are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In this template, we load a library in our browser called `jquery`, which is
    handy for interactive web pages. There is great documentation for jQuery at [https://api.jquery.com/](https://api.jquery.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have the image and header that we saw before, but new to this code is an
    `a` tag (for anchor), which when clicked will post the `exit` command to the `'/control'`
    route on our web app. `<br>` just creates a line break to show the exit link below
    the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you wanted to run this where internet access is difficult, you would need
    the server to serve the `jquery` library. This template tells the browser to download
    `jquery` directly from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have the components, we should try running our controllable behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Running the controllable image server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we have the components, let''s get this running and try out the commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the image server, you need to upload all three files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) `image_app_core.py`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `control_image_behavior.py`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `templates/control_image_behavior.html`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On your Pi, use `python3 control_image_behavior.py` to start the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Point your browser at `http://myrobot.local:5001` (or the address of your robot).
    You will see the pictures again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you click on the **Exit** link below the image, this will send a control
    instruction to your app, which should gracefully quit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You've now seen how to get image data from a behavior while sending control
    data back to the behavior. With the control and streaming technique tested and
    ready, and a framework to use for it, we can build a more interesting behavior.
    In the next section, we'll make the robot follow an object with a specific color.
  prefs: []
  type: TYPE_NORMAL
- en: Following colored objects with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have some basics ready; we can use this to build some more interesting
    behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: We will create a behavior that will chase, but not get too close to, a colored
    object. This behavior will make the robot seem very intelligent. We will revisit
    color models, covered in [*Chapter 9*](B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171),
    *Programming RGB Strips in Python*. We'll add color masking and filtering and
    use the OpenCV contours tools to detect the largest blob of color in an image
    and point the robot at it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building the color-chasing behavior requires a few steps. Let''s start with
    a diagram showing an overview of this whole behavior in *Figure 13.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – The color-tracking behavior
  prefs: []
  type: TYPE_NORMAL
- en: The flow of data in *Figure 13.12* starts from **camera images**. These go through
    **visual processing** to **get object info from image**. **get object info from
    image** outputs the object's size (based on the radius of a circle around it)
    and the object's position (the middle of the enclosing circle) and puts frames
    on the image queue for the web app/browser.
  prefs: []
  type: TYPE_NORMAL
- en: The object size goes into a speed **Proportional Integral Derivative** (**PID**)
    controller, which also has an object size reference as its set point. Depending
    on the difference between the expected size and actual size, this PID will output
    a speed for the motors, optimizing the radius to be the same as the reference
    size. That way, the robot will maintain a distance from an object of a known size.
    This is a base speed for both motors.
  prefs: []
  type: TYPE_NORMAL
- en: The object position has an `x` component and a `y` component. This behavior
    will turn to center the object, so we are interested in the `x` coordinate. The
    `x` coordinate goes into a PID for controlling the direction/heading. This PID
    takes a reference position—the center of the camera viewport. This direction PID
    will produce an output to try and get the difference between these coordinates
    to zero. By adding to one motor's speed and reducing the other's speed, the robot
    will turn to face the object (or, if you swap them for fun, it'll turn away!).
  prefs: []
  type: TYPE_NORMAL
- en: The images are sent, via an image queue using the app core, to the browser.
    A detail not shown in the diagram is the control queue with messages to start
    the motors, stop the motors, and exit the behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The final part of this system, and probably the most interesting, is the color
    tracking. The box labeled **get object info from image** performs the tracking.
    Let's see how that works next.
  prefs: []
  type: TYPE_NORMAL
- en: Turning a picture into information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are using colored pins from a kids' bowling set. They come in nice, bright,
    primary colors. I will use green as an example. We start with just a picture.
    However, a set of transformations to the data is needed to turn the picture into
    information the robot can use to make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pipeline is a good way to design a set of transformations. Let''s look at
    the color tracking as an image processing pipeline in *Figure 13.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – Getting color object information from a camera
  prefs: []
  type: TYPE_NORMAL
- en: As with other pipelines, *Figure 13.13* starts from the camera. This is converted
    to a low resolution to keep things fast. The figure shows a camera image above
    the step.
  prefs: []
  type: TYPE_NORMAL
- en: The process converts the output from the image capture to HSV, the colorspace
    we mentioned in [*Chapter 9*](B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171),
    *Programming RGB Strips in Python*. We use HSV because it means the process can
    filter colors in a specific range of hues, by their light (very dark objects may
    confuse us), and by saturation, so it won't include almost-gray objects. RGB (or
    BGR) images are tricky to filter, as getting the different light and saturation
    levels of a particular hue (say, the blues) is not viable. The figure shows the
    hue color wheel above this step.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV has a function, `cv2.cvtColor`, to convert whole images between colorspaces.
    Note that OpenCV uses 0–179 for the hue range, instead of 0–359\. This is so it
    fits in a byte (0–255), but you can convert hue values by simply dividing by 2
    if you know the value you want.
  prefs: []
  type: TYPE_NORMAL
- en: 'After converting to HSV, we then filter the colors in the image with a mask,
    highlighting pixels in a particular range. It will output white if the object
    is in the range, or black if it''s not. Above this step, the unshaded region on
    the hue color wheel shows the range, with the masked output next to it. There
    is a function in OpenCV to do this: `cv2.inRange`. This gives us a very easy binary
    output, a masked image, to draw around for our system.'
  prefs: []
  type: TYPE_NORMAL
- en: Our pipeline then uses the contours system to draw around our masked image.
    The contour specifies only the boundary points of our object. OpenCV provides
    a `cv2.findContours` function to do exactly this, which returns a list of shapes,
    each defined by its outlines. The preceding figure shows the contours (taken from
    the mask) drawn onto the raw image. Note how light and shade have made the bottom
    of the bowling pin a bit rough as it doesn't quite fit the mask.
  prefs: []
  type: TYPE_NORMAL
- en: The processing pipeline then takes the contours (outlines) and uses `cv2.minEnclosingCircle`
    to draw circles around them. We will then have some circles, described by a center
    `x`, `y` coordinate, and radius. The preceding figure shows these circles projected
    on the raw image.
  prefs: []
  type: TYPE_NORMAL
- en: Our object may have highlights, producing more than one circle, and other objects
    may also produce smaller circles. We are only interested in one, the largest of
    these, so we can loop through the circles, and keep only the largest. Above the
    **get the largest circle** step is the raw image with only the largest circle
    drawn.
  prefs: []
  type: TYPE_NORMAL
- en: This largest circle's coordinates and radius give us enough information for
    our robot to start chasing an object. Above this last step is just the circle,
    with crosshairs showing its position.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'A caveat about red objects: we will use green because red is slightly tricky,
    as it requires two masks. The hues for red cross a boundary between 179 (the upper
    limit of our hue range) and 0 (the lower limit), so we would have to mask the
    image twice and then combine these with an `or` operation. You could use the `cv2.bitwise_or`
    function to try masking red.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have examined how the pipeline will work and its caveats. We've seen
    how this pipeline will fit with PID controllers to create an interesting behavior.
    Let's build this code.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing the PID controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to be using more PID controllers. We still don't require the differential
    component, but we will develop an issue with our integral component building up
    while the motors take time to move. The integral has a sum that starts to grow
    if there is a constant error. It is good to correct for that error but it can
    result in a large overshoot. This overshoot, due to the integral still growing
    after the robot has started to react, albeit slowly, is called **integral windup**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can prevent this sum from getting too large by introducing a windup limit
    to our PID:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up the `pid_controller.py` file and make the changes in bold in the following
    snippet. First, add the `windup_limit` parameter, which defaults to `None` if
    you don''t set a limit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We want to prevent our integral growth if we have a limit and hit it. Our integral
    will change if any of the following occurs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) There is no windup limit (you set it to `None`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The absolute value of the sum is below the windup limit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The sign of the error would reduce the sum (by being opposed to it).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This prevents us from going above the limit if there is one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s see this in code—this code will replace the previous `handle_integral`
    method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can `start` and `stop` this behavior from the web page. If we start moving
    again, we won''t want the PIDs to carry old values. Let''s add a `reset` function
    to zero out the integral sum:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The PID controller is now able to reset and has a windup limit to stop big overshoots.
    Let's build the other behavior components that use it.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the behavior components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This behavior has two files—a template to pass to our app core with the control
    buttons, and then the main behavior code. Let's start by writing the template.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the control template
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This template is for the stream app, with some different controls:'
  prefs: []
  type: TYPE_NORMAL
- en: Copy the template from `templates/control_image_behavior.html` to `templates/color_track_behavior.html`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will add two further controls to this, `start` and `stop`, displayed here
    in bold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We intend to run the program with the robot stopped first, so we can tune in
    with our phone or browser, see what the robot is detecting, and click the **Start**
    button to get it moving.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With the template modified, we will need to write the behavior code next.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the behavior code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll put this new behavior in a file called `color_track_behavior.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s no surprise that we start with the imports. Because we are bringing together
    many elements, there are quite a few, but we have seen them all before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we add the `Behavior` class to find and get close to a colored object.
    We pass this the `robot` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These values are intended to be tuned for the color mask and object size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last member variable set here is `running`. This will be set to `True`
    when we want the robot to be moving. When set to `False`, the processing still
    occurs, but the motors and PIDs will stop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next bit of code is to process any control instructions from the web app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we have the code that will find an object from a frame. This implements
    the pipeline shown in *Figure 13.13*. We''ll break this function down a bit, though:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Because this code is complex, we have a documentation string or **docstring**
    explaining what it does and what it returns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, the method converts the frame to HSV, so it can be filtered using `inRange`
    to leave only the `masked` pixels from our frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the masked image, we can draw contours (outline points) around
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next thing is to find all the enclosing circles for each contour. We use
    a tiny loop to do this. The `minEnclosingCircle` method gets the smallest circle
    that entirely encloses all points in a contour:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'However, we only want the biggest one. Let''s filter for it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We end this method by returning the masked image, the largest coordinates,
    and the largest radius:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our next method will take an original frame and processed frame, then turn
    them into a dual-screen display (two images of the same scale joined together
    horizontally) on the output queue through to the web app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next method processes a frame of data through the preceding functions,
    finding the objects and setting the display. It then returns the object info as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next method is the actual behavior, turning the preceding coordinates and
    radius into robot movements. When we start our behavior, the pan-and-tilt mechanism
    may not be pointing straight forward. We should ensure that the mechanism is facing
    forward by setting both servos to `0`, then start the camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While the servos are moving and the camera is warming up, we can prepare the
    two PID controllers we need for speed (based on radius) and direction (based on
    distance from the horizontal middle):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These values I arrived at through much tuning; you may find you need to tune
    these further. The *Tuning the PID controller settings* section will cover how
    to tune the PIDs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we wait a little while for the camera and pan-and-tilt servos to settle,
    and then we turn off the servos in the center position:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We let the user know, with a `print` statement, and output some debug headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then enter the main loop. First, we get the processed data from the
    frame. Notice we use brackets to unpack `coordinates` into `x` and `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should check our control messages at this point. We then check whether we
    are allowed to move, or whether there is any object big enough to be worth looking
    for. If there is, we can start as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we know the robot should be moving, so let''s calculate error values to
    feed the PID controllers. We get the size error and feed it into the speed PID
    to get speed values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the center coordinate and current object, `x`, to calculate a direction
    error, feeding that into the direction PID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So we can debug this; we print a debug message here matching with the headers
    shown before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the speed and direction values to produce left and right motor speeds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ve handled what to do when the motors are running. If they are not, or
    there is no object worth examining, then we should stop the motors. If we have
    hit the **Stop** button, we should also reset the PIDs, so they do not accumulate
    odd values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have now finished that function and the `ColorTrackingBehavior` class. Now,
    all that is left is to set up our behavior and web app core, then start them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This behavior code is built and ready to run. You've seen how to convert the
    image, then mask it for a particular color, and how to draw around the blobs in
    the mask, and then find the largest one. I've also shown you how to turn this
    visual processing into robot moving behavior by feeding this data through PIDs
    and using their output to control motor movements. Let's try it out!
  prefs: []
  type: TYPE_NORMAL
- en: Running the behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I''m sure you are keen to see this working and fix any problems that there
    are. Let''s get into it:'
  prefs: []
  type: TYPE_NORMAL
- en: To run this behavior, you will need to upload `color_track_behavior.py`, the
    modified `pid_controller.py` file, and the template at `templates/color_track_behavior.html`.
    I'll assume that you already have `robot.py` and the other supporting files uploaded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the app with `python3 color_track_behavior.py`, which will start the web
    server and wait.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, you should use your browser to connect to `http://myrobot.local:5001`,
    and you should be able to see your robot's image feed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see the object and its circle, along with links to control the robot,
    as shown in the screenshot in *Figure 13.14*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B15660_13_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 13.14 – The color-tracking web app
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 13.14* shows a screenshot of our app server running the code to track
    a colored object. Under the address bar and heading is a dual-screen type output.
    The left has the direct feed from the camera, with a kids'' green bowling pin
    close to the middle and a blue circle outlining the pin, generated by the behavior
    to show it''s tracking the largest matching object. On the right is the mask''s
    output, so we can see what aspects of the image match and tune the mask values
    if we need to. Under this are **Start**, **Stop**, and **Exit** links, to start
    the motors, stop the motors, and exit the program.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To make the robot start moving, press the **Start** button on the web page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the robot starts moving, you will see the PID debug output in the console
    (PuTTY). This will only show when the robot is running.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can press the **Stop** button on the web page to stop the robot moving or
    the **Exit** button to exit the behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The robot won't be moving quite right; the movements may be understeering or
    overshooting. You'll need to tune the PID controllers to get this right, as shown
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the PID controller settings
  prefs: []
  type: TYPE_NORMAL
- en: I start with a proportional constant of 0.1, and raise it, using `nano` to make
    quick edits on the Pi, until the robot starts to overshoot—that is, it goes past
    its target, then returns far back—then I halve this proportional constant value.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may then have a constant error, so I start raising the integral constant
    by about 0.01 to counter this error. Tuning PIDs is a slow process: start by getting
    the object close to dead center and tuning `direction_pid` until it''s pretty
    good, then come back for `speed_pid`.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Do not try to tweak all the values at once—rather, change one thing and retry.
  prefs: []
  type: TYPE_NORMAL
- en: For a deeper look at this, see *Tuning a PID controller* in the *Further reading*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Color tracking is a tricky behavior, and there are some things that can go
    wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: If the motors stop or slow down, the simplest fix is to use fresh batteries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are syntax errors, please check your code carefully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the web app examples work with the camera and that you troubleshoot
    any problems there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need good lighting, as the mask may not pick up poorly lit objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beware of other objects in the view that may match; the mask may pick up things
    other than your intended items.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the web app to check your object is in view and that the mask shows your
    object mostly in white. If not, then you may need to tune the upper and lower
    HSV ranges. The hue is the factor most likely to cause problems, as the saturation
    and value ranges are quite permissive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the robot starts weaving from side to side, you may need to tune the direction
    PID. Reduce the proportional element somewhat.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the robot barely turns, you can increase the proportional element a little.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the robot is stopped but not facing the detected object, then increase the
    integral element for the direction PID by about 0.01\. If you see the same problems
    moving back and forward, try applying the same tweaks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You've seen how to track a brightly colored object with a camera, a technique
    you can use to spot objects in a room, or by industrial robots to detect ripe
    fruit. It is quite impressive to watch. However, some objects are more subtle
    than just a color, for example, a human face. In the next section, we look at
    how to use cascading feature matches to pick out objects.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking faces with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting faces (or other objects) by features is a smart behavior. Once our
    robot is detecting faces, it will point the pan-and-tilt mechanism at the nearest
    (well, largest) face.
  prefs: []
  type: TYPE_NORMAL
- en: Using **Haar cascades** is a common technique, well documented in a paper by
    Paul Viola and Michael Jones (known as *Viola Jones*). In essence, it means using
    a cascade of feature matches to search for a matching object. We will give an
    overview of this technique, then put it into use on our robot to create a fun
    behavior. Using different cascade model files, we could pick out faces or other
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: Finding objects in an image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using an algorithm implemented in OpenCV as a single and useful function,
    which makes it very easy to use. It provides a simple way to detect objects. More
    advanced and complex methods involve machine learning, but many systems use Haar
    cascades, including camera apps on phones. Our code will convert the images into
    grayscale (black through gray to white) for this detection method. Each pixel
    here holds a number for the intensity of light.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s dig into a way of representing these images: integral images.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting to integral images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two stages applied in the function. The first is to produce an **integral**
    image, or **summed-area table**, as shown in *Figure 13.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.15 – Integral images and summed-area tables
  prefs: []
  type: TYPE_NORMAL
- en: The left side of *Figure 13.15* shows a *smiling face* type image, with numeric
    pixels representing shades, with larger numbers making for a lighter color. Every
    shade has a number.
  prefs: []
  type: TYPE_NORMAL
- en: On the right of *Figure 13.15* is the integral image. Each pixel in the integral
    image is the sum or **integral** of the previous pixels. It adds itself to the
    original pixels above and left of it. The coordinate 2,2 is circled. It is the
    last in a 3x3 grid. The cell here has the value 44\. 44 is the sum of the pixels
    in the highlighted box (9 + 9 + 5 + 9 + 5 + 1 + 5 + 1 + 0).
  prefs: []
  type: TYPE_NORMAL
- en: When the code sums the pixels, the integral process can use a shortcut and use
    the previous sums. The new sum is equal to the pixel to the left plus the pixel
    above. For example, for a pixel much further down (8,8), also circled in the image,
    we could add all the numbers, but it will be faster to reuse the results we already
    have. We can take the pixel value (1), add the sum above (166), and add the sum
    to the left (164). This sum will have included the middle pixels twice, so we
    need to subtract those, so take away the value up and to the left (146). The sum
    for this would be 1 + 164 + 166 – 146 = 185\. The computer can do this pretty
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: This creates an array of numbers with the same dimensions as the image. Each
    coordinate is the sum of all the pixels' intensities between the current coordinate
    and 0,0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code can use the integral image to quickly find the intensity sum of any rectangle
    in it, of any size. You can start with the bottom-right pixel of the image, then
    subtract the top-right one, leaving the sum of pixels below the top-right pixel.
    We also then want to subtract the bottom-left pixel. This nearly constrains the
    sum to only the rectangle''s pixels, but we would have taken away sections above
    the top-left pixel twice. To correct this, add back the value of the top-left
    pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The equation works for a small rectangle of 2x2 or a large 300x200 rectangle.
    See the Viola Jones paper in the *Further reading* section for more details. The
    good news is, you don't need to write this code as it's already part of the OpenCV
    classifier. The cascade stage can use this integral image to perform its next
    potent trick quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Scanning for basic features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next part of this puzzle is scanning the image for features. The features
    are extremely simple, involving looking for the difference between two rectangles,
    so they are quick to apply. *Figure 13.16* shows a selection of these basic features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.16 – Simple rectangular feature types
  prefs: []
  type: TYPE_NORMAL
- en: The top left of *Figure 13.16* shows a left/right feature, where the left pixels
    are set to **1** and the right set to **0** (and shaded). This will match a vertical
    contrast feature. The figure's top right has two rows of **0**s (shaded), two
    rows of **1**s, and then two further rows of shaded **0**s; this will match a
    horizontal bar feature. The middle left has the top three rows set to **1**s and
    the lower three rows shaded and set to **0**s, matching a horizontal contrast
    feature. The figure's middle right has two columns of shaded **0**s, followed
    by two columns of **1**s, and then two further rows of shaded **0**s; this will
    match a vertical bar feature.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom image shows a feature with the first few rows as three **1**s followed
    by three **0**s. It follows these rows with three rows of three **0**s andt three
    **1**s. This makes a small checkerboard pattern that will match a feature with
    diagonal contrast.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm will apply rectangles like those from *Figure 13.16* in a particular
    order and relative locations, then each match will *cascade* to a further attempt
    to match another feature. Files describe objects as a set of features. There are
    face cascades with 16,000 features to apply. Applying every single one to every
    part of an image would take a long time. So they are applied in groups, starting
    perhaps with just one. If a feature check fails, that part of the image is not
    subject to further feature tests. Instead, they cascade into later group tests.
    The groups include weighting and applying groups of these features at different
    angles.
  prefs: []
  type: TYPE_NORMAL
- en: If all the feature checks pass, then the checked region is taken as a match.
    For this to work, we need to find the feature cascade that will identify our object.
    Luckily, OpenCV has such a file designed for face recognition, and we have already
    installed it on our Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: 'This whole operation of applying the summed area, then using the cascade file
    to look for potential matches, is all available through two OpenCV operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv2.CascadeClassifier(cascade_filename)` will open the given cascade file,
    which describes the features to test. The file only needs to be loaded once and
    can be used on all the frames. This is a constructor and returns a `CascadeClassifier`
    object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CascadeClassifier.detectMultiScale(image)` applies the classifier check to
    an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You now have a basic understanding of a common face (and object) recognition
    technique. Let's use cascade classifier visual processing with our existing behavior
    experience to plan the face-tracking behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Planning our behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use code fairly similar to our color-tracking behavior to track faces.
    We''ll set our robot up to use the pan-and-tilt mechanism to follow the largest
    face seen in the camera. The block diagram in *Figure 13.17* shows an overview
    of the face behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15660_13_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.17 – The face-tracking behavior
  prefs: []
  type: TYPE_NORMAL
- en: The flow in *Figure 13.17* will look very familiar. We have the same camera
    to visual behavior to image queue we've seen before. This time, the visual processing
    is `x` and `y` coordinate for the item. We feed position `x` into a PID with center
    `x` to get a pan position, which is then used by the pan servo motor. Position
    `y` is fed to a PID with center `y` and outputs a tilt position to the tilt servos.
    The servos move the camera, creating a feedback loop where the view moves.
  prefs: []
  type: TYPE_NORMAL
- en: The differences are in the data we are sending to the PID controllers, and that
    each PID controls a different servo motor.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a plan; let's write the code.
  prefs: []
  type: TYPE_NORMAL
- en: Writing face-tracking code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code for this behavior will seem very familiar—adapting the previous behavior
    code for this purpose. It''s possible that refactoring could yield more common
    code, but it is currently simpler to work with a copy for now. This code will
    go into the `face_track_behavior.py` file. I''ve not even created a new template,
    as the color track template will work just fine for this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The imports are nearly the same as our `color_track_behavior`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `init` function for the behavior class is slightly different, starting
    with loading the Haar cascade. There are many other cascade files in the same
    directory, with which you could try to track things other than a face. This code
    uses `assert` to verify that the file exists at the location here because OpenCV
    will instead return cryptic errors in `detectMultiscale` if it cannot find it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The tuning parameters have center positions and a minimum face size. I''ve
    also brought the PID controllers out to the class, so they can be tuned here,
    and then reset in the control handler (you could add the reset to the previous
    behavior too):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our constructor still tracks whether the behavior is running motors or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The process control here differs; when the `stop` instruction is received,
    it stops the motors and resets the PIDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This behavior still has a `find_object` method, taking the original frame.
    First, we convert the image to grayscale to reduce the amount of data to search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we use the grayscale image with the cascade `detectMultiScale` method
    to get a list of matches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use a loop similar to the color-tracking behavior to find the largest
    rectangle by area. First, we need to set up a store for the current largest rectangle,
    in a data structure holding the area, then a sub-list containing the `x`, `y`,
    width, and height:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We return the position and dimensions of that largest rectangle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `make_display` method is simpler than the color-tracking behavior, as there
    is only one image. It must still encode the image, though:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `process_frame` method finds the object and then draws a rectangle on the
    frame for output. The `cv2.rectangle` function takes two coordinates: a starting
    `x`,`y` and an ending `x`,`y`, along with a color value. To get the ending coordinates,
    we need to add the width and height back in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now comes the `run` function. We start with the camera setup and warm-up time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Like the color-tracking behavior, we start the main loop by processing the
    frame and checking for control instructions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We only want to be moving if we''ve detected a large enough object (using height,
    as faces tend to be bigger in this dimension) and if the robot is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we know the robot is running, we feed the PIDs and send the output values
    straight to the servo motors for both pan and tilt. Note that to find the middle
    of the object, we take the coordinate and add half its width or height:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So that we can track what is going on here, a debug `print` statement is recommended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we need to add the code to set up and run our behavior. Notice that
    we still use the color-tracking template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the code ready, including the setup functions, we can try it out and see
    the behavior running.
  prefs: []
  type: TYPE_NORMAL
- en: Running the face-tracking behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run this behavior, you will need to have uploaded the color-tracking behavior
    files already:'
  prefs: []
  type: TYPE_NORMAL
- en: Upload the `face_track_behavior.py` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start using `$ python3 face_track_behavior.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send your browser to `http://myrobot.local:5001`. You should see a single frame
    of the camera, with a rectangular outline around the largest face.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must press the **Start** button for the robot to move.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The servo motors on the pan-and-tilt mechanism should move to try and put your
    face in the middle of the screen, which will mean the camera is pointed right
    at you. If you move your head around, the camera will (slowly) follow you. If
    you have someone stand behind you, the behavior won't pick them up, but if you
    cover half your face with your hand, it will stop recognizing you, and turn to
    their face instead.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start with the troubleshooting steps that we covered for the previous behavior—that
    should get you most of the way—then try these if you need to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the app fails to find the Haar cascade file, check the location for the
    files there. These files have moved between OpenCV packaging versions and may
    do so again. Check that you haven''t mistyped it. If not, then try the following
    command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command should show the location of the files on the Raspberry Pi.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the camera fails to detect faces in the picture, try making sure the area
    is well lit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The detection algorithm is only for faces that face the camera head-on, and
    anything obscuring a part of the face will fool it. It is a little picky, so glasses
    and hats may confuse it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faces only partially in the frame are also likely to be missed. Faces that are
    too far away or small are filtered. Reducing the minimum parameter will pick up
    more objects but generate false positives from tiny face-like objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please check the indentation matches, as this can change the meaning of where
    things happen in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have now made code that will detect and track faces in a camera view. Face-tracking
    behavior is sure to be impressive. Let's summarize what we've seen in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw how to set up the Raspberry Pi Camera module. You then
    used it to see what your robot sees—the robot's view of the world.
  prefs: []
  type: TYPE_NORMAL
- en: You got the robot to display its camera as a web app on a phone or desktop,
    and then used the camera to drive smart color- and face-tracking behaviors. I've
    suggested ways the behaviors could be enhanced and hopefully given you a taste
    of what computer vision can do.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will extend our object-tracking visual processing to
    follow lines with the camera, seeing further ways to use the camera.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This code is fun, but there are many ways you could improve the behaviors.
    Here are some suggested ways to extend this code and deepen your learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the control pipeline to allow a user to tune the color filters, correct
    radius, and PID values from the web page. Perhaps the initial PID values should
    be close to the other tunable values?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is quite a lot of setup code. Could you put this into a function/method?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Could the queues to the web page be used to send the debug data to the page,
    instead of printing them in the console? Could the data be plotted in a graph?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The field of view for tracking with the Pi Camera is pretty narrow. A wide-angle
    lens would improve the field of view a lot, letting the robot see more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The camera doesn't perform too well when it's dark. The robot has an LED strip,
    but it's not illuminating much. Could you add a bright LED as a headlamp for the
    camera?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could track other objects by trying the other cascade files found in the
    `/usr/share/opencv/haarcascades` folder on the Raspberry Pi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perhaps you could try swapping features of the two behaviors to use the servo
    motors to track the colored object, or chase the faces?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Could you combine the pan-and-tilt mechanism with the main wheels to track an
    object, then engage the main wheels to chase the matching face and aim to center
    the pan while keeping the object in view? This may require some fancy PID controller
    thinking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these ideas, you should have plenty of ways to dig further into this type
    of visual processing.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Visual processing is a deep topic, so this is only a small selection of places
    where you can read more about using a camera for visual processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Official Raspberry Pi Camera Guide* at [https://magpi.raspberrypi.org/books/camera-guide](https://magpi.raspberrypi.org/books/camera-guide)
    is an excellent resource for getting to know the camera, with many practical projects
    for it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To delve in far greater depth into using the Raspberry Pi Camera, I recommend
    the PiCamera documentation, available at [https://picamera.readthedocs.io/](https://picamera.readthedocs.io/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To gain insight into further techniques, the PyImageSearch website, at [https://www.pyimagesearch.com](https://www.pyimagesearch.com),
    has great resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCV and visual processing is a complex topic, only briefly covered here.
    I recommend *OpenCV 3 Computer Vision with Python Cookbook*, by *Alexey Spizhevoy*
    and *Aleksandr Rybnikov*, from *Packt Publishing*, available at [https://www.packtpub.com/application-development/opencv-3-computer-vision-python-cookbook](https://www.packtpub.com/application-development/opencv-3-computer-vision-python-cookbook),
    for more information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming video through Flask is a neat trick and is explored further in *Video
    Streaming with Flask*, at [https://blog.miguelgrinberg.com/post/video-streaming-with-flask](https://blog.miguelgrinberg.com/post/video-streaming-with-flask).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I recommend [https://flaskbook.com/](https://flaskbook.com/) for other neat
    ways to use Flask to manage your robot from your phone or laptop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning a PID controller—we touched on this in [*Chapter 11*](B15660_11_Final_ASB_ePub.xhtml#_idTextAnchor219),
    *Programming Encoders with Python*, and needed more in this chapter. *Robots For
    Roboticists* | *PID Control*, available at [http://robotsforroboticists.com/pid-control/](http://robotsforroboticists.com/pid-control/),
    is a little heavy on the math but has an excellent section on manually tuning
    a PID.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rapid Object Detection Using a Boosted Cascade of Simple Features*, by Paul
    Viola and Michael Jones, available at [https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf).
    This paper, from 2001, discusses in more detail the Haar cascade object-finding
    technique that we used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good video introducing face tracking is *Detecting Faces (Viola Jones Algorithm)
    – Computerphile*, available at [https://www.youtube.com/watch?v=uEJ71VlUmMQ](https://www.youtube.com/watch?v=uEJ71VlUmMQ),
    which dives into the combination of techniques used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cascade classification OpenCV documentation, at [https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html](https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html),
    gives a reference for the library functions used in the face-tracking behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenCV also has a tutorial on face tracking (for version 3.0), called *OpenCV:
    Face Detection using Haar Cascades*, which is available at [https://docs.opencv.org/3.3.0/d7/d8b/tutorial_py_face_detection.html](https://docs.opencv.org/3.3.0/d7/d8b/tutorial_py_face_detection.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
