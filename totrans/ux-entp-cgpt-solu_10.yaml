- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Fine-Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: What happens when prompt engineering efforts have gone as far as they can go?
    If higher quality results are still needed, examples are overwhelming the prompt,
    performance issues appear, or token costs are excessive because of a large prompt,
    **fine-tuning** comes into the picture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示工程的努力已经达到极限时会发生什么？如果还需要更高质量的结果，示例已经压倒了提示，出现性能问题，或者由于长提示而导致令牌成本过高，**微调**就变得重要起来。
- en: 'As mentioned in the last chapter, solutions sometimes require overlapping approaches
    such as **Retrieval-Augmented Generation** (**RAG**), prompt engineering, and
    fine-tuning. Fine-tuning helps the model improve its understanding. We will focus
    on a few critical deliverables before contextualizing them by completing the Wove
    case study started in [*Chapter 6*](B21964_06_split_000.xhtml#_idTextAnchor134),
    *Gathering Data – Content* *is King*:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如上章所述，解决方案有时需要重叠的方法，例如**检索增强生成**（**RAG**）、提示工程和微调。微调有助于模型提高其理解能力。在通过完成在[*第6章*](B21964_06_split_000.xhtml#_idTextAnchor134)开始的Wove案例研究来具体化之前，我们将关注几个关键成果：
- en: Fine-tuning 101
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调101
- en: Creating fine-tuned models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建微调模型
- en: Fine-tuning tips
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调技巧
- en: Wove case study, continued
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wove案例研究，继续
- en: Regardless of the tools, the team must care and feed the **large language model**
    (**LLM**) to improve the output. Though the methods discussed in the book can
    reach limits, fine-tuning is another excellent trick.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用什么工具，团队都必须关心并培养**大型语言模型**（**LLM**）以提升输出质量。尽管书中讨论的方法可以达到极限，但微调又是另一个出色的技巧。
- en: Fine-tuning 101
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调101
- en: Think of fine-tuning as teaching the solution how to approach a problem. You
    are not telling it the exact answers. That is for RAG. You coach the LLM on approaching
    issues, thinking about the solution, and how it should respond. Even though specific
    examples are used in fine-tuning, don’t expect it to use that exact example *ever*.
    It is just an example. Imagine we need it to be like a science teacher, so the
    LLM is told in prompts to *be a science teacher*, but if it needs to *sound like*
    an 8th-grade science teacher, share examples of what it is expected to sound like.
    Then, when these examples are added to the models, compare them against output
    examples and decide whether they are doing a good job. We will do this work using
    fine-tuning in the ChatGPT playground, as shown in *Figure 8**.1*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将微调视为教授解决方案如何解决问题的过程。你并没有告诉它确切的答案。那是RAG的任务。你指导LLM如何处理问题，思考解决方案，以及它应该如何回应。尽管在微调中使用了具体示例，但不要期望它总是使用那个确切的示例。它只是一个示例。想象一下，我们需要它像一个科学老师，所以LLM在提示中被告知要**成为一位科学老师**，但如果它需要**听起来**像一位八年级的科学老师，分享一些它应该听起来像的示例。然后，当这些示例添加到模型中时，将它们与输出示例进行比较，以判断它们是否做得好。我们将使用ChatGPT游乐场中的微调来完成这项工作，如图*图8.1*所示。
- en: '![](img/B21964_08_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_08_01.jpg)'
- en: Figure 8.1 – Fine-tuning in ChatGPT
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – ChatGPT中的微调
- en: We will walk through an example. This will give a feel for what is being built,
    how to contribute examples for training and testing, and what the results are
    when the model is improved with fine-tuning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步通过一个示例。这将让你了解正在构建的内容，如何为训练和测试提供示例，以及当模型通过微调得到改进时的结果。
- en: Prompt engineering or fine-tuning? Where to spend resources
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程或微调？在哪里投入资源
- en: We already know you need both, but if examples are added to the prompts, each
    use of the prompt will incur a cost because the input tokens cost money every
    time they are sent to the model. One trick is to move training data from prompts
    to fine-tuning. As new examples or edge cases are discovered, add them to improve
    the model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道你需要两者兼备，但如果在提示中添加示例，每次使用提示都会产生成本，因为每次将输入令牌发送到模型时都会产生费用。一个技巧是将训练数据从提示移动到微调。当发现新的示例或边缘情况时，将它们添加进去以改进模型。
- en: Start with prompt engineering and move on to fine-tuning
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从提示工程开始，然后进行微调
- en: The prompt engineering tools in [*Chapter 7*](B21964_07.xhtml#_idTextAnchor150),
    *Prompt Engineering,* give value and include a faster feedback loop than the more
    technical efforts needed for fine-tuning. Creating datasets and running training
    jobs takes more effort and time to see results. In enterprise use cases, both
    will be required. Responding to a fine-tuned model can be much less expensive
    and faster than responding to a large prompt with many examples to process each
    turn.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](B21964_07.xhtml#_idTextAnchor150)中的提示工程工具，*提示工程*，提供了价值，并包括比微调所需的更技术性的努力更快的反馈循环。创建数据集和运行训练作业需要更多的努力和时间才能看到结果。在企业用例中，两者都将需要。对微调模型的响应可能比响应包含许多示例的大型提示便宜得多，并且更快。'
- en: Token costs do matter
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记成本很重要
- en: 'It is expected to start with growing prompts by including examples of how the
    model should respond. There can be significant costs if large prompts are used
    tens of millions of times as each customer interacts with the LLM. Compare the
    following prompts with learning examples to a fine-tuned model that contains the
    same examples:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 预计将开始通过包含模型应该如何响应的示例来增长提示。如果使用大型提示，并且每个客户与LLM交互数十亿次，那么可能会产生显著的成本。将以下提示与学习示例与包含相同示例的微调模型进行比较：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Remove these from the prompt and add them to a fine-tuned model behind the
    scenes with the exact examples. This leaves the prompt like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些从提示中移除，并在幕后使用精确示例添加到微调模型中。这样提示就变成了这样：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The former includes about 500 tokens, using just 50 examples, while the prompt
    alone is 14 tokens. By moving the examples into a fine-tuned model, each turn
    will save 97% in input tokens. A fine-tuning model can cost more than a generic
    model. We can compare the input costs, as shown in *Table 8.1*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 前者包含大约500个标记，仅使用50个示例，而单独的提示就有14个标记。通过将示例移动到微调模型中，每次转换将节省97%的输入标记。微调模型可能比通用模型成本更高。我们可以比较输入成本，如*表8.1*所示。
- en: '| **Model** | **Costs** | **Cost for 10,000 prompts @ 500 tokens** **per prompt**
    | **Cost for 10,000 prompts @ 14 tokens** **per prompt** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **成本** | **每个提示500个标记的10,000个提示的成本** | **每个提示14个标记的10,000个提示的成本**
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **GPT-3.5** **Turbo fine-tuned** | $3.00 / 1M input tokens | $15.00(good
    results) | **$****0.42**(savings of 97%)(good results) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **GPT-3.5 Turbo微调** | $3.00 / 1M 输入标记 | $15.00(良好结果) | **$****0.42**(节省97%)(良好结果)
    |'
- en: '| **GPT-3.5 Turbo** | $0.50 / 1M input tokens | **$****2.50**(hard to improve
    results) | $0.07(hard to improve results) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **GPT-3.5 Turbo** | $0.50 / 1M 输入标记 | **$****2.50**(难以改进结果) | $0.07(hard
    to improve results) |'
- en: '| **GPT-4o mini** | $0.15 / 1M input tokens | $0.75 | $0.021 (a little over
    2 cents) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **GPT-4o mini** | $0.15 / 1M 输入标记 | $0.75 | $0.021 (略超过2分) |'
- en: Table 8.1 – Comparison of costs for models using fine-tuning and reducing prompt
    size
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 – 使用微调和减少提示大小模型的成本比较
- en: The generic model won’t be able to return the robustness of the fine-tuned model.
    Yet, the generic mode, with the collection of examples in the prompt, is still
    five times more expensive in this trivial example ($2.50 compared to 42 cents).
    Prompting is faster and great for getting started, but fine-tuning will be how
    to customize the model in many cases. Recall that a solution can include generic
    (cheap) models in conjunction with fine-tuned models. This is reasonable. The
    token cost for a prompt can be calculated using the OpenAI tokenizer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通用模型无法返回微调模型的鲁棒性。然而，在这个简单的例子中，通用模型（与42美分相比，为2.50美元）仍然贵五倍，因为提示中包含了示例集合。提示更快，非常适合入门，但微调将是许多情况下定制模型的方法。记住，一个解决方案可以包括通用（便宜）模型和微调模型。这是合理的。提示的标记成本可以使用OpenAI分词器计算。
- en: 'Demo: [Tokenizer](https://platform.openai.com/tokenizer) ([https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer))'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：[分词器](https://platform.openai.com/tokenizer) ([https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer))
- en: Even though cost will be considered, many use cases require a fine-tuned model.
    In this example, if the quality is there with GPT-4o mini with the small prompt
    and no training examples, then costs can be dramatically smaller. The use case
    will dictate the extent to which examples are needed for training. Let’s get started
    by learning how to build a fine-tuned model. The Playground supports this without
    coding.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 即使会考虑成本，许多用例仍然需要微调模型。在这个例子中，如果GPT-4o mini使用小型提示且没有训练示例，质量仍然存在，那么成本可以显著降低。用例将决定需要多少示例进行训练。让我们开始学习如何构建微调模型。Playground支持这一功能，无需编码。
- en: Creating fine-tuned models
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建微调模型
- en: Every model will have different needs. With GPT-3.5 Turbo, a start might be
    50 to 100 examples. After reaching the end of a good return on investment from
    prompt engineering, prompt chaining, and even function calling, we wind up here
    at fine-tuning. Because so many enterprise use cases will have at least some requirement
    for fine-tuned models, the best you can do is optimize for small context windows
    in exchange for more fine-tuning examples. The fine-tuned model costs the same,
    with 50 examples or 5000\. So, if you take a 3000 token prompt, move all the examples
    into the model, and leave a prompt of 300 tokens (a few paragraphs), that is a
    significant saving for each interaction. To put this in perspective, this paragraph
    has 173 tokens (766 characters).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都会有不同的需求。对于GPT-3.5 Turbo来说，起始点可能是50到100个示例。在达到提示工程、提示链和甚至函数调用的良好投资回报率之后，我们最终来到了微调这一步。由于许多企业用例至少会有一些对微调模型的要求，所以最好的办法是在交换更多微调示例的同时优化小上下文窗口。微调模型的成本相同，无论是50个示例还是5000个。因此，如果你使用3000个token的提示，将所有示例移入模型，并留下300个token的提示（几段文字），这将每次交互都带来显著的节省。为了更直观地说明这一点，这个段落有173个token（766个字符）。
- en: If fine-tuning doesn’t improve the model, the data science folks will likely
    have to figure out a different way of restructuring the model (OpenAI doesn’t
    give an example, but if all of these methods fail, ask ChatGPT for fine-tuning
    tips).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果微调没有提高模型，数据科学家可能需要想出不同的方法来重新构建模型（OpenAI没有给出例子，但如果所有这些方法都失败了，可以向ChatGPT寻求微调技巧）。
- en: 'Article: [When to use fine-tuning](https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning)
    ([https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning](https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning))'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[何时使用微调](https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning)
    ([https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning](https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning))
- en: Anyone can assist in fine-tuning. It takes more effort than prompt engineering,
    but the formats are accessible, and effort needs to be put into the content. As
    designers, writers, linguists, and product people, put on the customer content
    hat and get going.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 任何人都可以协助微调。这比提示工程需要更多的努力，但格式是可访问的，并且需要投入内容。作为设计师、作家、语言学家和产品人员，戴上客户内容帽，开始行动。
- en: 'Each model might have different formats. Here is the format for GPT-3.5 Turbo:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型可能具有不同的格式。以下是GPT-3.5 Turbo的格式：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'These are easy to model in a spreadsheet and can be reviewed by others and
    edited quickly. OpenAI also provides an example of multi-turn training data. Notice
    the weight key in their example. A weight of `0` means the model will ignore that
    specific message:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些很容易在电子表格中建模，并且可以被他人快速审查和编辑。OpenAI还提供了一个多轮训练数据的例子。注意他们例子中的权重键。权重为`0`意味着模型将忽略那条特定的消息：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will use the Playground for examples, but the development team will build
    a pipeline to manage the testing and training data in real life. Split example
    data between training and testing examples. Don’t include testing examples in
    the training set, or test results will be wrong. Hold out about 20% of the data
    for testing. You need to know whether the model is improving, and this data can
    be used to provide a benchmark.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用游乐场作为示例，但开发团队将在实际生活中建立一个管道来管理测试和训练数据。将示例数据分为训练和测试示例。不要将测试示例包含在训练集中，否则测试结果将不正确。保留大约20%的数据用于测试。你需要知道模型是否在改进，这些数据可以用来提供基准。
- en: Fine-tuning for style and tone
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风格和语调的微调
- en: Take prompt engineering as far as possible to train the system, but style and
    tone, format, and other qualitative features can be expressed with examples. In
    [*Chapter 1*](B21964_01.xhtml#_idTextAnchor016), *Recognizing the Power of Design
    in ChatGPT*, there is an example of a surf shop being compared to a bank. Instructions
    on talking like a surfer or performing tasks as a trusted business advisor for
    a prestigious international financial company will help. However, examples of
    how interactions sound and feel for a surf shop and a bank can help tweak that
    style, tone, and sophistication for the LLM persona.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能地将提示工程进行到底以训练系统，但风格、语调、格式和其他定性特征可以通过示例来表达。在[*第一章*](B21964_01.xhtml#_idTextAnchor016)《认识ChatGPT中设计的力量》中，有一个将冲浪店与银行进行比较的例子。关于如何像冲浪者一样说话或作为一家知名国际金融公司的受信任的商业顾问执行任务的说明将有所帮助。然而，冲浪店和银行交互的音调和感觉的例子可以帮助调整LLM角色的风格、语调和复杂度。
- en: '*Round 1* is the first experiment for fine-tuning a model with ten training
    examples. There is nothing special about this example. We need to showcase how
    fine-tuning works and how to read the results from the output. As this hands-on
    activity progresses, keep your use cases in mind. For actual work, start with
    prompt engineering and learn what doesn’t work there. Then, think about how to
    apply fine-tuning. Let’s get started with the example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一轮* 是使用十个训练示例微调模型的第一个实验。这个例子没有什么特别之处。我们需要展示微调是如何工作的，以及如何从输出中读取结果。随着这个动手活动的进行，请记住你的用例。对于实际工作，从提示工程开始，了解那里不奏效的地方。然后，考虑如何应用微调。让我们从以下例子开始：'
- en: 'Head to the playground and go to the fine-tuning tab: [https://platform.openai.com/finetune](https://platform.openai.com/finetune).'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往游乐场并转到微调标签页：[https://platform.openai.com/finetune](https://platform.openai.com/finetune)。
- en: Click the **+ Create** button to add a new data set.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**+ 创建**按钮添加新的数据集。
- en: 'This is the first file used that includes the training data:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是第一个包含训练数据的文件：
- en: 'GitHub: [Training Data with ten examples](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData10.jsonl)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData10.jsonl](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData10.jsonl))'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GitHub: [十个示例的训练数据](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData10.jsonl)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData10.jsonl](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData10.jsonl))'
- en: 'The example follows previous instructions for Alli, the sarcastic chatbot:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下示例遵循了之前为Alli这个讽刺聊天机器人提供的说明：
- en: '[PRE4]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Far. Like a quarter million
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 远。就像25万美元
- en: miles. Or about how far you
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 英里。或者你一生中能走多远
- en: might drive in a lifetime.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能会在一生中开车。
- en: Create the model within the **Create a fine-tuned model** dialog box. *Figure
    8**.2* shows selecting the **Base Model** (feel free to use the latest models;
    the cost won’t be an issue for this experiment), selecting the training data file,
    and the form is ready to submit. Notice that, at this point, the optional parameters
    are left alone. We will explain them in the upcoming *Fine-tuning tips* section.
    For now, don't include any validation data to test the model.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**创建微调模型**对话框中创建模型。*图8.2*显示了选择**基础模型**（你可以自由使用最新的模型；对于这个实验，成本不会成为问题），选择训练数据文件，表单已准备好提交。请注意，在此阶段，可选参数被保留。我们将在即将到来的*微调技巧*部分中解释它们。现在，不要包括任何用于测试模型的验证数据。
- en: '![](img/B21964_08_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_02.jpg)'
- en: Figure 8.2 – Setting up a fine-tuning job in OpenAI
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 在OpenAI中设置微调作业
- en: You will be returned to the fine-tuning page, and the job will take a few minutes
    to complete. The results should look similar to *Figure 8**.3*.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将被返回到微调页面，作业需要几分钟才能完成。结果应该看起来类似于*图8.3*。
- en: '![](img/B21964_08_03.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_03.jpg)'
- en: Figure 8.3 – Results from a fine-tuning job with ten examples (Round 1)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 使用十个示例的微调作业结果（第一轮）
- en: Notice the training chart. We aim for it to tend to zero as it moves to the
    right. The number of entries in the chart equals the number of training examples
    times the number of epochs or a single pass through the training data. The `10`.
    We will consider this number of iterations very high and a function of having
    so few training examples. We will explain this in more detail as this testing
    process continues.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意训练图表。我们的目标是当它向右移动时，逐渐接近零。图表中的条目数等于训练示例数乘以epoch数或训练数据的单次遍历。`10`。我们将认为这个迭代次数非常高，并且与训练示例数量很少有关。随着这个测试过程的继续，我们将更详细地解释这一点。
- en: Failure is an option
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 失败是一个选项
- en: Although the file shared work, the first time I did a fine-tuning job, it took
    me five tries to debug the typos. If the job fails, it will provide feedback.
    Fix it and try again. We will discuss third-party tools later in the chapter that
    help us avoid file formatting issues. To be fair to this process, the results
    are as shown. I have not gone back and made the results better or tweaked anything.
    The intent is to appreciate the process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然共享的文件工作，但我在第一次做微调作业时，调试错误需要尝试五次。如果作业失败，它将提供反馈。修复它并再次尝试。我们将在本章后面讨论帮助避免文件格式问题的第三方工具。为了公平地处理这个过程，结果就是这样。我没有回去改进结果或调整任何东西。意图是欣赏这个过程。
- en: Now that the basics are working, it is time to explain what happened. A base
    model with billions of parameters was selected and fine-tuned to understand how
    to respond in the manner defined in the file. You can test this model at any time
    on the **Playground** | **Chat** window by selecting it by the name assigned in
    the output model field. You can even copy the name to make it easier to find in
    the **Chat** drop menu, as shown in *Figure 8**.4*. You will need to enter the
    system instructions; *Alli is a factual chatbot that is also sarcastic*. This
    is ignored by the model, even though it is in the training file.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基本功能已经正常工作，现在是时候解释发生了什么。选择了一个拥有数十亿参数的基础模型，并对其进行微调以理解如何以文件中定义的方式做出响应。您可以通过在输出模型字段中选择其名称来在任何时候在**游乐场**
    | **聊天**窗口测试此模型。您甚至可以将名称复制下来，以便在**聊天**下拉菜单中更容易找到，如图*图 8*所示。您需要输入系统指令；*Alli 是一个既客观又带讽刺意味的聊天机器人*。尽管它在训练文件中，但模型会忽略这些指令。
- en: '![](img/B21964_08_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_04.jpg)'
- en: Figure 8.4 – Copy and paste the fine-tuned model name into the Chat window
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 将微调后的模型名称复制粘贴到聊天窗口
- en: Now, re-run the same set of tests, but this time, include this file for the
    validation data with the same training data as in *Figure 8**.5*. *Round 2* will
    take a few more minutes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，重新运行相同的测试集，但这次，包括与*图 8*.5 中相同的训练数据相同的验证数据文件。*第 2 轮*将需要更多时间。
- en: 'GitHub: [20 validation examples](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-ValidationData20.jsonl)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-ValidationData20.jsonl](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-ValidationData20.jsonl))'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'GitHub: [20 个验证示例](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-ValidationData20.jsonl)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-ValidationData20.jsonl](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-ValidationData20.jsonl))'
- en: '![](img/B21964_08_05.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_05.jpg)'
- en: Figure 8.5 – Creating a model with training data
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 使用训练数据创建模型
- en: The validation file is a collection that is *not* used for training but by the
    model to compare its results to what is expected. The validation data is very
    human-like, oddly similar to the training date, and not very logical, or clearly
    defined, such as classifying service requests by priority or determining sentiment.
    This means getting the charts to tend to zero will be hard.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 验证文件是一个集合，*不是*用于训练，而是由模型用来将其结果与预期值进行比较。验证数据非常像人类，奇怪地类似于训练数据，并且不太逻辑，或定义得不够明确，例如按优先级对服务请求进行分类或确定情感。这意味着让图表接近零将很困难。
- en: We can view the results and see what to do as the next steps in *Figure 8**.6*.
    We will take time to explain what is happening, but there are still a few training
    rounds.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 8*.6 中查看结果，并了解下一步要做什么。我们将花时间解释正在发生的事情，但仍有几轮训练。
- en: '![](img/B21964_08_06.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_06.jpg)'
- en: Figure 8.6 – Results from a fine-tuning job with a validation set (Round 2)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 使用验证集进行微调作业的结果（第 2 轮）
- en: 'Please open the file on GitHub and look at the examples; they are single-turn
    interactions. The user prompts, and the system responds. Each row looks like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请在 GitHub 上打开文件并查看示例；它们是单轮交互。用户提示，系统响应。每一行看起来像这样：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A multi-turn conversation (without the JSON formatting) would look like this
    example from ChatGPT 4o:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个多轮对话（不包含 JSON 格式）将像 ChatGPT 4o 的这个示例一样：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The model can be trained using the same basic format with multi-turn conversational
    interactions. In this OpenAI example, we highlighted the user and assistant responses
    that create a multi-turn interaction. Here is what it would look like when included
    in a training file:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以使用相同的基本格式进行训练，包括多轮对话交互。在这个 OpenAI 示例中，我们突出了用户和助手响应，这些响应创建了一个多轮交互。当包含在训练文件中时，它看起来是这样的：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This example set is not complex. Suppose the use case calls for multi-turn or
    extended examples to showcase how the model should react. Each entry should reflect
    a coherent dialogue that teaches the model to handle context over several interactions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例集并不复杂。假设用例需要多轮或扩展示例来展示模型应该如何反应。每个条目都应该反映一个连贯的对话，教会模型在多次交互中处理上下文。
- en: That is the input. Let’s review the output.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是输入。让我们回顾一下输出。
- en: Using the fine-tuned model
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用微调后的模型
- en: Once the fine-tuned model is ready, test it. You can see a **Playground** button
    on the bottom right of the fine-tuning job. You can also go to **Chat** in the
    left tab and then select the name of the fine-tuning job.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦微调模型准备就绪，对其进行测试。您可以在微调作业的右下角看到**游乐场**按钮。您也可以在左侧标签页中选择**聊天**，然后选择微调作业的名称。
- en: Optionally, there is a **Compare** button to manually test the model and compare
    it to previous model versions (or other models entirely). Be sure to copy the
    exact instructions in the **SYSTEM** field. We just clicked the **Playground**
    button to open the chat, selecting the fine-tuned model, as in *Figure 8**.7*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的，有一个**比较**按钮可以手动测试模型并将其与之前的模型版本（或完全不同的模型）进行比较。务必复制**系统**字段中的确切说明。我们刚刚点击了**游乐场**按钮来打开聊天，选择微调模型，如图**8.7**所示。
- en: '![](img/B21964_08_07.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_07.jpg)'
- en: Figure 8.7 – Selecting the fine-tuned job to interact with the new model
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 选择微调作业与新的模型进行交互
- en: Gotcha
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We can’t do both the file retrieval demo (to incorporate the 100s of FAQs from
    the Alli demo) *and* fine-tune the model in the Playground. This is a current
    limitation. We will use this time to focus on fine-tuning improvements, knowing
    a production system can address this shortcoming.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法同时进行文件检索演示（以整合Alli演示中的数百个常见问题解答）**并且**在游乐场中微调模型。这是一个当前的限制。我们将利用这段时间专注于微调改进，因为我们知道生产系统可以解决这个不足。
- en: Because of this limitation, this focus is on learning about fine-tuning rather
    than fine-tuning the RAG model. Comparing outputs, the foundation model is wordy,
    and neither has specific results from our FAQs; the model must be trained with
    FAQS. Compare the results in *Table 8.2* with the trained FAQs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个限制，这个重点是在学习微调而不是微调RAG模型。比较输出，基础模型文字较多，并且都没有我们常见问题解答的具体结果；模型必须使用常见问题解答进行训练。将**表8.2**中的结果与训练的常见问题解答进行比较。
- en: '| **Model** | **Response** |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **响应** |'
- en: '| --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **1) GPT-3.5 Turbo trained with** **the FAQs** | To lower your advisory fee,
    you can consider the following options:* Consolidating assets: The more you invest
    with us, the lower your advisory fee could be.* Grouping accounts together: You
    can also lower the advisory fee percentage for your immediate family members by
    grouping accounts together [1]. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **1) GPT-3.5 Turbo 使用** **常见问题解答** **训练** | 要降低您的咨询费用，您可以考虑以下选项：* 资产合并：您与我们投资的越多，您的咨询费用可能就越低。*
    将账户分组：您还可以通过将账户分组来降低您直系家庭成员的咨询费用百分比 [1]。|'
- en: '| **2) GPT-3.5-turbo-1105+** **fine-tuned** | You should contact customer service
    for that. They can escalate to the appropriate department if necessary. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **2) GPT-3.5-turbo-1105+** **微调** | 您应该联系客户服务。如果需要，他们可以升级到适当的部门。|'
- en: '| **3) GPT-4o No fine-tuning and no FAQs** **as resources** | Optimizing advisory
    fees can lead to significant savings over time. Here are several strategies to
    consider:1\. **Compare Advisors**: Research and compare fees among different advisors.
    Look for advisors with transparent fee structures.2\. **Consider Fee-only Advisors**:
    Fee-only advisors charge a flat rate or an hourly fee rather than a percentage
    of your assets. This can sometimes be more cost-effective.3\. **Negotiate Fees**:
    Don’t be afraid to negotiate. Advisors might be willing to reduce their fees to
    retain you as a client.4\. **Bundled Services**: Some advisors offer packaged
    services that might come at a lower total cost than à la carte services.5\. **Review
    Fee Structure**, 6\. **Use Robo-Advisors**, 7\. **DIY Approach**, 8\. **Regular
    Fee. (5 to 8 are truncated for brevity) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **3) GPT-4o 无微调和无常见问题解答** **作为资源** | 优化咨询费用可以随着时间的推移带来显著的节省。以下是一些可以考虑的策略：1.
    **比较顾问**：研究和比较不同顾问的费用。寻找具有透明费用结构的顾问。2. **考虑仅收费顾问**：仅收费顾问收取固定费用或按小时收费，而不是按您的资产百分比收费。这有时可能更经济。3.
    **协商费用**：不要害怕协商。顾问可能会愿意降低费用以保留您作为客户。4. **捆绑服务**：一些顾问提供打包服务，其总成本可能低于点菜服务。5. **审查费用结构**，6.
    **使用机器人顾问**，7. **DIY方法**，8. **定期费用**。（5到8项因简洁而省略）|'
- en: '| **4) GTP-3.5 1106 with fine-tuning** **78 examples** | Negotiate a lower
    fee or invest more money with me; I’m sure I can find a way to help you with that.
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **4) GTP-3.5 1106 微调** **78个示例** | 与我协商降低费用或投资更多资金；我相信我一定能找到帮助您的办法。|'
- en: '| **5) Best match from 78** **fine-tuning examples** | No question existed
    close to this question. Only one question even discussed advisory, and it was
    an unrelated question, “Can I pay Portfolio Advisory Service or other Alligiance-managed
    account fees using mobile check deposit?” |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **5) 来自78个** **微调例子**的最佳匹配 | 没有与这个问题相似的问题存在。只有一个问题甚至讨论了咨询，但它是一个无关的问题，“我可以用移动支票存款支付投资组合咨询服务或其他Alligiance管理的账户费用吗？”
    |'
- en: '| **6)** **Source Answer****FAQ 86\. How can Iower my** **advisory fee?** |
    consider the benefits of consolidating assets. the more you invest with us, the
    lower your advisory fee could be. you can also lower the advisory fee percentage
    for your immediate family members (whether or not they live with you) by grouping
    accounts together. for additional details, please ask a Alligiance representative
    for details or view the fee aggregation policy and form. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **6)** **源答案****FAQ 86\. 如何降低我的** **咨询费**？ | 考虑资产整合的好处。你与我们投资的越多，你的咨询费可能会越低。你也可以通过将账户合并在一起来降低你直系亲属（无论他们是否与你同住）的咨询费百分比。有关详细信息，请咨询Alligiance代表或查看费用汇总政策和表格。
    |'
- en: Table 8.2 – Output for “How can I lower my advisory fee?” with different models
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.2 – 使用不同模型对“如何降低我的咨询费？”的输出
- en: '*We must have the FAQs to offer specific business data*. Fine-tuning will only
    help present results in a particular way. Only the model (1) with the FAQs could
    match the source material. RAG (or, in this case, the proxy for RAG – File Search)
    can handle the factual data. The other models can hallucinate (2) or be long-winded
    (3). The fine-tuned model (4) was slightly sarcastic but couldn’t return a valid
    answer without the knowledge. It isn’t trained on the answer, as the closest example
    wasn’t close (5), and it can’t invent the facts from the source FAQ (6). The size
    of the result should be noted. Because training was with short responses, the
    model (5) returned short responses.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们必须有FAQ来提供特定的业务数据*。微调只能帮助以特定方式呈现结果。只有包含FAQ的模型（1）才能匹配原始材料。RAG（或者在这种情况下，RAG的代理
    – 文件搜索）可以处理事实数据。其他模型可能会产生幻觉（2）或长篇大论（3）。微调模型（4）略显讽刺，但如果没有知识，它无法给出有效答案。它没有在答案上进行训练，因为最接近的例子并不接近（5），而且它不能从原始FAQ中创造事实（6）。应该注意结果的大小。因为训练使用了简短的回答，所以模型（5）返回了简短的回答。'
- en: 'Since ChatGPT-3.5 wants at least 10, if not 100, examples, *Round 3* will re-run
    the build with double the examples. Doubling examples is a typical strategy to
    increase quality by the same amount as the last doubling. Ten was too small in
    this case, so now it is 20\. In hindsight, this experiment should have started
    at 50\. We used the exact simple system instructions. You need to copy and paste
    the instructions again when testing; they are ignored in the playground when uploading
    examples. Include this training set and reuse the same validation data:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ChatGPT-3.5至少需要10个例子，如果不是100个，因此*第三轮*将使用双倍的例子重新运行构建。将例子翻倍是典型的策略，通过与上一次翻倍相同的数量来提高质量。在这种情况下，10个例子太少，所以现在变成了20个。事后看来，这个实验应该从50个例子开始。我们使用了精确的简单系统指令。测试时需要再次复制和粘贴指令；当上传例子到沙盒时，这些指令会被忽略。包括这个训练集并重用相同的验证数据：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'GitHub: [Training Data with 30 examples](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData30.jsonl)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData30.jsonl](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData30.jsonl))'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'GitHub: [包含30个例子的训练数据](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData30.jsonl)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData30.jsonl](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData30.jsonl))'
- en: '*Figure 8**.8* shows the results of *Round 3*. We can now start to examine
    the metrics more closely.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8**.8*显示了*第三轮*的结果。我们现在可以更仔细地检查指标。'
- en: '![](img/B21964_08_08.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_08.jpg)'
- en: Figure 8.8 – Improving the model with double the training examples (Round 3)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 通过双倍训练例子改进模型（第三轮）
- en: 'We can explain a few more concepts with the validation data and then do one
    more round of training. Let’s review what the chart means:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用验证数据解释更多概念，然后进行最后一轮训练。让我们回顾一下图表的含义：
- en: The red dot represents the end of an epoch, which is one round of training.
    Because the last example had 29 examples, one epoch was 29 tests long. Because
    it decided it needed three runs, it did 87 tests. The red dot represents the average
    validation loss across that group. We are progressing since the validation loss
    is decreasing, and the training loss is tending to zero.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红点代表一个epoch的结束，即一轮训练。由于最后一个例子有29个示例，一个epoch有29次测试。因为它决定需要三次运行，所以它进行了87次测试。红点代表该组平均验证损失。由于验证损失在减少，训练损失趋于零，我们正在取得进展。
- en: We still see many ups and downs along the way. This model compares the expected
    outcome to its generated outcome. Once it improves with suitable matches, the
    graph tends to zero. When there is a big difference, the graph shows jumps. The
    large validation loss still needs to be solved. It still needs to converge towards
    zero.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仍然在过程中看到很多起伏。这个模型将预期结果与其生成的结果进行比较。一旦它通过合适的匹配得到改善，图表就会趋于零。当存在较大差异时，图表会显示跳跃。大的验证损失仍然需要解决。它仍然需要收敛到零。
- en: This model is looking a little volatile compared to Round 1\. I suspect the
    training data is too similar to the validation data, which is causing this issue.
    This would have to be reviewed and tested. A real solution might take dozens to
    hundreds of rounds of iteration.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与第一轮相比，这个模型看起来有点不稳定。我怀疑训练数据与验证数据过于相似，这导致了这个问题。这需要被审查和测试。真正的解决方案可能需要数十到数百轮的迭代。
- en: Don't generalize from the following Boolean classifier visualization. Graphs
    from simple Boolean classifiers (whether data is true or false, positive or negative
    sentiment, etc.) might not be helpful. If items are easy to classify, the graph
    will be like *Figure 8**.9*. Michael Cacic at [https://miha.academy/](https://miha.academy/)
    provided this example. Only a little can be learned from this chart. This model
    is working well.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不要从以下布尔分类器可视化中推广。来自简单布尔分类器（数据是真是假，正面或负面情绪等）的图表可能没有帮助。如果项目容易分类，图表将类似于*图8**.9*。Michael
    Cacic在[https://miha.academy/](https://miha.academy/)提供了这个示例。从这个图表中只能学到很少的东西。这个模型运行良好。
- en: '![](img/B21964_08_09.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_09.jpg)'
- en: Figure 8.9 – A fine-tuning graph for a classifier task that is doing well
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 一个表现良好的分类任务的微调图
- en: For complex data, like the Round 3 results, it is hopeful that the trend will
    be toward zero and that validation loss will decrease. Since there was a lack
    of convergence in the early run, add more varied examples to continue trending
    down. Improving the diversity of the validation examples (not done in this demo)
    would likely help.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于复杂的数据，如第三轮的结果，希望趋势会趋向于零，验证损失会减少。由于早期运行中缺乏收敛，添加更多样化的示例以继续下降趋势。提高验证示例的多样性（在本演示中没有完成）可能会有所帮助。
- en: Refrain from relying only on the graph for complex data. Review results and
    score them for quality. We discussed this in [*Chapter 4*](B21964_04.xhtml#_idTextAnchor085),
    *Scoring Stories*, and continue discussing measuring and monitoring in [*Chapter
    10*](B21964_10_split_000.xhtml#_idTextAnchor216), *Monitoring* *and Evaluation*.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要只依赖图表来分析复杂数据。审查结果并对它们进行质量评分。我们已经在[*第4章*](B21964_04.xhtml#_idTextAnchor085)“评分故事”中讨论了这一点，并在[*第10章*](B21964_10_split_000.xhtml#_idTextAnchor216)“监控和评估”中继续讨论了测量和监控。
- en: Remember, the goal is to improve the model’s reasoning, not to teach it knowledge.
    Use RAG for memory and scope. Use fine-tuning to hone how the model thinks and
    responds.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住，目标是提高模型的推理能力，而不是教授它知识。使用RAG进行记忆和范围。使用微调来磨练模型思考和响应的方式。
- en: Be careful about non-enterprise data intrusions
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 小心非企业数据入侵
- en: In instructions and prompting, specify to use the data provided in RAG. This
    protects from pulling facts from the model that might confuse customers. Although
    “Alli” is the short name for the fictional financial services model example, hallucinations
    occurred when the instructions, “Only provide answers from the attached document,”
    were removed from the file attached model. During some additional research for
    the book, this model assumed Alli was *Ally*, a bank in Pennsylvania. This error
    will only be found in the field by monitoring logs. Customers will complain about
    these errors, but it would be tragic to find this out *after* your customer mailed
    a large check to the wrong address because the foundational model used some random
    address. Every model vendor is working on this problem. It will get better but
    still watch for it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在指令和提示中，指定使用RAG提供的数据。这可以防止从模型中提取可能使客户困惑的事实。尽管“Alli”是虚构的金融服务模型示例的简称，但在从附件模型中移除“仅提供附件文档中的答案”的指令后，发生了幻觉。在为书籍进行的一些额外研究中，该模型假设Alli是*Ally*，一家位于宾夕法尼亚州的银行。这个错误只有在通过监控日志在实地被发现时才会被发现。客户会对此类错误进行投诉，但如果你发现客户因为基础模型使用了某个随机地址而将大额支票邮寄到错误地址，那将是悲剧性的。每个模型供应商都在努力解决这个问题。它将变得更好，但仍然要留意这个问题。
- en: 'Fine-tuning is well-suited for getting the style and tone right. Yes, good
    results can come from instructions, but fine-tuning is just that: it is fine;
    it is more granular and specific. *General* goals are in the prompts, while examples
    that could have been in the prompt can be moved to fine-tuning. These *specific*
    examples extend how the model should always respond. This is why the sarcastic
    example is so good. It was only trained on a few dozen examples, but it can now
    use those examples to drive its personality. Another use of fine-tuning is for
    manipulating data and transforming information.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 微调非常适合把握风格和语气。是的，好的结果可以来自指令，但微调正是如此：它是微调的；它更细致、更具体。在提示中，*一般*目标，而本可以包含在提示中的示例可以移动到微调中。这些*具体*示例扩展了模型应该如何始终响应。这就是为什么讽刺的示例如此之好。它只训练了数十个示例，但现在它可以利用这些示例来塑造其个性。微调的另一个用途是操纵数据和转换信息。
- en: Fine-tuning for structuring output
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化输出的微调
- en: 'In cases where tables, lists, or other formatting is essential to the results,
    feed the fine-tuning system with examples to give it an edge in providing structured
    output. This is perfect for those who deal with table data, like in the Wove use
    case. It is helpful for any structured data, even with the expense examples. It
    is also beneficial to train on the name: value pattern (e.g., Amount: $12.34).
    Fine-tuning can be used for integrations, where you move data from one system
    to another and specify the format for the input for the next step. You might need
    XML, JSON, CSV, colon-delimited, or other formats to ensure your downstream system
    can accept the input.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些表格、列表或其他格式对于结果至关重要的场合，向微调系统提供示例，以便它能在提供结构化输出方面获得优势。这对于处理表格数据的用户来说非常适用，例如在Wove用例中。对于任何结构化数据，即使是成本示例，这也很有帮助。在训练名称：值模式（例如，金额：$12.34）时，这也很有益。微调可用于集成，其中您将数据从一个系统移动到另一个系统，并指定下一步输入的格式。您可能需要XML、JSON、CSV、冒号分隔或其他格式，以确保您的下游系统可以接受输入。
- en: Tip for generating examples for fine-tuning
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 生成微调示例的小贴士
- en: ChatGPT can be used to generate examples to build a training model. Not only
    can it do the examples, but it can generate the structured format needed for the
    file. So, now ChatGPT is training itself. The output can be reviewed and tweaked.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT可以用来生成示例以构建训练模型。它不仅能做示例，还能生成文件所需的格式化结构。因此，现在ChatGPT正在自我训练。输出可以审查和调整。
- en: 'It also helps our tutorial to introduce another concept. You can instruct an
    LLM to create more examples. We call this **synthetic data**. This can be tried
    in the Playground. Notice the context, tone, and data format:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它还帮助我们的教程引入另一个概念。您可以指示一个LLM创建更多示例。我们称之为**合成数据**。这可以在Playground中尝试。注意上下文、语气和数据格式：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'More examples were provided; they are just truncated here. The model returned
    the results, and the next prompt was given to the model to make it easier to copy
    and paste the results. The prompt was:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了更多示例；这里只是截断了一部分。模型返回了结果，下一个提示被提供给模型，以便更容易地复制和粘贴结果。提示如下：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It returned a single file, making copying much easier, with results like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回了一个单独的文件，这使得复制变得容易得多，结果如下：
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Impressive. ChatGPT can help generate structured output but can’t generate factual
    information about my business. It saves a lot of time. We can use this method
    to scale up test cases. We can manually review for quality, saving 90% or more
    of our effort.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 值得称赞。ChatGPT可以帮助生成结构化输出，但不能生成关于我业务的实际信息。这节省了很多时间。我们可以使用这种方法来扩展测试用例。我们可以手动审查质量，节省90%或更多的努力。
- en: This is a tiny example. Transforming data to move between disparate systems
    has been an ongoing enterprise problem. This back-end process has various uses,
    including presenting data in a UI in a format or with a helpful structure for
    customers to interpret. The use cases in this space abound. But it will only be
    apparent when this problem strikes you. From my experience, it is not common.
    It is just critical when it comes up. Even though ChatGPT can generate data, it
    doesn’t guarantee quality.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个微小的例子。将数据转换以在不同系统之间移动一直是一个持续的企业问题。这个后端过程有多种用途，包括以格式或以有助于客户解释的结构在用户界面中展示数据。这个领域的用例繁多。但只有在问题出现时才会变得明显。根据我的经验，这种情况并不常见。但一旦出现就至关重要。尽管ChatGPT可以生成数据，但它不能保证质量。
- en: Generating data should still need a check and balance
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成数据仍然需要检查和平衡
- en: You can use a variety of methods to generate data. This synthetic data could
    be similar to what real humans might have done. Or it might not. This is a place
    where human-in-the-loop analysis can be valuable. A model can generate examples
    very quickly. It only took a few minutes with the LLM to create many examples.
    Even with the time to review the results, it was much easier than writing them
    by hand.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用各种方法来生成数据。这种合成数据可能与真实人类可能完成的数据相似。或者也可能不相似。这是一个人类在回路分析可以很有价值的地方。模型可以非常快速地生成示例。使用LLM只需要几分钟就能创建许多示例。即使有审查结果的时间，也比手动编写要容易得多。
- en: You can do this with Google Sheets, Microsoft Excel, and third-party fine-tuning
    tools that support generation. A wealth of integrations are available to help
    with this process. Regardless of the tool, review results and decide whether to
    include the content in training or validation examples. You might accept them
    outright, edit them to make them better or reject them. Depending on the solution’s
    robustness, consider a workflow that scores results, as discussed in [*Chapter
    4*](B21964_04.xhtml#_idTextAnchor085), *Scoring Stories*. Scoring tools can help
    evaluate what to keep, reject, and tweak. Then, plan a course of action to improve
    based on how you feel about the results. We see some options in *Table 8.3*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Google Sheets、Microsoft Excel以及支持生成的第三方微调工具来做这件事。有许多集成可以帮助这个过程。无论使用哪种工具，都要审查结果并决定是否将内容包含在训练或验证示例中。你可能直接接受它们，编辑它们以使它们更好，或者拒绝它们。根据解决方案的稳健性，考虑一个如第[*第4章*](B21964_04.xhtml#_idTextAnchor085)“评分故事”中讨论的评分工作流程。评分工具可以帮助评估保留、拒绝和调整的内容。然后，根据你对结果的感觉制定行动计划来改进。我们在*表8.3*中看到了一些选项。
- en: '| **Fine** **Tuning Status** | **Plan** |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **精细调整状态** | **计划** |'
- en: '| **Happy** **with results** | Do nothing |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **对结果** **满意** | 不采取行动 |'
- en: '| **Works well, but is expensive** **or slow** | Chain a fine-tuned lighter
    model (GPT-3.5) on all of the completions of a more expensive model (GPT-4) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **效果良好，但** **昂贵** **或缓慢** | 将微调后的轻量级模型（GPT-3.5）连接到更昂贵模型（GPT-4）的所有完成项 |'
- en: '| **Results are** **not consistent** | Chain a fine-tuned lighter model (GPT-3.5)
    on all of the best completions of a more expensive model (GPT-4) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **结果不** **一致** | 将微调后的轻量级模型（GPT-3.5）连接到更昂贵模型（GPT-4）的所有最佳完成项 |'
- en: '| **Results are close to what I want but the style and tone** **are off** |
    Manually edit examples to the desired quality, or edit the prompt to adjust results
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **结果接近我想要的结果，但** **风格和语气** **不合适** | 手动编辑示例以达到所需的质量，或者编辑提示以调整结果 |'
- en: '| **I don’t have a model, or can’t create** **one easily** | Fine-tune a model
    with manually generated high-quality examples |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **我没有模型，或者无法** **轻松创建** **一个模型** | 使用手动生成的高质量示例微调模型 |'
- en: Table 8.3 – Courses of action when tuning is not going as planned
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.3 – 调整未按计划进行时的行动方案
- en: Even this part of fine-tuning can undergo multiple care and feeding cycles.
    You might loop back around and find an even lighter model or something needing
    more editing. Iteration is fundamental to every step of the generative AI journey.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个微调的部分也可能需要经历多个精心照料和喂养周期。你可能需要回过头来找到一个更轻的模型或者需要更多编辑的内容。迭代是生成式人工智能旅程的每一步的基础。
- en: Spreadsheet user tips
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 电子表格用户技巧
- en: 'The format for fine-tuning has changed over time. Each model can use different
    formats. Just adapt the data to the model format. You can use a spreadsheet to
    maintain the source content and then use the tools in spreadsheets to build strings
    combining source content with the correct formatting, for example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，微调的格式已经发生了变化。每个模型都可以使用不同的格式。只需将数据适配到模型格式即可。您可以使用电子表格来维护源内容，然后使用电子表格中的工具构建字符串，将源内容与正确的格式结合起来，例如：
- en: '**A1 cell = '' {"messages": [{"role": "system", "content": "Alli is a factual
    chatbot that is also sarcastic."}, {"role": "user", "****content": "''**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**A1 单元格 = '' {"messages": [{"role": "system", "content": "Alli 是一个既客观又讽刺的聊天机器人。"},
    {"role": "user", "****content": "''**'
- en: '**B1 cell =** **question**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**B1 单元格 =** **问题**'
- en: '**C1 cell = ''"}, {"role": "assistant", "****content": "''**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**C1 单元格 = ''"}, {"role": "assistant", "****content": "''**'
- en: '**D1 contains the** **synthetic string**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**D1 包含** **合成字符串**'
- en: '**E1 cell = '' "}]}''**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**E1 单元格 = '' "}]}''**'
- en: '**So F1 = A1 & B1 & C1 & D1 & E1 then** **export F1**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**因此，如果 F1 = A1 & B1 & C1 & D1 & E1，那么** **导出 F1**'
- en: Excel and Google Sheets have ChatGPT (and other LLM) integrations to generate
    synthetic data. Spreadsheet integration with ChatGPT has all kinds of uses. It
    helps this process and can improve personal productivity.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Excel 和 Google Sheets 都有 ChatGPT（和其他 LLM）的集成来生成合成数据。与 ChatGPT 的电子表格集成有多种用途。它有助于这个过程，并可以提高个人生产力。
- en: One great trick is not rebuilding the model from scratch each time examples
    are added. After reviewing the generated results and fixing some formatting errors,
    49 more examples were incorporated from a separate file to add to the model. In
    total, there are now 78 examples and 20 test cases.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的技巧是每次添加示例时不必从头开始重建模型。在审查生成的结果并修复一些格式错误后，从另一个文件中又纳入了 49 个示例以添加到模型中。现在总共有
    78 个示例和 20 个测试用例。
- en: Re-use the model from *Round 3*. The training will go faster. As shown in *Figure
    8**.10,* pick the previous fine-tuned model in the `Base Model` menu to create
    a revised fine-tuned model. You are picking up where you left off. Only upload
    the incrementally new rows in the data for *Round 4*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 重新使用第 3 轮的模型。训练将更快。如图 8**.10** 所示，在“基础模型”菜单中选择先前的微调模型以创建修订的微调模型。您将继续之前的工作。只需上传第
    4 轮数据中增量新行。
- en: '![](img/B21964_08_10.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_10.jpg)'
- en: Figure 8.10 – Appending to an existing fine-tuned base model to continue fine-tuning
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 向现有的微调基础模型追加以继续微调
- en: 'GitHub: [49 training examples](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData49.jsonl)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData49.jsonl](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData49.jsonl))'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'GitHub: [49 个训练示例](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData49.jsonl)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData49.jsonl](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter8-Style-TrainingData49.jsonl))'
- en: We can now view the training, which shows the results from Round 4 in *Figure
    8**.11*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以查看训练结果，它显示了第 4 轮的*图 8**.11*结果。
- en: '![](img/B21964_08_11.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_11.jpg)'
- en: Figure 8.11 – The final fine-tuning run for ChatGPT-3.5 9 compared to ChatGPT-4o
    mini, including synthetic data (Round 4)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 与 ChatGPT-3.5 9 相比，ChatGPT-4o mini 的最终微调运行，包括合成数据（第 4 轮）
- en: Chat GPT 3.5 (on the top of the image) was improved by adding more examples.
    At least 50 to 100 examples were suggested; this is in the middle of that range.
    Take the 78 examples, double the training set, extend the testing set to 20%,
    review and clean up overlapping concepts, and test again. See if the next round
    will get the validation loss closer to zero. Looking at the results, it is better
    but not perfect. The slope of validation loss is trending down, but not as much
    as in more realistic data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Chat GPT 3.5（在图片上方）通过添加更多示例进行了改进。至少建议了 50 到 100 个示例；这在这个范围内。取 78 个示例，将训练集翻倍，将测试集扩展到
    20%，审查并清理重叠的概念，然后再次测试。看看下一轮是否能将验证损失更接近于零。查看结果，它更好但并不完美。验证损失的斜率正在下降，但不如在更真实的数据中下降得多。
- en: The output for ChatGPT-4o mini with the same data is included for comparison.
    The training loss is almost zero. The validation loss is still high and only slightly
    trending down (the red dots). Because OpenAI changed the vertical scale (bad design!)
    look carefully to compare the results. The second chart is scale is 25% different
    and thus the data is better than from ChatGPT-3.5\. Without more testing, it is
    hard to tell if this is acceptable for the data we trained it on.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 包含了与相同数据相同的ChatGPT-4o mini的输出，以供比较。训练损失几乎为零。验证损失仍然很高，并且仅略有下降趋势（红色点）。由于OpenAI改变了垂直刻度（设计不佳！），请仔细比较结果。第二个图表的刻度是25%不同，因此数据比ChatGPT-3.5更好。没有更多的测试，很难判断这对我们训练的数据是否可接受。
- en: 'If the results are not good, try other techniques. Consider some of these expert
    moves:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果结果不好，尝试其他技术。考虑以下一些专家动作：
- en: You can compare and test along the way. Each epoch generates a checkpoint. This
    file compares one checkpoint to another or even a different model. ChatGPT saves
    the last three checkpoints. *Figure 8**.12* zooms in on the checkpoints section.
    They are listed in the fine-tuning job and can then be selected, cut, and pasted
    into the chat, or mouse over them and jump directly into the playground using
    the link.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在过程中进行比较和测试。每个epoch都会生成一个检查点。这个文件比较一个检查点与另一个或甚至是不同的模型。ChatGPT保存了最后三个检查点。*图8.12*放大了检查点部分。它们列在微调作业中，然后可以选择、剪切和粘贴到聊天中，或者鼠标悬停在其上，通过链接直接跳转到游乐场。
- en: '![](img/B21964_08_12.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_08_12.jpg)'
- en: Figure 8.12 – Checkpoints can be directly opened in the playground
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – 检查点可以直接在游乐场中打开
- en: Notice that the final checkpoint was selected and copied to the clipboard. Clicking
    the **Playground** link on the first checkpoint opens it in the playground; a
    comparison model can be selected, as shown in *Figure 8**.13*. You can now paste
    the model path from the final checkpoint into the field as a shortcut.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，最后一个检查点已被选中并复制到剪贴板。点击第一个检查点的**游乐场**链接，它将在游乐场中打开；可以选择比较模型，如*图8.13*所示。现在，您可以将最终检查点的模型路径粘贴到字段中作为快捷方式。
- en: '![](img/B21964_08_13.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21964_08_13.jpg)'
- en: Figure 8.13 – Model names can be pasted into the model select field even in
    a comparison
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 模型名称甚至可以在比较中粘贴到模型选择字段中
- en: Now, you can compare results from the two models. The demo won’t reveal any
    exciting results, but this method is helpful for comparisons with large data sets.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，您可以比较两个模型的结果。演示不会揭示任何令人兴奋的结果，但这种方法对于与大数据集的比较很有帮助。
- en: Increase the default Epochs from 2 to 3 or 4 for a strict classifier. However,
    consider this only after it has enough examples.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将默认的Epochs从2增加到3或4以获得严格的分类器。然而，只有在它有足够的例子之后才考虑这一点。
- en: If the model is too loose, increase the epochs for additional training.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型太宽松，增加epochs进行额外训练。
- en: In this last round, the training loss increased. When re-running this model
    in Chat GPT-4o mini, the loss was much better (closer to zero). Look at more and
    better data to stick with this model and decrease training loss. As mentioned,
    the data is very similar and at risk of overfitting. Use synonyms, introduce more
    variation, and insert or delete words in the statement to scale up the variety
    and number of examples. The data scientists have far more approaches at their
    fingertips. These are too advanced for this book. But the intern knows the answer.
    Ask ChatGPT.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一轮中，训练损失增加了。当在Chat GPT-4o mini中重新运行此模型时，损失要好得多（更接近零）。查看更多和更好的数据，以保持此模型并减少训练损失。如前所述，数据非常相似，存在过拟合的风险。使用同义词，引入更多变化，并在陈述中插入或删除单词以扩大例子种类和数量。数据科学家有更多方法可供选择。这些方法对于本书来说太高级了。但实习生知道答案。问ChatGPT。
- en: '[PRE12]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Adding examples and expanding test cases will improve results. Continue to explore,
    grow test cases, improve the quality of the examples, play with the parameters,
    and learn. The best resource in my journey, outside of ChatGPT itself, was a four-hour
    training masterclass from Mihael Cacic. It was the most valuable of all resources,
    and it is recommended (I am not compensated for this; I was just a student). It
    is perfect for product people. It is the right level for an introduction class.
    Check it out.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 添加例子和扩展测试用例将提高结果。继续探索，增长测试用例，提高例子质量，玩转参数，并学习。在我旅程中，除了ChatGPT本身之外，最好的资源是一个来自Mihael
    Cacic的四个小时的培训大师班。这是最有价值的资源之一，并且推荐（我没有为此获得报酬；我只是个学生）。它非常适合产品人员。这是入门课程的正确水平。去看看。
- en: 'Training: [Miha’s training website](https://miha.academy/) ([https://miha.academy/](https://miha.academy/))'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 训练：[Miha的训练网站](https://miha.academy/) ([https://miha.academy/](https://miha.academy/))
- en: 'Entry Point, his company, also has tools that support speeding up the training
    process and experimenting with fine-tuning jobs across multiple LLMs. You can
    use Entry Point, connect to OpenAI and other LLM vendors, and never deal with
    the JSON format for fine-tuning. Keep this in mind: tools help reduce the complexity
    of model management. New tools are becoming available every day.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Entry Point，他的公司，也有支持加速训练过程和跨多个LLM进行微调作业实验的工具。你可以使用Entry Point，连接到OpenAI和其他LLM供应商，并且永远不需要处理微调的JSON格式。请记住：工具有助于降低模型管理的复杂性。每天都有新的工具出现。
- en: Look at Vijay’s article for more on fine-tuning and the different types of losses.
    It discusses metrics, is a good resource, and only takes a few minutes to read.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 查看Vijay的文章，了解更多关于微调和不同类型损失的信息。它讨论了指标，是一个很好的资源，只需几分钟就能阅读。
- en: 'Article: [Training vs. Validation Loss](https://medium.com/@penpencil.blr/what-is-the-difference-between-training-loss-validation-loss-and-evaluation-loss-c169ddeccd59)
    by Vijay M (https://medium.com/@penpencil.blr/what-is-the-difference-between-training-loss-validation-loss-and-evaluation-loss-c169ddeccd59'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[训练损失与验证损失](https://medium.com/@penpencil.blr/what-is-the-difference-between-training-loss-validation-loss-and-evaluation-loss-c169ddeccd59)
    by Vijay M (https://medium.com/@penpencil.blr/what-is-the-difference-between-training-loss-validation-loss-and-evaluation-loss-c169ddeccd59)
- en: Having gone end to end with a fine-tuning model, there are other areas to explore
    besides conversational style and tone. In the enterprise, connecting to different
    data sources to gather information and push results also exists. You will need
    function calling.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成微调模型的全过程后，除了对话风格和语气之外，还有其他领域可以探索。在企业中，连接到不同的数据源以收集信息和推送结果也是存在的。你需要函数调用。
- en: Fine-tuning for function and tool calling
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 功能和工具调用的微调
- en: When passing data back and forth from existing systems, it is typical to conform
    to the other systems’ formats because these legacy systems are likely to stay
    the same for you. The most recent models are getting better at matching the `tool_choice`,
    and it is set to `auto`. You can force this process by setting it to `required`,
    specifying a specific named function, or telling it `none` to disable it.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在从现有系统传递数据时，通常需要遵循其他系统的格式，因为这些遗留系统可能对你来说会保持不变。最新的模型在匹配`tool_choice`方面变得越来越好，并且默认设置为`auto`。你可以通过将其设置为`required`，指定一个特定的命名函数，或者告诉它`none`来禁用它。
- en: There is little for us to do on the product side here. It is included for completeness.
    Of course, these interactions should be monitored to ensure the proper functions
    are called. This requires additional training data to differentiate between similar
    functions, making sure that data is mapped to the correct fields when there is
    a collection of similar fields (for example, total price, discounted price, itemized
    prices, tax, and other dollar values), and to verify that the model extracted
    data correctly (not too much, not too little, but just right).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在产品方面，我们这里几乎没有可以做的。这是为了完整性而包含的。当然，这些交互应该被监控以确保正确调用功能。这需要额外的训练数据来区分相似的功能，确保在存在相似字段集合（例如，总价、折扣价、明细价格、税费和其他美元价值）时数据映射到正确的字段，并验证模型提取的数据是否正确（不多也不少，恰到好处）。
- en: 'Product people should be aware of one neat trick: **parallel function calling**.
    This allows multiple queries to be sent in parallel, dramatically reducing the
    response time with no additional model cost. *There is no such thing as a slow,
    good user interface*. People today are impatient.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 产品人员应该注意一个巧妙的小技巧：**并行函数调用**。这允许并行发送多个查询，无需额外模型成本即可显著减少响应时间。*没有所谓的慢速但好的用户界面*。如今的人们都很不耐烦。
- en: One gotcha. OpenAI suggests that SQL generation is “not perfectly reliable.”
    Therefore, use caution and extensive testing and monitoring. I have not had success
    building AI SQL generators from text input, but this will improve over time.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要注意的问题。OpenAI建议SQL生成“并不完全可靠”。因此，请谨慎使用，并进行广泛的测试和监控。我还没有成功从文本输入构建AI SQL生成器，但这一点会随着时间的推移而改进。
- en: 'Documentation: [Calling functions with chat models](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)
    ([https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models))'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 文档：[使用聊天模型调用函数](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)
    ([https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models))
- en: Of all the ways to use fine-tuning, plenty of tips can help with this process.
    We cover some of the critical items.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有使用微调的方法中，有很多技巧可以帮助这个过程。我们涵盖了一些关键事项。
- en: Fine-tuning tips
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调技巧
- en: 'You must care for and feed the fine-tuned set to improve training quality (based
    on the metrics or experience with certain test cases). Here is a summary of OpenAI’s
    suggestions for fine-tuning:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须关心并喂养微调集以改善训练质量（基于指标或某些测试案例的经验）。以下是OpenAI对微调的建议总结：
- en: '**Review existing examples for issues**: You might have introduced style, logic,
    or grammar issues into the dataset, including examples with errors. Review the
    material against how the model performed before and after adding the data. You
    can use the epoch checkpoints as a tool.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**审查现有示例以发现问题**：你可能已经将风格、逻辑或语法问题引入数据集，包括包含错误示例。将材料与模型在添加数据前后的表现进行对比。可以使用epoch检查点作为工具。'
- en: '**Gather more examples to fill the gaps**: Additional training examples might
    show the model how to address gaps in its abilities. It is always hard to say
    how much is too much.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收集更多示例来填补空白**：额外的训练示例可能表明模型如何解决其能力上的空白。总是很难说多少是过多的。'
- en: '**Include examples with errors**: Sometimes, it is best to learn from the master.
    Let’s ask ChatGPT about including mistakes in fine-tuned examples:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**包含包含错误的示例**：有时，从大师那里学习是最好的。让我们询问ChatGPT关于在微调示例中包含错误的问题：'
- en: '[PRE13]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Look for imbalance and the need for more diversity in data**: Does the data
    set provide a range of examples? Refrain from biasing data with one kind of answer.
    It is not helpful to rephrase the same question. Think about the expense examples.
    Not just data format but style, idiomatic language, lack of data, too much data,
    and even irrelevant data. Even the 78 sarcastic examples for the four rounds of
    testing were a little homogenous. This is something the team has to notice.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**寻找数据不平衡和更多样化的需求**：数据集是否提供了各种示例？避免使用一种答案来偏置数据。重述相同的问题并没有帮助。考虑费用示例。不仅仅是数据格式，还有风格、惯用语、数据不足、数据过多，甚至无关数据。即使是四轮测试中的78个讽刺性示例也略显单一。这是团队必须注意的事情。'
- en: Imbalance example
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡示例
- en: I had a case that puzzled me for a while. We had two primary tasks that a conversational
    assistant did. It did expenses and time reporting. However, both were needed in
    the same assistant. The training data was biased toward the expense assistant’s
    round numbers and needed the correct numbers in the time reporting model. While
    a customer might say “15 minutes,” “30 minutes,” or “an hour and 45 minutes” for
    a time record, most expenses are not “$15,” “$30,” or “$45”, so by overweighting
    on those round numbers in expenses it pushed understanding for the typical time
    units towards the expense model. This bias can be improved by using better data
    for the expense model and ensuring the time model has the right balance of examples.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经遇到过一个让我困惑了一段时间的案例。我们有两个主要任务是由一个对话助手完成的。它负责费用和时间报告。然而，这两个任务都需要同一个助手来完成。训练数据偏向于费用助手的四舍五入数字，而时间报告模型需要正确的数字。虽然客户可能会说“15分钟”、“30分钟”或“1小时45分钟”作为时间记录，但大多数费用并不是“$15”、“$30”或“$45”，因此，在费用中过度强调这些四舍五入的数字，推动了典型时间单位对费用模型的理解。这种偏差可以通过为费用模型使用更好的数据并确保时间模型有正确的示例平衡来改善。
- en: '**Match the training examples to expectations**: If it is expected that most
    people will give the information needed to ask a question, fill out a form, or
    move forward, then training examples should mirror those examples.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使训练示例与预期相匹配**：如果预期大多数人会提供所需的信息来提问、填写表格或前进，那么训练示例应该反映这些示例。'
- en: '**Validate the consistency of the examples when coming from multiple sources**:
    Even humans are expected to disagree on specific values when labeling data. If
    one person tags a company name as “The Business Center” and the other as “Business
    Center,” then the model can likely have similar issues.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**验证来自多个来源的示例一致性**：即使人类在标记数据时也可能会对特定值有不同的看法。如果一个人将公司名称标记为“The Business Center”，而另一个人标记为“Business
    Center”，那么模型可能也会出现类似的问题。'
- en: Training examples should be in the same format.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练示例应采用相同的格式。
- en: Judge improvements based on previous improvements.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据之前的改进来判断改进。
- en: '**Edge case issues take work to include**: As a rule of thumb, expect a similar
    improvement for each doubling of quality training data. But it is more work to
    find the edge cases than the happy paths. Take the time to express those edge
    cases. It will make the fine-tuned model more robust. Edge cases could be in several
    attributes: length of the question, use of multiple languages in one question,
    multi-part complex questions, lots of chit chat with a small portion for the actual
    question, the use of data that is not well formatted or in a format not expected
    (military time, currencies with extra numbers), written words for numbers, or
    just plain wrong information. Judge this by comparing the testing results between
    the fully tuned model and one using one-half of the training data. Use this to
    judge future stepwise improvements.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**边缘情况问题需要工作才能包括**：作为一个经验法则，预期质量翻倍时会有类似的改进。但是，找到边缘情况比找到快乐路径要困难得多。花时间表达这些边缘情况。这将使微调后的模型更加稳健。边缘情况可能涉及多个属性：问题的长度、一个问题中使用多种语言、多部分复杂问题、大量闲聊与实际问题的少量部分、使用格式不佳或格式不符合预期的数据（军事时间、带有额外数字的货币）、用文字表示数字，或者仅仅是错误的信息。通过比较完全微调的模型和仅使用一半训练数据的模型之间的测试结果来判断这一点。用这个来判断未来的逐步改进。'
- en: '**Specify and adjust the hyperparameters**: Start with the defaults set by
    OpenAI. If the model doesn’t follow the training data, increase epochs by one
    or two. Notice how they were at five in the Wove case study coming up next. This
    is more typical for classification, entity, extraction, or structured parsing
    tasks. All of these have a known answer. With more wide-ranging model tasks, decrease
    the epochs by 1 or 2 to see if that improves diversity.'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**指定和调整超参数**：从OpenAI设定的默认值开始。如果模型不遵循训练数据，可以增加一两个epoch。注意接下来Wove案例研究中它们是五。这在分类、实体提取或结构化解析任务中更为典型。所有这些都有已知答案。对于更广泛的模型任务，减少1或2个epoch以查看是否提高了多样性。'
- en: 'If the model isn’t converging, increase the **learning rate** (**LR**) multiplier.
    The graphs from the Wove case study are in the next section. *Figure 8**.5* shows
    this convergence issue, as does the Wove example, which is covered shortly. Only
    make small changes at one time. Here is some background:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型没有收敛，增加**学习率**（**LR**）乘数。Wove案例研究的图表在下一节中。*图8**.5*显示了这个问题，Wove的例子也将在稍后介绍。一次只做小的改变。以下是一些背景信息：
- en: 'Article: [Learning Rate](https://www.chatgptguide.ai/2024/02/25/what-is-learning-rate/)
    ([https://www.chatgptguide.ai/2024/02/25/what-is-learning-rate/](https://www.chatgptguide.ai/2024/02/25/what-is-learning-rate/))'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文章：[学习率](https://www.chatgptguide.ai/2024/02/25/what-is-learning-rate/) ([https://www.chatgptguide.ai/2024/02/25/what-is-learning-rate/](https://www.chatgptguide.ai/2024/02/25/what-is-learning-rate/))
- en: One issue is overfitting. Mihael from Entry Point provided the example in *Figure
    8**.14*, in which the validation loss continued to grow while the training loss
    was acceptable. This is a classic overfitting example.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个问题是过拟合。Entry Point的Mihael在*图8**.14*中提供了例子。14，其中验证损失持续增长，而训练损失是可以接受的。这是一个经典的过拟合例子。
- en: '![](img/B21964_08_14.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_14.jpg)'
- en: Figure 8.14 – An example of overfitting is shown with the increase in validation
    loss
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – 使用验证损失增加显示过拟合的例子
- en: The analogy is studying for a test by memorizing the exact questions and answers
    from a practice test. Then, in the actual test, none of the questions are similar
    enough to allow the taker to answer correctly. Aligned too closely with the study
    material, fails to translate to answering correctly.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 类比是通过对练习测试中的确切问题和答案进行记忆来为考试做准备。然后，在实际考试中，没有足够相似的问题让考生正确回答。与学习材料过于接近，无法转化为正确回答。
- en: '**Use a second model to verify a fine-tuned model**: One approach is to use
    a second model to test the first model to confirm results. The second model might
    be the same or completely different fine-tuned or off-the-shelf LLM. Set a quality
    threshold; if the AI answer fails, it might route the request to a human customer
    service agent. It takes some experience to figure this out.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用第二个模型验证微调模型**：一种方法是用第二个模型测试第一个模型以确认结果。第二个模型可能是相同的或完全不同的微调或现成的LLM。设定一个质量阈值；如果AI答案失败，它可能会将请求路由到人工客户服务代表。这需要一些经验才能弄清楚。'
- en: If, after all this work, the assistants’ style or tone needs to change, all
    of the examples don’t necessarily have to change. Consider tweaking the prompts
    to override the examples. If it is just a tweak, the fine-tuning is not going
    to waste; it is still helping give it the experience.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在这项工作之后，助手的风格或语气需要改变，所有示例不一定都需要改变。考虑调整提示以覆盖示例。如果只是微调，微调就不会浪费；它仍然有助于提供体验。
- en: 'This is a vast area for research, testing, and learning. This is only a start
    to ensure you can apply these skills at the team level. We hope it is easy to
    appreciate how much the discussion impacts the quality of the user experience.
    Here is one more resource as a companion to the earlier OpenAI Fine-tuning documentation:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个广泛的研究、测试和学习领域。这只是一个开始，以确保你可以在团队层面应用这些技能。我们希望你能容易地理解讨论对用户体验质量的影响有多大。这里还有一个资源作为之前OpenAI微调文档的补充：
- en: 'Article: [ChatGPT Fine Tuning Documentation](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations)
    ([https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations))'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：[ChatGPT微调文档](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations)
    ([https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations))
- en: We want to examine how everything learned about prompt engineering and fine-tuning
    works for the Wove use case.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想检验关于提示工程和微调的所有学习内容如何适用于Wove的使用案例。
- en: Wove case study, continued
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wove案例研究，继续
- en: In [*Chapter 6*](B21964_06_split_000.xhtml#_idTextAnchor134), *Gathering Data
    – Content is King,* the Wove case study on data cleansing for rate sheets used
    by freight forwarders was kicked off. They had to scrub the data before ingesting
    it to output a clean, unified view of all the rates from many carriers. Now, it
    is time to explore their prompt engineering (we covered the basics in [*Chapter
    7*](B21964_07.xhtml#_idTextAnchor150), *Prompt Engineering*) and fine-tuning efforts
    for this solution.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B21964_06_split_000.xhtml#_idTextAnchor134)，“数据收集——内容为王”，Wove关于货运代理人使用的费率表数据清理案例研究开始了。他们必须在摄取之前清理数据，以输出来自许多承运人的所有费率的干净、统一视图。现在，是时候探索他们为这个解决方案的提示工程（我们在[*第7章*](B21964_07.xhtml#_idTextAnchor150)，“提示工程”）和微调努力了。
- en: Prompt engineering
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程
- en: 'They want the LLM to think like a customer who does this step manually. They
    created the prompt for the spreadsheets during the ingestion process (this early
    version was shared with us to maintain the proprietary nature of their latest
    efforts):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 他们希望大型语言模型（LLM）能像手动执行这一步骤的客户一样思考。他们在摄取过程中为电子表格创建了提示（这个早期版本与我们分享，以保持他们最新努力的专有性质）：
- en: '[PRE14]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s review a few highlights from this prompt:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这个提示的几个亮点：
- en: It sets the stage for the persona to be adopted.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为采用角色设定了舞台。
- en: It gives context for understanding tables.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为理解表格提供了上下文。
- en: It explains the input (snippets of text and more details)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它解释了输入（文本片段和更多细节）
- en: It helps with some exceptions (ambiguous columns)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有助于一些例外情况（模糊的列）
- en: It defined the response format (**Yet Another Markup** **Language** (**YAML**))
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它定义了响应格式（**另一种标记语言** **（YAML**））
- en: 'Great job following the guidance! Honestly, they were already following this
    same advice. They use a lot of fine-tuned models, so examples don’t appear in
    their prompts. One small thing: where is the idea of an emotive prompt? They should
    explore if that helps their quality. We can talk to them about that!'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 非常棒，遵循了指导！说实话，他们已经遵循了同样的建议。他们使用了大量微调的模型，所以示例不会出现在他们的提示中。一个小问题：情感提示的想法在哪里？他们应该探索这是否能帮助他们提高质量。我们可以和他们谈谈这个话题！
- en: Fine-Tuning for Wove
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Wove的微调
- en: As discussed in the last chapter, Wove has a collection of models to perform
    specific tasks in cleaning the spreadsheet data. The fine-tuning process adapts
    generic models to improve the understanding of rate spreadsheets critical to Wove’s
    customers. It is similar to teaching a 5th grader a new subject. A 5th grader
    knows the basic language and can answer simple questions—they might even be into
    ships and trains and understand the concept of moving goods, but no 5th grader
    has ever seen a rate sheet.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一章所述，Wove有一系列模型来执行清理电子表格数据的特定任务。微调过程将通用模型调整为提高对Wove客户至关重要的费率表格的理解。这类似于教一个五年级学生一门新学科。五年级学生知道基本语言并能回答简单问题——他们甚至可能对船只和火车感兴趣，并理解货物移动的概念，但没有任何五年级学生见过费率表。
- en: 'We know that a model can only provide so much. ChatGPT recommends starting
    with prompt engineering before going to fine-tuning. In conversations, Kevin Mullet
    suggested an excellent way to remember this: “*First*, figure out what to say,
    *then* figure out how to say it.” We have shown how this can help, but this extra
    effort is needed.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道模型只能提供这么多。ChatGPT 建议在微调之前先进行提示工程。在对话中，Kevin Mullet 提出了一个很好的记住这个方法的方式：“*首先*，弄清楚要说什么，*然后*弄清楚如何表达。”我们已经展示了这如何有所帮助，但需要额外的努力。
- en: 'Here are some checks that Wove does to verify that the data is being processed
    correctly. This covers data quality, prompt quality, and fine-tuning:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些 Wove 做的检查，以验证数据是否被正确处理。这包括数据质量、提示质量和微调：
- en: They manually review the interpretation of the data to look for hallucinations.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们手动审查数据的解释，以寻找幻觉。
- en: 'They look for validation and training loss converging to zero. They run additional
    evaluation data sets using OpenAI Evals to ensure that models pass established
    tests. OpenAI provides a collection of templates to evaluate models using standard
    measures. The evals allow judgment of how different model versions and prompts
    impact usage. Here is a good introduction:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们寻找验证和训练损失收敛到零的情况。他们使用 OpenAI Evals 运行额外的评估数据集，以确保模型通过既定的测试。OpenAI 提供了一系列模板，用于使用标准度量评估模型。evals
    允许判断不同模型版本和提示对使用的影响。以下是一个很好的介绍：
- en: 'Article: [Getting Started with OpenAI Evals](https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals)
    ([https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals](https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals))'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文章：[开始使用 OpenAI Evals](https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals)
    ([https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals](https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals))
- en: They use multiple steps in a chain and different models to focus on doing one
    thing well. They regularly revisit models to adopt newer and cheaper ones.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们在一个链中使用了多个步骤和不同的模型，以专注于做好一件事。他们定期回顾模型，采用更新且更便宜的模型。
- en: In one of the models, they do location mapping and use over 600 training examples.
    10% is for validation data, but for some models, they bump up to 20%, depending
    on how expensive it is to generate the data.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其中一个模型中，他们进行位置映射并使用了超过 600 个训练示例。10% 是用于验证数据，但对于某些模型，如果生成数据成本较高，他们会将其提高到 20%。
- en: Their training graph in *Figure 8**.15* looks good. It is converging to zero
    for training loss. They have a validation test set that works well and uses what
    appear to be default parameters.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的训练图在 *图 8**.15* 中看起来不错。训练损失正在收敛到零。他们有一个工作良好的验证测试集，并使用看起来像是默认参数。
- en: '![](img/B21964_08_15.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_15.jpg)'
- en: Figure 8.15 – An example of a training model converging
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 – 训练模型收敛的一个示例
- en: Initially, they had learning rate issues. In machine learning and statistics,
    the learning rate is a tuned parameter in an optimization algorithm that determines
    the iteration step size while moving toward a minimum loss function.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始时，他们遇到了学习率问题。在机器学习和统计学中，学习率是优化算法中的一个调整参数，它决定了在向最小损失函数移动时的迭代步长。
- en: Even if trained again, it never quite gets to the optimal point. Sometimes,
    there is poor convergence or overfitting, as discussed. *Figure 8**.16* shows
    an earlier run showing a lack of convergence.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 即使再次训练，也永远无法达到最佳点。有时，会出现收敛不良或过拟合，如前所述。*图 8**.16* 展示了一个早期运行，显示了缺乏收敛。
- en: '![](img/B21964_08_16.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21964_08_16.jpg)'
- en: Figure 8.16 – An example of an early Wove training model that didn’t converge
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 – 一个早期 Wove 训练模型未收敛的示例
- en: If there is bouncing and the lines do not quite converge, retrain with a lower
    learning rate. Convergence will be much slower if the initial weights are too
    high. To put models in perspective, the four rounds for the sarcastic chatbot
    experiment ran with less than 5000 training tokens for each round. The Wove model
    above used over 500,000 training tokens for this one piece of their solution.
    Bigger doesn’t guarantee convergence.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有波动且线条没有完全收敛，则使用较低的学习率重新训练。如果初始权重过高，收敛将会慢得多。为了将模型置于正确的视角，讽刺聊天机器人实验的四轮运行中，每轮的训练标记少于
    5000。上面的 Wove 模型为这个解决方案的这一部分使用了超过 500,000 个训练标记。更大并不保证收敛。
- en: 'Our friends at Wove have a few more tips for those ingesting spreadsheets:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Wove 的朋友们为那些正在处理电子表格的人提供了一些额外的建议：
- en: They use expensive models to fine-tune cheaper models. For example, Anthropic’s
    Opus 3 Claude is (at this time of writing) about 30 to 50 times more costly than
    OpenAI 3.5\. Opus 3 is 15$/1M input and 75$/1M output tokens versus ChatGPT 3.5
    turbo-0125 at $0.50/1M input and $1.50/1M output tokens. This is an essential
    point for the product team. You want to get the best bang for the buck, especially
    when dealing with customers who will use the model more often if it provides excellent
    service. They found significantly better quality from fine-tuning ChatGPT 4.0
    than earlier 3.5 models. Chat GPT-4o mini is now being incorporated.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们使用昂贵的模型来微调更便宜的模型。例如，Anthropic的Opus 3 Claude（在撰写本文时）比OpenAI 3.5贵30到50倍。Opus
    3是15美元/1M输入和75美元/1M输出令牌，而ChatGPT 3.5 turbo-0125是0.50美元/1M输入和1.50美元/1M输出令牌。这对于产品团队来说是一个关键点。你希望物有所值，尤其是当与那些如果模型提供优质服务就会更频繁使用该模型的客户打交道时。他们发现，与早期的3.5模型相比，微调ChatGPT
    4.0的质量显著更好。现在正在整合Chat GPT-4o mini。
- en: They started using a higher learning rate, which they could reduce when they
    made significant steps. The amount of data impacts the learning rate steps (e.g.,
    0.5, 0.1, and 0.01). This can be tweaked based on the size of the data set and
    other factors. The goal is to refine so the training loss decreases in an orderly
    fashion without wild swings or divergence.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们开始使用更高的学习率，当他们取得重大进展时可以降低学习率。数据量影响学习率步骤（例如，0.5、0.1和0.01）。这可以根据数据集的大小和其他因素进行调整。目标是精细调整，以便训练损失以有序的方式减少，而不是剧烈波动或发散。
- en: Working around the context window, they started with a 4K context window and
    grew to support 16K. They can preprocess data and use other tools to deal with
    smaller chunks. Even at 16K, this is small for handling large spreadsheets.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上下文窗口周围工作，他们从4K上下文窗口开始，扩展到支持16K。他们可以预处理数据并使用其他工具处理更小的数据块。即使在16K，这对于处理大型电子表格来说也相对较小。
- en: '**Make sure the data is accurate**: With good input data, success is more likely.
    If the trained data isn’t tagged correctly or has missing data, the data team
    will have issues. This real example is for an early Wove’s table end detection
    version. You can see this in the chart bouncing all over the place compared to
    earlier examples. You don’t see a convergence.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保数据准确**：良好的输入数据更有可能带来成功。如果训练数据未正确标记或存在缺失数据，数据团队将遇到问题。这个真实例子是关于早期Wove的表格末端检测版本。你可以从图表中看到，与早期示例相比，图表四处跳动。你看不到收敛。'
- en: We can look at one example of a hallucination and how they corrected it in the
    part of the flow where they took the text version of the spreadsheet (left column)
    and asked ChatGPT to define (understand) the row’s context (middle column). This
    is *Step 1*. Then, they edited this output, completing the process with the final
    output (right column) shown in *Table 8.4*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看看一个关于幻觉的例子以及他们在流程中如何纠正它。在这个部分，他们取了电子表格的文本版本（左侧列）并要求ChatGPT定义（理解）行的上下文（中间列）。这是**步骤1**。然后，他们编辑了这个输出，通过最终输出（右侧列）完成这个过程，如**表8.4**所示。
- en: '| **Input File** | **Intermediary output** **from ChatGPT** | **Final normalized
    file** **from ChatGPT** |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| **输入文件** | **ChatGPT的中间输出** | **ChatGPT的最终标准化文件** |'
- en: '| The table on the sheets “Rate Sheet”, is from the VN to US UPDATE file.The
    table has these columns: - id: 0name: POLExamples:- DA NANG, VIETNAM- HO CHI MINH
    CITY, VIETNAM- HAIPHONG, VIETNAM- VUNG TAU, VIETNAM | Purpose: This table provides
    ocean freight rates and details from various ports in Vietnam to destination in
    the United States, carriers and services, valid for a certain date range. columns:-
    id: 0name: POLdescription: The port of loading in Vietnam, such as De NANG, HAIPHONG,
    QUI NHON, or VUNG TAU.examples:- DA NANG, VIETNAM- HAIPHONG, VIETNAM- VUNG TAU,
    VIETNAM | carrier: 7service_line: 4origin: 0origin_country: Vietnamorigin_via:
    nullorigin_via_country: nulldestination: 1destination_country: United Statesdestination_via:
    2destination_via_county: United Statesloading_port: nullloading_port_country:
    nullunloading_port: null |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| “费率表”表格上的表格来自VN到US UPDATE文件。该表格包含以下列：- id: 0name: POL示例：- DA NANG, VIETNAM-
    HO CHI MINH CITY, VIETNAM- HAIPHONG, VIETNAM- VUNG TAU, VIETNAM | 目的：此表格提供从越南各个港口到美国目的地的海运费率和详细信息，承运人和服务，适用于特定日期范围。列：-
    id: 0name: POLdescription: 越南的装货港，例如DE NANG、HAIPHONG、QUI NHON或VUNG TAU。示例：- DA
    NANG, VIETNAM- HAIPHONG, VIETNAM- VUNG TAU, VIETNAM | carrier: 7service_line:
    4origin: 0origin_country: Vietnamorigin_via: nullorigin_via_country: nulldestination:
    1destination_country: United Statesdestination_via: 2destination_via_county: United
    Statesloading_port: nullloading_port_country: nullunloading_port: null |'
- en: '| **Input File** | **Intermediary output** **from ChatGPT** | **Final normalized
    file** **from ChatGPT** |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| **输入文件** | **ChatGPT的中间输出** | **ChatGPT的最终规范化文件** |'
- en: '| - id: 2name: Dest (Via Port)examples:- OAKLAND, CA- NORFOLK, VA- LOS ANGELES,
    CA- SAVANNAH, GAVANCOUVER, CANADA- None- TACOMA, WA- BALTIMORE, MD- CHARLESTON,
    SC- NEW YORK, NY | - id: 1name: Destinationdescription: The destination city and
    state in the United States, such as LONG BEACH, CA, SEATTLE, WA, or CLEVELAND,
    OHexamples:- ATLANTA, GA- PITTSBURGH, PA- CHARLOTTE, NC- CLEVELAND, OH- id: 2name:
    Dest (via Port)description: The port in the United States that the shipment will
    go to reaching its final destination, such as NORFOLK, VA, SEATTLE,WA or LOS ANGELES,
    CA. Can also be “None” if the shipping direct to the destination.examples:- NORFOLK,
    VA- SAVANNAH, GA | unloading_port_country: nulleffective_date: 8expiry_date: 9container_dry_or_reefer:
    nullcommodity: nullrates:- id: 10currency: nulltype: totalcontainer_size: “20ft”-
    id: 11currency: nulltype: totalcontainer_size: “40ft”- id: 12currency: nulltype:
    totalcontainer_size: “40ft_hc” |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| - id: 2name: 目的地 (通过港口)examples:- OAKLAND, CA- NORFOLK, VA- LOS ANGELES,
    CA- SAVANNAH, GAVANCOUVER, CANADA- None- TACOMA, WA- BALTIMORE, MD- CHARLESTON,
    SC- NEW YORK, NY | - id: 1name: 目的地description: 美国城市和州，例如 LONG BEACH, CA, SEATTLE,
    WA, 或 CLEVELAND, OHexamples:- ATLANTA, GA- PITTSBURGH, PA- CHARLOTTE, NC- CLEVELAND,
    OH- id: 2name: 目的地 (通过港口)description: 货物在到达最终目的地前将前往的美国港口，例如 NORFOLK, VA, SEATTLE,WA
    或 LOS ANGELES, CA。如果直接运往目的地，也可以是“None”。examples:- NORFOLK, VA- SAVANNAH, GA |
    unloading_port_country: nulleffective_date: 8expiry_date: 9container_dry_or_reefer:
    nullcommodity: nullrates:- id: 10currency: nulltype: totalcontainer_size: “20ft”-
    id: 11currency: nulltype: totalcontainer_size: “40ft”- id: 12currency: nulltype:
    totalcontainer_size: “40ft_hc” |'
- en: Table 8.4 – Wove file improvement steps (some data are truncated)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.4 – 纺织文件改进步骤（部分数据已截断）
- en: 'The table shows the description of the destination, aka dest (via port), generated
    in the middle column: “The port in the United States that the shipment will go…”.
    However, the data includes Vancouver, Canada. Although the destination is always
    the US, the port it goes via might be outside the US.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表格显示了中间列中生成的目的地描述，即“via port”的dest：“货物将前往美国的港口……”。然而，数据包括加拿大温哥华。尽管目的地始终是美国，但经过的港口可能在美国之外。
- en: To improve the model, this hallucination needs to be corrected. It could be
    decided that the correct answer is to edit this to be “North America,” or better
    is to remove the “in the United States” entirely, making the description more
    generic. This means that the output file in the next step also needs correction.
    The `destination_via_country` field will be changed from United States to `null`.
    This file on the right is a second round of ChatGPT generation, creating the unified
    model that makes all the spreadsheet data a consistent, normalized output. It
    is vital to catch these errors. With this final output file, they re-run the test
    data to see whether the quality improves.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进模型，这个幻觉需要被纠正。可以决定正确的答案是将其编辑为“北美”，或者更好的是，完全删除“在美国”，使描述更加通用。这意味着下一步的输出文件也需要纠正。`destination_via_country`字段将从美国更改为`null`。这个右边的文件是ChatGPT的第二轮生成，创建了一个使所有电子表格数据保持一致、规范化的统一模型。捕捉这些错误至关重要。使用这个最终的输出文件，他们重新运行测试数据以查看质量是否有所提高。
- en: Overall, this fine-tuning process requires many iterations on the prompts, editing
    of the tags, and evaluating against their test data. We can’t expose all of Wove’s
    secret sauce, but hopefully, this gives a sense. A modeler’s work is never done.
    Although ongoing effort might be reduced, work will not be done. Format changes
    can occur, including new vendors, normalizations, better, cheaper, and faster
    models, and everyone’s favorite, bugs will require rework. The point is to be
    involved and invested in these steps to ensure quality. Readers can imagine the
    next steps for Wove once all this data is normalized and available. Customers
    will want to ask questions about the best route based on shipping characteristics.
    They will not want to pour through even a normalized sheet of rates.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这个微调过程需要对提示进行多次迭代，编辑标签，并对其测试数据进行评估。我们无法透露Wove的所有秘密配方，但希望这能给你一个感觉。模型师的工作永远不会完成。尽管持续的努力可能会减少，但工作不会停止。可能会发生格式变化，包括新的供应商、规范化、更好的、更便宜、更快的模型，以及大家最喜欢的，错误需要重做。关键是参与并投资这些步骤以确保质量。读者可以想象一旦所有这些数据都规范化并可用，Wove的下一步是什么。客户将希望根据运输特性询问最佳路线。他们不希望翻阅即使是规范化的费率表。
- en: This is an exciting use case because it starts as a backend solution, still
    needs product understanding and feedback to be successful, and will likely lead
    to even more UX efforts when (inevitability) a customer-facing chat experience
    will converse with customers to help shop rates. Product and UX efforts will be
    needed there.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人兴奋的用例，因为它最初是一个后端解决方案，仍然需要产品理解和反馈才能成功，并且当（不可避免地）面向客户的聊天体验与客户交谈以帮助提高购物率时，很可能会导致更多的用户体验（UX）工作。那里需要产品和UX的努力。
- en: Wove used a series of models to understand the complexities of tables. Picking
    and chaining suitable models is part of the prompt engineering and fine-tuning
    process.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Wove使用一系列模型来理解表格的复杂性。选择和链式连接合适的模型是提示工程和微调过程的一部分。
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning is the most technical piece of this book. With this little glimpse
    into this world, there is much to cover. Your data scientists and engineers will
    go deeper. When building production-ready systems, mix and match fine-tuned and
    generic models with internal software and third-party tools to balance speed of
    delivery, price, and performance (recall the saying, *cheap, fast, or good, choose
    two*). Innovative solutions have workflow steps that allow the solution to bail
    out if the AI isn’t performing, use a function to solve or address a specific
    problem, or use a more deterministic element to provide a robust solution. Injecting
    the suitable model and prompts for the correct part of a use case is one of the
    most critical decisions. Do this work before embarking on the fine-tuning approach.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是本书中最技术性的部分。通过对这个世界的这一小瞥，有许多内容需要涵盖。你的数据科学家和工程师将深入探索。在构建生产就绪的系统时，将微调和通用模型与内部软件和第三方工具混合匹配，以平衡交付速度、价格和性能（回想一下那句俗语，*便宜、快或好，只能选择两个*）。创新解决方案的工作流程步骤允许解决方案在AI表现不佳时退出，使用一个函数来解决或处理特定问题，或使用更确定性的元素来提供稳健的解决方案。为用例的正确部分注入合适的模型和提示是其中最重要的决策之一。在开始微调方法之前，先完成这项工作。
- en: Contribute to the process by helping define and improve these task flows through
    use case expertise, editing and improving prompts, creating, verifying, and editing
    fine-tuning examples, and monitoring if changes are moving the solution in the
    right direction. Go forth and tune!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运用用例专业知识、编辑和改进提示、创建、验证和编辑微调示例，以及监控变化是否将解决方案推向正确的方向，来帮助定义和改进这些任务流程，从而为这个过程做出贡献。勇敢前行，进行微调！
- en: '[*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190), *Guidelines and
    Heuristics,* will cover guidelines and heuristics to support prompting and fine-tuning
    efforts based on well-documented techniques in the design community to help explain
    conversational AI’s usability.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第9章*](B21964_09_split_000.xhtml#_idTextAnchor190)，*指南和启发式方法*，将涵盖基于设计社区中记录良好的技术来支持提示和微调努力的指南和启发式方法，以帮助解释对话式人工智能的可用性。'
- en: References
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '| ![](img/B21964_08_QR.jpg) | The links, book recommendations, and GitHub files
    in this chapter are posted on the reference page.Web Page: [Chapter 8 References](https://uxdforai.com/references#C8)
    ([https://uxdforai.com/references#C8](https://uxdforai.com/references#C8)) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B21964_08_QR.jpg) | 本章中的链接、书籍推荐和GitHub文件已发布在参考页面上。网页：[第8章参考文献](https://uxdforai.com/references#C8)
    ([https://uxdforai.com/references#C8](https://uxdforai.com/references#C8)) |'
