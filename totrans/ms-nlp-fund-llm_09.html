<html><head></head><body>
<div id="_idContainer366" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-204"><a id="_idTextAnchor506" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-205" class="calibre4"><a id="_idTextAnchor507" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Exploring the Frontiers: Advanced Applications and Innovations Driven by LLMs</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.3.1">In the rapidly evolving landscape of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">natural language processing</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">NLP</span></strong><span class="kobospan" id="kobo.7.1">), </span><strong class="bold"><span class="kobospan" id="kobo.8.1">large language models</span></strong><span class="kobospan" id="kobo.9.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.10.1">LLMs</span></strong><span class="kobospan" id="kobo.11.1">) have marked a revolutionary step forward, reshaping how we interact with information, automate processes, and derive insights from vast data pools. </span><span class="kobospan" id="kobo.11.2">This chapter represents the culmination of our journey through the emergence and development of NLP methods. </span><span class="kobospan" id="kobo.11.3">It is here that the theoretical foundations laid in previous chapters converge with practical, cutting-edge applications, illuminating the remarkable capabilities of LLMs when harnessed with the right tools </span><span><span class="kobospan" id="kobo.12.1">and techniques.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.13.1">We delve into the most recent and thrilling advancements in LLM applications, presented through detailed Python code examples designed for hands-on learning. </span><span class="kobospan" id="kobo.13.2">This approach not only illustrates the power of LLMs but also equips you with the skills to implement these technologies in real-world scenarios. </span><span class="kobospan" id="kobo.13.3">The subjects covered in this chapter are meticulously selected to showcase a spectrum of advanced functionalities </span><span><span class="kobospan" id="kobo.14.1">and applications.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.15.1">The importance of this chapter cannot be overstated. </span><span class="kobospan" id="kobo.15.2">It not only reflects the state of the art in NLP but also serves as a bridge to the future, where the integration of these technologies into everyday solutions becomes seamless. </span><span class="kobospan" id="kobo.15.3">By the end of this chapter, you will have a comprehensive understanding of how to apply the latest LLM techniques and innovations, empowering you to push the boundaries of what’s possible in NLP and beyond. </span><span class="kobospan" id="kobo.15.4">Join us on this exciting journey to unlock the full potential </span><span><span class="kobospan" id="kobo.16.1">of LLMs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.17.1">Let’s go through the main headings covered in </span><span><span class="kobospan" id="kobo.18.1">the chapter:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.19.1">Enhancing LLM performance with RAG and LangChain – a dive into </span><span><span class="kobospan" id="kobo.20.1">advanced functionalities</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.21.1">Advanced methods </span><span><span class="kobospan" id="kobo.22.1">with chains</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.23.1">Retrieving information from various web </span><span><span class="kobospan" id="kobo.24.1">sources automatically</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.25.1">Prompt compression and API </span><span><span class="kobospan" id="kobo.26.1">cost reduction</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.27.1">Multiple agents – forming a team of LLMs </span><span><span class="kobospan" id="kobo.28.1">who collaborate</span></span></li>
</ul>
<h1 id="_idParaDest-206" class="calibre4"><a id="_idTextAnchor508" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.29.1">Technical requirements</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.30.1">For this chapter, the following will </span><span><span class="kobospan" id="kobo.31.1">be necessary:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.32.1">Programming knowledge</span></strong><span class="kobospan" id="kobo.33.1">: Familiarity with Python programming is a must since the open source models, OpenAI’s API, and LangChain are all illustrated using </span><span><span class="kobospan" id="kobo.34.1">Python code.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.35.1">Access to OpenAI’s API</span></strong><span class="kobospan" id="kobo.36.1">: An API key from OpenAI will be required to explore closed source models. </span><span class="kobospan" id="kobo.36.2">This can be obtained by creating an account with OpenAI and agreeing to their terms </span><span><span class="kobospan" id="kobo.37.1">of service.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.38.1">Open source models</span></strong><span class="kobospan" id="kobo.39.1">: Access to the specific open source models mentioned in this chapter will be necessary. </span><span class="kobospan" id="kobo.39.2">These can be accessed and downloaded from their respective repositories or via package managers such as pip </span><span><span class="kobospan" id="kobo.40.1">or conda.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.41.1">Local development environment</span></strong><span class="kobospan" id="kobo.42.1">: A local development environment setup with Python installed is required. </span><span class="kobospan" id="kobo.42.2">An </span><strong class="bold"><span class="kobospan" id="kobo.43.1">integrated development environment</span></strong><span class="kobospan" id="kobo.44.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.45.1">IDE</span></strong><span class="kobospan" id="kobo.46.1">) such as </span><strong class="bold"><span class="kobospan" id="kobo.47.1">PyCharm</span></strong><span class="kobospan" id="kobo.48.1">, </span><strong class="bold"><span class="kobospan" id="kobo.49.1">Jupyter Notebook</span></strong><span class="kobospan" id="kobo.50.1">, or a simple text editor can be used. </span><span class="kobospan" id="kobo.50.2">Note that we recommend a free </span><strong class="bold"><span class="kobospan" id="kobo.51.1">Google Colab</span></strong><span class="kobospan" id="kobo.52.1"> notebook, as it encapsulates all these requirements in a seamless </span><span><span class="kobospan" id="kobo.53.1">web interface.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.54.1">Ability to install libraries</span></strong><span class="kobospan" id="kobo.55.1">: You must have permission for the installation of the required Python libraries such as </span><strong class="bold"><span class="kobospan" id="kobo.56.1">NumPy</span></strong><span class="kobospan" id="kobo.57.1">, </span><strong class="bold"><span class="kobospan" id="kobo.58.1">SciPy</span></strong><span class="kobospan" id="kobo.59.1">, </span><strong class="bold"><span class="kobospan" id="kobo.60.1">TensorFlow</span></strong><span class="kobospan" id="kobo.61.1">, and </span><strong class="bold"><span class="kobospan" id="kobo.62.1">PyTorch</span></strong><span class="kobospan" id="kobo.63.1">. </span><span class="kobospan" id="kobo.63.2">Note that the code we provide includes the required installations so you won’t have to install them ahead of time. </span><span class="kobospan" id="kobo.63.3">We simply stress that you should have permission to do so, which we expect you would. </span><span class="kobospan" id="kobo.63.4">Specifically, using a free Google Colab notebook </span><span><span class="kobospan" id="kobo.64.1">would suffice.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.65.1">Hardware requirements</span></strong><span class="kobospan" id="kobo.66.1">: Depending on the complexity and size of the models you’re working with, a computer with sufficient processing power (potentially including a good GPU for ML tasks) and ample memory will be required. </span><span class="kobospan" id="kobo.66.2">This is only relevant when choosing to not use the free </span><span><span class="kobospan" id="kobo.67.1">Google Colab.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.68.1">Now that we’ve set up LLM applications using APIs and locally, we can finally deploy the advanced applications of LLMs that let us leverage their </span><span><span class="kobospan" id="kobo.69.1">immense power.</span></span></p>
<h1 id="_idParaDest-207" class="calibre4"><a id="_idTextAnchor509" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.70.1">Enhancing LLM performance with RAG and LangChain – a dive into advanced functionalities</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.71.1">The </span><strong class="bold"><span class="kobospan" id="kobo.72.1">retrieval-augmented generation</span></strong><span class="kobospan" id="kobo.73.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.74.1">RAG</span></strong><span class="kobospan" id="kobo.75.1">) framework has become instrumental in tailoring </span><strong class="bold"><span class="kobospan" id="kobo.76.1">large language models</span></strong><span class="kobospan" id="kobo.77.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.78.1">LLMs</span></strong><span class="kobospan" id="kobo.79.1">) for specific</span><a id="_idIndexMarker897" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.80.1"> domains or tasks, bridging the gap between the simplicity of prompt engineering and the complexity </span><a id="_idIndexMarker898" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.81.1">of </span><span><span class="kobospan" id="kobo.82.1">model fine-tuning.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.83.1">Prompt engineering</span><a id="_idIndexMarker899" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.84.1"> stands as the initial, most accessible technique for</span><a id="_idIndexMarker900" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.85.1"> customizing LLMs. </span><span class="kobospan" id="kobo.85.2">It leverages the model’s capacity to interpret and respond to queries based on the input prompt. </span><span class="kobospan" id="kobo.85.3">For example, to inquire if Nvidia surpassed earnings expectations in its latest announcement, directly providing the earnings call content within the prompt can compensate for the LLM’s lack of immediate, up-to-date context. </span><span class="kobospan" id="kobo.85.4">This approach, while straightforward, hinges on the model’s ability to digest and analyze the provided information within a single or a series of carefully </span><span><span class="kobospan" id="kobo.86.1">crafted prompts.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.87.1">When the scope of inquiry exceeds what prompt engineering can accommodate—such as analyzing a decade’s worth of tech sector earnings calls—RAG becomes indispensable. </span><span class="kobospan" id="kobo.87.2">Prior to RAG’s adoption, the alternative was fine-tuning, a resource-intensive process requiring significant adjustments to the LLM’s architecture to incorporate extensive datasets. </span><span class="kobospan" id="kobo.87.3">RAG simplifies this by preprocessing and storing large amounts of data in a vector database. </span><span class="kobospan" id="kobo.87.4">It intelligently isolates and retrieves the data segments pertinent to the query, effectively condensing the vast information into a manageable, prompt-size context for the LLM. </span><span class="kobospan" id="kobo.87.5">This innovation drastically reduces the time, resources, and expertise needed for such extensive data </span><span><span class="kobospan" id="kobo.88.1">familiarization tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.89.1">In </span><a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.90.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.91.1">, we introduced the general concept of RAGs and, in particular, LangChain, a RAG framework distinguished by its </span><span><span class="kobospan" id="kobo.92.1">advanced capabilities.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.93.1">We will now discuss the additional unique features LangChain offers for enhancing LLM applications, providing you with practical insights into its implementation and utility in complex </span><span><span class="kobospan" id="kobo.94.1">NLP tasks.</span></span></p>
<h2 id="_idParaDest-208" class="calibre7"><a id="_idTextAnchor510" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.95.1">LangChain pipeline with Python – enhancing performance with LLMs</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.96.1">In this section, we will pick up where we left off with our last example from </span><a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.97.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.98.1">. </span><span class="kobospan" id="kobo.98.2">In this scenario, we are in</span><a id="_idIndexMarker901" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.99.1"> the healthcare sector, and in our hospital, our care providers are expressing a need to be able to quickly surface patients’ records based on rough descriptions of the patient or their condition. </span><span class="kobospan" id="kobo.99.2">For example, “Who was that patient I saw last year who was pregnant with triplets?” </span><span class="kobospan" id="kobo.99.3">“Did I ever have a patient with a history of cancer from both of their parents and they were interested in a clinical trial?” </span><span class="kobospan" id="kobo.99.4">and </span><span><span class="kobospan" id="kobo.100.1">so on.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.101.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.102.1">We stress that these aren’t real medical notes and that the people described in the notes </span><span><span class="kobospan" id="kobo.103.1">aren’t real.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.104.1">In our example in </span><a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.105.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.106.1">, we kept the pipeline at minimum complexity by simply leveraging the vector databases of embeddings of clinical notes, and then we applied similarity search to look for notes based on simple requests. </span><span class="kobospan" id="kobo.106.2">We noticed how one of the questions, the second question, received a wrong answer with the similarity </span><span><span class="kobospan" id="kobo.107.1">search algorithm.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.108.1">We will now enhance that pipeline. </span><span class="kobospan" id="kobo.108.2">We will not settle for the results of the similarity search and surface those to the physicians; we will take those results that were deemed to be similar in content to the request, and we will employ an LLM to go through these results, vet them, and tell us which ones are indeed relevant to </span><span><span class="kobospan" id="kobo.109.1">the physician.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.110.1">Paid LLMs versus free</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.111.1">We'll use this pipeline to</span><a id="_idIndexMarker902" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.112.1"> exemplify the utility of either type of LLM, paid or free. </span><span class="kobospan" id="kobo.112.2">We give you the choice, via the </span><strong class="source-inline"><span class="kobospan" id="kobo.113.1">paid_vs_free</span></strong><span class="kobospan" id="kobo.114.1"> variable, to either use OpenAI’s paid GPT model or a free LLM. </span><span class="kobospan" id="kobo.114.2">Using OpenAI’s paid model would leverage their API and would require an API key. </span><span class="kobospan" id="kobo.114.3">However, the free LLM is imported to the local environment where the Python code is run, thus making it available to anyone who has an internet connection and sufficient </span><span><span class="kobospan" id="kobo.115.1">computational resources.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.116.1">Let’s start getting hands-on and experimenting with </span><span><span class="kobospan" id="kobo.117.1">the code.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.118.1">Applying advanced LangChain configurations and pipelines</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.119.1">Refer to the following </span><span><span class="kobospan" id="kobo.120.1">notebook: </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.121.1">Ch9_Advanced_LangChain_Configurations_and_Pipeline.ipynb</span></strong></span><span><span class="kobospan" id="kobo.122.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.123.1">Note that the first</span><a id="_idIndexMarker903" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.124.1"> part of the notebook is identical to the notebook from </span><a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.125.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.126.1">, so we will skip the description of </span><span><span class="kobospan" id="kobo.127.1">that part.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.128.1">Installing the required Python libraries</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.129.1">Here, we need to expand the </span><a id="_idIndexMarker904" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.130.1">set of installed libraries and install </span><strong class="source-inline"><span class="kobospan" id="kobo.131.1">openai</span></strong><span class="kobospan" id="kobo.132.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.133.1">gpt4all</span></strong><span class="kobospan" id="kobo.134.1">. </span><span class="kobospan" id="kobo.134.2">Moreover, in order to utilize </span><strong class="source-inline"><span class="kobospan" id="kobo.135.1">gpt4all</span></strong><span class="kobospan" id="kobo.136.1">, we will need to download a </span><strong class="source-inline"><span class="kobospan" id="kobo.137.1">.bin</span></strong><span class="kobospan" id="kobo.138.1"> file from </span><span><span class="kobospan" id="kobo.139.1">the web.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.140.1">These two steps are easy to perform via </span><span><span class="kobospan" id="kobo.141.1">the notebook.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.142.1">Setting up an LLM – choose between a paid LLM (OpenAI’s GPT) and a free LLM (from Hugging Face)</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.143.1">As explained above, we let you choose </span><a id="_idIndexMarker905" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.144.1">whether you want to run this example via a paid API by OpenAI or a </span><span><span class="kobospan" id="kobo.145.1">free LLM.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.146.1">Remember, since OpenAI’s service includes hosting the LLM and processing the prompts, it requires minimal resources and time and a basic internet connection. </span><span class="kobospan" id="kobo.146.2">It also involves sending our prompts to OpenAI’s API service. </span><span class="kobospan" id="kobo.146.3">Prompts typically include information that, in real-world settings, may be proprietary. </span><span class="kobospan" id="kobo.146.4">Thus, an executive decision needs to be made regarding the security of the data. </span><span class="kobospan" id="kobo.146.5">Similar considerations were central, in the last decade, to the transition of companies’ computation from on-premises to </span><span><span class="kobospan" id="kobo.147.1">the cloud.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.148.1">In contrast to that requirement, with a free LLM, you would host it locally, you would avoid exporting any information outside of your computation environment, but you would take on </span><span><span class="kobospan" id="kobo.149.1">the processing.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.150.1">Another aspect to consider is the terms of use of each LLM, as each may have different license terms. </span><span class="kobospan" id="kobo.150.2">While an LLM may allow you to experiment with it for free, it may present constrictions on whether you may use it in a </span><span><span class="kobospan" id="kobo.151.1">commercial product.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.152.1">In the context of constraints around runtime and computational resources, choosing the paid LLM for this example will yield </span><span><span class="kobospan" id="kobo.153.1">quicker responses.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.154.1">In order to accommodate your wish to experiment with a free LLM, and since we aspire to let you run the code quickly and for free on Google Colab, we must restrict our choice of LLMs to those that can be </span><a id="_idIndexMarker906" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.155.1">run on the limited RAM that Google lets us have with a free account. </span><span class="kobospan" id="kobo.155.2">In order to do that, we chose an LLM with reduced precision, also known as a </span><span><span class="kobospan" id="kobo.156.1">quantized LLM.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.157.1">Based on your choice between an API-based LLM and a free local LLM, the LLM will be assigned to the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.158.1">llm variable</span></strong></span><span><span class="kobospan" id="kobo.159.1">.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.160.1">Creating a QA chain</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.161.1">Here, we set up a</span><a id="_idIndexMarker907" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.162.1"> RAG framework. </span><span class="kobospan" id="kobo.162.2">It is designed to accept various text documents and set them up </span><span><span class="kobospan" id="kobo.163.1">for retrieval.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.164.1">Search based on the same requirements when using the LLM as the “brain” instead of embedding similarity</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.165.1">We will now run the exact same requests as we did in the example in </span><a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.166.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.167.1">. </span><span class="kobospan" id="kobo.167.2">Those will be performed across the same notes, and the same vector DB that holds the same embedding. </span><span class="kobospan" id="kobo.167.3">None of that has been changed or enhanced. </span><span class="kobospan" id="kobo.167.4">The difference is that we will have the LLM oversee the processing of </span><span><span class="kobospan" id="kobo.168.1">the answers.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.169.1">In </span><a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.170.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.171.1">, we saw that question number two received a wrong answer. </span><span class="kobospan" id="kobo.171.2">The question was, “Are there any pregnant patients who are due to deliver </span><span><span class="kobospan" id="kobo.172.1">in September?”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.173.1">The answer we saw in </span><a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.174.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.175.1"> was about a patient who is due to give birth in August. </span><span class="kobospan" id="kobo.175.2">The mistake was due to the deficiency of the similarity algorithm. </span><span class="kobospan" id="kobo.175.3">Indeed, that patient’s notes had content similar to that of the question, but the fine detail of giving birth in a different month should have been the factor that made those </span><span><span class="kobospan" id="kobo.176.1">note irrelevant.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.177.1">Here, in our current pipeline, where OpenAI’s LLM is applied, it gets it right, telling us that there are no patients who are due to deliver </span><span><span class="kobospan" id="kobo.178.1">in September.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.179.1">Note that when opting for the free LLM, it gets it wrong. </span><span class="kobospan" id="kobo.179.2">This exemplifies the sub-optimal aspects of that model, as it is quantized in an effort to save on </span><span><span class="kobospan" id="kobo.180.1">RAM requirements.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.181.1">To conclude this example, we have put together an in-house search mechanism that lets the user, in our example, a physician, search through their patients’ notes to find patients based on some </span><a id="_idIndexMarker908" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.182.1">criteria. </span><span class="kobospan" id="kobo.182.2">A unique aspect of this system design is the ability to let the LLM retrieve the relevant answer from an external data source and not be limited to the data it was trained on. </span><span class="kobospan" id="kobo.182.3">This paradigm is the basis </span><span><span class="kobospan" id="kobo.183.1">of RAG.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.184.1">In the next section, we will showcase more uses </span><span><span class="kobospan" id="kobo.185.1">for LLMs.</span></span></p>
<h1 id="_idParaDest-209" class="calibre4"><a id="_idTextAnchor511" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.186.1">Advanced methods with chains</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.187.1">In this section, we will continue our exploration of ways one can utilize LLM pipelines. </span><span class="kobospan" id="kobo.187.2">We will focus </span><span><span class="kobospan" id="kobo.188.1">on chains.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.189.1">Refer to the following notebook: </span><strong class="source-inline"><span class="kobospan" id="kobo.190.1">Ch9_Advanced_Methods_with_Chains.ipynb</span></strong><span class="kobospan" id="kobo.191.1">. </span><span class="kobospan" id="kobo.191.2">This notebook presents an evolution of a chain</span><a id="_idIndexMarker909" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.192.1"> pipeline, as every iteration exemplifies another feature that LangChain allows us </span><span><span class="kobospan" id="kobo.193.1">to employ.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.194.1">For the sake of using minimal computational resources, memory, and time, we use OpenAI’s API. </span><span class="kobospan" id="kobo.194.2">You can choose to use a free LLM instead and may do so in a similar way to how we set up the notebook from the previous example in </span><span><span class="kobospan" id="kobo.195.1">this chapter.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.196.1">The notebook starts with the basic configurations, as always, so we can skip to reviewing the </span><span><span class="kobospan" id="kobo.197.1">notebook’s content.</span></span></p>
<h2 id="_idParaDest-210" class="calibre7"><a id="_idTextAnchor512" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.198.1">Asking the LLM a general knowledge question</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.199.1">In this example, we want to </span><a id="_idIndexMarker910" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.200.1">use the LLM to tell us an answer to a simple question that would require common knowledge that a trained LLM is expected </span><span><span class="kobospan" id="kobo.201.1">to have:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.202.1">
"Who are the members of Metallica. </span><span class="kobospan1" id="kobo.202.2">List them as comma separated."</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.203.1">We then define a simple chain called </span><strong class="source-inline"><span class="kobospan" id="kobo.204.1">LLMChain</span></strong><span class="kobospan" id="kobo.205.1">, and we feed it with the </span><strong class="source-inline"><span class="kobospan" id="kobo.206.1">LLM</span></strong><span class="kobospan" id="kobo.207.1"> variable and </span><span><span class="kobospan" id="kobo.208.1">the prompt.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.209.1">The LLM, indeed, knows the</span><a id="_idIndexMarker911" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.210.1"> answer from its knowledge base </span><span><span class="kobospan" id="kobo.211.1">and returns:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.212.1">
'James Hetfield, Lars Ulrich, Kirk Hammett, Robert Trujillo'</span></pre> <h2 id="_idParaDest-211" class="calibre7"><a id="_idTextAnchor513" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.213.1">Requesting output structure – making the LLM provide output in a particular data format</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.214.1">This time, we would like the </span><a id="_idIndexMarker912" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.215.1">output to be in a particular syntax, potentially allowing us to use it in a computational manner for </span><span><span class="kobospan" id="kobo.216.1">downstream tasks:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.217.1">
"List the first 10 elements from the periodical table as comma separated list."</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.218.1">Now, we add a feature for achieving the syntax. </span><span class="kobospan" id="kobo.218.2">We define the </span><strong class="source-inline"><span class="kobospan" id="kobo.219.1">output_parser variable</span></strong><span class="kobospan" id="kobo.220.1">, and we use a different function for generating the </span><span><span class="kobospan" id="kobo.221.1">output, </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.222.1">predict_and_parse()</span></strong></span><span><span class="kobospan" id="kobo.223.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.224.1">The output is </span><span><span class="kobospan" id="kobo.225.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.226.1">
['Hydrogen',
'Helium',
'Lithium',
'Beryllium',
'Boron',
'Carbon',
'Nitrogen',
'Oxygen',
'Fluorine',
'Neon']</span></pre> <h2 id="_idParaDest-212" class="calibre7"><a id="_idTextAnchor514" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.227.1">Evolving to a fluent conversation – inserting an element of memory to have previous interactions as reference and context for follow-up prompts</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.228.1">This feature brings a</span><a id="_idIndexMarker913" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.229.1"> new level of value to the chain. </span><span class="kobospan" id="kobo.229.2">Until this point, the prompts didn’t have any context. </span><span class="kobospan" id="kobo.229.3">The LLM processed each prompt independently. </span><span class="kobospan" id="kobo.229.4">For instance, if you wanted to ask a follow-up question, you couldn’t. </span><span class="kobospan" id="kobo.229.5">The pipeline didn’t have your prior prompts and the responses to them </span><span><span class="kobospan" id="kobo.230.1">as reference.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.231.1">In order to go from asking disjointed questions to having an ongoing, rolling conversation-like experience, LangChain offers </span><strong class="source-inline"><span class="kobospan" id="kobo.232.1">ConversationChain()</span></strong><span class="kobospan" id="kobo.233.1">. </span><span class="kobospan" id="kobo.233.2">Within this function, we have a </span><strong class="source-inline"><span class="kobospan" id="kobo.234.1">memory</span></strong><span class="kobospan" id="kobo.235.1"> parameter that maps the prior interactions with the chain to the current prompt. </span><span class="kobospan" id="kobo.235.2">Therefore, the prompt template is where that </span><span><span class="kobospan" id="kobo.236.1">memory “lives.”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.237.1">Instead of prompting with a basic template, </span><span><span class="kobospan" id="kobo.238.1">such as</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.239.1">
"List all the holidays you know as comma separated list."</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.240.1">the template now accommodates the </span><span><span class="kobospan" id="kobo.241.1">memory feature:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.242.1">
"Current conversation:
{history}
Your task:
{input}}"</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.243.1">Here, you can think of this string as being formatted similarly to a Python </span><strong class="source-inline"><span class="kobospan" id="kobo.244.1">f"…"</span></strong><span class="kobospan" id="kobo.245.1"> string, where </span><strong class="source-inline"><span class="kobospan" id="kobo.246.1">history</span></strong><span class="kobospan" id="kobo.247.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.248.1">input</span></strong><span class="kobospan" id="kobo.249.1"> are string variables. </span><span class="kobospan" id="kobo.249.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.250.1">ConversationChain()</span></strong><span class="kobospan" id="kobo.251.1"> function processes this prompt template and inserts these two variables to complete the prompt string. </span><span class="kobospan" id="kobo.251.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.252.1">input</span></strong><span class="kobospan" id="kobo.253.1"> variable is produced by the function itself as we activate the memory mechanism, and the input variable is then supplied by us as we run </span><span><span class="kobospan" id="kobo.254.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.255.1">
conversation.predict_and_parse(input="Write the first 10 holidays you know, as a comma separated list.")</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.256.1">Where the output is </span><span><span class="kobospan" id="kobo.257.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.258.1">
['Christmas',
'Thanksgiving',
"New Year's Day",
'Halloween',
'Easter',
'Independence Day',
"Valentine's Day",
"St. </span><span class="kobospan1" id="kobo.258.2">Patrick's Day",
'Labor Day',
'Memorial Day']</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.259.1">Now, let’s make a follow-up </span><a id="_idIndexMarker914" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.260.1">request that would only be understood in the context of the previous request </span><span><span class="kobospan" id="kobo.261.1">and output:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.262.1">
conversation.predict_and_parse(input=" Observe the list of holidays you printed and remove all the non-religious holidays from the list.")</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.263.1">Indeed, we get the </span><span><span class="kobospan" id="kobo.264.1">appropriate output:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.265.1">
['Christmas',
'Thanksgiving',
"New Year's Day",
'Easter',
"Valentine's Day",
"St. </span><span class="kobospan1" id="kobo.265.2">Patrick's Day,"]</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.266.1">To complete this example, let’s assume the intention we had was to quickly generate a table of some holidays that includes their names </span><span><span class="kobospan" id="kobo.267.1">and descriptions:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.268.1">
"For each of these, tell about the holiday in 2 sentences.
</span><span class="kobospan1" id="kobo.268.2">Form the output in a json format table.
</span><span class="kobospan1" id="kobo.268.3">The table's name is "holidays" and the fields are "name" and "description".
</span><span class="kobospan1" id="kobo.268.4">For each row, the "name" is the holiday's name, and the "description" is the description you generated.
</span><span class="kobospan1" id="kobo.268.5">The syntax of the output should be a json format, without newline characters."</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.269.1">Now, we get a formatted string </span><a id="_idIndexMarker915" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.270.1">from </span><span><span class="kobospan" id="kobo.271.1">the chain:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.272.1">
{
  "holidays": [
    {
      "name": "Christmas",
      "description": "Christmas is a religious holiday that celebrates the birth of Jesus Christ and is widely observed as a secular cultural and commercial phenomenon."
</span><span class="kobospan1" id="kobo.272.2">    },
    {
      "name": "Thanksgiving",
      "description": "Thanksgiving is a national holiday in the United States, celebrated on the fourth Thursday of November, and originated as a harvest festival."
</span><span class="kobospan1" id="kobo.272.3">    },
    {
      "name": "Easter",
      "description": "Easter is […]</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.273.1">We can then use pandas to convert this string to </span><span><span class="kobospan" id="kobo.274.1">a table:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.275.1">
dict = json.loads(output)
pd.json_normalize(dict[ "holidays"])</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.276.1">After pandas processes </span><strong class="source-inline"><span class="kobospan" id="kobo.277.1">dict</span></strong><span class="kobospan" id="kobo.278.1"> to be a DataFrame, we can observe it in </span><span><em class="italic"><span class="kobospan" id="kobo.279.1">Table 9.1</span></em></span><span><span class="kobospan" id="kobo.280.1">:</span></span></p>
<table class="no-table-style" id="table001-5">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.281.1">Name</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.282.1">Description</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.283.1">0</span></strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.284.1">Christmas</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.285.1">Christmas is a Christian holiday that celebrates the birth of Jesus Christ. </span><span class="kobospan" id="kobo.285.2">It is observed on December 25 </span><span><span class="kobospan" id="kobo.286.1">each year.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.287.1">1</span></strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.288.1">Thanksgiving</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.289.1">Thanksgiving is a holiday in which people gather together to express gratitude for the blessings in their lives. </span><span class="kobospan" id="kobo.289.2">It is celebrated on the fourth Thursday in November in the </span><span><span class="kobospan" id="kobo.290.1">United States.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.291.1">2</span></strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.292.1">New </span><span><span class="kobospan" id="kobo.293.1">Year’s Day</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.294.1">New Year’s Day marks the beginning of the Gregorian calendar year. </span><span class="kobospan" id="kobo.294.2">It is celebrated on January 1 with various traditions </span><span><span class="kobospan" id="kobo.295.1">and festivities.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.296.1">3</span></strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.297.1">Easter</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.298.1">Easter is a </span><a id="_idIndexMarker916" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.299.1">Christian holiday that commemorates the resurrection of Jesus Christ from the dead. </span><span class="kobospan" id="kobo.299.2">It is observed on the first Sunday following the first full moon after the </span><span><span class="kobospan" id="kobo.300.1">vernal equinox.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.301.1">4</span></strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.302.1">Valentine’s Day</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.303.1">Valentine’s Day is a day to celebrate love and affection. </span><span class="kobospan" id="kobo.303.2">It is traditionally associated with romantic love, but it is also a time to express appreciation for friends </span><span><span class="kobospan" id="kobo.304.1">and family.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.305.1">5</span></strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.306.1">St. </span><span><span class="kobospan" id="kobo.307.1">Patrick’s Day</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.308.1">St. </span><span class="kobospan" id="kobo.308.2">Patrick’s Day is a cultural and religious holiday that honors the patron saint of Ireland, St. </span><span class="kobospan" id="kobo.308.3">Patrick. </span><span class="kobospan" id="kobo.308.4">It is celebrated on March 17 with parades, wearing green, and other </span><span><span class="kobospan" id="kobo.309.1">festive activities.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.310.1">Table 9.1 – pandas transformed the table from dict to a DataFrame, thus suiting down-stream processing</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.311.1">This concludes the various chain features that this notebook presents. </span><span class="kobospan" id="kobo.311.2">Notice how we leveraged the features that both chains bring us and that LLMs bring us. </span><span class="kobospan" id="kobo.311.3">For instance, while the memory and parsing features are completely handled on the chain’s side, the ability to present a response in a particular format, such as a JSON format, is solely accredited to </span><span><span class="kobospan" id="kobo.312.1">the LLM.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.313.1">In our next example, we</span><a id="_idIndexMarker917" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.314.1"> will continue to present novel utilities with LLMs </span><span><span class="kobospan" id="kobo.315.1">and LangChain.</span></span></p>
<h1 id="_idParaDest-213" class="calibre4"><a id="_idTextAnchor515" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.316.1">Retrieving information from various web sources automatically</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.317.1">In this example, we will review </span><a id="_idIndexMarker918" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.318.1">how simple it is to leverage LLMs to access the web and extract information. </span><span class="kobospan" id="kobo.318.2">We may wish to research a particular topic, and so we would like to consolidate all the information from a few web pages, several YouTube videos that present that topic, and so on. </span><span class="kobospan" id="kobo.318.3">Such an endeavor can take a while, as the content may be massive. </span><span class="kobospan" id="kobo.318.4">For instance, several YouTube videos can sometimes take hours to review. </span><span class="kobospan" id="kobo.318.5">Often, one doesn’t know how useful the video is until one has watched a significant portion </span><span><span class="kobospan" id="kobo.319.1">of it.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.320.1">Another use case is when looking to track various trends in real time. </span><span class="kobospan" id="kobo.320.2">This may include tracking news sources, YouTube videos, and so on. </span><span class="kobospan" id="kobo.320.3">Here, speed is crucial. </span><span class="kobospan" id="kobo.320.4">Unlike the previous example where speed was important to save us personal time, here, speed is necessary for getting our algorithm to be relevant for identifying real-time </span><span><span class="kobospan" id="kobo.321.1">emerging trends.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.322.1">In this section, we put together a very simple and </span><span><span class="kobospan" id="kobo.323.1">limited example.</span></span></p>
<h2 id="_idParaDest-214" class="calibre7"><a id="_idTextAnchor516" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.324.1">Retrieving content from a YouTube video and summarizing it</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.325.1">Refer to the following notebook: </span><strong class="source-inline"><span class="kobospan" id="kobo.326.1">Ch9_Retrieve_Content_from_a_YouTube_Video_and_Summarize.ipynb</span></strong><span class="kobospan" id="kobo.327.1"> (</span><a href="https://github.com/embedchain/embedchain" class="calibre5 pcalibre1 pcalibre"><span class="kobospan" id="kobo.328.1">https://github.com/embedchain/embedchain</span></a><span class="kobospan" id="kobo.329.1">). </span><span class="kobospan" id="kobo.329.2">We will build our application on a library called EmbedChain (</span><a href="https://github.com/embedchain/embedchain" class="calibre5 pcalibre1 pcalibre"><span class="kobospan" id="kobo.330.1">https://github.com/embedchain/embedchain</span></a><span class="kobospan" id="kobo.331.1">). </span><span class="kobospan" id="kobo.331.2">EmbedChain leverages a RAG framework</span><a id="_idIndexMarker919" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.332.1"> and enhances it by allowing the vector database to include information from various </span><span><span class="kobospan" id="kobo.333.1">web sources.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.334.1">In our example, we will choose a particular YouTube video (</span><em class="italic"><span class="kobospan" id="kobo.335.1">Robert Waldinger: What makes a good life? </span><span class="kobospan" id="kobo.335.2">Lessons from the longest study on happiness | TED</span></em><span class="kobospan" id="kobo.336.1">: </span><a href="https://www.youtube.com/watch?v=8KkKuTCFvzI&amp;ab_channel=TED" class="calibre5 pcalibre1 pcalibre"><span class="kobospan" id="kobo.337.1">https://www.youtube.com/watch?v=8KkKuTCFvzI&amp;ab_channel=TED</span></a><span class="kobospan" id="kobo.338.1">). </span><span class="kobospan" id="kobo.338.2">We would like the content of that video to be processed into the RAG framework. </span><span class="kobospan" id="kobo.338.3">Then, we will prompt an LLM with questions and tasks related to the content of that video, thus allowing us to extract everything we care to learn about the video without having to </span><span><span class="kobospan" id="kobo.339.1">watch it.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.340.1">It should be stressed that a key feature that this method relies on is that YouTube accompanies many of its verbal videos with a written transcript. </span><span class="kobospan" id="kobo.340.2">This makes the importing of the video’s text</span><a id="_idIndexMarker920" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.341.1"> context seamless. </span><span class="kobospan" id="kobo.341.2">If, however, one wishes to apply this method to a video that isn’t accompanied by a transcript, this is not a problem. </span><span class="kobospan" id="kobo.341.3">One would need to pick a speech-to-text model, many of which are free and of very high quality. </span><span class="kobospan" id="kobo.341.4">The audio of the video would be processed, a transcript would be extracted, and you may then import it into the </span><span><span class="kobospan" id="kobo.342.1">RAG process.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.343.1">Installs, imports, and settings</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.344.1">As with previous notebooks, here too, we</span><a id="_idIndexMarker921" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.345.1"> install the necessary packages, import all the relevant packages, and set our OpenAI </span><span><span class="kobospan" id="kobo.346.1">API key.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.347.1">We then do </span><span><span class="kobospan" id="kobo.348.1">the following:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.349.1">Make our choice </span><span><span class="kobospan" id="kobo.350.1">of model.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.351.1">Choose an embedding model that will serve the RAG’s vector </span><span><span class="kobospan" id="kobo.352.1">database feature.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.353.1">Choose a prompting LLM. </span><span class="kobospan" id="kobo.353.2">Notice how you can set up further parameters that control the model’s output, such as the maximal number of returned tokens or </span><span><span class="kobospan" id="kobo.354.1">the temperature.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.355.1">Pick the YouTube video to which you would like to apply this code and set a string variable using the </span><span><span class="kobospan" id="kobo.356.1">video’s URL.</span></span></li>
</ol>
<h3 class="calibre8"><span class="kobospan" id="kobo.357.1">Setting up the retrieval mechanism</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.358.1">We need to set </span><a id="_idIndexMarker922" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.359.1">EmbedChain’s RAG process. </span><span class="kobospan" id="kobo.359.2">We specify that we are passing a path to a YouTube video, and we provide the </span><span><span class="kobospan" id="kobo.360.1">video’s URL.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.361.1">We can then print out the text that was fetched and verify that it is, indeed, aligned with the video we are looking </span><span><span class="kobospan" id="kobo.362.1">to analyze.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.363.1">Reviewing, summarizing, and translating</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.364.1">We will now observe the value that this </span><span><span class="kobospan" id="kobo.365.1">code yields.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.366.1">We ask the LLM to review the</span><a id="_idIndexMarker923" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.367.1"> content, to put together a summary, and to present that summary in English, Russian, </span><span><span class="kobospan" id="kobo.368.1">and German:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.369.1">
Please review the entire content, summarize it to the length of 4 sentence, then translate it to Russian and to German.
</span><span class="kobospan1" id="kobo.369.2">Make sure the summary is consistent with the content.
</span><span class="kobospan1" id="kobo.369.3">Put the string '\n----\n' between the English part of the answer and the Russian part.
</span><span class="kobospan1" id="kobo.369.4">Put the string '\n****\n' between the Russian part of the answer and the German part.</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.370.1">The returned output is spot on, as it completely captures the essence of the TED talk. </span><span class="kobospan" id="kobo.370.2">We edit it to remove the delimiter strings </span><span><span class="kobospan" id="kobo.371.1">and get:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.372.1">
The content emphasizes the importance of good
relationships in keeping us happy and healthy
throughout our lives. </span><span class="kobospan1" id="kobo.372.2">It discusses how social
connections, quality of close relationships, and
avoiding conflict play crucial roles in our well-
being. </span><span class="kobospan1" id="kobo.372.3">The study follows the lives of 724 men over
75 years, highlighting the significance of
relationships over wealth and fame in leading a
fulfilling life.
</span><span class="kobospan1" id="kobo.372.4">Russian:
Содержание подчеркивает
Важность [...]
German:
Der
Inhalt betont die Bedeutung  [...]</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.373.1">Now, to make the content simple for, say, a German speaker, we ask the LLM to form the German summary into several bullet points that best describe the content of </span><span><span class="kobospan" id="kobo.374.1">the video.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.375.1">It does this well, and the </span><a id="_idIndexMarker924" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.376.1">outputs are </span><span><span class="kobospan" id="kobo.377.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.378.1">
- Betonung der Bedeutung guter Beziehungen für Glück und Gesundheit
- Diskussion über soziale Verbindungen, Qualität enger Beziehungen und Konfliktvermeidung
- Verfolgung des Lebens von 724 Männern über 75 Jahre in der Studie
- Hervorhebung der Bedeutung von Beziehungen im Vergleich zu Reichtum und Ruhm
- Fokus auf Beziehungen als Schlüssel zu einem erfüllten Leben</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.379.1">While this code is meant to serve as a basic proof of concept, one can see how simple it would be to add more data sources, automate it to run constantly, and act based on the findings. </span><span class="kobospan" id="kobo.379.2">While a readable summary is helpful, one could change the code to act based on the identified content and execute </span><span><span class="kobospan" id="kobo.380.1">downstream applications.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.381.1">Now that we have observed several capabilities that LLMs can perform, we can take a step back and refine the way we utilize those LLMs. </span><span class="kobospan" id="kobo.381.2">In our next section, we will exemplify how one may reduce LLM processing, thus saving API costs, or, when employing a local LLM, reducing </span><span><span class="kobospan" id="kobo.382.1">inference computation.</span></span></p>
<h1 id="_idParaDest-215" class="calibre4"><a id="_idTextAnchor517" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.383.1">Prompt compression and API cost reduction</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.384.1">This part is dedicated to a recent development in resource optimization for when employing API-based LLMs, such as OpenAI’s services. </span><span class="kobospan" id="kobo.384.2">When considering the many trade-offs between employing a remote LLM as a service and hosting an LLM locally, one key metric is cost. </span><span class="kobospan" id="kobo.384.3">In particular, based on the application and usage, the API costs can accumulate to a significant amount. </span><span class="kobospan" id="kobo.384.4">API costs are mainly driven by the number of tokens that are being sent to and from the </span><span><span class="kobospan" id="kobo.385.1">LLM service.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.386.1">In order to illustrate the significance of this payment model on a business plan, consider business units for which the product or service relies on API calls to OpenAI’s GPT, where OpenAI serves as a third-party vendor. </span><span class="kobospan" id="kobo.386.2">As a particular example, imagine a social network that lets its users have LLM assistance to comment on posts. </span><span class="kobospan" id="kobo.386.3">In that use case, a user is interested in commenting on a post, and instead of having to write a complete comment, a feature lets the user describe their feelings about the post in three–five words, and a backend process augments a </span><span><span class="kobospan" id="kobo.387.1">full comment.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.388.1">In this particular example, the engine collects the user’s three–five words, and it also collects the content of the post that the comment is meant for, meaning it will also collect all other relevant information that the social network’s experts would think is relevant for augmenting a comment. </span><span class="kobospan" id="kobo.388.2">For instance, the user’s profile description, their past few comments, and </span><span><span class="kobospan" id="kobo.389.1">so on.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.390.1">This would mean that every time a user wishes to have a comment augmented, a detailed prompt is sent from the social network’s servers to the third party’s LLM </span><span><span class="kobospan" id="kobo.391.1">via theAPI.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.392.1">Now, this type of process can accumulate </span><span><span class="kobospan" id="kobo.393.1">high costs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.394.1">In this section, we will analyze an approach to reducing this cost by reducing the number of tokens sent to the LLM through the API. </span><span class="kobospan" id="kobo.394.2">The basic assumption is that one can always reduce the number of words sent to the LLM and, thus, reduce cost, but the reduction in performance could be significant. </span><span class="kobospan" id="kobo.394.3">Our motivation is to reduce that amount while maintaining high-quality performance. </span><span class="kobospan" id="kobo.394.4">We then asked if only the “right” words could be sent, ignoring other “non-material” words. </span><span class="kobospan" id="kobo.394.5">This notion reminds us of the concept of file compression, where a smart and tailored algorithm is employed to reduce the size a file takes while maintaining its purpose </span><span><span class="kobospan" id="kobo.395.1">and value.</span></span></p>
<h2 id="_idParaDest-216" class="calibre7"><a id="_idTextAnchor518" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.396.1">Prompt compression</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.397.1">Here, we introduce </span><strong class="bold"><span class="kobospan" id="kobo.398.1">LLMLingua</span></strong><span class="kobospan" id="kobo.399.1">, a development by Microsoft that is meant to address prompts that are “sparse” in information by</span><a id="_idIndexMarker925" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.400.1">compressing them.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.401.1">LLMLingua utilizes a compact, well-trained language</span><a id="_idIndexMarker926" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.402.1"> model, such as LLaMA-7B, to identify and remove non-essential tokens within prompts. </span><span class="kobospan" id="kobo.402.2">This approach enables efficient inference with LLMs, achieving up to 20x compression with minimal performance </span><span><span class="kobospan" id="kobo.403.1">loss (</span></span><a href="https://github.com/microsoft/LLMLingua" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.404.1">https://github.com/microsoft/LLMLingua</span></span></a><span><span class="kobospan" id="kobo.405.1">).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.406.1">In their papers (</span><a href="https://arxiv.org/abs/2310.05736" class="calibre5 pcalibre1 pcalibre"><span class="kobospan" id="kobo.407.1">https://arxiv.org/abs/2310.05736</span></a><span class="kobospan" id="kobo.408.1"> and https://arxiv.org/abs/2310.06839), the authors explain the algorithm and the advantages it proposes. </span><span class="kobospan" id="kobo.408.2">It is interesting to note that besides the reduction in cost, the compression also aims to focus the remaining content, which is shown by the authors to lead to an improvement in performance by the LLM, as it avoids a sparse and </span><span><span class="kobospan" id="kobo.409.1">noisy prompt.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.410.1">Let’s experiment with prompt compression in a real-world example and evaluate its impact and </span><span><span class="kobospan" id="kobo.411.1">various trade-offs.</span></span></p>
<h2 id="_idParaDest-217" class="calibre7"><a id="_idTextAnchor519" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.412.1">Experimenting with prompt compression and evaluating trade-offs</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.413.1">For the sake of this experiment, we'll illustrate a </span><span><span class="kobospan" id="kobo.414.1">real-world example.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.415.1">In our current use case, we are developing </span><a id="_idIndexMarker927" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.416.1">a feature that sits on top of a database of academic publications. </span><span class="kobospan" id="kobo.416.2">The feature allows the user to pick a specific publication</span><a id="_idIndexMarker928" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.417.1"> and ask questions about it. </span><span class="kobospan" id="kobo.417.2">A backend engine evaluates the question, reviews the publication, and derives </span><span><span class="kobospan" id="kobo.418.1">an answer.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.419.1">To narrow down the scope of the feature for the sake of putting together a series of experiments, the publications are from the particular category of AI publications, and the question that the user asks is </span><span><span class="kobospan" id="kobo.420.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.421.1">
"Does this publication involve Reinforcement Learning?"</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.422.1">This question requires a deep and insightful review of each publication, as there are cases where a publication discusses a novel algorithm where the term reinforcement learning isn’t explicitly mentioned at any point in the publication, yet the feature is expected to infer from the description of the algorithm whether it indeed leverages the concepts of reinforcement learning and flag it </span><span><span class="kobospan" id="kobo.423.1">as such.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.424.1">Refer to the following </span><span><span class="kobospan" id="kobo.425.1">notebook: </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.426.1">Ch9_RAGLlamaIndex_Prompt_Compression.ipynb</span></strong></span><span><span class="kobospan" id="kobo.427.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.428.1">In this code, we run a set of experiments, each </span><a id="_idIndexMarker929" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.429.1">per the above feature description. </span><span class="kobospan" id="kobo.429.2">Each experiment is in the form of a</span><a id="_idIndexMarker930" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.430.1"> full, end-to-end RAG task. </span><span class="kobospan" id="kobo.430.2">While we employed LangChain in the previous RAG examples, here, we introduce LlamaIndex. </span><span class="kobospan" id="kobo.430.3">LlamaIndex is an open source Python library that employs a RAG framework (</span><a href="https://docs.llamaindex.ai/en/stable/index.html" class="calibre5 pcalibre1 pcalibre"><span class="kobospan" id="kobo.431.1">https://docs.llamaindex.ai/en/stable/index.html</span></a><span class="kobospan" id="kobo.432.1">). </span><span class="kobospan" id="kobo.432.2">LlamaIndex is similar to LangChain in </span><span><span class="kobospan" id="kobo.433.1">that way.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.434.1">The LLMLingua code stack that the folks at Microsoft put together is integrated </span><span><span class="kobospan" id="kobo.435.1">with LlamaIndex.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.436.1">Let’s review the code </span><span><span class="kobospan" id="kobo.437.1">in detail.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.438.1">Code settings</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.439.1">Similar to the previous notebooks, here too, we </span><a id="_idIndexMarker931" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.440.1">set the initial settings with </span><span><span class="kobospan" id="kobo.441.1">the following:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.442.1">In this code section, we start by defining some </span><span><span class="kobospan" id="kobo.443.1">key variables.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.444.1">We set the number of experiments that we want to run. </span><span class="kobospan" id="kobo.444.2">We want to make sure we choose a number that is large enough so as to get a good statistical representation of the impact that the </span><span><span class="kobospan" id="kobo.445.1">compression has.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.446.1">We set the top-k, which is the number of chunks to be retrieved by the RAG framework for </span><span><span class="kobospan" id="kobo.447.1">prompt context.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.448.1">We predefined the target number of the token we would like the compression </span><span><span class="kobospan" id="kobo.449.1">to reduce.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.450.1">Finally, as in the previous code, we set our OpenAI </span><span><span class="kobospan" id="kobo.451.1">API key.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.452.1">We take this opportunity to stress that some of the parameters in this evaluation were fixed for the sake of limiting its complexity and keeping it appropriate for educational purposes. </span><span class="kobospan" id="kobo.452.2">When conducting such an evaluation in business or academic settings, there should be either qualitative or quantitative reasoning for the value chosen. </span><span class="kobospan" id="kobo.452.3">Qualitative may be of the form “We shell fix the desired reduction to 999 tokens due to budget constraints," whereas quantitative may seek to not fix it but rather optimize it as a part of the other trade-offs. </span><span class="kobospan" id="kobo.452.4">In our case, we fixed this particular parameter to a value that was found to allow for an impressive compression rate while maintaining a decent agreement rate between the two evaluated approaches. </span><span class="kobospan" id="kobo.452.5">Another example was the number of experiments we</span><a id="_idIndexMarker932" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.453.1"> chose, which was a trade-off between runtime, GPU memory allocation, and </span><span><span class="kobospan" id="kobo.454.1">statistical power.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.455.1">Gathering the data</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.456.1">We need to gather the dataset of the</span><a id="_idIndexMarker933" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.457.1"> publications, and we also filter it so as to be left with only the limited cohort of publications that are in the </span><span><span class="kobospan" id="kobo.458.1">AI category.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.459.1">LLM configurations</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.460.1">Here, we set the ground for the two </span><a id="_idIndexMarker934" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.461.1">LLMs we will </span><span><span class="kobospan" id="kobo.462.1">be employing.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.463.1">The compression method, LLMLingua, employs Llama2 as the compressing LLM. </span><span class="kobospan" id="kobo.463.2">It will obtain the context retrieved by the LlamaIndex RAG pipeline, the user’s question, and it will compress and reduce the size of the </span><span><span class="kobospan" id="kobo.464.1">context content.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.465.1">OpenAI’s GPT is to be used as the downstream LLM for prompting, meaning it will obtain the question about reinforcement learning and the additional relevant context and return </span><span><span class="kobospan" id="kobo.466.1">an answer.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.467.1">Additionally, here, we define the user’s question. </span><span class="kobospan" id="kobo.467.2">Note that we added instructions for OpenAI’s GPT on how to present </span><span><span class="kobospan" id="kobo.468.1">the answer.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.469.1">The experiments</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.470.1">This is the core of the notebook. </span><span class="kobospan" id="kobo.470.2">A </span><strong class="source-inline"><span class="kobospan" id="kobo.471.1">for</span></strong><span class="kobospan" id="kobo.472.1"> loop iterates </span><a id="_idIndexMarker935" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.473.1">over the various experiments. </span><span class="kobospan" id="kobo.473.2">In each iteration, two scenarios </span><span><span class="kobospan" id="kobo.474.1">are evaluate:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.475.1">First scenario</span></strong><span class="kobospan" id="kobo.476.1">: an ordinary RAG task is deployed where the context is retrieved without being compressed. </span><span class="kobospan" id="kobo.476.2">The prompt is comprised of the retrieved context and the user’s question, and the answer that the LLM returns is recorded along with the number of sent tokens and the </span><span><span class="kobospan" id="kobo.477.1">processing time.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.478.1">Second scenario</span></strong><span class="kobospan" id="kobo.479.1">: LLMLingua is employed. </span><span class="kobospan" id="kobo.479.2">The retrieved context is compressed. </span><span class="kobospan" id="kobo.479.3">The compressed context is sent to the LLM along with the user’s question. </span><span class="kobospan" id="kobo.479.4">Again, the returned answer is recorded along with the number of sent tokens and the </span><span><span class="kobospan" id="kobo.480.1">processing time.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.481.1">When this code cell is completed, we have a dictionary, </span><strong class="source-inline"><span class="kobospan" id="kobo.482.1">record</span></strong><span class="kobospan" id="kobo.483.1">, that holds the relevant values for each iteration that </span><a id="_idIndexMarker936" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.484.1">will be used to aggregate and </span><span><span class="kobospan" id="kobo.485.1">derive conclusions.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.486.1">Analyzing the impact of context compression – a reduction in classification performance versus an increase in resource efficiency</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.487.1">Here, we sum up the </span><a id="_idIndexMarker937" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.488.1">values of the experiments and deduce what impact the prompt compression has on the performance of the LLM, the processing time, and the cost of </span><span><span class="kobospan" id="kobo.489.1">the API:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.490.1">We found that the reduction in the context length had yielded an agreement rate </span><span><span class="kobospan" id="kobo.491.1">of 92%.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.492.1">We found that the process of compression had extended the processing time by </span><span><span class="kobospan" id="kobo.493.1">11 times.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.494.1">We found that the reduction in the context length saved 92% of the total cost of </span><span><span class="kobospan" id="kobo.495.1">sent tokens!</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.496.1">Note that cost reduction is negatively dependent on the agreement rate, as we expect an increase in cost savings to reduce the </span><span><span class="kobospan" id="kobo.497.1">agreement rate.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.498.1">This reduction is significant and, in some cases, may tilt the scale from a loss-making service to a </span><span><span class="kobospan" id="kobo.499.1">profitable service.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.500.1">Here are some notes to keep in mind regarding the meaning of a disagreement and additional trade-offs. </span><span class="kobospan" id="kobo.500.2">Regarding the drop in agreement rate between the two approaches, while an agreement between the two approaches insinuates that they are both correct, a disagreement could go either way. </span><span class="kobospan" id="kobo.500.3">It could be that in the second scenario, the compression distorted the context and, thus, made the model unable to properly classify it. </span><span class="kobospan" id="kobo.500.4">However, the opposite may be true, as the compression may have reduced the irrelevant content and made the LLM focus on the relevant aspects of the content, thus making the scenario with the compressed context yield a </span><span><span class="kobospan" id="kobo.501.1">correct answer.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.502.1">Regarding additional trade-offs, the above metrics of LLM performance, processing time, and API cost don’t reveal additional considerations such as the computational resources that the compression requires. </span><span class="kobospan" id="kobo.502.2">The local compressing LLM, in our case, Llama2, requires local hosting and local GPUs. </span><span class="kobospan" id="kobo.502.3">These are non-trivial resources that don’t exist on an ordinary laptop. </span><span class="kobospan" id="kobo.502.4">Remember the original approach, i.e., the first scenario, does not require those. </span><span class="kobospan" id="kobo.502.5">An ordinary RAG approach can perform embeddings using either a smaller LM, such as one that is BERT-based, or even an API-based embedding. </span><span class="kobospan" id="kobo.502.6">The prompted LLM, under</span><a id="_idIndexMarker938" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.503.1"> our original assumption, is chosen to be remote and API-based, thus enabling the deployment environment to have minimal computation resources, like a common laptop </span><span><span class="kobospan" id="kobo.504.1">would provide.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.505.1">This evaluation proves that the LLMLingua prompt compression method is very impactful and useful as a means of </span><span><span class="kobospan" id="kobo.506.1">cost reduction.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.507.1">In the next and last code demonstration of this chapter, we will continue to observe the results of this experience, and we will do so by forming a team of experts, each played by an LLM, so as to enhance the process of deriving a concl</span><a id="_idTextAnchor520" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.508.1">usion to </span><span><span class="kobospan" id="kobo.509.1">the analysis.</span></span></p>
<h1 id="_idParaDest-218" class="calibre4"><a id="_idTextAnchor521" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.510.1">Multiple agents – forming a team of LLMs that collaborate</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.511.1">This section deals with one of the most exciting recent methods in the world of LLMs, employing multiple LLMs simultaneously. </span><span class="kobospan" id="kobo.511.2">In the context of this section, we seek to define multiple agents, each backed by an LLM and given a different designated role to play. </span><span class="kobospan" id="kobo.511.3">Instead of the user working</span><a id="_idIndexMarker939" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.512.1"> directly with the LLM, as we see in ChatGPT, here, the user sets up multiple LLMs and sets their role by defining a different system prompt for each </span><span><span class="kobospan" id="kobo.513.1">of them.</span></span></p>
<h2 id="_idParaDest-219" class="calibre7"><a id="_idTextAnchor522" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.514.1">Potential advantages of multiple LLM agents working simultaneously</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.515.1">Much like with people working </span><a id="_idIndexMarker940" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.516.1">together, here too, we see the advantages of employing several </span><span><span class="kobospan" id="kobo.517.1">LLMs simultaneously.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.518.1">Some advantages are </span><span><span class="kobospan" id="kobo.519.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.520.1">Enhancing validation and reducing hallucinations</span></strong><span class="kobospan" id="kobo.521.1">: It has been shown that when providing feedback to an LLM and asking it to reason or to check its response, the reliability of its response improves. </span><span class="kobospan" id="kobo.521.2">When designating roles for the various LLM agents on a team, the system prompt of at least one of them may include the requirement to criticize and validate </span><span><span class="kobospan" id="kobo.522.1">the answers.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.523.1">Allowing the person to be involved as much or as little as they want in the process</span></strong><span class="kobospan" id="kobo.524.1">: When designating the various roles, the user may insert themselves into the team, such that when it is the user’s turn to participate in the conversation, the rest of the agents wait while the user enters their input. </span><span class="kobospan" id="kobo.524.2">However, if desired, the user may remove themselves altogether and just let the LLMs </span><span><span class="kobospan" id="kobo.525.1">work automatically.</span></span><p class="calibre6"><span class="kobospan" id="kobo.526.1">In the following examples, we will see examples of </span><span><span class="kobospan" id="kobo.527.1">the latter.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.528.1">Allowing different LLM models to be best utilized</span></strong><span class="kobospan" id="kobo.529.1">: Today, we have several leading LLMs </span><a id="_idIndexMarker941" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.530.1">available. </span><span class="kobospan" id="kobo.530.2">Some are free and hosted locally, and some are API-based. </span><span class="kobospan" id="kobo.530.3">They are different in their size and capabilities, and some of them are stronger in particular tasks than others. </span><span class="kobospan" id="kobo.530.4">When forming a team of agents where each agent is assigned a different role, a different LLM may be set that best suits that role. </span><span class="kobospan" id="kobo.530.5">For instance, in the context of a coding project, where one of the agents is a programmer of a particular coding language, the user may choose to set the LLM for that agent to be an LLM that is superior for code generation in that particular </span><span><span class="kobospan" id="kobo.531.1">coding language.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.532.1">Optimizing resources – employing several smaller LLMs</span></strong><span class="kobospan" id="kobo.533.1">: Imagine a project that involves technical functions and also domain expertise. </span><span class="kobospan" id="kobo.533.2">For example, building a user platform in the medical space. </span><span class="kobospan" id="kobo.533.3">You would want there to be a frontend engineer, a backend engineer, a designer, and a medical expert, all governed by the project manager and the product manager. </span><span class="kobospan" id="kobo.533.4">If you were to develop this platform using the multiple-agents framework, you would define the agents, assign the various roles to them, and pick an LLM to drive them. </span><span class="kobospan" id="kobo.533.5">If you were to use the same LLM for all of the agents, say, OpenAI’s most recent GPT, then that model would have to be very generic, thus requiring it to be very large and perhaps very expensive and maybe even slow. </span><span class="kobospan" id="kobo.533.6">However, if you had access to individual LLMs, each pre-trained to only fulfill a limited function, for instance, one LLM dedicated to the medical service domain and a different LLM dedicated to backend development in Python, then you would assign each of those particular LLMs to their </span><span><span class="kobospan" id="kobo.534.1">corresponding agents.</span></span><p class="calibre6"><span class="kobospan" id="kobo.535.1">That may present a major reduction in model size, as the combined architecture of several specialized LLMs may be smaller in size than the architecture of one generic LLM when assuming equal performance between the </span><span><span class="kobospan" id="kobo.536.1">two scenarios.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.537.1">Optimizing resources – optimal designation of a single LLM</span></strong><span class="kobospan" id="kobo.538.1">: A unique and particular case of employing multiple LLMs is that in which we are seeking to optimize the LLM being</span><a id="_idIndexMarker942" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.539.1"> chosen per the current task. </span><span class="kobospan" id="kobo.539.2">This case is different than all the above, as it does not refer to a case where several LLMs are working simultaneously. </span><span class="kobospan" id="kobo.539.3">In this case, a routing algorithm chooses one LLM based on a current state of constraints and variables. </span><span class="kobospan" id="kobo.539.4">These may include </span><span><span class="kobospan" id="kobo.540.1">the following:</span></span><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.541.1">The current load on each of the different parts of the </span><span><span class="kobospan" id="kobo.542.1">computation system</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.543.1">The cost constraints, which may vary </span><span><span class="kobospan" id="kobo.544.1">over time</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.545.1">The source of the prompt, as different clients/regions may have </span><span><span class="kobospan" id="kobo.546.1">different priorities</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.547.1">The purpose of the prompt, as different use cases may be prioritized by </span><span><span class="kobospan" id="kobo.548.1">the business</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.549.1">The prompt’s requirements, as a code-generating task, may be obtaining excellent responses with a small and efficient code-generating LLM, whereas a request to review a legal document and to suggest precedents may call for a completely </span><span><span class="kobospan" id="kobo.550.1">different model</span></span></li></ul></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.551.1">AutoGen</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.552.1">The particular framework we</span><a id="_idIndexMarker943" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.553.1"> employ in this section is called AutoGen, and it is made available by Microsoft (GitHub </span><span><span class="kobospan" id="kobo.554.1">repo: </span></span><a href="https://github.com/microsoft/autogen/tree/main" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.555.1">https://github.com/microsoft/autogen/tree/main</span></span></a><span><span class="kobospan" id="kobo.556.1">).</span></span></p>
<p class="calibre6"><span><em class="italic"><span class="kobospan" id="kobo.557.1">Figure 9</span></em></span><em class="italic"><span class="kobospan" id="kobo.558.1">.1</span></em><span class="kobospan" id="kobo.559.1"> conveys the AutoGen framework. </span><span class="kobospan" id="kobo.559.2">The following was obtained from the statement made in the </span><span><span class="kobospan" id="kobo.560.1">GitHub repo:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.561.1">AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. </span><span class="kobospan" id="kobo.561.2">AutoGen agents are customizable, conversable, and seamlessly allow human participation. </span><span class="kobospan" id="kobo.561.3">They can operate in various modes that employ combinations of LLMs, human inputs, </span></em><span><em class="italic"><span class="kobospan" id="kobo.562.1">and tools</span></em></span><span><span class="kobospan" id="kobo.563.1">.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer364">
<span class="kobospan" id="kobo.564.1"><img alt="Figure 9.1 – AutoGen functionality" src="image/B18949_09_1.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.565.1">Figure 9.1 – AutoGen functionality</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.566.1">On the left of </span><span><em class="italic"><span class="kobospan" id="kobo.567.1">Figure 9</span></em></span><em class="italic"><span class="kobospan" id="kobo.568.1">.1</span></em><span class="kobospan" id="kobo.569.1">, we observe the designation of roles and capabilities to individual agents; on the right, we observe a few of the conversation structures that </span><span><span class="kobospan" id="kobo.570.1">are available.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.571.1">AutoGen’s key capabilities as presented in the </span><span><span class="kobospan" id="kobo.572.1">code repo:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.573.1">AutoGen enables building</span><a id="_idIndexMarker944" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.574.1"> next-gen LLM applications based on multi-agent conversations with minimal effort. </span><span class="kobospan" id="kobo.574.2">It simplifies the orchestration, automation, and optimization of a complex LLM workflow. </span><span class="kobospan" id="kobo.574.3">It maximizes the performance of LLM models and overcomes </span><span><span class="kobospan" id="kobo.575.1">their weaknesses.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.576.1">It supports diverse conversation patterns for complex workflows. </span><span class="kobospan" id="kobo.576.2">With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy, the number of agents, and agent </span><span><span class="kobospan" id="kobo.577.1">conversation topology.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.578.1">It provides a collection of working systems with different complexities. </span><span class="kobospan" id="kobo.578.2">These systems span a wide range of applications from various domains and complexities. </span><span class="kobospan" id="kobo.578.3">This demonstrates how AutoGen can easily support diverse </span><span><span class="kobospan" id="kobo.579.1">conversation patterns.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.580.1">AutoGen provides enhanced LLM inference. </span><span class="kobospan" id="kobo.580.2">It offers utilities such as API unification and caching, and advanced usage patterns such as error handling, multi-config inference, context programming, and </span><span><span class="kobospan" id="kobo.581.1">so on.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.582.1">AutoGen is powered by collaborative research studies from Microsoft, Penn State University, and the University </span><span><span class="kobospan" id="kobo.583.1">of Washington.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.584.1">Next, we can dive into a practical example in </span><span><span class="kobospan" id="kobo.585.1">the code.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.586.1">Completing a complex analysis – visualizing the results and forming a conclusion</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.587.1">Here, we will show how a team</span><a id="_idIndexMarker945" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.588.1"> of multiple agents, each with a different designated role, could serve as a professional team. </span><span class="kobospan" id="kobo.588.2">The use case we chose is a continuation of the </span><a id="_idIndexMarker946" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.589.1">previous code we ran. </span><span class="kobospan" id="kobo.589.2">In the last code, we performed a complex evaluation of employing prompt compression, and when that code finished, we had two resulting items: the </span><strong class="source-inline"><span class="kobospan" id="kobo.590.1">dict</span></strong><span class="kobospan" id="kobo.591.1"> that holds the numeric measurements of the experiments, called </span><strong class="source-inline"><span class="kobospan" id="kobo.592.1">record,</span></strong><span class="kobospan" id="kobo.593.1"> and the verbal statements about the resulting agreement rate, the reduction in tokens and cost, and the change in </span><span><span class="kobospan" id="kobo.594.1">processing time.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.595.1">With that previous notebook, we intentionally stopped short. </span><span class="kobospan" id="kobo.595.2">We didn’t visualize the reduction in tokens and cost, and we didn’t form an opinion as to whether we would advocate for employing the prompt reductions. </span><span class="kobospan" id="kobo.595.3">However, in business or academic settings, one would be required to offer both. </span><span class="kobospan" id="kobo.595.4">When you present your findings to stakeholders, decision-makers, or the research community, you are expected, when feasible, to visualize the statistical significance of the experiments. </span><span class="kobospan" id="kobo.595.5">As a subject expert in NLP and ML, you are also </span><a id="_idIndexMarker947" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.596.1">expected to provide your recommendation on whether to adopt the experimented method </span><span><span class="kobospan" id="kobo.597.1">or not.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.598.1">We will take the results from that </span><a id="_idIndexMarker948" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.599.1">evaluation, and we will task a team of agents to do the work </span><span><span class="kobospan" id="kobo.600.1">for us!</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.601.1">Refer to the following notebook: </span><strong class="source-inline"><span class="kobospan" id="kobo.602.1">Ch9_Completing_a_Complex_Analysis_with_a_Team_of_LLM_Agents.ipynb</span></strong><span class="kobospan" id="kobo.603.1">. </span><span class="kobospan" id="kobo.603.2">The notebook starts with the common aspects of installs, imports, and settings. </span><span class="kobospan" id="kobo.603.3">You will notice that AutoGen has a particular format of settings in the form of a dictionary. </span><span class="kobospan" id="kobo.603.4">They provide the details, as you can see in </span><span><span class="kobospan" id="kobo.604.1">our notebook.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.605.1">Now, we move on to the </span><span><span class="kobospan" id="kobo.606.1">interesting parts!</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.607.1">Creating a visualization of the significance of the experiments</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.608.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.609.1">record.pickle</span></strong> <strong class="source-inline"><span class="kobospan" id="kobo.610.1">file</span></strong><span class="kobospan" id="kobo.611.1"> is of a </span><strong class="source-inline"><span class="kobospan" id="kobo.612.1">dict</span></strong><span class="kobospan" id="kobo.613.1"> variable. </span><span class="kobospan" id="kobo.613.2">It is the collection of numerical results from the previous evaluation notebook. </span><span class="kobospan" id="kobo.613.3">Our wish is to visualize the distributions of the token counts for each of the</span><a id="_idIndexMarker949" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.614.1"> experiments. </span><span class="kobospan" id="kobo.614.2">There are token counts for original prompts and token counts for compressed prompts. </span><span class="kobospan" id="kobo.614.3">There are </span><a id="_idIndexMarker950" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.615.1">also the ratios between the two for </span><span><span class="kobospan" id="kobo.616.1">each experiment.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.617.1">In this section, we'll form a team to put code together that would visualize the distributions of each of </span><span><span class="kobospan" id="kobo.618.1">the three.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.619.1">Defining the task to be fulfilled by the team</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.620.1">First, we define the task to be fulfilled by the</span><a id="_idIndexMarker951" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.621.1"> team. </span><span class="kobospan" id="kobo.621.2">We tell the team where the file is saved and the context and the nature of the values in the </span><strong class="source-inline"><span class="kobospan" id="kobo.622.1">dict</span></strong><span class="kobospan" id="kobo.623.1">, thus giving the team the understanding they need to ideate a solution to the task. </span><span class="kobospan" id="kobo.623.2">Then, we describe the task of creating a plot and visualizing the distributions. </span><span class="kobospan" id="kobo.623.3">All those details are in the one string that describes the task. </span><span class="kobospan" id="kobo.623.4">Note that in an Agile Scrum work setting, this task string is similar to the purpose of </span><span><span class="kobospan" id="kobo.624.1">the story.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.625.1">Now that we have formed a comprehensive description, it should be clear what is expected. </span><span class="kobospan" id="kobo.625.2">For instance, we ask for the figures and axes to be labeled, but we don’t explicitly state what labels are </span><a id="_idIndexMarker952" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.626.1">expected. </span><span class="kobospan" id="kobo.626.2">The agents will understand on their own, just as we would have understood this on our own, as the labels are inferred from the task and the data </span><span><span class="kobospan" id="kobo.627.1">field names.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.628.1">Defining the agents and assigning the team members roles</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.629.1">For this task, we would need three team members: a programmer to write the code, a QA engineer to run the code and provide feedback, and a team lead to verify when the task </span><span><span class="kobospan" id="kobo.630.1">is complete.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.631.1">For each of the roles, we articulate a</span><a id="_idIndexMarker953" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.632.1"> system prompt. </span><span class="kobospan" id="kobo.632.2">This system prompt, as we learned in </span><a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.633.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.634.1">, has a significant impact on the LLM’s function. </span><span class="kobospan" id="kobo.634.2">Notice that we also provide the QA engineer and the team lead with the ability to run code on their own. </span><span class="kobospan" id="kobo.634.3">In this way, they will be able to verify the programmer’s code and provide objective feedback. </span><span class="kobospan" id="kobo.634.4">If we told the same agent to write the</span><a id="_idIndexMarker954" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.635.1"> code and to confirm that it is correct, we might find that, in practice, it would generate a first draft, wouldn’t bother to run and verify it, and it would conclude that task without having </span><span><span class="kobospan" id="kobo.636.1">verified it.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.637.1">Defining a group conversation</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.638.1">Here, we define the conversation</span><a id="_idIndexMarker955" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.639.1"> to be a multi-agent conversation; this is one of the features of AutoGen. </span><span class="kobospan" id="kobo.639.2">This is slightly different from the case where you define a series of conversations where each conversation involves just two agents. </span><span class="kobospan" id="kobo.639.3">The group conversation involves </span><span><span class="kobospan" id="kobo.640.1">more agents.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.641.1">When defining a group conversation, we also define a manager for </span><span><span class="kobospan" id="kobo.642.1">the conversation.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.643.1">Deploying the team</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.644.1">The team lead tasks the manager with the task we defined. </span><span class="kobospan" id="kobo.644.2">The manager then delegates the work to the programmer and the </span><a id="_idIndexMarker956" class="calibre5 pcalibre1 pcalibre"/><span><span class="kobospan" id="kobo.645.1">QA engineer.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.646.1">Here are the highlights of that automated conversation as it appears on </span><span><span class="kobospan" id="kobo.647.1">the screen:</span></span></p>
<pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.648.1">lead (to manager_0):</span></strong><span class="kobospan1" id="kobo.649.1">
Refer to the Python dict that is in this [...]
</span><strong class="bold1"><span class="kobospan1" id="kobo.650.1">programmer (to manager_0):</span></strong><span class="kobospan1" id="kobo.651.1">
```python
import pandas as pd
import matplotlib.pyplot as plt
# Load the record dict from URL
import requests
import pickle
[...]
</span><strong class="bold1"><span class="kobospan1" id="kobo.652.1">qa_engineer (to manager_0):</span></strong><span class="kobospan1" id="kobo.653.1">
exitcode: 0 (execution succeeded)
Code output:
Figure(640x480)
</span><strong class="bold1"><span class="kobospan1" id="kobo.654.1">programmer (to manager_0):</span></strong><span class="kobospan1" id="kobo.655.1">
TERMINATE</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.656.1">As can be seen, the conversation had four interactions, each between two agents. </span><span class="kobospan" id="kobo.656.2">Each interaction starts by telling the user which agent is talking to which other agent; these parts are in bold letters in the </span><span><span class="kobospan" id="kobo.657.1">preceding printout.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.658.1">In the second interaction, the programmer provided a complete Python script. </span><span class="kobospan" id="kobo.658.2">We pasted only the first four commands to keep it short, but you can observe the full script in the notebook. </span><span class="kobospan" id="kobo.658.3">The QA engineer ran the script and reported that it ran well. </span><span class="kobospan" id="kobo.658.4">If it hadn’t run well, it would have returned an </span><strong class="source-inline"><span class="kobospan" id="kobo.659.1">exitcode: 1</span></strong><span class="kobospan" id="kobo.660.1"> and would have provided the programmer with the error specification for the programmer to fix the code; the conversation would have continued until a solution was found, or, if not, the team would report failure and conclude </span><span><span class="kobospan" id="kobo.661.1">the conversation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.662.1">This task provided us with the code to create the visual we wanted. </span><span class="kobospan" id="kobo.662.2">Note that we didn’t ask the agents to run the code and provide us with the visual; we asked for the code itself. </span><span class="kobospan" id="kobo.662.3">One could, if desired, configure</span><a id="_idIndexMarker957" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.663.1"> the LLMs to run the code and provide us with the resulting image. </span><span class="kobospan" id="kobo.663.2">See AutoGen’s repo for the various examples </span><span><span class="kobospan" id="kobo.664.1">and capabilities.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.665.1">In the next code cell, we pasted the code that the team created. </span><span class="kobospan" id="kobo.665.2">The code runs well and visualizes the three distributions exactly as we asked the team (see </span><span><em class="italic"><span class="kobospan" id="kobo.666.1">Figure 9</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.667.1">.2</span></em></span><span><span class="kobospan" id="kobo.668.1">):</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer365">
<span class="kobospan" id="kobo.669.1"><img alt="Figure 9.2 – Visualizing the value that prompt compression provides" src="image/B18949_09_2.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.670.1">Figure 9.2 – Visualizing the value that prompt compression provides</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.671.1">The top visualization displays the distributions of the token count for the original prompts (blue/light shade) and the compressed prompts (orange/dark shade), and the bottom part of the figure shows the</span><a id="_idIndexMarker958" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.672.1"> distribution of the ratio between each pair of prompts. </span><span><em class="italic"><span class="kobospan" id="kobo.673.1">Figure 9</span></em></span><em class="italic"><span class="kobospan" id="kobo.674.1">.2</span></em><span class="kobospan" id="kobo.675.1"> shows just how effective the reduction rate is, as this ratio translates to a reduction in </span><span><span class="kobospan" id="kobo.676.1">API cost.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.677.1">This concludes the visualization of the significance of </span><span><span class="kobospan" id="kobo.678.1">the experiments.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.679.1">Human intervention in the team’s tasks</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.680.1">Note that all three agents are driven by LLMs, thus making this entire task automatically performed without human</span><a id="_idIndexMarker959" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.681.1"> intervention. </span><span class="kobospan" id="kobo.681.2">One could change the lead’s configuration to represent a human user, meaning you. </span><span class="kobospan" id="kobo.681.3">If you did that, then you would be</span><a id="_idIndexMarker960" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.682.1"> able to intervene and demand certain verifications from the QA engineer or certain additional features in the code from </span><span><span class="kobospan" id="kobo.683.1">the programmer.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.684.1">This could be particularly useful if you wanted to run the code yourself in your environment instead of letting the QA engineer agent run it in its own environment. </span><span class="kobospan" id="kobo.684.2">Your environments are different. </span><span class="kobospan" id="kobo.684.3">One advantage of doing this is when the code is required to load a data file that you have locally. </span><span class="kobospan" id="kobo.684.4">If you told the agent to write code that loads this file, then when the QA engineer agent ran it, it would tell you the code failed since that data file doesn’t exist in its environment. </span><span class="kobospan" id="kobo.684.5">In this case, you may elect to be the one who iterates with the programmer and the one who runs the code during the iterations and </span><span><span class="kobospan" id="kobo.685.1">provides feedback.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.686.1">Another case where you would want to be the one running the code and providing feedback is when the QA engineer encounters an error or a bug in the programmer’s code, but the two agents aren’t able to figure out the solution. </span><span class="kobospan" id="kobo.686.2">In that case, you would want to intervene and provide your insight. </span><span class="kobospan" id="kobo.686.3">For instance, in a case where a for loop iterates over a dict’s keys instead of its values, you may intervene and enter </span><em class="italic"><span class="kobospan" id="kobo.687.1">The code runs but the for loop is iterating on the dict’s keys. </span><span class="kobospan" id="kobo.687.2">It should iterate over its values for the </span></em><span><em class="italic"><span class="kobospan" id="kobo.688.1">key ‘key1.</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.689.1">We can now move on to the second part of concluding </span><span><span class="kobospan" id="kobo.690.1">the evaluation.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.691.1">Reviewing the results of the experiments and forming an educated conclusion</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.692.1">As with every complex evaluation where we perform experiments to target the impact of a particular feature, we would now like to derive a qualitative summary of the results and suggest a conclusion for</span><a id="_idIndexMarker961" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.693.1"> our audience, whether it is the decision-makers in the company or the research community </span><span><span class="kobospan" id="kobo.694.1">in academia.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.695.1">What is unique about this part is that the act of deriving a conclusion has never been left to any mathematical or algorithmic model to derive. </span><span class="kobospan" id="kobo.695.2">As we humans govern the various evaluations, and although we may seek to automate as much as possible to feed into the final conclusion, we are the entity that forms the final impression </span><span><span class="kobospan" id="kobo.696.1">and conclusion.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.697.1">Here, we attempt to automate that final part. </span><span class="kobospan" id="kobo.697.2">We will assign a team of expert agents to provide an educated summary of the results that the evaluation notebook printed out. </span><span class="kobospan" id="kobo.697.3">We'll then push the team to provide us with a recommendation as to whether we should implement the new feature of prompt compression or not. </span><span class="kobospan" id="kobo.697.4">We provide the team with the actual results of the evaluation notebook, but in order to examine its reliability, we then task it again, this time providing it with mocked results that are much poorer, hoping that the team will apply judgment and provide a different recommendation. </span><span class="kobospan" id="kobo.697.5">All of this is done without any </span><span><span class="kobospan" id="kobo.698.1">human intervention.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.699.1">As we did before, we start by defining the task for our team </span><span><span class="kobospan" id="kobo.700.1">to fulfill.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.701.1">Defining the task to be fulfilled by the team</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.702.1">Our aim is to provide </span><a id="_idIndexMarker962" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.703.1">the team with the printout of the evaluation notebook from the previous section. </span><span class="kobospan" id="kobo.703.2">That printout describes, in words, the change in agreement rate, the impact on the number of prompt tokens, and the processing runtime, all due to employing the LLMLingua prompt </span><span><span class="kobospan" id="kobo.704.1">compression method.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.705.1">We then copy that from the previous notebook and paste it as a </span><span><span class="kobospan" id="kobo.706.1">text string.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.707.1">Note that we have also created another text string of results (which are mocked results that are much worse than the true results), but we see that the agreement rate is very low, and the reduction in token count due to compression is much </span><span><span class="kobospan" id="kobo.708.1">less significant.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.709.1">As we did in the visualization case, we then create the instructions for the team; we paste the results into the task description for the team to refer to when deriving its conclusion. </span><span class="kobospan" id="kobo.709.2">We have two task descriptions, as we will have two separate runs, one with the true results and one </span><a id="_idIndexMarker963" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.710.1">with the mocked </span><span><span class="kobospan" id="kobo.711.1">bad results.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.712.1">We will now allocate </span><span><span class="kobospan" id="kobo.713.1">the roles.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.714.1">Defining the agents and assigning team members roles</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.715.1">For this task, we would </span><a id="_idIndexMarker964" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.716.1">need three team members: a</span><a id="_idIndexMarker965" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.717.1"> principal engineer who is an experienced technical person, a technical writer who writes the conclusion as per the principal engineer’s feedback, and a team lead to verify when the task is complete, which was defined in the </span><span><span class="kobospan" id="kobo.718.1">previous task.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.719.1">Defining a group conversation</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.720.1">Here, we define the group</span><a id="_idIndexMarker966" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.721.1"> conversation, just like we did in the visualization part. </span><span class="kobospan" id="kobo.721.2">This time, we have a new group conversation manager, as the group consists of </span><span><span class="kobospan" id="kobo.722.1">different agents.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.723.1">Deploying the team</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.724.1">The team lead tasks the manager </span><a id="_idIndexMarker967" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.725.1">with the task we defined. </span><span class="kobospan" id="kobo.725.2">The manager then delegates the work to the writer and the </span><span><span class="kobospan" id="kobo.726.1">principal engineer.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.727.1">Here are the highlights of that automated conversation as it appears on </span><span><span class="kobospan" id="kobo.728.1">the screen:</span></span></p>
<pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.729.1">lead (to manager_1):</span></strong><span class="kobospan1" id="kobo.730.1">
Refer to the results printed below.
</span><span class="kobospan1" id="kobo.730.2">These are the results that stem from [...]
</span><strong class="bold1"><span class="kobospan1" id="kobo.731.1">writer (to manager_1):</span></strong><span class="kobospan1" id="kobo.732.1">
The experiments on prompt compression using LLMLingua have produced the following results:
- Classification Performance:
  - Agreement rate of [...]
</span><strong class="bold1"><span class="kobospan1" id="kobo.733.1">principal_engineer (to manager_1):</span></strong><span class="kobospan1" id="kobo.734.1">
[...]</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.735.1">The agents have a few iterations between them and come to an agreement regarding the summary and </span><span><span class="kobospan" id="kobo.736.1">the </span></span><span><a id="_idIndexMarker968" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.737.1">conclusion.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.738.1">They provide a summary of the numeric results and seal it with the </span><span><span class="kobospan" id="kobo.739.1">following recommendation:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.740.1">
It is imperative to carefully consider the trade-offs presented by prompt compression, as while it may lead to resource savings, there might be implications on processing efficiency. </span><span class="kobospan1" id="kobo.740.2">The decision to adopt prompt compression should be made with a thorough understanding of these trade-offs.</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.741.1">The team agrees on a cautious approach to presenting the various trade-offs and avoids making a decision in spite of being tasked to </span><span><span class="kobospan" id="kobo.742.1">do so.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.743.1">One would wonder, could a definite decision to adopt or not to adopt the method be </span><span><span class="kobospan" id="kobo.744.1">made here?</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.745.1">Evaluation of the team’s judgment</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.746.1">Now, we will ask the team to</span><a id="_idIndexMarker969" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.747.1"> perform the same action, this time providing it with the mocked results that make the compression method seem much less effective and with a great reduction in agreement with the classification of the </span><span><span class="kobospan" id="kobo.748.1">noncompressed method.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.749.1">The team has a conversation, and the final agreement summary is sealed with the </span><span><span class="kobospan" id="kobo.750.1">following statement:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.751.1">
Overall, the results indicate that while prompt compression may lead to cost savings and resource reduction, it comes at the expense of decreased classification performance and significantly increased processing times.
</span><span class="kobospan1" id="kobo.751.2">**Recommendation:** Prompt compression using LLMLinguam is **not recommended** as it can negatively impact classification performance and significantly increase processing times, outweighing the potential cost savings.</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.752.1">Here, the team found it much easier to draw a definite conclusion. </span><span class="kobospan" id="kobo.752.2">It did so without any human intervention and </span><a id="_idIndexMarker970" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.753.1">solely based on the numerical results it </span><span><span class="kobospan" id="kobo.754.1">was given.</span></span></p>
<h2 id="_idParaDest-220" class="calibre7"><a id="_idTextAnchor523" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.755.1">Concluding thoughts on the multiple-agent team</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.756.1">This emerging method </span><a id="_idIndexMarker971" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.757.1">of simultaneously employing several LLMs is gaining interest and traction in the world of AI. </span><span class="kobospan" id="kobo.757.2">In the code experiments that we present in this section, it was proven without a doubt that AutoGen’s group conversation can provide tangible and actionable value in the professional setting. </span><span class="kobospan" id="kobo.757.3">Although setting these code experiments required a series of trials and errors for properly setting the agent roles and properly describing the tasks, it suggests that this framework is moving in a direction where less human intervention is required. </span><span class="kobospan" id="kobo.757.4">What seems to remain a monumental</span><a id="_idIndexMarker972" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.758.1"> component is the human oversight, feedback, and evaluation of the resulting relics of those agent teams’ </span><em class="italic"><span class="kobospan" id="kobo.759.1">work. </span></em><span class="kobospan" id="kobo.760.1">We would like to stress to the reader that of the various application and innovations that we share in this book, we have marked the multiple-agent framework as the one that is most likely to grow and to also become the most popular. </span><span class="kobospan" id="kobo.760.2">This is based on the overwhelming expectations that industries have from AI to automate and demonstrate human-like expertise, while innovations such as Autogen, and later Autodev, both by Microsoft, are exemplifying growing feasibility </span><span><span class="kobospan" id="kobo.761.1">and competency.</span></span></p>
<h1 id="_idParaDest-221" class="calibre4"><a id="_idTextAnchor524" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.762.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.763.1">Throughout this pivotal chapter, we have embarked on an in-depth exploration of the most recent and groundbreaking applications of LLMs, presented through comprehensive Python code examples. </span><span class="kobospan" id="kobo.763.2">We began by unlocking advanced functionalities by using the RAG framework and LangChain, enhancing LLM performance for domain-specific tasks. </span><span class="kobospan" id="kobo.763.3">The journey continued with advanced methods in chains for sophisticated formatting and processing, followed by the automation of information retrieval from diverse web sources. </span><span class="kobospan" id="kobo.763.4">We also tackled the optimization of prompt engineering through prompt compression techniques, significantly reducing API costs. </span><span class="kobospan" id="kobo.763.5">Finally, we ventured into the collaborative potential of LLMs by forming a team of models that work in concert to solve </span><span><span class="kobospan" id="kobo.764.1">complex problems.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.765.1">By mastering these topics, you have now acquired a robust set of skills, enabling you to harness the power of LLMs for a variety of applications. </span><span class="kobospan" id="kobo.765.2">These newfound abilities not only prepare you to tackle current challenges in NLP but also equip you with the insights to innovate and push the boundaries of what’s possible in the field. </span><span class="kobospan" id="kobo.765.3">The practical knowledge gained from this chapter will empower you to apply advanced LLM techniques to real-world issues, opening up new opportunities for efficiency, creativity, </span><span><span class="kobospan" id="kobo.766.1">and problem-solving.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.767.1">As we turn the page, the next chapter will take us into the realm of emerging trends in AI and LLM technology. </span><span class="kobospan" id="kobo.767.2">We will delve into the latest algorithmic developments, assess their impact on various business sectors, and consider the future landscape of AI. </span><span class="kobospan" id="kobo.767.3">This forthcoming discussion promises to provide you with a comprehensive understanding of where the field is headed and how you can stay at the forefront of </span><span><span class="kobospan" id="kobo.768.1">technological innovation.</span></span></p>
</div>
</body></html>