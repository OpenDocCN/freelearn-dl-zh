<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-204"><a id="_idTextAnchor506" class="calibre5 pcalibre1 pcalibre"/>9</h1>
<h1 id="_idParaDest-205" class="calibre4"><a id="_idTextAnchor507" class="calibre5 pcalibre1 pcalibre"/>Exploring the Frontiers: Advanced Applications and Innovations Driven by LLMs</h1>
<p class="calibre6">In the rapidly evolving landscape of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) have marked a revolutionary step forward, reshaping how we interact with information, automate processes, and derive insights from vast data pools. This chapter represents the culmination of our journey through the emergence and development of NLP methods. It is here that the theoretical foundations laid in previous chapters converge with practical, cutting-edge applications, illuminating the remarkable capabilities of LLMs when harnessed with the right tools and techniques.</p>
<p class="calibre6">We delve into the most recent and thrilling advancements in LLM applications, presented through detailed Python code examples designed for hands-on learning. This approach not only illustrates the power of LLMs but also equips you with the skills to implement these technologies in real-world scenarios. The subjects covered in this chapter are meticulously selected to showcase a spectrum of advanced functionalities and applications.</p>
<p class="calibre6">The importance of this chapter cannot be overstated. It not only reflects the state of the art in NLP but also serves as a bridge to the future, where the integration of these technologies into everyday solutions becomes seamless. By the end of this chapter, you will have a comprehensive understanding of how to apply the latest LLM techniques and innovations, empowering you to push the boundaries of what’s possible in NLP and beyond. Join us on this exciting journey to unlock the full potential of LLMs.</p>
<p class="calibre6">Let’s go through the main headings covered in the chapter:</p>
<ul class="calibre14">
<li class="calibre15">Enhancing LLM performance with RAG and LangChain – a dive into advanced functionalities</li>
<li class="calibre15">Advanced methods with chains</li>
<li class="calibre15">Retrieving information from various web sources automatically</li>
<li class="calibre15">Prompt compression and API cost reduction</li>
<li class="calibre15">Multiple agents – forming a team of LLMs who collaborate</li>
</ul>
<h1 id="_idParaDest-206" class="calibre4"><a id="_idTextAnchor508" class="calibre5 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre6">For this chapter, the following will be necessary:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Programming knowledge</strong>: Familiarity with Python programming is a must since the open source models, OpenAI’s API, and LangChain are all illustrated using Python code.</li>
<li class="calibre15"><strong class="bold">Access to OpenAI’s API</strong>: An API key from OpenAI will be required to explore closed source models. This can be obtained by creating an account with OpenAI and agreeing to their terms of service.</li>
<li class="calibre15"><strong class="bold">Open source models</strong>: Access to the specific open source models mentioned in this chapter will be necessary. These can be accessed and downloaded from their respective repositories or via package managers such as pip or conda.</li>
<li class="calibre15"><strong class="bold">Local development environment</strong>: A local development environment setup with Python installed is required. An <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) such as <strong class="bold">PyCharm</strong>, <strong class="bold">Jupyter Notebook</strong>, or a simple text editor can be used. Note that we recommend a free <strong class="bold">Google Colab</strong> notebook, as it encapsulates all these requirements in a seamless web interface.</li>
<li class="calibre15"><strong class="bold">Ability to install libraries</strong>: You must have permission for the installation of the required Python libraries such as <strong class="bold">NumPy</strong>, <strong class="bold">SciPy</strong>, <strong class="bold">TensorFlow</strong>, and <strong class="bold">PyTorch</strong>. Note that the code we provide includes the required installations so you won’t have to install them ahead of time. We simply stress that you should have permission to do so, which we expect you would. Specifically, using a free Google Colab notebook would suffice.</li>
<li class="calibre15"><strong class="bold">Hardware requirements</strong>: Depending on the complexity and size of the models you’re working with, a computer with sufficient processing power (potentially including a good GPU for ML tasks) and ample memory will be required. This is only relevant when choosing to not use the free Google Colab.</li>
</ul>
<p class="calibre6">Now that we’ve set up LLM applications using APIs and locally, we can finally deploy the advanced applications of LLMs that let us leverage their immense power.</p>
<h1 id="_idParaDest-207" class="calibre4"><a id="_idTextAnchor509" class="calibre5 pcalibre1 pcalibre"/>Enhancing LLM performance with RAG and LangChain – a dive into advanced functionalities</h1>
<p class="calibre6">The <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) framework has become instrumental in tailoring <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) for specific<a id="_idIndexMarker897" class="calibre5 pcalibre1 pcalibre"/> domains or tasks, bridging the gap between the simplicity of prompt engineering and the complexity <a id="_idIndexMarker898" class="calibre5 pcalibre1 pcalibre"/>of model fine-tuning.</p>
<p class="calibre6">Prompt engineering<a id="_idIndexMarker899" class="calibre5 pcalibre1 pcalibre"/> stands as the initial, most accessible technique for<a id="_idIndexMarker900" class="calibre5 pcalibre1 pcalibre"/> customizing LLMs. It leverages the model’s capacity to interpret and respond to queries based on the input prompt. For example, to inquire if Nvidia surpassed earnings expectations in its latest announcement, directly providing the earnings call content within the prompt can compensate for the LLM’s lack of immediate, up-to-date context. This approach, while straightforward, hinges on the model’s ability to digest and analyze the provided information within a single or a series of carefully crafted prompts.</p>
<p class="calibre6">When the scope of inquiry exceeds what prompt engineering can accommodate—such as analyzing a decade’s worth of tech sector earnings calls—RAG becomes indispensable. Prior to RAG’s adoption, the alternative was fine-tuning, a resource-intensive process requiring significant adjustments to the LLM’s architecture to incorporate extensive datasets. RAG simplifies this by preprocessing and storing large amounts of data in a vector database. It intelligently isolates and retrieves the data segments pertinent to the query, effectively condensing the vast information into a manageable, prompt-size context for the LLM. This innovation drastically reduces the time, resources, and expertise needed for such extensive data familiarization tasks.</p>
<p class="calibre6">In <a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 8</em></a>, we introduced the general concept of RAGs and, in particular, LangChain, a RAG framework distinguished by its advanced capabilities.</p>
<p class="calibre6">We will now discuss the additional unique features LangChain offers for enhancing LLM applications, providing you with practical insights into its implementation and utility in complex NLP tasks.</p>
<h2 id="_idParaDest-208" class="calibre7"><a id="_idTextAnchor510" class="calibre5 pcalibre1 pcalibre"/>LangChain pipeline with Python – enhancing performance with LLMs</h2>
<p class="calibre6">In this section, we will pick up where we left off with our last example from <a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 8</em></a>. In this scenario, we are in<a id="_idIndexMarker901" class="calibre5 pcalibre1 pcalibre"/> the healthcare sector, and in our hospital, our care providers are expressing a need to be able to quickly surface patients’ records based on rough descriptions of the patient or their condition. For example, “Who was that patient I saw last year who was pregnant with triplets?” “Did I ever have a patient with a history of cancer from both of their parents and they were interested in a clinical trial?” and so on.</p>
<p class="callout-heading">Important note</p>
<p class="callout">We stress that these aren’t real medical notes and that the people described in the notes aren’t real.</p>
<p class="calibre6">In our example in <a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 8</em></a>, we kept the pipeline at minimum complexity by simply leveraging the vector databases of embeddings of clinical notes, and then we applied similarity search to look for notes based on simple requests. We noticed how one of the questions, the second question, received a wrong answer with the similarity search algorithm.</p>
<p class="calibre6">We will now enhance that pipeline. We will not settle for the results of the similarity search and surface those to the physicians; we will take those results that were deemed to be similar in content to the request, and we will employ an LLM to go through these results, vet them, and tell us which ones are indeed relevant to the physician.</p>
<h3 class="calibre8">Paid LLMs versus free</h3>
<p class="calibre6">We'll use this pipeline to<a id="_idIndexMarker902" class="calibre5 pcalibre1 pcalibre"/> exemplify the utility of either type of LLM, paid or free. We give you the choice, via the <code>paid_vs_free</code> variable, to either use OpenAI’s paid GPT model or a free LLM. Using OpenAI’s paid model would leverage their API and would require an API key. However, the free LLM is imported to the local environment where the Python code is run, thus making it available to anyone who has an internet connection and sufficient computational resources.</p>
<p class="calibre6">Let’s start getting hands-on and experimenting with the code.</p>
<h3 class="calibre8">Applying advanced LangChain configurations and pipelines</h3>
<p class="calibre6">Refer to the following notebook: <code>Ch9_Advanced_LangChain_Configurations_and_Pipeline.ipynb</code>.</p>
<p class="calibre6">Note that the first<a id="_idIndexMarker903" class="calibre5 pcalibre1 pcalibre"/> part of the notebook is identical to the notebook from <a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 8</em></a>, so we will skip the description of that part.</p>
<h4 class="calibre135">Installing the required Python libraries</h4>
<p class="calibre6">Here, we need to expand the <a id="_idIndexMarker904" class="calibre5 pcalibre1 pcalibre"/>set of installed libraries and install <code>openai</code> and <code>gpt4all</code>. Moreover, in order to utilize <code>gpt4all</code>, we will need to download a <code>.bin</code> file from the web.</p>
<p class="calibre6">These two steps are easy to perform via the notebook.</p>
<h4 class="calibre135">Setting up an LLM – choose between a paid LLM (OpenAI’s GPT) and a free LLM (from Hugging Face)</h4>
<p class="calibre6">As explained above, we let you choose <a id="_idIndexMarker905" class="calibre5 pcalibre1 pcalibre"/>whether you want to run this example via a paid API by OpenAI or a free LLM.</p>
<p class="calibre6">Remember, since OpenAI’s service includes hosting the LLM and processing the prompts, it requires minimal resources and time and a basic internet connection. It also involves sending our prompts to OpenAI’s API service. Prompts typically include information that, in real-world settings, may be proprietary. Thus, an executive decision needs to be made regarding the security of the data. Similar considerations were central, in the last decade, to the transition of companies’ computation from on-premises to the cloud.</p>
<p class="calibre6">In contrast to that requirement, with a free LLM, you would host it locally, you would avoid exporting any information outside of your computation environment, but you would take on the processing.</p>
<p class="calibre6">Another aspect to consider is the terms of use of each LLM, as each may have different license terms. While an LLM may allow you to experiment with it for free, it may present constrictions on whether you may use it in a commercial product.</p>
<p class="calibre6">In the context of constraints around runtime and computational resources, choosing the paid LLM for this example will yield quicker responses.</p>
<p class="calibre6">In order to accommodate your wish to experiment with a free LLM, and since we aspire to let you run the code quickly and for free on Google Colab, we must restrict our choice of LLMs to those that can be <a id="_idIndexMarker906" class="calibre5 pcalibre1 pcalibre"/>run on the limited RAM that Google lets us have with a free account. In order to do that, we chose an LLM with reduced precision, also known as a quantized LLM.</p>
<p class="calibre6">Based on your choice between an API-based LLM and a free local LLM, the LLM will be assigned to the <code>llm variable</code>.</p>
<h4 class="calibre135">Creating a QA chain</h4>
<p class="calibre6">Here, we set up a<a id="_idIndexMarker907" class="calibre5 pcalibre1 pcalibre"/> RAG framework. It is designed to accept various text documents and set them up for retrieval.</p>
<h4 class="calibre135">Search based on the same requirements when using the LLM as the “brain” instead of embedding similarity</h4>
<p class="calibre6">We will now run the exact same requests as we did in the example in <a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 8</em></a>. Those will be performed across the same notes, and the same vector DB that holds the same embedding. None of that has been changed or enhanced. The difference is that we will have the LLM oversee the processing of the answers.</p>
<p class="calibre6">In <a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 8</em></a>, we saw that question number two received a wrong answer. The question was, “Are there any pregnant patients who are due to deliver in September?”</p>
<p class="calibre6">The answer we saw in <a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 8</em></a> was about a patient who is due to give birth in August. The mistake was due to the deficiency of the similarity algorithm. Indeed, that patient’s notes had content similar to that of the question, but the fine detail of giving birth in a different month should have been the factor that made those note irrelevant.</p>
<p class="calibre6">Here, in our current pipeline, where OpenAI’s LLM is applied, it gets it right, telling us that there are no patients who are due to deliver in September.</p>
<p class="calibre6">Note that when opting for the free LLM, it gets it wrong. This exemplifies the sub-optimal aspects of that model, as it is quantized in an effort to save on RAM requirements.</p>
<p class="calibre6">To conclude this example, we have put together an in-house search mechanism that lets the user, in our example, a physician, search through their patients’ notes to find patients based on some <a id="_idIndexMarker908" class="calibre5 pcalibre1 pcalibre"/>criteria. A unique aspect of this system design is the ability to let the LLM retrieve the relevant answer from an external data source and not be limited to the data it was trained on. This paradigm is the basis of RAG.</p>
<p class="calibre6">In the next section, we will showcase more uses for LLMs.</p>
<h1 id="_idParaDest-209" class="calibre4"><a id="_idTextAnchor511" class="calibre5 pcalibre1 pcalibre"/>Advanced methods with chains</h1>
<p class="calibre6">In this section, we will continue our exploration of ways one can utilize LLM pipelines. We will focus on chains.</p>
<p class="calibre6">Refer to the following notebook: <code>Ch9_Advanced_Methods_with_Chains.ipynb</code>. This notebook presents an evolution of a chain<a id="_idIndexMarker909" class="calibre5 pcalibre1 pcalibre"/> pipeline, as every iteration exemplifies another feature that LangChain allows us to employ.</p>
<p class="calibre6">For the sake of using minimal computational resources, memory, and time, we use OpenAI’s API. You can choose to use a free LLM instead and may do so in a similar way to how we set up the notebook from the previous example in this chapter.</p>
<p class="calibre6">The notebook starts with the basic configurations, as always, so we can skip to reviewing the notebook’s content.</p>
<h2 id="_idParaDest-210" class="calibre7"><a id="_idTextAnchor512" class="calibre5 pcalibre1 pcalibre"/>Asking the LLM a general knowledge question</h2>
<p class="calibre6">In this example, we want to <a id="_idIndexMarker910" class="calibre5 pcalibre1 pcalibre"/>use the LLM to tell us an answer to a simple question that would require common knowledge that a trained LLM is expected to have:</p>
<pre class="source-code">
"Who are the members of Metallica. List them as comma separated."</pre> <p class="calibre6">We then define a simple chain called <code>LLMChain</code>, and we feed it with the <code>LLM</code> variable and the prompt.</p>
<p class="calibre6">The LLM, indeed, knows the<a id="_idIndexMarker911" class="calibre5 pcalibre1 pcalibre"/> answer from its knowledge base and returns:</p>
<pre class="source-code">
'James Hetfield, Lars Ulrich, Kirk Hammett, Robert Trujillo'</pre> <h2 id="_idParaDest-211" class="calibre7"><a id="_idTextAnchor513" class="calibre5 pcalibre1 pcalibre"/>Requesting output structure – making the LLM provide output in a particular data format</h2>
<p class="calibre6">This time, we would like the <a id="_idIndexMarker912" class="calibre5 pcalibre1 pcalibre"/>output to be in a particular syntax, potentially allowing us to use it in a computational manner for downstream tasks:</p>
<pre class="source-code">
"List the first 10 elements from the periodical table as comma separated list."</pre> <p class="calibre6">Now, we add a feature for achieving the syntax. We define the <code>output_parser variable</code>, and we use a different function for generating the output, <code>predict_and_parse()</code>.</p>
<p class="calibre6">The output is the following:</p>
<pre class="source-code">
['Hydrogen',
'Helium',
'Lithium',
'Beryllium',
'Boron',
'Carbon',
'Nitrogen',
'Oxygen',
'Fluorine',
'Neon']</pre> <h2 id="_idParaDest-212" class="calibre7"><a id="_idTextAnchor514" class="calibre5 pcalibre1 pcalibre"/>Evolving to a fluent conversation – inserting an element of memory to have previous interactions as reference and context for follow-up prompts</h2>
<p class="calibre6">This feature brings a<a id="_idIndexMarker913" class="calibre5 pcalibre1 pcalibre"/> new level of value to the chain. Until this point, the prompts didn’t have any context. The LLM processed each prompt independently. For instance, if you wanted to ask a follow-up question, you couldn’t. The pipeline didn’t have your prior prompts and the responses to them as reference.</p>
<p class="calibre6">In order to go from asking disjointed questions to having an ongoing, rolling conversation-like experience, LangChain offers <code>ConversationChain()</code>. Within this function, we have a <code>memory</code> parameter that maps the prior interactions with the chain to the current prompt. Therefore, the prompt template is where that memory “lives.”</p>
<p class="calibre6">Instead of prompting with a basic template, such as</p>
<pre class="source-code">
"List all the holidays you know as comma separated list."</pre> <p class="calibre6">the template now accommodates the memory feature:</p>
<pre class="source-code">
"Current conversation:
{history}
Your task:
{input}}"</pre> <p class="calibre6">Here, you can think of this string as being formatted similarly to a Python <code>f"…"</code> string, where <code>history</code> and <code>input</code> are string variables. The <code>ConversationChain()</code> function processes this prompt template and inserts these two variables to complete the prompt string. The <code>input</code> variable is produced by the function itself as we activate the memory mechanism, and the input variable is then supplied by us as we run the following:</p>
<pre class="source-code">
conversation.predict_and_parse(input="Write the first 10 holidays you know, as a comma separated list.")</pre> <p class="calibre6">Where the output is the following:</p>
<pre class="source-code">
['Christmas',
'Thanksgiving',
"New Year's Day",
'Halloween',
'Easter',
'Independence Day',
"Valentine's Day",
"St. Patrick's Day",
'Labor Day',
'Memorial Day']</pre> <p class="calibre6">Now, let’s make a follow-up <a id="_idIndexMarker914" class="calibre5 pcalibre1 pcalibre"/>request that would only be understood in the context of the previous request and output:</p>
<pre class="source-code">
conversation.predict_and_parse(input=" Observe the list of holidays you printed and remove all the non-religious holidays from the list.")</pre> <p class="calibre6">Indeed, we get the appropriate output:</p>
<pre class="source-code">
['Christmas',
'Thanksgiving',
"New Year's Day",
'Easter',
"Valentine's Day",
"St. Patrick's Day,"]</pre> <p class="calibre6">To complete this example, let’s assume the intention we had was to quickly generate a table of some holidays that includes their names and descriptions:</p>
<pre class="source-code">
"For each of these, tell about the holiday in 2 sentences.
Form the output in a json format table.
The table's name is "holidays" and the fields are "name" and "description".
For each row, the "name" is the holiday's name, and the "description" is the description you generated.
The syntax of the output should be a json format, without newline characters."</pre> <p class="calibre6">Now, we get a formatted string <a id="_idIndexMarker915" class="calibre5 pcalibre1 pcalibre"/>from the chain:</p>
<pre class="source-code">
{
  "holidays": [
    {
      "name": "Christmas",
      "description": "Christmas is a religious holiday that celebrates the birth of Jesus Christ and is widely observed as a secular cultural and commercial phenomenon."
    },
    {
      "name": "Thanksgiving",
      "description": "Thanksgiving is a national holiday in the United States, celebrated on the fourth Thursday of November, and originated as a harvest festival."
    },
    {
      "name": "Easter",
      "description": "Easter is […]</pre> <p class="calibre6">We can then use pandas to convert this string to a table:</p>
<pre class="source-code">
dict = json.loads(output)
pd.json_normalize(dict[ "holidays"])</pre> <p class="calibre6">After pandas processes <code>dict</code> to be a DataFrame, we can observe it in <em class="italic">Table 9.1</em>:</p>
<table class="no-table-style" id="table001-5">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Name</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Description</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">0</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6">Christmas</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Christmas is a Christian holiday that celebrates the birth of Jesus Christ. It is observed on December 25 each year.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">1</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6">Thanksgiving</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Thanksgiving is a holiday in which people gather together to express gratitude for the blessings in their lives. It is celebrated on the fourth Thursday in November in the United States.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">2</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6">New Year’s Day</p>
</td>
<td class="no-table-style2">
<p class="calibre6">New Year’s Day marks the beginning of the Gregorian calendar year. It is celebrated on January 1 with various traditions and festivities.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">3</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6">Easter</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Easter is a <a id="_idIndexMarker916" class="calibre5 pcalibre1 pcalibre"/>Christian holiday that commemorates the resurrection of Jesus Christ from the dead. It is observed on the first Sunday following the first full moon after the vernal equinox.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">4</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6">Valentine’s Day</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Valentine’s Day is a day to celebrate love and affection. It is traditionally associated with romantic love, but it is also a time to express appreciation for friends and family.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">5</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6">St. Patrick’s Day</p>
</td>
<td class="no-table-style2">
<p class="calibre6">St. Patrick’s Day is a cultural and religious holiday that honors the patron saint of Ireland, St. Patrick. It is celebrated on March 17 with parades, wearing green, and other festive activities.</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 9.1 – pandas transformed the table from dict to a DataFrame, thus suiting down-stream processing</p>
<p class="calibre6">This concludes the various chain features that this notebook presents. Notice how we leveraged the features that both chains bring us and that LLMs bring us. For instance, while the memory and parsing features are completely handled on the chain’s side, the ability to present a response in a particular format, such as a JSON format, is solely accredited to the LLM.</p>
<p class="calibre6">In our next example, we<a id="_idIndexMarker917" class="calibre5 pcalibre1 pcalibre"/> will continue to present novel utilities with LLMs and LangChain.</p>
<h1 id="_idParaDest-213" class="calibre4"><a id="_idTextAnchor515" class="calibre5 pcalibre1 pcalibre"/>Retrieving information from various web sources automatically</h1>
<p class="calibre6">In this example, we will review <a id="_idIndexMarker918" class="calibre5 pcalibre1 pcalibre"/>how simple it is to leverage LLMs to access the web and extract information. We may wish to research a particular topic, and so we would like to consolidate all the information from a few web pages, several YouTube videos that present that topic, and so on. Such an endeavor can take a while, as the content may be massive. For instance, several YouTube videos can sometimes take hours to review. Often, one doesn’t know how useful the video is until one has watched a significant portion of it.</p>
<p class="calibre6">Another use case is when looking to track various trends in real time. This may include tracking news sources, YouTube videos, and so on. Here, speed is crucial. Unlike the previous example where speed was important to save us personal time, here, speed is necessary for getting our algorithm to be relevant for identifying real-time emerging trends.</p>
<p class="calibre6">In this section, we put together a very simple and limited example.</p>
<h2 id="_idParaDest-214" class="calibre7"><a id="_idTextAnchor516" class="calibre5 pcalibre1 pcalibre"/>Retrieving content from a YouTube video and summarizing it</h2>
<p class="calibre6">Refer to the following notebook: <code>Ch9_Retrieve_Content_from_a_YouTube_Video_and_Summarize.ipynb</code> (<a href="https://github.com/embedchain/embedchain" class="calibre5 pcalibre1 pcalibre">https://github.com/embedchain/embedchain</a>). We will build our application on a library called EmbedChain (<a href="https://github.com/embedchain/embedchain" class="calibre5 pcalibre1 pcalibre">https://github.com/embedchain/embedchain</a>). EmbedChain leverages a RAG framework<a id="_idIndexMarker919" class="calibre5 pcalibre1 pcalibre"/> and enhances it by allowing the vector database to include information from various web sources.</p>
<p class="calibre6">In our example, we will choose a particular YouTube video (<em class="italic">Robert Waldinger: What makes a good life? Lessons from the longest study on happiness | TED</em>: <a href="https://www.youtube.com/watch?v=8KkKuTCFvzI&amp;ab_channel=TED" class="calibre5 pcalibre1 pcalibre">https://www.youtube.com/watch?v=8KkKuTCFvzI&amp;ab_channel=TED</a>). We would like the content of that video to be processed into the RAG framework. Then, we will prompt an LLM with questions and tasks related to the content of that video, thus allowing us to extract everything we care to learn about the video without having to watch it.</p>
<p class="calibre6">It should be stressed that a key feature that this method relies on is that YouTube accompanies many of its verbal videos with a written transcript. This makes the importing of the video’s text<a id="_idIndexMarker920" class="calibre5 pcalibre1 pcalibre"/> context seamless. If, however, one wishes to apply this method to a video that isn’t accompanied by a transcript, this is not a problem. One would need to pick a speech-to-text model, many of which are free and of very high quality. The audio of the video would be processed, a transcript would be extracted, and you may then import it into the RAG process.</p>
<h3 class="calibre8">Installs, imports, and settings</h3>
<p class="calibre6">As with previous notebooks, here too, we<a id="_idIndexMarker921" class="calibre5 pcalibre1 pcalibre"/> install the necessary packages, import all the relevant packages, and set our OpenAI API key.</p>
<p class="calibre6">We then do the following:</p>
<ol class="calibre16">
<li class="calibre15">Make our choice of model.</li>
<li class="calibre15">Choose an embedding model that will serve the RAG’s vector database feature.</li>
<li class="calibre15">Choose a prompting LLM. Notice how you can set up further parameters that control the model’s output, such as the maximal number of returned tokens or the temperature.</li>
<li class="calibre15">Pick the YouTube video to which you would like to apply this code and set a string variable using the video’s URL.</li>
</ol>
<h3 class="calibre8">Setting up the retrieval mechanism</h3>
<p class="calibre6">We need to set <a id="_idIndexMarker922" class="calibre5 pcalibre1 pcalibre"/>EmbedChain’s RAG process. We specify that we are passing a path to a YouTube video, and we provide the video’s URL.</p>
<p class="calibre6">We can then print out the text that was fetched and verify that it is, indeed, aligned with the video we are looking to analyze.</p>
<h3 class="calibre8">Reviewing, summarizing, and translating</h3>
<p class="calibre6">We will now observe the value that this code yields.</p>
<p class="calibre6">We ask the LLM to review the<a id="_idIndexMarker923" class="calibre5 pcalibre1 pcalibre"/> content, to put together a summary, and to present that summary in English, Russian, and German:</p>
<pre class="source-code">
Please review the entire content, summarize it to the length of 4 sentence, then translate it to Russian and to German.
Make sure the summary is consistent with the content.
Put the string '\n----\n' between the English part of the answer and the Russian part.
Put the string '\n****\n' between the Russian part of the answer and the German part.</pre> <p class="calibre6">The returned output is spot on, as it completely captures the essence of the TED talk. We edit it to remove the delimiter strings and get:</p>
<pre class="source-code">
The content emphasizes the importance of good
relationships in keeping us happy and healthy
throughout our lives. It discusses how social
connections, quality of close relationships, and
avoiding conflict play crucial roles in our well-
being. The study follows the lives of 724 men over
75 years, highlighting the significance of
relationships over wealth and fame in leading a
fulfilling life.
Russian:
Содержание подчеркивает
Важность [...]
German:
Der
Inhalt betont die Bedeutung  [...]</pre> <p class="calibre6">Now, to make the content simple for, say, a German speaker, we ask the LLM to form the German summary into several bullet points that best describe the content of the video.</p>
<p class="calibre6">It does this well, and the <a id="_idIndexMarker924" class="calibre5 pcalibre1 pcalibre"/>outputs are as follows:</p>
<pre class="source-code">
- Betonung der Bedeutung guter Beziehungen für Glück und Gesundheit
- Diskussion über soziale Verbindungen, Qualität enger Beziehungen und Konfliktvermeidung
- Verfolgung des Lebens von 724 Männern über 75 Jahre in der Studie
- Hervorhebung der Bedeutung von Beziehungen im Vergleich zu Reichtum und Ruhm
- Fokus auf Beziehungen als Schlüssel zu einem erfüllten Leben</pre> <p class="calibre6">While this code is meant to serve as a basic proof of concept, one can see how simple it would be to add more data sources, automate it to run constantly, and act based on the findings. While a readable summary is helpful, one could change the code to act based on the identified content and execute downstream applications.</p>
<p class="calibre6">Now that we have observed several capabilities that LLMs can perform, we can take a step back and refine the way we utilize those LLMs. In our next section, we will exemplify how one may reduce LLM processing, thus saving API costs, or, when employing a local LLM, reducing inference computation.</p>
<h1 id="_idParaDest-215" class="calibre4"><a id="_idTextAnchor517" class="calibre5 pcalibre1 pcalibre"/>Prompt compression and API cost reduction</h1>
<p class="calibre6">This part is dedicated to a recent development in resource optimization for when employing API-based LLMs, such as OpenAI’s services. When considering the many trade-offs between employing a remote LLM as a service and hosting an LLM locally, one key metric is cost. In particular, based on the application and usage, the API costs can accumulate to a significant amount. API costs are mainly driven by the number of tokens that are being sent to and from the LLM service.</p>
<p class="calibre6">In order to illustrate the significance of this payment model on a business plan, consider business units for which the product or service relies on API calls to OpenAI’s GPT, where OpenAI serves as a third-party vendor. As a particular example, imagine a social network that lets its users have LLM assistance to comment on posts. In that use case, a user is interested in commenting on a post, and instead of having to write a complete comment, a feature lets the user describe their feelings about the post in three–five words, and a backend process augments a full comment.</p>
<p class="calibre6">In this particular example, the engine collects the user’s three–five words, and it also collects the content of the post that the comment is meant for, meaning it will also collect all other relevant information that the social network’s experts would think is relevant for augmenting a comment. For instance, the user’s profile description, their past few comments, and so on.</p>
<p class="calibre6">This would mean that every time a user wishes to have a comment augmented, a detailed prompt is sent from the social network’s servers to the third party’s LLM via theAPI.</p>
<p class="calibre6">Now, this type of process can accumulate high costs.</p>
<p class="calibre6">In this section, we will analyze an approach to reducing this cost by reducing the number of tokens sent to the LLM through the API. The basic assumption is that one can always reduce the number of words sent to the LLM and, thus, reduce cost, but the reduction in performance could be significant. Our motivation is to reduce that amount while maintaining high-quality performance. We then asked if only the “right” words could be sent, ignoring other “non-material” words. This notion reminds us of the concept of file compression, where a smart and tailored algorithm is employed to reduce the size a file takes while maintaining its purpose and value.</p>
<h2 id="_idParaDest-216" class="calibre7"><a id="_idTextAnchor518" class="calibre5 pcalibre1 pcalibre"/>Prompt compression</h2>
<p class="calibre6">Here, we introduce <strong class="bold">LLMLingua</strong>, a development by Microsoft that is meant to address prompts that are “sparse” in information by<a id="_idIndexMarker925" class="calibre5 pcalibre1 pcalibre"/> compressing them.</p>
<p class="calibre6">LLMLingua utilizes a compact, well-trained language<a id="_idIndexMarker926" class="calibre5 pcalibre1 pcalibre"/> model, such as LLaMA-7B, to identify and remove non-essential tokens within prompts. This approach enables efficient inference with LLMs, achieving up to 20x compression with minimal performance loss (<a href="https://github.com/microsoft/LLMLingua" class="calibre5 pcalibre1 pcalibre">https://github.com/microsoft/LLMLingua</a>).</p>
<p class="calibre6">In their papers (<a href="https://arxiv.org/abs/2310.05736" class="calibre5 pcalibre1 pcalibre">https://arxiv.org/abs/2310.05736</a> and https://arxiv.org/abs/2310.06839), the authors explain the algorithm and the advantages it proposes. It is interesting to note that besides the reduction in cost, the compression also aims to focus the remaining content, which is shown by the authors to lead to an improvement in performance by the LLM, as it avoids a sparse and noisy prompt.</p>
<p class="calibre6">Let’s experiment with prompt compression in a real-world example and evaluate its impact and various trade-offs.</p>
<h2 id="_idParaDest-217" class="calibre7"><a id="_idTextAnchor519" class="calibre5 pcalibre1 pcalibre"/>Experimenting with prompt compression and evaluating trade-offs</h2>
<p class="calibre6">For the sake of this experiment, we'll illustrate a real-world example.</p>
<p class="calibre6">In our current use case, we are developing <a id="_idIndexMarker927" class="calibre5 pcalibre1 pcalibre"/>a feature that sits on top of a database of academic publications. The feature allows the user to pick a specific publication<a id="_idIndexMarker928" class="calibre5 pcalibre1 pcalibre"/> and ask questions about it. A backend engine evaluates the question, reviews the publication, and derives an answer.</p>
<p class="calibre6">To narrow down the scope of the feature for the sake of putting together a series of experiments, the publications are from the particular category of AI publications, and the question that the user asks is the following:</p>
<pre class="source-code">
"Does this publication involve Reinforcement Learning?"</pre> <p class="calibre6">This question requires a deep and insightful review of each publication, as there are cases where a publication discusses a novel algorithm where the term reinforcement learning isn’t explicitly mentioned at any point in the publication, yet the feature is expected to infer from the description of the algorithm whether it indeed leverages the concepts of reinforcement learning and flag it as such.</p>
<p class="calibre6">Refer to the following notebook: <code>Ch9_RAGLlamaIndex_Prompt_Compression.ipynb</code>.</p>
<p class="calibre6">In this code, we run a set of experiments, each <a id="_idIndexMarker929" class="calibre5 pcalibre1 pcalibre"/>per the above feature description. Each experiment is in the form of a<a id="_idIndexMarker930" class="calibre5 pcalibre1 pcalibre"/> full, end-to-end RAG task. While we employed LangChain in the previous RAG examples, here, we introduce LlamaIndex. LlamaIndex is an open source Python library that employs a RAG framework (<a href="https://docs.llamaindex.ai/en/stable/index.html" class="calibre5 pcalibre1 pcalibre">https://docs.llamaindex.ai/en/stable/index.html</a>). LlamaIndex is similar to LangChain in that way.</p>
<p class="calibre6">The LLMLingua code stack that the folks at Microsoft put together is integrated with LlamaIndex.</p>
<p class="calibre6">Let’s review the code in detail.</p>
<h3 class="calibre8">Code settings</h3>
<p class="calibre6">Similar to the previous notebooks, here too, we <a id="_idIndexMarker931" class="calibre5 pcalibre1 pcalibre"/>set the initial settings with the following:</p>
<ol class="calibre16">
<li class="calibre15">In this code section, we start by defining some key variables.</li>
<li class="calibre15">We set the number of experiments that we want to run. We want to make sure we choose a number that is large enough so as to get a good statistical representation of the impact that the compression has.</li>
<li class="calibre15">We set the top-k, which is the number of chunks to be retrieved by the RAG framework for prompt context.</li>
<li class="calibre15">We predefined the target number of the token we would like the compression to reduce.</li>
<li class="calibre15">Finally, as in the previous code, we set our OpenAI API key.</li>
</ol>
<p class="calibre6">We take this opportunity to stress that some of the parameters in this evaluation were fixed for the sake of limiting its complexity and keeping it appropriate for educational purposes. When conducting such an evaluation in business or academic settings, there should be either qualitative or quantitative reasoning for the value chosen. Qualitative may be of the form “We shell fix the desired reduction to 999 tokens due to budget constraints," whereas quantitative may seek to not fix it but rather optimize it as a part of the other trade-offs. In our case, we fixed this particular parameter to a value that was found to allow for an impressive compression rate while maintaining a decent agreement rate between the two evaluated approaches. Another example was the number of experiments we<a id="_idIndexMarker932" class="calibre5 pcalibre1 pcalibre"/> chose, which was a trade-off between runtime, GPU memory allocation, and statistical power.</p>
<h3 class="calibre8">Gathering the data</h3>
<p class="calibre6">We need to gather the dataset of the<a id="_idIndexMarker933" class="calibre5 pcalibre1 pcalibre"/> publications, and we also filter it so as to be left with only the limited cohort of publications that are in the AI category.</p>
<h3 class="calibre8">LLM configurations</h3>
<p class="calibre6">Here, we set the ground for the two <a id="_idIndexMarker934" class="calibre5 pcalibre1 pcalibre"/>LLMs we will be employing.</p>
<p class="calibre6">The compression method, LLMLingua, employs Llama2 as the compressing LLM. It will obtain the context retrieved by the LlamaIndex RAG pipeline, the user’s question, and it will compress and reduce the size of the context content.</p>
<p class="calibre6">OpenAI’s GPT is to be used as the downstream LLM for prompting, meaning it will obtain the question about reinforcement learning and the additional relevant context and return an answer.</p>
<p class="calibre6">Additionally, here, we define the user’s question. Note that we added instructions for OpenAI’s GPT on how to present the answer.</p>
<h3 class="calibre8">The experiments</h3>
<p class="calibre6">This is the core of the notebook. A <code>for</code> loop iterates <a id="_idIndexMarker935" class="calibre5 pcalibre1 pcalibre"/>over the various experiments. In each iteration, two scenarios are evaluate:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">First scenario</strong>: an ordinary RAG task is deployed where the context is retrieved without being compressed. The prompt is comprised of the retrieved context and the user’s question, and the answer that the LLM returns is recorded along with the number of sent tokens and the processing time.</li>
<li class="calibre15"><strong class="bold">Second scenario</strong>: LLMLingua is employed. The retrieved context is compressed. The compressed context is sent to the LLM along with the user’s question. Again, the returned answer is recorded along with the number of sent tokens and the processing time.</li>
</ol>
<p class="calibre6">When this code cell is completed, we have a dictionary, <code>record</code>, that holds the relevant values for each iteration that <a id="_idIndexMarker936" class="calibre5 pcalibre1 pcalibre"/>will be used to aggregate and derive conclusions.</p>
<h3 class="calibre8">Analyzing the impact of context compression – a reduction in classification performance versus an increase in resource efficiency</h3>
<p class="calibre6">Here, we sum up the <a id="_idIndexMarker937" class="calibre5 pcalibre1 pcalibre"/>values of the experiments and deduce what impact the prompt compression has on the performance of the LLM, the processing time, and the cost of the API:</p>
<ul class="calibre14">
<li class="calibre15">We found that the reduction in the context length had yielded an agreement rate of 92%.</li>
<li class="calibre15">We found that the process of compression had extended the processing time by 11 times.</li>
<li class="calibre15">We found that the reduction in the context length saved 92% of the total cost of sent tokens!</li>
</ul>
<p class="calibre6">Note that cost reduction is negatively dependent on the agreement rate, as we expect an increase in cost savings to reduce the agreement rate.</p>
<p class="calibre6">This reduction is significant and, in some cases, may tilt the scale from a loss-making service to a profitable service.</p>
<p class="calibre6">Here are some notes to keep in mind regarding the meaning of a disagreement and additional trade-offs. Regarding the drop in agreement rate between the two approaches, while an agreement between the two approaches insinuates that they are both correct, a disagreement could go either way. It could be that in the second scenario, the compression distorted the context and, thus, made the model unable to properly classify it. However, the opposite may be true, as the compression may have reduced the irrelevant content and made the LLM focus on the relevant aspects of the content, thus making the scenario with the compressed context yield a correct answer.</p>
<p class="calibre6">Regarding additional trade-offs, the above metrics of LLM performance, processing time, and API cost don’t reveal additional considerations such as the computational resources that the compression requires. The local compressing LLM, in our case, Llama2, requires local hosting and local GPUs. These are non-trivial resources that don’t exist on an ordinary laptop. Remember the original approach, i.e., the first scenario, does not require those. An ordinary RAG approach can perform embeddings using either a smaller LM, such as one that is BERT-based, or even an API-based embedding. The prompted LLM, under<a id="_idIndexMarker938" class="calibre5 pcalibre1 pcalibre"/> our original assumption, is chosen to be remote and API-based, thus enabling the deployment environment to have minimal computation resources, like a common laptop would provide.</p>
<p class="calibre6">This evaluation proves that the LLMLingua prompt compression method is very impactful and useful as a means of cost reduction.</p>
<p class="calibre6">In the next and last code demonstration of this chapter, we will continue to observe the results of this experience, and we will do so by forming a team of experts, each played by an LLM, so as to enhance the process of deriving a concl<a id="_idTextAnchor520" class="calibre5 pcalibre1 pcalibre"/>usion to the analysis.</p>
<h1 id="_idParaDest-218" class="calibre4"><a id="_idTextAnchor521" class="calibre5 pcalibre1 pcalibre"/>Multiple agents – forming a team of LLMs that collaborate</h1>
<p class="calibre6">This section deals with one of the most exciting recent methods in the world of LLMs, employing multiple LLMs simultaneously. In the context of this section, we seek to define multiple agents, each backed by an LLM and given a different designated role to play. Instead of the user working<a id="_idIndexMarker939" class="calibre5 pcalibre1 pcalibre"/> directly with the LLM, as we see in ChatGPT, here, the user sets up multiple LLMs and sets their role by defining a different system prompt for each of them.</p>
<h2 id="_idParaDest-219" class="calibre7"><a id="_idTextAnchor522" class="calibre5 pcalibre1 pcalibre"/>Potential advantages of multiple LLM agents working simultaneously</h2>
<p class="calibre6">Much like with people working <a id="_idIndexMarker940" class="calibre5 pcalibre1 pcalibre"/>together, here too, we see the advantages of employing several LLMs simultaneously.</p>
<p class="calibre6">Some advantages are the following:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Enhancing validation and reducing hallucinations</strong>: It has been shown that when providing feedback to an LLM and asking it to reason or to check its response, the reliability of its response improves. When designating roles for the various LLM agents on a team, the system prompt of at least one of them may include the requirement to criticize and validate the answers.</li>
<li class="calibre15"><strong class="bold">Allowing the person to be involved as much or as little as they want in the process</strong>: When designating the various roles, the user may insert themselves into the team, such that when it is the user’s turn to participate in the conversation, the rest of the agents wait while the user enters their input. However, if desired, the user may remove themselves altogether and just let the LLMs work automatically.<p class="calibre6">In the following examples, we will see examples of the latter.</p></li>
<li class="calibre15"><strong class="bold">Allowing different LLM models to be best utilized</strong>: Today, we have several leading LLMs <a id="_idIndexMarker941" class="calibre5 pcalibre1 pcalibre"/>available. Some are free and hosted locally, and some are API-based. They are different in their size and capabilities, and some of them are stronger in particular tasks than others. When forming a team of agents where each agent is assigned a different role, a different LLM may be set that best suits that role. For instance, in the context of a coding project, where one of the agents is a programmer of a particular coding language, the user may choose to set the LLM for that agent to be an LLM that is superior for code generation in that particular coding language.</li>
<li class="calibre15"><strong class="bold">Optimizing resources – employing several smaller LLMs</strong>: Imagine a project that involves technical functions and also domain expertise. For example, building a user platform in the medical space. You would want there to be a frontend engineer, a backend engineer, a designer, and a medical expert, all governed by the project manager and the product manager. If you were to develop this platform using the multiple-agents framework, you would define the agents, assign the various roles to them, and pick an LLM to drive them. If you were to use the same LLM for all of the agents, say, OpenAI’s most recent GPT, then that model would have to be very generic, thus requiring it to be very large and perhaps very expensive and maybe even slow. However, if you had access to individual LLMs, each pre-trained to only fulfill a limited function, for instance, one LLM dedicated to the medical service domain and a different LLM dedicated to backend development in Python, then you would assign each of those particular LLMs to their corresponding agents.<p class="calibre6">That may present a major reduction in model size, as the combined architecture of several specialized LLMs may be smaller in size than the architecture of one generic LLM when assuming equal performance between the two scenarios.</p></li>
<li class="calibre15"><strong class="bold">Optimizing resources – optimal designation of a single LLM</strong>: A unique and particular case of employing multiple LLMs is that in which we are seeking to optimize the LLM being<a id="_idIndexMarker942" class="calibre5 pcalibre1 pcalibre"/> chosen per the current task. This case is different than all the above, as it does not refer to a case where several LLMs are working simultaneously. In this case, a routing algorithm chooses one LLM based on a current state of constraints and variables. These may include the following:<ul class="calibre17"><li class="calibre15">The current load on each of the different parts of the computation system</li><li class="calibre15">The cost constraints, which may vary over time</li><li class="calibre15">The source of the prompt, as different clients/regions may have different priorities</li><li class="calibre15">The purpose of the prompt, as different use cases may be prioritized by the business</li><li class="calibre15">The prompt’s requirements, as a code-generating task, may be obtaining excellent responses with a small and efficient code-generating LLM, whereas a request to review a legal document and to suggest precedents may call for a completely different model</li></ul></li>
</ul>
<h3 class="calibre8">AutoGen</h3>
<p class="calibre6">The particular framework we<a id="_idIndexMarker943" class="calibre5 pcalibre1 pcalibre"/> employ in this section is called AutoGen, and it is made available by Microsoft (GitHub repo: <a href="https://github.com/microsoft/autogen/tree/main" class="calibre5 pcalibre1 pcalibre">https://github.com/microsoft/autogen/tree/main</a>).</p>
<p class="calibre6"><em class="italic">Figure 9</em><em class="italic">.1</em> conveys the AutoGen framework. The following was obtained from the statement made in the GitHub repo:</p>
<p class="calibre6"><em class="italic">AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, </em><em class="italic">and tools</em>.</p>
<div><div><img alt="Figure 9.1 – AutoGen functionality" src="img/B18949_09_1.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 9.1 – AutoGen functionality</p>
<p class="calibre6">On the left of <em class="italic">Figure 9</em><em class="italic">.1</em>, we observe the designation of roles and capabilities to individual agents; on the right, we observe a few of the conversation structures that are available.</p>
<p class="calibre6">AutoGen’s key capabilities as presented in the code repo:</p>
<ul class="calibre14">
<li class="calibre15">AutoGen enables building<a id="_idIndexMarker944" class="calibre5 pcalibre1 pcalibre"/> next-gen LLM applications based on multi-agent conversations with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.</li>
<li class="calibre15">It supports diverse conversation patterns for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy, the number of agents, and agent conversation topology.</li>
<li class="calibre15">It provides a collection of working systems with different complexities. These systems span a wide range of applications from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.</li>
<li class="calibre15">AutoGen provides enhanced LLM inference. It offers utilities such as API unification and caching, and advanced usage patterns such as error handling, multi-config inference, context programming, and so on.</li>
</ul>
<p class="calibre6">AutoGen is powered by collaborative research studies from Microsoft, Penn State University, and the University of Washington.</p>
<p class="calibre6">Next, we can dive into a practical example in the code.</p>
<h3 class="calibre8">Completing a complex analysis – visualizing the results and forming a conclusion</h3>
<p class="calibre6">Here, we will show how a team<a id="_idIndexMarker945" class="calibre5 pcalibre1 pcalibre"/> of multiple agents, each with a different designated role, could serve as a professional team. The use case we chose is a continuation of the <a id="_idIndexMarker946" class="calibre5 pcalibre1 pcalibre"/>previous code we ran. In the last code, we performed a complex evaluation of employing prompt compression, and when that code finished, we had two resulting items: the <code>dict</code> that holds the numeric measurements of the experiments, called <code>record,</code> and the verbal statements about the resulting agreement rate, the reduction in tokens and cost, and the change in processing time.</p>
<p class="calibre6">With that previous notebook, we intentionally stopped short. We didn’t visualize the reduction in tokens and cost, and we didn’t form an opinion as to whether we would advocate for employing the prompt reductions. However, in business or academic settings, one would be required to offer both. When you present your findings to stakeholders, decision-makers, or the research community, you are expected, when feasible, to visualize the statistical significance of the experiments. As a subject expert in NLP and ML, you are also <a id="_idIndexMarker947" class="calibre5 pcalibre1 pcalibre"/>expected to provide your recommendation on whether to adopt the experimented method or not.</p>
<p class="calibre6">We will take the results from that <a id="_idIndexMarker948" class="calibre5 pcalibre1 pcalibre"/>evaluation, and we will task a team of agents to do the work for us!</p>
<p class="calibre6">Refer to the following notebook: <code>Ch9_Completing_a_Complex_Analysis_with_a_Team_of_LLM_Agents.ipynb</code>. The notebook starts with the common aspects of installs, imports, and settings. You will notice that AutoGen has a particular format of settings in the form of a dictionary. They provide the details, as you can see in our notebook.</p>
<p class="calibre6">Now, we move on to the interesting parts!</p>
<h4 class="calibre135">Creating a visualization of the significance of the experiments</h4>
<p class="calibre6">The <code>record.pickle</code> <code>file</code> is of a <code>dict</code> variable. It is the collection of numerical results from the previous evaluation notebook. Our wish is to visualize the distributions of the token counts for each of the<a id="_idIndexMarker949" class="calibre5 pcalibre1 pcalibre"/> experiments. There are token counts for original prompts and token counts for compressed prompts. There are <a id="_idIndexMarker950" class="calibre5 pcalibre1 pcalibre"/>also the ratios between the two for each experiment.</p>
<p class="calibre6">In this section, we'll form a team to put code together that would visualize the distributions of each of the three.</p>
<h4 class="calibre135">Defining the task to be fulfilled by the team</h4>
<p class="calibre6">First, we define the task to be fulfilled by the<a id="_idIndexMarker951" class="calibre5 pcalibre1 pcalibre"/> team. We tell the team where the file is saved and the context and the nature of the values in the <code>dict</code>, thus giving the team the understanding they need to ideate a solution to the task. Then, we describe the task of creating a plot and visualizing the distributions. All those details are in the one string that describes the task. Note that in an Agile Scrum work setting, this task string is similar to the purpose of the story.</p>
<p class="calibre6">Now that we have formed a comprehensive description, it should be clear what is expected. For instance, we ask for the figures and axes to be labeled, but we don’t explicitly state what labels are <a id="_idIndexMarker952" class="calibre5 pcalibre1 pcalibre"/>expected. The agents will understand on their own, just as we would have understood this on our own, as the labels are inferred from the task and the data field names.</p>
<h4 class="calibre135">Defining the agents and assigning the team members roles</h4>
<p class="calibre6">For this task, we would need three team members: a programmer to write the code, a QA engineer to run the code and provide feedback, and a team lead to verify when the task is complete.</p>
<p class="calibre6">For each of the roles, we articulate a<a id="_idIndexMarker953" class="calibre5 pcalibre1 pcalibre"/> system prompt. This system prompt, as we learned in <a href="B18949_08.xhtml#_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 8</em></a>, has a significant impact on the LLM’s function. Notice that we also provide the QA engineer and the team lead with the ability to run code on their own. In this way, they will be able to verify the programmer’s code and provide objective feedback. If we told the same agent to write the<a id="_idIndexMarker954" class="calibre5 pcalibre1 pcalibre"/> code and to confirm that it is correct, we might find that, in practice, it would generate a first draft, wouldn’t bother to run and verify it, and it would conclude that task without having verified it.</p>
<h4 class="calibre135">Defining a group conversation</h4>
<p class="calibre6">Here, we define the conversation<a id="_idIndexMarker955" class="calibre5 pcalibre1 pcalibre"/> to be a multi-agent conversation; this is one of the features of AutoGen. This is slightly different from the case where you define a series of conversations where each conversation involves just two agents. The group conversation involves more agents.</p>
<p class="calibre6">When defining a group conversation, we also define a manager for the conversation.</p>
<h4 class="calibre135">Deploying the team</h4>
<p class="calibre6">The team lead tasks the manager with the task we defined. The manager then delegates the work to the programmer and the <a id="_idIndexMarker956" class="calibre5 pcalibre1 pcalibre"/>QA engineer.</p>
<p class="calibre6">Here are the highlights of that automated conversation as it appears on the screen:</p>
<pre class="source-code">
<strong class="bold1">lead (to manager_0):</strong>
Refer to the Python dict that is in this [...]
<strong class="bold1">programmer (to manager_0):</strong>
```python
import pandas as pd
import matplotlib.pyplot as plt
# Load the record dict from URL
import requests
import pickle
[...]
<strong class="bold1">qa_engineer (to manager_0):</strong>
exitcode: 0 (execution succeeded)
Code output:
Figure(640x480)
<strong class="bold1">programmer (to manager_0):</strong>
TERMINATE</pre> <p class="calibre6">As can be seen, the conversation had four interactions, each between two agents. Each interaction starts by telling the user which agent is talking to which other agent; these parts are in bold letters in the preceding printout.</p>
<p class="calibre6">In the second interaction, the programmer provided a complete Python script. We pasted only the first four commands to keep it short, but you can observe the full script in the notebook. The QA engineer ran the script and reported that it ran well. If it hadn’t run well, it would have returned an <code>exitcode: 1</code> and would have provided the programmer with the error specification for the programmer to fix the code; the conversation would have continued until a solution was found, or, if not, the team would report failure and conclude the conversation.</p>
<p class="calibre6">This task provided us with the code to create the visual we wanted. Note that we didn’t ask the agents to run the code and provide us with the visual; we asked for the code itself. One could, if desired, configure<a id="_idIndexMarker957" class="calibre5 pcalibre1 pcalibre"/> the LLMs to run the code and provide us with the resulting image. See AutoGen’s repo for the various examples and capabilities.</p>
<p class="calibre6">In the next code cell, we pasted the code that the team created. The code runs well and visualizes the three distributions exactly as we asked the team (see <em class="italic">Figure 9</em><em class="italic">.2</em>):</p>
<div><div><img alt="Figure 9.2 – Visualizing the value that prompt compression provides" src="img/B18949_09_2.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Visualizing the value that prompt compression provides</p>
<p class="calibre6">The top visualization displays the distributions of the token count for the original prompts (blue/light shade) and the compressed prompts (orange/dark shade), and the bottom part of the figure shows the<a id="_idIndexMarker958" class="calibre5 pcalibre1 pcalibre"/> distribution of the ratio between each pair of prompts. <em class="italic">Figure 9</em><em class="italic">.2</em> shows just how effective the reduction rate is, as this ratio translates to a reduction in API cost.</p>
<p class="calibre6">This concludes the visualization of the significance of the experiments.</p>
<h4 class="calibre135">Human intervention in the team’s tasks</h4>
<p class="calibre6">Note that all three agents are driven by LLMs, thus making this entire task automatically performed without human<a id="_idIndexMarker959" class="calibre5 pcalibre1 pcalibre"/> intervention. One could change the lead’s configuration to represent a human user, meaning you. If you did that, then you would be<a id="_idIndexMarker960" class="calibre5 pcalibre1 pcalibre"/> able to intervene and demand certain verifications from the QA engineer or certain additional features in the code from the programmer.</p>
<p class="calibre6">This could be particularly useful if you wanted to run the code yourself in your environment instead of letting the QA engineer agent run it in its own environment. Your environments are different. One advantage of doing this is when the code is required to load a data file that you have locally. If you told the agent to write code that loads this file, then when the QA engineer agent ran it, it would tell you the code failed since that data file doesn’t exist in its environment. In this case, you may elect to be the one who iterates with the programmer and the one who runs the code during the iterations and provides feedback.</p>
<p class="calibre6">Another case where you would want to be the one running the code and providing feedback is when the QA engineer encounters an error or a bug in the programmer’s code, but the two agents aren’t able to figure out the solution. In that case, you would want to intervene and provide your insight. For instance, in a case where a for loop iterates over a dict’s keys instead of its values, you may intervene and enter <em class="italic">The code runs but the for loop is iterating on the dict’s keys. It should iterate over its values for the </em><em class="italic">key ‘key1.</em></p>
<p class="calibre6">We can now move on to the second part of concluding the evaluation.</p>
<h4 class="calibre135">Reviewing the results of the experiments and forming an educated conclusion</h4>
<p class="calibre6">As with every complex evaluation where we perform experiments to target the impact of a particular feature, we would now like to derive a qualitative summary of the results and suggest a conclusion for<a id="_idIndexMarker961" class="calibre5 pcalibre1 pcalibre"/> our audience, whether it is the decision-makers in the company or the research community in academia.</p>
<p class="calibre6">What is unique about this part is that the act of deriving a conclusion has never been left to any mathematical or algorithmic model to derive. As we humans govern the various evaluations, and although we may seek to automate as much as possible to feed into the final conclusion, we are the entity that forms the final impression and conclusion.</p>
<p class="calibre6">Here, we attempt to automate that final part. We will assign a team of expert agents to provide an educated summary of the results that the evaluation notebook printed out. We'll then push the team to provide us with a recommendation as to whether we should implement the new feature of prompt compression or not. We provide the team with the actual results of the evaluation notebook, but in order to examine its reliability, we then task it again, this time providing it with mocked results that are much poorer, hoping that the team will apply judgment and provide a different recommendation. All of this is done without any human intervention.</p>
<p class="calibre6">As we did before, we start by defining the task for our team to fulfill.</p>
<h4 class="calibre135">Defining the task to be fulfilled by the team</h4>
<p class="calibre6">Our aim is to provide <a id="_idIndexMarker962" class="calibre5 pcalibre1 pcalibre"/>the team with the printout of the evaluation notebook from the previous section. That printout describes, in words, the change in agreement rate, the impact on the number of prompt tokens, and the processing runtime, all due to employing the LLMLingua prompt compression method.</p>
<p class="calibre6">We then copy that from the previous notebook and paste it as a text string.</p>
<p class="calibre6">Note that we have also created another text string of results (which are mocked results that are much worse than the true results), but we see that the agreement rate is very low, and the reduction in token count due to compression is much less significant.</p>
<p class="calibre6">As we did in the visualization case, we then create the instructions for the team; we paste the results into the task description for the team to refer to when deriving its conclusion. We have two task descriptions, as we will have two separate runs, one with the true results and one <a id="_idIndexMarker963" class="calibre5 pcalibre1 pcalibre"/>with the mocked bad results.</p>
<p class="calibre6">We will now allocate the roles.</p>
<h4 class="calibre135">Defining the agents and assigning team members roles</h4>
<p class="calibre6">For this task, we would <a id="_idIndexMarker964" class="calibre5 pcalibre1 pcalibre"/>need three team members: a<a id="_idIndexMarker965" class="calibre5 pcalibre1 pcalibre"/> principal engineer who is an experienced technical person, a technical writer who writes the conclusion as per the principal engineer’s feedback, and a team lead to verify when the task is complete, which was defined in the previous task.</p>
<h4 class="calibre135">Defining a group conversation</h4>
<p class="calibre6">Here, we define the group<a id="_idIndexMarker966" class="calibre5 pcalibre1 pcalibre"/> conversation, just like we did in the visualization part. This time, we have a new group conversation manager, as the group consists of different agents.</p>
<h4 class="calibre135">Deploying the team</h4>
<p class="calibre6">The team lead tasks the manager <a id="_idIndexMarker967" class="calibre5 pcalibre1 pcalibre"/>with the task we defined. The manager then delegates the work to the writer and the principal engineer.</p>
<p class="calibre6">Here are the highlights of that automated conversation as it appears on the screen:</p>
<pre class="source-code">
<strong class="bold1">lead (to manager_1):</strong>
Refer to the results printed below.
These are the results that stem from [...]
<strong class="bold1">writer (to manager_1):</strong>
The experiments on prompt compression using LLMLingua have produced the following results:
- Classification Performance:
  - Agreement rate of [...]
<strong class="bold1">principal_engineer (to manager_1):</strong>
[...]</pre> <p class="calibre6">The agents have a few iterations between them and come to an agreement regarding the summary and the <a id="_idIndexMarker968" class="calibre5 pcalibre1 pcalibre"/>conclusion.</p>
<p class="calibre6">They provide a summary of the numeric results and seal it with the following recommendation:</p>
<pre class="source-code">
It is imperative to carefully consider the trade-offs presented by prompt compression, as while it may lead to resource savings, there might be implications on processing efficiency. The decision to adopt prompt compression should be made with a thorough understanding of these trade-offs.</pre> <p class="calibre6">The team agrees on a cautious approach to presenting the various trade-offs and avoids making a decision in spite of being tasked to do so.</p>
<p class="calibre6">One would wonder, could a definite decision to adopt or not to adopt the method be made here?</p>
<h4 class="calibre135">Evaluation of the team’s judgment</h4>
<p class="calibre6">Now, we will ask the team to<a id="_idIndexMarker969" class="calibre5 pcalibre1 pcalibre"/> perform the same action, this time providing it with the mocked results that make the compression method seem much less effective and with a great reduction in agreement with the classification of the noncompressed method.</p>
<p class="calibre6">The team has a conversation, and the final agreement summary is sealed with the following statement:</p>
<pre class="source-code">
Overall, the results indicate that while prompt compression may lead to cost savings and resource reduction, it comes at the expense of decreased classification performance and significantly increased processing times.
**Recommendation:** Prompt compression using LLMLinguam is **not recommended** as it can negatively impact classification performance and significantly increase processing times, outweighing the potential cost savings.</pre> <p class="calibre6">Here, the team found it much easier to draw a definite conclusion. It did so without any human intervention and <a id="_idIndexMarker970" class="calibre5 pcalibre1 pcalibre"/>solely based on the numerical results it was given.</p>
<h2 id="_idParaDest-220" class="calibre7"><a id="_idTextAnchor523" class="calibre5 pcalibre1 pcalibre"/>Concluding thoughts on the multiple-agent team</h2>
<p class="calibre6">This emerging method <a id="_idIndexMarker971" class="calibre5 pcalibre1 pcalibre"/>of simultaneously employing several LLMs is gaining interest and traction in the world of AI. In the code experiments that we present in this section, it was proven without a doubt that AutoGen’s group conversation can provide tangible and actionable value in the professional setting. Although setting these code experiments required a series of trials and errors for properly setting the agent roles and properly describing the tasks, it suggests that this framework is moving in a direction where less human intervention is required. What seems to remain a monumental<a id="_idIndexMarker972" class="calibre5 pcalibre1 pcalibre"/> component is the human oversight, feedback, and evaluation of the resulting relics of those agent teams’ <em class="italic">work. </em>We would like to stress to the reader that of the various application and innovations that we share in this book, we have marked the multiple-agent framework as the one that is most likely to grow and to also become the most popular. This is based on the overwhelming expectations that industries have from AI to automate and demonstrate human-like expertise, while innovations such as Autogen, and later Autodev, both by Microsoft, are exemplifying growing feasibility and competency.</p>
<h1 id="_idParaDest-221" class="calibre4"><a id="_idTextAnchor524" class="calibre5 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre6">Throughout this pivotal chapter, we have embarked on an in-depth exploration of the most recent and groundbreaking applications of LLMs, presented through comprehensive Python code examples. We began by unlocking advanced functionalities by using the RAG framework and LangChain, enhancing LLM performance for domain-specific tasks. The journey continued with advanced methods in chains for sophisticated formatting and processing, followed by the automation of information retrieval from diverse web sources. We also tackled the optimization of prompt engineering through prompt compression techniques, significantly reducing API costs. Finally, we ventured into the collaborative potential of LLMs by forming a team of models that work in concert to solve complex problems.</p>
<p class="calibre6">By mastering these topics, you have now acquired a robust set of skills, enabling you to harness the power of LLMs for a variety of applications. These newfound abilities not only prepare you to tackle current challenges in NLP but also equip you with the insights to innovate and push the boundaries of what’s possible in the field. The practical knowledge gained from this chapter will empower you to apply advanced LLM techniques to real-world issues, opening up new opportunities for efficiency, creativity, and problem-solving.</p>
<p class="calibre6">As we turn the page, the next chapter will take us into the realm of emerging trends in AI and LLM technology. We will delve into the latest algorithmic developments, assess their impact on various business sectors, and consider the future landscape of AI. This forthcoming discussion promises to provide you with a comprehensive understanding of where the field is headed and how you can stay at the forefront of technological innovation.</p>
</div>
</body></html>