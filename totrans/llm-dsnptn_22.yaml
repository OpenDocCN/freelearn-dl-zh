- en: '22'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reasoning and Acting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reasoning and Acting** (**ReAct**) is a prompting technique developed by
    researchers from Princeton University and Google that enhances an LLM’s ability
    to perform reasoning and acting in simulated environments ([https://arxiv.org/pdf/2210.03629](https://arxiv.org/pdf/2210.03629)).
    It allows LLMs to mimic human-like operations in the real world, where we reason
    verbally and take actions to gain information. ReAct combines *reasoning* and
    *acting* to solve complex language reasoning and decision-making tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: While CoT prompting enables LLMs to generate reasoning traces, its lack of access
    to the external world can lead to issues such as fact hallucination. ReAct addresses
    this by allowing LLMs to generate both *verbal reasoning traces* and *text actions*
    for a task. These text actions enable the model to interact with its environment
    (for example, by querying an external knowledge source or using a tool), gather
    information, and adjust its reasoning accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key characteristics of ReAct are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reasoning traces**: LLMs generate text that explains their thought process
    step by step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action generation**: LLMs produce text actions that represent interactions
    with external tools or environments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observation incorporation**: The results of actions (observations) are fed
    back into the LLM’s context, influencing subsequent reasoning and actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative process**: ReAct typically involves multiple *Thought*/*Action*/*Observation*
    steps, allowing for dynamic problem solving'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ReAct excels in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: When a task requires information beyond the LLM’s pre-trained knowledge (for
    example, multi-hop question answering or fact verification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an LLM needs to navigate and interact with a simulated environment (for
    example, online shopping or text-based games)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you need to combine the power of LLMs with the capabilities of external
    tools (for example, search engines, calculators, and APIs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the task requires a problem to be broken down into smaller steps and decisions
    must be made based on intermediate results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ReAct in LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building ReAct agents with LangChain’s Expression Language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completing tasks and solving problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating ReAct’s performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safety, control, and ethical considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations and future directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing ReAct in LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The open source LLM framework LangChain ([https://www.langchain.com/](https://www.langchain.com/))
    provides a powerful and flexible implementation of the ReAct framework through
    its `Agent` class. Let’s explore how to create and use ReAct agents in LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`wikipedia` enables language models to access and utilize information from
    Wikipedia, broadening their knowledge base'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`langchainhub` is a central repository for sharing and discovering LangChain
    assets, such as prompts, chains, and agents'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the language model and tools such as `wikipedia`, `ddg-search`,
    and `llm-math`. These are listed in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we import the necessary modules from `langchain`. Then, a language model
    (`ChatOpenAI`) is initialized with the specified model (`gpt-4-1106-preview` and
    `temperature`. Finally, we load some tools that our agent will use.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Initialize the ReAct agent. Here, the `initialize_agent` function creates and
    initializes an agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we list `tools` and `llm`, which refers to the language
    model, as well as specify the agent type as `AgentType.ZERO_SHOT_REACT_DESCRIPTION`.
    Here, `verbose=True` enables detailed logging of the agent’s thought process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Inspect the ReAct agent’s prompt. The following line prints the prompt template
    used by the ReAct agent. This prompt provides instructions to the LLM on how to
    use the available tools and follow the ReAct format (*Thought*, *Action*, *Action
    Input*, and *Observation*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Inspecting the ReAct agent’s prompt is important because it reveals the structure
    and logic that guide the behavior of the language model during tool-augmented
    reasoning and action. By printing the prompt template with `print(agent.agent.llm_chain.prompt.template)`,
    you’re not just seeing arbitrary instructions – you’re inspecting the behavioral
    scaffold that governs how the agent sequences its reasoning and tool use. This
    includes how it interprets a user query, chooses a tool from its available action
    set, constructs the input to the tool, and integrates the tool’s output (Observation)
    into further reasoning. If the prompt is poorly constructed, the model may misinterpret
    the tools, take invalid actions, or fail to chain thoughts coherently. Additionally,
    the template often includes few-shot examples that demonstrate how to alternate
    between the ReAct components properly. These examples act as implicit instructions
    for formatting and logic, helping the model generalize to unseen tasks. Inspecting
    them can reveal whether the agent was trained or instructed using general patterns
    or highly specific use cases. It also helps developers debug unexpected behavior
    or hallucinations since modifying the template directly influences the agent’s
    action selection, reasoning fidelity, and overall alignment with the intended
    ReAct cycle.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code block demonstrates how to customize the prompt template.
    You can modify the instructions, examples, and formatting to better suit your
    specific use case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `agent.agent.llm_chain.prompt.template = prompt` updates the agent’s prompt
    with the custom template.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can modify the descriptions of the tools to provide more specific
    guidance to the LLM on when and how to use each tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following line executes the agent with a sample query. The agent will use
    the ReAct framework to perform reasoning, select tools, execute actions, and generate
    a final answer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we’ll look at an example of using ReAct for document processing that leverages
    LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: ReAct Document Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LangChain also provides a `DocstoreExplorer` class for implementing ReAct logic
    with document stores such as Wikipedia. We’ll demonstrate an example by using
    `DocstoreExplorer` with Wikipedia for document-based ReAct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This code sets up a LangChain agent designed to answer questions by interacting
    with Wikipedia. Here’s a breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Wikipedia access**: First, it initializes a connection to Wikipedia, allowing
    the agent to retrieve information from it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Search` and `Lookup`. The `Search` tool enables the agent to find relevant
    Wikipedia pages, whereas the `Lookup` tool lets it extract specific information
    from those pages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AgentType.REACT_DOCSTORE` explicitly configures the agent for document store
    interactions – in this case, those for Wikipedia.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Search` tool to find relevant pages and the `Lookup` tool to extract the answer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building ReAct agents with LangChain’s Expression Language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LangChain Expression Language** (**LCEL**) offers a declarative approach
    to constructing ReAct agents. Instead of manually orchestrating the steps, LCEL
    allows you to define a processing graph that handles user input, reasoning, action
    selection, and final response generation. This section demonstrates how to implement
    a ReAct agent using this powerful framework.'
  prefs: []
  type: TYPE_NORMAL
- en: The core idea is to establish a data pipeline that takes a user’s query, uses
    an LLM to reason through a series of steps, potentially leveraging external tools,
    and ultimately arrives at an answer. This pipeline can be succinctly expressed
    using LCEL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a Python code example demonstrating this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a closer look at this setup:'
  prefs: []
  type: TYPE_NORMAL
- en: A custom prompt template is defined to guide the LLM’s reasoning and action
    selection, instead of one being fetched from a hub. This template instructs the
    LLM on the expected format of the interaction (*Question*, *Thought*, *Action*,
    *Observation*, *Final Answer*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ChatOpenAI serves as the LLM, configured to halt generation upon encountering
    the `\nObservation:` string. This signal indicates that the agent has completed
    an action and is awaiting the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The agent pipeline is constructed via LCEL, which is the chaining operation
    (`|`). This pipeline orchestrates the flow of information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It formats the input and agent’s scratchpad (previous reasoning steps)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It feeds the formatted input to the prompt
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM, with its configured stopping criteria, processes the prompt
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, `ReActSingleInputOutputParser` parses the LLM’s output, distinguishing
    between actions to be taken and the final answer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining ReActSingleInputOutputParser
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This component is key for interpreting the LLM’s output and determining the
    next step in the ReAct loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instantiation**: You create an instance of the parser, ready to process LLM-generated
    text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AgentAction` object (requesting the execution of a tool) or an `AgentFinish`
    object (providing the final answer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it detects `AgentAction`, it extracts the tool’s name and the input to be
    passed to the tool
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If it finds `AgentFinish`, it extracts the final answer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AgentAction` or `AgentFinish`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Action:` or `Final Answer:`), the parser raises an exception, indicating a
    problem with the LLM’s reasoning or the prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the agent with AgentExecutor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following code, `AgentExecutor` is a component responsible for managing
    the execution of an agent’s actions, which are chosen based on the agent’s decision-making
    process. It acts as a driver for the agent, facilitating the interaction between
    the agent and external tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: We create an `AgentExecutor` instance, providing it with the agent pipeline
    we defined earlier and the available tools, before setting `verbose=True` to see
    the agent’s thought process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `agent_executor.invoke` method starts the process. It takes a dictionary
    containing the user’s input (`"input": "Who is the current CEO of Microsoft and
    what is their` `age squared?"`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, `AgentExecutor` manages the ReAct loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It feeds the input to the agent pipeline.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent (LLM and parser) decides on an action (for example, using a search
    tool to find the CEO’s name).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The `AgentExecutor` executes the action (calls the search tool).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It passes the result back to the agent as an “`Observation`.”
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This process repeats until the agent decides it has enough information to produce
    a final answer.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This example demonstrates the basic structure of a ReAct agent built with LCEL.
    It showcases how you can define a clear, modular pipeline for complex reasoning
    tasks by combining prompts, language models, parsers, and external tools. This
    approach promotes code readability, maintainability, and flexibility in designing
    intelligent agents. This particular example asks who the current CEO of Microsoft
    is and then what their age is squared, demonstrating simple multi-turn reasoning
    from name recall to arithmetic calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Completing tasks and solving problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ReAct framework, with its ability to integrate reasoning and acting, is
    highly applicable in various task completion and problem-solving scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question-Answering (QA) with external knowledge**: ReAct can be used to create
    QA systems that can access and reason about external knowledge sources, such as
    Wikipedia or a search engine, to provide more accurate and up-to-date answers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web navigation and interaction**: ReAct agents can navigate websites, interact
    with web elements, and gather information, enabling tasks such as automated web
    research, data scraping, and online shopping assistance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software application control**: By integrating with APIs and tools, ReAct
    agents can control software applications, automate workflows, and perform complex
    tasks that require interacting with multiple systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robotics and physical world interaction**: While LLMs primarily operate in
    the textual domain, ReAct principles can be extended to controlling robots or
    other physical systems, where actions involve physical movements or interactions
    with the real world'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-step problem solving**: ReAct is well-suited for tasks that require
    breaking down a complex problem into smaller steps, reasoning about each step,
    taking actions, and using the observations to inform subsequent steps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating ReAct’s performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluating ReAct agents involves assessing both the quality of the reasoning
    and the effectiveness of the actions taken. The following metrics can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Success rate**: The percentage of tasks successfully completed by the agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: The number of steps or the amount of time taken to complete
    a task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning accuracy**: The correctness and relevance of the LLM’s reasoning
    traces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action relevance**: The appropriateness of the actions chosen by the agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observation utilization**: How effectively the agent incorporates observations
    into its subsequent reasoning and actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error analysis**: Identifying common failure modes or weaknesses in the agent’s
    performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider some evaluation techniques that can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human evaluation**: Having human experts evaluate the agent’s reasoning,
    actions, and final outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated metrics**: Using automated scripts or LLMs to assess specific aspects
    of the agent’s performance, such as the correctness of answers or the relevance
    of actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Benchmarking**: Comparing the agent’s performance against predefined benchmarks
    or other agents on standardized tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ablation studies**: Systematically removing or modifying components of the
    ReAct framework (for example, removing the reasoning steps) to understand their
    contribution to overall performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safety, control, and ethical considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ReAct systems, especially when integrated with external tools, raise several
    safety, control, and ethical concerns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unpredictable behavior**: The combination of LLM reasoning and external tool
    use can lead to unpredictable or unintended behavior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety of actions**: Actions taken by the agent may have real-world consequences,
    especially if the agent is connected to systems that can affect the physical world'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias and fairness**: ReAct agents may inherit and amplify biases present
    in the training data of the LLM or the external tools they use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misuse potential**: Malicious actors could potentially use ReAct agents for
    harmful purposes, such as generating misinformation or automating attacks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accountability**: Determining responsibility for the actions and decisions
    of a ReAct agent can be challenging due to the non-deterministic nature of the
    underlying LLM models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some mitigation strategies for these issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sandboxing**: Running ReAct agents in isolated environments to limit their
    potential impact'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human oversight**: Incorporating human review and approval into the ReAct
    process, especially for critical decisions or actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety rules and constraints**: Implementing rules and constraints to prevent
    the agent from taking harmful or unethical actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and auditing**: Continuously monitoring the agent’s behavior and
    maintaining logs for auditing purposes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency and explainability**: Designing ReAct agents that can explain
    their reasoning and decision-making process to improve understanding and trust'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations and future directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While ReAct is a powerful framework, it has certain limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dependency on external tools**: ReAct’s effectiveness is partly dependent
    on the capabilities and reliability of the external tools it uses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error propagation**: Errors in tool use or interpretation of observations
    can propagate through the reasoning process, leading to incorrect conclusions
    or actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token limitations**: The iterative nature of ReAct can lead to long sequences
    of text, potentially exceeding the token limits of some LLMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational cost**: Multiple rounds of reasoning, action, and observation
    can be computationally expensive, especially when using LLMs or complex tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt engineering challenges**: Designing effective ReAct prompts that properly
    guide the LLM’s reasoning and action selection can be challenging and may require
    experimentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 22**.1* shows the limitations of ReAct pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 22.1 – Limitations of the ReAct pattern](img/B31249_22_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 22.1 – Limitations of the ReAct pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'However, by combining the power of LLMs with the ability to take actions and
    incorporate external information, ReAct provides new possibilities for creating
    more capable and versatile AI systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved tool integration**: Developing more seamless and robust methods
    for integrating LLMs with external tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced reasoning capabilities**: Combining ReAct with other advanced reasoning
    techniques, such as ToT, to handle more complex scenarios'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning from experience**: Enabling ReAct agents to learn from their past
    interactions and improve their performance over time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-agent ReAct**: Exploring scenarios where multiple ReAct agents collaborate
    or compete to solve problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-world deployment**: Moving beyond simulated environments and deploying
    ReAct agents in real-world applications with the appropriate safety and control
    mechanisms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the ReAct framework, a powerful technique
    for prompting your LLMs to not only reason through complex scenarios but also
    plan and simulate the execution of actions, similar to how humans operate in the
    real world.
  prefs: []
  type: TYPE_NORMAL
- en: The ReAct framework represents a significant advancement in the development
    of intelligent agents that can reason, plan, and interact with their environment.
    ReAct can also be considered a precursor to more advanced frameworks such as **Reasoning
    WithOut Observation** (**ReWOO**), something we’ll explore in the next chapter.
  prefs: []
  type: TYPE_NORMAL
