- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 2*, we introduced the concept of prompt engineering as the process
    of designing and optimizing prompts – the text input that guides the behavior
    of a **large language model** (**LLM**) – for LLMs for a wide variety of applications
    and research topics. Since prompts have a massive impact on LLM performance, prompt
    engineering is a crucial activity while designing LLM-powered applications. In
    fact, there are several techniques that can be implemented not only to refine
    your LLM’s responses but also to reduce risks associated with hallucination and
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to cover the emerging techniques in the field
    of prompt engineering, starting from basic approaches up to advanced frameworks.
    By the end of this chapter, you will have the foundations to build functional
    and solid prompts for your LLM-powered applications, which will also be relevant
    in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic principles of prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced techniques of prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the tasks in this chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI account and API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7.1 or later version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find all the code and examples in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_04.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A prompt is a text input that guides the behavior of an LLM to generate a text
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is the process of designing effective prompts that elicit
    high-quality and relevant output from LLMs. Prompt engineering requires creativity,
    understanding of the LLM, and precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows an example of how a well-written prompt can instruct
    the same model to perform three different tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21714_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Example of prompt engineering to specialize LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: As you might imagine, the prompt becomes one of the key elements for an LLM-powered
    application’s success. As such, it is pivotal to invest time and resources in
    this step, following some best practices and principles that we are going to cover
    in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Principles of prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally speaking, there are no fixed rules to obtain the “perfect” prompt
    since there are too many variables to be taken into account (the type of model
    used, the goal of the application, the supporting infrastructure, and so on).
    Nevertheless, there are some clear principles that have proven to produce positive
    effects if incorporated into the prompt. Let’s examine some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Clear instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The principle of giving clear instructions is to provide the model with enough
    information and guidance to perform the task correctly and efficiently. Clear
    instructions should include the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The goal or objective of the task, such as “write a poem” or “summarize an article”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The format or structure of the expected output, such as “use four lines with
    rhyming words” or “use bullet points with no more than 10 words each”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The constraints or limitations of the task, such as “do not use any profanity”
    or “do not copy any text from the source”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context or background of the task, such as “the poem is about autumn” or
    “the article is from a scientific journal”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s say, for example, that we want our model to fetch any kind of instructions
    from text and return to us a tutorial in a bullet list. Also, if there are no
    instructions in the provided text, the model should inform us about that. Here
    are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to initialize our model. For this purpose, we are going to leverage
    OpenAI’s GPT-3.5-turbo model. We first install the `openai` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To initialize the model, I used the `openai` Python library and set the OpenAI
    API key as the environmental variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see, the chat model comes with two variables placeholders: `system
    message` (or metaprompt), where we define how we want our model to behave, and
    `instructions` (or query), where the user will ask the model its questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, it takes the user’s query (in this case, the text instructions). For
    this scenario, I set the two variables `system_message` and `instructions` as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let’s test our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if we pass the model another text that does not contain any instructions,
    it will be able to respond as we instructed it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: By giving clear instructions, you can help the model understand what you want
    it to do and how you want it to do it. This can improve the quality and relevance
    of the model’s output and reduce the need for further revisions or corrections.
  prefs: []
  type: TYPE_NORMAL
- en: However, sometimes, there are scenarios where clarity is not enough. We might
    need to infer the way of thinking of our LLM to make it more robust with respect
    to its task. In the next section, we are going to examine one of these techniques,
    which will be very useful in the case of accomplishing complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Split complex tasks into subtasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed earlier, prompt engineering is a technique that involves designing
    effective inputs for LLMs to perform various tasks. Sometimes, the tasks are too
    complex or ambiguous for a single prompt to handle, and it is better to split
    them into simpler subtasks that can be solved by different prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of splitting complex tasks into subtasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text summarization:** A complex task that involves generating a concise and
    accurate summary of a long text. This task can be split into subtasks such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the main points or keywords from the text
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rewriting the main points or keywords in a coherent and fluent way
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Trimming the summary to fit a desired length or format
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine translation:** A complex task that involves translating a text from
    one language to another. This task can be split into subtasks such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting the source language of the text
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting the text into an intermediate representation that preserves the meaning
    and structure of the original text
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the text in the target language from the intermediate representation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poem generation**: A creative task that involves producing a poem that follows
    a certain style, theme, or mood. This task can be split into subtasks such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a poetic form (such as sonnet, haiku, limerick, etc.) and a rhyme scheme
    (such as ABAB, AABB, ABCB, etc.) for the poem
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a title and a topic for the poem based on the user’s input or preference
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the lines or verses of the poem that match the chosen form, rhyme
    scheme, and topic
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Refining and polishing the poem to ensure coherence, fluency, and originality
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: A technical task that involves producing a code snippet
    that performs a specific function or task. This task can be split into subtasks
    such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a programming language (such as Python, Java, C++, etc.) and a framework
    or library (such as TensorFlow, PyTorch, React, etc.) for the code
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a function name and a list of parameters and return values for the
    code based on the user’s input or specification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the body of the function that implements the logic and functionality
    of the code
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding comments and documentation to explain the code and its usage
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider the following example in Python, where we will ask our model
    to generate a summary of an article:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will leverage OpenAI’s GPT-3.5-turbo model in a manner similar to the example
    discussed earlier in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set both the `system_message` and `article` variables as follows (you
    can find the entire scripts in the book’s GitHub repository):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To see the output, you can run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the obtained output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model was able to produce a high-quality summary based on
    the key topics extracted (and displayed) from the given article. The fact that
    we prompted the model to split the task into subtasks “forced” it to reduce the
    complexity of each subtask, hence improving the quality of the final result. This
    approach can also lead to noticeable results when we deal with scenarios such
    as mathematical problems since it enhances the analytical reasoning capabilities
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: In a landscape of many different LLMs, it is crucial to know that the very same
    system message may not be as efficient in all models. A system message that perfectly
    works with GPT-4 might not be as efficient when applied to Llama 2, for example.
    Therefore, it is pivotal to design the prompt in accordance with the type of LLM
    you decide to pick for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting complex tasks into easier subtasks is a powerful technique; nevertheless,
    it does not address one of the main risks of LLM-generated content, that is, having
    a wrong output. In the next two sections, we are going to see some techniques
    that are mainly aimed at addressing this risk.
  prefs: []
  type: TYPE_NORMAL
- en: Ask for justification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs are built in such a way that they predict the next token based on the
    previous ones without looking back at their generations. This might lead the model
    to output wrong content to the user, yet in a very convincing way. If the LLM-powered
    application does not provide a specific reference to that response, it might be
    hard to validate the ground truth behind it. Henceforth, specifying in the prompt
    to support the LLM’s answer with some reflections and justification could prompt
    the model to recover from its actions. Furthermore, asking for justification might
    be useful also in case of answers that are right but we simply don’t know the
    LLM’s reasoning behind it. For example, let’s say we want our LLM to solve riddles.
    To do so, we can instruct it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, I’ve specified in the metaprompt to the LLM to justify its
    answer and also provide its reasoning. Let’s see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the obtained output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Justifications are a great tool to make your model more reliable and robust
    since they force it to “rethink” its output, as well as provide us with a view
    of how the reasoning was set to solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: With a similar approach, we could also intervene at different prompt levels
    to improve our LLM’s performance. For example, we might discover that the model
    is systematically tackling a mathematical problem in the wrong way; henceforth,
    we might want to suggest the right approach directly at the metaprompt level.
    Another example might be that of asking the model to generate multiple outputs
    – along with their justifications – to evaluate different reasoning techniques
    and prompt the best one in the metaprompt.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to focus on one of these examples, more specifically,
    the possibility of generating multiple outputs and then picking the most likely
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Generate many outputs, then use the model to pick the best one
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in the previous section, LLMs are built in such a way that they predict
    the next token based on the previous ones without looking back at their generations.
    If this is the case, if one sampled token is the wrong one (in other words, if
    the model is unlucky), the LLM will keep generating wrong tokens and, henceforth,
    wrong content. Now, the bad news is that, unlike humans, LLMs cannot recover from
    errors on their own. This means that, if we ask them, they acknowledge the error,
    but we need to explicitly prompt them to think about that.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to overcome this limitation is to broaden the space of probabilities
    of picking the right token. Rather than generating just one response, we can prompt
    the model to generate multiple responses, and then pick the one that is most suitable
    for the user’s query. This splits the job into two subtasks for our LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating multiple responses to the user’s query
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comparing those responses and picking the best one, according to some criteria
    we can specify in the metaprompt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s see an example, following up from the riddles examined in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, I’ve prompted the model to generate three answers to the riddle,
    then to give me the most likely, justifying why. Let’s see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model selected the most plausible answer along with a justification
    of its choice. It is interesting to note that “clock” and “watch” might seem similar
    responses; however, the model specified that “watch” is usually worn on a person’s
    wrist and, even though it doesn’t mean it has arms or legs, this element might
    have lowered the probability of being the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: What would you have picked?
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, forcing the model to tackle a problem with different approaches
    is a way to collect multiple samples of reasonings, which might serve as further
    instructions in the metaprompt. For example, if we want the model to always propose
    something that is not the most straightforward solution to a problem – in other
    words, if we want it to “think differently” – we might force it to solve a problem
    in N ways and then use the most creative reasoning as a framework in the metaprompt.
  prefs: []
  type: TYPE_NORMAL
- en: The last element we are going to examine is the overall structure we want to
    give to our metaprompt. In fact, in previous examples, we saw a sample system
    message with some statements and instructions. In the next section, we will see
    how the order and “strength” of those statements and instructions are not invariants.
  prefs: []
  type: TYPE_NORMAL
- en: Repeat instructions at the end
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs tend not to process the metaprompt attributing the same weight or imprortance
    to all the sections. In fact, in his blog post *Large Language Model Prompt Engineering
    for Complex Summarization*, John Stewart (a software engineer at Microsoft) found
    some interesting outcomes from arranging prompt sections ([https://devblogs.microsoft.com/ise/gpt-summary-prompt-engineering/](https://devblogs.microsoft.com/ise/gpt-summary-prompt-engineering/)).
    More specifically, after several experimentations, he found that repeating the
    main instruction at the end of the prompt can help the model overcome its inner
    **recency bias**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Recency bias is the tendency of LLMs to give more weight to the information
    that appears near the end of a prompt, and ignore or forget the information that
    appears earlier. This can lead to inaccurate or inconsistent responses that do
    not take into account the whole context of the task. For example, if the prompt
    is a long conversation between two people, the model may only focus on the last
    few messages and disregard the previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some ways to overcome recency bias:'
  prefs: []
  type: TYPE_NORMAL
- en: One possible way to overcome recency bias is to break down the task into smaller
    steps or subtasks and provide feedback or guidance along the way. This can help
    the model focus on each step and avoid getting lost in irrelevant details. We’ve
    covered this technique in the *Split complex tasks into subtasks* section in,
    which we discussed splitting complex tasks into easier subtasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way to overcome recency bias with prompt engineering techniques is to
    repeat the instructions or the main goal of the task at the end of the prompt.
    This can help remind the model of what it is supposed to do and what kind of response
    it should generate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, let’s say we want our model to output the sentiment of a whole
    chat history between an AI agent and the user. We want to make sure that the model
    will output the sentiment in lowercase and without punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the following example (the conversation is truncated, but you
    can find the whole code in the book’s GitHub repository). In this case, the key
    instruction is that of having as output only the sentiment in lowercase and without
    punctuation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In this scenario, we have key instructions before the conversation, so let’s
    initialize our model and feed it with the two variables `system_message` and `conversation`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output that we receive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The model didn’t follow the instruction of having only lowercase letters. Let’s
    try to repeat the instruction also at the end of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, let’s invoke our model with the updated `system_message`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, now the model was able to provide exactly the output we desired.
    This approach is particularly useful whenever we have a conversation history to
    keep storing in the context window. If this is the case, having the main instructions
    at the beginning might induce the model not to have them in mind once it also
    goes through the whole history, hence reducing their strength.
  prefs: []
  type: TYPE_NORMAL
- en: Use delimiters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last principle to be covered is related to the format we want to give to
    our metaprompt. This helps our LLM to better understand its intents as well as
    relate different sections and paragraphs to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we can use delimiters within our prompt. A delimiter can be
    any sequence of characters or symbols that is clearly mapping a schema rather
    than a concept. For example, we can consider the following sequences to be delimiters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`>>>>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`====`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`------`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`####`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` ` ` ` ` ` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This leads to a series of benefits, including:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clear separation: Delimiters mark distinct sections within a prompt, separating
    instructions, examples, and desired output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guidance for LLMs: Proper use of delimiters removes ambiguity, guiding the
    model effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enhanced precision: Delimiters improve prompt understanding, resulting in more
    relevant responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Improved coherence: Effective use of delimiters organizes instructions, inputs,
    and outputs, leading to coherent responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider, for example, a metaprompt that aims at instructing the model
    to translate user’s tasks into Python code, providing an example to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]def my_print(text):'
  prefs: []
  type: TYPE_NORMAL
- en: return print(text)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the above example, we’ve used delimiters to both specify the beginning and
    end of an example for a one-shot learning approach and, within the example, specify
    the Python code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]python def fibonacci(n):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if n < 0:'
  prefs: []
  type: TYPE_NORMAL
- en: return None
  prefs: []
  type: TYPE_NORMAL
- en: 'elif n == 0:'
  prefs: []
  type: TYPE_NORMAL
- en: return 0
  prefs: []
  type: TYPE_NORMAL
- en: 'elif n == 1:'
  prefs: []
  type: TYPE_NORMAL
- en: return 1
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: return fibonacci(n-1) + fibonacci(n-2) [PRE27]
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it also printed the code with backticks, as shown within the
    system message.
  prefs: []
  type: TYPE_NORMAL
- en: All the principles examined up to this point are general rules that can make
    your LLM-powered application more robust. Those techniques should be kept in mind
    regardless of the type of application you are developing since they are general
    best practices that improve your LLM performance. In the following section, we
    are going to see some advanced techniques for prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced techniques might be implemented for specific scenarios and address
    the way the model reasons and thinks about the answer before providing it to the
    final user. Let’s look at some of these in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In their paper *Language Models are Few-Shot Learners*, Tom Brown et al. demonstrate
    that GPT-3 can achieve strong performance on many NLP tasks in a few-shot setting.
    This means that for all tasks, GPT-3 is applied without any fine-tuning, with
    tasks and few-shot demonstrations specified purely via text interaction with the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: This is an example and evidence of how the concept of few-shot learning – which
    means providing the model with examples of how we would like it to respond – is
    a powerful technique that enables model customization without interfering with
    the overall architecture.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we want our model to generate a tagline for a new product
    line of climbing shoes we’ve just coined – Elevation Embrace. We have an idea
    of what the tagline should be like – concise and direct. We could explain it to
    the model in plain text; however, it might be more effective simply to provide
    it with some examples of similar projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an implementation with code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how our model will handle this request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it maintained the style, length, and also writing convention
    of the provided taglines. This is extremely useful when you want your model to
    follow examples you already have, such as fixed templates.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, most of the time, few-shot learning is powerful enough to customize
    a model even in extremely specialized scenarios, where we could think about fine-tuning
    as the proper tool. In fact, proper few-shot learning could be as effective as
    a fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at another example. Let’s say we want to develop a model that specializes
    in sentiment analysis. To do so, we provide it with a series of examples of texts
    with different sentiments, alongside the output we would like – positive or negative.
    Note that this set of examples is nothing but a small training set for supervised
    learning tasks; the only difference from fine-tuning is that we are not updating
    the model’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide you with a concrete representation of what was said above, let’s
    provide our model with just two examples for each label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To test our classifier, I’ve used the IMDb database of movie reviews available
    on Kaggle at [https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/data](https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/data).
    As you can see, the dataset contains many movie reviews along with their associated
    sentiment – positive or negative. Let’s substitute the binary label of 0–1 with
    a verbose label of Negative–Positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the first few records of the dataset, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21714_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: First observations of the movie dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want to test the performance of our model over a sample of 10 observations
    of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A text on a white background  Description automatically generated](img/B21714_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Output of a GPT-3.5 model with few-shot examples'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, by comparing the `label` and `predicted` columns, the model
    was able to correctly classify all the reviews, without even fine-tuning! This
    is just an example of what you can achieve – in terms of model specialization
    – with the technique of few-shot learning.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of thought
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced in the paper *Chain-of-Thought Prompting Elicits Reasoning in Large
    Language Models* by Wei et al., **chain of thought** (**CoT**) is a technique
    that enables complex reasoning capabilities through intermediate reasoning steps.
    It also encourages the model to explain its reasoning, “forcing” it not to be
    too fast and risking giving the wrong response (as we saw in previous sections).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we want to prompt our LLM to solve generic first-degree equations.
    To do so, we are going to provide it with a basic reasoning list that it might
    want to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how it can be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model clearly followed the seven steps specified in the
    metaprompt, which also allows the model to “take its time” to perform this task.
    Note that you can also combine it with few-shot prompting to get better results
    on more complex tasks that require reasoning before responding.
  prefs: []
  type: TYPE_NORMAL
- en: With CoT, we are prompting the model to generate intermediate reasoning steps.
    This is also a component of another reasoning technique, which we are going to
    examine in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: ReAct
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Introduced in the paper *ReAct: Synergizing Reasoning and Acting in Language
    Models* by Yao et al., **ReAct** (**Reason and Act**) is a general paradigm that
    combines reasoning and acting with LLMs. ReAct prompts the language model to generate
    verbal reasoning traces and actions for a task, and also receives observations
    from external sources such as web searches or databases. This allows the language
    model to perform dynamic reasoning and quickly adapt its action plan based on
    external information. For example, you can prompt the language model to answer
    a question by first reasoning about the question, then performing an action to
    send a query to the web, then receiving an observation from the search results,
    and then continuing with this thought, action, observation loop until it reaches
    a conclusion.'
  prefs: []
  type: TYPE_NORMAL
- en: The difference between CoT and ReAct approaches is that CoT prompts the language
    model to generate intermediate reasoning steps for a task, while ReAct prompts
    the language model to generate intermediate reasoning steps, actions, and observations
    for a task.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the “action” phase is generally related to the possibility for our
    LLM to interact with external tools, such as a web search.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say we want to ask our model for some up-to-date information
    about the upcoming Olympic games. To do so, we are going to build a smart LangChain
    agent (as described in *Chapter 2*) leveraging `SerpAPIWrapperWrapper` (to wrap
    the `SerpApi` to navigate the web), the `AgentType` tool (to decide which type
    of agent to use for our goal), and other prompt-related modules (to make it easier
    to “templatize” our instructions). Let’s see how we can do this (I won’t dive
    deeper into each component of the following code since the next chapter will be
    entirely focused on LangChain and its main components):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, for this purpose, I’ve used a pre-built agent type available
    in LangChain called `ZERO_SHOT_REACT_DESCRIPTION`. It comes with a precompiled
    prompt that follows the ReAct approach. Let’s inspect that prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now test our agent by asking something about the upcoming Olympic games
    and zooming in on the intermediate steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output with intermediate steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the obtained output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: At the time of this question (7th of October 2023), the answer is definitely
    correct. Note how the model went through several iterations of `Observation`/`Thought`/`Action`
    until it reached the conclusion. This is a great example of how prompting a model
    to think step by step and explicitly define each step of the reasoning makes it
    “wiser” and more cautious before answering. It is also a great technique to prevent
    hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, prompt engineering is a powerful discipline, still in its emerging
    phase yet already widely adopted within LLM-powered applications. In the following
    chapters, we are going to see concrete applications of this technique.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered many aspects of the activity of prompt engineering,
    a core step in the context of improving the performance of LLMs within your application,
    as well as customizing it depending on the scenario. Prompt engineering is an
    emerging discipline that is paving the way for a new category of applications,
    infused with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We started with an introduction to the concept of prompt engineering and why
    it is important, and then moved toward the basic principles – including clear
    instructions, asking for justification, etc. Then, we moved on to more advanced
    techniques that are meant to shape the reasoning approach of our LLM: few-shot
    learning, CoT, and ReAct.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters, we will see those techniques in action by building real-world
    applications using LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ReAct approach: [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What is prompt engineering?: [https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering](https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt engineering techniques: [https://blog.mrsharm.com/prompt-engineering-guide/](https://blog.mrsharm.com/prompt-engineering-guide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt engineering principles: [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recency bias: [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#repeat-instructions-at-the-end](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#repeat-instructions-at-the-end)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Large Language Model Prompt Engineering for Complex Summarization: [https://devblogs.microsoft.com/ise/2023/06/27/gpt-summary-prompt-engineering/](https://devblogs.microsoft.com/ise/2023/06/27/gpt-summary-prompt-engineering/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Language Models are Few-Shot Learners: [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IMDb dataset: [https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/code](https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/code)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ReAct: [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chain of Thought Prompting Elicits Reasoning in Large Language Models: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/llm](https://packt.link/llm)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code214329708533108046.png)'
  prefs: []
  type: TYPE_IMG
