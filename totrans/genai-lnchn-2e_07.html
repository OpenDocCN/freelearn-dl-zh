<html><head></head><body>
<div><div><h1 class="chapterNumber"><a id="_idTextAnchor274"/>6</h1>
<h1 class="chapterTitle" id="_idParaDest-145"><a id="_idTextAnchor275"/>Advanced Applications and Multi-Agent Systems</h1>
<p class="normal">In the previous chapter, we defined what an agent is. But how do we design and build a high-performing agent? Unlike the prompt engineering techniques we’ve previously explored, developing effective agents involves several distinct design patterns every developer should be familiar with. In this chapter, we’re going to discuss key architectural patterns behind agentic AI. We’ll look into multi-agentic architectures and the ways to organize communication between agents. We will develop an advanced agent with self-reflection that uses tools to answer complex exam questions. We will learn about additional LangChain and LangGraph APIs that are useful when implementing agentic architectures, such as details about LangGraph streaming and ways to implement handoff as part of advanced control flows.</p>
<p class="normal">Then, we’ll briefly touch on the LangGraph platform and discuss how to develop adaptive systems, by including humans in the loop, and what kind of prebuilt building blocks LangGraph offers for this. We will also look<a id="_idIndexMarker498"/> into the <strong class="keyWord">Tree-of-Thoughts</strong> (<strong class="keyWord">ToT</strong>) pattern and develop a ToT agent ourselves, discussing further ways to improve it by implementing advanced trimming mechanisms. Finally, we’ll learn about advanced long-term memory mechanisms on LangChain and LangGraph, such as caches and stores.</p>
<div><p class="normal">In all, we’ll touch on the following topics in this chapter:</p>
<ul>
<li class="b lletList">Agentic architectures</li>
<li class="b lletList">Multi-agent architectures</li>
<li class="b lletList">Building adaptive systems</li>
<li class="b lletList">Exploring reasoning paths</li>
<li class="b lletList">Agent memory<a id="_idTextAnchor276"/></li>
</ul>
<h1 class="heading-1" id="_idParaDest-146"><a id="_idTextAnchor277"/>Agentic architectures</h1>
<p class="normal">As we learned in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>, agents <a id="_idIndexMarker499"/>help humans solve tasks. Building an agent involves balancing two elements. On one side, it’s very similar to application development in the sense that you’re combining APIs (including calling foundational models) with production-ready quality. On the other side, you’re helping LLMs think and solve a task.</p>
<p class="normal">As we discussed in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>, agents don’t have a specific algorithm to follow. We give an LLM partial control over the execution flow, but to guide it, we use various tricks that help us as humans to reason, solve tasks, and think clearly. We should not assume that an LLM can magically figure everything out itself; at the current stage, we should guide it by creating reasoning workflows. Let’s recall the ReACT agent we learned about in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>, an example of a tool-calling pattern:</p>
<figure class="mediaobject"><img alt="Figure 6.1: A prebuilt REACT workflow on LangGraph" src="img/B32363_06_01.png"/></figure>
<p class="packt_figref">Figure 6.1: A prebuilt REACT workflow on LangGraph</p>
<div><p class="normal">Let’s look at a few relatively simple design patterns that help with building well-performing agents. You will see these patterns in various combinations across different domains and agentic architectures:</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Tool calling</strong>: LLMs are <a id="_idIndexMarker500"/>trained to do controlled generation via tool calling. Hence, wrap your problem as a tool-calling problem when appropriate instead of creating complex prompts. Keep in mind that tools should have clear descriptions and property names, and experimenting with them is part of the prompt engineering exercise. We discussed this pattern in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>. </li>
<li class="b lletList"><strong class="keyWord">Task decomposition</strong>: Keep your prompts relatively simple. Provide specific instructions with few-shot examples and split complex tasks into smaller steps. You can give an LLM partial control over the task decomposition and planning process, managing the flow by an external orchestrator. We used this pattern in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a> when we built a plan-and-solve agent.</li>
<li class="b lletList"><strong class="keyWord">Cooperation and diversity</strong>: Final outputs on complex tasks can be improved if you introduce cooperation between multiple instances of LLM-enabled agents. Communicating, debating, and sharing different perspectives helps, and you can also benefit from various skill sets by initiating your agents with different system prompts, available toolsets, etc. Natural language is a native way for such agents to communicate since LLMs were trained on natural language tasks.</li>
<li class="b lletList"><strong class="keyWord">Reflection and adaptation</strong>: Adding implicit loops of reflection generally improves the quality of end-to-end reasoning on complex tasks. LLMs get feedback from the external environment by calling the tools (and these calls might fail or produce unexpected results), but at the same time, LLMs can continue iterating and self-recover from their mistakes. As an exaggeration, remember that we often use the same LLM-as-a-judge, so adding a loop when we ask an LLM to evaluate its own reasoning and find errors often helps it to recover. We will learn how to build adaptive systems later in this chapter.</li>
<li class="b lletList"><strong class="keyWord">Models are nondeterministic and can generate multiple candidates</strong>: Do not focus on a single output; explore different reasoning paths by expanding the dimension of potential options to try out when an LLM interacts with the external environment when looking for the solution. We will investigate this pattern in more detail in the section below when we discuss ToT<a id="_idIndexMarker501"/> and <strong class="keyWord">Language Agent Tree Search</strong> (<strong class="keyWord">LATS</strong>) examples.</li>
<li class="b lletList"><strong class="keyWord">Code-centric problem framing</strong>: Writing code is very natural for an LLM, so try to frame the problem as a code-writing problem if possible. This might become a very powerful way of solving the task, especially if you wrap it with a code-executing sandbox, a loop for<a id="_idIndexMarker502"/> improvement based on the output, access to various powerful libraries for data analysis or visualization, and a generation step afterward. We will go into more detail in <a href="E_Chapter_7.xhtml#_idTextAnchor327"><em class="italic">Chapter 7</em></a>.</li>
</ul>
<p class="normal">Two important comments: first, develop your agents aligned with the best software development practices, and make them agile, modular, and easily configurable. That would allow you to put multiple specialized agents together, and give users the opportunity to easily tune each agent based on their specific task.</p>
<p class="normal">Second, we want to emphasize (once again!) the importance of evaluation and experimentation. We will talk about evaluation in more detail in <a href="E_Chapter_9.xhtml#_idTextAnchor448"><em class="italic">Chapter 9</em></a>. But it’s important to keep in mind that there is no clear path to success. Different patterns work better on different types of tasks. Try things, experiment, iterate, and don’t forget to evaluate the results of your work. Data, such as tasks and expected outputs, and simulators, a safe way for LLMs to interact with tools, are key to building really<a id="_idIndexMarker503"/> complex and effective agents.</p>
<p class="normal">Now that we have created a mental map of various design patterns, we’ll look deeper into these principles by discussing various agentic architectures and looking at examples. We will start by enhancing the RAG architecture we discussed in <a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic">Chapter 4</em></a> with an agentic appro<a id="_idTextAnchor278"/>ach.</p>
<h2 class="heading-2" id="_idParaDest-147"><a id="_idTextAnchor279"/>Agentic RAG</h2>
<p class="normal">LLMs enable the <a id="_idIndexMarker504"/>development of intelligent agents capable of tackling complex, non-repetitive tasks that defy description as deterministic workflows. By splitting reasoning into steps in different ways and orchestrating them in a relatively simple way, agents can demonstrate a significantly higher task completion rate on complex open tasks.</p>
<p class="normal">This agent-based approach can be applied across numerous domains, including RAG systems, which we discussed in <a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic">Chapter 4</em></a>. As a reminder, what exactly is <em class="italic">agentic RAG</em>? Remember, a classic pattern for a RAG system is to retrieve chunks given the query, combine them into the context, and ask an LLM to generate an answer given a system prompt, combined context, and the question.</p>
<p class="normal">We can improve each of these steps using the principles discussed above (decomposition, tool calling, and adaptation):</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Dynamic retrieval</strong> hands over the retrieval query generation to the LLM. It can decide itself whether to use sparse embeddings, hybrid methods, keyword search, or web search. You can wrap retrievals as tools and orchestrate them as a LangGraph graph.</li>
<li class="b lletList"><strong class="keyWord">Query expansion</strong> tasks an LLM to generate multiple queries based on initial ones, and then you combine search outputs based on reciprocal fusion or another technique.</li>
<li class="b lletList"><strong class="keyWord">Decomposition of reasoning on retrieved chunks</strong> allows you to ask an LLM to evaluate each individual chunk given the question (and filter it out if it’s irrelevant) to compensate for retrieval inaccuracies. Or you can ask an LLM to summarize each chunk by keeping only information given for the input question. Anyway, instead of throwing a huge piece of context in front of an LLM, you perform many smaller reasoning steps in parallel first.This can not only improve the RAG quality by itself but also increase the amount of initially retrieved chunks (by decreasing the relevance threshold) or expand each individual chunk with its neighbors. In other words, you can overcome some retrieval challenges with LLM reasoning. It might increase the overall performance of your application, but of course, it comes with latency and potential cost implications.</li>
<li class="b lletList"><strong class="keyWord">Reflection steps and iterations</strong> task LLMs to dynamically iterate on retrieval and query expansion by evaluating the outputs after each iteration. You can also use additional grounding and attribution tools as a separate step in your workflow and, based on that, reason whether you need to continue working on the answer or the answer can be returned to the user.</li>
</ul>
<p class="normal">Based on our definition from the previous chapters, RAG becomes agentic RAG when you have shared partial control with the LLM over the execution flow. For example, if the LLM decides how to retrieve, reflects on retrieved chunks, and adapts based on the first version of the answer, it becomes agentic RAG. From our perspective, at this point, it starts making sense to migrate to LangGraph since it’s designed specifically for building such applications, but of course, you can stay with LangChain or any other framework you prefer (compare how we implemented map-reduce video<a id="_idIndexMarker505"/> summarization with LangChain and LangGraph separately in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>).</p>
<h1 class="heading-1" id="_idParaDest-148"><a id="_idTextAnchor280"/>Multi-agent architectures</h1>
<p class="normal">In <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>, we learned<a id="_idIndexMarker506"/> that decomposing a complex task into simpler subtasks typically increases LLM performance. We built a plan-and-solve agent that goes a step further than CoT and encourages the LLM to generate a plan and follow it. To a certain extent, this architecture was a multi-agent one since the research agent (which was responsible for generating and following the plan) invoked another agent that focused on a different type of task – solving very specific tasks with provided tools. Multi-agentic workflows orchestrate multiple agents, allowing them to enhance each other and at the same time keep agents modular (which makes it easier to test and reuse them).</p>
<div><p class="normal">We will look into a few core agentic architectures in the remainder of this chapter, and introduce some important LangGraph interfaces (such as streaming details and handoffs) that are useful to develop agents. If you’re interested, you can find more examples and tutorials on the LangChain documentation page at <a href="https://langchain-ai.github.io/langgraph/tutorials/#agent-architectures">https://langchain-ai.github.io/langgraph/tutorials/#agent-architectures</a>. We’ll begin with discussing the importance of specialization in multi-agentic systems, including what the consensus mechanism is and the different consensus mech<a id="_idTextAnchor281"/>anisms.</p>
<h2 class="heading-2" id="_idParaDest-149"><a id="_idTextAnchor282"/>Agent roles and specialization</h2>
<p class="normal">When working on a<a id="_idIndexMarker507"/> complex task, we as humans know that usually, it’s beneficial to have a team with diverse skills and backgrounds. There is much evidence from research and experiments that suggests this is also true for generative AI agents. In fact, developing specialized agents offers several advantages for complex AI systems.</p>
<p class="normal">First, specialization improves performance on specific tasks. This allows you to:</p>
<ul>
<li class="b lletList">Select the optimal set of tools for each task type.</li>
<li class="b lletList">Craft tailored prompts and workflows.</li>
<li class="b lletList">Fine-tune hyperparameters such as temperature for specific contexts.</li>
</ul>
<p class="normal">Second, specialized agents help manage complexity. Current LLMs struggle when handling too many tools at once. As a best practice, limit each agent to 5-15 different tools, rather than overloading a single agent with all available tools. How to group tools is still an open question; typically, grouping them into toolkits to create coherent specialized agents helps.</p>
<figure class="mediaobject"><img alt="Figure 6.2: A supervisor pattern" src="img/B32363_06_02.png"/></figure>
<p class="packt_figref">Figure 6.2: A supervisor pattern</p>
<div><p class="normal">Besides becoming <em class="italic">specialized</em>, keep your agents <em class="italic">modular</em>. It becomes easier to maintain and improve such agents. Also, by working on enterprise assistant use cases, you will eventually end up with many different agents available for users and developers within your organization that can be composed<a id="_idIndexMarker508"/> together. Hence, keep in mind that you should make such specialized agents configurable.</p>
<p class="normal">LangGraph allows you to easily compose graphs by including them as a subgraph in a larger graph. There are two ways of doing this:</p>
<ul>
<li class="b lletList">Compile an agent as a graph and pass it as a callable when defining a node of another agent:<pre>builder.add_node("pay", payments_agent)</pre></li>
<li class="b lletList">Wrap the child agent’s invocation with a Python function and use it within the definition of the parent’s node:<pre>def _run_payment(state):
  result = payments_agent.invoke({"client_id"; state["client_id"]})
 return {"payment status": ...}
...
builder.add_node("pay", _run_payment)</pre></li>
</ul>
<p class="normal">Note, that your agents might have different schemas (since they perform different tasks). In the first case, the parent agent would pass the same keys in schemas with the child agent when invoking it. In turn, when the child agent finishes, it would update the parent’s state and send back the values corresponding to matching keys in both schemas. At the same time, the second option gives you full control over how you construct a state that is passed to the child agent, and how the state of the <a id="_idIndexMarker509"/>parent agent should be updated as a result. For more information, take a look at the documentation at <a href="https://langchain-ai.github.io/langgraph/how-tos/subgraph/">https://langchain-ai.github.io/langgraph/how-tos/subgraph/</a>.</p>
<h2 class="heading-2" id="_idParaDest-150"><a id="_idTextAnchor283"/>Consensus mechanism</h2>
<p class="normal">We can let multiple <a id="_idIndexMarker510"/>agents work on the same tasks in<a id="_idIndexMarker511"/> parallel as well. These agents might have a different “personality” (introduced by their system prompts; for example, some of them might be more curious and explorative, and others might be more strict and heavily grounded) or even varying architectures. Each of them independently works on getting a solution for the problem, and then you use a consensus mechanism to choose the best solution from a few drafts.</p>
<div><figure class="mediaobject"><img alt="Figure 6.3: A parallel execution of the task with a final consensus step" src="img/B32363_06_03.png"/></figure>
<p class="packt_figref">Figure 6.3: A parallel execution of the task with a final consensus step</p>
<p class="normal">We saw an example of implementing a consensus mechanism based on majority voting in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>. You can wrap it as a separate LangGraph node, and there are alternative ways of coming to a consensus across multiple agents:</p>
<ul>
<li class="b lletList">Let each agent see other solutions and score each of them on a scale of 0 to 1, and then take the solution with the maximum score.</li>
<li class="b lletList">Use an alternative voting mechanism.</li>
<li class="b lletList">Use majority voting. It typically works for classification or similar tasks, but it might be difficult to implement majority voting if you have a free-text output. This is the fastest and the cheapest (in terms of token consumption) mechanism since you don’t need to run any additional prompts.</li>
<li class="b lletList">Use an external oracle if it exists. For instance, when solving a mathematical equation, you can easily verify if the solution is feasible. Computational costs depend on the problem but typically are low.</li>
<li class="b lletList">Use another (maybe more powerful) LLM as a judge to pick the best solution. You can ask an LLM to come up with a score for each solution, or you can task it with a multi-class classification problem by presenting all of them and asking it to pick the best one.</li>
<li class="b lletList">Develop another agent that excels at the task of selecting the best solution for a general task from a set of solutions.</li>
</ul>
<div><p class="normal">It’s worth mentioning that a consensus mechanism has certain latency and cost implications, but typically they’re negligible relative to the costs of solving a task itself. If you task N agents with the same task, your token consumption increases N times, and the consensus mechanism adds a relatively small overhead on top of that difference.</p>
<p class="normal">You can also implement your own consensus mechanism. When you do this, consider the following:</p>
<ul>
<li class="b lletList">Use few-shot prompting when using an LLM as a judge.</li>
<li class="b lletList">Add examples demonstrating how to score different input-output pairs.</li>
<li class="b lletList">Consider including scoring rubrics for different types of responses.</li>
<li class="b lletList">Test the mechanism on diverse outputs to ensure consistency.</li>
</ul>
<p class="normal">One important<a id="_idIndexMarker512"/> note on parallelization – when you let LangGraph<a id="_idIndexMarker513"/> execute nodes in parallel, updates are applied to the main state in the same order as you’ve added nodes to<a id="_idTextAnchor284"/> your graph.</p>
<h2 class="heading-2" id="_idParaDest-151"><a id="_idTextAnchor285"/>Communication protocols</h2>
<p class="normal">The third architecture<a id="_idIndexMarker514"/> option is to let agents<a id="_idIndexMarker515"/> communicate and work collaboratively on a task. For example, the agents might benefit from various personalities configured through system prompts. Decomposition of a complex task into smaller subtasks also helps you retain control over your application and how your agents c<a id="_idTextAnchor286"/>ommunicate.</p>
<figure class="mediaobject"><img alt="Figure 6.4: Reflection pattern" src="img/B32363_06_04.png"/></figure>
<p class="packt_figref">Figure 6.4: Reflection pattern</p>
<div><p class="normal">Agents can work collaboratively on a task by providing critique and reflection. There are multiple reflection patterns starting from self-reflection, when the agent analyzes its own steps and identifies areas for improvements (but as mentioned above, you might initiate the reflecting agent with a slightly different system prompt); cross-reflection, when you use another agent (for example, using another foundational model); or even reflection, which includes <strong class="keyWord">Human-in-the-Loop</strong> (<strong class="keyWord">HIL</strong>) on<a id="_idIndexMarker516"/> critical checkpoints (we’ll see in the next section how to build adaptive systems of this kind).</p>
<p class="normal">You can keep one agent <a id="_idIndexMarker517"/>as a supervisor, allow agents to communicate <a id="_idIndexMarker518"/>in a network (allowing them to decide which agent to send a message or a task), introduce a certain hierarchy, or develop more complex flows (for inspiration, take a look at some diagrams on the LangGraph documentation page at <a href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/">https://langchain-ai.github.io/langgraph/concepts/multi_agent/</a>).</p>
<p class="normal">Designing multi-agent workflows is still an open area of research and experimentation, and you need to answer a lot of questions:</p>
<ul>
<li class="b lletList">What and how many agents should we include in our system?</li>
<li class="b lletList">What roles should we assign to these agents?</li>
<li class="b lletList">What tools should each agent have access to?</li>
<li class="b lletList">How should agents interact with each other and through which mechanism?</li>
<li class="b lletList">What specific parts of the workflow should we automate?</li>
<li class="b lletList">How do we evaluate our automation and how can we collect data for this evaluation? Additionally, what are our success criteria?</li>
</ul>
<p class="normal">Now that we’ve examined some core considerations and open questions around multi-agent communication, let’s explore two practical mechanisms to structure and facilitate agent interactions: <em class="italic">semantic routing</em>, which directs tasks intelligently based on their content, and <em class="italic">organizing interaction</em>, detailing the <a id="_idIndexMarker519"/>specific formats and structures that agents can use to effectively exch<a id="_idTextAnchor287"/>ange information.</p>
<h3 class="heading-3" id="_idParaDest-152"><a id="_idTextAnchor288"/>Semantic router</h3>
<p class="normal">Among many different ways to organize communication between agents in a true multi-agent setup, an important one is a semantic router. Imagine developing an enterprise assistant. Typically it becomes more and more complex because it starts dealing with various types of questions – general questions (requiring public data and general knowledge), questions about the company (requiring access to the proprietary company-wide data sources), and questions specific to the user (requiring access to the data provided by the user itself). Maintaining such an application as a<a id="_idIndexMarker520"/> single agent becomes very difficult very soon. Again, we can apply our design patterns – decomposition and collaboration!</p>
<div><p class="normal">Imagine we have implemented three types of agents – one answering general questions grounded on public data, another one grounded on a company-wide dataset and knowing about company specifics, and the third one specialized on working with a small source of user-provided documents. Such specialization helps us to use patterns such as few-shot prompting and controlled generation. Now we can add a semantic router – the first layer that asks an LLM to classify the question and routes it to the corresponding agent based on classification results. Each agent (or some of them) might even use a self-consistency approach, as we learned in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>, to increase the LLM classification accuracy.</p>
<figure class="mediaobject"><img alt="Figure 6.5: Semantic router pattern" src="img/B32363_06_05.png"/></figure>
<p class="packt_figref">Figure 6.5: Semantic router pattern</p>
<p class="normal">It’s worth mentioning that a task might fall into two or more categories – for example, I can ask, “<em class="italic">What is X and how can I do Y? </em>“ This might not be such a common use case in an assistant setting, and you can decide what to do in that case. First of all, you might just educate the user by replying with an explanation that they should task your application with a single problem per turn. Sometimes developers tend to be too focused on trying to solve everything programmatically. But some product features are relatively easy to solve via the UI, and users (especially in the enterprise setup) are ready to provide their input. Maybe, instead of solving a classification problem on the prompt, just add a simple checkbox in the UI, or let the system double-check if the level of confidence is low.</p>
<p class="normal">You can also use tool calling or other controlled generation techniques we’ve learned about to extract both goals and route the execution to two specialized agents with different tasks.</p>
<div><p class="normal">Another important aspect of semantic routing is that the performance of your application depends a lot on <a id="_idIndexMarker521"/>classification accuracy. You can use all the techniques we have discussed in the book to improve it – few-shot prompting (including dynamic one), incorporating user feedback, sam<a id="_idTextAnchor289"/>pling, and others.</p>
<h3 class="heading-3" id="_idParaDest-153"><a id="_idTextAnchor290"/>Organizing interactions</h3>
<p class="normal">There are two ways to organize communication in multi-agent systems:</p>
<ul>
<li class="b lletList">Agents communicate via specific structures that force them to put their thoughts and reasoning traces in a specific form, as we saw in the <em class="italic">plan-and-solve</em> example in the previous chapter. We saw how our planning node communicated with the ReACT agent via a Pydantic model with a well-structured plan (which, in turn, was a result of an LLM’s controlled generation).</li>
<li class="b lletList">On the other hand, LLMs were trained to take natural language as input and produce an output in the same format. Hence, it’s a very natural way for them to communicate via messages, and you can implement a communication mechanism by applying messages from different agents to the shared list of messages!.</li>
</ul>
<p class="normal">When communicating with messages, you can share all messages via a so-called <em class="italic">scratchpad</em> – a shared list of messages. In that case, your context can grow relatively quickly and you might need to use some of the mechanisms to trim the chat memory (like preparing running summaries) that we discussed in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>. But as general advice, if you need to filter or prioritize messages in the history of communication between multiple agents, go with the first approach and let them communicate through a controlled output. It would give you more control of the state of your workflow at any given point in time. Also, you might end up with a situation where you have a complicated sequence of messages, for example, <em class="italic">[SystemMessage, HumanMessage, AIMessage, ToolMessage, AIMessage, AIMessage, SystemMessage, …]</em>. Depending on the foundational model you’re using, double-check that the model’s provider supports such sequences, since previously, many providers supported only relatively simple sequences – SystemMessages followed by alternating HumanMessage and AIMessage (maybe with a ToolMessage instead of a human one if a tool invocation was decided).</p>
<p class="normal">Another alternative is to share only the final results of each execution. This keeps the list of messages relatively short.</p>
<div><p class="normal">Now it’s time to<a id="_idIndexMarker522"/> look at a practical example. Let’s develop a research agent that uses tools to answer complex multiple-choice questions based on the public MMLU dataset (we’ll use high school geography questions). First, we need to grab a dataset from Hugging Face:</p>
<pre>from datasets import load_dataset
ds = load_dataset("cais/mmlu", "high_school_geography")
ds_dict = ds["test"].take(2).to_dict()
print(ds_dict["question"][0])
&gt;&gt; The main factor preventing subsistence economies from advancing economically is the lack of</pre>
<p class="normal">These are our answer options:</p>
<pre>print(ds_dict["choices"][0])
&gt;&gt; ['a currency.', 'a well-connected transportation infrastructure.', 'government activity.', 'a banking service.']</pre>
<p class="normal">Let’s start with a ReACT agent, but let’s deviate from a default system prompt and write our own prompt. Let’s focus this agent on being creative and working on an evidence-based solution (please note that we used elements of CoT prompting, which we discussed in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>):</p>
<pre>from langchain.agents import load_tools
from langgraph.prebuilt import create_react_agent
research_tools = load_tools(
  tool_names=["ddg-search", "arxiv", "wikipedia"],
  llm=llm)
system_prompt = (
 "You're a hard-working, curious and creative student. "
 "You're preparing an answer to an exam quesion. "
 "Work hard, think step by step."
 "Always provide an argumentation for your answer. "
 "Do not assume anything, use available tools to search "
 "for evidence and supporting statements."
)</pre>
<div><p class="normal">Now, let’s create the agent itself. Since we have a custom prompt for the agent, we need a prompt template that includes a system message, a template that formats the first user message based on a question and answers provided, and a placeholder for further messages to be added to the graph’s state. We<a id="_idIndexMarker523"/> also redefine the default agent’s state by inheriting from <code class="inlineCode">AgentState</code> and adding additional keys to it:</p>
<pre>from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langgraph.graph import MessagesState
from langgraph.prebuilt.chat_agent_executor import AgentState
raw_prompt_template = (
 "Answer the following multiple-choice question. "
 "\nQUESTION:\n{question}\n\nANSWER OPTIONS:\n{option}\n"
)
prompt = ChatPromptTemplate.from_messages(
   [("system", system_prompt),
    ("user", raw_prompt_template),
    ("placeholder", "{messages}")
    ]
)
class MyAgentState(AgentState):
 question: str
 options: str
research_agent = create_react_agent(
  model=llm_small, tools=research_tools, state_schema=MyAgentState,
  prompt=prompt)</pre>
<p class="normal">We could have stopped here, but let’s go further. We used a specialized research agent based on the ReACT pattern (and we slightly adjusted its default configuration). Now let’s add a reflection step to it, and use another role profile for an agent who will actionably criticize our “student’s” work:</p>
<pre>reflection_prompt = (
 "You are a university professor and you're supervising a student who is "
 "working on multiple-choice exam question. "
 "nQUESTION: {question}.\nANSWER OPTIONS:\n{options}\n."
 "STUDENT'S ANSWER:\n{answer}\n"</pre>
<div><pre> "Reflect on the answer and provide a feedback whether the answer "
 "is right or wrong. If you think the final answer is correct, reply with "
 "the final answer. Only provide critique if you think the answer might "
 "be incorrect or there are reasoning flaws. Do not assume anything, "
 "evaluate only the reasoning the student provided and whether there is "
 "enough evidence for their answer."
)
class Response(BaseModel):
 """A final response to the user."""
   answer: Optional[str] = Field(
       description="The final answer. It should be empty if critique has been provided.",
       default=None,
   )
   critique: Optional[str] = Field(
       description="A critique of the initial answer. If you think it might be incorrect, provide an actionable feedback",
       default=None,
   )
reflection_chain = PromptTemplate.from_template(reflection_prompt) | llm.with_structured_output(Response)</pre>
<p class="normal">Now we need<a id="_idIndexMarker524"/> another research agent that takes not only question and answer options but also the previous answer and the feedback. The research agent is tasked with using tools to improve the answer and address the critique. We created a simplistic and illustrative example. You can always improve it by adding error handling, Pydantic validation (for example, checking that either an answer or critique is provided), or handling conflicting or ambiguous feedback (for example, structure prompts that help the agent prioritize feedback points when there are multiple criticisms).</p>
<div><p class="normal">Note that we use a less capable LLM for our ReACT agents, just to demonstrate the power of the reflection approach (otherwise the graph might finish in a single iteration since the agent would figure out the correct answer with the first attempt):</p>
<pre>raw_prompt_template_with_critique = (
 "You tried to answer the exam question and you get feedback from your "
 "professor. Work on improving your answer and incorporating the feedback. "
 "\nQUESTION:\n{question}\n\nANSWER OPTIONS:\n{options}\n\n"
 "INITIAL ANSWER:\n{answer}\n\nFEEDBACK:\n{feedback}"
)
prompt = ChatPromptTemplate.from_messages(
   [("system", system_prompt),
    ("user", raw_prompt_template_with_critique),
    ("placeholder", "{messages}")
    ]
)
class ReflectionState(ResearchState):
 answer: str
 feedback: str
research_agent_with_critique = create_react_agent(model=llm_small, tools=research_tools, state_schema=ReflectionState, prompt=prompt)</pre>
<p class="normal">When defining the<a id="_idIndexMarker525"/> state of our graph, we need to keep track of the question and answer options, the current answer, and the critique. Also note that we track the amount of interaction between a <em class="italic">student</em> and a <em class="italic">professor</em> (to avoid infinite cycles between them) and we use a custom reducer for that (which summarizes old steps and new steps on each run). Let’s define the full state, nodes, and conditional edges:</p>
<pre>from typing import Annotated, Literal, TypedDict
from langchain_core.runnables.config import RunnableConfig
from operator import add
from langchain_core.output_parsers import StrOutputParser
class ReflectionAgentState(TypedDict):
   question: str</pre>
<div><pre>   options: str
   answer: str
   steps: Annotated[int, add]
   response: Response
def _should_end(state: AgentState, config: RunnableConfig) -&gt; Literal["research", END]:
   max_reasoning_steps = config["configurable"].get("max_reasoning_steps", 10)
 if state.get("response") and state["response"].answer:
 return END
 if state.get("steps", 1) &gt; max_reasoning_steps:
 return END
 return "research"
reflection_chain = PromptTemplate.from_template(reflection_prompt) | llm.with_structured_output(Response)
def _reflection_step(state):
   result = reflection_chain.invoke(state)
 return {"response": result, "steps": 1}
def _research_start(state):
 answer = research_agent.invoke(state)
 return {"answer": answer["messages"][-1].content}
def _research(state):
 agent_state = {
 "answer": state["answer"],
 "question": state["question"],
 "options": state["options"],
 "feedback": state["response"].critique
 }
 answer = research_agent_with_critique.invoke(agent_state)
 return {"answer": answer["messages"][-1].content}</pre>
<div><p class="normal">Let’s put it<a id="_idIndexMarker526"/> all together and create our graph:</p>
<pre>builder = StateGraph(ReflectionAgentState)
builder.add_node("research_start", _research_start)
builder.add_node("research", _research)
builder.add_node("reflect", _reflection_step)
builder.add_edge(START, "research_start")
builder.add_edge("research_start", "reflect")
builder.add_edge("research", "reflect")
builder.add_conditional_edges("reflect", _should_end)
graph = builder.compile()
display(Image(graph.get_graph().draw_mermaid_png()))</pre>
<figure class="mediaobject"><img alt="Figure 6.6: A research agent with reflection" src="img/B32363_06_06.png"/></figure>
<p class="packt_figref">Figure 6.6: A research agent with reflection</p>
<p class="normal">Let’s run it <a id="_idIndexMarker527"/>and inspect what’s happening:</p>
<pre>question = ds_dict["question"][0]
options = "\n".join(
  [f"{i}. {a}" for i, a in enumerate(ds_dict["choices"][0])])
async for _, event in graph.astream({"question": question, "options": options}, stream_mode=["updates"]):
 print(event)</pre>
<div><p class="normal">We have omitted the full output here (you’re welcome to take the code from our GitHub repository and experiment with it yourself), but the first answer was wrong:</p>
<pre>Based on the DuckDuckGo search results, none of the provided statements are entirely true.  The searches reveal that while there has been significant progress in women's labor force participation globally,  it hasn't reached a point where most women work in agriculture, nor has there been a worldwide decline in participation.  Furthermore, the information about working hours suggests that it's not universally true that women work longer hours than men in most regions. Therefore, there is no correct answer among the options provided.</pre>
<p class="normal">After five iterations, the weaker LLM was able to figure out the correct answer (keep in mind that the “professor” only evaluated the reasoning itself and it didn’t use external tools or its own knowledge). Note that, technically speaking, we implemented cross-reflection and not self-reflection (since we’ve used a different LLM for reflection than the one we used for the reasoning). Here’s an example of the feedback provided during the first round:</p>
<pre>The student's reasoning relies on outside search results which are not provided, making it difficult to assess the accuracy of their claims. The student states that none of the answers are entirely true, but multiple-choice questions often have one best answer even if it requires nuance. To properly evaluate the answer, the search results need to be provided, and each option should be evaluated against those results to identify the most accurate choice, rather than dismissing them all. It is possible one of the options is more correct than the others, even if not perfectly true. Without the search results, it's impossible to determine if the student's conclusion that no answer is correct is valid. Additionally, the student should explicitly state what the search results were.</pre>
<p class="normal">Next, let’s <a id="_idIndexMarker528"/>discuss an alternative communication style for a multi-agent setup, via a shared list of messages. But before that, we should discuss the LangGraph handoff mechanism and dive into some details of s<a id="_idTextAnchor291"/>treaming with LangGraph.</p>
<h2 class="heading-2" id="_idParaDest-154"><a id="_idTextAnchor292"/>LangGraph streaming</h2>
<p class="normal">LangGraph streaming<a id="_idIndexMarker529"/> might sometimes be a source of <a id="_idIndexMarker530"/>confusion. Each graph has not only a <code class="inlineCode">stream</code> and a corresponding asynchronous <code class="inlineCode">astream</code> method, but also an <code class="inlineCode">astream_events</code>. Let’s dive into the difference.</p>
<div><p class="normal">The <code class="inlineCode">Stream</code> method allows you to stream changes to the graph’s state after each super-step. Remember, we discussed what a super-step is in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>, but to keep it short, it’s a single iteration over the graph where parallel nodes belong to a single super-step while sequential nodes belong to different super-steps. If you need actual streaming behavior (like in a chatbot, so that users feel like something is happening and the model is actually thinking), you should use <code class="inlineCode">astream</code> with <code class="inlineCode">messages</code> mode.</p>
<p class="normal">You have five modes with <code class="inlineCode">stream/astream</code> methods (of course, you can combine multiple modes):</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-6">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Mode</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Output</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">updates</p>
</td>
<td class="No-Table-Style">
<p class="normal">Streams only updates to the graph produced by the node</p>
</td>
<td class="No-Table-Style">
<p class="normal">A dictionary where each node name maps to its corresponding state update)</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">values</p>
</td>
<td class="No-Table-Style">
<p class="normal">Streams the full state of the graph after each super-step</p>
</td>
<td class="No-Table-Style">
<p class="normal">A dictionary with the entire graph’s state</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">debug</p>
</td>
<td class="No-Table-Style">
<p class="normal">Attempts to stream as much information as possible in the debug mode</p>
</td>
<td class="No-Table-Style">
<p class="normal">A dictionary with a timestamp, task_type, and all the corresponding information for every event</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">custom</p>
</td>
<td class="No-Table-Style">
<p class="normal">Streams events emitted by the node using a StreamWriter</p>
</td>
<td class="No-Table-Style">
<p class="normal">A dictionary that was written from the node to a custom writer </p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">messages</p>
</td>
<td class="No-Table-Style">
<p class="normal">Streams full events (for example, ToolMessages) or its chunks in a streaming node if possible (e.g., AI Messages)</p>
</td>
<td class="No-Table-Style">
<p class="normal">A tuple with token or message segment and a dictionary containing metadata from the node</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 6.1: Different streaming modes for LangGraph</p>
<p class="normal">Let’s look at an <a id="_idIndexMarker531"/>example. If we take the ReACT agent we used in <a id="_idIndexMarker532"/>the section above and stream with the <code class="inlineCode">values</code> mode, we’ll get the full state returned after every super-step (you can see that the total number of messages is always increasing):</p>
<pre>async for _, event in research_agent.astream({"question": question, "options": options}, stream_mode=["values"]):
 print(len(event["messages"]))
&gt;&gt; 0
<a id="_idTextAnchor293"/>1
<a id="_idTextAnchor294"/>3
<a id="_idTextAnchor295"/>4</pre>
<div><p class="normal">If we switch to the <code class="inlineCode">update</code> mode, we’ll get a dictionary where the key is the node’s name (remember that parallel nodes can be called within a single super-step) and a corresponding update to the state sent by this node:</p>
<pre>async for _, event in research_agent.astream({"question": question, "options": options}, stream_mode=["updates"]):
 node = list(event.keys())[0]
 print(node, len(event[node].get("messages", [])))
&gt;&gt; agent 1
tools 2
agent 1</pre>
<p class="normal">LangGraph <code class="inlineCode">stream</code> always emits a tuple where the first value is a stream mode (since you can pass multiple modes by adding them to the list).</p>
<p class="normal">Then you need an <code class="inlineCode">astream_events </code>method that streams back events happening within the nodes – not just tokens generated by the LLM but any event available for a callback:</p>
<pre>seen_events = set([])
async for event in research_agent.astream_events({"question": question, "options": options}, version="v1"):
 if event["event"] not in seen_events:
   seen_events.add(event["event"])
print(seen_events)
&gt;&gt; {'on_chat_model_end', 'on_chat_model_stream', 'on_chain_end', 'on_prompt_end', 'on_tool_start', 'on_chain_stream', 'on_chain_start', 'on_prompt_start', 'on_chat_model_start', 'on_tool_end'}</pre>
<p class="normal">You can<a id="_idIndexMarker533"/> find<a id="_idIndexMarker534"/> a full list of the events at <a href="https://python.langchain.com/docs/concepts/callbacks/#callback-even﻿ts">https://python.langchain.com/docs/concepts/callbacks/#callback-events</a>.</p>
<h2 class="heading-2" id="_idParaDest-155"><a id="_idTextAnchor297"/>Handoffs</h2>
<p class="normal">So far, we have<a id="_idIndexMarker535"/> learned that a node in LangGraph does a chunk of work and sends updates to a common state, and an edge controls the flow – it decides which node to invoke next (in a deterministic manner or based on the current state). When implementing multi-agent architectures, your nodes can be not only functions but other agents, or subgraphs (with their own state). You might need to combine state updates and flow controls.</p>
<div><p class="normal">LangGraph allows you to do that with a <code class="inlineCode">Command</code> – you can update your graph’s state and at the same time invoke another agent by passing a custom state to it. This is called a <em class="italic">handoff</em> – since an agent hands off control to another one. You need to pass an <code class="inlineCode">update</code> – a dictionary with an update of the current state to be sent to your graph – and <code class="inlineCode">goto</code> – a name (or list of names) of the nodes to hand off control to:</p>
<pre>from langgraph.types import Command
def _make_payment(state):
  ...
 if ...:
 return Command(
     update={"payment_id": payment_id},
     goto="refresh_balance"
  )
  ...</pre>
<p class="normal">A destination agent can be a node from the current or a parent (<code class="inlineCode">Command.PARENT</code>) graph. In other words, you can change the control flow only within the current graph, or you can pass it back to the workflow that initiated this one (for example, you can’t pass control to any random workflow by ID). You can also invoke a <code class="inlineCode">Command</code> from a tool, or wrap a <code class="inlineCode">Command </code>as a tool, and then an LLM can decide to hand off control to a specific agent. In <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>, we discussed the map-reduce pattern and the <code class="inlineCode">Send</code> class, which allowed us to invoke a node in the graph by passing a specific input state to it. We can use <code class="inlineCode">Command</code> together with <code class="inlineCode">Send</code> (in this example, the destination agent<a id="_idIndexMarker536"/> belongs to the parent gr<a id="_idTextAnchor298"/>aph):</p>
<pre>from langgraph.types import Send
def _make_payment(state):
  ...
 if ...:
 return Command(
     update={"payment_id": payment_id},
     goto=[Send("refresh_balance", {"payment_id": payment_id}, ...],
     graph=Command.PARENT
  )
  ...</pre>
<div><h3 class="heading-3" id="_idParaDest-156"><a id="_idTextAnchor299"/>Communication via a shared messages list</h3>
<p class="normal">A few chapters <a id="_idIndexMarker537"/>earlier, we discussed how two agents can communicate via controlled output (by sending each other special Pydantic instances). Now let’s go back to the communication topic and illustrate how agents can communicate with native LangChain messages. Let’s take the research agent with a cross-reflection and make it work with a shared list of messages. First, the research agent itself looks simpler – it has a default state since it gets a user’s question as a HumanMessage:</p>
<pre>system_prompt = (
 "You're a hard-working, curious and creative student. "
 "You're working on exam quesion. Think step by step."
 "Always provide an argumentation for your answer. "
 "Do not assume anything, use available tools to search "
 "for evidence and supporting statements."
)
research_agent = create_react_agent(
  model=llm_small, tools=research_tools, prompt=system_prompt)</pre>
<p class="normal">We also need to slightly modify the reflection prompt:</p>
<pre>reflection_prompt = (
 "You are a university professor and you're supervising a student who is "
 "working on multiple-choice exam question. Given the dialogue above, "
 "reflect on the answer provided and give a feedback "
 " if needed. If you think the final answer is correct, reply with "
 "an empty message. Only provide critique if you think the last answer "
 "might be incorrect or there are reasoning flaws. Do not assume anything, "
 "evaluate only the reasoning the student provided and whether there is "
 "enough evidence for their answer."
)</pre>
<div><p class="normal">The nodes themselves also look simpler, but we add <code class="inlineCode">Command</code> after the reflection node since we decide what to call <a id="_idIndexMarker538"/>next with the node itself. Also, we don’t wrap a ReACT research agent as a node anymore:</p>
<pre>from langgraph.types import Command
question_template = PromptTemplate.from_template(
 "QUESTION:\n{question}\n\nANSWER OPTIONS:\n{options}\n\n"
)
def _ask_question(state):
 return {"messages": [("human", question_template.invoke(state).text)]}
def _give_feedback(state, config: RunnableConfig):
 messages = event["messages"] + [("human", reflection_prompt)]
 max_messages = config["configurable"].get("max_messages", 20)
 if len(messages) &gt; max_messages:
 return Command(update={}, goto=END)
 result = llm.invoke(messages)
 if result.content:
 return Command(
     update={"messages": [("assistant", result.content)]},
     goto="research"
 )
 return Command(update={}, goto=END)</pre>
<p class="normal">The graph itself also looks very simple:</p>
<pre>class ReflectionAgentState(MessagesState):
 question: str
 options: str
builder = StateGraph(ReflectionAgentState)
builder.add_node("ask_question", _ask_question)
builder.add_node("research", research_agent)
builder.add_node("reflect", _give_feedback)
builder.add_edge(START, "ask_question")</pre>
<div><pre>builder.add_edge("ask_question", "research")
builder.add_edge("research", "reflect")
graph = builder.compile()</pre>
<p class="normal">If we run it, we will <a id="_idIndexMarker539"/>see that at every stage, the graph operates on the same (and growing) list of m<a id="_idTextAnchor300"/>essages.</p>
<h2 class="heading-2" id="_idParaDest-157"><a id="_idTextAnchor301"/>LangGraph platform</h2>
<p class="normal">LangGraph and <a id="_idIndexMarker540"/>LangChain, as you know, are open-source <a id="_idIndexMarker541"/>frameworks, but LangChain as a company offers the LangGraph platform – a commercial solution that helps you develop, manage, and deploy agentic applications. One component of the LangGraph platform is LangGraph Studio – an IDE that helps you visualize and debug your agents – and another is LangGraph Server.</p>
<p class="normal">You can read more about the LangGraph platform at the official website (<a href="https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform">https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform</a>), but let’s discuss a few key concepts for a better understanding of what it means to develop an agent.</p>
<p class="normal">After you’ve developed an agent, you can wrap it as an HTTP API (using Flask, FastAPI, or any other web framework). The LangGraph platform offers you a native way to deploy agents, and it wraps them with a unified API (which makes it easier for your applications to use these agents). When you’ve built your agent as a LangGraph graph object, you deploy an <em class="italic">assistant</em> – a specific deployment that includes an instance of your graph coupled together with a configuration. You can easily version and configure assistants in the UI, but it’s important to keep parameters configurable (and pass them as <code class="inlineCode">RunnableConfig</code> to your nodes and tools).</p>
<p class="normal">Another important concept is a <em class="italic">thread</em>. Don’t be confused, a LangGraph thread is a different concept from a Python thread (and when you pass a <code class="inlineCode">thread_id</code> in your <code class="inlineCode">RunnableConfig</code>, you’re passing a LangGraph thread ID). When you think about LangGraph threads, think about conversation or Reddit threads. A thread represents a session between your assistant (a graph with a specific configuration) and a user. You can add per-thread persistence using the checkpointing mechanism we discussed in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>.</p>
<p class="normal">A <em class="italic">run</em> is an invocation of an assistant. In most cases, runs are executed on a thread (for persistence). LangGraph Server also allows you to schedule stateless runs – they are not assigned to any thread, and because of that, the history of interactions is not persisted. LangGraph Server allows you to schedule long-running runs, scheduled runs (a.k.a. crons), etc., and it also offers a<a id="_idIndexMarker542"/> rich mechanism for webhooks attached to <a id="_idIndexMarker543"/>runs and polling results back to the user.</p>
<div><p class="normal">We’re not going to discuss the LangGraph Server API in this book. Please take a look at the documentat<a id="_idTextAnchor302"/>ion instead.</p>
<h1 class="heading-1" id="_idParaDest-158"><a id="_idTextAnchor303"/>Building adaptive systems</h1>
<p class="normal">Adaptability is a great <a id="_idIndexMarker544"/>attribute of agents. They should adapt to external and user feedback and correct their actions accordingly. As we discussed in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>, generative AI agents are adaptive through:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Tool interaction</strong>: They incorporate feedback from previous tool calls and their outputs (by including <code class="inlineCode">ToolMessages</code> that represent tool-calling results) when planning the next steps (like our ReACT agent adjusting based on search results).</li>
<li class="b lletList"><strong class="keyWord">Explicit reflection</strong>: They can be instructed to analyze current results and deliberately adjust their behavior.</li>
<li class="b lletList"><strong class="keyWord">Human feedback</strong>: They can incorporate user input at critical dec<a id="_idTextAnchor304"/>ision points.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-159"><a id="_idTextAnchor305"/>Dynamic behavior adjustment</h2>
<p class="normal">We saw how to add a<a id="_idIndexMarker545"/> reflection step to our plan-and-solve agent. Given the initial plan, and the output of the steps performed so far, we’ll ask the LLM to reflect on the plan and adjust it. Again, we continue reiterating the key idea – such reflection might not happen naturally; you might add it as a separate task (decomposition), and you keep partial control over the execution flow by designing its gener<a id="_idTextAnchor306"/>ic components.</p>
<h2 class="heading-2" id="_idParaDest-160"><a id="_idTextAnchor307"/>Human-in-the-loop</h2>
<p class="normal">Additionally, when<a id="_idIndexMarker546"/> developing agents with complex reasoning trajectories, it might be beneficial to incorporate human feedback at a certain point. An agent can ask a human to approve or reject certain actions (for example, when it’s invoking a tool that is irreversible, like a tool that makes a payment), provide additional context to the agent, or give a specific input by modifying the graph’s state.</p>
<p class="normal">Imagine we’re developing an agent that searches for job postings, generates an application, and sends this application. We might want to ask the user before submitting an application, or the logic might be more complex – the agent might be collecting data about the user, and for some job postings, it might be missing relevant context about past job experience. It should ask the user and persist this knowledge in long-term memory for better long-term adaptation.</p>
<div><p class="normal">LangGraph has a special <code class="inlineCode">interrupt</code> function to implement <strong class="keyWord">HIL</strong>-type interactions. You should include this function in the node, and by the first execution, it would throw a <code class="inlineCode">GraphInterrupt</code> exception (the value of which would be presented to the user). To resume the execution of the graph, a client should use the <code class="inlineCode">Command</code> class, which we discussed earlier in this chapter. LangGraph would start from the same node, re-execute it, and return corresponding values as a result of the node invoking the <code class="inlineCode">interrupt</code> function (if there are multiple <code class="inlineCode">interrupts</code> in your node, LangGraph would keep an ordering). You can also use <code class="inlineCode">Command</code> to route to different nodes based on the user’s input. Of course, you can use <code class="inlineCode">interrupt</code> only when a checkpointer is provided to the graph since its state should be persisted.</p>
<p class="normal">Let’s construct a very simple graph with only the node that asks a user for their home address:</p>
<pre>from langgraph.types import interrupt, Command
class State(MessagesState):
   home_address: Optional[str]
def _human_input(state: State):
   address = interrupt("What is your address?")
 return {"home_address": address}
builder = StateGraph(State)
builder.add_node("human_input", _human_input)
builder.add_edge(START, "human_input")
checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)
config = {"configurable": {"thread_id": "1"}}
for chunk in graph.stream({"messages": [("human", "What is weather today?")]}, config):
 print(chunk)
&gt;&gt; {'__interrupt__': (Interrupt(value='What is your address?', resumable=True, ns=['human_input:b7e8a744-b404-0a60-7967-ddb8d30b11e3'], when='during'),)}</pre>
<div><p class="normal">The graph returns us a special <code class="inlineCode">__interrupt__</code> state and stops. Now our application (the client) should ask the user this question, and then we can resume. Please note that we’re providing the same <code class="inlineCode">thread_id</code> to restore from the checkpoint:</p>
<pre>for chunk in graph.stream(Command(resume="Munich"), config):
 print(chunk)
&gt;&gt; {'human_input': {'home_address': 'Munich'}}</pre>
<p class="normal">Note that the graph continued to execute the human_input node, but this time the <code class="inlineCode">interrupt</code> function returned the result, and the graph’s state was updated.</p>
<p class="normal">So far, we’ve<a id="_idIndexMarker547"/> discussed a few architectural patterns on how to develop an agent. Now let’s take a look at another interesting one that allows LLMs to run multiple simulations while they’re looking<a id="_idTextAnchor308"/> for a solution.</p>
<h1 class="heading-1" id="_idParaDest-161"><a id="_idTextAnchor309"/>Exploring reasoning paths</h1>
<p class="normal">In <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>, we discussed<a id="_idIndexMarker548"/> CoT prompting. But with CoT prompting, the LLM creates a reasoning path within a single turn. What if we combine the decomposition pattern and the adaptation pattern by splitting this reaso<a id="_idTextAnchor310"/>ning into pieces?</p>
<h2 class="heading-2" id="_idParaDest-162"><a id="_idTextAnchor311"/>Tree of Thoughts</h2>
<p class="normal">Researchers from <a id="_idIndexMarker549"/>Google DeepMind and Princeton University <a id="_idIndexMarker550"/>introduced<strong class="keyWord"> the ToT </strong>technique in December 2023. They generalize the CoT pattern and use thoughts as intermediate steps in the exploration process toward the global solution.</p>
<p class="normal">Let’s return to the plan-and-solve agent we built in the previous chapter. Let’s use the non-deterministic nature of LLMs to improve it. We can generate multiple candidates for the next action in the plan on every step (we might need to increase the temperature of the underlying LLM). That would help the agent to be more adaptive since the next plan generated will take into account the outputs of the previous step.</p>
<p class="normal">Now we can build a tree of various options and explore this tree with the depth-for-search or breadth-for-search method. At the end, we’ll get multiple solutions, and we’ll use some of the consensus mechanisms discussed above to pick the best one (for example, LLM-as-a-judge).</p>
<div><figure class="mediaobject"><img alt="Figure 6.7: Solution path exploration with ToT" src="img/B32363_06_07.png"/></figure>
<p class="packt_figref">Figure 6.7: Solution path exploration with ToT</p>
<p class="normal">Please note that the model’s provider should support the generation of multiple candidates in the response (not all providers support this feature).</p>
<p class="normal">We would like to highlight (and we’re not tired of doing this repeatedly in this chapter) that there’s nothing entirely new in the ToT pattern. You take what algorithms and patterns have been used already in other areas, and you use them to build capable agents.</p>
<p class="normal">Now it’s time to do some coding. We’ll take the same components of the plan-and-solve agents we developed in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a> – a planner that creates an initial plan and <code class="inlineCode">execution_agent</code>, which is a research agent with access to tools and works on a specific step in the plan. We can make our execution agent simpler since we don’t need a custom state:</p>
<pre>execution_agent = prompt_template | create_react_agent(model=llm, tools=tools)</pre>
<div><p class="normal">We also need a <code class="inlineCode">replanner</code><em class="italic"> </em>component, which will take care of adjusting the plan based on previous <a id="_idIndexMarker551"/>observations<a id="_idIndexMarker552"/> and generating multiple candidates for the next action:</p>
<pre>from langchain_core.prompts import ChatPromptTemplate
class ReplanStep(BaseModel):
 """Replanned next step in the plan."""
   steps: list[str] = Field(
       description="different options of the proposed next step"
   )
llm_replanner = llm.with_structured_output(ReplanStep)
replanner_prompt_template = (
 "Suggest next action in the plan. Do not add any superfluous steps.\n"
 "If you think no actions are needed, just return an empty list of steps. "
 "TASK: {task}\n PREVIOUS STEPS WITH OUTPUTS: {current_plan}"
)
replanner_prompt = ChatPromptTemplate.from_messages(
   [("system", "You're a helpful assistant. You goal is to help with planning actions to solve the task. Do not solve the task itself."),
    ("user", replanner_prompt_template)
   ]
)
replanner = replanner_prompt | llm_replanner</pre>
<p class="normal">This <code class="inlineCode">replanner</code> component is crucial for our ToT approach. It takes the current plan state and generates multiple potential next steps, encouraging exploration of different solution paths rather than following a single linear sequence.</p>
<p class="normal">To track our exploration path, we need a tree data structure. The <code class="inlineCode">TreeNode</code> class below helps us maintain it:</p>
<pre>class TreeNode:
 def __init__(</pre>
<div><pre>       self,
       node_id: int,
       step: str,
       step_output: Optional[str] = None,
       parent: Optional["TreeNode"] = None,
 ):
 self.node_id = node_id
 self.step = step
 self.step_output = step_output
 self.parent = parent
 self.children = []
 self.final_response = None
 def __repr__(self):
   parent_id = self.parent.node_id if self.parent else "None"
 return f"Node_id: {self.node_id}, parent: {parent_id}, {len(self.children)} children."
 def get_full_plan(self) -&gt; str:
 """Returns formatted plan with step numbers and past results."""
   steps = []
   node = self
 while node.parent:
     steps.append((node.step, node.step_output))
     node = node.parent
   full_plan = []
 for i, (step, result) in enumerate(steps[::-1]):
 if result:
       full_plan.append(f"# {i+1}. Planned step: {step}\nResult: {result}\n")
 return "\n".join(full_plan)</pre>
<p class="normal">Each <code class="inlineCode">TreeNode</code> tracks its identity, current step, output, parent relationship, and children. We also created a method <a id="_idIndexMarker553"/>to get a formatted full plan (we’ll substitute it in <a id="_idIndexMarker554"/>place of the prompt’s template), and just to make debugging more convenient, we overrode a <code class="inlineCode">__repr__</code> method that returns a readable description of the node.</p>
<div><p class="normal">Now we need to implement the core logic of our agent. We will explore our tree of actions in a depth-for-search mode. This is where the real power of the ToT pattern comes into play:</p>
<pre>async def _run_node(state: PlanState, config: RunnableConfig):
 node = state.get("next_node")
 visited_ids = state.get("visited_ids", set())
 queue = state["queue"]
 if node is None:
 while queue and not node:
     node = state["queue"].popleft()
 if node.node_id in visited_ids:
       node = None
 if not node:
 return Command(goto="vote", update={})
 step = await execution_agent.ainvoke({
 "previous_steps": node.get_full_plan(),
 "step": node.step,
 "task": state["task"]})
 node.step_output = step["messages"][-1].content
 visited_ids.add(node.node_id)
 return {"current_node": node, "queue": queue, "visited_ids": visited_ids, "next_node": None}
async def _plan_next(state: PlanState, config: RunnableConfig) -&gt; PlanState:
 max_candidates = config["configurable"].get("max_candidates", 1)
 node = state["current_node"]
 next_step = await replanner.ainvoke({"task": state["task"], "current_plan": node.get_full_plan()})
 if not next_step.steps:
 return {"is_current_node_final": True}
 max_id = state["max_id"]
 for step in next_step.steps[:max_candidates]:
   child = TreeNode(node_id=max_id+1, step=step, parent=node)
   max_id += 1
   node.children.append(child)
   state["queue"].append(child)</pre>
<div><pre> return {"is_current_node_final": False, "next_node": child, "max_id": max_id}
async def _get_final_response(state: PlanState) -&gt; PlanState:
 node = state["current_node"]
 final_response = await responder.ainvoke({"task": state["task"], "plan": node.get_full_plan()})
 node.final_response = final_response
 return {"paths_explored": 1, "candidates": [final_response]}</pre>
<p class="normal">The <code class="inlineCode">_run_node</code> function executes the current step, while <code class="inlineCode">_plan_next</code> generates new candidate steps and adds<a id="_idIndexMarker555"/> them <a id="_idIndexMarker556"/>to our exploration queue. When we reach a final node (one where no further steps are needed), <code class="inlineCode">_get_final_response</code> generates a final solution by picking the best one from multiple candidates (originating from different solution paths explored). Hence, in our agent’s state, we should keep track of the root node, the next node, the queue of nodes to be explored, and the nodes we’ve already explored:</p>
<pre>import operator
from collections import deque
from typing import Annotated
class PlanState(TypedDict):
   task: str
   root: TreeNode
   queue: deque[TreeNode]
   current_node: TreeNode
   next_node: TreeNode
   is_current_node_final: bool
   paths_explored: Annotated[int, operator.add]
   visited_ids: set[int]
   max_id: int
   candidates: Annotated[list[str], operator.add]
   best_candidate: str</pre>
<p class="normal">This state structure keeps track of everything we need: the original task, our tree structure, exploration queue, path metadata, and candidate solutions. Note the special <code class="inlineCode">Annotated</code> types that use custom reducers (like<code class="inlineCode"> operator.add</code>) to handle merging state values properly.</p>
<div><p class="normal">One important thing to keep in mind is that LangGraph doesn’t allow you to modify <code class="inlineCode">state</code> directly. In other words, if we execute something like the following within a node, it won’t have an effect on the actual queue in the agent’s state:</p>
<pre>def my_node(state):
  queue = state["queue"]
  node = queue.pop()
  ...
  queue.append(another_node)
 return {"key": "value"}</pre>
<p class="normal">If we want to <a id="_idIndexMarker557"/>modify the queue that belongs to the state itself, we should <a id="_idIndexMarker558"/>either use a custom reducer (as we discussed in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>) or return the queue object to be replaced (since under the hood, LangGraph always created deep copies of the state before passing it to the node).</p>
<p class="normal">We need to define the final step now – the consensus mechanism to choose the final answer based on multiple generated candidates:</p>
<pre>prompt_voting = PromptTemplate.from_template(
 "Pick the best solution for a given task. "
 "\nTASK:{task}\n\nSOLUTIONS:\n{candidates}\n"
)
def _vote_for_the_best_option(state):
 candidates = state.get("candidates", [])
 if not candidates:
 return {"best_response": None}
 all_candidates = []
 for i, candidate in enumerate(candidates):
   all_candidates.append(f"OPTION {i+1}: {candidate}")
 response_schema = {
 "type": "STRING",
 "enum": [str(i+1) for i in range(len(all_candidates))]}
 llm_enum = ChatVertexAI(
     model_name="gemini-2.0-flash-001", response_mime_type="text/x.enum",
     response_schema=response_schema)
 result = (prompt_voting | llm_enum | StrOutputParser()).invoke(
     {"candidates": "\n".join(all_candidates), "task": state["task"]}</pre>
<div><pre> )
 return {"best_candidate": candidates[int(result)-1]}</pre>
<p class="normal">This voting mechanism presents all candidate solutions to the model and asks it to select the best one, leveraging the model’s ability to evaluate and compare options. </p>
<p class="normal">Now let’s add the remaining nodes and edges of the agent. We need two nodes – the one that creates an initial plan and another that evaluates the final output. Alongside these, we define two corresponding <a id="_idIndexMarker559"/>edges that evaluate whether the agent should continue on<a id="_idIndexMarker560"/> its exploration and whether it’s ready to provide a final response to the user:</p>
<pre>from typing import Literal
from langgraph.graph import StateGraph, START, END
from langchain_core.runnables import RunnableConfig
from langchain_core.output_parsers import StrOutputParser
from langgraph.types import Command
final_prompt = PromptTemplate.from_template(
 "You're a helpful assistant that has executed on a plan."
 "Given the results of the execution, prepare the final response.\n"
 "Don't assume anything\nTASK:\n{task}\n\nPLAN WITH RESUlTS:\n{plan}\n"
 "FINAL RESPONSE:\n"
)
responder = final_prompt | llm | StrOutputParser()
async def _build_initial_plan(state: PlanState) -&gt; PlanState:
 plan = await planner.ainvoke(state["task"])
 queue = deque()
 root = TreeNode(step=plan.steps[0], node_id=1)
 queue.append(root)
 current_root = root
 for i, step in enumerate(plan.steps[1:]):
   child = TreeNode(node_id=i+2, step=step, parent=current_root)
   current_root.children.append(child)
   queue.append(child)
   current_root = child
 return {"root": root, "queue": queue, "max_id": i+2}</pre>
<div><pre>async def _get_final_response(state: PlanState) -&gt; PlanState:
 node = state["current_node"]
 final_response = await responder.ainvoke({"task": state["task"], "plan": node.get_full_plan()})
 node.final_response = final_response
 return {"paths_explored": 1, "candidates": [final_response]}
def _should_create_final_response(state: PlanState) -&gt; Literal["run", "generate_response"]:
 return "generate_response" if state["is_current_node_final"] else "run"
def _should_continue(state: PlanState, config: RunnableConfig) -&gt; Literal["run", "vote"]:
 max_paths = config["configurable"].get("max_paths", 30)
 if state.get("paths_explored", 1) &gt; max_paths:
 return "vote"
 if state["queue"] or state.get("next_node"):
 return "run"
 return "vote"</pre>
<p class="normal">These functions round out our implementation by defining the initial plan creation, final response<a id="_idIndexMarker561"/> generation, and <a id="_idIndexMarker562"/>flow control logic. The <code class="inlineCode">_should_create_final_response</code> and <code class="inlineCode">_should_continue</code> functions determine when to generate a final response and when to continue exploration. With all the components in place, we construct the final state graph:</p>
<pre>builder = StateGraph(PlanState)
builder.add_node("initial_plan", _build_initial_plan)
builder.add_node("run", _run_node)
builder.add_node("plan_next", _plan_next)
builder.add_node("generate_response", _get_final_response)
builder.add_node("vote", _vote_for_the_best_option)
builder.add_edge(START, "initial_plan")
builder.add_edge("initial_plan", "run")
builder.add_edge("run", "plan_next")
builder.add_conditional_edges("plan_next", _should_create_final_response)
builder.add_conditional_edges("generate_response", _should_continue)
builder.add_edge("vote", END)</pre>
<div><pre>graph = builder.compile()
from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))</pre>
<p class="normal">This creates our finished agent with a complete execution flow. The graph begins with initial planning, proceeds through execution and replanning steps, generates responses for completed paths, and finally selects the best solution through voting. We can visualize the flow using the Mermaid diagram <a id="_idIndexMarker563"/>generator, giving us a clear picture of our agent’s <a id="_idIndexMarker564"/>decision-making process:</p>
<figure class="mediaobject"><img alt="Figure 6.8: LATS agent" src="img/B32363_06_08.png"/></figure>
<p class="packt_figref">Figure 6.8: LATS agent</p>
<div><p class="normal">We can control the maximum number of super-steps, the maximum number of paths in the tree to be explored (in particular, the maximum number of candidates for the final solution to be generated), and the number of candidates per step. Potentially, we could extend our config and control the maximum depth of the tree. Let’s run our graph:</p>
<pre>task = "Write a strategic one-pager of building an AI startup"
result = await graph.ainvoke({"task": task}, config={"recursion_limit": 10000, "configurable": {"max_paths": 10}})
print(len(result["candidates"]))
print(result["best_candidate"])</pre>
<p class="normal">We can also <a id="_idIndexMarker565"/>visualize the explored tree:</p>
<figure class="mediaobject"><img alt="Figure 6.9: Example of an explored execution tree" src="img/B32363_06_09-01.png"/></figure>
<p class="packt_figref">Figure 6.9: Example of an explored execution tree</p>
<div><p class="normal">We limited the number of candidates, but we can potentially increase it and add additional pruning logic (which <a id="_idIndexMarker566"/>will prune the leaves that are not promising). We can<a id="_idIndexMarker567"/> use the same LLM-as-a-judge approach, or use some other heuristic for pruning. We can also explore more advanced pruning strategies; we’ll talk about<a id="_idTextAnchor312"/> one of them in the next section.</p>
<h2 class="heading-2" id="_idParaDest-163"><a id="_idTextAnchor313"/>Trimming ToT with MCTS</h2>
<p class="normal">Some of you might<a id="_idIndexMarker568"/> remember AlphaGo – the first<a id="_idIndexMarker569"/> computer program that <a id="_idIndexMarker570"/>defeated humans in a game of Go. Google DeepMind developed it back in 2015, and it used <strong class="keyWord">Monte Carlo Tree Search</strong> (<strong class="keyWord">MCTS</strong>) as the core decision-making algorithm. Here’s a simple idea of how it works. Before taking the next move in a game, the algorithm builds a decision tree with potential future moves, with nodes representing your moves and your opponent’s potential responses (this tree expands quickly, as you can imagine). To keep the tree from expanding too fast, they used MCTS to search only through the most promising paths that lead to a better state in the game.</p>
<p class="normal">Now, coming back to the ToT pattern we learned about in the previous chapter. Think about the fact that the dimensionality of the ToT we’ve been building in the previous section might grow really fast. If, on every step, we’re generating 3 candidates and there are only 5 steps in the workflow, we’ll end up with 3<sup class="superscript">5</sup>=243 steps to evaluate. That incurs a lot of cost and time. We can trim the dimensionality in different ways, for example, by using MCTS. It includes selection and simulation components:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Selection</strong> helps you pick the next node when analyzing the tree. You do that by balancing exploration and exploitation (you estimate the most promising node but add some randomness to this process).</li>
<li class="b lletList">After you <strong class="keyWord">expand</strong> the tree by adding a new child to it, if it’s not a terminal node, you need to simulate the consequences of it. This might be done just by randomly playing all the next moves until the end, or using more sophisticated simulation approaches. After evaluating the child, you backpropagate the results to all the parent nodes by adjusting their probability scores for the next round of selection.</li>
</ul>
<p class="normal">We’re not aiming to go into the details and teach you MCTS. We only want to demonstrate how you apply already-existing algorithms to agentic workflows to increase their performance. One such example is a <strong class="keyWord">LATS</strong> approach suggested by<a id="_idIndexMarker571"/> Andy Zhou and colleagues in June 2024 in their paper <em class="italic">Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models</em>. Without going into too much detail (you’re welcome to look at the original paper or the corresponding tutorials), the authors added MCTS on top of ToT, and they demonstrated an increased performance on complex tasks by getting number 1 on the HumanEval benchmark. </p>
<div><p class="normal">The key idea was that instead of exploring the whole tree, they use an LLM to evaluate the quality of the solution you get at every step (by looking at the sequence of all the steps on these specific reasoning steps and the outputs you’ve got so far).</p>
<p class="normal">Now, as we’ve discussed some more advanced architectures that allow us to build better agents, there’s one last component<a id="_idIndexMarker572"/> to<a id="_idIndexMarker573"/> briefly touch on – memory. Helping agents to retain and<a id="_idIndexMarker574"/> retrieve relevant information from long-term interactions helps us to d<a id="_idTextAnchor314"/>evelop more advanced and helpful agents.</p>
<h1 class="heading-1" id="_idParaDest-164"><a id="_idTextAnchor315"/>Agent memory</h1>
<p class="normal">We discussed memory <a id="_idIndexMarker575"/>mechanisms in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>. To recap, LangGraph has the notion of short-term memory via the <code class="inlineCode">Checkpointer</code> mechanism, which saves checkpoints to persistent storage. This is the so-called per-thread persistence (remember, we discussed earlier in this chapter that the notion of a thread in LangGraph is similar to a conversation). In other words, the agent remembers our interactions within a given session, but it starts from scratch each time.</p>
<p class="normal">As you can imagine, for complex agents, this memory mechanism might be inefficient for two reasons. First, you might lose important information about the user. Second, during the exploration phase when looking for a solution, an agent might learn something important about the environment that it forgets each time – and it doesn’t look efficient. That’s why there’s the concept<a id="_idIndexMarker576"/> of <strong class="keyWord">long-term memory</strong>, which helps an agent to accumulate knowledge and gain from historical experiences, and enables its continuous improvement on the long horizon.</p>
<p class="normal">How to design and use long-term memory in practice is still an open question. First, you need to extract useful information (keeping in mind privacy requirements too; more about that in <a href="E_Chapter_9.xhtml#_idTextAnchor448"><em class="italic">Chapter 9</em></a>) that you want to store during the runtime and then you need to extract it during the next execution. Extraction is close to the retrieval problem we discussed while talking about RAG since we need to extract only knowledge relevant to the given context. The last component is the compaction of memory – you probably want to periodically self-reflect on what you have learned, optimize it, and forget irrelevant facts.</p>
<p class="normal">These are key considerations to take into account, but we haven’t seen any great practical implementations of long-term memory for agentic workflows yet. In practice, these days people typically use two components – a built-in <strong class="keyWord">cache</strong> (a mechanism to cache LLMs responses), a built-in <strong class="keyWord">store</strong> (a persistent key-value store), and a custom cache or database. Use the custom option when:</p>
<div><ul>
<li class="b lletList">You need additional flexibility for how you organize memory – for example, you would like to keep track of all memory states.</li>
<li class="b lletList">You need advanced read or write access patterns when working with this memory.</li>
<li class="b lletList">You need to keep the memory distributed and across multiple workers, and you’d lik<a id="_idTextAnchor316"/>e to use a database other than PostgreSQL.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-165"><a id="_idTextAnchor317"/>Cache</h2>
<p class="normal">Caching allows you to<a id="_idIndexMarker577"/> save and retrieve key values. Imagine you’re working on an enterprise question-answering assistance application, and in the UI, you ask a user whether they like the answer. If the answer is positive, or if you have a curated dataset of question-answer pairs for the most important topics, you can store these in a cache. When the same (or a similar) question is asked later, the system can quickly return the cached response instead of regenerating it from scratch.</p>
<p class="normal">LangChain allows you to set a global cache for LLM responses in the following way (after you have initialized the cache, the LLM’s response will be added to the cache, as we’ll see below):</p>
<pre>from langchain_core.caches import InMemoryCache
from langchain_core.globals import set_llm_cache
cache = InMemoryCache()
set_llm_cache(cache)
llm = ChatVertexAI(model="gemini-2.0-flash-001", temperature=0.5)
llm.invoke("What is the capital of UK?")</pre>
<p class="normal">Caching with LangChain works as follows: Each vendor’s implementation of a <code class="inlineCode">ChatModel</code> inherits from the base class, and the base class first tries to look up a value in the cache during generation. cache is a global variable that we can expect (of course, only after it has been initialized). It caches responses based on the key that consists of a string representation of the prompt and the string representation of the LLM instance (produced by the <code class="inlineCode">llm._get_llm_string</code> method).</p>
<p class="normal">This means the LLM’s generation parameters (such as <code class="inlineCode">stop_words</code> or <code class="inlineCode">temperature</code>) are included in the cache key:</p>
<pre>import langchain
print(langchain.llm_cache._cache)</pre>
<p class="normal">LangChain supports in-memory and SQLite caches out of the box (they form part of <code class="inlineCode">langchain_core.caches</code>), and there are also many vendor integrations – available through the <code class="inlineCode">langchain_community.cache</code> subpackage at <a href="https://python.langchain.com/api_reference/community/cache.html">https://python.langchain.com/api_reference/community/cache.html</a> or through specific vendor integrations (for example, <code class="inlineCode">langchain-mongodb</code> offers cache integration for MongoDB: <a href="https://langchain-mongodb.readthedocs.io/en/latest/langchain_mongodb/api_docs.html">https://langchain-mongodb.readthedocs.io/en/latest/langchain_mongodb/api_docs.html</a>).</p>
<div><p class="normal">We recommend introducing a separate LangGraph node instead that hits an actual cache (based on Redis or another database), since it allows you to control whether you’d like to search for similar questions using the embedding mechanism we discussed i<a id="_idTextAnchor318"/>n <a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic">Chapter 4</em></a> when we were talking about RAG.</p>
<h2 class="heading-2" id="_idParaDest-166"><a id="_idTextAnchor319"/>Store</h2>
<p class="normal">As we have learned<a id="_idIndexMarker578"/> before, a <code class="inlineCode">Checkpointer</code> mechanism allows you to enhance your workflows with a thread-level persistent memory; by thread-level, we mean a conversation-level persistence. Each conversation can be started where it stops, and the workflow executes the previously collected context.</p>
<p class="normal">A <code class="inlineCode">BaseStore</code> is a persistent key-value storage system that organizes your values by namespace (hierarchical tuples of string paths, similar to folders. It supports standard operations such as <code class="inlineCode">put</code>, <code class="inlineCode">delete</code> and <code class="inlineCode">get</code> operations, as well as a <code class="inlineCode">search</code> method that implements different semantic search capabilities (typically, based on the embedding mechanism) and accounts for a hierarchical nature of namespaces.</p>
<p class="normal">Let’s initialize a store and add some values to it:</p>
<pre>from langgraph.store.memory import InMemoryStore
in_memory_store = InMemoryStore()
in_memory_store.put(namespace=("users", "user1"), key="fact1", value={"message1": "My name is John."})
in_memory_store.put(namespace=("users", "user1", "conv1"), key="address", value={"message": "I live in Berlin."})</pre>
<p class="normal">We can easily query the value:</p>
<pre>in_memory_store.get(namespace=("users", "user1", "conv1"), key="address")
&gt;&gt;  Item(namespace=['users', 'user1'], key='fact1', value={'message1': 'My name is John.'}, created_at='2025-03-18T14:25:23.305405+00:00', updated_at='2025-03-18T14:25:23.305408+00:00')</pre>
<p class="normal">If we query it by a partial <a id="_idIndexMarker579"/>path of the namespace, we won’t get any results (we need a full matching namespace). The following would return no results:</p>
<pre>in_memory_store.get(namespace=("users", "user1"), key="conv1")</pre>
<div><p class="normal">On the other side, when using <code class="inlineCode">search</code>, we can use a partial namespace path:</p>
<pre>print(len(in_memory_store.search(("users", "user1", "conv1"), query="name")))
print(len(in_memory_store.search(("users", "user1"), query="name")))
&gt;&gt; 1
<a id="_idTextAnchor320"/>2</pre>
<p class="normal">As you can see, we were able to retrieve all relevant facts stored in memory by using a partial search.</p>
<p class="normal">LangGraph has built-in <code class="inlineCode">InMemoryStore</code> and <code class="inlineCode">PostgresStore</code> implementations. Agentic memory mechanisms are still evolving. You can build your own implementation from available components, but we <a id="_idIndexMarker580"/>should see a lot o<a id="_idTextAnchor321"/>f progress in the coming years or even months.</p>
<h1 class="heading-1" id="_idParaDest-167"><a id="_idTextAnchor322"/>Summary</h1>
<p class="normal">In this chapter, we dived deep into advanced applications of LLMs and the architectural patterns that enable them, leveraging LangChain and LangGraph. The key takeaway is that effectively building complex AI systems goes beyond simply prompting an LLM; it requires careful architectural design of the workflow itself, tool usage, and giving an LLM partial control over the workflow. We also discussed different agentic AI design patterns and how to develop agents that leverage LLMs’ tool-calling abilities to solve complex tasks.</p>
<p class="normal">We explored how LangGraph streaming works and how to control what information is streamed back during execution. We discussed the difference between streaming state updates and partial streaming answer tokens, learned about the Command interface as a way to hand off execution to a specific node within or outside the current LangGraph workflow, looked at the LangGraph platform and its main capabilities, and discussed how to implement HIL with LangGraph. We discussed how a thread on LangGraph differs from a traditional Pythonic definition (a thread is somewhat similar to a conversation instance), and we learned how to add memory to our workflow per-thread and with cross-thread persistence. Finally, we learned how to expand beyond basic LLM applications and build robust, adaptive, and intelligent systems by leveraging the advanced capabilities of LangChain and LangGraph.</p>
<p class="normal">In the next chapter, we’ll take a look at how generative AI transforms the software engineering industry by as<a id="_idTextAnchor323"/>sisting in code development and data analysis.</p>
<div><h1 class="heading-1" id="_idParaDest-168"><a id="_idTextAnchor324"/>Questions</h1>
<ol>
<li class="numberedList" value="1">Name at least three design patterns to consider when building generative AI agents.</li>
<li class="numberedList">Explain the concept of “dynamic retrieval” in the context of agentic RAG.</li>
<li class="numberedList">How can cooperation between agents improve the outputs of complex tasks? How can you increase the diversity of cooperating agents, and what impact on performance might it have?</li>
<li class="numberedList">Describe examples of reaching consensus across multiple agents’ outputs.</li>
<li class="numberedList">What are the two main ways to organize communication in a multi-agent system with LangGraph?</li>
<li class="numberedList">Explain the differences between stream, astream, and astream_events in LangGraph.</li>
<li class="numberedList">What is a command in LangGraph, and how is it related to handoffs? </li>
<li class="numberedList">Explain the concept of a thread in the LangGraph platform. How is it different from Pythonic threads?</li>
<li class="numberedList">Explain the core idea behind the Tree of Thoughts (ToT) technique. How is ToT related to the decomposition pattern?</li>
<li class="numberedList">Describe the difference between short-term and long-term memory in the context of agentic systems.</li>
</ol>
<h1 class="heading-1" id="_idParaDest-169"><a id="_idTextAnchor325"/><a id="_idTextAnchor326"/>Subscribe to our weekly newsletter</h1>
<p class="normal">Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers, and innovators, at <a href="E_Chapter_6.xhtml">https://packt.link/Q5UyU</a>.</p>
<p class="normal"><img alt="" src="img/Newsletter_QRcode1.jpg"/></p>
</div>
</body></html>