["```py\ndef create_visual_observation_encoder(\n        self,\n        image_input: tf.Tensor,\n        h_size: int,\n        activation: ActivationFunction,\n        num_layers: int,\n        scope: str,\n        reuse: bool,\n    ) -> tf.Tensor:        \n        with tf.variable_scope(scope):\n            conv1 = tf.layers.conv2d(\n                image_input,\n                16,\n                kernel_size=[8, 8],\n                strides=[4, 4],\n                activation=tf.nn.elu,\n                reuse=reuse,\n                name=\"conv_1\",\n            )\n            conv2 = tf.layers.conv2d(\n                conv1,\n                32,\n                kernel_size=[4, 4],\n                strides=[2, 2],\n                activation=tf.nn.elu,\n                reuse=reuse,\n                name=\"conv_2\",\n            )\n            hidden = c_layers.flatten(conv2)\n\n        with tf.variable_scope(scope + \"/\" + \"flat_encoding\"):\n            hidden_flat = self.create_vector_observation_encoder(\n                hidden, h_size, activation, num_layers, scope, reuse\n            )\n        return hidden_flat\n```", "```py\nmlagents-learn config/trainer_config.yaml --run-id=vishall_1 --train\n```", "```py\nVisualHallwayLearning:\n    use_recurrent: true\n    sequence_length: 64\n    num_layers: 1\n    hidden_units: 128\n    memory_size: 256\n    beta: 1.0e-2\n    num_epoch: 3\n    buffer_size: 1024\n    batch_size: 64\n    max_steps: 5.0e5\n    summary_freq: 1000\n    time_horizon: 64\n```", "```py\nidefault:\n    trainer: ppo\n    batch_size: 1024\n    beta: 5.0e-3\n    buffer_size: 10240\n    epsilon: 0.2\n    hidden_units: 128\n    lambd: 0.95\n    learning_rate: 3.0e-4\n    learning_rate_schedule: linear\n    max_steps: 5.0e4\n    memory_size: 256\n    normalize: false\n    num_epoch: 3\n    num_layers: 2\n    time_horizon: 64\n    sequence_length: 64\n    summary_freq: 1000\n    use_recurrent: false\n    vis_encode_type: simple\n    reward_signals:\n        extrinsic:\n            strength: 1.0\n            gamma: 0.99\n```", "```py\nVisualHallwayLearning:\n    use_recurrent: true\n    sequence_length: 64\n    num_layers: 1\n    hidden_units: 128\n    memory_size: 256\n    beta: 1.0e-2\n    num_epoch: 3\n    buffer_size: 1024\n    batch_size: 64\n    max_steps: 5.0e5\n    summary_freq: 1000\n    time_horizon: 64\n    vis_enc_type: nature_cnn --or-- resnet\n```", "```py\ndef create_nature_cnn_visual_observation_encoder(\n        self,\n        image_input: tf.Tensor,\n        h_size: int,\n        activation: ActivationFunction,\n        num_layers: int,\n        scope: str,\n        reuse: bool,\n    ) -> tf.Tensor:        \n        with tf.variable_scope(scope):\n            conv1 = tf.layers.conv2d(\n                image_input,\n                32,\n                kernel_size=[8, 8],\n                strides=[4, 4],\n                activation=tf.nn.elu,\n                reuse=reuse,\n                name=\"conv_1\",\n            )\n            conv2 = tf.layers.conv2d(\n                conv1,\n                64,\n                kernel_size=[4, 4],\n                strides=[2, 2],\n                activation=tf.nn.elu,\n                reuse=reuse,\n                name=\"conv_2\",\n            )\n            conv3 = tf.layers.conv2d(\n                conv2,\n                64,\n                kernel_size=[3, 3],\n                strides=[1, 1],\n                activation=tf.nn.elu,\n                reuse=reuse,\n                name=\"conv_3\",\n            )\n            hidden = c_layers.flatten(conv3)\n\n        with tf.variable_scope(scope + \"/\" + \"flat_encoding\"):\n            hidden_flat = self.create_vector_observation_encoder(\n                hidden, h_size, activation, num_layers, scope, reuse\n            )\n        return hidden_flat\n```", "```py\n def create_resnet_visual_observation_encoder(\n        self,\n        image_input: tf.Tensor,\n        h_size: int,\n        activation: ActivationFunction,\n        num_layers: int,\n        scope: str,\n        reuse: bool,\n    ) -> tf.Tensor:       \n        n_channels = [16, 32, 32] \n        n_blocks = 2 \n        with tf.variable_scope(scope):\n            hidden = image_input\n            for i, ch in enumerate(n_channels):\n                hidden = tf.layers.conv2d(\n                    hidden,\n                    ch,\n                    kernel_size=[3, 3],\n                    strides=[1, 1],\n                    reuse=reuse,\n                    name=\"layer%dconv_1\" % i,\n                )\n                hidden = tf.layers.max_pooling2d(\n                    hidden, pool_size=[3, 3], strides=[2, 2], padding=\"same\"\n                )                \n                for j in range(n_blocks):\n                    block_input = hidden\n                    hidden = tf.nn.relu(hidden)\n                    hidden = tf.layers.conv2d(\n                        hidden,\n                        ch,\n                        kernel_size=[3, 3],\n                        strides=[1, 1],\n                        padding=\"same\",\n                        reuse=reuse,\n                        name=\"layer%d_%d_conv1\" % (i, j),\n                    )\n                    hidden = tf.nn.relu(hidden)\n                    hidden = tf.layers.conv2d(\n                        hidden,\n                        ch,\n                        kernel_size=[3, 3],\n                        strides=[1, 1],\n                        padding=\"same\",\n                        reuse=reuse,\n                        name=\"layer%d_%d_conv2\" % (i, j),\n                    )\n                    hidden = tf.add(block_input, hidden)\n            hidden = tf.nn.relu(hidden)\n            hidden = c_layers.flatten(hidden)\n\n        with tf.variable_scope(scope + \"/\" + \"flat_encoding\"):\n            hidden_flat = self.create_vector_observation_encoder(\n                hidden, h_size, activation, num_layers, scope, reuse\n            )\n        return hidden_flat\n```", "```py\nn_channels = [16, 32, 32] # channel for each stack\nn_blocks = 2 # number of residual blocks\n```", "```py\nfor i, ch in enumerate(n_channels):\n    hidden = tf.layers.conv2d(\n        hidden, \n        ch, \n        kernel_size=[3, 3], \n        strides=[1, 1], \n        reuse=reuse,    \n        name=\"layer%dconv_1\" % i,)\n        hidden = tf.layers.max_pooling2d(\n            hidden, pool_size=[3, 3], strides=[2, 2], padding=\"same\")\n```", "```py\nfor j in range(n_blocks):\n    block_input = hidden\n    hidden = tf.nn.relu(hidden)\n    hidden = tf.layers.conv2d(\n        hidden,\n        ch,\n        kernel_size=[3, 3],\n        strides=[1, 1],\n        padding=\"same\",\n        reuse=reuse,\n        name=\"layer%d_%d_conv1\" % (i, j),)\n    hidden = tf.nn.relu(hidden)\n    hidden = tf.layers.conv2d(\n        hidden,\n        ch,\n        kernel_size=[3, 3],\n        strides=[1, 1],\n        padding=\"same\",\n        reuse=reuse,\n        name=\"layer%d_%d_conv2\" % (i, j),)\n    hidden = tf.add(block_input, hidden)\n```", "```py\nconda create -n obtower python=3.6\nconda activate obstower\n```", "```py\ngit clone git@github.com:Unity-Technologies/obstacle-tower-env.git\ncd obstacle-tower-env\npip install -e .\n```", "```py\nfrom obstacle_tower_env import ObstacleTowerEnv, ObstacleTowerEvaluation\ndef run_episode(env):\n    done = False\n    episode_return = 0.0\n\n    while not done:\n        action = env.action_space.sample()\n        obs, reward, done, info = env.step(action)\n        episode_return += reward\n    return episode_return\n\nif __name__ == '__main__':    \n    eval_seeds = [1001, 1002, 1003, 1004, 1005]    \n    env = ObstacleTowerEnv('./ObstacleTower/obstacletower')    \n    env = ObstacleTowerEvaluation(env, eval_seeds)    \n    while not env.evaluation_complete:\n        episode_rew = run_episode(env)    \n    print(env.results)\n    env.close()\n```", "```py\npip install -e .\n```", "```py\n `OBS_TOWER_PATH` - the path to the obstacle tower binary.\n `OBS_TOWER_RECORDINGS` - the path to a directory where demonstrations are stored.\n `OBS_TOWER_IMAGE_LABELS` - the path to the directory of labeled images.\n```", "```py\ncd obs_tower2/scripts\npython run_classifier.py\n```", "```py\npython run_clone.py\n```", "```py\ncp save_clone.pkl save_prior.pkl\n```", "```py\nimport itertools\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom .util import atomic_save\n\nclass PPO:    \n    def __init__(self, model, epsilon=0.2, gamma=0.99, lam=0.95, lr=1e-4, ent_reg=0.001):\n        self.model = model\n        self.epsilon = epsilon\n        self.gamma = gamma\n        self.lam = lam\n        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n        self.ent_reg = ent_reg\n\n    def outer_loop(self, roller, save_path='save.pkl', **kwargs):        \n        for i in itertools.count():\n            terms, last_terms = self.inner_loop(roller.rollout(), **kwargs)\n            self.print_outer_loop(i, terms, last_terms)\n            atomic_save(self.model.state_dict(), save_path)\n\n    def print_outer_loop(self, i, terms, last_terms):\n        print('step %d: clipped=%f entropy=%f explained=%f' %\n              (i, last_terms['clip_frac'], terms['entropy'], terms['explained']))\n\n    def inner_loop(self, rollout, num_steps=12, batch_size=None):\n        if batch_size is None:\n            batch_size = rollout.num_steps * rollout.batch_size\n        advs = rollout.advantages(self.gamma, self.lam)\n        targets = advs + rollout.value_predictions()[:-1]\n        advs = (advs - np.mean(advs)) / (1e-8 + np.std(advs))\n        actions = rollout.actions()\n        log_probs = rollout.log_probs()\n        firstterms = None\n        lastterms = None\n        for entries in rollout.batches(batch_size, num_steps):\n            def choose(values):\n                return self.model.tensor(np.array([values[t, b] for t, b in entries]))\n            terms = self.terms(choose(rollout.states),\n                               choose(rollout.obses),\n                               choose(advs),\n                               choose(targets),\n                               choose(actions),\n                               choose(log_probs))\n            self.optimizer.zero_grad()\n            terms['loss'].backward()\n            self.optimizer.step()\n            lastterms = {k: v.item() for k, v in terms.items() if k != 'model_outs'}\n            if firstterms is None:\n                firstterms = lastterms\n            del terms\n        return firstterms, lastterms\n\n    def terms(self, states, obses, advs, targets, actions, log_probs):\n        model_outs = self.model(states, obses)\n\n        vf_loss = torch.mean(torch.pow(model_outs['critic'] - targets, 2))\n        variance = torch.var(targets)\n        explained = 1 - vf_loss / variance\n\n        new_log_probs = -F.cross_entropy(model_outs['actor'], actions.long(), reduction='none')\n        ratio = torch.exp(new_log_probs - log_probs)\n        clip_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n        pi_loss = -torch.mean(torch.min(ratio * advs, clip_ratio * advs))\n        clip_frac = torch.mean(torch.gt(ratio * advs, clip_ratio * advs).float())\n\n        all_probs = torch.log_softmax(model_outs['actor'], dim=-1)\n        neg_entropy = torch.mean(torch.sum(torch.exp(all_probs) * all_probs, dim=-1))\n        ent_loss = self.ent_reg * neg_entropy\n\n        return {\n            'explained': explained,\n            'clip_frac': clip_frac,\n            'entropy': -neg_entropy,\n            'vf_loss': vf_loss,\n            'pi_loss': pi_loss,\n            'ent_loss': ent_loss,\n            'loss': vf_loss + pi_loss + ent_loss,\n            'model_outs': model_outs,\n        }\n```", "```py\ndef inner_loop(self, rollout, num_steps=12, batch_size=None):\n```", "```py\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom .ppo import PPO\n\nclass Prierarchy(PPO):  \n    def __init__(self, prior, *args, kl_coeff=0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.prior = prior\n        self.kl_coeff = kl_coeff\n\n    def print_outer_loop(self, i, terms, last_terms):\n        print('step %d: clipped=%f entropy=%f explained=%f kl=%f' %\n              (i, last_terms['clip_frac'], last_terms['entropy'], terms['explained'],\n               terms['kl']))\n\n    def inner_loop(self, rollout, num_steps=12, batch_size=None):\n        if batch_size is None:\n            batch_size = rollout.num_steps * rollout.batch_size\n        prior_rollout = self.prior.run_for_rollout(rollout)\n        prior_logits = prior_rollout.logits()\n        rollout = self.add_rewards(rollout, prior_rollout)\n        advs = rollout.advantages(self.gamma, self.lam)\n        targets = advs + rollout.value_predictions()[:-1]\n        actions = rollout.actions()\n        log_probs = rollout.log_probs()\n        firstterms = None\n        lastterms = None\n        for entries in rollout.batches(batch_size, num_steps):\n            def choose(values):\n                return self.model.tensor(np.array([values[t, b] for t, b in entries]))\n            terms = self.extended_terms(choose(prior_logits),\n                                        choose(rollout.states),\n                                        choose(rollout.obses),\n                                        choose(advs),\n                                        choose(targets),\n                                        choose(actions),\n                                        choose(log_probs))\n            self.optimizer.zero_grad()\n            terms['loss'].backward()\n            self.optimizer.step()\n            lastterms = {k: v.item() for k, v in terms.items() if k != 'model_outs'}\n            if firstterms is None:\n                firstterms = lastterms\n            del terms\n        return firstterms, lastterms\n\n    def extended_terms(self, prior_logits, states, obses, advs, targets, actions, log_probs):\n        super_out = self.terms(states, obses, advs, targets, actions, log_probs)\n        log_prior = F.log_softmax(prior_logits, dim=-1)\n        log_posterior = F.log_softmax(super_out['model_outs']['actor'], dim=-1)\n        kl = torch.mean(torch.sum(torch.exp(log_posterior) * (log_posterior - log_prior), dim=-1))\n        kl_loss = kl * self.ent_reg\n        super_out['kl'] = kl\n        super_out['kl_loss'] = kl_loss\n        super_out['loss'] = super_out['vf_loss'] + super_out['pi_loss'] + kl_loss\n        return super_out\n\n    def add_rewards(self, rollout, prior_rollout):\n        rollout = rollout.copy()\n        rollout.rews = rollout.rews.copy()\n\n        def log_probs(r):\n            return F.log_softmax(torch.from_numpy(np.array([m['actor'] for m in r.model_outs])),\n                                 dim=-1)\n\n        q = log_probs(prior_rollout)\n        p = log_probs(rollout)\n        kls = torch.sum(torch.exp(p) * (p - q), dim=-1).numpy()\n\n        rollout.rews -= kls[:-1] * self.kl_coeff\n\n        return rollout\n```", "```py\ncp save_prior.pkl save.pkl\npython run_tail.py --min 0 --max 1 --path save.pkl\n```", "```py\ncp save_prior.pkl save_tail.pkl\npython run_tail.py --min 10 --max 15 --path save_tail.pkl\n```", "```py\ngit clone --branch stable git@github.com:facebookresearch/habitat-sim.git \ncd habitat-sim\n```", "```py\nconda create -n habitat python=3.6 cmake=3.14.0 \nconda activate habitat \npip install -r requirements.txt\n```", "```py\npython setup.py install\n```", "```py\npython examples/example.py --scene /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\n```", "```py\ngit clone --branch stable git@github.com:facebookresearch/habitat-api.git \ncd habitat-api pip install -r requirements.txt \npython setup.py develop --all\n\n```", "```py\nimport habitat\n\n# Load embodied AI task (PointNav) and a pre-specified virtual robot\nenv = habitat.Env(\n    config=habitat.get_config(\"configs/tasks/pointnav.yaml\")\n)\n\nobservations = env.reset()\n\n# Step through environment with random actions\nwhile not env.episode_over:\n    observations = env.step(env.action_space.sample())\n```", "```py\n# be sure to cd to the habitat_baselines folder\npip install -r requirements.txt \npython setup.py develop --all\n```", "```py\npython -u habitat_baselines/run.py --exp-config habitat_baselines/config/pointnav/ppo_pointnav.yaml --run-type train\n```", "```py\npython -u habitat_baselines/run.py --exp-config habitat_baselines/config/pointnav/ppo_pointnav.yaml --run-type eval\n```"]