<html><head></head><body>
		<div id="_idContainer072">
			<h1 class="chapter-number" id="_idParaDest-123"><a id="_idTextAnchor162"/>7</h1>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor163"/>Useful Frameworks, Libraries, and APIs</h1>
			<p>As you might expect, <strong class="bold">Python</strong> is the most popular programming language for building intelligent AI applications. This is due to its flexibility and ease of use, as well as for its vast number of <strong class="bold">AI and machine learning</strong> (<strong class="bold">ML</strong>) libraries. Python has a specialized library for nearly all the necessary tasks required to build a <strong class="bold">generative AI</strong> (<span class="No-Break"><strong class="bold">GenAI</strong></span><span class="No-Break">) application.</span></p>
			<p>In <a href="B22495_01.xhtml#_idTextAnchor009"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Getting Started with Generative AI</em>, you read about the GenAI stack and the evolution of AI. Like the AI landscape, the Python library and framework space also went through an evolution phase. Earlier, libraries such as pandas, NumPy, and polars were used for data cleanup and transformation work, while PyTorch, TensorFlow, and scikit-learn were used for training ML models. Now, with the rise of the GenAI stack, LLMs, and vector databases, a new type of AI framework <span class="No-Break">has emerged.</span></p>
			<p>These new libraries and frameworks are designed to simplify the creation of new applications powered by LLMs. Since building GenAI applications requires the seamless integration of data from many sources and the use of diverse AI models, these AI frameworks provide built-in functionalities to facilitate acquiring, migrating, and <span class="No-Break">transforming data.</span></p>
			<p>This chapter delves into the world of AI/ML frameworks, exploring their importance and highlighting why Python has emerged as the go-to language for AI/ML development. By the end of this chapter, you’ll be able to understand the most popular frameworks and libraries, as well as how they help you—the developer—build your <span class="No-Break">GenAI application.</span></p>
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">AI/ML frameworks</span></li>
				<li><span class="No-Break">Python libraries</span></li>
				<li>Publicly available APIs and <span class="No-Break">other tools</span></li>
			</ul>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor164"/>Technical requirements</h1>
			<p>To perform the steps shown in this chapter, you will need <span class="No-Break">the following:</span></p>
			<ul>
				<li>Latest major version <span class="No-Break">of Python.</span></li>
				<li>A free tier Atlas cluster running MongoDB version 6.0.11, 7.0.2, <span class="No-Break">or later.</span></li>
				<li>Your current IP address added to your Atlas project <span class="No-Break">access list.</span></li>
				<li>An environment set up to run Python code in an interactive environment, such as Jupyter Notebook or Colab. This chapter uses <span class="No-Break">Jupyter Notebook.</span></li>
			</ul>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor165"/>Python for AI/ML</h1>
			<p>Python has established itself as the go-to programming language in various fields, but most notably in AI, ML, and building applications powered by <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>). Python offers simplicity, readability, and a robust ecosystem of libraries, making it an ideal choice for all kinds of users, whether they are developers, researchers, or even students just getting started with programming. Python has also emerged as the language of choice for building new LLM-powered applications, underscoring Python’s usefulness, popularity, <span class="No-Break">and versatility.</span></p>
			<p>In this section, you will learn some of the reasons that make Python a great choice for building modern <span class="No-Break">AI-powered applications:</span></p>
			<ul>
				<li><strong class="bold">Simplicity and readability</strong>: Python’s syntax is designed to be intuitive and clear, which is one of its core strengths. Python can represent complex algorithms and tasks in a few lines of code that are easily readable <span class="No-Break">and understandable.</span></li>
				<li><strong class="bold">Rich ecosystem of libraries and frameworks</strong>: Python offers an extensive range of libraries and frameworks specifically designed for AI/ML use cases. Libraries such as TensorFlow, PyTorch, and scikit-learn have traditionally been popular for ML tasks. Hugging Face’s Transformers library has also become an indispensable part of the developer workflow for building modern LLM-powered applications. It provides pre-trained models and straightforward APIs to fine-tune models for specific tasks. These libraries not only accelerate development time but also provide cutting-edge solutions to developers across <span class="No-Break">the world.</span></li>
				<li><strong class="bold">Strong community and support</strong>: Python is one of the most popular programming languages in the world, and hence has a huge community. According to the Stack Overflow survey 2023 (<a href="https://survey.stackoverflow.co/2023/"><span class="P---URL">https://survey.stackoverflow.co/2023/</span></a>), it’s the second most popular programming language after JavaScript (excluding HTML/CSS). This strong and large community provides a wealth of resources, including tutorials, discussion forum engagements, and open source projects, which offer a helpful support system for someone working on building <span class="No-Break">modern applications.</span></li>
				<li><strong class="bold">Integration with other technologies</strong>: Python’s ability to integrate seamlessly with other technologies and programming languages makes it a great choice for AI/ML tasks and building LLM-powered applications. For example, Python can easily interface with programming languages such as C/C++ for performance-critical tasks. It also interfaces well with languages such as Java and C#. This flexibility of Python is helpful for deploying LLM-powered applications in diverse environments, ensuring that Python can be part of large <span class="No-Break">heterogeneous systems.</span></li>
				<li><strong class="bold">Rapid prototyping and experimentation</strong>: Building a sophisticated AI/ML-powered application requires many iterations of tests, experiments, and fine-tuning. Python allows developers to quickly build prototypes in a few lines of code. Easy testing and debugging also help to prototype a quick solution. Python’s interactive environments, such as Jupyter Notebook, provide an excellent platform for this purpose. With Python, developers building LLM-powered applications can quickly test hypotheses, visualize data, and debug code in an <span class="No-Break">interactive manner.</span></li>
			</ul>
			<p>Python combines speed, simplicity, specialized libraries and frameworks, and strong community support with easy integration with other languages and technologies, all of which make it an excellent choice for building modern <span class="No-Break">LLM-powered applications.</span></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor166"/>AI/ML frameworks</h1>
			<p><strong class="bold">AI/ML frameworks</strong> are essential tools that streamline the development and deployment of ML models, providing pre-built algorithms, optimized performance, and scalable solutions. They enable developers to focus on refining their models and GenAI applications rather than getting bogged down by low-level implementations. Using frameworks ensures efficiency, adaptability, and the ability to harness cutting-edge AI advancements. Developers should be interested in these frameworks as they also reduce development time and enhance the potential for breakthroughs <span class="No-Break">in GenAI.</span></p>
			<p>MongoDB has integrations with many AI/ML frameworks that may be familiar to developers, such as LangChain, LlamaIndex, Haystack, Microsoft Semantic Kernel, DocArray, <span class="No-Break">and Flowise.</span></p>
			<p>In this section, you will learn about <strong class="bold">LangChain</strong>, one of the most popular GenAI frameworks. Although it is very popular, it is certainly not the only popular framework. If you are interested in other frameworks, you can check out the documentation linked in the <em class="italic">Appendix: Further Reading</em> chapter at the end of this book or see the latest list of supported AI/ML frameworks for Python <span class="No-Break">at </span><a href="https://www.mongodb.com/docs/languages/python/"><span class="No-Break"><span class="P---URL">https://www.mongodb.com/docs/languages/python/</span></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor167"/>LangChain</h2>
			<p>LangChain is a framework for developing applications powered by LLMs. LangChain simplifies every stage of the LLM application lifecycle. It enables building applications that connect external sources of data and computation to LLMs. The basic LLM chain relies solely on the information provided in the prompt template to generate a response, and the concept of a <em class="italic">LangChain</em> allows you to extend these chains for <span class="No-Break">advanced processing.</span></p>
			<p>In this section, you will learn how to use LangChain to perform semantic search on your data and build a <strong class="bold">retrieval-augmented generation (RAG) </strong>implementation. Before you begin, make sure you have all the necessary tools installed and set up on your computer, as listed in the <em class="italic">Technical requirements</em> section of <span class="No-Break">this chapter.</span></p>
			<h3>Getting started with LangChain</h3>
			<p>Perform the following steps to set up your environment <span class="No-Break">for LangChain:</span></p>
			<ol>
				<li>Start by installing the <span class="No-Break">necessary dependencies:</span><pre class="source-code">
pip3 install --quiet --upgrade langchain==0.1.22 langchain-mongodb==0.1.8 langchain_community==0.2.12 langchain-openai==0.1.21 pymongo==4.5.1 polars==1.5.0 pypdf==3.15.0</pre></li>				<li>Run the following code to import the <span class="No-Break">required packages:</span><pre class="source-code">
import getpass, os, pymongo, pprint
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pymongo import MongoClient</pre></li>				<li>After you have imported the necessary packages, make sure the environment variables are set properly. You have two important secrets to store as environment variables: your <strong class="bold">OpenAI API key</strong> and <strong class="bold">MongoDB Atlas </strong><span class="No-Break"><strong class="bold">connection string</strong></span><span class="No-Break">.</span><p class="list-inset">Run the following command to store your OpenAI API key as an <span class="No-Break">environment variable:</span></p><pre class="source-code">
os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")</pre><p class="list-inset">Make sure your connection string for the drivers is in the following format, which has both <strong class="source-inline">username</strong> and <strong class="source-inline">password</strong> included in the <span class="No-Break">connection string:</span></p><pre class="source-code">mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;clusterName&gt;.&lt;hostname&gt;.mongodb.net</pre><p class="list-inset">Run the following command to store your MongoDB Atlas connection string as an <span class="No-Break">environment variable:</span></p><pre class="source-code">ATLAS_CONNECTION_STRING = getpass.getpass("MongoDB Atlas SRV Connection String:")</pre><p class="list-inset">You are now ready to connect to the MongoDB <span class="No-Break">Atlas cluster.</span></p></li>				<li>Next, you’ll instantiate the <strong class="source-inline">MongoClient</strong> and pass your connection string to establish communications with your MongoDB Atlas database. Run the following code to establish <span class="No-Break">the connection:</span><pre class="source-code">
# Connect to your Atlas cluster
client = MongoClient(ATLAS_CONNECTION_STRING)</pre></li>				<li>Next, you’ll specify the name of the database and the collection you want to create. In this example, you’ll create a database named <strong class="source-inline">langchain_db</strong> and a collection called <strong class="source-inline">test</strong>. You’ll also define the name of the vector search index to create and use with the <span class="No-Break">following code:</span><pre class="source-code">
# Define collection and index name
db_name = "langchain_db"
collection_name = "test"
atlas_collection = client[db_name][collection_name]
vector_search_index = "vector_index"</pre></li>			</ol>
			<p>With these steps, you’ve set up the basics of connectivity. Now that you have the bare bones of your database, you’ll want to define what your <span class="No-Break">application does.</span></p>
			<p>In this case, you will do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Fetch a publicly accessible <span class="No-Break">PDF document.</span></li>
				<li>Split it into smaller chunks of information for easy consumption by your <span class="No-Break">GenAI application.</span></li>
				<li>Upload the data into the <span class="No-Break">MongoDB database.</span></li>
			</ol>
			<p>This functionality is not something you have to build from scratch. Instead, you’ll use the free, open source library integration provided by LangChain called <strong class="source-inline">PyPDFLoader</strong>, which you imported in <em class="italic">Step 2</em> earlier in <span class="No-Break">this section.</span></p>
			<h3>Fetching and splitting public PDF documents</h3>
			<p>Using <strong class="source-inline">PyPDFLoader</strong> to fetch publicly available PDFs is quite simple. In the following code, you will fetch a publicly accessible PDF document and split it into smaller chunks that you can later upload into your <span class="No-Break">MongoDB database:</span></p>
			<pre class="source-code">
# Load the PDF
loader = PyPDFLoader("https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HkJP")
data = loader.load()
# Split PDF into documents
text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
docs = text_splitter.split_documents(data)
# Print the first document
docs[0]</pre>			<p>You will then receive the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
Document(metadata={'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 0}, page_content='Mong oDB Atlas Best Practices January 20 19A MongoD B White P aper')</pre>			<p>With this code, you first instantiated <strong class="source-inline">PyPDFLoader</strong> and then passed it the URL to the publicly accessible PDF file: <a href="https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HkJP"><span class="P---URL">https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HkJP</span></a>. Next, you loaded the fetched PDF file into the <span class="No-Break"><strong class="source-inline">data</strong></span><span class="No-Break"> variable.</span></p>
			<p>After that, you split the PDF file’s text into smaller chunks. For this example, you set the chunk size to 200 characters and allowed an overlap of 20 characters between chunks. The overlap maintains the context between chunks. Note that this number is not arbitrary, and there are many opinions about what your chunking strategy should be. Some of those resources are discussed in the <em class="italic">Appendix: Further Reading</em> chapter of <span class="No-Break">this book.</span></p>
			<p>You stored the split chunks in the <strong class="source-inline">docs</strong> variable and printed the first chunk of the split document. This indicates that your output request via the <strong class="source-inline">print</strong> command was successful, and you can easily confirm whether the information is correct for <span class="No-Break">this entry.</span></p>
			<h3>Creating the vector store</h3>
			<p>After you have split your documents into chunks, you will instantiate the vector store with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# Create the vector store
vector_store = MongoDBAtlasVectorSearch.from_documents(
    documents = docs,
    embedding = OpenAIEmbeddings(disallowed_special=()),
    collection = atlas_collection,
    index_name = vector_search_index
)</pre>			<p>In the preceding code, you created a vector store named <strong class="source-inline">vector_store</strong> using the <strong class="source-inline">MongoDBAtlasVectorSearch.from_documents</strong> method and specified <span class="No-Break">various parameters:</span></p>
			<ul>
				<li><strong class="source-inline">documents = docs</strong>: The name of the document that you want to store in your <span class="No-Break">vector database</span></li>
				<li><strong class="source-inline">embedding = OpenAIEmbeddings(disallowed_special=())</strong>: The class that generates vector embeddings for the documents using OpenAI’s <span class="No-Break">embedding model</span></li>
				<li><strong class="source-inline">collection = atlas_collection</strong>: The Atlas collection where documents will <span class="No-Break">be stored</span></li>
				<li><strong class="source-inline">index_name = vector_search_index</strong>: The name of the index to use for querying the <span class="No-Break">vector store</span></li>
			</ul>
			<p>You’ll also need to create your <strong class="bold">Atlas Vector Search index</strong> in the MongoDB database. For explicit instructions on how this is done, see <a href="B22495_08.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>,<em class="italic"> Implementing Vector Search in AI Applications</em>. This must be completed before you can successfully run the previous code. When you are creating a Vector Search index, use the following <span class="No-Break">index definition:</span></p>
			<pre class="source-code">
{
   "fields":[
      {
         "type": "vector",
         "path": "embedding",
         "numDimensions": 1536,
         "similarity": "cosine"
      },
      {
         "type": "filter",
         "path": "page"
      }
   ]
}</pre>			<p>This index defines <span class="No-Break">two fields:</span></p>
			<ul>
				<li><strong class="bold">Embedding field</strong>: A vector type field for storing embeddings created using OpenAI’s <strong class="source-inline">text-embedding-ada-002</strong> model. It has 1,536 dimensions and uses cosine similarity to measure similarity. You may also want to consider other newer models from OpenAI, <strong class="source-inline">text-embedding-3-small</strong> and <strong class="source-inline">text-embedding-3-large</strong>, which are optimized for different use cases and therefore have a different number of dimensions. See <a href="https://platform.openai.com/docs/guides/embeddings"><span class="P---URL">https://platform.openai.com/docs/guides/embeddings</span></a> for more details as well as <span class="No-Break">current options.</span></li>
				<li><strong class="bold">Page field</strong>: A filter type field used for pre-filtering data based on the page number in <span class="No-Break">the PDF.</span></li>
			</ul>
			<p>Now, you can run your code successfully, fetch a publicly available PDF, chunk it into smaller portions of data, and store them in a MongoDB Atlas database. With these steps accomplished, you can conduct additional tasks, such as running queries to perform semantic search on your data. You can learn about basic semantic search in <a href="B22495_08.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Implementing Vector Search in AI Applications</em>, and <a href="B22495_10.xhtml#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Refining the Semantic Data Model to </em><span class="No-Break"><em class="italic">Improve Accuracy</em></span><span class="No-Break">.</span></p>
			<p>For more information on this topic, you can also consult the official documentation from LangChain, available <span class="No-Break">at </span><a href="https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/#pre-filtering-with-similarity-search"><span class="No-Break"><span class="P---URL">https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/#pre-filtering-with-similarity-search</span></span></a><span class="No-Break">.</span></p>
			<p>Next, let’s cover some specific LangChain functionalities that you will find most useful when building <span class="No-Break">GenAI applications.</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor168"/>LangChain semantic search with score</h2>
			<p>LangChain provides some particularly helpful methods to perform semantic search on your data and return a <strong class="bold">score</strong>. This score refers to the measure of relevance between the query and the matching documents based on their semantic content. You can use this score when you want to return more than one result to your users and also limit the number of results. For example, this score can prove useful in returning the top three most relevant pieces of content about <span class="No-Break">a topic.</span></p>
			<p>The method that you will use here <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">similarity_search_with_score</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
query = "MongoDB Atlas security"
results = vector_store.similarity_search_with_score(
   query = query, k = 3
)
pprint.pprint(results)</pre>			<p>You pass the query to the <strong class="source-inline">similarity_search_with_score</strong> function and specify the <strong class="source-inline">k</strong> parameter as <strong class="source-inline">3</strong> to limit the number of documents to return to 3. Then, you can print <span class="No-Break">the output:</span></p>
			<pre class="source-code">
[(Document (page_content='To ensure a secure system right out of the box, \nauthentication and IP Address whitelist ing are\nautomatically enabled. \nReview the security section of the MongoD B Atlas', metadata={'_id': {'Soid": "667 20a81b6cb1d87043c0171'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 17}),
0.9350903034210205),
(Document(page_content='MongoD B Atlas team are also monitoring the underlying\ninfrastructure, ensuring that it i s always in a healthy state. \nApplication Logs And Database L ogs', metadata={'_id': {'soid': '66720a81b6cb1d87043 c013c'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 15}),
0.9336163997650146),
(Document(page_content="MongoD B.\nMongoD B Atlas incorporates best practices to help keep\nmanaged databases heal thy and optimized. T hey ensure\noperational continuity by converting complex manual tasks', metadata={'_id': {'so id: '66728a81b6cb1d87043c011f'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'p age': 13)),
0.9317773580551147)]</pre>			<p>As you can see in the output, three documents are returned that have the highest relevance score. Each returned document also has a relevance score attached to it that ranges between 0 <span class="No-Break">and 1.</span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor169"/>Semantic search with pre-filtering</h2>
			<p>MongoDB allows you to pre-filter your data using a match expression to narrow down the search space before performing a more computationally intensive vector search. This offers several benefits to developers, such as increased performance, improved accuracy, and enhanced query relevancy. When pre-filtering, remember to index any metadata fields by which you want to filter during <span class="No-Break">index creation.</span></p>
			<p>Here is a code snippet that shows how you can perform semantic search <span class="No-Break">with pre-filtering:</span></p>
			<pre class="source-code">
query = "MongoDB Atlas security"
results = vector_store.similarity_search_with_score(
   query = query,
   k = 3,
   pre_filter = { "page": { "$eq": 17 } }
)
pprint.pprint(results)</pre>			<p>In this code example, you have the same query string for which you performed a plain semantic search earlier. The <strong class="source-inline">k</strong> value is set to <strong class="source-inline">3</strong> so that it only returns the top three matching documents. You have also provided a <strong class="source-inline">pre_filter</strong> query, which is basically an MQL expression that uses the <strong class="source-inline">$eq</strong> operator to specify that MongoDB should only return content and chunked information that is on page <strong class="source-inline">17</strong> of the original <span class="No-Break">PDF document.</span></p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor170"/>Implementing a basic RAG solution with LangChain</h2>
			<p>LangChain’s functionalities are not only limited to performing semantic search queries on your data stored in vector databases. It also allows you to build powerful GenAI applications. With the following code snippet, you will learn an easy way to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Set up a MongoDB Atlas Vector Search retriever for <span class="No-Break">similarity-based search.</span></li>
				<li>Return the 10 most <span class="No-Break">relevant documents.</span></li>
				<li>Utilize a custom RAG prompt with an LLM to answer questions based on the <span class="No-Break">retrieved documents:</span></li>
			</ul>
			<pre class="source-code">
# Instantiate Atlas Vector Search as a retriever
retriever = vector_store.as_retriever(
   search_type = "similarity",
   search_kwargs = { "k": 3 }
)
# Define a prompt template
template = """
Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}
Question: {question}
"""
custom_rag_prompt = PromptTemplate.from_template(template)
llm = ChatOpenAI()
def format_docs(docs):
   return "\n\n".join(doc.page_content for doc in docs)
# Construct a chain to answer questions on your data
rag_chain = (
   { "context": retriever | format_docs, "question": RunnablePassthrough()}
   | custom_rag_prompt
   | llm
   | StrOutputParser()
)
# Prompt the chain
question = "How can I secure my MongoDB Atlas cluster?"
answer = rag_chain.invoke(question)
print(«Question: « + question)
print(«Answer: « + answer)
# Return source documents
documents = retriever.get_relevant_documents(question)
print(«\nSource documents:»)
pprint.pprint(documents)</pre>			<p>The preceding code instantiates Atlas Vector Search as a <strong class="bold">retriever</strong> to query for similar documents in the vector database. In LangChain, a retriever is an interface that returns documents given an unstructured query. Retrievers accept a string query as input and return a list of documents as output. Note that you are setting the value of <strong class="source-inline">k</strong> as <strong class="source-inline">3</strong> to search for only the three most <span class="No-Break">relevant documents.</span></p>
			<p>In the preceding code, notice the line that says <strong class="source-inline">custom_rag_prompt = PromptTemplate.from_template(template)</strong>. It refers to prompt templates, which are detailed in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor171"/>LangChain prompt templates and chains</h2>
			<p><strong class="bold">Prompt templates</strong> in LangChain are predefined recipes for generating prompts for language models. Prompt templates may contain various elements, such as instructions, few-shot examples, and specific contexts and questions that are appropriate for a given task. In this case, you have added some instructions and are passing <strong class="source-inline">context</strong> as an input variable and the original query for <span class="No-Break">the LLM.</span></p>
			<p>Let’s set up a <strong class="bold">chain</strong>, a key feature of LangChain that specifies three <span class="No-Break">main components:</span></p>
			<ul>
				<li><strong class="bold">Retriever</strong>: You will use MongoDB Atlas Vector Search to find relevant documents that provide context for the <span class="No-Break">language model</span></li>
				<li><strong class="bold">Prompt template</strong>: This is the template you created earlier to format the query and the <span class="No-Break">contextual information</span></li>
				<li><strong class="bold">LLM</strong>: You will use the OpenAI chat model to generate responses based on the <span class="No-Break">provided context</span></li>
			</ul>
			<p>You will use this chain to process a sample input query about MongoDB Atlas Security recommendations, format the query, retrieve the results of the query, and then return a response to the user along with the documents used as context. Due to LLM variability, you will likely never receive the exact same response twice, but here is an example showing the <span class="No-Break">potential output:</span></p>
			<pre class="source-code">
Question: How can I secure my MongoDB Atlas cluster?
Answer: To secure your MongoDB Atlas cluster, you can enable authentication and IP Address whitelisting, review the security section of the MongoDB Atlas documentation, and utilize encryption of data at rest with encrypted storage volumes. Additionally, you can set up global clusters with a few clicks in the MongoDB Atlas UI, ensure operational continuity by converting complex manual tasks, and consider setting up a larger number of replica nodes for increased protection against database downtime.
Source documents:
[Document (page_content='To ensure a secure system right out of the box, \nauthentication and IP Address whitelisti ng are\nautomatically enabled.\nReview the security section of the MongoD B Atlas', metadata={'_id': {'$oid': '6672
@a81b6cb1d87043c0171'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 17}),
Document(page_content='MongoD B Atlas team are also monitoring the underlying\ninfrastructure, ensuring that it is always in a healthy state. \nApplication L ogs And Database L ogs', metadata('id': ('soid': '66728a81b6cb1d87043c0 13c'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 15}),
Document(page_content='All the user needs to do in order for MongoD B Atlas to\nautomatically deploy the cluster i s to select a handful of\noptions: \n Instance size\n•Storage size (optional) \n Storage speed (optional)', metadata= {"_id": "soid: '66728a81b6cb1d87043c012a'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/ RE4HKJP', 'page': 14)),</pre>			<p>This output both answers the user’s inquiry and provides the source information, increasing not only user trust but also the ability of the user to follow up and get more details as <span class="No-Break">they require.</span></p>
			<p>This brief overview of the LangChain framework has tried to convince you of this framework’s utility and potential and give you a preview of its capabilities to save you valuable time when crafting your <span class="No-Break">GenAI application.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor172"/>Key Python libraries</h1>
			<p>In addition to AI/ML frameworks, there are also many Python libraries that will make the experience of building your GenAI application easier. Whether you require assistance with data cleansing, formatting, or transformation, there are likely half a dozen potential Python libraries to solve every problem. The following subsections list some favorites and explain how they can assist you during your <span class="No-Break">GenAI journey.</span></p>
			<p>For this book, you can broadly divide these libraries into <span class="No-Break">three categories:</span></p>
			<ul>
				<li><strong class="bold">General-purpose scientific libraries</strong> such as pandas, NumPy, <span class="No-Break">and scikit-learn</span></li>
				<li><strong class="bold">MongoDB-specific libraries</strong> such <span class="No-Break">as PyMongoArrow</span></li>
				<li><strong class="bold">Deep learning frameworks</strong> such as PyTorch <span class="No-Break">and TensorFlow</span></li>
			</ul>
			<p>The rest of this section covers one relevant and popular library from each of <span class="No-Break">these categories</span></p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor173"/>pandas</h2>
			<p>The pandas library is a powerful and flexible open source data manipulation and analysis library for Python. It provides data structures such as DataFrames and Series, which are designed to handle structured data intuitively and efficiently. When working with tabular data stored in spreadsheets or databases, pandas is a great tool for data analysis. With pandas, you can perform a wide range of operations, including cleaning, transforming, and <span class="No-Break">aggregating data.</span></p>
			<p>Among many other noticeably out-of-the-box functionalities, pandas also offers great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data. In addition to providing a wide range of methods to work with numerical data, pandas gives great support for working with <span class="No-Break">text-based data.</span></p>
			<p>Here is a short example of how to work with the pandas library. In the following example, you will create a pandas DataFrame from a Python dictionary. Then, you will print the entire DataFrame. Next, you will select a specific column, which is <strong class="source-inline">Age</strong>, and print it. Then, you will filter data by row label or by the specific position of <span class="No-Break">a row.</span></p>
			<p>The next line shows how you can filter data using Boolean masking in pandas. Here, you will print out the <span class="No-Break">DataFrame format:</span></p>
			<pre class="source-code">
pip3 install pandas==1.5.3
import pandas as pd
# Create a DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],
    'Age': [24, 27, 22, 32, 29],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']
}
df = pd.DataFrame(data)
# Display the DataFrame
print("DataFrame:")
print(df)</pre>			<p>Your output should be in the format of a pandas DataFrame, similar to <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer064">
					<img alt="" role="presentation" src="image/B22495_07_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1: DataFrame output from pandas</p>
			<p>You can then manipulate this data in various ways, each time outputting the results as you see fit, but always formatted as a pandas DataFrame. To print only the ages of the users, you would use the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# Select a column
print("\nAges:")
print(df['Age'])</pre>			<p>You’ll get the output shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer065">
					<img alt="" role="presentation" src="image/B22495_07_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2: DataFrame output of ages</p>
			<p>You can also filter the output. Here, you will filter data to show only those people who are older than 25, and then present the results as <span class="No-Break">a DataFrame:</span></p>
			<pre class="source-code">
# Filter data
print("\nPeople older than 25:")
print(df[df['Age'] &gt; 25])</pre>			<p>This code will filter the data and then output the results in DataFrame format, as in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer066">
					<img alt="" role="presentation" src="image/B22495_07_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3: Filtered DataFrame output</p>
			<p>You can also perform calculations with the pandas library in a straightforward way. To calculate the average age, for instance, you would use code such <span class="No-Break">as this:</span></p>
			<pre class="source-code">
# Calculate average age
average_age = df['Age'].mean()
print("\nAverage Age:")
print(average_age)</pre>			<p>And your output would look like <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer067">
					<img alt="" role="presentation" src="image/B22495_07_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4: Calculated field output</p>
			<p>As you can see, data manipulation in pandas is fairly easy, and the outputs are immediately readable and well-formatted for further analysis. The intuitive syntax and powerful functions of pandas make it an essential tool for Python developers, enabling them to handle large datasets with ease and precision. For those building GenAI applications, pandas streamlines the data preprocessing steps, ensuring that data is clean, structured, and ready for model training. Additionally, its robust integration with other Python libraries enhances its utility, making complex data analysis and visualization straightforward <span class="No-Break">and efficient.</span></p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor174"/>PyMongoArrow</h2>
			<p><strong class="bold">PyMongoArrow</strong> is a Python library built on top of the official MongoDB Python driver, <strong class="bold">PyMongo</strong>, which allows you to move data out of the MongoDB database into some of the most popular Python libraries, such as pandas, NumPy, PyArrow, and polars, and <span class="No-Break">vice versa.</span></p>
			<p>PyMongoArrow simplifies loading data from MongoDB into other supported data formats. The example covered below demonstrates how you can work with MongoDB, PyMongoArrow, and libraries such as pandas and NumPy. You may find this useful in the context of GenAI applications in the <span class="No-Break">following situations:</span></p>
			<ul>
				<li>When you require data in a specific format for summarization and analysis (CSV, DataFrame, NumPy array, Parquet file, etc.) <span class="No-Break">from MongoDB</span></li>
				<li>If you need to merge data of various types for calculations or transformations that are then used for <span class="No-Break">GenAI analysis</span></li>
			</ul>
			<p>As an example, if you have inbound financial data from <em class="italic">Application A</em>, inbound sales data from <em class="italic">Application B</em>, PDF files from <em class="italic">Team 1</em>, and <strong class="source-inline">.txt</strong> files from <em class="italic">Team 2</em>, and you’d like your GenAI application to summarize annual data from all these different places, you will likely get more accurate results if all types of data are in the same format. This will require some upfront programmatic effort, and PyMongoArrow simplifies transforming MongoDB JSON into other data types as well as ingesting those other data types and converting them <span class="No-Break">into JSON.</span></p>
			<p>Follow these steps to complete this example <span class="No-Break">with PyMongoArrow:</span></p>
			<ol>
				<li>Start by installing and importing the latest version <span class="No-Break">of PyMongoArrow:</span><pre class="source-code">
pip3 install PyMongoArrow
import pymongoarrow as pa</pre></li>				<li>Now, make sure you have your Atlas cluster connection <span class="No-Break">string handy:</span><pre class="source-code">
import getpass, os, pymongo, pprint</pre></li>				<li>Next, you will extend the PyMongo driver via the <strong class="source-inline">pymongoarrow.monkey</strong> module. This allows you to add the PyMongoArrow functionality directly to MongoDB collections in Atlas. By calling <strong class="source-inline">patch_all()</strong> from <strong class="source-inline">pymongoarrow.monkey</strong>, new collection instances will include PyMongoArrow APIs, such as <strong class="source-inline">pymongoarrow.api.find_pandas_all()</strong>. This is useful because you can now easily export your data from MongoDB to various formats such <span class="No-Break">as pandas.</span><pre class="source-code">
from pymongoarrow.monkey import patch_all
patch_all()</pre></li>				<li>Add some test data to <span class="No-Break">your collection:</span><pre class="source-code">
from datetime import datetime
from pymongo import MongoClient
client = MongoClient(ATLAS_CONNECTION_STRING)
client.db.data.insert_many([
  {'_id': 1, 'amount': 21, 'last_updated': datetime(2020, 12, 10, 1, 3, 1), 'account': {'name': 'Customer1', 'account_number': 1}, 'txns': ['A']},
  {'_id': 2, 'amount': 16, 'last_updated': datetime(2020, 7, 23, 6, 7, 11), 'account': {'name': 'Customer2', 'account_number': 2}, 'txns': ['A', 'B']},
  {'_id': 3, 'amount': 3,  'last_updated': datetime(2021, 3, 10, 18, 43, 9), 'account': {'name': 'Customer3', 'account_number': 3}, 'txns': ['A', 'B', 'C']},
  {'_id': 4, 'amount': 0,  'last_updated': datetime(2021, 2, 25, 3, 50, 31), 'account': {'name': 'Customer4', 'account_number': 4}, 'txns': ['A', 'B', 'C', 'D']}])</pre></li>				<li>PyMongoArrow uses a <strong class="bold">data schema</strong> to convert query results into tabular form. If no schema is provided, it infers one from the data. You can define a schema by creating a <strong class="source-inline">schema</strong> object and mapping field names <span class="No-Break">to type-specifiers:</span><pre class="source-code">
from pymongoarrow.api import Schema
schema = Schema({'_id': int, 'amount': float, 'last_updated': datetime})</pre><p class="list-inset">MongoDB’s key feature is its ability to represent nested data using embedded documents, along with its support for lists and nested lists. PyMongoArrow fully supports these features out of the box, providing first-class functionality for handling embedded documents, lists, and nested <span class="No-Break">lists seamlessly.</span></p></li>				<li>Let’s perform some <strong class="source-inline">find</strong> operations on the data. The following code demonstrates querying a MongoDB collection called <strong class="source-inline">data</strong> for documents where the <strong class="source-inline">amount</strong> field is greater than <strong class="source-inline">0</strong>, using PyMongoArrow to convert the results into different data formats. A predefined schema is used for the conversion, but it’s optional. If you omit the schema, PyMongoArrow tries to automatically apply a schema based on the data contained in the <span class="No-Break">first batch:</span><pre class="source-code">
df = client.db.data.find_pandas_all({'amount': {'$gt': 0}}, schema=schema)
arrow_table = client.db.data.find_arrow_all({'amount': {'$gt': 0}}, schema=schema)
df = client.db.data.find_polars_all({'amount': {'$gt': 0}}, schema=schema)
ndarrays = client.db.data.find_numpy_all({'amount': {'$gt': 0}}, schema=schema)</pre><p class="list-inset">The first line of code converts the query results into a pandas DataFrame. The second line of code converts the query results set into an arrow table. The third line converts the query results set into a polars DataFrame, and finally, the fourth line converts the query result set into a <span class="No-Break">NumPy array.</span></p></li>			</ol>
			<p>You are not limited to performing <strong class="source-inline">find</strong> operations to convert the query result set into other supported data formats. PyMongoArrow also allows you to use MongoDB’s powerful aggregation pipeline to perform complex queries on your data to filter out the needed data before exporting it to other <span class="No-Break">data formats.</span></p>
			<p>For example, the following code performs an aggregation query on the data collection in a MongoDB database, grouping all documents and calculating the total sum of the <span class="No-Break"><strong class="source-inline">amount</strong></span><span class="No-Break"> field:</span></p>
			<pre class="source-code">
df = client.db.data.aggregate_pandas_all([{'$group': {'_id': None, 'total_amount': { '$sum': '$amount' }}}])</pre>			<p>The result of this code is converted into a pandas DataFrame that would consist of a <span class="No-Break">total sum.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor175"/>PyTorch</h2>
			<p>Now that you have learned a little bit about pandas and NumPy, it’s important you also have some knowledge of another popular Python ML <span class="No-Break">library, PyTorch.</span></p>
			<p>PyTorch, developed by Meta’s AI Research lab, is an open source deep learning framework known for its flexibility and ease of use. It is widely appreciated for its dynamic computation graph, which allows intuitive coding and immediate execution of code. This feature is particularly useful for researchers and developers who need to experiment and <span class="No-Break">iterate quickly.</span></p>
			<p>In the context of building a GenAI application, PyTorch serves as a powerful tool for <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Model training and development</strong>: PyTorch is utilized for developing and training the core generative models, such as <strong class="bold">generative pre-trained transformer</strong> (<strong class="bold">GPT</strong>) variants, which form the backbone of the <span class="No-Break">GenAI application.</span></li>
				<li><strong class="bold">Flexibility and real-time experimentation</strong>: The dynamic computation graph in PyTorch allows on-the-fly modifications and real-time experimentation, which are crucial for fine-tuning generative models to produce <span class="No-Break">high-quality output.</span></li>
			</ul>
			<p>Developers who are adapting pre-trained language models to their specific requirements or developing their own custom model for unique tasks may be interested in using this library, along with some of the APIs discussed in the <span class="No-Break">following section.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor176"/>AI/ML APIs</h1>
			<p>When developing GenAI applications, developers have access to a variety of APIs that can significantly enhance the capabilities and efficiency of their projects. As these APIs are widely used, they offer performance, stability, and consistency across thousands of projects, ensuring that developers don’t need to reinvent the wheel. Here are just some of the functionalities that these <span class="No-Break">APIs offer:</span></p>
			<ul>
				<li><strong class="bold">Text generation and processing</strong>: APIs such as <strong class="bold">OpenAI</strong>, <strong class="bold">Hugging Face</strong>, and <strong class="bold">Google Gemini API</strong> enable developers to generate coherent and contextually appropriate text, which is crucial for applications such as content creation, dialogue systems, and <span class="No-Break">virtual assistants.</span></li>
				<li><strong class="bold">Translation capabilities</strong>: The <strong class="bold">Google Cloud Translation API</strong>, <strong class="bold">Azure AI Translator</strong>, and <strong class="bold">Amazon Translate API</strong> provide robust translation capabilities, making GenAI applications multilingual and <span class="No-Break">globally accessible.</span></li>
				<li><strong class="bold">Speech synthesis and recognition</strong>: Services such as <strong class="bold">Google Text-to-Speech</strong>, <strong class="bold">Amazon Polly</strong>, and <strong class="bold">IBM Watson Text-to-Speech</strong> convert generated text into natural-sounding speech, enhancing user interaction <span class="No-Break">and accessibility.</span></li>
				<li><strong class="bold">Image and video processing</strong>: APIs from <strong class="bold">Clarifai</strong> and <strong class="bold">DeepAI</strong> allow GenAI applications to create, modify, and analyze visual content, enabling tasks such as image generation from text and <span class="No-Break">object recognition.</span></li>
			</ul>
			<p>These APIs provide a range of capabilities that, when combined, can significantly accelerate the development and enhance the functionality of GenAI applications. Next, you’re going to dig deeper into two of these APIs, the OpenAI API and the Hugging Face <span class="No-Break">Transformers APIs.</span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor177"/>OpenAI API</h2>
			<p>As you may recall from <a href="B22495_03.xhtml#_idTextAnchor041"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Large Language Models</em>, OpenAI provides a foundational model trained on a broad spectrum of data. It offers this model via an API, which allows you to harness the power of advanced ML models without needing to manage the underlying infrastructure. The computational and financial costs of retraining or hosting a custom LLM for an organization or domain-specific information are very high, so most developers will utilize someone else’s LLM to provide GenAI capabilities to <span class="No-Break">their applications.</span></p>
			<p>Although each API has its own strengths and weaknesses, the OpenAI API is currently the most widely used. It provides a simple interface for developers to create an intelligence layer in their applications. It is powered by OpenAI’s state-of-the-art models and cutting-edge <strong class="bold">natural language processing </strong>(<strong class="bold">NLP</strong>) capabilities, enabling applications to perform tasks such as text generation, summarization, translation, and conversation. The API is designed to be flexible and scalable, making it suitable for a wide range of use cases, from chatbots to content creation tools. It is also well documented, with a large community, and there are many tutorials available for seemingly every <span class="No-Break">use case.</span></p>
			<p>The OpenAI API is already somewhat of an industry standard, and many GenAI tools and technologies have support and seamless integrations with it. If you’d like to avoid a lot of unnecessary effort and costs, your best bet is to work with the <span class="No-Break">OpenAI API.</span></p>
			<p>Let’s get started with the OpenAI API in the <span class="No-Break">following example:</span></p>
			<ol>
				<li>To get started, you’ll need to install <strong class="source-inline">openai</strong> from the terminal or <span class="No-Break">command line:</span><pre class="source-code">
pip3 install --upgrade openai==1.41.0</pre></li>				<li>Include your API key from OpenAI in the environment <span class="No-Break">variable file:</span><pre class="source-code">
export OPENAI_API_KEY='your-api-key-here'</pre></li>				<li>Send your first API test request to the OpenAI API using the Python library. To do this, create a file named <strong class="source-inline">openai-test.py</strong> using the terminal or an IDE. Then, inside the file, copy and paste one of the <span class="No-Break">following examples:</span><pre class="source-code">
from openai import OpenAI
client = OpenAI()
completion = client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
    {"role": "system", "content": "You are a poetic assistant, skilled in explaining complex programming concepts with creative flair."},
    {"role": "user", "content": "Compose a poem that explains the concept of recursion in programming."}
  ]
)
print(completion.choices[0].message)</pre></li>				<li>Run the code by entering <strong class="source-inline">python openai-test.py</strong> into the terminal or command line. This should output a creative poem about recursion. Every result is different because the GPT will use creativity to invent something new each time. This is what it created in <span class="No-Break">this attempt:</span><pre class="source-code">
In code’s endless labyrinth, a tale is spun,
Of functions nested deep, where paths rerun.
A whisper in the dark, a loop within,
Where journeys start anew as they begin.
Behold the call, a serpent chasing tail,
The dragon’s circle, a fractal’s holy grail.
In depths unseen, the echoing refrain,
A self-same mirror where the parts contain.
A climb up winding stairs, each step the same,
Yet every twist, a slight and altered game.
In finite bounds, infinity unfurls,
A loop of dreams within its spiral swirls.</pre><p class="list-inset">The result is surprisingly good. You should try it for yourself to see what new creative poem will <span class="No-Break">be crafted.</span></p></li>			</ol>
			<p>GPT excels at answering questions, but only on the topics it recalls from its training data. In most cases, you’ll want GPT to answer questions about your business or products or answer commonly asked questions from your users. In such cases, you’ll want to add the ability to search a library of your own documents for relevant text, and then have GPT use that text as part of its reference information for responses. This is referred to as RAG, which you can read more about in <a href="B22495_08.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Implementing Vector Search in </em><span class="No-Break"><em class="italic">AI Applications</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor178"/>Hugging Face</h2>
			<p><strong class="bold">Hugging Face</strong> is a prominent AI community and ML platform. Its ecosystem is the <strong class="bold">Hugging Face Hub</strong>, a platform designed to facilitate collaboration and innovation in the AI community. The Hub, located at <a href="https://huggingface.co/docs/hub/en/index"><span class="P---URL">https://huggingface.co/docs/hub/en/index</span></a>, boasts a vast repository of over 120,000 models, 20,000 datasets, and 50,000 demonstrations as of writing, making it one of the largest collections of ML resources available. It has <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Extensive model repositories</strong>: The Hub includes pre-trained models for a variety of tasks, such as text classification, translation, summarization, and question answering, providing a wide range of options <span class="No-Break">for developers.</span></li>
				<li><strong class="bold">Datasets</strong>: It provides access to a diverse array of datasets that are crucial for training and evaluating ML models. Datasets cover multiple domains and languages, supporting the development of robust and versatile <span class="No-Break">AI applications.</span></li>
				<li><strong class="bold">Community and collaboration</strong>: The platform supports collaboration by allowing users to share models, datasets, and code. Developers can contribute to the community by uploading their own models and datasets, fostering a <span class="No-Break">collaborative environment.</span></li>
				<li><strong class="bold">Integration and deployment options</strong>: The Hugging Face Hub integrates seamlessly with popular ML frameworks, such as PyTorch and TensorFlow. The Hub also provides deployment solutions, enabling developers to deploy their models in production <span class="No-Break">environments easily.</span></li>
			</ul>
			<p>GenAI application developers can use the <strong class="bold">Hugging Face Transformers APIs</strong> to get access to thousands of pre-trained ML models on specific datasets for specific tasks. With transformer models, you can use pre-trained models for inference or fine-tune them with your own data using PyTorch and <span class="No-Break">TensorFlow libraries.</span></p>
			<p>To illustrate what is possible for your GenAI application, let’s see how to use a pre-trained transformer model for inference in order to perform two tasks: basic sentiment analysis and text generation. Both could be useful for your GenAI projects if you, for instance, want to sort customer feedback or score it based on sentiment and generate <span class="No-Break">a response.</span></p>
			<h3>Sentiment analysis</h3>
			<p>You’ll use the <strong class="source-inline">transformers</strong> library to utilize shared models, then explore the <strong class="source-inline">pipeline()</strong> function, the core component of the <strong class="source-inline">transformers</strong> library. This function seamlessly integrates the model with necessary pre-processing and post-processing steps, enabling direct text input and generating <span class="No-Break">intelligible responses:</span></p>
			<ol>
				<li>First, ensure you have the necessary packages installed. Note that at least one of TensorFlow or PyTorch should be installed. Here, let’s <span class="No-Break">use TensorFlow:</span><pre class="source-code">
pip3 install transformers tensorflow</pre></li>				<li>Next, import the <strong class="source-inline">pipeline()</strong> function. You’ll also create an instance of the <strong class="source-inline">pipeline()</strong> function and specify the task you want to use it for, that is, <span class="No-Break">sentiment analysis:</span><pre class="source-code">
from transformers import pipeline
analyse_sentiment = pipeline(«sentiment-analysis»)</pre><p class="list-inset">Internally, the pipeline downloads and caches a default pre-trained model and tokenizer for performing sentiment analysis on the <span class="No-Break">input text:</span></p><pre class="source-code">analyse_sentiment("The weather is very nice today.")</pre><p class="list-inset">You’ll receive the <span class="No-Break">following output:</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer068">
					<img alt="" role="presentation" src="image/B22495_07_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5: Hugging Face Transformers sentiment analysis output</p>
			<p class="list-inset">The model performs the analysis and outputs a label and a score. The <strong class="source-inline">label</strong> indicates the sentiment type as positive or negative, and the <strong class="source-inline">score</strong> indicates the degree of confidence in <span class="No-Break">the output.</span></p>
			<p class="list-inset">You can also pass multiple input texts as an array for sentiment classification to <span class="No-Break">the model:</span></p>
			<pre class="source-code">
analyse_sentiment(["The weather is very nice today.", "I don't like it when it rains in winter."])</pre>			<p class="list-inset">You’ll receive the following as <span class="No-Break">the output:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer069">
					<img alt="" role="presentation" src="image/B22495_07_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6: Multiple input texts for sentiment classification in Hugging Face</p>
			<p>In this case, the model outputs an array of objects. Each output object corresponds to the individual <span class="No-Break">text inputs.</span></p>
			<p>You might be holding your breath, expecting things to become more complicated—but they won’t. You conducted your first sentiment analysis in Hugging Face with a pre-trained model with just those few lines <span class="No-Break">of code.</span></p>
			<h3>Text generation</h3>
			<p>In addition to sentiment analysis, you can also perform many other NLP tasks with Transformers libraries, such as text generation. Here, you will provide a prompt, and the model will auto-complete it by generating the <span class="No-Break">remaining text:</span></p>
			<pre class="source-code">
generator = pipeline("text-generation")
generator("I love AI, it has")</pre>			<p>You’ll get the following output for the <span class="No-Break">preceding code:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer070">
					<img alt="" role="presentation" src="image/B22495_07_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7: Text generation using the Hugging Face Transformers</p>
			<p>Since you did not provide a model name to the pipeline instance, it decided to use the default, which in this case is GPT-2. You may or may not get the same results as the ones here because text generation involves some randomness. Again, however, you can see how easy this <span class="No-Break">task was.</span></p>
			<p>Next, specify a model name to be used in the <strong class="source-inline">pipeline</strong> function at the time of text generation. With the following code, you provide some more custom details, such as the number of different sequences to be generated and the maximum length of the <span class="No-Break">output texts:</span></p>
			<pre class="source-code">
generator = pipeline("text-generation", model="distilgpt2")
generator(
    "I love AI, it has",
    max_length=25,
    num_return_sequences=2,
)</pre>			<p>With these additional parameters provided, you’ll now receive the <span class="No-Break">following output:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer071">
					<img alt="" role="presentation" src="image/B22495_07_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8: Hugging Face text generation output with parameters</p>
			<p>The preceding code outputs two different pairs of text, each having fewer than <span class="No-Break">25 words.</span></p>
			<p>As you might expect, Hugging Face offers many more tools and functionalities that developers can use to build their GenAI applications. With its comprehensive library support and active community, Hugging Face continues to be a pivotal resource for advancing NLP and ML projects. Additionally, its seamless integration with various AI/ML frameworks ensures that developers can efficiently deploy and scale their GenAI models with minimal effort and <span class="No-Break">maximum productivity.</span></p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor179"/>Summary</h1>
			<p>In this chapter, you looked at the evolution of AI/ML frameworks in the Python space as LLM-powered applications have gained prominence. You also learned why Python remains a top choice for building modern LLM-powered applications. You reviewed the most popular Python frameworks, libraries, and APIs that can assist you in the different stages of GenAI <span class="No-Break">application development.</span></p>
			<p>The GenAI space is evolving so rapidly that by the time this book is published, there will probably be more libraries available, more APIs in use, and the framework’s capabilities will have expanded. You owe it to yourself to do your own due diligence about which framework is best suited for your business needs, but also make sure to choose one that is appropriately supported. As with any rapidly evolving technology, some of the tools and technologies that are in existence today will be gone tomorrow. This chapter has tried, therefore, to only include those that have the community, enablement, and feature set to ensure <span class="No-Break">their longevity.</span></p>
			<p>Undoubtedly there is still plenty of innovation to be done, and new tools to be created, even in the short term—the tools discussed in this chapter are barely the tip of the iceberg. So, take a deep breath and begin your own discovery. You will inevitably realize that there are tools you need, and that you have too many choices on how to fulfill <span class="No-Break">those needs.</span></p>
			<p>In the next chapter, you will explore how to leverage the vector search feature of MongoDB Atlas to create intelligent applications. You will learn about RAG architecture systems and gain a deeper understanding of various complex RAG architecture patterns with <span class="No-Break">MongoDB Atlas.</span></p>
		</div>
	</body></html>