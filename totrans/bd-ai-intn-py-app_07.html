<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-123"><a id="_idTextAnchor162"/>7</h1>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor163"/>Useful Frameworks, Libraries, and APIs</h1>
			<p>As you might expect, <strong class="bold">Python</strong> is the most popular programming language for building intelligent AI applications. This is due to its flexibility and ease of use, as well as for its vast number of <strong class="bold">AI and machine learning</strong> (<strong class="bold">ML</strong>) libraries. Python has a specialized library for nearly all the necessary tasks required to build a <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>) application.</p>
			<p>In <a href="B22495_01.xhtml#_idTextAnchor009"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Generative AI</em>, you read about the GenAI stack and the evolution of AI. Like the AI landscape, the Python library and framework space also went through an evolution phase. Earlier, libraries such as pandas, NumPy, and polars were used for data cleanup and transformation work, while PyTorch, TensorFlow, and scikit-learn were used for training ML models. Now, with the rise of the GenAI stack, LLMs, and vector databases, a new type of AI framework has emerged.</p>
			<p>These new libraries and frameworks are designed to simplify the creation of new applications powered by LLMs. Since building GenAI applications requires the seamless integration of data from many sources and the use of diverse AI models, these AI frameworks provide built-in functionalities to facilitate acquiring, migrating, and transforming data.</p>
			<p>This chapter delves into the world of AI/ML frameworks, exploring their importance and highlighting why Python has emerged as the go-to language for AI/ML development. By the end of this chapter, you’ll be able to understand the most popular frameworks and libraries, as well as how they help you—the developer—build your GenAI application.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>AI/ML frameworks</li>
				<li>Python libraries</li>
				<li>Publicly available APIs and other tools</li>
			</ul>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor164"/>Technical requirements</h1>
			<p>To perform the steps shown in this chapter, you will need the following:</p>
			<ul>
				<li>Latest major version of Python.</li>
				<li>A free tier Atlas cluster running MongoDB version 6.0.11, 7.0.2, or later.</li>
				<li>Your current IP address added to your Atlas project access list.</li>
				<li>An environment set up to run Python code in an interactive environment, such as Jupyter Notebook or Colab. This chapter uses Jupyter Notebook.</li>
			</ul>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor165"/>Python for AI/ML</h1>
			<p>Python has established itself as the go-to programming language in various fields, but most notably in AI, ML, and building applications powered by <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>). Python offers simplicity, readability, and a robust ecosystem of libraries, making it an ideal choice for all kinds of users, whether they are developers, researchers, or even students just getting started with programming. Python has also emerged as the language of choice for building new LLM-powered applications, underscoring Python’s usefulness, popularity, and versatility.</p>
			<p>In this section, you will learn some of the reasons that make Python a great choice for building modern AI-powered applications:</p>
			<ul>
				<li><strong class="bold">Simplicity and readability</strong>: Python’s syntax is designed to be intuitive and clear, which is one of its core strengths. Python can represent complex algorithms and tasks in a few lines of code that are easily readable and understandable.</li>
				<li><strong class="bold">Rich ecosystem of libraries and frameworks</strong>: Python offers an extensive range of libraries and frameworks specifically designed for AI/ML use cases. Libraries such as TensorFlow, PyTorch, and scikit-learn have traditionally been popular for ML tasks. Hugging Face’s Transformers library has also become an indispensable part of the developer workflow for building modern LLM-powered applications. It provides pre-trained models and straightforward APIs to fine-tune models for specific tasks. These libraries not only accelerate development time but also provide cutting-edge solutions to developers across the world.</li>
				<li><strong class="bold">Strong community and support</strong>: Python is one of the most popular programming languages in the world, and hence has a huge community. According to the Stack Overflow survey 2023 (<a href="https://survey.stackoverflow.co/2023/">https://survey.stackoverflow.co/2023/</a>), it’s the second most popular programming language after JavaScript (excluding HTML/CSS). This strong and large community provides a wealth of resources, including tutorials, discussion forum engagements, and open source projects, which offer a helpful support system for someone working on building modern applications.</li>
				<li><strong class="bold">Integration with other technologies</strong>: Python’s ability to integrate seamlessly with other technologies and programming languages makes it a great choice for AI/ML tasks and building LLM-powered applications. For example, Python can easily interface with programming languages such as C/C++ for performance-critical tasks. It also interfaces well with languages such as Java and C#. This flexibility of Python is helpful for deploying LLM-powered applications in diverse environments, ensuring that Python can be part of large heterogeneous systems.</li>
				<li><strong class="bold">Rapid prototyping and experimentation</strong>: Building a sophisticated AI/ML-powered application requires many iterations of tests, experiments, and fine-tuning. Python allows developers to quickly build prototypes in a few lines of code. Easy testing and debugging also help to prototype a quick solution. Python’s interactive environments, such as Jupyter Notebook, provide an excellent platform for this purpose. With Python, developers building LLM-powered applications can quickly test hypotheses, visualize data, and debug code in an interactive manner.</li>
			</ul>
			<p>Python combines speed, simplicity, specialized libraries and frameworks, and strong community support with easy integration with other languages and technologies, all of which make it an excellent choice for building modern LLM-powered applications.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor166"/>AI/ML frameworks</h1>
			<p><strong class="bold">AI/ML frameworks</strong> are essential tools that streamline the development and deployment of ML models, providing pre-built algorithms, optimized performance, and scalable solutions. They enable developers to focus on refining their models and GenAI applications rather than getting bogged down by low-level implementations. Using frameworks ensures efficiency, adaptability, and the ability to harness cutting-edge AI advancements. Developers should be interested in these frameworks as they also reduce development time and enhance the potential for breakthroughs in GenAI.</p>
			<p>MongoDB has integrations with many AI/ML frameworks that may be familiar to developers, such as LangChain, LlamaIndex, Haystack, Microsoft Semantic Kernel, DocArray, and Flowise.</p>
			<p>In this section, you will learn about <strong class="bold">LangChain</strong>, one of the most popular GenAI frameworks. Although it is very popular, it is certainly not the only popular framework. If you are interested in other frameworks, you can check out the documentation linked in the <em class="italic">Appendix: Further Reading</em> chapter at the end of this book or see the latest list of supported AI/ML frameworks for Python at <a href="https://www.mongodb.com/docs/languages/python/">https://www.mongodb.com/docs/languages/python/</a>.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor167"/>LangChain</h2>
			<p>LangChain is a framework for developing applications powered by LLMs. LangChain simplifies every stage of the LLM application lifecycle. It enables building applications that connect external sources of data and computation to LLMs. The basic LLM chain relies solely on the information provided in the prompt template to generate a response, and the concept of a <em class="italic">LangChain</em> allows you to extend these chains for advanced processing.</p>
			<p>In this section, you will learn how to use LangChain to perform semantic search on your data and build a <strong class="bold">retrieval-augmented generation (RAG) </strong>implementation. Before you begin, make sure you have all the necessary tools installed and set up on your computer, as listed in the <em class="italic">Technical requirements</em> section of this chapter.</p>
			<h3>Getting started with LangChain</h3>
			<p>Perform the following steps to set up your environment for LangChain:</p>
			<ol>
				<li>Start by installing the necessary dependencies:<pre class="source-code">
pip3 install --quiet --upgrade langchain==0.1.22 langchain-mongodb==0.1.8 langchain_community==0.2.12 langchain-openai==0.1.21 pymongo==4.5.1 polars==1.5.0 pypdf==3.15.0</pre></li>				<li>Run the following code to import the required packages:<pre class="source-code">
import getpass, os, pymongo, pprint
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pymongo import MongoClient</pre></li>				<li>After you have imported the necessary packages, make sure the environment variables are set properly. You have two important secrets to store as environment variables: your <strong class="bold">OpenAI API key</strong> and <strong class="bold">MongoDB Atlas </strong><strong class="bold">connection string</strong>.<p class="list-inset">Run the following command to store your OpenAI API key as an environment variable:</p><pre class="source-code">
os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;clusterName&gt;.&lt;hostname&gt;.mongodb.net</pre><p class="list-inset">Run the following command to store your MongoDB Atlas connection string as an environment variable:</pre><pre class="source-code">ATLAS_CONNECTION_STRING = getpass.getpass("MongoDB Atlas SRV Connection String:")</pre><p class="list-inset">You are now ready to connect to the MongoDB Atlas cluster.</p></li>				<li>Next, you’ll instantiate the <code>MongoClient</code> and pass your connection string to establish communications with your MongoDB Atlas database. Run the following code to establish the connection:<pre class="source-code">
# Connect to your Atlas cluster
client = MongoClient(ATLAS_CONNECTION_STRING)</pre></li>				<li>Next, you’ll specify the name of the database and the collection you want to create. In this example, you’ll create a database named <code>langchain_db</code> and a collection called <code>test</code>. You’ll also define the name of the vector search index to create and use with the following code:<pre class="source-code">
# Define collection and index name
db_name = "langchain_db"
collection_name = "test"
atlas_collection = client[db_name][collection_name]
vector_search_index = "vector_index"</pre></li>			</ol>
			<p>With these steps, you’ve set up the basics of connectivity. Now that you have the bare bones of your database, you’ll want to define what your application does.</p>
			<p>In this case, you will do the following:</p>
			<ol>
				<li>Fetch a publicly accessible PDF document.</li>
				<li>Split it into smaller chunks of information for easy consumption by your GenAI application.</li>
				<li>Upload the data into the MongoDB database.</li>
			</ol>
			<p>This functionality is not something you have to build from scratch. Instead, you’ll use the free, open source library integration provided by LangChain called <code>PyPDFLoader</code>, which you imported in <em class="italic">Step 2</em> earlier in this section.</p>
			<h3>Fetching and splitting public PDF documents</h3>
			<p>Using <code>PyPDFLoader</code> to fetch publicly available PDFs is quite simple. In the following code, you will fetch a publicly accessible PDF document and split it into smaller chunks that you can later upload into your MongoDB database:</p>
			<pre class="source-code">
# Load the PDF
loader = PyPDFLoader("https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HkJP")
data = loader.load()
# Split PDF into documents
text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
docs = text_splitter.split_documents(data)
# Print the first document
docs[0]</pre>			<p>You will then receive the following output:</p>
			<pre class="source-code">
Document(metadata={'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 0}, page_content='Mong oDB Atlas Best Practices January 20 19A MongoD B White P aper')</pre>			<p>With this code, you first instantiated <code>PyPDFLoader</code> and then passed it the URL to the publicly accessible PDF file: <a href="https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HkJP">https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HkJP</a>. Next, you loaded the fetched PDF file into the <code>data</code> variable.</p>
			<p>After that, you split the PDF file’s text into smaller chunks. For this example, you set the chunk size to 200 characters and allowed an overlap of 20 characters between chunks. The overlap maintains the context between chunks. Note that this number is not arbitrary, and there are many opinions about what your chunking strategy should be. Some of those resources are discussed in the <em class="italic">Appendix: Further Reading</em> chapter of this book.</p>
			<p>You stored the split chunks in the <code>docs</code> variable and printed the first chunk of the split document. This indicates that your output request via the <code>print</code> command was successful, and you can easily confirm whether the information is correct for this entry.</p>
			<h3>Creating the vector store</h3>
			<p>After you have split your documents into chunks, you will instantiate the vector store with the following code:</p>
			<pre class="source-code">
# Create the vector store
vector_store = MongoDBAtlasVectorSearch.from_documents(
    documents = docs,
    embedding = OpenAIEmbeddings(disallowed_special=()),
    collection = atlas_collection,
    index_name = vector_search_index
)</pre>			<p>In the preceding code, you created a vector store named <code>vector_store</code> using the <code>MongoDBAtlasVectorSearch.from_documents</code> method and specified various parameters:</p>
			<ul>
				<li><code>documents = docs</code>: The name of the document that you want to store in your vector database</li>
				<li><code>embedding = OpenAIEmbeddings(disallowed_special=())</code>: The class that generates vector embeddings for the documents using OpenAI’s embedding model</li>
				<li><code>collection = atlas_collection</code>: The Atlas collection where documents will be stored</li>
				<li><code>index_name = vector_search_index</code>: The name of the index to use for querying the vector store</li>
			</ul>
			<p>You’ll also need to create your <strong class="bold">Atlas Vector Search index</strong> in the MongoDB database. For explicit instructions on how this is done, see <a href="B22495_08.xhtml#_idTextAnchor180"><em class="italic">Chapter 8</em></a>,<em class="italic"> Implementing Vector Search in AI Applications</em>. This must be completed before you can successfully run the previous code. When you are creating a Vector Search index, use the following index definition:</p>
			<pre class="source-code">
{
   "fields":[
      {
         "type": "vector",
         "path": "embedding",
         "numDimensions": 1536,
         "similarity": "cosine"
      },
      {
         "type": "filter",
         "path": "page"
      }
   ]
}</pre>			<p>This index defines two fields:</p>
			<ul>
				<li><code>text-embedding-ada-002</code> model. It has 1,536 dimensions and uses cosine similarity to measure similarity. You may also want to consider other newer models from OpenAI, <code>text-embedding-3-small</code> and <code>text-embedding-3-large</code>, which are optimized for different use cases and therefore have a different number of dimensions. See <a href="https://platform.openai.com/docs/guides/embeddings">https://platform.openai.com/docs/guides/embeddings</a> for more details as well as current options.</li>
				<li><strong class="bold">Page field</strong>: A filter type field used for pre-filtering data based on the page number in the PDF.</li>
			</ul>
			<p>Now, you can run your code successfully, fetch a publicly available PDF, chunk it into smaller portions of data, and store them in a MongoDB Atlas database. With these steps accomplished, you can conduct additional tasks, such as running queries to perform semantic search on your data. You can learn about basic semantic search in <a href="B22495_08.xhtml#_idTextAnchor180"><em class="italic">Chapter 8</em></a>, <em class="italic">Implementing Vector Search in AI Applications</em>, and <a href="B22495_10.xhtml#_idTextAnchor214"><em class="italic">Chapter 10</em></a>, <em class="italic">Refining the Semantic Data Model to </em><em class="italic">Improve Accuracy</em>.</p>
			<p>For more information on this topic, you can also consult the official documentation from LangChain, available at <a href="https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/#pre-filtering-with-similarity-search">https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/#pre-filtering-with-similarity-search</a>.</p>
			<p>Next, let’s cover some specific LangChain functionalities that you will find most useful when building GenAI applications.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor168"/>LangChain semantic search with score</h2>
			<p>LangChain provides some particularly helpful methods to perform semantic search on your data and return a <strong class="bold">score</strong>. This score refers to the measure of relevance between the query and the matching documents based on their semantic content. You can use this score when you want to return more than one result to your users and also limit the number of results. For example, this score can prove useful in returning the top three most relevant pieces of content about a topic.</p>
			<p>The method that you will use here is <code>similarity_search_with_score</code>:</p>
			<pre class="source-code">
query = "MongoDB Atlas security"
results = vector_store.similarity_search_with_score(
   query = query, k = 3
)
pprint.pprint(results)</pre>			<p>You pass the query to the <code>similarity_search_with_score</code> function and specify the <code>k</code> parameter as <code>3</code> to limit the number of documents to return to 3. Then, you can print the output:</p>
			<pre class="source-code">
[(Document (page_content='To ensure a secure system right out of the box, \nauthentication and IP Address whitelist ing are\nautomatically enabled. \nReview the security section of the MongoD B Atlas', metadata={'_id': {'Soid": "667 20a81b6cb1d87043c0171'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 17}),
0.9350903034210205),
(Document(page_content='MongoD B Atlas team are also monitoring the underlying\ninfrastructure, ensuring that it i s always in a healthy state. \nApplication Logs And Database L ogs', metadata={'_id': {'soid': '66720a81b6cb1d87043 c013c'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 15}),
0.9336163997650146),
(Document(page_content="MongoD B.\nMongoD B Atlas incorporates best practices to help keep\nmanaged databases heal thy and optimized. T hey ensure\noperational continuity by converting complex manual tasks', metadata={'_id': {'so id: '66728a81b6cb1d87043c011f'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'p age': 13)),
0.9317773580551147)]</pre>			<p>As you can see in the output, three documents are returned that have the highest relevance score. Each returned document also has a relevance score attached to it that ranges between 0 and 1.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor169"/>Semantic search with pre-filtering</h2>
			<p>MongoDB allows you to pre-filter your data using a match expression to narrow down the search space before performing a more computationally intensive vector search. This offers several benefits to developers, such as increased performance, improved accuracy, and enhanced query relevancy. When pre-filtering, remember to index any metadata fields by which you want to filter during index creation.</p>
			<p>Here is a code snippet that shows how you can perform semantic search with pre-filtering:</p>
			<pre class="source-code">
query = "MongoDB Atlas security"
results = vector_store.similarity_search_with_score(
   query = query,
   k = 3,
   pre_filter = { "page": { "$eq": 17 } }
)
pprint.pprint(results)</pre>			<p>In this code example, you have the same query string for which you performed a plain semantic search earlier. The <code>k</code> value is set to <code>3</code> so that it only returns the top three matching documents. You have also provided a <code>pre_filter</code> query, which is basically an MQL expression that uses the <code>$eq</code> operator to specify that MongoDB should only return content and chunked information that is on page <code>17</code> of the original PDF document.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor170"/>Implementing a basic RAG solution with LangChain</h2>
			<p>LangChain’s functionalities are not only limited to performing semantic search queries on your data stored in vector databases. It also allows you to build powerful GenAI applications. With the following code snippet, you will learn an easy way to do the following:</p>
			<ul>
				<li>Set up a MongoDB Atlas Vector Search retriever for similarity-based search.</li>
				<li>Return the 10 most relevant documents.</li>
				<li>Utilize a custom RAG prompt with an LLM to answer questions based on the retrieved documents:</li>
			</ul>
			<pre class="source-code">
# Instantiate Atlas Vector Search as a retriever
retriever = vector_store.as_retriever(
   search_type = "similarity",
   search_kwargs = { "k": 3 }
)
# Define a prompt template
template = """
Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}
Question: {question}
"""
custom_rag_prompt = PromptTemplate.from_template(template)
llm = ChatOpenAI()
def format_docs(docs):
   return "\n\n".join(doc.page_content for doc in docs)
# Construct a chain to answer questions on your data
rag_chain = (
   { "context": retriever | format_docs, "question": RunnablePassthrough()}
   | custom_rag_prompt
   | llm
   | StrOutputParser()
)
# Prompt the chain
question = "How can I secure my MongoDB Atlas cluster?"
answer = rag_chain.invoke(question)
print(«Question: « + question)
print(«Answer: « + answer)
# Return source documents
documents = retriever.get_relevant_documents(question)
print(«\nSource documents:»)
pprint.pprint(documents)</pre>			<p>The preceding code instantiates Atlas Vector Search as a <code>k</code> as <code>3</code> to search for only the three most relevant documents.</p>
			<p>In the preceding code, notice the line that says <code>custom_rag_prompt = PromptTemplate.from_template(template)</code>. It refers to prompt templates, which are detailed in the next section.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor171"/>LangChain prompt templates and chains</h2>
			<p><code>context</code> as an input variable and the original query for the LLM.</p>
			<p>Let’s set up a <strong class="bold">chain</strong>, a key feature of LangChain that specifies three main components:</p>
			<ul>
				<li><strong class="bold">Retriever</strong>: You will use MongoDB Atlas Vector Search to find relevant documents that provide context for the language model</li>
				<li><strong class="bold">Prompt template</strong>: This is the template you created earlier to format the query and the contextual information</li>
				<li><strong class="bold">LLM</strong>: You will use the OpenAI chat model to generate responses based on the provided context</li>
			</ul>
			<p>You will use this chain to process a sample input query about MongoDB Atlas Security recommendations, format the query, retrieve the results of the query, and then return a response to the user along with the documents used as context. Due to LLM variability, you will likely never receive the exact same response twice, but here is an example showing the potential output:</p>
			<pre class="source-code">
Question: How can I secure my MongoDB Atlas cluster?
Answer: To secure your MongoDB Atlas cluster, you can enable authentication and IP Address whitelisting, review the security section of the MongoDB Atlas documentation, and utilize encryption of data at rest with encrypted storage volumes. Additionally, you can set up global clusters with a few clicks in the MongoDB Atlas UI, ensure operational continuity by converting complex manual tasks, and consider setting up a larger number of replica nodes for increased protection against database downtime.
Source documents:
[Document (page_content='To ensure a secure system right out of the box, \nauthentication and IP Address whitelisti ng are\nautomatically enabled.\nReview the security section of the MongoD B Atlas', metadata={'_id': {'$oid': '6672
@a81b6cb1d87043c0171'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 17}),
Document(page_content='MongoD B Atlas team are also monitoring the underlying\ninfrastructure, ensuring that it is always in a healthy state. \nApplication L ogs And Database L ogs', metadata('id': ('soid': '66728a81b6cb1d87043c0 13c'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 15}),
Document(page_content='All the user needs to do in order for MongoD B Atlas to\nautomatically deploy the cluster i s to select a handful of\noptions: \n Instance size\n•Storage size (optional) \n Storage speed (optional)', metadata= {"_id": "soid: '66728a81b6cb1d87043c012a'), 'source': 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/ RE4HKJP', 'page': 14)),</pre>			<p>This output both answers the user’s inquiry and provides the source information, increasing not only user trust but also the ability of the user to follow up and get more details as they require.</p>
			<p>This brief overview of the LangChain framework has tried to convince you of this framework’s utility and potential and give you a preview of its capabilities to save you valuable time when crafting your GenAI application.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor172"/>Key Python libraries</h1>
			<p>In addition to AI/ML frameworks, there are also many Python libraries that will make the experience of building your GenAI application easier. Whether you require assistance with data cleansing, formatting, or transformation, there are likely half a dozen potential Python libraries to solve every problem. The following subsections list some favorites and explain how they can assist you during your GenAI journey.</p>
			<p>For this book, you can broadly divide these libraries into three categories:</p>
			<ul>
				<li><strong class="bold">General-purpose scientific libraries</strong> such as pandas, NumPy, and scikit-learn</li>
				<li><strong class="bold">MongoDB-specific libraries</strong> such as PyMongoArrow</li>
				<li><strong class="bold">Deep learning frameworks</strong> such as PyTorch and TensorFlow</li>
			</ul>
			<p>The rest of this section covers one relevant and popular library from each of these categories</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor173"/>pandas</h2>
			<p>The pandas library is a powerful and flexible open source data manipulation and analysis library for Python. It provides data structures such as DataFrames and Series, which are designed to handle structured data intuitively and efficiently. When working with tabular data stored in spreadsheets or databases, pandas is a great tool for data analysis. With pandas, you can perform a wide range of operations, including cleaning, transforming, and aggregating data.</p>
			<p>Among many other noticeably out-of-the-box functionalities, pandas also offers great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data. In addition to providing a wide range of methods to work with numerical data, pandas gives great support for working with text-based data.</p>
			<p>Here is a short example of how to work with the pandas library. In the following example, you will create a pandas DataFrame from a Python dictionary. Then, you will print the entire DataFrame. Next, you will select a specific column, which is <code>Age</code>, and print it. Then, you will filter data by row label or by the specific position of a row.</p>
			<p>The next line shows how you can filter data using Boolean masking in pandas. Here, you will print out the DataFrame format:</p>
			<pre class="source-code">
pip3 install pandas==1.5.3
import pandas as pd
# Create a DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],
    'Age': [24, 27, 22, 32, 29],
    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']
}
df = pd.DataFrame(data)
# Display the DataFrame
print("DataFrame:")
print(df)</pre>			<p>Your output should be in the format of a pandas DataFrame, similar to <em class="italic">Figure 7</em><em class="italic">.1</em>:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_07_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1: DataFrame output from pandas</p>
			<p>You can then manipulate this data in various ways, each time outputting the results as you see fit, but always formatted as a pandas DataFrame. To print only the ages of the users, you would use the following code:</p>
			<pre class="source-code">
# Select a column
print("\nAges:")
print(df['Age'])</pre>			<p>You’ll get the output shown in <em class="italic">Figure 7</em><em class="italic">.2</em>:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_07_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2: DataFrame output of ages</p>
			<p>You can also filter the output. Here, you will filter data to show only those people who are older than 25, and then present the results as a DataFrame:</p>
			<pre class="source-code">
# Filter data
print("\nPeople older than 25:")
print(df[df['Age'] &gt; 25])</pre>			<p>This code will filter the data and then output the results in DataFrame format, as in <em class="italic">Figure 7</em><em class="italic">.3</em>:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_07_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3: Filtered DataFrame output</p>
			<p>You can also perform calculations with the pandas library in a straightforward way. To calculate the average age, for instance, you would use code such as this:</p>
			<pre class="source-code">
# Calculate average age
average_age = df['Age'].mean()
print("\nAverage Age:")
print(average_age)</pre>			<p>And your output would look like <em class="italic">Figure 7</em><em class="italic">.4</em>:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_07_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4: Calculated field output</p>
			<p>As you can see, data manipulation in pandas is fairly easy, and the outputs are immediately readable and well-formatted for further analysis. The intuitive syntax and powerful functions of pandas make it an essential tool for Python developers, enabling them to handle large datasets with ease and precision. For those building GenAI applications, pandas streamlines the data preprocessing steps, ensuring that data is clean, structured, and ready for model training. Additionally, its robust integration with other Python libraries enhances its utility, making complex data analysis and visualization straightforward and efficient.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor174"/>PyMongoArrow</h2>
			<p><strong class="bold">PyMongoArrow</strong> is a Python library built on top of the official MongoDB Python driver, <strong class="bold">PyMongo</strong>, which allows you to move data out of the MongoDB database into some of the most popular Python libraries, such as pandas, NumPy, PyArrow, and polars, and vice versa.</p>
			<p>PyMongoArrow simplifies loading data from MongoDB into other supported data formats. The example covered below demonstrates how you can work with MongoDB, PyMongoArrow, and libraries such as pandas and NumPy. You may find this useful in the context of GenAI applications in the following situations:</p>
			<ul>
				<li>When you require data in a specific format for summarization and analysis (CSV, DataFrame, NumPy array, Parquet file, etc.) from MongoDB</li>
				<li>If you need to merge data of various types for calculations or transformations that are then used for GenAI analysis</li>
			</ul>
			<p>As an example, if you have inbound financial data from <em class="italic">Application A</em>, inbound sales data from <em class="italic">Application B</em>, PDF files from <em class="italic">Team 1</em>, and <code>.txt</code> files from <em class="italic">Team 2</em>, and you’d like your GenAI application to summarize annual data from all these different places, you will likely get more accurate results if all types of data are in the same format. This will require some upfront programmatic effort, and PyMongoArrow simplifies transforming MongoDB JSON into other data types as well as ingesting those other data types and converting them into JSON.</p>
			<p>Follow these steps to complete this example with PyMongoArrow:</p>
			<ol>
				<li>Start by installing and importing the latest version of PyMongoArrow:<pre class="source-code">
pip3 install PyMongoArrow
import pymongoarrow as pa</pre></li>				<li>Now, make sure you have your Atlas cluster connection string handy:<pre class="source-code">
import getpass, os, pymongo, pprint</pre></li>				<li>Next, you will extend the PyMongo driver via the <code>pymongoarrow.monkey</code> module. This allows you to add the PyMongoArrow functionality directly to MongoDB collections in Atlas. By calling <code>patch_all()</code> from <code>pymongoarrow.monkey</code>, new collection instances will include PyMongoArrow APIs, such as <code>pymongoarrow.api.find_pandas_all()</code>. This is useful because you can now easily export your data from MongoDB to various formats such as pandas.<pre class="source-code">
from pymongoarrow.monkey import patch_all
patch_all()</pre></li>				<li>Add some test data to your collection:<pre class="source-code">
from datetime import datetime
from pymongo import MongoClient
client = MongoClient(ATLAS_CONNECTION_STRING)
client.db.data.insert_many([
  {'_id': 1, 'amount': 21, 'last_updated': datetime(2020, 12, 10, 1, 3, 1), 'account': {'name': 'Customer1', 'account_number': 1}, 'txns': ['A']},
  {'_id': 2, 'amount': 16, 'last_updated': datetime(2020, 7, 23, 6, 7, 11), 'account': {'name': 'Customer2', 'account_number': 2}, 'txns': ['A', 'B']},
  {'_id': 3, 'amount': 3,  'last_updated': datetime(2021, 3, 10, 18, 43, 9), 'account': {'name': 'Customer3', 'account_number': 3}, 'txns': ['A', 'B', 'C']},
  {'_id': 4, 'amount': 0,  'last_updated': datetime(2021, 2, 25, 3, 50, 31), 'account': {'name': 'Customer4', 'account_number': 4}, 'txns': ['A', 'B', 'C', 'D']}])</pre></li>				<li>PyMongoArrow uses a <code>schema</code> object and mapping field names to type-specifiers:<pre class="source-code">
from pymongoarrow.api import Schema
schema = Schema({'_id': int, 'amount': float, 'last_updated': datetime})</pre><p class="list-inset">MongoDB’s key feature is its ability to represent nested data using embedded documents, along with its support for lists and nested lists. PyMongoArrow fully supports these features out of the box, providing first-class functionality for handling embedded documents, lists, and nested lists seamlessly.</p></li>				<li>Let’s perform some <code>find</code> operations on the data. The following code demonstrates querying a MongoDB collection called <code>data</code> for documents where the <code>amount</code> field is greater than <code>0</code>, using PyMongoArrow to convert the results into different data formats. A predefined schema is used for the conversion, but it’s optional. If you omit the schema, PyMongoArrow tries to automatically apply a schema based on the data contained in the first batch:<pre class="source-code">
df = client.db.data.find_pandas_all({'amount': {'$gt': 0}}, schema=schema)
arrow_table = client.db.data.find_arrow_all({'amount': {'$gt': 0}}, schema=schema)
df = client.db.data.find_polars_all({'amount': {'$gt': 0}}, schema=schema)
ndarrays = client.db.data.find_numpy_all({'amount': {'$gt': 0}}, schema=schema)</pre><p class="list-inset">The first line of code converts the query results into a pandas DataFrame. The second line of code converts the query results set into an arrow table. The third line converts the query results set into a polars DataFrame, and finally, the fourth line converts the query result set into a NumPy array.</p></li>			</ol>
			<p>You are not limited to performing <code>find</code> operations to convert the query result set into other supported data formats. PyMongoArrow also allows you to use MongoDB’s powerful aggregation pipeline to perform complex queries on your data to filter out the needed data before exporting it to other data formats.</p>
			<p>For example, the following code performs an aggregation query on the data collection in a MongoDB database, grouping all documents and calculating the total sum of the <code>amount</code> field:</p>
			<pre class="source-code">
df = client.db.data.aggregate_pandas_all([{'$group': {'_id': None, 'total_amount': { '$sum': '$amount' }}}])</pre>			<p>The result of this code is converted into a pandas DataFrame that would consist of a total sum.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor175"/>PyTorch</h2>
			<p>Now that you have learned a little bit about pandas and NumPy, it’s important you also have some knowledge of another popular Python ML library, PyTorch.</p>
			<p>PyTorch, developed by Meta’s AI Research lab, is an open source deep learning framework known for its flexibility and ease of use. It is widely appreciated for its dynamic computation graph, which allows intuitive coding and immediate execution of code. This feature is particularly useful for researchers and developers who need to experiment and iterate quickly.</p>
			<p>In the context of building a GenAI application, PyTorch serves as a powerful tool for the following:</p>
			<ul>
				<li><strong class="bold">Model training and development</strong>: PyTorch is utilized for developing and training the core generative models, such as <strong class="bold">generative pre-trained transformer</strong> (<strong class="bold">GPT</strong>) variants, which form the backbone of the GenAI application.</li>
				<li><strong class="bold">Flexibility and real-time experimentation</strong>: The dynamic computation graph in PyTorch allows on-the-fly modifications and real-time experimentation, which are crucial for fine-tuning generative models to produce high-quality output.</li>
			</ul>
			<p>Developers who are adapting pre-trained language models to their specific requirements or developing their own custom model for unique tasks may be interested in using this library, along with some of the APIs discussed in the following section.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor176"/>AI/ML APIs</h1>
			<p>When developing GenAI applications, developers have access to a variety of APIs that can significantly enhance the capabilities and efficiency of their projects. As these APIs are widely used, they offer performance, stability, and consistency across thousands of projects, ensuring that developers don’t need to reinvent the wheel. Here are just some of the functionalities that these APIs offer:</p>
			<ul>
				<li><strong class="bold">Text generation and processing</strong>: APIs such as <strong class="bold">OpenAI</strong>, <strong class="bold">Hugging Face</strong>, and <strong class="bold">Google Gemini API</strong> enable developers to generate coherent and contextually appropriate text, which is crucial for applications such as content creation, dialogue systems, and virtual assistants.</li>
				<li><strong class="bold">Translation capabilities</strong>: The <strong class="bold">Google Cloud Translation API</strong>, <strong class="bold">Azure AI Translator</strong>, and <strong class="bold">Amazon Translate API</strong> provide robust translation capabilities, making GenAI applications multilingual and globally accessible.</li>
				<li><strong class="bold">Speech synthesis and recognition</strong>: Services such as <strong class="bold">Google Text-to-Speech</strong>, <strong class="bold">Amazon Polly</strong>, and <strong class="bold">IBM Watson Text-to-Speech</strong> convert generated text into natural-sounding speech, enhancing user interaction and accessibility.</li>
				<li><strong class="bold">Image and video processing</strong>: APIs from <strong class="bold">Clarifai</strong> and <strong class="bold">DeepAI</strong> allow GenAI applications to create, modify, and analyze visual content, enabling tasks such as image generation from text and object recognition.</li>
			</ul>
			<p>These APIs provide a range of capabilities that, when combined, can significantly accelerate the development and enhance the functionality of GenAI applications. Next, you’re going to dig deeper into two of these APIs, the OpenAI API and the Hugging Face Transformers APIs.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor177"/>OpenAI API</h2>
			<p>As you may recall from <a href="B22495_03.xhtml#_idTextAnchor041"><em class="italic">Chapter 3</em></a>, <em class="italic">Large Language Models</em>, OpenAI provides a foundational model trained on a broad spectrum of data. It offers this model via an API, which allows you to harness the power of advanced ML models without needing to manage the underlying infrastructure. The computational and financial costs of retraining or hosting a custom LLM for an organization or domain-specific information are very high, so most developers will utilize someone else’s LLM to provide GenAI capabilities to their applications.</p>
			<p>Although each API has its own strengths and weaknesses, the OpenAI API is currently the most widely used. It provides a simple interface for developers to create an intelligence layer in their applications. It is powered by OpenAI’s state-of-the-art models and cutting-edge <strong class="bold">natural language processing </strong>(<strong class="bold">NLP</strong>) capabilities, enabling applications to perform tasks such as text generation, summarization, translation, and conversation. The API is designed to be flexible and scalable, making it suitable for a wide range of use cases, from chatbots to content creation tools. It is also well documented, with a large community, and there are many tutorials available for seemingly every use case.</p>
			<p>The OpenAI API is already somewhat of an industry standard, and many GenAI tools and technologies have support and seamless integrations with it. If you’d like to avoid a lot of unnecessary effort and costs, your best bet is to work with the OpenAI API.</p>
			<p>Let’s get started with the OpenAI API in the following example:</p>
			<ol>
				<li>To get started, you’ll need to install <code>openai</code> from the terminal or command line:<pre class="source-code">
pip3 install --upgrade openai==1.41.0</pre></li>				<li>Include your API key from OpenAI in the environment variable file:<pre class="source-code">
export OPENAI_API_KEY='your-api-key-here'</pre></li>				<li>Send your first API test request to the OpenAI API using the Python library. To do this, create a file named <code>openai-test.py</code> using the terminal or an IDE. Then, inside the file, copy and paste one of the following examples:<pre class="source-code">
from openai import OpenAI
client = OpenAI()
completion = client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
    {"role": "system", "content": "You are a poetic assistant, skilled in explaining complex programming concepts with creative flair."},
    {"role": "user", "content": "Compose a poem that explains the concept of recursion in programming."}
  ]
)
print(completion.choices[0].message)</pre></li>				<li>Run the code by entering <code>python openai-test.py</code> into the terminal or command line. This should output a creative poem about recursion. Every result is different because the GPT will use creativity to invent something new each time. This is what it created in this attempt:<pre class="source-code">
In code’s endless labyrinth, a tale is spun,
Of functions nested deep, where paths rerun.
A whisper in the dark, a loop within,
Where journeys start anew as they begin.
Behold the call, a serpent chasing tail,
The dragon’s circle, a fractal’s holy grail.
In depths unseen, the echoing refrain,
A self-same mirror where the parts contain.
A climb up winding stairs, each step the same,
Yet every twist, a slight and altered game.
In finite bounds, infinity unfurls,
A loop of dreams within its spiral swirls.</pre><p class="list-inset">The result is surprisingly good. You should try it for yourself to see what new creative poem will be crafted.</p></li>			</ol>
			<p>GPT excels at answering questions, but only on the topics it recalls from its training data. In most cases, you’ll want GPT to answer questions about your business or products or answer commonly asked questions from your users. In such cases, you’ll want to add the ability to search a library of your own documents for relevant text, and then have GPT use that text as part of its reference information for responses. This is referred to as RAG, which you can read more about in <a href="B22495_08.xhtml#_idTextAnchor180"><em class="italic">Chapter 8</em></a>, <em class="italic">Implementing Vector Search in </em><em class="italic">AI Applications</em>.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor178"/>Hugging Face</h2>
			<p><strong class="bold">Hugging Face</strong> is a prominent AI community and ML platform. Its ecosystem is the <strong class="bold">Hugging Face Hub</strong>, a platform designed to facilitate collaboration and innovation in the AI community. The Hub, located at <a href="https://huggingface.co/docs/hub/en/index">https://huggingface.co/docs/hub/en/index</a>, boasts a vast repository of over 120,000 models, 20,000 datasets, and 50,000 demonstrations as of writing, making it one of the largest collections of ML resources available. It has the following:</p>
			<ul>
				<li><strong class="bold">Extensive model repositories</strong>: The Hub includes pre-trained models for a variety of tasks, such as text classification, translation, summarization, and question answering, providing a wide range of options for developers.</li>
				<li><strong class="bold">Datasets</strong>: It provides access to a diverse array of datasets that are crucial for training and evaluating ML models. Datasets cover multiple domains and languages, supporting the development of robust and versatile AI applications.</li>
				<li><strong class="bold">Community and collaboration</strong>: The platform supports collaboration by allowing users to share models, datasets, and code. Developers can contribute to the community by uploading their own models and datasets, fostering a collaborative environment.</li>
				<li><strong class="bold">Integration and deployment options</strong>: The Hugging Face Hub integrates seamlessly with popular ML frameworks, such as PyTorch and TensorFlow. The Hub also provides deployment solutions, enabling developers to deploy their models in production environments easily.</li>
			</ul>
			<p>GenAI application developers can use the <strong class="bold">Hugging Face Transformers APIs</strong> to get access to thousands of pre-trained ML models on specific datasets for specific tasks. With transformer models, you can use pre-trained models for inference or fine-tune them with your own data using PyTorch and TensorFlow libraries.</p>
			<p>To illustrate what is possible for your GenAI application, let’s see how to use a pre-trained transformer model for inference in order to perform two tasks: basic sentiment analysis and text generation. Both could be useful for your GenAI projects if you, for instance, want to sort customer feedback or score it based on sentiment and generate a response.</p>
			<h3>Sentiment analysis</h3>
			<p>You’ll use the <code>transformers</code> library to utilize shared models, then explore the <code>pipeline()</code> function, the core component of the <code>transformers</code> library. This function seamlessly integrates the model with necessary pre-processing and post-processing steps, enabling direct text input and generating intelligible responses:</p>
			<ol>
				<li>First, ensure you have the necessary packages installed. Note that at least one of TensorFlow or PyTorch should be installed. Here, let’s use TensorFlow:<pre class="source-code">
pip3 install transformers tensorflow</pre></li>				<li>Next, import the <code>pipeline()</code> function. You’ll also create an instance of the <code>pipeline()</code> function and specify the task you want to use it for, that is, sentiment analysis:<pre class="source-code">
from transformers import pipeline
analyse_sentiment = pipeline(«sentiment-analysis»)
analyse_sentiment("The weather is very nice today.")</pre><p class="list-inset">You’ll receive the following output:</pre></li>			</ol>
			<div><div><img alt="" role="presentation" src="img/B22495_07_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5: Hugging Face Transformers sentiment analysis output</p>
			<p class="list-inset">The model performs the analysis and outputs a label and a score. The <code>label</code> indicates the sentiment type as positive or negative, and the <code>score</code> indicates the degree of confidence in the output.</p>
			<p class="list-inset">You can also pass multiple input texts as an array for sentiment classification to the model:</p>
			<pre class="source-code">
analyse_sentiment(["The weather is very nice today.", "I don't like it when it rains in winter."])</pre>			<p class="list-inset">You’ll receive the following as the output:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_07_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6: Multiple input texts for sentiment classification in Hugging Face</p>
			<p>In this case, the model outputs an array of objects. Each output object corresponds to the individual text inputs.</p>
			<p>You might be holding your breath, expecting things to become more complicated—but they won’t. You conducted your first sentiment analysis in Hugging Face with a pre-trained model with just those few lines of code.</p>
			<h3>Text generation</h3>
			<p>In addition to sentiment analysis, you can also perform many other NLP tasks with Transformers libraries, such as text generation. Here, you will provide a prompt, and the model will auto-complete it by generating the remaining text:</p>
			<pre class="source-code">
generator = pipeline("text-generation")
generator("I love AI, it has")</pre>			<p>You’ll get the following output for the preceding code:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_07_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7: Text generation using the Hugging Face Transformers</p>
			<p>Since you did not provide a model name to the pipeline instance, it decided to use the default, which in this case is GPT-2. You may or may not get the same results as the ones here because text generation involves some randomness. Again, however, you can see how easy this task was.</p>
			<p>Next, specify a model name to be used in the <code>pipeline</code> function at the time of text generation. With the following code, you provide some more custom details, such as the number of different sequences to be generated and the maximum length of the output texts:</p>
			<pre class="source-code">
generator = pipeline("text-generation", model="distilgpt2")
generator(
    "I love AI, it has",
    max_length=25,
    num_return_sequences=2,
)</pre>			<p>With these additional parameters provided, you’ll now receive the following output:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_07_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8: Hugging Face text generation output with parameters</p>
			<p>The preceding code outputs two different pairs of text, each having fewer than 25 words.</p>
			<p>As you might expect, Hugging Face offers many more tools and functionalities that developers can use to build their GenAI applications. With its comprehensive library support and active community, Hugging Face continues to be a pivotal resource for advancing NLP and ML projects. Additionally, its seamless integration with various AI/ML frameworks ensures that developers can efficiently deploy and scale their GenAI models with minimal effort and maximum productivity.</p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor179"/>Summary</h1>
			<p>In this chapter, you looked at the evolution of AI/ML frameworks in the Python space as LLM-powered applications have gained prominence. You also learned why Python remains a top choice for building modern LLM-powered applications. You reviewed the most popular Python frameworks, libraries, and APIs that can assist you in the different stages of GenAI application development.</p>
			<p>The GenAI space is evolving so rapidly that by the time this book is published, there will probably be more libraries available, more APIs in use, and the framework’s capabilities will have expanded. You owe it to yourself to do your own due diligence about which framework is best suited for your business needs, but also make sure to choose one that is appropriately supported. As with any rapidly evolving technology, some of the tools and technologies that are in existence today will be gone tomorrow. This chapter has tried, therefore, to only include those that have the community, enablement, and feature set to ensure their longevity.</p>
			<p>Undoubtedly there is still plenty of innovation to be done, and new tools to be created, even in the short term—the tools discussed in this chapter are barely the tip of the iceberg. So, take a deep breath and begin your own discovery. You will inevitably realize that there are tools you need, and that you have too many choices on how to fulfill those needs.</p>
			<p>In the next chapter, you will explore how to leverage the vector search feature of MongoDB Atlas to create intelligent applications. You will learn about RAG architecture systems and gain a deeper understanding of various complex RAG architecture patterns with MongoDB Atlas.</p>
		</div>
	</body></html>