<html><head></head><body><div><div><div><h1 id="_idParaDest-94" class="chapter-number"><a id="_idTextAnchor108"/>7</h1>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor109"/>Training Pipeline</h1>
			<p>In this chapter, we’ll explore the key components of an LLM training pipeline, from data ingestion and preprocessing to model architecture and optimization strategies.</p>
			<p>You’ll gain insights into implementing effective monitoring and logging systems, ensuring you can track your model’s progress and make data-driven decisions throughout the training process.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Components of a training pipeline</li>
				<li>Data input and preprocessing</li>
				<li>LLM architecture design considerations</li>
				<li>Loss functions and optimization strategies</li>
				<li>Logging</li>
				<li>Pipeline modularity and reusability</li>
				<li>Scaling your training pipeline for larger models</li>
			</ul>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor110"/>Components of a training pipeline</h1>
			<p>An LLM training pipeline<a id="_idIndexMarker306"/> consists of several interconnected steps, each playing a role in the model’s development. We’ll present a basic pipeline here and explore many of these components in further depth as we progress through the chapter:</p>
			<ul>
				<li><strong class="bold">Dataset creation</strong>: Builds <a id="_idIndexMarker307"/>preprocessed data into a format suitable for training, often involving shuffling and batching.</li>
				<li><strong class="bold">Model architecture</strong>: Defines the <a id="_idIndexMarker308"/>structure of the LLM, including the number of layers, attention mechanisms, and other architectural choices.</li>
				<li><strong class="bold">Training loop</strong>: The <a id="_idIndexMarker309"/>core of the pipeline where the model learns from the data through forward and backward passes.</li>
				<li><strong class="bold">Optimization</strong>: Handles parameter <a id="_idIndexMarker310"/>updates based on calculated gradients and chosen optimization strategies.</li>
				<li><strong class="bold">Evaluation</strong>: Regularly <a id="_idIndexMarker311"/>assesses model performance on validation data to track progress and prevent overfitting. We will cover this topic in more detail in <a href="B31249_14.xhtml#_idTextAnchor230"><em class="italic">Chapter 14</em></a>.</li>
				<li><strong class="bold">Checkpointing</strong>: Periodically<a id="_idIndexMarker312"/> saves model states to resume training or use for inference. We will cover this topic in detail in <a href="B31249_10.xhtml#_idTextAnchor162"><em class="italic">Chapter 10</em></a>.</li>
				<li><strong class="bold">Logging and monitoring</strong>: Continuously <a id="_idIndexMarker313"/>tracks training metrics and resource utilization.</li>
			</ul>
			<p>We’ll implement a basic <a id="_idIndexMarker314"/>LLM training pipeline using PyTorch and the Transformers library:</p>
			<pre class="source-code">
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AdamW,
    get_linear_schedule_with_warmup
from datasets import load_dataset
import torch
from torch.nn import functional as F
import wandb</pre>			<p>PyTorch<a id="_idIndexMarker315"/> is a popular deep learning framework that enables building neural networks through a dynamic computational graph, while the Transformers library implements the popular transformer architecture we discussed in <a href="B31249_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>.</p>
			<p>The following code block<a id="_idIndexMarker316"/> demonstrates the loading of a Wikipedia dataset and the tokenization of its text content using a pre-trained GPT-2 tokenizer:</p>
			<pre class="source-code">
# Dataset Creation: Ingestion and Preprocessing
dataset = load_dataset("wikipedia", "20220301.en", split="train")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True,
        max_length=512, padding="max_length")
tokenized_dataset = dataset.map(preprocess_function,
    batched=True, remove_columns=dataset.column_names)</pre>			<p>In the preceding code block, we’re setting up the data ingestion and preprocessing components of our pipeline. We use the Hugging Face Datasets library to load a Wikipedia dataset, which provides a large corpus of text suitable for training an LLM. We then initialize a tokenizer based on<a id="_idIndexMarker317"/> the <strong class="bold">GPT-2 model</strong>, which will be used to preprocess our text data.</p>
			<p>The <code>preprocess_function</code> defined above takes raw text examples and tokenizes them, truncating to a maximum length of 512 tokens and padding shorter sequences to this length. This ensures all our input sequences have the same length, which is necessary for efficient batch processing. We choose a <code>max_length</code> value of <code>512</code> as a balance between context length and memory efficiency. Longer sequences provide more context but require more memory and computation. Some recent LLM models, such as <strong class="bold">Gemini 1.5 Pro</strong>, can <a id="_idIndexMarker318"/>get as many as 2 million tokens in content length (<a href="https://cloud.google.com/vertex-ai/generative-ai/docs/long-context">https://cloud.google.com/vertex-ai/generative-ai/docs/long-context</a>).</p>
			<p>Next, we create our training DataLoader, which will handle batching and shuffling of our dataset during training:</p>
			<pre class="source-code">
# Dataset Creation: Loading
train_dataloader = DataLoader(
    tokenized_dataset, shuffle=True, batch_size=8)</pre>			<p>We set the batch size to <code>8</code>, which is chosen as a balance between memory usage and training efficiency. Larger batch sizes can lead to faster training but require more GPU memory. For LLMs, which often have a large number of parameters, smaller batch sizes are often necessary to fit the model and data in GPU memory.</p>
			<p>We then initialize our model architecture using the pre-trained GPT-2 model. This gives us a strong starting point for our LLM, leveraging the knowledge already captured in the pre-trained weights. Using a pre-trained model as a starting point is a common practice in transfer learning, allowing us to benefit from the general language understanding learned by the model on a large corpus of text. See the following code:</p>
			<pre class="source-code">
# Model Architecture
model = AutoModelForCausalLM.from_pretrained("gpt2")
# Optimization
optimizer = AdamW(model.parameters(), lr=5e-5)</pre>			<p>As shown in the preceding code, for <a id="_idIndexMarker319"/>optimization, we use <a id="_idIndexMarker320"/>the <code>lr</code>) to <code>5e-5</code>, which is a common choice for fine-tuning pre-trained models. The learning rate is a hyperparameter that determines the size of the adjustments made to the model’s weights during training, influencing how quickly and effectively the model learns.</p>
			<p>This learning rate offers a good balance between learning speed and stability. It’s small enough to allow for fine-grained updates to the pre-trained weights, but large enough to allow meaningful learning to occur.</p>
			<p>The subsequent code<a id="_idIndexMarker321"/> blocks outline the essential stages of training a language model, including setting up the training process, initializing a logging tool, executing the main training loop with forward and backward passes, performing evaluation to assess model performance, and saving checkpoints of the model’s parameters during training.</p>
			<ol>
				<li>We start by setting up the training loop:<pre class="source-code">
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=100,
    num_training_steps=num_training_steps)</pre></li>				<li>Then, we initialize the Weights &amp; Biases (<code>wandb</code>) library for experiment tracking and logging of training metrics:<pre class="source-code">
wandb.init(project="llm_training", name="gpt2_finetune")
device = torch.device("cuda" if torch.cuda.is_available() 
    else "cpu")
model.to(device)
for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        wandb.log({"loss": loss.item()})</pre></li>				<li>We next implement an evaluation phase to assess the model’s performance on the training data:<pre class="source-code">
    model.eval()
    eval_loss = 0
    with torch.no_grad():
        for batch in train_dataloader:  # Using training data for simplicity
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(batch)
            eval_loss += outputs.loss.item()
    eval_loss /= len(train_dataloader)
    wandb.log({"eval_loss": eval_loss})</pre></li>				<li>Finally, <a id="_idIndexMarker322"/>we save a checkpoint of the model’s state dictionary at the end of each epoch:<pre class="source-code">
    torch.save(model.state_dict(), 
        f"model_checkpoint_epoch_{epoch}.pt")
wandb.finish()</pre></li>			</ol>
			<p>These snippets implement the training loop, evaluation, checkpointing, and logging components of our pipeline:</p>
			<p>We set the<a id="_idIndexMarker323"/> number of training epochs to <code>3</code>, which means the model will iterate through the entire dataset three times during training. This hyperparameter can be adjusted based on your specific needs – increasing it may lead to better model performance if the model is underfitting, and decreasing it can help prevent overfitting and reduce training time. Monitor validation loss during training to determine the optimal number of epochs for your particular dataset and model architecture.</p>
			<p>The learning rate scheduler implements a linear decay with warmup, which helps stabilize training in the early stages and then gradually reduces the learning rate to fine-tune the model more precisely. The learning rate controls how much a model adjusts its internal parameters during training – a higher rate means bigger adjustments but potential overshooting, while a lower rate means more precise but slower learning.</p>
			<p>We use <code>wandb</code>) for logging, which allows us to track our training progress in real time and <a id="_idIndexMarker324"/>compare different runs (<a href="https://wandb.ai/site">https://wandb.ai/site</a>). This is crucial for monitoring the training process and making informed decisions about hyperparameter tuning and model architecture changes.</p>
			<p>The training loop iterates over our data for the specified number of epochs. In each iteration, we do the following:</p>
			<ol>
				<li>Move the batch to the appropriate device (GPU if available)</li>
				<li>Perform a forward pass through the model</li>
				<li>Calculate the loss</li>
				<li>Perform backpropagation</li>
				<li>Update the model parameters</li>
				<li>Update the learning rate scheduler</li>
				<li>Log the training loss</li>
			</ol>
			<p>After each epoch, we perform a simple evaluation of the training data (in a real scenario, you’d use a separate validation set), log the evaluation loss, and save a checkpoint of the model. Checkpointing is needed for long-running training processes, allowing us to resume training from a saved state if needed.</p>
			<p>As we’ve seen, the training pipeline involves several essential steps. Before the model architecture and training loop can function effectively, however, we must address data input and preprocessing, which we will discuss next.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor111"/>Data input and preprocessing</h1>
			<p>Efficient data handling is <a id="_idIndexMarker325"/>crucial for LLM training, as we discussed in <em class="italic">Part 1</em> of this book. Here, let’s explore advanced techniques for data input and preprocessing:</p>
			<ol>
				<li>Import the required Python packages:<pre class="source-code">
from datasets import load_dataset, concatenate_datasets
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
import numpy as np</pre></li>				<li>Load and combine multiple datasets:<pre class="source-code">
wiki_dataset = load_dataset("wikipedia", "20220301.en", split="train")
books_dataset = load_dataset("bookcorpus", split="train")
# Combine datasets
combined_dataset = concatenate_
    datasets([wiki_dataset, books_dataset])</pre></li>				<li>Initialize the tokenizer and perform <code>preprocess</code>:<pre class="source-code">
tokenizer = AutoTokenizer.from_pretrained("gpt2")
def preprocess_function(examples):
    # Tokenize the texts
    tokenized = tokenizer(
        examples["text"], truncation=True, max_length=1024)
    # Create input_ids and attention_mask
    input_ids = tokenized["input_ids"]
    attention_mask = tokenized["attention_mask"]
    # Create labels for causal language modeling
    labels = [
        ids[1:] + [tokenizer.eos_token_id] for ids in input_ids]
    return {"input_ids": input_ids, 
        "attention_mask": attention_mask, "labels": labels}
# Apply preprocessing
tokenized_dataset = combined_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=combined_dataset.column_names,
    num_proc=4  # Adjust based on your CPU cores
)</pre></li>				<li>Create <a id="_idIndexMarker326"/>the DataLoader:<pre class="source-code">
train_dataloader = DataLoader(
    tokenized_dataset,
    shuffle=True,
    batch_size=16,
    collate_fn=lambda x: {k: np.stack([xi[k] for xi in x]) 
        for k in x[0]}
)</pre></li>			</ol>
			<p>In this enhanced preprocessing pipeline, we’re loading multiple datasets to increase the diversity of our training data. This is needed for LLMs, as a diverse dataset helps the model learn a broader range of language patterns and knowledge.</p>
			<p>We use a longer <code>max_length</code> value of <code>1024</code> tokens to provide more context to the model. This increased context length allows the model to capture longer-range dependencies in the text, which can be beneficial for many language-understanding tasks. However, it also increases memory usage and computational requirements, so there’s a trade-off to consider.</p>
			<p>The <code>preprocess_function</code> now creates labels for causal language modeling by shifting the input sequences. This is a common approach for training language models, where the model’s task is to predict the next token given the previous tokens. During preprocessing, handling edge cases such as emojis, URLs, and non-standard characters can enhance model performance. Emojis can convey nuanced emotions and context, requiring appropriate encoding or tokenization to preserve their meaning without introducing noise. URLs often contain valuable information but can vary widely in structure, so they might be replaced with placeholder tokens to maintain consistency while preventing the model from overfitting to specific links. Non-standard characters, including symbols from different languages or special punctuation, need careful normalization or removal to reduce complexity and avoid confusion during training. By addressing these edge cases through strategies such as normalization, token replacement, and selective filtering, preprocessing pipelines can better prepare diverse and complex data, enhancing the robustness and accuracy of the resulting language models.</p>
			<p>We use multiprocessing (<code>num_proc=4</code>) to speed up the preprocessing. The number of processes should be adjusted based on your CPU cores and available memory. Multiprocessing can significantly reduce preprocessing time, especially for large datasets.</p>
			<p>The batch size is<a id="_idIndexMarker327"/> increased to <code>16</code>, which is more suitable for larger GPU memory. The custom <code>collate_fn</code> in the DataLoader ensures proper batching of our preprocessed data. This function stacks the arrays for each key in the batch, creating tensor-like structures that can be efficiently processed by PyTorch.</p>
			<p>With the data appropriately prepared, we now turn our attention to the LLM architecture design considerations, which dictate the model’s capacity to effectively learn from and understand data input.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor112"/>LLM architecture design considerations</h1>
			<p>When <a id="_idIndexMarker328"/>designing the architecture for an LLM, several factors come into play.</p>
			<p>Here are the key factors influencing LLM architecture:</p>
			<ul>
				<li><strong class="bold">Vocabulary size</strong>: Determines the size of the input and output embedding layers</li>
				<li><strong class="bold">Maximum sequence length (context size)</strong>: Defines the amount of preceding text the model can consider</li>
				<li><strong class="bold">Embedding dimension</strong>: Specifies the size of each token’s vector representation, influencing the model’s ability to capture information</li>
				<li><strong class="bold">Number of transformer layers</strong>: Represents the depth of the network, impacting the complexity of patterns the model can learn</li>
				<li><strong class="bold">Number of attention heads</strong>: Allows the model to attend to different parts of the input simultaneously</li>
				<li><strong class="bold">Model size (number of parameters)</strong>: Overall capacity of the model, influenced by embedding dimension, number of layers, and attention heads</li>
				<li><strong class="bold">Dataset size</strong>: The amount and diversity of training data</li>
				<li><strong class="bold">Number of training steps</strong>: The duration of the optimization process</li>
				<li><strong class="bold">Computational resources</strong>: Hardware constraints that affect model size, training speed, and overall feasibility.</li>
				<li><strong class="bold">Risk of overfitting</strong>: Higher with larger models and smaller datasets</li>
				<li><strong class="bold">Data quality</strong>: The cleanliness and relevance of the training data</li>
				<li><strong class="bold">Efficiency of model architecture</strong>: Design choices that can improve performance without drastically increasing model size</li>
				<li><strong class="bold">Training algorithms</strong>: Optimization techniques and strategies</li>
				<li><strong class="bold">Data curation practices</strong>: Methods for selecting and preparing training data</li>
				<li><strong class="bold">Test time compute</strong>: Computational <a id="_idIndexMarker329"/>resources available during inference</li>
			</ul>
			<p>In the following code block, we provide examples of configuring some of these factors using a GPT-2 style language model, specifying key architectural parameters.</p>
			<pre class="source-code">
from transformers import GPT2Config, GPT2LMHeadModel
# Define custom model configuration
config = GPT2Config(
    vocab_size=50257,  # GPT-2 vocabulary size
    n_positions=1024,  # Maximum sequence length
    n_ctx=1024,        # Context size
    n_embd=768,        # Embedding dimension
    n_layer=12,        # Number of transformer layers
    n_head=12          # Number of attention heads
)
# Initialize the model with custom configuration
model = GPT2LMHeadModel(config)
print(f"Model parameters: {model.num_parameters():,}")</pre>			<p>This configuration creates<a id="_idIndexMarker330"/> a GPT-2 style model with <code>12</code> layers and <code>12</code> attention heads. Let’s break down the key parameters:</p>
			<ul>
				<li><code>vocab_size</code>: Set to <code>50257</code>, which is the vocabulary size of the original GPT-2 model. This determines the size of the embedding layer and the output layer.</li>
				<li><code>n_positions</code> and <code>n_ctx</code>: Both are set to <code>1024</code>, matching our preprocessing step. This defines the maximum sequence length the model can handle.</li>
				<li><code>n_embd</code>: The embedding dimension, set to <code>768</code>. This determines the size of the hidden states throughout the model.</li>
				<li><code>n_layer</code>: The number of transformer layers, set to <code>12</code>. More layers can capture more complex patterns but increase computational requirements.</li>
				<li><code>n_head</code>: The number of attention heads, set to <code>12</code>. Multiple attention heads allow the model to focus on different aspects of the input simultaneously.</li>
			</ul>
			<p>The embedding dimension of <code>768</code> and the <code>12</code> layers provide a balanced trade-off between model capacity and computational efficiency. This configuration results in a model with about 124 million parameters, which is substantial but still trainable on common GPU hardware.</p>
			<p>For larger models, you might increase <code>n_layer</code>, <code>n_embd</code>, and <code>n_head</code>. However, this would also increase the computational requirements and the risk of overfitting, especially on smaller datasets. When scaling up, consider techniques such as gradient accumulation, mixed precision training, and distributed training to manage the increased computational load.</p>
			<p>In a broader scope, <strong class="bold">scaling laws</strong> can <a id="_idIndexMarker331"/>be considered. The scaling laws for LLMs describe how performance improves predictably as three key factors increase: model size (number of parameters), dataset size (amount of training data), and the number of training steps (optimization iterations). Specifically, larger models tend to capture more complex patterns and exhibit better generalization, larger datasets provide more diverse information for learning, and more training steps allow the model to refine its understanding and reduce errors. For optimal performance, these factors should scale proportionally – for instance, increasing the model size should be matched by a corresponding increase in dataset size and training steps. This balanced scaling ensures that each component supports the others, preventing bottlenecks such as overfitting smaller models on vast datasets or undertraining large models with insufficient data.</p>
			<p>However, recent <a id="_idIndexMarker332"/>advancements and practical challenges have shown that simply scaling these factors is not always sufficient for continual performance improvements. Issues such as diminishing returns, where each additional parameter or data point contributes less to overall performance, have become more apparent. Additionally, the immense computational and energy resources required for training increasingly large models raise sustainability and accessibility concerns. Data quality also becomes a critical factor, as larger datasets may introduce more noise and biases, potentially degrading model performance. For more details about this, please see the article at <a href="https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/">https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/</a>.</p>
			<p>To address these challenges, researchers are exploring more efficient model architectures, improved training algorithms, better data curation practices, and test time compute. See my Medium article for more details on test time compute: <a href="https://kenhuangus.medium.com/test-time-compute-3633a4c55716">https://kenhuangus.medium.com/test-time-compute-3633a4c55716</a></p>
			<p>At the beginning of 2025, DeepSeek (an AI startup in China) announced some model training innovations by <a id="_idIndexMarker333"/>introducing a suite of techniques aimed at significantly increasing efficiency and reducing costs, while simultaneously enhancing the model’s reasoning capabilities (<a href="https://arxiv.org/abs/2501.12948">https://arxiv.org/abs/2501.12948</a>). Unlike traditional approaches that rely heavily on vast computational resources and human-supervised fine-tuning, DeepSeek leverages large-scale reinforcement learning focused on reasoning tasks, using automated reward systems rather than human feedback. Key innovations include multi-token prediction, which allows the model to learn from multiple future tokens at once, increasing sample efficiency, and speeding up training. DeepSeek also employs a mixture-of-experts architecture to activate only relevant sub-networks for each task, thus reducing computational load. By optimizing both algorithms and hardware, DeepSeek has managed to train highly capable models at a fraction of the cost and time required by competitors, setting new standards for open, efficient, and powerful AI<a id="_idIndexMarker334"/> development.</p>
			<p>Having explored the architectural design considerations and model training innovations for LLMs — along with a code example demonstrating how to configure model training parameters — we are now ready to examine how these architectural choices are actually learned during training. In this next section, we will discuss the loss function and optimization strategies, which serve as the engine that drives the model to adjust its internal parameters based on both the training data and the architecture we have defined.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor113"/>Loss functions and optimization strategies</h1>
			<p>LLMs typically use <strong class="bold">cross-entropy loss</strong> for training. This approach measures the difference between the model’s <a id="_idIndexMarker335"/>predicted probability distribution of words and the actual <a id="_idIndexMarker336"/>distribution in the training data. By minimizing this loss, LLMs learn to generate more accurate and contextually appropriate text. Cross-entropy loss is particularly well-suited for language tasks due to its ability to handle the high dimensionality and discrete nature of textual data.</p>
			<p>Let’s implement this along<a id="_idIndexMarker337"/> with some advanced optimization techniques:</p>
			<ol>
				<li>First, we import the required PyTorch libraries and specific modules from the Transformers library for optimization:<pre class="source-code">
import torch
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup</pre></li>				<li>Next, we configure the AdamW optimizer with a specified learning rate and weight decay:<pre class="source-code">
optimizer = AdamW(model.parameters(), lr=5e-5, 
    weight_decay=0.01)</pre></li>				<li>Then, we define a linear learning rate scheduler with a warm-up period:<pre class="source-code">
num_epochs = 3
total_steps = len(train_dataloader) * num_epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=total_steps
)</pre></li>				<li>Subsequently, we<a id="_idIndexMarker338"/> set up the training device <a id="_idIndexMarker339"/>and initiate the main training loop:<pre class="source-code">
device = torch.device("cuda" if torch.cuda.is_available() 
    else "cpu")
model.to(device)
for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        batch = {k: torch.tensor(v).to(device) 
            for k, v in batch.items()
        }
        outputs = model(batch)
        loss = outputs.loss
        loss.backward()</pre></li>				<li>Finally, we implement gradient clipping to prevent exploding gradients during training:<pre class="source-code">
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()</pre></li>			</ol>
			<p>In this optimization<a id="_idIndexMarker340"/> setup, we use the <code>AdamW</code> optimizer with a learning rate of <code>5e-5</code> and weight decay of <code>0.01</code>. The algorithm adapts the learning rate for each parameter based on the first and second moments of the gradients, allowing it to handle sparse gradients effectively. This makes <code>AdamW</code> particularly useful for<a id="_idIndexMarker341"/> training large neural networks.</p>
			<p>The weight decay of <code>0.01</code> adds a small regularization term to the loss function, which can help prevent overfitting by penalizing large weight values.</p>
			<p>We implement a <code>warmup</code>. The <code>warmup</code> phase helps stabilize training in<a id="_idIndexMarker342"/> the early stages by gradually increasing the learning rate from a very small value. After the <code>warmup</code> phase, the learning rate decreases linearly. This schedule can help the model converge to a better optimum.</p>
			<p>In the training loop, we<a id="_idIndexMarker343"/> implement <code>max_norm</code> value of <code>1.0</code>. Gradient clipping prevents exploding gradients by scaling down gradient values that exceed a certain threshold. This is particularly important for LLMs, which can be prone to unstable gradients due to their depth and the long-range dependencies they capture.</p>
			<p>In this section, we learned about AdamW optimization, learning rate scheduling with warmup, and gradient clipping for stable LLM training. Next, we talk about logging the training process, which is crucial for monitoring progress and using tools like <strong class="bold">TensorBoard</strong> to gain insights<a id="_idIndexMarker344"/> for improvement.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor114"/>Logging</h1>
			<p>Effective logging<a id="_idIndexMarker345"/> can be useful for tracking the progress of LLM training.</p>
			<p>The following code blocks demonstrate how to integrate TensorBoard for effective logging during the training of an LLM using PyTorch. Let’s break down each part.</p>
			<ol>
				<li>We first initialize the TensorBoard <code>SummaryWriter</code> for logging training progress:<pre class="source-code">
from torch.utils.tensorboard import SummaryWriter
import time
# Initialize TensorBoard writer
writer = SummaryWriter()</pre></li>				<li>Then, we set the model to training mode, initialize variables for tracking loss, define the logging interval, and record the start time to monitor training performance:<pre class="source-code">
model.train()
total_loss = 0
log_interval = 100
start_time = time.time()</pre></li>				<li>Then, we move on to the training loop. We process each batch by moving data to the appropriate device, performing forward and backward passes, applying gradient clipping, and updating the model’s parameters using the optimizer and scheduler:<pre class="source-code">
for i, batch in enumerate(train_dataloader):
    batch = {k: torch.tensor(v).to(device) 
        for k, v in batch.items()}
    outputs = model(batch)
    loss = outputs.loss
    total_loss += loss.item()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 
        max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()</pre></li>				<li>We log the training metrics to TensorBoard at specified intervals, calculate the average loss, measure the elapsed time, print the progress to the console, and reset the tracking variables for the next interval:<pre class="source-code">
    if (i + 1) % log_interval == 0:
        cur_loss = total_loss / log_interval
        elapsed = time.time() - start_time
        writer.add_scalar(
            'training_loss', cur_loss, global_step=i
        )
        writer.add_scalar(
            'learning_rate', scheduler.get_last_lr()[0], 
            global_step=i
        )
        print(
            f'| epoch {epoch:3d} '
            f'| {i:5d}/{len(train_dataloader):5d} batches | '
            f'lr {scheduler.get_last_lr()[0]:02.2f} | '
            f'ms/batch {elapsed * 1000 / log_interval:5.2f} | '
            f'loss {cur_loss:5.2f}'
        )
        total_loss = 0
        start_time = time.time()
writer.close()</pre><p class="list-inset">This enhanced training loop uses TensorBoard for logging training loss and learning rate. TensorBoard is a powerful tool for visualizing training progress and comparing different runs. We log the following metrics:</p><ul><li><code>log_interval</code> batches. A decreasing trend in this metric indicates that the model is learning.</li><li><strong class="bold">Learning rate</strong>: We log the <a id="_idIndexMarker347"/>current learning rate to visualize how it changes over time due to our learning rate scheduler.</li></ul></li>			</ol>
			<p>We set <code>log_interval</code> to <code>100</code>, meaning we log and print out progress information every 100 batches. This interval strikes a balance between getting frequent updates and not slowing down training too much with logging operations. You may need to adjust this based on your dataset size and training speed.</p>
			<p>The output or log information includes the following:</p>
			<ul>
				<li>Current epoch and batch number</li>
				<li>Current learning rate</li>
				<li>Time per batch (in milliseconds)</li>
				<li>Current<a id="_idIndexMarker348"/> loss</li>
			</ul>
			<p>This detailed logging allows you to monitor the training process closely, helping you identify issues such as unstable loss, learning rate problems, or unexpectedly slow training.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor115"/>Pipeline modularity and reusability</h1>
			<p><strong class="bold">Modularity</strong> and <strong class="bold">reusability</strong> are<a id="_idIndexMarker349"/> fundamental principles for building efficient<a id="_idIndexMarker350"/> pipelines because they make code more maintainable, adaptable, and reliable. By breaking down a pipeline into independent, reusable modules (such as data preprocessing, model training, and evaluation components), developers can easily modify individual parts without affecting others, test each component separately, and reuse proven code across different projects.</p>
			<p>This approach not only saves development time but also ensures consistency in operations, reduces the chance of errors, and makes it easier for teams to collaborate by working on separate modules while maintaining clear interfaces between components. In the case of training pipelines, encapsulating processes in reusable classes allows for flexible configuration, seamless integration with different datasets, and straightforward sharing of standardized implementations across multiple projects.</p>
			<p>To make our pipeline <a id="_idIndexMarker351"/>more modular and reusable, let’s encapsulate our<a id="_idIndexMarker352"/> training process in a class:</p>
			<ol>
				<li>We start with class definition:<pre class="source-code">
class LLMTrainer:
    def __init__(self, model, train_dataloader, optimizer,
    scheduler, device
    ):
        self.model = model
        self.train_dataloader = train_dataloader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.writer = SummaryWriter()</pre></li>				<li>Then, we define the train epoch function. The function sets the model to training mode and iterates over the training data, processing each batch by computing the loss, performing backpropagation with gradient clipping, and updating the model parameters using the optimizer and scheduler:<pre class="source-code">
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        log_interval = 100
        start_time = time.time()
        for i, batch in enumerate(self.train_dataloader):
            batch = {k: torch.tensor(v).to(self.device)
                for k, v in batch.items()
            }
            outputs = self.model(batch)
            loss = outputs.loss
            total_loss += loss.item()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()</pre></li>				<li>Next, we periodically log<a id="_idIndexMarker353"/> the training progress to both <a id="_idIndexMarker354"/>TensorBoard and the console by checking if the current batch index is a multiple of the <code>log_interval</code>; if it is, we calculate the average loss and elapsed time since the last log, record the training loss and learning rate to TensorBoard using the <code>SummaryWriter</code>, print a formatted progress update including batch number, learning rate, milliseconds per batch, and current loss to the console, and then reset the accumulated <code>total_loss</code> and <code>start_time</code> for the next logging interval:<pre class="source-code">
            if (i + 1) % log_interval == 0:
                cur_loss = total_loss / log_interval
                elapsed = time.time() - start_time
                self.writer.add_scalar(
                    'training_loss', cur_loss, global_step=i
                )
                self.writer.add_scalar(
                    'learning_rate', 
                    self.scheduler.get_last_lr()[0], 
                    global_step=i
                )
                    print(
                        f'| {i:5d}/'
                        f'{len(self.train_dataloader):5d} '
                        f'batches | '
                        f'lr '
                        f'{self.scheduler.get_last_lr()'
                        f'[0]:02.2f} | '
                        f'ms/batch '
                        f'{elapsed * 1000 / log_interval:5.2f} '
                        f'| '
                        f'loss '
                        f'{cur_loss:5.2f}'
                    )
                total_loss = 0
                start_time = time.time()</pre></li>				<li>Next, the <code>train</code> function<a id="_idIndexMarker355"/> orchestrates the training process by<a id="_idIndexMarker356"/> looping through the specified number of epochs, printing a message at the start of each epoch, invoking the <code>train_epoch</code> method to perform training for that epoch, and, finally, closing the writer once all epochs are completed. It serves as the main entry point for training, providing a structure where additional features such as validation and checkpointing can be integrated as needed:<pre class="source-code">
    def train(self, num_epochs):
        for epoch in range(num_epochs):
            print(f'Starting epoch {epoch+1}')
            self.train_epoch()
            # Here you could add validation, checkpointing, etc.
        self.writer.close()</pre></li>				<li>Lastly, we instantiate the <code>LLMTrainer</code> class with the specified model, training data loader, optimizer, scheduler, and device. Then, the training process is started by calling the <code>train</code> method to execute three full training epochs, thereby initiating and managing the model’s learning cycle:<pre class="source-code">
trainer = LLMTrainer(model, train_dataloader,
    optimizer, scheduler, device)
trainer.train(num_epochs=3)</pre></li>			</ol>
			<p>This modular design <a id="_idIndexMarker357"/>offers several advantages:</p>
			<ul>
				<li><code>LLMTrainer</code> class, making it easier to manage and understand.</li>
				<li><strong class="bold">Reusability</strong>: You can easily use this trainer for different models or datasets by creating a new instance with different parameters.</li>
				<li><strong class="bold">Extensibility</strong>: The class structure makes it easy to add new functionality. For example, you could add methods for validation, checkpointing, or early stopping.</li>
				<li><strong class="bold">Separation of concerns</strong>: The training logic is separated from the model definition and data preparation, following good software engineering principles.</li>
			</ul>
			<p>The following log demonstrates the training process over 3 epochs, with periodic logging every 100 batches. Each log entry includes the current batch number, total batches, learning rate, milliseconds per batch, and the average loss:</p>
			<pre class="console">
Starting epoch 1
|   100/1000 batches | lr 0.01 | ms/batch 45.67 | loss 2.35
|   200/1000 batches | lr 0.01 | ms/batch 44.89 | loss 2.10
|   300/1000 batches | lr 0.01 | ms/batch 46.12 | loss 1.95
|   400/1000 batches | lr 0.01 | ms/batch 45.50 | loss 1.80
|   500/1000 batches | lr 0.01 | ms/batch 44.75 | loss 1.65
|   600/1000 batches | lr 0.009 | ms/batch 45.30 | loss 1.50
|   700/1000 batches | lr 0.009 | ms/batch 44.95 | loss 1.40
|   800/1000 batches | lr 0.009 | ms/batch 45.10 | loss 1.30
|   900/1000 batches | lr 0.009 | ms/batch 45.00 | loss 1.25
|  1000/1000 batches | lr 0.009 | ms/batch 44.80 | loss 1.20
Starting epoch 2
|   100/1000 batches | lr 0.009 | ms/batch 44.60 | loss 1.18
|   200/1000 batches | lr 0.009 | ms/batch 44.70 | loss 1.15
|   300/1000 batches | lr 0.009 | ms/batch 44.80 | loss 1.12
|   400/1000 batches | lr 0.008 | ms/batch 44.50 | loss 1.10
|   500/1000 batches | lr 0.008 | ms/batch 44.60 | loss 1.08
|   600/1000 batches | lr 0.008 | ms/batch 44.55 | loss 1.05
|   700/1000 batches | lr 0.008 | ms/batch 44.65 | loss 1.03
|   800/1000 batches | lr 0.007 | ms/batch 44.50 | loss 1.00
|   900/1000 batches | lr 0.007 | ms/batch 44.60 | loss 0.98
|  1000/1000 batches | lr 0.007 | ms/batch 44.55 | loss 0.95
Starting epoch 3
|   100/1000 batches | lr 0.007 | ms/batch 44.50 | loss 0.93
|   200/1000 batches | lr 0.007 | ms/batch 44.60 | loss 0.90
|   300/1000 batches | lr 0.006 | ms/batch 44.55 | loss 0.88
|   400/1000 batches | lr 0.006 | ms/batch 44.50 | loss 0.85
|   500/1000 batches | lr 0.006 | ms/batch 44.60 | loss 0.83
|   600/1000 batches | lr 0.006 | ms/batch 44.55 | loss 0.80
|   700/1000 batches | lr 0.005 | ms/batch 44.50 | loss 0.78
|   800/1000 batches | lr 0.005 | ms/batch 44.60 | loss 0.75
|   900/1000 batches | lr 0.005 | ms/batch 44.55 | loss 0.73
|  1000/1000 batches | lr 0.005 | ms/batch<a id="_idTextAnchor116"/> 44.50 | loss 0.70
Training completed. Writer closed.</pre>			<p>Here is an<a id="_idIndexMarker359"/> explanation of the <a id="_idIndexMarker360"/>above simulated log:</p>
			<ul>
				<li><code>Starting epoch 1</code>, indicating the commencement of a new training cycle</li>
				<li><code>100/1000 batches</code></li><li><code>lr 0.01</code></li><li><code>ms/batch 45.67</code></li><li><code>loss 2.35</code></li></ul></li>
				<li><strong class="bold">Learning rate schedule</strong>: Notice how the learning rate decreases over epochs, reflecting the scheduler’s adjustments to facilitate better convergence</li>
				<li><code>Training completed. Writer closed.</code>) indicates the end of the training process and the closure of the logging writer</li>
			</ul>
			<p>The log provides a clear overview of the training dynamics, allowing developers and researchers to monitor the model’s learning progress, adjust hyperparameters if necessary, and ensure that the training is proceeding as expected.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor117"/>Scaling your training pipeline for larger models</h1>
			<p>To train larger models, we need <a id="_idIndexMarker361"/>to employ techniques such as gradient accumulation and mixed precision training.</p>
			<p>To train very large language models that might not fit on a single GPU, the following code introduces a special <code>LargeScaleLLMTrainer</code>. It uses two main tricks to handle this:</p>
			<p>First, gradient accumulation <a id="_idIndexMarker362"/>allows us to simulate having access to a larger GPU. Instead of updating the model's parameters after every small batch of data, we process several small batches, accumulating their gradients along the way. Only after a predefined number of batches do we perform an actual update to the model's parameters. This technique enables the model to learn as if it had seen a much larger batch of data, without requiring the memory capacity of an extremely large GPU.</p>
			<p>Second, it employs mixed precision training, a technique where the computer performs many calculations using smaller, lower-precision numbers (which require less memory and are faster to compute), while reserving higher-precision numbers for situations where accuracy is critical. This approach accelerates training and reduces overall memory usage. To mitigate potential issues that can arise from using lower-precision values, GradScaler is used to maintain numerical stability during backpropagation.</p>
			<p>The following code defines how this special trainer works, including how it processes data, calculates the loss, and updates the model’s learning using these tricks. It also still includes important steps like making sure the gradients (how the model should change) don’t get too big and logging progress so we can see how the training is going. Finally, it shows a simple example of how to use this special trainer. Now, let us break it into several parts:</p>
			<ol>
				<li>Let us start by importing the relevant Python package and defining the class:<pre class="source-code">
import torch.cuda.amp as amp
class LargeScaleLLMTrainer(LLMTrainer):
    def __init__(self, model, train_dataloader,
        optimizer, scheduler, device, accumulation_steps=4
    ):
        super().__init__(model, train_dataloader,
            optimizer, scheduler, device)
        self.accumulation_steps = accumulation_steps
        self.scaler = amp.GradScaler()</pre></li>				<li>We can then <a id="_idIndexMarker363"/>define the training epoch:<pre class="source-code">
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        log_interval = 100
        start_time = time.time()
        for i, batch in enumerate(self.train_dataloader):
            batch = {
                k: torch.tensor(v).to(self.device)
                for k, v in batch.items()
            }
            with amp.autocast():
                outputs = self.model(batch)
                loss = outputs.loss / self.accumulation_steps
            self.scaler.scale(loss).backward()</pre></li>				<li>Then, we implement the following code<a id="_idIndexMarker364"/> block, which updates the model’s parameters, learning rate, and gradient scaler only after processing a defined number of batches (<code>accumulation_steps)</code>, effectively simulating a larger batch size while managing memory constraints:<pre class="source-code">
            if (i + 1) % self.accumulation_steps == 0:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), max_norm=1.0
                )
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.scheduler.step()
                self.optimizer.zero_grad()
            total_loss += loss.item() * self.accumulation_steps</pre></li>				<li>We then periodically calculate and log the average training loss and learning rate to TensorBoard, while also printing a summary of the current training progress to the console at intervals defined by <code>log_interval</code>:<pre class="source-code">
            if (i + 1) % log_interval == 0:
                cur_loss = total_loss / log_interval
                elapsed = time.time() start_time
                self.writer.add_scalar('training_loss',
                    cur_loss, global_step=i)
                self.writer.add_scalar('learning_rate',
                    self.scheduler.get_last_lr()[0],
                    global_step=i)
                print(
                    f'| {i:5d}/{len(self.train_dataloader):5d}
                        batches | '
                    f'lr {self.scheduler.get_last_lr()[0]:02.2f}
                        | '
                    f'ms/batch {elapsed * 1000 /
                        log_interval:5.2f} | '
                    f'loss {cur_loss:5.2f}'
                 )
                total_loss = 0
                start_time = time.time()</pre></li>				<li>We demonstrate the initialization and execution of a large-scale language model training process:<pre class="source-code">
large_trainer = LargeScaleLLMTrainer(
    model, train_dataloader, optimizer, scheduler, device)
large_trainer.train(num_epochs=3)</pre><p class="list-inset">This enhanced trainer uses two key techniques for scaling to larger models:</p><ul><li><code>accumulation_steps</code>). This allows us to effectively increase the batch size without increasing memory usage, which is effective for training large models on limited GPU memory. We divide the loss by <code>accumulation_steps</code> to maintain the same effective learning rate.</li><li><code>float16</code>, where possible, while <a id="_idIndexMarker367"/>maintaining <code>float32</code> master weights. This can significantly speed up training and reduce memory usage, especially on modern GPUs with tensor cores.</li></ul></li>			</ol>
			<p><code>GradScaler</code> is used to prevent underflow in <code>float16</code> calculations. It scales the loss to prevent small gradient values, then unscales before the optimizer step.</p>
			<p>We still apply gradient clipping, but now it’s done after unscaling the gradients to ensure we’re clipping the true gradient values.</p>
			<p>For even larger models, you might consider techniques such<a id="_idIndexMarker368"/> as <strong class="bold">model parallelism</strong> (splitting the model across multiple GPUs), <strong class="bold">pipeline parallelism</strong> (splitting the model into stages), or using specialized <a id="_idIndexMarker369"/>libraries such as <strong class="bold">DeepSpeed</strong> or <strong class="bold">Megatron-LM</strong>. These<a id="_idIndexMarker370"/> advanced techniques allow the training of models with <a id="_idIndexMarker371"/>billions of parameters across multiple GPUs or even multiple machines. Memory offloading can be a good alternative when GPU memory is insufficient to handle vast amounts of data and model parameters. Memory offloading involves transferring parts of the model’s data or computations to alternative <a id="_idIndexMarker372"/>memory storage, such as <strong class="bold">Non-Volatile Memory Express</strong> (<strong class="bold">NVMe</strong>) SSDs. By<a id="_idIndexMarker373"/> leveraging NVMe memory, which offers high-speed data access compared to traditional storage, systems can effectively manage and store intermediate activations, gradients, and model states that exceed GPU memory capacity. This approach allows for training larger models or using higher batch sizes without requiring immediate GPU memory expansion. However, it introduces additional latency due to data transfer between the GPU and NVMe storage, which can impact training speed. Optimizing data access patterns and utilizing efficient offloading strategies can minimize performance overhead and maintain eff<a id="_idTextAnchor118"/>ective training workflows when employing memory offloading techniques.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor119"/>Summary</h1>
			<p>In this chapter, you learned about a practical pattern of pipeline design for training LLMs. You learned how to create efficient data preprocessing workflows, implement model architectures, and apply advanced optimization strategies. You now understand how to set up effective logging systems to track your model’s progress. You also explored techniques for building modular and reusable pipelines and discovered methods for scaling your training process to accommodate larger models. With these skills, you’re well equipped to train state-of-the-art language models efficiently and effectively.</p>
			<p>In the next chapter, we’ll explore the hyperparameter tuning pattern.</p>
		</div>
	</div></div></body></html>