<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer022">
			<h1 id="_idParaDest-94" class="chapter-number"><a id="_idTextAnchor108"/>7</h1>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor109"/>Training Pipeline</h1>
			<p>In this chapter, we’ll explore the key components of an LLM training pipeline, from data ingestion and preprocessing to model architecture and <span class="No-Break">optimization strategies.</span></p>
			<p>You’ll gain insights into implementing effective monitoring and logging systems, ensuring you can track your model’s progress and make data-driven decisions throughout the <span class="No-Break">training process.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Components of a <span class="No-Break">training pipeline</span></li>
				<li>Data input <span class="No-Break">and preprocessing</span></li>
				<li>LLM architecture <span class="No-Break">design considerations</span></li>
				<li>Loss functions and <span class="No-Break">optimization strategies</span></li>
				<li><span class="No-Break">Logging</span></li>
				<li>Pipeline modularity <span class="No-Break">and reusability</span></li>
				<li>Scaling your training pipeline for <span class="No-Break">larger models</span></li>
			</ul>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor110"/>Components of a training pipeline</h1>
			<p>An LLM training pipeline<a id="_idIndexMarker306"/> consists of several interconnected steps, each playing a role in the model’s development. We’ll present a basic pipeline here and explore many of these components in further depth as we progress through <span class="No-Break">the chapter:</span></p>
			<ul>
				<li><strong class="bold">Dataset creation</strong>: Builds <a id="_idIndexMarker307"/>preprocessed data into a format suitable for training, often involving shuffling <span class="No-Break">and batching.</span></li>
				<li><strong class="bold">Model architecture</strong>: Defines the <a id="_idIndexMarker308"/>structure of the LLM, including the number of layers, attention mechanisms, and other <span class="No-Break">architectural choices.</span></li>
				<li><strong class="bold">Training loop</strong>: The <a id="_idIndexMarker309"/>core of the pipeline where the model learns from the data through forward and <span class="No-Break">backward passes.</span></li>
				<li><strong class="bold">Optimization</strong>: Handles parameter <a id="_idIndexMarker310"/>updates based on calculated gradients and chosen <span class="No-Break">optimization strategies.</span></li>
				<li><strong class="bold">Evaluation</strong>: Regularly <a id="_idIndexMarker311"/>assesses model performance on validation data to track progress and prevent overfitting. We will cover this topic in more detail in <a href="B31249_14.xhtml#_idTextAnchor230"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Checkpointing</strong>: Periodically<a id="_idIndexMarker312"/> saves model states to resume training or use for inference. We will cover this topic in detail in <a href="B31249_10.xhtml#_idTextAnchor162"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Logging and monitoring</strong>: Continuously <a id="_idIndexMarker313"/>tracks training metrics and <span class="No-Break">resource utilization.</span></li>
			</ul>
			<p>We’ll implement a basic <a id="_idIndexMarker314"/>LLM training pipeline using PyTorch and the <span class="No-Break">Transformers library:</span></p>
			<pre class="source-code">
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AdamW,
    get_linear_schedule_with_warmup
from datasets import load_dataset
import torch
from torch.nn import functional as F
import wandb</pre>			<p>PyTorch<a id="_idIndexMarker315"/> is a popular deep learning framework that enables building neural networks through a dynamic computational graph, while the Transformers library implements the popular transformer architecture we discussed in <a href="B31249_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
			<p>The following code block<a id="_idIndexMarker316"/> demonstrates the loading of a Wikipedia dataset and the tokenization of its text content using a pre-trained <span class="No-Break">GPT-2 tokenizer:</span></p>
			<pre class="source-code">
# Dataset Creation: Ingestion and Preprocessing
dataset = load_dataset("wikipedia", "20220301.en", split="train")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True,
        max_length=512, padding="max_length")
tokenized_dataset = dataset.map(preprocess_function,
    batched=True, remove_columns=dataset.column_names)</pre>			<p>In the preceding code block, we’re setting up the data ingestion and preprocessing components of our pipeline. We use the Hugging Face Datasets library to load a Wikipedia dataset, which provides a large corpus of text suitable for training an LLM. We then initialize a tokenizer based on<a id="_idIndexMarker317"/> the <strong class="bold">GPT-2 model</strong>, which will be used to preprocess our <span class="No-Break">text data.</span></p>
			<p>The <strong class="source-inline">preprocess_function</strong> defined above takes raw text examples and tokenizes them, truncating to a maximum length of 512 tokens and padding shorter sequences to this length. This ensures all our input sequences have the same length, which is necessary for efficient batch processing. We choose a <strong class="source-inline">max_length</strong> value of <strong class="source-inline">512</strong> as a balance between context length and memory efficiency. Longer sequences provide more context but require more memory and computation. Some recent LLM models, such as <strong class="bold">Gemini 1.5 Pro</strong>, can <a id="_idIndexMarker318"/>get as many as 2 million tokens in content <span class="No-Break">length (</span><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/long-context"><span class="No-Break">https://cloud.google.com/vertex-ai/generative-ai/docs/long-context</span></a><span class="No-Break">).</span></p>
			<p>Next, we create our training DataLoader, which will handle batching and shuffling of our dataset <span class="No-Break">during training:</span></p>
			<pre class="source-code">
# Dataset Creation: Loading
train_dataloader = DataLoader(
    tokenized_dataset, shuffle=True, batch_size=8)</pre>			<p>We set the batch size to <strong class="source-inline">8</strong>, which is chosen as a balance between memory usage and training efficiency. Larger batch sizes can lead to faster training but require more GPU memory. For LLMs, which often have a large number of parameters, smaller batch sizes are often necessary to fit the model and data in <span class="No-Break">GPU memory.</span></p>
			<p>We then initialize our model architecture using the pre-trained GPT-2 model. This gives us a strong starting point for our LLM, leveraging the knowledge already captured in the pre-trained weights. Using a pre-trained model as a starting point is a common practice in transfer learning, allowing us to benefit from the general language understanding learned by the model on a large corpus of text. See the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# Model Architecture
model = AutoModelForCausalLM.from_pretrained("gpt2")
# Optimization
optimizer = AdamW(model.parameters(), lr=5e-5)</pre>			<p>As shown in the preceding code, for <a id="_idIndexMarker319"/>optimization, we use <a id="_idIndexMarker320"/>the <strong class="bold">AdamW optimizer</strong>, which is an improved version of Adam that implements weight decay correctly. We set the learning rate (<strong class="source-inline">lr</strong>) to <strong class="source-inline">5e-5</strong>, which is a common choice for fine-tuning pre-trained models. The learning rate is a hyperparameter that determines the size of the adjustments made to the model’s weights during training, influencing how quickly and effectively the <span class="No-Break">model learns.</span></p>
			<p>This learning rate offers a good balance between learning speed and stability. It’s small enough to allow for fine-grained updates to the pre-trained weights, but large enough to allow meaningful learning <span class="No-Break">to occur.</span></p>
			<p>The subsequent code<a id="_idIndexMarker321"/> blocks outline the essential stages of training a language model, including setting up the training process, initializing a logging tool, executing the main training loop with forward and backward passes, performing evaluation to assess model performance, and saving checkpoints of the model’s parameters <span class="No-Break">during training.</span></p>
			<ol>
				<li>We start by setting up the <span class="No-Break">training loop:</span><pre class="source-code">
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=100,
    num_training_steps=num_training_steps)</pre></li>				<li>Then, we initialize the Weights &amp; Biases (<strong class="source-inline">wandb</strong>) library for experiment tracking and logging of <span class="No-Break">training metrics:</span><pre class="source-code">
wandb.init(project="llm_training", name="gpt2_finetune")
device = torch.device("cuda" if torch.cuda.is_available() 
    else "cpu")
model.to(device)
for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        wandb.log({"loss": loss.item()})</pre></li>				<li>We next implement an evaluation phase to assess the model’s performance on the <span class="No-Break">training data:</span><pre class="source-code">
    model.eval()
    eval_loss = 0
    with torch.no_grad():
        for batch in train_dataloader:  # Using training data for simplicity
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(batch)
            eval_loss += outputs.loss.item()
    eval_loss /= len(train_dataloader)
    wandb.log({"eval_loss": eval_loss})</pre></li>				<li>Finally, <a id="_idIndexMarker322"/>we save a checkpoint of the model’s state dictionary at the end of <span class="No-Break">each epoch:</span><pre class="source-code">
    torch.save(model.state_dict(), 
        f"model_checkpoint_epoch_{epoch}.pt")
wandb.finish()</pre></li>			</ol>
			<p>These snippets implement the training loop, evaluation, checkpointing, and logging components of <span class="No-Break">our pipeline:</span></p>
			<p>We set the<a id="_idIndexMarker323"/> number of training epochs to <strong class="source-inline">3</strong>, which means the model will iterate through the entire dataset three times during training. This hyperparameter can be adjusted based on your specific needs – increasing it may lead to better model performance if the model is underfitting, and decreasing it can help prevent overfitting and reduce training time. Monitor validation loss during training to determine the optimal number of epochs for your particular dataset and <span class="No-Break">model architecture.</span></p>
			<p>The learning rate scheduler implements a linear decay with warmup, which helps stabilize training in the early stages and then gradually reduces the learning rate to fine-tune the model more precisely. The learning rate controls how much a model adjusts its internal parameters during training – a higher rate means bigger adjustments but potential overshooting, while a lower rate means more precise but <span class="No-Break">slower learning.</span></p>
			<p>We use <strong class="bold">Weights &amp; Biases</strong> (<strong class="source-inline">wandb</strong>) for logging, which allows us to track our training progress in real time and <a id="_idIndexMarker324"/>compare different runs (<a href="https://wandb.ai/site">https://wandb.ai/site</a>). This is crucial for monitoring the training process and making informed decisions about hyperparameter tuning and model <span class="No-Break">architecture changes.</span></p>
			<p>The training loop iterates over our data for the specified number of epochs. In each iteration, we do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Move the batch to the appropriate device (GPU <span class="No-Break">if available)</span></li>
				<li>Perform a forward pass through <span class="No-Break">the model</span></li>
				<li>Calculate <span class="No-Break">the loss</span></li>
				<li><span class="No-Break">Perform backpropagation</span></li>
				<li>Update the <span class="No-Break">model parameters</span></li>
				<li>Update the learning <span class="No-Break">rate scheduler</span></li>
				<li>Log the <span class="No-Break">training loss</span></li>
			</ol>
			<p>After each epoch, we perform a simple evaluation of the training data (in a real scenario, you’d use a separate validation set), log the evaluation loss, and save a checkpoint of the model. Checkpointing is needed for long-running training processes, allowing us to resume training from a saved state <span class="No-Break">if needed.</span></p>
			<p>As we’ve seen, the training pipeline involves several essential steps. Before the model architecture and training loop can function effectively, however, we must address data input and preprocessing, which we will <span class="No-Break">discuss next.</span></p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor111"/>Data input and preprocessing</h1>
			<p>Efficient data handling is <a id="_idIndexMarker325"/>crucial for LLM training, as we discussed in <em class="italic">Part 1</em> of this book. Here, let’s explore advanced techniques for data input <span class="No-Break">and preprocessing:</span></p>
			<ol>
				<li>Import the required <span class="No-Break">Python packages:</span><pre class="source-code">
from datasets import load_dataset, concatenate_datasets
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
import numpy as np</pre></li>				<li>Load and combine <span class="No-Break">multiple datasets:</span><pre class="source-code">
wiki_dataset = load_dataset("wikipedia", "20220301.en", split="train")
books_dataset = load_dataset("bookcorpus", split="train")
# Combine datasets
combined_dataset = concatenate_
    datasets([wiki_dataset, books_dataset])</pre></li>				<li>Initialize the tokenizer and <span class="No-Break">perform </span><span class="No-Break"><strong class="source-inline">preprocess</strong></span><span class="No-Break">:</span><pre class="source-code">
tokenizer = AutoTokenizer.from_pretrained("gpt2")
def preprocess_function(examples):
    # Tokenize the texts
    tokenized = tokenizer(
        examples["text"], truncation=True, max_length=1024)
    # Create input_ids and attention_mask
    input_ids = tokenized["input_ids"]
    attention_mask = tokenized["attention_mask"]
    # Create labels for causal language modeling
    labels = [
        ids[1:] + [tokenizer.eos_token_id] for ids in input_ids]
    return {"input_ids": input_ids, 
        "attention_mask": attention_mask, "labels": labels}
# Apply preprocessing
tokenized_dataset = combined_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=combined_dataset.column_names,
    num_proc=4  # Adjust based on your CPU cores
)</pre></li>				<li>Create <a id="_idIndexMarker326"/><span class="No-Break">the DataLoader:</span><pre class="source-code">
train_dataloader = DataLoader(
    tokenized_dataset,
    shuffle=True,
    batch_size=16,
    collate_fn=lambda x: {k: np.stack([xi[k] for xi in x]) 
        for k in x[0]}
)</pre></li>			</ol>
			<p>In this enhanced preprocessing pipeline, we’re loading multiple datasets to increase the diversity of our training data. This is needed for LLMs, as a diverse dataset helps the model learn a broader range of language patterns <span class="No-Break">and knowledge.</span></p>
			<p>We use a longer <strong class="source-inline">max_length</strong> value of <strong class="source-inline">1024</strong> tokens to provide more context to the model. This increased context length allows the model to capture longer-range dependencies in the text, which can be beneficial for many language-understanding tasks. However, it also increases memory usage and computational requirements, so there’s a trade-off <span class="No-Break">to consider.</span></p>
			<p>The <strong class="source-inline">preprocess_function</strong> now creates labels for causal language modeling by shifting the input sequences. This is a common approach for training language models, where the model’s task is to predict the next token given the previous tokens. During preprocessing, handling edge cases such as emojis, URLs, and non-standard characters can enhance model performance. Emojis can convey nuanced emotions and context, requiring appropriate encoding or tokenization to preserve their meaning without introducing noise. URLs often contain valuable information but can vary widely in structure, so they might be replaced with placeholder tokens to maintain consistency while preventing the model from overfitting to specific links. Non-standard characters, including symbols from different languages or special punctuation, need careful normalization or removal to reduce complexity and avoid confusion during training. By addressing these edge cases through strategies such as normalization, token replacement, and selective filtering, preprocessing pipelines can better prepare diverse and complex data, enhancing the robustness and accuracy of the resulting <span class="No-Break">language models.</span></p>
			<p>We use multiprocessing (<strong class="source-inline">num_proc=4</strong>) to speed up the preprocessing. The number of processes should be adjusted based on your CPU cores and available memory. Multiprocessing can significantly reduce preprocessing time, especially for <span class="No-Break">large datasets.</span></p>
			<p>The batch size is<a id="_idIndexMarker327"/> increased to <strong class="source-inline">16</strong>, which is more suitable for larger GPU memory. The custom <strong class="source-inline">collate_fn</strong> in the DataLoader ensures proper batching of our preprocessed data. This function stacks the arrays for each key in the batch, creating tensor-like structures that can be efficiently processed <span class="No-Break">by PyTorch.</span></p>
			<p>With the data appropriately prepared, we now turn our attention to the LLM architecture design considerations, which dictate the model’s capacity to effectively learn from and understand <span class="No-Break">data input.</span></p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor112"/>LLM architecture design considerations</h1>
			<p>When <a id="_idIndexMarker328"/>designing the architecture for an LLM, several factors come <span class="No-Break">into play.</span></p>
			<p>Here are the key factors influencing <span class="No-Break">LLM architecture:</span></p>
			<ul>
				<li><strong class="bold">Vocabulary size</strong>: Determines the size of the input and output <span class="No-Break">embedding layers</span></li>
				<li><strong class="bold">Maximum sequence length (context size)</strong>: Defines the amount of preceding text the model <span class="No-Break">can consider</span></li>
				<li><strong class="bold">Embedding dimension</strong>: Specifies the size of each token’s vector representation, influencing the model’s ability to <span class="No-Break">capture information</span></li>
				<li><strong class="bold">Number of transformer layers</strong>: Represents the depth of the network, impacting the complexity of patterns the model <span class="No-Break">can learn</span></li>
				<li><strong class="bold">Number of attention heads</strong>: Allows the model to attend to different parts of the <span class="No-Break">input simultaneously</span></li>
				<li><strong class="bold">Model size (number of parameters)</strong>: Overall capacity of the model, influenced by embedding dimension, number of layers, and <span class="No-Break">attention heads</span></li>
				<li><strong class="bold">Dataset size</strong>: The amount and diversity of <span class="No-Break">training data</span></li>
				<li><strong class="bold">Number of training steps</strong>: The duration of the <span class="No-Break">optimization process</span></li>
				<li><strong class="bold">Computational resources</strong>: Hardware constraints that affect model size, training speed, and <span class="No-Break">overall feasibility.</span></li>
				<li><strong class="bold">Risk of overfitting</strong>: Higher with larger models and <span class="No-Break">smaller datasets</span></li>
				<li><strong class="bold">Data quality</strong>: The cleanliness and relevance of the <span class="No-Break">training data</span></li>
				<li><strong class="bold">Efficiency of model architecture</strong>: Design choices that can improve performance without drastically increasing <span class="No-Break">model size</span></li>
				<li><strong class="bold">Training algorithms</strong>: Optimization techniques <span class="No-Break">and strategies</span></li>
				<li><strong class="bold">Data curation practices</strong>: Methods for selecting and preparing <span class="No-Break">training data</span></li>
				<li><strong class="bold">Test time compute</strong>: Computational <a id="_idIndexMarker329"/>resources available <span class="No-Break">during inference</span></li>
			</ul>
			<p>In the following code block, we provide examples of configuring some of these factors using a GPT-2 style language model, specifying key <span class="No-Break">architectural parameters.</span></p>
			<pre class="source-code">
from transformers import GPT2Config, GPT2LMHeadModel
# Define custom model configuration
config = GPT2Config(
    vocab_size=50257,  # GPT-2 vocabulary size
    n_positions=1024,  # Maximum sequence length
    n_ctx=1024,        # Context size
    n_embd=768,        # Embedding dimension
    n_layer=12,        # Number of transformer layers
    n_head=12          # Number of attention heads
)
# Initialize the model with custom configuration
model = GPT2LMHeadModel(config)
print(f"Model parameters: {model.num_parameters():,}")</pre>			<p>This configuration creates<a id="_idIndexMarker330"/> a GPT-2 style model with <strong class="source-inline">12</strong> layers and <strong class="source-inline">12</strong> attention heads. Let’s break down the <span class="No-Break">key parameters:</span></p>
			<ul>
				<li><strong class="source-inline">vocab_size</strong>: Set to <strong class="source-inline">50257</strong>, which is the vocabulary size of the original GPT-2 model. This determines the size of the embedding layer and the <span class="No-Break">output layer.</span></li>
				<li><strong class="source-inline">n_positions</strong> and <strong class="source-inline">n_ctx</strong>: Both are set to <strong class="source-inline">1024</strong>, matching our preprocessing step. This defines the maximum sequence length the model <span class="No-Break">can handle.</span></li>
				<li><strong class="source-inline">n_embd</strong>: The embedding dimension, set to <strong class="source-inline">768</strong>. This determines the size of the hidden states throughout <span class="No-Break">the model.</span></li>
				<li><strong class="source-inline">n_layer</strong>: The number of transformer layers, set to <strong class="source-inline">12</strong>. More layers can capture more complex patterns but increase <span class="No-Break">computational requirements.</span></li>
				<li><strong class="source-inline">n_head</strong>: The number of attention heads, set to <strong class="source-inline">12</strong>. Multiple attention heads allow the model to focus on different aspects of the <span class="No-Break">input simultaneously.</span></li>
			</ul>
			<p>The embedding dimension of <strong class="source-inline">768</strong> and the <strong class="source-inline">12</strong> layers provide a balanced trade-off between model capacity and computational efficiency. This configuration results in a model with about 124 million parameters, which is substantial but still trainable on common <span class="No-Break">GPU hardware.</span></p>
			<p>For larger models, you might increase <strong class="source-inline">n_layer</strong>, <strong class="source-inline">n_embd</strong>, and <strong class="source-inline">n_head</strong>. However, this would also increase the computational requirements and the risk of overfitting, especially on smaller datasets. When scaling up, consider techniques such as gradient accumulation, mixed precision training, and distributed training to manage the increased <span class="No-Break">computational load.</span></p>
			<p>In a broader scope, <strong class="bold">scaling laws</strong> can <a id="_idIndexMarker331"/>be considered. The scaling laws for LLMs describe how performance improves predictably as three key factors increase: model size (number of parameters), dataset size (amount of training data), and the number of training steps (optimization iterations). Specifically, larger models tend to capture more complex patterns and exhibit better generalization, larger datasets provide more diverse information for learning, and more training steps allow the model to refine its understanding and reduce errors. For optimal performance, these factors should scale proportionally – for instance, increasing the model size should be matched by a corresponding increase in dataset size and training steps. This balanced scaling ensures that each component supports the others, preventing bottlenecks such as overfitting smaller models on vast datasets or undertraining large models with <span class="No-Break">insufficient data.</span></p>
			<p>However, recent <a id="_idIndexMarker332"/>advancements and practical challenges have shown that simply scaling these factors is not always sufficient for continual performance improvements. Issues such as diminishing returns, where each additional parameter or data point contributes less to overall performance, have become more apparent. Additionally, the immense computational and energy resources required for training increasingly large models raise sustainability and accessibility concerns. Data quality also becomes a critical factor, as larger datasets may introduce more noise and biases, potentially degrading model performance. For more details about this, please see the article <span class="No-Break">at </span><a href="https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/"><span class="No-Break">https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/</span></a><span class="No-Break">.</span></p>
			<p>To address these challenges, researchers are exploring more efficient model architectures, improved training algorithms, better data curation practices, and test time compute. See my Medium article for more details on test time <span class="No-Break">compute: </span><a href="https://kenhuangus.medium.com/test-time-compute-3633a4c55716"><span class="No-Break">https://kenhuangus.medium.com/test-time-compute-3633a4c55716</span></a></p>
			<p>At the beginning of 2025, DeepSeek (an AI startup in China) announced some model training innovations by <a id="_idIndexMarker333"/>introducing a suite of techniques aimed at significantly increasing efficiency and reducing costs, while simultaneously enhancing the model’s reasoning capabilities (<a href="https://arxiv.org/abs/2501.12948">https://arxiv.org/abs/2501.12948</a>). Unlike traditional approaches that rely heavily on vast computational resources and human-supervised fine-tuning, DeepSeek leverages large-scale reinforcement learning focused on reasoning tasks, using automated reward systems rather than human feedback. Key innovations include multi-token prediction, which allows the model to learn from multiple future tokens at once, increasing sample efficiency, and speeding up training. DeepSeek also employs a mixture-of-experts architecture to activate only relevant sub-networks for each task, thus reducing computational load. By optimizing both algorithms and hardware, DeepSeek has managed to train highly capable models at a fraction of the cost and time required by competitors, setting new standards for open, efficient, and powerful <span class="No-Break">AI</span><span class="No-Break"><a id="_idIndexMarker334"/></span><span class="No-Break"> development.</span></p>
			<p>Having explored the architectural design considerations and model training innovations for LLMs — along with a code example demonstrating how to configure model training parameters — we are now ready to examine how these architectural choices are actually learned during training. In this next section, we will discuss the loss function and optimization strategies, which serve as the engine that drives the model to adjust its internal parameters based on both the training data and the architecture we <span class="No-Break">have defined.</span></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor113"/>Loss functions and optimization strategies</h1>
			<p>LLMs typically use <strong class="bold">cross-entropy loss</strong> for training. This approach measures the difference between the model’s <a id="_idIndexMarker335"/>predicted probability distribution of words and the actual <a id="_idIndexMarker336"/>distribution in the training data. By minimizing this loss, LLMs learn to generate more accurate and contextually appropriate text. Cross-entropy loss is particularly well-suited for language tasks due to its ability to handle the high dimensionality and discrete nature of <span class="No-Break">textual data.</span></p>
			<p>Let’s implement this along<a id="_idIndexMarker337"/> with some advanced <span class="No-Break">optimization techniques:</span></p>
			<ol>
				<li>First, we import the required PyTorch libraries and specific modules from the Transformers library <span class="No-Break">for optimization:</span><pre class="source-code">
import torch
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup</pre></li>				<li>Next, we configure the AdamW optimizer with a specified learning rate and <span class="No-Break">weight decay:</span><pre class="source-code">
optimizer = AdamW(model.parameters(), lr=5e-5, 
    weight_decay=0.01)</pre></li>				<li>Then, we define a linear learning rate scheduler with a <span class="No-Break">warm-up period:</span><pre class="source-code">
num_epochs = 3
total_steps = len(train_dataloader) * num_epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=total_steps
)</pre></li>				<li>Subsequently, we<a id="_idIndexMarker338"/> set up the training device <a id="_idIndexMarker339"/>and initiate the main <span class="No-Break">training loop:</span><pre class="source-code">
device = torch.device("cuda" if torch.cuda.is_available() 
    else "cpu")
model.to(device)
for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        batch = {k: torch.tensor(v).to(device) 
            for k, v in batch.items()
        }
        outputs = model(batch)
        loss = outputs.loss
        loss.backward()</pre></li>				<li>Finally, we implement gradient clipping to prevent exploding gradients <span class="No-Break">during training:</span><pre class="source-code">
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()</pre></li>			</ol>
			<p>In this optimization<a id="_idIndexMarker340"/> setup, we use the <strong class="source-inline">AdamW</strong> optimizer with a learning rate of <strong class="source-inline">5e-5</strong> and weight decay of <strong class="source-inline">0.01</strong>. The algorithm adapts the learning rate for each parameter based on the first and second moments of the gradients, allowing it to handle sparse gradients effectively. This makes <strong class="source-inline">AdamW</strong> particularly useful for<a id="_idIndexMarker341"/> training large <span class="No-Break">neural networks.</span></p>
			<p>The weight decay of <strong class="source-inline">0.01</strong> adds a small regularization term to the loss function, which can help prevent overfitting by penalizing large <span class="No-Break">weight values.</span></p>
			<p>We implement a <strong class="bold">learning rate scheduler</strong> with <strong class="source-inline">warmup</strong>. The <strong class="source-inline">warmup</strong> phase helps stabilize training in<a id="_idIndexMarker342"/> the early stages by gradually increasing the learning rate from a very small value. After the <strong class="source-inline">warmup</strong> phase, the learning rate decreases linearly. This schedule can help the model converge to a <span class="No-Break">better optimum.</span></p>
			<p>In the training loop, we<a id="_idIndexMarker343"/> implement <strong class="bold">gradient clipping</strong> with a <strong class="source-inline">max_norm</strong> value of <strong class="source-inline">1.0</strong>. Gradient clipping prevents exploding gradients by scaling down gradient values that exceed a certain threshold. This is particularly important for LLMs, which can be prone to unstable gradients due to their depth and the long-range dependencies <span class="No-Break">they capture.</span></p>
			<p>In this section, we learned about AdamW optimization, learning rate scheduling with warmup, and gradient clipping for stable LLM training. Next, we talk about logging the training process, which is crucial for monitoring progress and using tools like <strong class="bold">TensorBoard</strong> to gain insights<a id="_idIndexMarker344"/> <span class="No-Break">for improvement.</span></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor114"/>Logging</h1>
			<p>Effective logging<a id="_idIndexMarker345"/> can be useful for tracking the progress of <span class="No-Break">LLM training.</span></p>
			<p>The following code blocks demonstrate how to integrate TensorBoard for effective logging during the training of an LLM using PyTorch. Let’s break down <span class="No-Break">each part.</span></p>
			<ol>
				<li>We first initialize the TensorBoard <strong class="source-inline">SummaryWriter</strong> for logging <span class="No-Break">training progress:</span><pre class="source-code">
from torch.utils.tensorboard import SummaryWriter
import time
# Initialize TensorBoard writer
writer = SummaryWriter()</pre></li>				<li>Then, we set the model to training mode, initialize variables for tracking loss, define the logging interval, and record the start time to monitor <span class="No-Break">training performance:</span><pre class="source-code">
model.train()
total_loss = 0
log_interval = 100
start_time = time.time()</pre></li>				<li>Then, we move on to the training loop. We process each batch by moving data to the appropriate device, performing forward and backward passes, applying gradient clipping, and updating the model’s parameters using the optimizer <span class="No-Break">and scheduler:</span><pre class="source-code">
for i, batch in enumerate(train_dataloader):
    batch = {k: torch.tensor(v).to(device) 
        for k, v in batch.items()}
    outputs = model(batch)
    loss = outputs.loss
    total_loss += loss.item()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 
        max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()</pre></li>				<li>We log the training metrics to TensorBoard at specified intervals, calculate the average loss, measure the elapsed time, print the progress to the console, and reset the tracking variables for the <span class="No-Break">next interval:</span><pre class="source-code">
    if (i + 1) % log_interval == 0:
        cur_loss = total_loss / log_interval
        elapsed = time.time() - start_time
        writer.add_scalar(
            'training_loss', cur_loss, global_step=i
        )
        writer.add_scalar(
            'learning_rate', scheduler.get_last_lr()[0], 
            global_step=i
        )
        print(
            f'| epoch {epoch:3d} '
            f'| {i:5d}/{len(train_dataloader):5d} batches | '
            f'lr {scheduler.get_last_lr()[0]:02.2f} | '
            f'ms/batch {elapsed * 1000 / log_interval:5.2f} | '
            f'loss {cur_loss:5.2f}'
        )
        total_loss = 0
        start_time = time.time()
writer.close()</pre><p class="list-inset">This enhanced training loop uses TensorBoard for logging training loss and learning rate. TensorBoard is a powerful tool for visualizing training progress and comparing different runs. We log the <span class="No-Break">following metrics:</span></p><ul><li><strong class="bold">Training loss</strong>: This is the<a id="_idIndexMarker346"/> average loss over the last <strong class="source-inline">log_interval</strong> batches. A decreasing trend in this metric indicates that the model <span class="No-Break">is learning.</span></li><li><strong class="bold">Learning rate</strong>: We log the <a id="_idIndexMarker347"/>current learning rate to visualize how it changes over time due to our learning <span class="No-Break">rate scheduler.</span></li></ul></li>			</ol>
			<p>We set <strong class="source-inline">log_interval</strong> to <strong class="source-inline">100</strong>, meaning we log and print out progress information every 100 batches. This interval strikes a balance between getting frequent updates and not slowing down training too much with logging operations. You may need to adjust this based on your dataset size and <span class="No-Break">training speed.</span></p>
			<p>The output or log information includes <span class="No-Break">the following:</span></p>
			<ul>
				<li>Current epoch and <span class="No-Break">batch number</span></li>
				<li>Current <span class="No-Break">learning rate</span></li>
				<li>Time per batch (<span class="No-Break">in milliseconds)</span></li>
				<li><span class="No-Break">Current</span><span class="No-Break"><a id="_idIndexMarker348"/></span><span class="No-Break"> loss</span></li>
			</ul>
			<p>This detailed logging allows you to monitor the training process closely, helping you identify issues such as unstable loss, learning rate problems, or unexpectedly <span class="No-Break">slow training.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor115"/>Pipeline modularity and reusability</h1>
			<p><strong class="bold">Modularity</strong> and <strong class="bold">reusability</strong> are<a id="_idIndexMarker349"/> fundamental principles for building efficient<a id="_idIndexMarker350"/> pipelines because they make code more maintainable, adaptable, and reliable. By breaking down a pipeline into independent, reusable modules (such as data preprocessing, model training, and evaluation components), developers can easily modify individual parts without affecting others, test each component separately, and reuse proven code across <span class="No-Break">different projects.</span></p>
			<p>This approach not only saves development time but also ensures consistency in operations, reduces the chance of errors, and makes it easier for teams to collaborate by working on separate modules while maintaining clear interfaces between components. In the case of training pipelines, encapsulating processes in reusable classes allows for flexible configuration, seamless integration with different datasets, and straightforward sharing of standardized implementations across <span class="No-Break">multiple projects.</span></p>
			<p>To make our pipeline <a id="_idIndexMarker351"/>more modular and reusable, let’s encapsulate our<a id="_idIndexMarker352"/> training process in <span class="No-Break">a class:</span></p>
			<ol>
				<li>We start with <span class="No-Break">class definition:</span><pre class="source-code">
class LLMTrainer:
    def __init__(self, model, train_dataloader, optimizer,
    scheduler, device
    ):
        self.model = model
        self.train_dataloader = train_dataloader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.writer = SummaryWriter()</pre></li>				<li>Then, we define the train epoch function. The function sets the model to training mode and iterates over the training data, processing each batch by computing the loss, performing backpropagation with gradient clipping, and updating the model parameters using the optimizer <span class="No-Break">and scheduler:</span><pre class="source-code">
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        log_interval = 100
        start_time = time.time()
        for i, batch in enumerate(self.train_dataloader):
            batch = {k: torch.tensor(v).to(self.device)
                for k, v in batch.items()
            }
            outputs = self.model(batch)
            loss = outputs.loss
            total_loss += loss.item()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()</pre></li>				<li>Next, we periodically log<a id="_idIndexMarker353"/> the training progress to both <a id="_idIndexMarker354"/>TensorBoard and the console by checking if the current batch index is a multiple of the <strong class="source-inline">log_interval</strong>; if it is, we calculate the average loss and elapsed time since the last log, record the training loss and learning rate to TensorBoard using the <strong class="source-inline">SummaryWriter</strong>, print a formatted progress update including batch number, learning rate, milliseconds per batch, and current loss to the console, and then reset the accumulated <strong class="source-inline">total_loss</strong> and <strong class="source-inline">start_time</strong> for the next <span class="No-Break">logging interval:</span><pre class="source-code">
            if (i + 1) % log_interval == 0:
                cur_loss = total_loss / log_interval
                elapsed = time.time() - start_time
                self.writer.add_scalar(
                    'training_loss', cur_loss, global_step=i
                )
                self.writer.add_scalar(
                    'learning_rate', 
                    self.scheduler.get_last_lr()[0], 
                    global_step=i
                )
                    print(
                        f'| {i:5d}/'
                        f'{len(self.train_dataloader):5d} '
                        f'batches | '
                        f'lr '
                        f'{self.scheduler.get_last_lr()'
                        f'[0]:02.2f} | '
                        f'ms/batch '
                        f'{elapsed * 1000 / log_interval:5.2f} '
                        f'| '
                        f'loss '
                        f'{cur_loss:5.2f}'
                    )
                total_loss = 0
                start_time = time.time()</pre></li>				<li>Next, the <strong class="source-inline">train</strong> function<a id="_idIndexMarker355"/> orchestrates the training process by<a id="_idIndexMarker356"/> looping through the specified number of epochs, printing a message at the start of each epoch, invoking the <strong class="source-inline">train_epoch</strong> method to perform training for that epoch, and, finally, closing the writer once all epochs are completed. It serves as the main entry point for training, providing a structure where additional features such as validation and checkpointing can be integrated <span class="No-Break">as needed:</span><pre class="source-code">
    def train(self, num_epochs):
        for epoch in range(num_epochs):
            print(f'Starting epoch {epoch+1}')
            self.train_epoch()
            # Here you could add validation, checkpointing, etc.
        self.writer.close()</pre></li>				<li>Lastly, we instantiate the <strong class="source-inline">LLMTrainer</strong> class with the specified model, training data loader, optimizer, scheduler, and device. Then, the training process is started by calling the <strong class="source-inline">train</strong> method to execute three full training epochs, thereby initiating and managing the model’s <span class="No-Break">learning cycle:</span><pre class="source-code">
trainer = LLMTrainer(model, train_dataloader,
    optimizer, scheduler, device)
trainer.train(num_epochs=3)</pre></li>			</ol>
			<p>This modular design <a id="_idIndexMarker357"/>offers <span class="No-Break">several advantages:</span></p>
			<ul>
				<li><strong class="bold">Encapsulation</strong>: All the <a id="_idIndexMarker358"/>training logic is contained within the <strong class="source-inline">LLMTrainer</strong> class, making it easier to manage <span class="No-Break">and understand.</span></li>
				<li><strong class="bold">Reusability</strong>: You can easily use this trainer for different models or datasets by creating a new instance with <span class="No-Break">different parameters.</span></li>
				<li><strong class="bold">Extensibility</strong>: The class structure makes it easy to add new functionality. For example, you could add methods for validation, checkpointing, or <span class="No-Break">early stopping.</span></li>
				<li><strong class="bold">Separation of concerns</strong>: The training logic is separated from the model definition and data preparation, following good software <span class="No-Break">engineering principles.</span></li>
			</ul>
			<p>The following log demonstrates the training process over 3 epochs, with periodic logging every 100 batches. Each log entry includes the current batch number, total batches, learning rate, milliseconds per batch, and the <span class="No-Break">average loss:</span></p>
			<pre class="console">
Starting epoch 1
|   100/1000 batches | lr 0.01 | ms/batch 45.67 | loss 2.35
|   200/1000 batches | lr 0.01 | ms/batch 44.89 | loss 2.10
|   300/1000 batches | lr 0.01 | ms/batch 46.12 | loss 1.95
|   400/1000 batches | lr 0.01 | ms/batch 45.50 | loss 1.80
|   500/1000 batches | lr 0.01 | ms/batch 44.75 | loss 1.65
|   600/1000 batches | lr 0.009 | ms/batch 45.30 | loss 1.50
|   700/1000 batches | lr 0.009 | ms/batch 44.95 | loss 1.40
|   800/1000 batches | lr 0.009 | ms/batch 45.10 | loss 1.30
|   900/1000 batches | lr 0.009 | ms/batch 45.00 | loss 1.25
|  1000/1000 batches | lr 0.009 | ms/batch 44.80 | loss 1.20
Starting epoch 2
|   100/1000 batches | lr 0.009 | ms/batch 44.60 | loss 1.18
|   200/1000 batches | lr 0.009 | ms/batch 44.70 | loss 1.15
|   300/1000 batches | lr 0.009 | ms/batch 44.80 | loss 1.12
|   400/1000 batches | lr 0.008 | ms/batch 44.50 | loss 1.10
|   500/1000 batches | lr 0.008 | ms/batch 44.60 | loss 1.08
|   600/1000 batches | lr 0.008 | ms/batch 44.55 | loss 1.05
|   700/1000 batches | lr 0.008 | ms/batch 44.65 | loss 1.03
|   800/1000 batches | lr 0.007 | ms/batch 44.50 | loss 1.00
|   900/1000 batches | lr 0.007 | ms/batch 44.60 | loss 0.98
|  1000/1000 batches | lr 0.007 | ms/batch 44.55 | loss 0.95
Starting epoch 3
|   100/1000 batches | lr 0.007 | ms/batch 44.50 | loss 0.93
|   200/1000 batches | lr 0.007 | ms/batch 44.60 | loss 0.90
|   300/1000 batches | lr 0.006 | ms/batch 44.55 | loss 0.88
|   400/1000 batches | lr 0.006 | ms/batch 44.50 | loss 0.85
|   500/1000 batches | lr 0.006 | ms/batch 44.60 | loss 0.83
|   600/1000 batches | lr 0.006 | ms/batch 44.55 | loss 0.80
|   700/1000 batches | lr 0.005 | ms/batch 44.50 | loss 0.78
|   800/1000 batches | lr 0.005 | ms/batch 44.60 | loss 0.75
|   900/1000 batches | lr 0.005 | ms/batch 44.55 | loss 0.73
|  1000/1000 batches | lr 0.005 | ms/batch<a id="_idTextAnchor116"/> 44.50 | loss 0.70
Training completed. Writer closed.</pre>			<p>Here is an<a id="_idIndexMarker359"/> explanation of the <a id="_idIndexMarker360"/>above <span class="No-Break">simulated log:</span></p>
			<ul>
				<li><strong class="bold">Epoch start</strong>: Each epoch begins with a message such as <strong class="source-inline">Starting epoch 1</strong>, indicating the commencement of a new <span class="No-Break">training cycle</span></li>
				<li><strong class="bold">Batch logging</strong>: Every 100 batches, the following information <span class="No-Break">is logged:</span><ul><li><strong class="bold">Batch progress</strong>: Displays the current batch number out of the total batches, such as <span class="No-Break"><strong class="source-inline">100/1000 batches</strong></span></li><li><strong class="bold">Learning rate (lr)</strong>: Shows the current learning rate, which may decrease over epochs due to the scheduler, such as <span class="No-Break"><strong class="source-inline">lr 0.01</strong></span></li><li><strong class="bold">Milliseconds per batch (ms/batch)</strong>: Indicates the time taken to process each batch, such as <span class="No-Break"><strong class="source-inline">ms/batch 45.67</strong></span></li><li><strong class="bold">Loss</strong>: Represents the average loss over the last 100 batches, showing the model’s performance, such as <span class="No-Break"><strong class="source-inline">loss 2.35</strong></span></li></ul></li>
				<li><strong class="bold">Learning rate schedule</strong>: Notice how the learning rate decreases over epochs, reflecting the scheduler’s adjustments to facilitate <span class="No-Break">better convergence</span></li>
				<li><strong class="bold">Training completion</strong>: After all epochs are completed, a final message (<strong class="source-inline">Training completed. Writer closed.</strong>) indicates the end of the training process and the closure of the <span class="No-Break">logging writer</span></li>
			</ul>
			<p>The log provides a clear overview of the training dynamics, allowing developers and researchers to monitor the model’s learning progress, adjust hyperparameters if necessary, and ensure that the training is proceeding <span class="No-Break">as expected.</span></p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor117"/>Scaling your training pipeline for larger models</h1>
			<p>To train larger models, we need <a id="_idIndexMarker361"/>to employ techniques such as gradient accumulation and mixed <span class="No-Break">precision training.</span></p>
			<p>To train very large language models that might not fit on a single GPU, the following code introduces a special <strong class="source-inline">LargeScaleLLMTrainer</strong>. It uses two main tricks to <span class="No-Break">handle this:</span></p>
			<p>First, gradient accumulation <a id="_idIndexMarker362"/>allows us to simulate having access to a larger GPU. Instead of updating the model's parameters after every small batch of data, we process several small batches, accumulating their gradients along the way. Only after a predefined number of batches do we perform an actual update to the model's parameters. This technique enables the model to learn as if it had seen a much larger batch of data, without requiring the memory capacity of an extremely <span class="No-Break">large GPU.</span></p>
			<p>Second, it employs mixed precision training, a technique where the computer performs many calculations using smaller, lower-precision numbers (which require less memory and are faster to compute), while reserving higher-precision numbers for situations where accuracy is critical. This approach accelerates training and reduces overall memory usage. To mitigate potential issues that can arise from using lower-precision values, GradScaler is used to maintain numerical stability <span class="No-Break">during backpropagation.</span></p>
			<p>The following code defines how this special trainer works, including how it processes data, calculates the loss, and updates the model’s learning using these tricks. It also still includes important steps like making sure the gradients (how the model should change) don’t get too big and logging progress so we can see how the training is going. Finally, it shows a simple example of how to use this special trainer. Now, let us break it into <span class="No-Break">several parts:</span></p>
			<ol>
				<li>Let us start by importing the relevant Python package and defining <span class="No-Break">the class:</span><pre class="source-code">
import torch.cuda.amp as amp
class LargeScaleLLMTrainer(LLMTrainer):
    def __init__(self, model, train_dataloader,
        optimizer, scheduler, device, accumulation_steps=4
    ):
        super().__init__(model, train_dataloader,
            optimizer, scheduler, device)
        self.accumulation_steps = accumulation_steps
        self.scaler = amp.GradScaler()</pre></li>				<li>We can then <a id="_idIndexMarker363"/>define the <span class="No-Break">training epoch:</span><pre class="source-code">
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        log_interval = 100
        start_time = time.time()
        for i, batch in enumerate(self.train_dataloader):
            batch = {
                k: torch.tensor(v).to(self.device)
                for k, v in batch.items()
            }
            with amp.autocast():
                outputs = self.model(batch)
                loss = outputs.loss / self.accumulation_steps
            self.scaler.scale(loss).backward()</pre></li>				<li>Then, we implement the following code<a id="_idIndexMarker364"/> block, which updates the model’s parameters, learning rate, and gradient scaler only after processing a defined number of batches (<strong class="source-inline">accumulation_steps)</strong>, effectively simulating a larger batch size while managing <span class="No-Break">memory constraints:</span><pre class="source-code">
            if (i + 1) % self.accumulation_steps == 0:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), max_norm=1.0
                )
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.scheduler.step()
                self.optimizer.zero_grad()
            total_loss += loss.item() * self.accumulation_steps</pre></li>				<li>We then periodically calculate and log the average training loss and learning rate to TensorBoard, while also printing a summary of the current training progress to the console at intervals defined <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">log_interval</strong></span><span class="No-Break">:</span><pre class="source-code">
            if (i + 1) % log_interval == 0:
                cur_loss = total_loss / log_interval
                elapsed = time.time() start_time
                self.writer.add_scalar('training_loss',
                    cur_loss, global_step=i)
                self.writer.add_scalar('learning_rate',
                    self.scheduler.get_last_lr()[0],
                    global_step=i)
                print(
                    f'| {i:5d}/{len(self.train_dataloader):5d}
                        batches | '
                    f'lr {self.scheduler.get_last_lr()[0]:02.2f}
                        | '
                    f'ms/batch {elapsed * 1000 /
                        log_interval:5.2f} | '
                    f'loss {cur_loss:5.2f}'
                 )
                total_loss = 0
                start_time = time.time()</pre></li>				<li>We demonstrate the initialization and execution of a large-scale language model <span class="No-Break">training process:</span><pre class="source-code">
large_trainer = LargeScaleLLMTrainer(
    model, train_dataloader, optimizer, scheduler, device)
large_trainer.train(num_epochs=3)</pre><p class="list-inset">This enhanced trainer uses two key techniques for scaling to <span class="No-Break">larger models:</span></p><ul><li><strong class="bold">Gradient accumulation</strong>: We update<a id="_idIndexMarker365"/> weights every four batches (set by <strong class="source-inline">accumulation_steps</strong>). This allows us to effectively increase the batch size without increasing memory usage, which is effective for training large models on limited GPU memory. We divide the loss by <strong class="source-inline">accumulation_steps</strong> to maintain the same effective <span class="No-Break">learning rate.</span></li><li><strong class="bold">Mixed precision training</strong>: We use PyTorch’s <strong class="bold">Automatic Mixed Precision</strong> (<strong class="bold">AMP</strong>) to perform computations<a id="_idIndexMarker366"/> in <strong class="source-inline">float16</strong>, where possible, while <a id="_idIndexMarker367"/>maintaining <strong class="source-inline">float32</strong> master weights. This can significantly speed up training and reduce memory usage, especially on modern GPUs with <span class="No-Break">tensor cores.</span></li></ul></li>			</ol>
			<p><strong class="source-inline">GradScaler</strong> is used to prevent underflow in <strong class="source-inline">float16</strong> calculations. It scales the loss to prevent small gradient values, then unscales before the <span class="No-Break">optimizer step.</span></p>
			<p>We still apply gradient clipping, but now it’s done after unscaling the gradients to ensure we’re clipping the true <span class="No-Break">gradient values.</span></p>
			<p>For even larger models, you might consider techniques such<a id="_idIndexMarker368"/> as <strong class="bold">model parallelism</strong> (splitting the model across multiple GPUs), <strong class="bold">pipeline parallelism</strong> (splitting the model into stages), or using specialized <a id="_idIndexMarker369"/>libraries such as <strong class="bold">DeepSpeed</strong> or <strong class="bold">Megatron-LM</strong>. These<a id="_idIndexMarker370"/> advanced techniques allow the training of models with <a id="_idIndexMarker371"/>billions of parameters across multiple GPUs or even multiple machines. Memory offloading can be a good alternative when GPU memory is insufficient to handle vast amounts of data and model parameters. Memory offloading involves transferring parts of the model’s data or computations to alternative <a id="_idIndexMarker372"/>memory storage, such as <strong class="bold">Non-Volatile Memory Express</strong> (<strong class="bold">NVMe</strong>) SSDs. By<a id="_idIndexMarker373"/> leveraging NVMe memory, which offers high-speed data access compared to traditional storage, systems can effectively manage and store intermediate activations, gradients, and model states that exceed GPU memory capacity. This approach allows for training larger models or using higher batch sizes without requiring immediate GPU memory expansion. However, it introduces additional latency due to data transfer between the GPU and NVMe storage, which can impact training speed. Optimizing data access patterns and utilizing efficient offloading strategies can minimize performance overhead and maintain eff<a id="_idTextAnchor118"/>ective training workflows when employing memory <span class="No-Break">offloading techniques.</span></p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor119"/>Summary</h1>
			<p>In this chapter, you learned about a practical pattern of pipeline design for training LLMs. You learned how to create efficient data preprocessing workflows, implement model architectures, and apply advanced optimization strategies. You now understand how to set up effective logging systems to track your model’s progress. You also explored techniques for building modular and reusable pipelines and discovered methods for scaling your training process to accommodate larger models. With these skills, you’re well equipped to train state-of-the-art language models efficiently <span class="No-Break">and effectively.</span></p>
			<p>In the next chapter, we’ll explore the hyperparameter <span class="No-Break">tuning pattern.</span></p>
		</div>
	</div></div></body></html>