- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Evaluation Metrics
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: In this chapter, we will explore the most recent and commonly used benchmarks
    for evaluating LLMs across various domains. We’ll delve into metrics for **natural
    language understanding** (**NLU**), reasoning and problem solving, coding and
    programming, conversational ability, and commonsense reasoning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨评估 LLM 在各个领域中最新的和最常用的基准。我们将深入研究自然语言理解（**NLU**）、推理和问题解决、编码和编程、对话能力和常识推理的指标。
- en: You’ll learn how to apply these benchmarks to assess your LLM’s performance
    comprehensively. By the end of this chapter, you’ll be equipped to design robust
    evaluation strategies for your LLM projects, compare models effectively, and make
    data-driven decisions to improve your models based on state-of-the-art evaluation
    techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何应用这些基准全面评估你的 LLM 的性能。到本章结束时，你将能够为你的 LLM 项目设计稳健的评估策略，有效地比较模型，并根据最先进的评估技术做出基于数据的决策来改进你的模型。
- en: 'In this chapter we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: NLU benchmarks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLU 基准
- en: Reasoning and problem-solving metrics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理和问题解决指标
- en: Coding and programming evaluation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码和编程评估
- en: Conversational ability assessment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话能力评估
- en: Commonsense and general knowledge benchmarks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常识和一般知识基准
- en: Other commonly used benchmarks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他常用基准
- en: Developing custom metrics and benchmarks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发自定义指标和基准
- en: Interpreting and comparing LLM evaluation results
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释和比较 LLM 评估结果
- en: NLU benchmarks
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLU 基准
- en: NLU is a crucial capability of LLMs. Let’s explore some of the most recent and
    widely used benchmarks in this domain.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLU 是 LLM 的关键能力。让我们探索这个领域中最新的和最广泛使用的基准。
- en: Massive multitask language understanding
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大规模多任务语言理解
- en: '**Massive multitask language understanding** (**MMLU**) is a comprehensive
    benchmark that tests models across 57 subjects, including science, mathematics,
    engineering, and more. It’s designed to assess both breadth and depth of knowledge.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**大规模多任务语言理解**（**MMLU**）是一个全面的基准，测试模型在 57 个科目上的表现，包括科学、数学、工程等。它旨在评估知识的广度和深度。'
- en: 'Here’s an example of how you might evaluate an LLM on MMLU using the `lm-evaluation-harness`
    library:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用 `lm-evaluation-harness` 库评估 LLM 在 MMLU 上的示例：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code evaluates the model on MMLU tasks with five-shot learning (learning
    by using 5 examples). The score represents the average accuracy across all subjects.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用五次学习（通过使用 5 个示例进行学习）评估模型在 MMLU 任务上的表现。分数代表所有科目平均准确率。
- en: SuperGLUE
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SuperGLUE
- en: '**SuperGLUE** is a benchmark designed to be more challenging than its predecessor,
    **GLUE**. It includes tasks that require more complex reasoning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**SuperGLUE** 是一个比其前辈 **GLUE** 更具挑战性的基准测试。它包括需要更复杂推理的任务。'
- en: GLUE and SuperGLUE are benchmarks designed to evaluate NLU models across a range
    of tasks. GLUE includes tasks such as sentiment analysis, linguistic acceptability,
    paraphrase detection, and semantic similarity, with datasets such as SST-2, CoLA,
    MRPC, and STS-B. SuperGLUE extends GLUE by adding more challenging tasks such
    as question answering, coreference resolution, and logical reasoning, with datasets
    such as **Boolean Questions** (**BoolQ**), **Reading Comprehension with Commonsense
    Reasoning Dataset** (**ReCoRD**), and the Winograd schema challenge. Together,
    they provide a comprehensive assessment of a model’s ability to handle diverse
    and complex language tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE 和 SuperGLUE 是旨在评估 NLU 模型在一系列任务上的表现的基准测试。GLUE 包括诸如情感分析、语言可接受性、释义检测和语义相似性等任务，数据集包括
    SST-2、CoLA、MRPC 和 STS-B。SuperGLUE 通过增加更具挑战性的任务，如问答、指代消解和逻辑推理，扩展了 GLUE，数据集包括 **布尔问题**（**BoolQ**）、**带有常识推理数据集的阅读理解**（**ReCoRD**）和
    Winograd 方案挑战。它们共同提供了一个对模型处理多样化和复杂语言任务能力的全面评估。
- en: SuperGLUE significantly extends the complexity level beyond GLUE by deliberately
    incorporating tasks that demand sophisticated reasoning capabilities, including
    challenging commonsense inference problems such as **Word-in-Context** (**WiC**)
    and BoolQ, causal reasoning assessments in **Choice of Plausible Alternatives**
    (**COPA**), and more nuanced reading comprehension challenges through ReCoRD and
    **Multi-Sentence Reading Comprehension** (**MultiRC**)—all requiring models to
    demonstrate deeper linguistic understanding and logical thinking than GLUE’s primarily
    classification-based tasks that focus on more straightforward linguistic phenomena
    such as grammatical acceptability, sentiment analysis, and textual entailment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGLUE通过故意纳入需要高级推理能力的任务，显著提高了复杂度，这些任务包括诸如**Word-in-Context** (**WiC**)和BoolQ等具有挑战性的常识推理问题，**Choice
    of Plausible Alternatives** (**COPA**)中的因果推理评估，以及通过ReCoRD和**Multi-Sentence Reading
    Comprehension** (**MultiRC**)带来的更细致的阅读理解挑战——所有这些都需要模型展现出比GLUE主要基于分类的任务更深层次的语语言学理解和逻辑思维，而GLUE的任务主要关注更直接的语语言学现象，如语法可接受性、情感分析和文本蕴涵。
- en: Here’s how you might evaluate on SuperGLUE.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在SuperGLUE上进行评估的方法。
- en: 'First, here are the necessary imports for working with datasets and transformer
    models:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，以下是用于处理数据集和转换器模型所需的必要导入：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following code example contains the main evaluation function for SuperGLUE.
    It handles model initialization, dataset loading, preprocessing, and training
    setup:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例包含了SuperGLUE的主要评估函数。它处理模型初始化、数据集加载、预处理和训练设置：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code defines an `evaluate_superglue` function that takes a pre-trained
    language model name and an optional SuperGLUE task name (defaulting to `"cb"`)
    as input. It loads the specified pre-trained model and its tokenizer, then loads
    the corresponding SuperGLUE dataset. It tokenizes the premise and hypothesis of
    the examples in the dataset, prepares training arguments for evaluation, initializes
    a `Trainer` object with the model, training arguments, and tokenized training
    and validation datasets, and finally evaluates the model on the validation set,
    returning the evaluation results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个`evaluate_superglue`函数，它接受一个预训练语言模型名称和一个可选的SuperGLUE任务名称（默认为`"cb"`）作为输入。它加载指定的预训练模型及其分词器，然后加载相应的SuperGLUE数据集。它对数据集中的示例的论据和假设进行分词，准备评估的训练参数，使用模型、训练参数和分词后的训练和验证数据集初始化一个`Trainer`对象，并最终在验证集上评估模型，返回评估结果。
- en: In the next code block, we use the **CommitmentBank** (**CB**) dataset. CB is
    an NLU dataset and benchmark task that focuses on determining whether a speaker
    is committed to the truth of a hypothesis given a premise statement, essentially
    measuring a model’s ability to understand textual entailment and speaker commitment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码块中，我们使用**CommitmentBank** (**CB**)数据集。CB是一个NLU数据集和基准任务，专注于确定说话者是否对前提陈述中的假设的真实性负责，本质上衡量模型理解文本蕴涵和说话者承诺的能力。
- en: For example, given a premise such as *I think it’s going to rain today* and
    a hypothesis of *It will rain today*, the task is to determine whether the speaker
    is fully committed to the hypothesis (entailment), denies it (contradiction),
    or remains uncommitted (neither)—in this case, the use of *I think* indicates
    the speaker isn’t fully committed to the claim. This task is particularly challenging
    as it requires models to understand subtle linguistic features such as reported
    speech, modal expressions, hedging language, and embedded clauses, making it a
    valuable tool for evaluating language models’ grasp of semantic nuances and speaker
    commitment levels in natural communication.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一个前提如“我认为今天会下雨”和一个假设“今天会下雨”，任务是确定说话者是否完全承诺于假设（蕴涵）、否认它（矛盾）或保持不承诺（既不蕴涵也不矛盾）——在这种情况下，“我认为”的使用表明说话者并不完全承诺这个主张。这个任务特别具有挑战性，因为它要求模型理解诸如直接引语、情态表达、保留语言和嵌套子句等细微的语言特征，使其成为评估语言模型掌握语义细微差别和说话者在自然交流中的承诺水平的有价值工具。
- en: 'Here’s a code block showing how to run the evaluation with a specific model
    on the CB task:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个代码块，展示了如何在CB任务上使用特定模型进行评估：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: TruthfulQA
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TruthfulQA
- en: '**TruthfulQA** is designed to measure a model’s tendency to reproduce falsehoods
    commonly believed by humans. It’s crucial for assessing the reliability of LLMs
    in real-world applications.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**TruthfulQA**旨在衡量模型复制人类普遍相信的错误倾向。这对于评估LLMs在实际应用中的可靠性至关重要。'
- en: 'Here’s an example of a falsehood that TruthfulQA might test:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是TruthfulQA可能测试的一个错误示例：
- en: '**Claim**: *Cracking your knuckles will give* *you arthritis*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**主张**：*扭动手指会给你关节炎*。'
- en: This claim is a common belief, but research suggests that knuckle cracking (also
    known as knuckle popping) is not a significant risk factor for developing arthritis.
    While it may have other effects, such as joint instability or weakened grip strength,
    the link to arthritis is not strongly supported.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主张是一个普遍的信念，但研究表明，指关节弹响（也称为指关节爆裂）并不是发展关节炎的显著风险因素。虽然它可能产生其他影响，如关节不稳定或握力减弱，但与关节炎的联系没有得到强有力的支持。
- en: 'Here’s a simplified approach to evaluate on TruthfulQA:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是评估 TruthfulQA 的简化方法：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `evaluate_truthfulqa` Python function takes a pre-trained language `model`,
    its corresponding `tokenizer`, and the `data_path` to a JSON file containing TruthfulQA
    questions and their correct answers. It reads the data, iterates through each
    question, tokenizes the question, generates a response from the model, decodes
    the response, and checks if any of the correct answers (case-insensitive) are
    present in the generated response. Finally, it calculates and returns the accuracy
    of the model on the provided TruthfulQA dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_truthfulqa` Python 函数接受一个预训练的语言 `model`，其对应的 `tokenizer`，以及包含 TruthfulQA
    问题及其正确答案的 JSON 文件所在的 `data_path`。它读取数据，遍历每个问题，对问题进行分词，从模型生成响应，解码响应，并检查生成的响应中是否包含任何正确的答案（不区分大小写）。最后，它计算并返回模型在提供的
    TruthfulQA 数据集上的准确率。'
- en: 'To run the evaluation code, use the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行评估代码，请使用以下命令：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code assumes you have the TruthfulQA dataset in a JSON format. It generates
    responses to questions and checks whether they contain any of the correct answers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码假定您已将 TruthfulQA 数据集以 JSON 格式存储。它生成对问题的响应，并检查它们是否包含任何正确答案。
- en: Now, we will shift our focus to reasoning and problem-solving metrics to examine
    how effectively LLMs can perform tasks requiring logical thought and problem-solving
    skills.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将重点转向推理和问题解决指标，以检查大型语言模型在执行需要逻辑思维和问题解决技能的任务方面的有效性。
- en: Reasoning and problem-solving metrics
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理和问题解决指标
- en: Evaluating an LLM’s ability to reason and solve problems is crucial for many
    applications. Let’s look at some key benchmarks in this area.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 评估大型语言模型（LLM）推理和解决问题的能力对于许多应用至关重要。让我们看看这个领域的几个关键基准。
- en: AI2 Reasoning Challenge
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI2 推理挑战
- en: '**AI2 Reasoning Challenge** (**ARC**) is designed to test grade-school-level
    science questions that require reasoning. See also: [https://huggingface.co/datasets/allenai/ai2_arc](https://huggingface.co/datasets/allenai/ai2_arc)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**AI2 推理挑战**（**ARC**）旨在测试需要推理的年级学校水平的科学问题。另请参阅：[https://huggingface.co/datasets/allenai/ai2_arc](https://huggingface.co/datasets/allenai/ai2_arc)'
- en: 'Here is an example of an ARC question:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个 ARC 问题的示例：
- en: '*One year, the oak trees in a park began producing more acorns than usual.
    The next year, the population of chipmunks in the park also increased. Which best
    explains why there were more chipmunks the* *next year?*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*一年中，公园里的橡树开始产生比以往更多的橡子。第二年，公园里松鼠的种群数量也增加了。以下哪个最好地解释了为什么第二年有更多的松鼠？*'
- en: '*Shady* *areas increased*'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*阴影区域增加*'
- en: '*Food* *sources increased*'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*食物来源增加*'
- en: '*Oxygen* *levels increased*'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*氧气水平增加*'
- en: '*Available* *water increased*'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可用水资源增加*'
- en: '**Correct answer**: *B. Food* *sources increased*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**正确答案**：*B. 食物来源增加*'
- en: This question requires the student to reason about the relationship between
    the increase in acorns (a food source for chipmunks) and the subsequent rise in
    the chipmunk population, rather than simply recalling a fact.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题要求学生推理橡树（松鼠的食物来源）增加与松鼠种群随后增加之间的关系，而不仅仅是简单地回忆一个事实。
- en: ARC serves as a strong benchmark to distinguish models that rely on pattern
    recognition from those capable of true reasoning, making it valuable for assessing
    AI robustness, comparing performance against humans, and developing more capable
    reasoning-based AI models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ARC 作为区分依赖模式识别的模型和能够进行真正推理的模型的强大基准，对于评估 AI 的鲁棒性、与人类比较性能以及开发更强大的基于推理的 AI 模型非常有价值。
- en: 'Here’s how you might evaluate on ARC:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在 ARC 上进行评估的示例：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code provides a standardized way to assess the multiple-choice reasoning
    capabilities of a given pre-trained language model on a challenging science question-answering
    benchmark. By tokenizing each question-choice pair separately and training/evaluating
    with a multiple-choice head, the process directly measures the model’s ability
    to select the correct answer from a set of plausible alternatives, offering insights
    into its understanding and reasoning over scientific concepts.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码提供了一个标准化的方法来评估给定预训练语言模型在具有挑战性的科学问答基准上的多项选择推理能力。通过分别对每个问题-选项对进行分词，并使用多项选择头进行训练/评估，这个过程直接衡量模型从一组合理的替代答案中选择正确答案的能力，从而对其对科学概念的理解和推理提供见解。
- en: 'To run the evaluation code, use the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行评估代码，请使用以下命令：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code evaluates a model on the **ARC-Challenge** dataset, which contains
    the more difficult questions from ARC.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码在**ARC-Challenge**数据集上评估模型，该数据集包含了ARC中的更难问题。
- en: Grade School Math 8K
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小学数学8K
- en: '**Grade School Math 8K** (**GSM8K**) is a dataset of 8.5K grade school math
    word problems ([https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math)).
    It’s designed to test an LLM’s ability to solve multi-step math problems. Here’s
    a simplified evaluation approach:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**小学数学8K**（**GSM8K**）是一个包含8.5K个小学数学应用题的数据集（[https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math)）。它旨在测试一个大型语言模型解决多步数学问题的能力。以下是一个简化的评估方法：'
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This Python code defines two functions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这段Python代码定义了两个函数：
- en: '`extract_answer`: This function uses a regular expression to find and extract
    the last numerical value from a given text string. If a number is found at the
    end of the string, it is returned as an integer. If no such number is found, the
    function returns `None`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_answer`：这个函数使用正则表达式从给定的文本字符串中查找并提取最后一个数值。如果字符串末尾找到数字，则将其作为整数返回。如果没有找到这样的数字，则函数返回`None`。'
- en: '`evaluate_gsm8k`: This function takes a language model, its tokenizer, and
    a dataset of math word problems. It iterates through each problem, encodes the
    question, generates a response from the model, decodes the response, extracts
    the predicted numerical answer using `extract_answer`, and compares it to the
    true answer to calculate the accuracy of the model on the provided GSM8k dataset.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluate_gsm8k`：这个函数接受一个语言模型、其分词器和一组数学应用题数据集。它遍历每个问题，编码问题，从模型生成响应，解码响应，使用`extract_answer`提取预测的数值答案，并将其与真实答案比较，以计算模型在提供的GSM8k数据集上的准确率。'
- en: This evaluation approach specifically targets the model’s ability to solve math
    word problems and, importantly, to produce the final numerical answer in a format
    that can be easily extracted. The `extract_answer` function highlights an assumption
    that the correct answer will be the last number mentioned in the model’s response.
    While this may not always hold true, it serves as a practical heuristic for this
    dataset. The overall process measures the model’s combined capabilities in understanding
    the problem, performing the necessary calculations, and presenting the result
    in an expected format.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种评估方法专门针对模型解决数学应用问题的能力，以及更重要的是，以易于提取的格式生成最终的数值答案。`extract_answer`函数强调了这样一个假设：正确答案将是模型响应中最后提到的数字。虽然这并不总是成立，但它为这个数据集提供了一个实用的启发式方法。整个过程衡量了模型在理解问题、执行必要的计算并以预期格式呈现结果的综合能力。
- en: 'To run the evaluation code, use the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行评估代码，请使用以下命令：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code generates responses to GSM8K problems and extracts the final numerical
    answer for comparison with the ground truth.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成对GSM8K问题的响应，并提取最终的数值答案以与真实答案进行比较。
- en: Next, we’ll explore coding and programming evaluation to see how we can measure
    the code generation and code execution abilities of an LLM; this is becoming increasingly
    vital in software development.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨编码和编程评估，看看我们如何衡量一个大型语言模型的代码生成和代码执行能力；这在软件开发中变得越来越重要。
- en: Coding and programming evaluation
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码和编程评估
- en: 'Evaluating an LLM’s coding abilities is becoming increasingly important. Let’s
    look at how we can use HumanEval to evaluate this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个大型语言模型的编码能力变得越来越重要。让我们看看我们如何使用HumanEval来评估这一点：
- en: '**HumanEval** is a benchmark for evaluating code generation capabilities. It
    includes a set of programming problems with unit tests.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**HumanEval**是一个评估代码生成能力的基准。它包含一系列带有单元测试的编程问题。'
- en: 'Here’s a simplified approach to evaluate on HumanEval:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简化的评估HumanEval的方法：
- en: 'The following code snippet sets up the core execution functionality. It defines
    a `run_code` function that takes generated code and a test case, combines them,
    and executes them in a safe subprocess with a timeout. It handles execution errors
    and timeouts gracefully, making it robust for evaluating potentially problematic
    code:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码片段设置了核心执行功能。它定义了一个`run_code`函数，该函数接受生成的代码和测试用例，将它们组合起来，并在一个具有超时限制的安全子进程中执行。它优雅地处理执行错误和超时，使其在评估可能存在问题的代码时非常稳健：
- en: '[PRE10]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following code example contains the main evaluation function that implements
    the HumanEval benchmark. It loads coding problems from a JSON file, uses a model
    to generate solutions for each problem, runs the solutions against test cases,
    and calculates the overall accuracy of the model’s performance:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码示例包含实现HumanEval基准的主要评估函数。它从JSON文件中加载编码问题，使用模型为每个问题生成解决方案，对解决方案进行测试用例测试，并计算模型性能的整体准确率：
- en: '[PRE11]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here’s a code snippet showing the usage of the evaluation framework. It’s a
    template for loading a specific code generation model and its tokenizer, then
    running the HumanEval evaluation on that model and printing the results. This
    section would need to be customized with actual model loading code depending on
    the specific model being used:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面是一个展示评估框架使用的代码片段。它是一个加载特定代码生成模型及其分词器，然后在该模型上运行HumanEval评估并打印结果的模板。本节需要根据所使用的特定模型进行实际模型加载代码的定制：
- en: '[PRE12]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We now turn to evaluating the conversational abilities of LLMs (LLMs), focusing
    on their performance in interactive dialogue—a critical capability for applications
    such as chatbots.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向评估大型语言模型（LLMs）的对话能力，重点关注它们在交互式对话中的表现——这是聊天机器人等应用的关键能力。
- en: Conversational ability assessment
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对话能力评估
- en: 'Evaluating the conversational abilities of LLMs is crucial for chatbot and
    dialogue system applications. Let’s look at a key benchmark in this area: MT-Bench.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLMs的对话能力对于聊天机器人和对话系统应用至关重要。让我们看看这个领域的一个关键基准：MT-Bench。
- en: '**MT-Bench** is a benchmark for evaluating multi-turn conversations. It assesses
    the model’s ability to maintain context and provide coherent responses over multiple
    turns.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**MT-Bench**是一个用于评估多轮对话的基准。它评估模型在多个回合中维持上下文并提供连贯回答的能力。'
- en: MT-Bench evaluations often combine automated scoring with human assessments
    to ensure a more comprehensive evaluation of AI models, particularly for tasks
    requiring nuanced reasoning, coherence, and contextual understanding. While automated
    metrics provide consistency and scalability, human evaluations help capture qualitative
    aspects such as reasoning depth, relevance, and fluency, which may not be fully
    captured by automated methods alone.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'MT-Bench评估通常结合自动评分和人工评估，以确保对AI模型进行更全面的评估，特别是对于需要细微推理、连贯性和上下文理解的任务。虽然自动指标提供了一致性和可扩展性，但人工评估有助于捕捉定性方面，如推理深度、相关性和流畅性，这些可能无法仅通过自动化方法完全捕捉。 '
- en: 'Here’s a simplified approach to evaluate on MT-Bench:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个在MT-Bench上评估的简化方法：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This function provides a basic framework for evaluating a conversational model
    based on its ability to incorporate context and generate relevant responses as
    judged by the presence of specific keywords. The simplified scoring method offers
    a coarse-grained assessment of the model’s output. A more sophisticated evaluation
    of MT-Bench typically involves human evaluation or more nuanced automated metrics
    that consider factors such as coherence, helpfulness, and correctness, which this
    simplified keyword-based approach does not capture. Therefore, the returned average
    score should be interpreted as a very preliminary indicator of performance based
    solely on the presence of specified keywords.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数提供了一个基于其结合上下文和生成相关响应能力的基本框架，这些响应通过特定关键词的存在来判断。简化的评分方法提供了对模型输出的粗略评估。MT-Bench的更复杂评估通常涉及人工评估或更细微的自动化指标，这些指标考虑了连贯性、有用性和正确性等因素，而简化的基于关键词的方法无法捕捉到这些因素。因此，返回的平均分数应被视为仅基于指定关键词存在性的非常初步的性能指标。
- en: 'The following code snippet shows how to use the evaluation framework with a
    specific model. It demonstrates loading the model and tokenizer, and then running
    the evaluation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何使用特定的评估框架。它演示了加载模型和分词器，然后运行评估：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This code simulates multi-turn conversations and scores responses based on the
    presence of expected keywords. In practice, MT-Bench often involves human evaluation
    or more sophisticated automated metrics.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码模拟多轮对话，并根据预期关键词的存在评分响应。在实践中，MT-Bench通常涉及人工评估或更复杂的自动化指标。
- en: To evaluate LLMs in real-world applications, we must also assess their commonsense
    and general knowledge benchmarks. Let’s look at how to do so.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM在实际应用中的表现，我们还必须评估它们的常识和一般知识基准。让我们看看如何做到这一点。
- en: Commonsense and general knowledge benchmarks
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常识和一般知识基准
- en: 'Assessing an LLM’s commonsense reasoning and general knowledge is crucial for
    many real-world applications. Let’s look at a key benchmark in this area: WinoGrande.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLM的常识推理和一般知识对于许多实际应用至关重要。让我们看看这个领域的关键基准：WinoGrande。
- en: '**WinoGrande** is a large-scale dataset of schemas, designed to test commonsense
    reasoning about complex situations described in natural language.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**WinoGrande**是一个大规模的架构数据集，旨在测试对自然语言描述的复杂情况进行常识推理的能力。'
- en: 'Here’s how you might evaluate on WinoGrande:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在WinoGrande上进行评估的方法：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function specifically assesses a language model’s ability to perform pronoun
    resolution, a crucial aspect of natural language understanding that requires contextual
    reasoning. By presenting pairs of sentences differing only in the pronoun and
    its antecedent, the Winogrande benchmark challenges models to identify the correct
    referent. Evaluating on this task provides insight into a model’s capacity to
    understand subtle semantic relationships and handle ambiguities in text, which
    is essential for more complex language processing tasks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数专门评估语言模型执行代词解析的能力，这是自然语言理解的一个关键方面，需要上下文推理。通过呈现只有代词及其先行词不同的句子对，Winogrande基准挑战模型识别正确的指代。在此任务上的评估提供了对模型理解微妙语义关系和处理文本歧义能力洞察，这对于更复杂的语言处理任务至关重要。
- en: 'Here’s a code example showing how to run the evaluation with a specific model:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个代码示例，展示了如何使用特定模型运行评估：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This code evaluates a model on the WinoGrande dataset, testing its ability to
    resolve ambiguities in sentences that require commonsense reasoning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码在WinoGrande数据集上评估模型，测试其解决需要常识推理的句子歧义的能力。
- en: Other commonly used benchmarks
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他常用基准
- en: 'Other commonly used benchmarks provide diverse ways to evaluate the performance
    and capabilities of language models in various domains and task complexities:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常用基准提供了多种方式来评估语言模型在各个领域和任务复杂度上的性能和能力：
- en: '**Instruction Following Evaluation** (**IFEval**): This benchmark assesses
    a model’s ability to follow natural language instructions across diverse tasks.
    It evaluates both task completion and instruction adherence.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指令遵循评估**（**IFEval**）：此基准评估模型在多样化任务中遵循自然语言指令的能力。它评估任务完成情况和指令遵循情况。'
- en: '**Big Bench Hard** (**BBH**): BBH is a subset of the larger BIG-Bench benchmark,
    focusing on particularly challenging tasks that even LLMs struggle with. It covers
    areas such as logical reasoning, common sense, and abstract thinking.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大型基准难题**（**BBH**）：BBH是更大的BIG-Bench基准的一个子集，专注于即使是LLM也难以应对的特别具有挑战性的任务。它涵盖了逻辑推理、常识和抽象思维等领域。'
- en: '**Massive Multitask Language Understanding – Professional** (**MMLU-PRO**):
    This is an expanded version of the original MMLU benchmark, with a focus on professional
    and specialized knowledge domains. It tests models on subjects such as law, medicine,
    engineering, and other expert fields.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大规模多任务语言理解 – 专业版**（**MMLU-PRO**）：这是原始MMLU基准的扩展版本，专注于专业和专门的知识领域。它测试模型在法律、医学、工程和其他专家领域等主题上的能力。'
- en: 'Here’s a comparison of IFEval, BBH, and MMLU-PRO:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是IFEval、BBH和MMLU-PRO的比较：
- en: IFEval focuses on evaluating a model’s ability to follow natural language instructions
    across various tasks, emphasizing task completion and instruction adherence rather
    than domain-specific knowledge or reasoning complexity
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IFEval专注于评估模型在多样化任务中遵循自然语言指令的能力，强调任务完成情况和指令遵循情况，而不是特定领域的知识或推理复杂性。
- en: BBH is a subset of BIG-Bench that targets especially difficult reasoning tasks,
    making it a strong test for logical reasoning, abstract thinking, and common sense—areas
    where LLMs typically struggle
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BBH是BIG-Bench的一个子集，特别针对尤其困难的推理任务，使其成为逻辑推理、抽象思维和常识——这些领域是LLMs通常挣扎的地方——的一个强大测试。
- en: MMLU-PRO extends MMLU to professional and specialized fields, assessing a model’s
    expertise in law, medicine, engineering, and other technical domains, making it
    ideal for evaluating domain-specific proficiency rather than general reasoning
    or instruction-following
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MMLU-PRO 将 MMLU 扩展到专业和特定领域，评估模型在法律、医学、工程和其他技术领域的专业知识，使其非常适合评估特定领域的熟练度，而不是一般推理或指令遵循
- en: 'Each benchmark serves a distinct purpose: IFEval for instruction-following,
    BBH for reasoning under difficulty, and MMLU-PRO for professional knowledge assessment.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个基准都有其独特的作用：IFEval 用于指令遵循，BBH 用于困难条件下的推理，MMLU-PRO 用于专业知识评估。
- en: Developing custom metrics and benchmarks
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发自定义指标和基准
- en: Custom metrics are essential because commonly used benchmarks such as MMLU,
    HumanEval, and SuperGLUE often provide a general evaluation framework but may
    not align with the specific requirements of a particular application. Custom metrics
    provide a more tailored and meaningful evaluation, allowing developers to align
    models with their specific performance goals.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义指标至关重要，因为常用的基准，如 MMLU、HumanEval 和 SuperGLUE，通常提供了一个通用的评估框架，但可能不符合特定应用的特定要求。自定义指标提供了更定制和有意义的评估，使开发者能够将模型与其特定的性能目标对齐。
- en: 'When creating custom metrics or benchmarks, consider the following best practices:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建自定义指标或基准时，请考虑以下最佳实践：
- en: '**Define clear objectives**: Determine exactly what aspects of model performance
    you want to measure. This could be task-specific accuracy, reasoning ability,
    or adherence to certain constraints.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**明确目标**：确定你想要衡量模型性能的哪些方面。这可能包括特定任务的准确性、推理能力或遵守某些约束。'
- en: '**Ensure dataset quality**: Curate a high-quality, diverse dataset that represents
    the full spectrum of challenges in your domain of interest. Consider factors such
    as the following:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保数据集质量**：精心策划一个高质量、多样化的数据集，代表你感兴趣领域中的所有挑战。考虑以下因素：'
- en: Balanced representation of different categories or difficulty levels
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类别或难度水平的平衡表示
- en: Removal of biased or problematic examples
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除有偏见或问题示例
- en: Inclusion of edge cases and rare scenarios
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含边缘案例和罕见场景
- en: '**Design robust evaluation criteria**: Develop clear, quantifiable metrics
    for assessing performance. This might involve the following:'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设计稳健的评估标准**：为评估性能开发清晰、可量化的指标。这可能包括以下内容：'
- en: Creating rubrics for human evaluation
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为人工评估创建评分标准
- en: Defining automated scoring mechanisms
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义自动评分机制
- en: Establishing baselines for comparison
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立比较的基线
- en: '**Consider multiple dimensions**: Don’t rely on a single metric. Evaluate models
    across various dimensions such as the following:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑多个维度**：不要依赖于单一指标。从以下维度评估模型，例如：'
- en: Accuracy
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性
- en: Consistency
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致性
- en: Safety and bias mitigation
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性和偏见缓解
- en: Efficiency (e.g., inference time and resource usage)
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 效率（例如，推理时间和资源使用）
- en: '**Implement rigorous testing protocols**: Establish standardized procedures
    for running benchmarks, including the following:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实施严格的测试协议**：建立运行基准的标准程序，包括以下内容：'
- en: Consistent model configurations and prompts
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致的模型配置和提示
- en: Multiple runs to account for variability
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到可变性进行多次运行
- en: Statistical analysis of results
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果的统计分析
- en: '**Iterate and refine**: Continuously improve your benchmark based on feedback
    and emerging challenges in the field. This might involve the following:'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代和改进**：根据反馈和领域中的新兴挑战持续改进你的基准。这可能包括以下内容：'
- en: Adding new test cases
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加新的测试案例
- en: Adjusting scoring methods
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整评分方法
- en: Incorporating insights from the research community
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合研究社区的见解
- en: Interpreting and comparing LLM evaluation results
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释和比较 LLM 评估结果
- en: 'When interpreting and comparing results across these diverse benchmarks, it’s
    important to consider the strengths and limitations of each metric. It is also
    important to consider differences in model size, training data, and fine-tuning
    approaches. Here’s an example of how you might visualize and compare results across
    multiple benchmarks:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释和比较这些不同基准的结果时，考虑每个指标的优势和局限性很重要。同时，也要考虑模型大小、训练数据和微调方法的差异。以下是如何可视化和比较多个基准结果的示例：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This code creates a bar chart comparing two models across different benchmarks,
    providing a visual aid for interpreting results.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码创建了一个条形图，比较了不同基准下两个模型的性能，为解释结果提供了视觉辅助。
- en: 'When interpreting these results, consider the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释这些结果时，请考虑以下：
- en: '**Task specificity**: Some benchmarks (e.g., GSM8K for math and HumanEval for
    coding) test specific capabilities. A model might excel in one area but underperform
    in others.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务特定性**：一些基准（例如，GSM8K用于数学和HumanEval用于编码）测试特定的能力。一个模型可能在某个领域表现出色，但在其他方面表现不佳。'
- en: '**Generalization**: Look for consistent performance across diverse tasks. This
    indicates good generalization abilities.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化能力**：寻找在多样化任务中表现一致的性能。这表明具有良好的泛化能力。'
- en: '**Improvement margins**: Consider where the largest improvements can be made.
    This can guide future fine-tuning or training efforts.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进空间**：考虑可以取得最大改进的地方。这可以指导未来的微调或训练工作。'
- en: '**Real-world relevance**: Prioritize benchmarks that align closely with your
    intended use case.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现实世界相关性**：优先考虑与您预期用例紧密相关的基准。'
- en: '**Limitations**: Be aware of each benchmark’s limitations. For example, automated
    metrics might not capture nuanced aspects of language understanding or generation.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局限性**：注意每个基准的局限性。例如，自动化指标可能无法捕捉到语言理解或生成的细微方面。'
- en: 'Here’s an example of how you might summarize and interpret these results:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个如何总结和解释这些结果的例子：
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This function provides a textual interpretation of the results, highlighting
    performance differences and their implications.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数提供结果的文本解释，突出性能差异及其影响。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Evaluating LLMs requires a variety of benchmarks. By understanding and effectively
    using these evaluation techniques, you can make informed decisions about model
    performance and guide further improvements in your LLM projects.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLMs需要各种基准。通过理解和有效使用这些评估技术，您可以就模型性能做出明智的决定，并指导您在LLM项目中的进一步改进。
- en: As we move forward, the next chapter will delve into cross-validation techniques
    specifically tailored for LLMs. We’ll explore methods for creating appropriate
    data splits for pre-training and fine-tuning, as well as strategies for few-shot
    and zero-shot evaluation. This will build upon the evaluation metrics we’ve discussed
    here, providing a more comprehensive framework for assessing LLM performance and
    generalization capabilities across different domains and tasks.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续前进，下一章将深入探讨专门针对大型语言模型（LLMs）的交叉验证技术。我们将探讨创建适当的数据拆分方法以进行预训练和微调，以及用于少样本和零样本评估的策略。这将基于我们在此处讨论的评估指标，为评估LLMs在不同领域和任务中的性能和泛化能力提供一个更全面的框架。
