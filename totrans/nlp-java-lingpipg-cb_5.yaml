- en: Chapter 5. Finding Spans in Text – Chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covers the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of sentence detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning sentence detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marking embedded chunks in a string – sentence chunk example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paragraph detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple noun phrases and verb phrases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular expression-based chunking for NER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary-based chunking for NER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating between word tagging and chunks – BIO codec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HMM-based NER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixing the NER sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRFs for chunking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NER using CRFs with better features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will tell us how to work with spans of text that typically cover
    one or more words/tokens. The LingPipe API represents this unit of text as a chunk
    with corresponding chunkers that produce chunkings. The following is some text
    with character offsets indicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Chunking the preceding text into sentences will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding in a chunking for named entities adds entities for LingPipe and Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define the named-entity chunkings with respect to their offsets from
    the sentences that contain them; this will make no difference to LingPipe, but
    Java will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is the basic idea of chunks. There are lots of ways to make them.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentences in written text roughly correspond to a spoken utterance. They are
    the standard unit of processing words in industrial applications. In almost all
    mature NLP applications, sentence detection is a part of the processing pipeline
    even in the case of tweets, which can have more than one sentence in the allotted
    140 characters.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we will play with some data first. Enter the following command in
    the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The program will provide a prompt for your sentence-detection experimentation.
    A new line / return terminates the text to be analyzed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is worth playing around a bit with different inputs. The following are some
    examples that explore the properties of the sentence detector. Drop the capitalized
    beginning of a sentence; this will prevent the detection of the second sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The detector does not require a final period—this is configurable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The detector balances parentheses, which will not allow sentences to break
    inside parentheses—this is also configurable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This sentence detector is a heuristic-based or rule-based sentence detector.
    A statistical sentence detector would be a reasonable approach as well. We will
    get through the entire source to run the detector, and later, we will discuss
    the modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Working from the top of the `main` class, the Boolean `endSent` parameter controls
    whether the string that is sentence detected is assumed to end with a sentence,
    no matter what—this means that the last character is a sentence boundary always—it
    does not need to be a period or other typical sentence-ending mark. Change it
    and try a sentence without a final period, and the result will be that no sentence
    is detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next Boolean `parenS` declaration gives priority to parentheses over sentence
    makers when finding sentences. Next, the actual sentence chunker will be set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tokFactory` should be familiar to you from [Chapter 2](part0027_split_000.html#page
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*.
    The `sentenceChunker` then can be constructed. Following is the standard I/O code
    for command-line interaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the text, then the sentence detector is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The chunking provides a `Set<Chunk>` parameter, which will noncontractually
    provide an appropriate ordering of `Chunks`; they will be added as per the `ChunkingImpl`
    Javadoc. The truly paranoid programmer might impose the proper sort order, which
    we will cover later in the chapter when we have to handle overlapping chunks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will check to see if any sentences were found, and if we don''t find
    them, we will report to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the first exposure to the `Chunker` interface in the book,
    and a few comments are in order. The `Chunker` interface generates the `Chunk`
    objects, which are typed and scored contiguous-character sequences over `CharSequence`—usually,
    `String`. `Chunks` can overlap. The `Chunk` objects are stored in `Chunking`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: First, we recovered the underlying text string `textStored` that the chunks
    are based on. It is the same string as `text`, but we wanted to illustrate this
    potentially useful method of the `Chunking` class, which can come up in recursive
    or other contexts when the chunking is far removed from where `CharSequence` that
    it uses is unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining `for` loop iterates over the sentences and prints them out with
    the `substring()` method of `String`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before moving on to how to roll your own sentence detector, it is worth mentioning
    that LingPipe has `MedlineSentenceModel`, which is oriented towards the kind of
    sentences found in the medical research literature. It has seen a lot of data
    and should be a starting place for your own sentence-detection efforts over these
    kinds of data.
  prefs: []
  type: TYPE_NORMAL
- en: Nested sentences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sentences, particularly in literature, can contain nested sentences. Consider
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding sentence will be marked up properly as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This sort of nesting is different from a linguist''s concept of a nested sentence,
    which is based on grammatical role. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This sentence consists of two linguistically complete sentences joined by `and`.
    The difference between the two is that the former is determined by punctuation
    and the latter by a grammatical function. Whether this distinction is significant
    or not can be debated. However, the former case is much easier to recognize programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: However, we have rarely needed to model nested sentences in industrial contexts,
    but we took it on in our MUC-6 system and various coreference resolution systems
    in research contexts. This is beyond the scope of a recipe book, but be aware
    of the issue. LingPipe has no out-of-the-box capabilities for nested sentence
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of sentence detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like most of the things we do, we want to be able to evaluate the performance
    of our components. Sentence detection is no different. Sentence detection is a
    span annotation that differs from our previous evaluations for classifiers and
    tokenization. As text can have characters that are not in any sentence, there
    is a notion of sentence start and sentence end. An example of characters that
    don't belong in a sentence will be JavaScript from an HTML page.
  prefs: []
  type: TYPE_NORMAL
- en: The following recipe will take you through the steps of creating evaluation
    data and running it past an evaluation class.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to evaluate sentence detection:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a text editor and copy and paste some literary gem that you want to evaluate
    sentence detection with, or you can go with our supplied default text, which is
    used if you don't provide your own data. It is easiest if you stick to plain text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Insert balanced `[` and `]` to indicate the beginnings and ends of sentences
    in the text. If the text already contains either `[` or `]`, pick another character
    that is not in the text as a sentence delimiter—curly brackets or slashes are
    a good choice. If you use different delimiters, you will have to modify the source
    appropriately and recreate the JAR file. The code assumes a single-character text
    delimiter. An example of a sentence-annotated text from *The Hitchhiker''s Guide
    to the Galaxy* is as follows—note that not every character is in a sentence; some
    whitespaces are between sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get yourself a command line and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For this data, the code will display two sentences that match perfectly with
    the sentences annotated with `[]`, as indicated by the `TruePos` label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A good exercise is to modify the annotation a bit to force errors. We will
    move the first sentence boundary one character in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rerunning the modified annotation file after saving it yields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By changing the truth annotation, a false negative is produced, because the
    sentence span was missed by one character. In addition, a false positive is created
    by the sentence detector that recognizes the 0-83 character sequence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is a good idea to play around with the annotation and various kinds of data
    to get a feel of how evaluation works and the capabilities of the sentence detector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The class starts by digesting the annotated text and storing the sentence chunks
    in an evaluation object. Then, the sentence detector is created, just as we did
    in the previous recipe. The code finishes by applying the created sentence detector
    to the text, and the results are printed.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing annotated data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given text annotated with `[]` for sentence boundaries means that the correct
    offsets of the sentences have to be recovered, and the original unannotated text
    must be created, that is, without any `[]`. Span parsers can be a bit tricky to
    code, and the following is offered for simplicity rather than efficiency or proper
    coding technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code reads in the entire file as a single `char[]` array with
    an appropriate character encoding. Also, note that for large files, a streaming
    approach will be more memory friendly. Next, an accumulator for unannotated chars
    is setup as a `StringBuilder` object with the `rawChars` variable. All characters
    encountered that are not either a `[` or `]` will be appended to the object. The
    remaining code sets up counters for sentence starts and ends that are indexed
    into the unannotated character array and an accumulator for `Set<Chunk>` for annotated
    sentence segments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `for` loop advances one character at a time over the annotated
    character sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The first `if (chars[i] == '[')` tests for starts of sentences in the annotation
    and sets the start variable to the length of `rawChars`. The iteration variable
    `i` includes the length added by the annotations. The corresponding `else if (chars[i]
    == ']')` statement handles the end of sentence case. Note that there are no error
    checks for this parser—this is a very bad idea because annotation errors are very
    likely if entered with a text editor. However, this is motivated by keeping the
    code as simple as possible. Later in the recipe, we will provide an example with
    some minimal error checking. Once the end of a sentence is found, a chunk is created
    for the sentence with `ChunkFactory.createChunk` with offsets and for the standard
    LingPipe sentence type `SentenceChunker.SENTENCE_CHUNK_TYPE`, which is required
    for the upcoming evaluation classes to work properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining `else` statement applies for all the characters that are not
    sentence boundaries, and it simply adds the character to the `rawChars` accumulator.
    The result of this accumulator can be seen outside the `for` loop when `String
    unannotatedText` is created. Now, we have sentence chunks indexed correctly into
    a text string. Next, we will create a proper `Chunking` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `ChunkingImpl` implementing class (`Chunking` is an interface) requires
    the underlying text on construction, which is why we didn't just populate it in
    the preceding loop. LingPipe generally tries to make object construction complete.
    If Chunkings can be created without the underlying `CharSequence` method, then
    what will be returned when the `charSequence()` method is called? An empty string
    is actively misleading. Alternatively, returning `null` needs to be caught and
    dealt with. Better to just force the object to make sense of construction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on, we will see the standard configuration of the sentence chunker from
    the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The interesting stuff follows with an evaluator that takes `sentenceChunker`
    as being evaluated as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, the `handle(sentChunking)` method will take the text we just parsed
    into `Chunking` and run the sentence detector on `CharSequence` supplied in `sentChunking`
    and set up the evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will just get the evaluation data and work our way through the differences
    between the truth sentence detection and what the system did:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This recipe does not cover all the evaluation methods—check out the Javadoc—but
    it does provide what a sentence detection tuner will likely be most in need of;
    this is a listing of what the sentence detector got right (true positives), sentences
    it found but were wrong (false positives), and sentences it missed (false negatives).
    Note that true negatives don't make much sense in span annotations, because they
    will be the set of all the possible spans that are not in the truth sentence detection.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning sentence detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lots of data will resist the charms of `IndoEuropeanSentenceModel`, so this
    recipe will provide a starting place to modify sentence detection to meet new
    kinds of sentences. Unfortunately, this is a very open-ended area of system building,
    so we will focus on techniques rather than likely formats for sentences.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe will follow a well-worn pattern: create evaluation data, set up
    evaluation, and start hacking. Here we go:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Haul out your favorite text editor and mark up some data—we will stick to the
    `[` and `]` markup approach. The following is an example that runs afoul of our
    standard `IndoEuropeanSentenceModel`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will put the preceding sentence in `data/saki.sentDetected.txt` and run
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The single false positive corresponds to the one sentence found, and the two
    false negatives are the two sentences not found that we annotated here. What happened?
    The sentence model missed `people's.` as a sentence end. If the apostrophe is
    removed, the sentence is detected properly—what is going on?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the code running in the background. `IndoEuropeanSentenceModel`
    extends `HeuristicSentenceModel` by configuring several categories of tokens from
    the Javadoc for `HeuristicSentenceModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Possible stops**: These are tokens that are allowed to be the final ones
    in a sentence. This set typically includes sentence-final punctuation tokens,
    such as periods (.) and double quotes (").'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impossible penultimates**: These are tokens that might not be the penultimate
    (second-to-last) token in a sentence. This set is typically made up of abbreviations
    or acronyms, such as `Mr`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impossible starts**: These are tokens that might not be the first ones in
    a sentence. This set typically includes punctuation characters that should be
    attached to the previous sentence, such as end quotes ('''').'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IndoEuropeanSentenceModel` is not configurable, but from the Javadoc, it is
    clear that all single characters are considered impossible penultimates. The words
    `people''s` is tokenized into `people`, `''`, `s`, and `.`. The single character
    `s` is penultimate to the `.` and is thus blocked. How to fix this?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A few options present themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: Ignore the mistake assuming that it won't happen frequently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fix by creating a custom sentence model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fix by modifying the tokenizer to not separate apostrophes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a complete sentence-detection model for the interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second option, create a custom sentence model, is handled most easily by
    copying the source from `IndoEuropeanSentenceModel` into a new class and modifying
    it, as the relevant data structures are private. This is done to simplify the
    serialization of the class—very little configuration needs to be written to disk.
    In the example classes, there is a `MySentenceModel.java` file that differs by
    obvious changes in the package name and imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code just comments out two of the likely single-letter cases of
    penultimate tokens that are a single-word character. To see it at work, change
    the sentence model to `SentenceModel sentenceModel = new MySentenceModel();` in
    the `EvaluateAnnotatedSentences.java` class and recompile and run it.
  prefs: []
  type: TYPE_NORMAL
- en: If you see the preceding code as a reasonable balancing of finding sentences
    that end in likely contractions versus non-sentence cases such as `[Hunter S.
    Thompson is a famous fellow.]`, which will detect `S.` as a sentence boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Extending `HeuristicSentenceModel` can work well for many sorts of data. Mitzi
    Morris built `MedlineSentenceModel.java`, which is designed to work well with
    the abstracts provided in the MEDLINE research index.
  prefs: []
  type: TYPE_NORMAL
- en: One way to look at the preceding problem is that contractions should not be
    broken up into tokens for the purpose of sentence detection. `IndoEuropeanTokenizerFactory`
    should be tuned up to keep "people's" and other contractions together. While it
    initially seems slightly better that the first solution, it might well run afoul
    of the fact that `IndoEuropeanSentenceModel` was tuned with a particular tokenization
    in mind, and the consequences of the change are unknown in the absence of an evaluation
    corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The other option is to write a completely novel sentence-detection class that
    supports the `SentenceModel` interface. Faced with a highly novel data collection
    such as Twitter feeds, we will consider using a machine-learning-driven span-annotation
    technique such as HMMs or CRFs covered in [Chapter 4](part0051_split_000.html#page
    "Chapter 4. Tagging Words and Tokens"), *Tagging Words and Tokens*, and at the
    end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Marking embedded chunks in a string – sentence chunk example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The method of displaying chunkings in the previous recipes is not well suited
    for applications that need to modify the underlying string. For example, a sentiment
    analyzer might want to highlight only sentences that are strongly positive and
    not mark up the remaining sentences while still displaying the entire text. The
    slight complication in producing the marked-up text is that adding markups changes
    the underlying string. This recipe provides working code to insert the chunking
    by adding chunks in reverse.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While this recipe may not be technically complex it is useful to get span annotations
    into a text without out having to invent the code from whole cloth. The `src/com/lingpipe/coobook/chapter5/WriteSentDetectedChunks`
    class has the referenced code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence chunking is created as per the first sentence-detection recipe.
    The following code extracts the chunks as `Set<Chunk>` and then sorts them by
    `Chunk.LONGEST_MATCH_ORDER_COMPARITOR`. In the Javadoc, the comparator is defined
    as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Compares two chunks based on their text position. A chunk is greater if it
    starts later than another chunk, or if it starts at the same position and ends
    earlier.*'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'There is also `TEXT_ORDER_COMPARITOR`, which is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will iterate over the chunks in the reverse order, which eliminates
    having to keep an offset variable for the changing length of the `StringBuilder`
    object. Offset variables are a common source of bugs, so this recipe avoids them
    as much as possible but does non-standard reverse loop iteration, which might
    be worse:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code does a very simple sentiment analysis by looking for the
    string `like` in the sentence and marking that sentence if `true`. Note that this
    code cannot handle overlapping chunks or nested chunks. It assumes a single, non-overlapping
    chunk set. Some example output is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To print nested chunks, look at the *Paragraph* *detection* recipe that follows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paragraph detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The typical containing structure of a set of sentences is a paragraph. It can
    be set off explicitly in a markup language such as `<p>` in HTML or with two or
    more new lines, which is how paragraphs are usually rendered. We are in the part
    of NLP where no hard-and-fast rules apply, so we apologize for the hedging. We
    will handle some common examples in this chapter and leave it to you to generalize.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have never set up an evaluation harness for paragraph detection, but it
    can be done in ways similar to sentence detection. This recipe, instead, will
    illustrate a simple paragraph-detection routine that does something very important—maintain
    offsets into the original document with embedded sentence detection. This attention
    to detail will serve you well if you ever need to mark up the document in a way
    that is sensitive to sentences or other subspans of the document, such as named
    entities. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It gets transformed into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The example code has little to offer in paragraph-detection techniques. It
    is an open-ended problem, and you will have to use your wiles to solve it. Our
    paragraph detector is a pathetic `split("\n\n")` that, in a more sophisticated
    approach, will take into account context, characters, and other features that
    are far too idiosyncratic for us to cover. Here is the beginning of the code that
    reads the entire document as a string and splits it into an array. Note that `paraSeperatorLength`
    is the number of characters that form the basis of the paragraph split—if the
    length of the split varies, then that length will have to be associated with the
    corresponding paragraph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The real point of the recipe is to help with the mechanics of maintaining character
    offsets into the original document and show embedded processing. This will be
    done by keeping two separate chunkings: one for paragraphs and one for sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the sentence detector will be set up in the same way as one in the previous
    recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The chunking iterates over the array of paragraphs and builds a sentence chunking
    for each paragraph. The somewhat-complicated part of this approach is that the
    sentence chunk offsets are with respect to the paragraph string, not the entire
    document. So, the variables'' starts and ends are updated with document offsets
    in the code. Chunks have no methods to adjust starts and ends, so a new chunk
    must be created, `adjustedSentChunk`, with appropriate offsets into the paragraph
    start and must be added to `sentChunking`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The rest of the loop adds the paragraph chunk and then updates the start of
    the paragraph with the length of the paragraph plus the length of the paragraph
    separator. This will complete the creation of correctly offset sentences and paragraphs
    into the original document string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The rest of the program is concerned with printing out the paragraphs and sentences
    with some markup. First, we will create a chunking that has both sentence and
    paragraph chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, `displayChunking` will be sorted by recovering `chunkSet`, converting
    it into an array of chunks and the application of the static comparator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will use the same trick as we did in the *Marking embedded chunks in a string
    – sentence chunk example* recipe, which is to insert the markup backwards into
    the string. We will have to keep an offset counter, because nested sentences will
    extend the finishing paragraph mark placement. The approach assumes that no chunks
    overlap and that sentences are contained within paragraphs always:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it for the recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simple noun phrases and verb phrases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will show you how to find simple **noun phrases** (**NP**) and **verb
    phrases** (**VP**). By "simple", we mean that there is no complex structure within
    the phrases. For example, the complex NP "The rain in Spain" will be broken into
    two simple NP chunks "The rain" and "Spain". These phrases are also called "basal".
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will not go into the details of how the basal NPs/VPs are calculated
    but rather how to use the class—it can come in handy, and the source can be included
    if you want to sort out how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like many of the recipes, we will provide a command-line-interactive interface
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Haul up the command line and type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `main()` method starts by deserializing a part-of-speech tagger and then
    creating `tokenizerFactory`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, `PhraseChunker` is constructed, which is a heuristic approach to the
    problem. Look at the source to see how it works—it scans the input left to right
    for NP/VP starts and attempts to add to the phrase incrementally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Our standard console I/O code is next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the input is tokenized, POS is tagged, and the tokens and tags are printed
    out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The NP/VP chunkings are then calculated and printed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: There is a more comprehensive tutorial at [http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html).
  prefs: []
  type: TYPE_NORMAL
- en: Regular expression-based chunking for NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Named Entity Recognition** (**NER**) is the process of finding mentions of
    specific things in text. Consider a simple name; location-named entity recognizer
    might find `Ford Prefect` and `Guildford` as the name and location mentions, respectively,
    in the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We will start by building rule-based NER systems and move up to machine-learning
    methods. Here, we'll take a look at building an NER system that can extract e-mail
    addresses from text.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Enter the following command into the command prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Interaction with the program proceeds as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see that both `foo@bar.com` as well as `foo.bar@gmail.com` were returned
    as valid `e-mail` type chunks. Also, note that the final period in the sentence
    is not part of the second e-mail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A regular expression chunker finds chunks that match the given regular expression.
    Essentially, the `java.util.regex.Matcher.find()` method is used to iteratively
    find matching text segments, and these are then converted into the Chunk objects.
    The `RegExChunker` class wraps these steps. The code of `src/com/lingpipe/cookbook/chapter5/RegExNer.java`
    is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: All the interesting work was done in the preceding lines of code. The `emailRegex`
    is pulled off of the Internet—see the following for the source, and the remaining
    bits are setting up `chunkType` and `score`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the code reads in the input and prints out the chunking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The regular expression for the e-mail address match is from [regexlib.com](http://regexlib.com)
    at [http://regexlib.com/DisplayPatterns.aspx?cattabindex=0&categoryId=1](http://regexlib.com/DisplayPatterns.aspx?cattabindex=0&categoryId=1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary-based chunking for NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many websites and blogs and certainly on web forums, you might see keyword
    highlighting that links pages you can buy a product from. Similarly, news websites
    also provide topic pages for people, places, and trending events, such as the
    one at [http://www.nytimes.com/pages/topics/](http://www.nytimes.com/pages/topics/).
  prefs: []
  type: TYPE_NORMAL
- en: A lot of this is fully automated and is easy to do with a dictionary-based `Chunker`.
    It is straightforward to compile lists of names for entities and their types.
    An exact dictionary chunker extracts chunks based on exact matches of tokenized
    dictionary entries.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of the dictionary-based chunker in LingPipe is based on the
    Aho-Corasick algorithm which finds all matches against a dictionary in linear
    time independent of the number of matches or size of the dictionary. This makes
    it much more efficient than the naïve approach of doing substring searches or
    using regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the IDE of your choice run the `DictionaryChunker` class in the `chapter5`
    package or type the following using the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since this particular chunker example is biased (very heavily) towards the
    Hitchhikers Guide, let''s use a sentence that involves some of the characters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we have overlapping chunks from `Heart` and `Heart of Gold`. As we
    will see, this can be configured to behave differently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dictionary-based NER drives a great deal of automatic linking against unstructured
    text data. We can build one using the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of the code will create `MapDictionary<String>` to store the
    dictionary entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will populate the dictionary with `DictionaryEntry<String>`, which
    includes type information and a score that will be used to create chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: In the `DictionaryEntry` constructor, the first argument is the phrase, the
    second string argument is the type, and the final double argument is the score
    for the chunk. Dictionary entries are always case sensitive. There is no limit
    to the number of different entity types in a dictionary. The scores will simply
    be passed along as chunk scores in the dictionary-based chunker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will build `Chunker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: An exact dictionary chunker might be configured either to extract all the matching
    chunks to restrict the results to a consistent set of non-overlapping chunks via
    the `returnAllMatches` boolean. Look at the Javadoc to understand the exact criteria.
    There is also a `caseSensitive` boolean. The chunker requires a tokenizer, as
    it matches tokens as symbols, and whitespaces are ignored in the matching process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is our standard I/O code for console interaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining code creates a chunking, goes through the chunks, and prints
    them out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Dictionary chunkers are very useful even in machine-learning-based systems.
    There tends to always be a class of entities that are best identified this way.
    The *Mixing the NER sources* recipe addresses how to work with multiple sources
    of named entities.
  prefs: []
  type: TYPE_NORMAL
- en: Translating between word tagging and chunks – BIO codec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](part0051_split_000.html#page "Chapter 4. Tagging Words and Tokens"),
    *Tagging Words and Tokens*, we used HMMs and CRFs to apply tags to words/tokens.
    This recipe addresses the case of creating chunks from taggings that use the **Begin,
    In, and Out** (**BIO**) tags to encode chunkings that can span multiple words/tokens.
    This, in turn, is the basis of modern named-entity detection systems.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The standard BIO-tagging scheme has the first token in a chunk of type X tagged
    B-X (begin), with all the subsequent tokens in the same chunk tagged I-X (in).
    All the tokens that are not in chunks are tagged O (out). For example, the string
    with character counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be tagged as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding chunks will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The program will show the simplest mapping between taggings and chunkings and
    the other way around:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The program first prints out the string that will be tagged with a tagging:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the chunking is printed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, the tagging is created from the chunking just displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code starts by manually constructing `StringTagging`—we will see HMMs and
    CRFs do the same programmatically, but here it is explicit. It then prints out
    the created `StringTagging`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, it will construct `BioTagChunkCodec` and convert the tagging just printed
    out to a chunking followed by printing the chunking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining code reverses the process. First, a different `BioTagChunkCodec`
    is created with `boolean` `enforceConsistency`, which, if `true`, checks that
    the tokens created by the supplied tokenizer align exactly with the chunk begins
    and ends. Without the alignment we end up with a perhaps untenable relationship
    between chunks and tokens depending on the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The last `for` loop simply prints out the tagging returned by the `codec2.toStringTagging()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recipe works through the simplest example of mapping between taggings and
    chunkings. `BioTagChunkCodec` also takes the `TagLattice<String>` objects to produce
    n-best output, as will be shown in the HMM and CRF chunkers to follow.
  prefs: []
  type: TYPE_NORMAL
- en: HMM-based NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`HmmChunker` uses an HMM to perform chunking over tokenized character sequences.
    Instances contain an HMM decoder for the model and tokenizer factory. The chunker
    requires the states of the HMM to conform to a token-by-token encoding of a chunking.
    It uses the tokenizer factory to break the chunks down into sequences of tokens
    and tags. Refer to the *Hidden Markov Models (HMM) – part of speech* recipe in
    [Chapter 4](part0051_split_000.html#page "Chapter 4. Tagging Words and Tokens"),
    *Tagging Words and Tokens*.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at training `HmmChunker` and using it for the `CoNLL2002` Spanish
    task. You can and should use your own data, but this recipe assumes that training
    data will be in the `CoNLL2002` format.
  prefs: []
  type: TYPE_NORMAL
- en: Training is done using an `ObjectHandler` which supplies the training instances.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we want to train this chunker, we need to either label some data using the
    **Computational Natural Language Learning** (**CoNLL**) schema or use the one
    that's publicly available. For speed, we'll choose to get a corpus that is available
    in the CoNLL 2002 task.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ConNLL is an annual meeting that sponsors a bakeoff. In 2002, the bakeoff
    involved Spanish and Dutch NER.
  prefs: []
  type: TYPE_NORMAL
- en: The data can be downloaded from [http://www.cnts.ua.ac.be/conll2002/ner.tgz](http://www.cnts.ua.ac.be/conll2002/ner.tgz).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to what we showed in the preceding recipe; let''s take a look at what
    this data looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: With this encoding scheme, the phrases *El Abogado General del Estado* and *Daryl
    Williams* are coded as persons, with their beginning and continuing tokens picked
    out with tags B-PER and I-PER, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a few formatting errors in the data that must be fixed before our
    parsers can handle them. After unpacking `ner.tgz` in the `data` directory you
    will have to go to `data/ner/data`, unzip the following files, and modify as indicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the command line, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It will run the training on the CoNLL training data if the model doesn''t exist.
    It might take a while, so be patient. The output of the training will be:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the prompt to enter the text is presented, type in some Spanish text from
    the CoNLL test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What we will see is a number of entities, their confidence score, the span in
    the original sentence, the type of entity, and the phrase that represents this
    entity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To find out the correct tags, take a look at the annotated `esp.testa` file,
    which contains the following tags for this sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This can be read as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, we got all the ones with 1.000 confidence correct and the rest wrong. This
    can help us set up a threshold in production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `CharLmRescoringChunker` provides a long-distance character language model-based
    chunker that operates by rescoring the output of a contained character language
    model HMM chunker. The underlying chunker is an instance of `CharLmHmmChunker`,
    which is configured with the specified tokenizer factory, n-gram length, number
    of characters, and interpolation ratio provided in the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the `main()` method; here, we will set up the chunker, train
    it if it doesn''t exist, and then allow for some input to get the named entities
    out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The training file will be in the correct place if you unpack the CoNLL data
    (`tar –xvzf ner.tgz`) in the data directory. Remember to correct the annotation
    on line 221619 of `esp.train`. If you use other data, then modify and recompile
    the class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next bit of code trains the model if it doesn''t exist and then loads the
    serialized version of the chunker. If you have questions about deserialization,
    see the *Deserializing and running a classifier* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*. Consider the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The `trainHMMChunker()` method starts with some `File` bookkeeping before setting
    up configuration parameters for `CharLmRescoringChunker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a look at parsing the CoNLL data. The source for this class
    is `src/com/lingpipe/cookbook/chapter5/Conll2002ChunkTagParser`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The statics set up the configuration of the `com.aliasi.tag.LineTaggingParser`
    LingPipe class. CoNLL, like many available data sets, uses a token/tag per line
    format, which is meant to be very easy to parse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The `LineTaggingParser` constructor requires a regular expression that identifies
    the token and tag strings via grouping. There is additionally a regular expression
    for lines to ignore and finally, a regular expression for sentence ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set up `TagChunkCodec`; this will handle the mapping from tagged tokens
    in the BIO format to proper chunks. See the previous recipe, *Translating between
    word tagging and chunks – BIO codec*, for more about what is going on here. The
    remaining parameters customize the tags to match those of the CoNLL training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the class provides methods for `parseString()`, which is immediately
    sent to the `LineTaggingParser` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the `ObjectHandler` parser is properly configured with the codec and
    supplied handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: It's a lot of odd-looking code, but all this does is set up a parser to read
    the lines from the input file and extract chunkings out of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s go back to the `main` method and look at the output loop. We
    will set up the `MAX_NBEST` chunkings value as 10 and then invoke the `nBestChunkings`
    method on the chunker. This provides the top 10 chunks and their probabilistic
    scores. Based on an evaluation, we can choose to cut off at a particular score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more details on running a complete evaluation, refer to the evaluation section
    of the tutorial at [http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more details on `CharLmRescoringChunker` and `HmmChunker`, refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/AbstractCharLmRescoringChunker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/AbstractCharLmRescoringChunker.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixing the NER sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've seen how to build a few different types of NERs, we can look
    at how to combine them. In this recipe, we will take a regular expression chunker,
    a dictionary-based chunker, and an HMM-based chunker and combine their outputs
    and look at overlaps.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will just initialize a few chunkers in the same way we did in the past few
    recipes and then pass the same text through these chunkers. The easiest possibility
    is that each chunker returns a unique output. For example, let''s consider a sentence
    such as "President Obama was scheduled to give a speech at the G-8 conference
    this evening". If we have a person chunker and an organization chunker, we might
    only get two unique chunks out. However, if we add a `Presidents of USA` chunker,
    we will get three chunks: `PERSON`, `ORGANIZATION`, and `PRESIDENT`. This very
    simple recipe will show us one way to handle these cases.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the command line or equivalent in your IDE, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The usual interactive prompt follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see the output from the three chunkers: `neChunking` is the output of an
    HMM chunker that is trained to return the MUC-6 entities, `pChunking` is a simple
    regular expression that recognizes male pronouns, and `dChunking` is a dictionary
    chunker that recognizes US Presidents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With overlaps allowed, we will see the chunks for `PRESIDENT` as well as `PERSON`
    in the merged output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With overlaps disallowed, they will be added to the set overlapped chunks and
    removed from the unique chunks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We initialized three chunkers that should be familiar to you from the previous
    recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will just chunk our input text via all three chunkers, combine the
    chunks into one set, and pass our `getCombinedChunks` method to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The meat of this recipe is in the `getCombinedChunks` method. We will just
    loop through all the chunks and check each pair if they overlap in their starts
    and ends. If they overlap and overlaps are not allowed, they are added to an overlapped
    set; otherwise, they are added to a combined set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Here is the place to add more rules for overlapping chunks. For example, you
    can make it score based, so if the `PRESIDENT` chunk type has a higher score than
    the HMM-based one, you can choose it instead.
  prefs: []
  type: TYPE_NORMAL
- en: CRFs for chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CRFs are best known to provide close to state-of-the-art performance for named-entity
    tagging. This recipe will tell us how to build one of these systems. The recipe
    assumes that you have read, understood, and played with the *Conditional r* *andom
    fields – CRF for word/token tagging* recipe in [Chapter 4](part0051_split_000.html#page
    "Chapter 4. Tagging Words and Tokens"), *Tagging Words and Tokens*, which addresses
    the underlying technology. Like HMMs, CRFs treat named entity detection as a word-tagging
    problem, with an interpretation layer that provides chunkings. Unlike HMMs, CRFs
    use a logistic-regression-based classification approach, which, in turn, allows
    for random features to be included. Also, there is an excellent tutorial on CRFs
    that this recipe follows closely (but omits details) at [http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html).
    There is also a lot of information in the Javadoc.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just as we did earlier, we will use a small hand-coded corpus to serve as training
    data. The corpus is in `src/com/lingpipe/cookbook/chapter5/TinyEntityCorpus.java`.
    It starts with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are only using this corpus to train, the `visitTest()` method does
    nothing. However, the `visitTrain()` method exposes the handler to all the chunkings
    stored in the `CHUNKINGS` constant. This, in turn, looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'We are still not done. Given that the creation of `Chunking` is fairly verbose,
    there are static methods to help dynamically create the requisite objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: This is all the setup; next, we will train and run a CRF on the preceding data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Type the `TrainAndRunSimplCrf` class in the command line or run the equivalent
    in your IDE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in loads of screen output that report on the health and progress
    of the CRF, it is mostly information from the underlying logistic-regression classifier
    that drives the whole show. The fun bit is that we will get an invitation to play
    with the new CRF:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The chunker reports the first best output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding output is the first best analysis by the CRF of what sorts of
    entities are in the sentence. It thinks that `John Smith` is `PER` with `the 0-10:PER@-Infinity`
    output. We know that it applies to the `John Smith` string by taking the substring
    from 0 to 10 in the input text. Ignore `–Infinity`, which is supplied for chunks
    that have no score. The first best chunking does not have scores. The other entity
    that it thinks is in the text is `New York` as an `LOC`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Immediately, the conditional probabilities follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding output provides the 10 best analyses of the whole phrase, along
    with their conditional (natural log) probabilities. In this case, we will see
    that the system isn't particularly confident of any of its analyses. For instance,
    the estimated probability of the first best analysis being correct is `exp(-1.66)=0.19`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, in the output, we see probabilities for individual chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As with the previous conditional output, the probabilities are logs, so we can
    see that the `John Smith` chunk has estimated probability `exp(-0.49) = 0.61`,
    which makes sense because in training the CRF saw `John` at the beginning of `PER`
    and `Smith` at the end of another, but not `John Smith` directly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding kind of probability distributions can really improve systems if
    there are sufficient resources to consider a broad range of analyses and ways
    of combining evidence to allow for improbable outcomes to be selected. First best
    analyses tend to be over committed to conservative outcomes that fit what training
    data looks like.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code in `src/com/lingpipe/cookbook/chapter5/TrainAndRunSimpleCRF.java`
    resembles our classifier and HMM recipes with a few differences. These differences
    are addressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: When we previously played with CRFs, the inputs were of the `Tagging<String>`
    type. Looking back at `TinyEntityCorpus.java`, the types are of the `Chunking`
    type. The preceding `BioTagChunkCodec` facilitates the translation of `Chunking`
    into `Tagging` via the efforts of a supplied `TokenizerFactory` and `boolean`
    that raise an exception if `TokenizerFactory` does not exactly agree with the
    `Chunk` starts and ends. Look back to the *Translating between word tagging and
    chunks–BIO codec* recipe better understand the role of this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'This codec will translate into a tagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'All the mechanics are hidden inside a new `ChainCrfChunker` class, and it is
    initialized in a manner similar to logistic regression, which is the underlying
    technology. Refer to the *Logistic regression* recipe of [Chapter 3](part0036_split_000.html#page
    "Chapter 3. Advanced Classifiers"), *Advanced Classifiers*, for more information
    on the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: The only new thing here is the `tagChunkCodec` parameter, which we just described.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training is over, we will access the chunker for first best with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Conditional chunkings are delivered by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'The individual chunks are accessed with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: That's it. You have access to one of the world's finest chunking technologies.
    Next, we will show you how to make it better.
  prefs: []
  type: TYPE_NORMAL
- en: NER using CRFs with better features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll show you how to create a realistic, though not quite state-of-the-art,
    set of features for CRFs. The features will include normalized tokens, part-of-speech
    tags, word-shape features, position features, and token prefixes and suffixes.
    Substitute it for the `SimpleCrfFeatureExtractor` in the *CRFs for chunking* recipe
    to use it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The source for this recipe is in `src/com/lingpipe/cookbook/chapter5/FancyCrfFeatureExtractor.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up your IDE or command prompt and type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Brace yourself for an explosion of features from the console. The data being
    used for feature extraction is `TinyEntityCorpus` of the previous recipe. Luckily,
    the first bit of data is just the node features for the "John" in the sentence
    `John ran.`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next word in the sequence adds edge features—we won''t bother showing you
    the node features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with other recipes, we won't bother discussing parts that are very similar
    to previous recipes—the relevant previous recipe here is the *Modifying CRFs*
    recipe in [Chapter 4](part0051_split_000.html#page "Chapter 4. Tagging Words and
    Tokens"), *Tagging Words and Tokens*. This is exactly the same, except for the
    fact that we will add in a lot more features—perhaps, from unexpected sources.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The tutorial for CRFs covers how to serialize/deserialize this class. This implementation
    does not cover it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Object construction is similar to the `Modifying CRFs` recipe in [Chapter 4](part0051_split_000.html#page
    "Chapter 4. Tagging Words and Tokens"), *Tagging Words and Tokens*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: The constructor sets up a part-of-speech tagger with a cache and shoves it into
    the `mPosTagger` member variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following method does very little, except supplying an inner `ChunkerFeatures`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ChunkerFeatures` class is where things get more interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: The `mPosTagger` function is used to set up `Tagging<String>` for the tokens
    presented on class creation. This will be aligned with the `tag()` and `token()`
    superclass methods and be the source of part-of-speech tags as a node feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can get on with the feature extraction. We will start with edge features,
    as they are the simplest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: The new feature is prefixed with `PREV_TAG_TOKEN_CAT_`, and the example is `PREV_TAG_TOKEN_CAT_PN_LET-CAP=1.0`.
    The `tokenCat()` method looks at the word shape feature for the previous token
    and returns it as a string. Look at the Javadoc for `IndoEuropeanTokenCategorizer`
    to see what is going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes the node features. There are many of these; each will be presented
    in turn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code sets up the method with the appropriate return type. The
    next two lines set up some state to know where the feature extractor is in the
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will compute the token categories, tokens, and part-of-speech tags
    for the current position, previous position, and the next position of the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: The previous and next methods check if we're at the begin or end of the sentence
    and return `null` accordingly. The part-of-speech tagging is taken from the saved
    part-of-speech taggings computed in the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The token methods provide some normalization of tokens to compress all numbers
    to the same kind of value. This method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: This just takes every sequence of numbers and replaces it with `*D...D*`. For
    instance, `12/3/08` is converted to `*DD*/*D*/*DD*`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then set feature values for the preceding, current, and following tokens.
    First, a flag indicates whether it begins or ends a sentence or an internal node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will include the tokens, token categories, and their parts of speech:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will add the prefix and suffix features, which add features for
    each suffix and prefix (up to a prespecified length):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: After this, we will just return the feature mapping generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `prefix` or `suffix` function is simply implemented with a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: That's a nice feature set for your named-entity detector.
  prefs: []
  type: TYPE_NORMAL
