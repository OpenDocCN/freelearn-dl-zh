- en: Chapter 5. Finding Spans in Text – Chunking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章. 在文本中查找跨度 – 块
- en: 'This chapter covers the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下食谱：
- en: Sentence detection
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子检测
- en: Evaluation of sentence detection
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子检测评估
- en: Tuning sentence detection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整句子检测
- en: Marking embedded chunks in a string – sentence chunk example
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在字符串中标记嵌入的块——句子块示例
- en: Paragraph detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段落检测
- en: Simple noun phrases and verb phrases
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的名词短语和动词短语
- en: Regular expression-based chunking for NER
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于正则表达式的命名实体识别块
- en: Dictionary-based chunking for NER
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于词典的命名实体识别块
- en: Translating between word tagging and chunks – BIO codec
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在词性标注和块之间进行翻译 – BIO 编码器
- en: HMM-based NER
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于HMM的命名实体识别
- en: Mixing the NER sources
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合命名实体识别源
- en: CRFs for chunking
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块的CRFs
- en: NER using CRFs with better features
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CRFs和更好特征的命名实体识别
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: 'This chapter will tell us how to work with spans of text that typically cover
    one or more words/tokens. The LingPipe API represents this unit of text as a chunk
    with corresponding chunkers that produce chunkings. The following is some text
    with character offsets indicated:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将告诉我们如何处理通常覆盖一个或多个单词/标记的文本跨度。LingPipe API将这个文本单元表示为块，并使用相应的块生成器来生成块。以下是一些带有字符偏移的文本：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Chunking the preceding text into sentences will give us the following output:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面的文本块成句子将给出以下输出：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Adding in a chunking for named entities adds entities for LingPipe and Java:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为命名实体添加块，为LingPipe和Java添加实体：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can define the named-entity chunkings with respect to their offsets from
    the sentences that contain them; this will make no difference to LingPipe, but
    Java will be:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据它们与包含它们的句子的偏移量来定义命名实体块；这不会对LingPipe产生影响，但对Java来说会是这样：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the basic idea of chunks. There are lots of ways to make them.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是块的基本思想。有很多方法可以创建它们。
- en: Sentence detection
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子检测
- en: Sentences in written text roughly correspond to a spoken utterance. They are
    the standard unit of processing words in industrial applications. In almost all
    mature NLP applications, sentence detection is a part of the processing pipeline
    even in the case of tweets, which can have more than one sentence in the allotted
    140 characters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 书面文本中的句子大致对应于口语表达。它们是工业应用中处理单词的标准单元。在几乎所有成熟的NLP应用中，句子检测都是处理流程的一部分，即使在推文中，推文可以有超过140个字符的多个句子。
- en: How to do it...
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'As usual, we will play with some data first. Enter the following command in
    the console:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如往常一样，我们首先会玩一些数据。在控制台中输入以下命令：
- en: '[PRE4]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The program will provide a prompt for your sentence-detection experimentation.
    A new line / return terminates the text to be analyzed:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序将为您的句子检测实验提供提示。一个新行/回车符终止要分析的文本：
- en: '[PRE5]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It is worth playing around a bit with different inputs. The following are some
    examples that explore the properties of the sentence detector. Drop the capitalized
    beginning of a sentence; this will prevent the detection of the second sentence:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值得尝试不同的输入。以下是一些探索句子检测器特性的示例。删除句子的首字母大写；这将防止检测第二个句子：
- en: '[PRE6]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The detector does not require a final period—this is configurable:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检测器不需要句尾的句号——这是可配置的：
- en: '[PRE7]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The detector balances parentheses, which will not allow sentences to break
    inside parentheses—this is also configurable:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检测器平衡括号，这不会允许句子在括号内断开——这也是可配置的：
- en: '[PRE8]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How it works...
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This sentence detector is a heuristic-based or rule-based sentence detector.
    A statistical sentence detector would be a reasonable approach as well. We will
    get through the entire source to run the detector, and later, we will discuss
    the modifications:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子检测器是基于启发式或基于规则的句子检测器。统计句子检测也是一个合理的方法。我们将运行整个源代码来运行检测器，稍后我们将讨论修改：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Working from the top of the `main` class, the Boolean `endSent` parameter controls
    whether the string that is sentence detected is assumed to end with a sentence,
    no matter what—this means that the last character is a sentence boundary always—it
    does not need to be a period or other typical sentence-ending mark. Change it
    and try a sentence without a final period, and the result will be that no sentence
    is detected.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从`main`类的顶部开始工作，布尔`endSent`参数控制是否假设检测到的句子以句子结束，无论什么情况——这意味着最后一个字符总是句子边界——它不需要是句号或其他典型的句子结束标记。更改它并尝试一个不带句尾句号的句子，结果将是没有检测到句子。
- en: 'The next Boolean `parenS` declaration gives priority to parentheses over sentence
    makers when finding sentences. Next, the actual sentence chunker will be set up:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个布尔变量`parenS`的声明在查找句子时优先考虑括号而不是句子生成器。接下来，将设置实际的句子分块器：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `tokFactory` should be familiar to you from [Chapter 2](part0027_split_000.html#page
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*.
    The `sentenceChunker` then can be constructed. Following is the standard I/O code
    for command-line interaction:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokFactory`应该对你很熟悉，来自[第2章](part0027_split_000.html#page "第2章. 寻找和使用单词")，*寻找和使用单词*。然后可以构建`sentenceChunker`。以下是为命令行交互的标准I/O代码：'
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we have the text, then the sentence detector is applied:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了文本，然后应用句子检测器：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The chunking provides a `Set<Chunk>` parameter, which will noncontractually
    provide an appropriate ordering of `Chunks`; they will be added as per the `ChunkingImpl`
    Javadoc. The truly paranoid programmer might impose the proper sort order, which
    we will cover later in the chapter when we have to handle overlapping chunks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分块提供了`Set<Chunk>`参数，它将非合同地提供`Chunks`的适当顺序；它们将按照`ChunkingImpl` Javadoc中的方式添加。真正偏执的程序员可能会强制执行正确的排序顺序，我们将在本章后面讨论，当我们必须处理重叠的块时。
- en: 'Next, we will check to see if any sentences were found, and if we don''t find
    them, we will report to the console:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查是否找到了任何句子，如果没有找到，我们将向控制台报告：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is the first exposure to the `Chunker` interface in the book,
    and a few comments are in order. The `Chunker` interface generates the `Chunk`
    objects, which are typed and scored contiguous-character sequences over `CharSequence`—usually,
    `String`. `Chunks` can overlap. The `Chunk` objects are stored in `Chunking`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在书中首次接触`Chunker`接口，有一些评论是必要的。`Chunker`接口生成`Chunk`对象，这些对象是`CharSequence`（通常是`String`）上类型化和评分的连续字符序列。`Chunks`可以重叠。`Chunk`对象存储在`Chunking`中：
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: First, we recovered the underlying text string `textStored` that the chunks
    are based on. It is the same string as `text`, but we wanted to illustrate this
    potentially useful method of the `Chunking` class, which can come up in recursive
    or other contexts when the chunking is far removed from where `CharSequence` that
    it uses is unavailable.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们恢复了基于块的基础文本字符串`textStored`。它与`text`相同，但我们想展示`Chunking`类中这种可能有用的方法，该方法可能在分块远离它所使用的`CharSequence`的递归或其他上下文中出现。
- en: The remaining `for` loop iterates over the sentences and prints them out with
    the `substring()` method of `String`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的`for`循环遍历句子，并使用`String`的`substring()`方法将它们打印出来。
- en: There's more...
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Before moving on to how to roll your own sentence detector, it is worth mentioning
    that LingPipe has `MedlineSentenceModel`, which is oriented towards the kind of
    sentences found in the medical research literature. It has seen a lot of data
    and should be a starting place for your own sentence-detection efforts over these
    kinds of data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续介绍如何自己实现句子检测器之前，值得提一下，LingPipe有`MedlineSentenceModel`，它面向医学研究文献中发现的句子类型。它已经看到了大量数据，应该成为你在这些类型数据上句子检测工作的起点。
- en: Nested sentences
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌套句子
- en: 'Sentences, particularly in literature, can contain nested sentences. Consider
    the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 句子，尤其是在文学中，可以包含嵌套的句子。考虑以下例子：
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding sentence will be marked up properly as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的句子将被正确标记为：
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This sort of nesting is different from a linguist''s concept of a nested sentence,
    which is based on grammatical role. Consider the following example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种嵌套与语言学家对嵌套句子的概念不同，后者基于语法角色。考虑以下例子：
- en: '[PRE17]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This sentence consists of two linguistically complete sentences joined by `and`.
    The difference between the two is that the former is determined by punctuation
    and the latter by a grammatical function. Whether this distinction is significant
    or not can be debated. However, the former case is much easier to recognize programmatically.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子由两个通过`and`连接的语言学上完整的句子组成。这两个句子的区别在于，前者由标点符号决定，而后者由语法功能决定。这种区别是否重要可以讨论。然而，前者在程序上更容易识别。
- en: However, we have rarely needed to model nested sentences in industrial contexts,
    but we took it on in our MUC-6 system and various coreference resolution systems
    in research contexts. This is beyond the scope of a recipe book, but be aware
    of the issue. LingPipe has no out-of-the-box capabilities for nested sentence
    detection.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在工业环境中很少需要建模嵌套句子，但我们在MUC-6系统和研究环境中的各种核心ference解析系统中承担了这项任务。这超出了食谱书的范围，但请注意这个问题。LingPipe没有现成的嵌套句子检测功能。
- en: Evaluation of sentence detection
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子检测评估
- en: Like most of the things we do, we want to be able to evaluate the performance
    of our components. Sentence detection is no different. Sentence detection is a
    span annotation that differs from our previous evaluations for classifiers and
    tokenization. As text can have characters that are not in any sentence, there
    is a notion of sentence start and sentence end. An example of characters that
    don't belong in a sentence will be JavaScript from an HTML page.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们做的许多事情一样，我们希望能够评估我们组件的性能。句子检测也不例外。句子检测是一种跨度注释，与我们之前对分类器和分词的评价不同。由于文本中可能包含不属于任何句子的字符，因此存在句子开始和句子结束的概念。一个不属于句子的字符的例子将来自HTML页面的JavaScript。
- en: The following recipe will take you through the steps of creating evaluation
    data and running it past an evaluation class.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将指导你创建评估数据并将其传递给评估类。
- en: How to do it...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行...
- en: 'Perform the following steps to evaluate sentence detection:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以评估句子检测：
- en: Open a text editor and copy and paste some literary gem that you want to evaluate
    sentence detection with, or you can go with our supplied default text, which is
    used if you don't provide your own data. It is easiest if you stick to plain text.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个文本编辑器，复制粘贴一些你想要用来评估句子检测的文学瑰宝，或者你可以使用我们提供的默认文本，如果你没有提供自己的数据，就会使用这个文本。如果你坚持使用纯文本，这会更容易。
- en: 'Insert balanced `[` and `]` to indicate the beginnings and ends of sentences
    in the text. If the text already contains either `[` or `]`, pick another character
    that is not in the text as a sentence delimiter—curly brackets or slashes are
    a good choice. If you use different delimiters, you will have to modify the source
    appropriately and recreate the JAR file. The code assumes a single-character text
    delimiter. An example of a sentence-annotated text from *The Hitchhiker''s Guide
    to the Galaxy* is as follows—note that not every character is in a sentence; some
    whitespaces are between sentences:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文本中插入平衡的`[`和`]`来指示句子的开始和结束。如果文本已经包含`[`或`]`，请选择一个不在文本中的字符作为句子分隔符——花括号或斜杠是一个不错的选择。如果你使用不同的分隔符，你将不得不相应地修改源代码并重新创建JAR文件。代码假设使用单字符文本分隔符。以下是从《银河系漫游指南》中摘取的句子注释文本的例子——请注意，并非每个字符都在句子中；句子之间有一些空白：
- en: '[PRE18]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Get yourself a command line and run the following command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令行并运行以下命令：
- en: '[PRE19]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For this data, the code will display two sentences that match perfectly with
    the sentences annotated with `[]`, as indicated by the `TruePos` label.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这些数据，代码将显示两个与用`[]`标注的句子完美匹配的句子，正如`TruePos`标签所示。
- en: 'A good exercise is to modify the annotation a bit to force errors. We will
    move the first sentence boundary one character in:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个好的练习是稍微修改一下注释以强制出现错误。我们将把第一个句子边界向前移动一个字符：
- en: '[PRE20]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Rerunning the modified annotation file after saving it yields:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存修改后的注释文件后重新运行会产生以下结果：
- en: '[PRE21]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: By changing the truth annotation, a false negative is produced, because the
    sentence span was missed by one character. In addition, a false positive is created
    by the sentence detector that recognizes the 0-83 character sequence.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过更改真实标注，产生了一个假阴性，因为句子跨度被遗漏了一个字符。此外，句子检测器还创建了一个假阳性，它识别了0-83字符序列。
- en: It is a good idea to play around with the annotation and various kinds of data
    to get a feel of how evaluation works and the capabilities of the sentence detector.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 试着玩一玩注释和各种类型的数据，以了解评估的工作方式和句子检测器的功能。
- en: How it works...
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The class starts by digesting the annotated text and storing the sentence chunks
    in an evaluation object. Then, the sentence detector is created, just as we did
    in the previous recipe. The code finishes by applying the created sentence detector
    to the text, and the results are printed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 类首先消化注释文本并将句子块存储在评估对象中。然后创建句子检测器，就像我们在前面的食谱中做的那样。代码最后将创建的句子检测器应用于文本，并打印结果。
- en: Parsing annotated data
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析注释数据
- en: 'Given text annotated with `[]` for sentence boundaries means that the correct
    offsets of the sentences have to be recovered, and the original unannotated text
    must be created, that is, without any `[]`. Span parsers can be a bit tricky to
    code, and the following is offered for simplicity rather than efficiency or proper
    coding technique:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 给定用`[]`注释句子边界的文本意味着必须恢复句子的正确偏移量，并且必须创建原始未注释的文本，即没有任何`[]`。跨度解析器可能有点难以编码，以下提供的是为了简单而不是效率或正确的编码技术：
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The preceding code reads in the entire file as a single `char[]` array with
    an appropriate character encoding. Also, note that for large files, a streaming
    approach will be more memory friendly. Next, an accumulator for unannotated chars
    is setup as a `StringBuilder` object with the `rawChars` variable. All characters
    encountered that are not either a `[` or `]` will be appended to the object. The
    remaining code sets up counters for sentence starts and ends that are indexed
    into the unannotated character array and an accumulator for `Set<Chunk>` for annotated
    sentence segments.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码以适当的字符编码将整个文件读入为一个`char[]`数组。另外，请注意，对于大文件，流式方法将更节省内存。接下来，设置一个未注释字符的累加器，作为一个`StringBuilder`对象，使用`rawChars`变量。所有遇到的既不是`[`也不是`]`的字符都将追加到该对象中。剩余的代码设置句子开始和结束的计数器，这些计数器索引到未注释的字符数组中，以及一个用于注释句子段的`Set<Chunk>`累加器。
- en: 'The following `for` loop advances one character at a time over the annotated
    character sequence:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的`for`循环逐个字符地遍历注释字符序列：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first `if (chars[i] == '[')` tests for starts of sentences in the annotation
    and sets the start variable to the length of `rawChars`. The iteration variable
    `i` includes the length added by the annotations. The corresponding `else if (chars[i]
    == ']')` statement handles the end of sentence case. Note that there are no error
    checks for this parser—this is a very bad idea because annotation errors are very
    likely if entered with a text editor. However, this is motivated by keeping the
    code as simple as possible. Later in the recipe, we will provide an example with
    some minimal error checking. Once the end of a sentence is found, a chunk is created
    for the sentence with `ChunkFactory.createChunk` with offsets and for the standard
    LingPipe sentence type `SentenceChunker.SENTENCE_CHUNK_TYPE`, which is required
    for the upcoming evaluation classes to work properly.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`if (chars[i] == '[')`测试注释中的句子开头，并将`start`变量设置为`rawChars`的长度。迭代变量`i`包括由注释添加的长度。相应的`else
    if (chars[i] == ']')`语句处理句子结尾的情况。请注意，这个解析器没有错误检查——这是一个非常糟糕的想法，因为如果使用文本编辑器输入，注释错误的可能性非常高。然而，这是为了保持代码尽可能简单。在后面的食谱中，我们将提供一个带有一些最小错误检查的示例。一旦找到句子结尾，就会使用`ChunkFactory.createChunk`创建一个句子块，带有偏移量和标准LingPipe句子类型`SentenceChunker.SENTENCE_CHUNK_TYPE`，这对于即将到来的评估类正常工作是必需的。
- en: 'The remaining `else` statement applies for all the characters that are not
    sentence boundaries, and it simply adds the character to the `rawChars` accumulator.
    The result of this accumulator can be seen outside the `for` loop when `String
    unannotatedText` is created. Now, we have sentence chunks indexed correctly into
    a text string. Next, we will create a proper `Chunking` object:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的`else`语句适用于所有不是句子边界的字符，它只是将字符添加到`rawChars`累加器中。当创建`String unannotatedText`时，可以在`for`循环外部看到这个累加器的结果。现在，我们已经将句子块正确地索引到一个文本字符串中。接下来，我们将创建一个合适的`Chunking`对象：
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `ChunkingImpl` implementing class (`Chunking` is an interface) requires
    the underlying text on construction, which is why we didn't just populate it in
    the preceding loop. LingPipe generally tries to make object construction complete.
    If Chunkings can be created without the underlying `CharSequence` method, then
    what will be returned when the `charSequence()` method is called? An empty string
    is actively misleading. Alternatively, returning `null` needs to be caught and
    dealt with. Better to just force the object to make sense of construction.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实现`ChunkingImpl`类的类（`Chunking`是一个接口）在构造时需要底层的文本，这就是为什么我们没有在先前的循环中直接填充它的原因。LingPipe通常试图使对象构造完整。如果可以在没有底层的`CharSequence`方法的情况下创建`Chunkings`，那么当调用`charSequence()`方法时将返回什么？一个空字符串会误导用户。或者，返回`null`需要被捕获并处理。最好是强制对象在构造时就有意义。
- en: 'Moving on, we will see the standard configuration of the sentence chunker from
    the previous recipe:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到来自先前食谱的标准句子分块器配置：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The interesting stuff follows with an evaluator that takes `sentenceChunker`
    as being evaluated as a parameter:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的部分紧随其后，有一个评估器，它将`sentenceChunker`作为评估参数：
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next up, the `handle(sentChunking)` method will take the text we just parsed
    into `Chunking` and run the sentence detector on `CharSequence` supplied in `sentChunking`
    and set up the evaluation:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`handle(sentChunking)`方法将接受我们刚刚解析到的`Chunking`文本，并在`sentChunking`提供的`CharSequence`上运行句子检测器，并设置评估：
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we will just get the evaluation data and work our way through the differences
    between the truth sentence detection and what the system did:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需获取评估数据，逐步分析真实句子检测和系统所做之间的差异：
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This recipe does not cover all the evaluation methods—check out the Javadoc—but
    it does provide what a sentence detection tuner will likely be most in need of;
    this is a listing of what the sentence detector got right (true positives), sentences
    it found but were wrong (false positives), and sentences it missed (false negatives).
    Note that true negatives don't make much sense in span annotations, because they
    will be the set of all the possible spans that are not in the truth sentence detection.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方并不涵盖所有的评估方法——查看Javadoc，但它确实提供了句子检测调整器可能最需要的；这是一个列出句子检测器正确识别的内容（真阳性）、它找到但错误的句子（假阳性）以及它遗漏的句子（假阴性）。请注意，在跨度注释中，真阴性没有太多意义，因为它们将是所有不在真实句子检测中的可能跨度集合。
- en: Tuning sentence detection
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整句子检测
- en: Lots of data will resist the charms of `IndoEuropeanSentenceModel`, so this
    recipe will provide a starting place to modify sentence detection to meet new
    kinds of sentences. Unfortunately, this is a very open-ended area of system building,
    so we will focus on techniques rather than likely formats for sentences.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的数据将抵制`IndoEuropeanSentenceModel`的诱惑，因此这个配方将提供一个起点来修改句子检测以适应新的句子类型。不幸的是，这是一个非常开放的系统构建领域，因此我们将关注技术而不是句子可能的格式。
- en: How to do it...
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'This recipe will follow a well-worn pattern: create evaluation data, set up
    evaluation, and start hacking. Here we go:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方将遵循一个熟悉的模式：创建评估数据，设置评估，然后开始修改。我们开始吧：
- en: 'Haul out your favorite text editor and mark up some data—we will stick to the
    `[` and `]` markup approach. The following is an example that runs afoul of our
    standard `IndoEuropeanSentenceModel`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拿出你最喜欢的文本编辑器并标记一些数据——我们将坚持使用`[`和`]`标记方法。以下是一个违反我们标准`IndoEuropeanSentenceModel`的例子：
- en: '[PRE29]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will put the preceding sentence in `data/saki.sentDetected.txt` and run
    it:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把前面的句子放入`data/saki.sentDetected.txt`并运行它：
- en: '[PRE30]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There's more...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The single false positive corresponds to the one sentence found, and the two
    false negatives are the two sentences not found that we annotated here. What happened?
    The sentence model missed `people's.` as a sentence end. If the apostrophe is
    removed, the sentence is detected properly—what is going on?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 单个假阳性对应于找到的一个句子，两个假阴性是我们在这里注释的两个未找到的句子。发生了什么？句子模型遗漏了`people's.`作为句子结束。如果去掉撇号，句子就能正确检测到——这是怎么回事？
- en: 'First, let''s look at the code running in the background. `IndoEuropeanSentenceModel`
    extends `HeuristicSentenceModel` by configuring several categories of tokens from
    the Javadoc for `HeuristicSentenceModel`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看在后台运行的代码。`IndoEuropeanSentenceModel`通过配置来自`HeuristicSentenceModel`的Javadoc中几个类别的标记来扩展`HeuristicSentenceModel`：
- en: '**Possible stops**: These are tokens that are allowed to be the final ones
    in a sentence. This set typically includes sentence-final punctuation tokens,
    such as periods (.) and double quotes (").'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可能的停止点**：这些是允许作为句子最后一个标记的标记。这个集合通常包括句子结尾的标点符号，例如句号(.)和双引号(")。'
- en: '**Impossible penultimates**: These are tokens that might not be the penultimate
    (second-to-last) token in a sentence. This set is typically made up of abbreviations
    or acronyms, such as `Mr`.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可能的次末点**：这些可能是句子中不是次末（倒数第二个）的标记。这个集合通常由缩写或首字母缩略词组成，例如`Mr`。'
- en: '**Impossible starts**: These are tokens that might not be the first ones in
    a sentence. This set typically includes punctuation characters that should be
    attached to the previous sentence, such as end quotes ('''').'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可能的起始点**：这些可能是句子中不是第一个的标记。这个集合通常包括应该附加到上一个句子的标点符号，例如结束引号('''')。'
- en: '`IndoEuropeanSentenceModel` is not configurable, but from the Javadoc, it is
    clear that all single characters are considered impossible penultimates. The words
    `people''s` is tokenized into `people`, `''`, `s`, and `.`. The single character
    `s` is penultimate to the `.` and is thus blocked. How to fix this?'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`IndoEuropeanSentenceModel` 不可配置，但从 Javadoc 中可以看出，所有单个字符都被视为不可能的末尾音节。单词 `people''s`
    被分词为 `people`、`''`、`s` 和 `.`。单个字符 `s` 是 `.` 的前缀音节，因此被阻止。如何解决这个问题？'
- en: 'A few options present themselves:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 几种选项呈现在眼前：
- en: Ignore the mistake assuming that it won't happen frequently
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设这种情况不会经常发生，忽略这个错误
- en: Fix by creating a custom sentence model
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过创建一个自定义句子模型来修复
- en: Fix by modifying the tokenizer to not separate apostrophes
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过修改分词器来不分离撇号来修复
- en: Write a complete sentence-detection model for the interface
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为接口编写一个完整的句子检测模型
- en: 'The second option, create a custom sentence model, is handled most easily by
    copying the source from `IndoEuropeanSentenceModel` into a new class and modifying
    it, as the relevant data structures are private. This is done to simplify the
    serialization of the class—very little configuration needs to be written to disk.
    In the example classes, there is a `MySentenceModel.java` file that differs by
    obvious changes in the package name and imports:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选项，创建一个自定义句子模型，最简单的处理方式是将 `IndoEuropeanSentenceModel` 的源代码复制到一个新类中并对其进行修改，因为相关数据结构是私有的。这样做是为了简化类的序列化——写入磁盘的配置非常少。在示例类中，有一个
    `MySentenceModel.java` 文件，它与原文件的区别在于包名和导入语句的明显变化：
- en: '[PRE31]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding code just comments out two of the likely single-letter cases of
    penultimate tokens that are a single-word character. To see it at work, change
    the sentence model to `SentenceModel sentenceModel = new MySentenceModel();` in
    the `EvaluateAnnotatedSentences.java` class and recompile and run it.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码只是注释掉了可能的单字符末尾音节的情况，这些音节是单个单词字符。要看到它的工作效果，请在 `EvaluateAnnotatedSentences.java`
    类中将句子模型更改为 `SentenceModel sentenceModel = new MySentenceModel();` 并重新编译和运行它。
- en: If you see the preceding code as a reasonable balancing of finding sentences
    that end in likely contractions versus non-sentence cases such as `[Hunter S.
    Thompson is a famous fellow.]`, which will detect `S.` as a sentence boundary.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为上述代码是找到以可能缩写结尾的句子与诸如 `[Hunter S. Thompson is a famous fellow.]` 这样的非句子情况之间合理平衡的合理方法，这将检测
    `S.` 作为句子边界。
- en: Extending `HeuristicSentenceModel` can work well for many sorts of data. Mitzi
    Morris built `MedlineSentenceModel.java`, which is designed to work well with
    the abstracts provided in the MEDLINE research index.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 `HeuristicSentenceModel` 对于许多类型的数据都适用。Mitzi Morris 构建了 `MedlineSentenceModel.java`，它旨在与
    MEDLINE 研究索引中提供的摘要很好地配合工作。
- en: One way to look at the preceding problem is that contractions should not be
    broken up into tokens for the purpose of sentence detection. `IndoEuropeanTokenizerFactory`
    should be tuned up to keep "people's" and other contractions together. While it
    initially seems slightly better that the first solution, it might well run afoul
    of the fact that `IndoEuropeanSentenceModel` was tuned with a particular tokenization
    in mind, and the consequences of the change are unknown in the absence of an evaluation
    corpus.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 看待上述问题的一种方式是，为了句子检测的目的，不应该将缩写拆分成标记。`IndoEuropeanTokenizerFactory` 应该调整以将 "people's"
    和其他缩写保留在一起。虽然最初似乎第一个解决方案稍微好一些，但它可能会违反 `IndoEuropeanSentenceModel` 是针对特定分词进行调整的事实，而在没有评估语料库的情况下，这种变化的影响是未知的。
- en: The other option is to write a completely novel sentence-detection class that
    supports the `SentenceModel` interface. Faced with a highly novel data collection
    such as Twitter feeds, we will consider using a machine-learning-driven span-annotation
    technique such as HMMs or CRFs covered in [Chapter 4](part0051_split_000.html#page
    "Chapter 4. Tagging Words and Tokens"), *Tagging Words and Tokens*, and at the
    end of this chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是编写一个完全新颖的句子检测类，该类支持 `SentenceModel` 接口。面对高度新颖的数据集，例如 Twitter 流，我们将考虑使用机器学习驱动的跨度标注技术，如
    HMMs 或 CRFs，这些技术在本章的 [第 4 章](part0051_split_000.html#page "第 4 章。标记单词和标记") 和本章末尾的
    “标记单词和标记” 中有所介绍。
- en: Marking embedded chunks in a string – sentence chunk example
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在字符串中标记嵌入的块 - 句子分块示例
- en: The method of displaying chunkings in the previous recipes is not well suited
    for applications that need to modify the underlying string. For example, a sentiment
    analyzer might want to highlight only sentences that are strongly positive and
    not mark up the remaining sentences while still displaying the entire text. The
    slight complication in producing the marked-up text is that adding markups changes
    the underlying string. This recipe provides working code to insert the chunking
    by adding chunks in reverse.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 之前配方中显示块的方法并不适合需要修改底层字符串的应用程序。例如，情感分析器可能只想突出显示强烈积极的句子，而不标记其他句子，同时仍然显示整个文本。产生标记文本的轻微复杂性在于添加标记会改变底层字符串。这个配方提供了插入块的工作代码，通过反向添加块来实现。
- en: How to do it...
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'While this recipe may not be technically complex it is useful to get span annotations
    into a text without out having to invent the code from whole cloth. The `src/com/lingpipe/coobook/chapter5/WriteSentDetectedChunks`
    class has the referenced code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个配方在技术上可能并不复杂，但它有助于将跨度注释添加到文本中，而无需从头编写代码。`src/com/lingpipe/coobook/chapter5/WriteSentDetectedChunks`类包含了引用的代码：
- en: 'The sentence chunking is created as per the first sentence-detection recipe.
    The following code extracts the chunks as `Set<Chunk>` and then sorts them by
    `Chunk.LONGEST_MATCH_ORDER_COMPARITOR`. In the Javadoc, the comparator is defined
    as:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子块是按照第一个句子检测配方创建的。以下代码将块提取为`Set<Chunk>`，然后按`Chunk.LONGEST_MATCH_ORDER_COMPARITOR`排序。在Javadoc中，比较器被定义为：
- en: '*Compares two chunks based on their text position. A chunk is greater if it
    starts later than another chunk, or if it starts at the same position and ends
    earlier.*'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*根据文本位置比较两个块。如果一个块比另一个块开始晚，或者如果它以相同的位置开始但结束得更早，则该块更大。*'
- en: 'There is also `TEXT_ORDER_COMPARITOR`, which is as follows:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，还有`TEXT_ORDER_COMPARITOR`，如下所示：
- en: '[PRE32]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we will iterate over the chunks in the reverse order, which eliminates
    having to keep an offset variable for the changing length of the `StringBuilder`
    object. Offset variables are a common source of bugs, so this recipe avoids them
    as much as possible but does non-standard reverse loop iteration, which might
    be worse:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将以相反的顺序遍历块，这样可以消除为`StringBuilder`对象的改变长度保持偏移变量的需要。偏移变量是常见的错误来源，所以这个配方尽可能地避免了它们，但进行了非标准的反向循环迭代，这可能会更糟：
- en: '[PRE33]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code does a very simple sentiment analysis by looking for the
    string `like` in the sentence and marking that sentence if `true`. Note that this
    code cannot handle overlapping chunks or nested chunks. It assumes a single, non-overlapping
    chunk set. Some example output is:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码通过查找句子中的字符串`like`来进行非常简单的情感分析，并在`true`时标记该句子。请注意，此代码无法处理重叠块或嵌套块。它假设一个单一的非重叠块集。以下是一些示例输出：
- en: '[PRE34]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: To print nested chunks, look at the *Paragraph* *detection* recipe that follows.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要打印嵌套块，请查看下面的*段落* *检测*配方。
- en: Paragraph detection
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 段落检测
- en: The typical containing structure of a set of sentences is a paragraph. It can
    be set off explicitly in a markup language such as `<p>` in HTML or with two or
    more new lines, which is how paragraphs are usually rendered. We are in the part
    of NLP where no hard-and-fast rules apply, so we apologize for the hedging. We
    will handle some common examples in this chapter and leave it to you to generalize.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 句子集的典型包含结构是段落。它可以在HTML中的`<p>`等标记语言中显式设置，或者通过两个或更多的新行，这是段落通常的渲染方式。我们处于NLP的这样一个部分，没有硬性规则适用，所以我们对此表示歉意。我们将在本章中处理一些常见示例，并将其留给你自己去推广。
- en: How to do it...
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We have never set up an evaluation harness for paragraph detection, but it
    can be done in ways similar to sentence detection. This recipe, instead, will
    illustrate a simple paragraph-detection routine that does something very important—maintain
    offsets into the original document with embedded sentence detection. This attention
    to detail will serve you well if you ever need to mark up the document in a way
    that is sensitive to sentences or other subspans of the document, such as named
    entities. Consider the following example:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从未为段落检测设置过评估工具，但它可以通过与句子检测类似的方式进行。这个配方将展示一个简单的段落检测程序，它做了一件非常重要的事情——使用嵌入的句子检测来维护对原始文档的偏移量。如果你需要以对句子或其他文档子跨度（如命名实体）敏感的方式标记文档，这种对细节的关注将对你大有裨益。考虑以下示例：
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It gets transformed into the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 它被转换成以下形式：
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The example code has little to offer in paragraph-detection techniques. It
    is an open-ended problem, and you will have to use your wiles to solve it. Our
    paragraph detector is a pathetic `split("\n\n")` that, in a more sophisticated
    approach, will take into account context, characters, and other features that
    are far too idiosyncratic for us to cover. Here is the beginning of the code that
    reads the entire document as a string and splits it into an array. Note that `paraSeperatorLength`
    is the number of characters that form the basis of the paragraph split—if the
    length of the split varies, then that length will have to be associated with the
    corresponding paragraph:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 示例代码在段落检测技术方面提供很少的内容。这是一个开放性问题，你将不得不运用你的技巧来解决它。我们的段落检测器是一个可怜的 `split("\n\n")`，在更复杂的方法中，将考虑上下文、字符和其他对我们来说过于特殊而无法涵盖的特征。以下是代码的起始部分，它将整个文档作为字符串读取并将其分割成数组。请注意，`paraSeperatorLength`
    是构成段落分割的基础字符数——如果分割的长度变化，那么这个长度将必须与相应的段落相关联：
- en: '[PRE38]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The real point of the recipe is to help with the mechanics of maintaining character
    offsets into the original document and show embedded processing. This will be
    done by keeping two separate chunkings: one for paragraphs and one for sentences:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 菜单的真正目的是帮助处理将字符偏移量维持到原始文档中的机制，并展示嵌入式处理。这将通过保持两个独立的分块来实现：一个用于段落，一个用于句子：
- en: '[PRE39]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, the sentence detector will be set up in the same way as one in the previous
    recipe:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，句子检测器将以与之前菜谱中相同的方式设置：
- en: '[PRE40]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The chunking iterates over the array of paragraphs and builds a sentence chunking
    for each paragraph. The somewhat-complicated part of this approach is that the
    sentence chunk offsets are with respect to the paragraph string, not the entire
    document. So, the variables'' starts and ends are updated with document offsets
    in the code. Chunks have no methods to adjust starts and ends, so a new chunk
    must be created, `adjustedSentChunk`, with appropriate offsets into the paragraph
    start and must be added to `sentChunking`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分块遍历段落数组并为每个段落构建一个句子分块。这种方法中较为复杂的部分是，句子分块的偏移量是相对于段落字符串的，而不是整个文档。因此，代码中变量的开始和结束位置将使用文档偏移量更新。块没有调整开始和结束的方法，因此必须创建一个新的块
    `adjustedSentChunk`，带有适当的段落起始偏移量，并将其添加到 `sentChunking`：
- en: '[PRE41]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The rest of the loop adds the paragraph chunk and then updates the start of
    the paragraph with the length of the paragraph plus the length of the paragraph
    separator. This will complete the creation of correctly offset sentences and paragraphs
    into the original document string:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环的其余部分添加段落块，然后更新段落的开始位置，该位置为段落长度加上段落分隔符的长度。这将完成在原始文档字符串中正确偏移的句子和段落的创建：
- en: '[PRE42]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The rest of the program is concerned with printing out the paragraphs and sentences
    with some markup. First, we will create a chunking that has both sentence and
    paragraph chunks:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序的其余部分关注于打印带有一些标记的段落和句子。首先，我们将创建一个同时包含句子和段落分块的块：
- en: '[PRE43]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, `displayChunking` will be sorted by recovering `chunkSet`, converting
    it into an array of chunks and the application of the static comparator:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，`displayChunking` 将通过恢复 `chunkSet`，将其转换为块数组并应用静态比较器来排序：
- en: '[PRE44]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We will use the same trick as we did in the *Marking embedded chunks in a string
    – sentence chunk example* recipe, which is to insert the markup backwards into
    the string. We will have to keep an offset counter, because nested sentences will
    extend the finishing paragraph mark placement. The approach assumes that no chunks
    overlap and that sentences are contained within paragraphs always:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用与我们在 *在字符串中标记嵌入式块 - 句子分块示例* 菜单中使用的相同技巧，即将标记反向插入到字符串中。我们将需要保留一个偏移计数器，因为嵌套句子将扩展完成段落标记的位置。这种方法假设没有块重叠，并且句子始终包含在段落中：
- en: '[PRE45]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: That's it for the recipe.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 菜单就到这里。
- en: Simple noun phrases and verb phrases
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的名词短语和动词短语
- en: This recipe will show you how to find simple **noun phrases** (**NP**) and **verb
    phrases** (**VP**). By "simple", we mean that there is no complex structure within
    the phrases. For example, the complex NP "The rain in Spain" will be broken into
    two simple NP chunks "The rain" and "Spain". These phrases are also called "basal".
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱将向您展示如何找到简单的**名词短语**（**NP**）和**动词短语**（**VP**）。这里的“简单”意味着短语内没有复杂结构。例如，复杂的
    NP “The rain in Spain” 将被拆分为两个简单的 NP 块 “The rain” 和 “Spain”。这些短语也被称为“基础”。
- en: This recipe will not go into the details of how the basal NPs/VPs are calculated
    but rather how to use the class—it can come in handy, and the source can be included
    if you want to sort out how it works.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱不会详细介绍如何计算基线NP/VP，而是如何使用这个类——它可能很有用，如果你想要弄清楚它是如何工作的，可以包含源代码。
- en: How to do it…
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Like many of the recipes, we will provide a command-line-interactive interface
    here:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 就像许多食谱一样，我们在这里将提供一个命令行交互式界面：
- en: 'Haul up the command line and type:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令行并输入：
- en: '[PRE46]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How it works…
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The `main()` method starts by deserializing a part-of-speech tagger and then
    creating `tokenizerFactory`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`方法首先反序列化一个词性标注器，然后创建`tokenizerFactory`：'
- en: '[PRE47]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, `PhraseChunker` is constructed, which is a heuristic approach to the
    problem. Look at the source to see how it works—it scans the input left to right
    for NP/VP starts and attempts to add to the phrase incrementally:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，构建`PhraseChunker`，这是一种针对该问题的启发式方法。查看源代码以了解其工作原理——它从左到右扫描输入以查找NP/VP的开始，并尝试增量地添加到短语中：
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Our standard console I/O code is next:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的标准控制台I/O代码如下：
- en: '[PRE49]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Then, the input is tokenized, POS is tagged, and the tokens and tags are printed
    out:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对输入进行分词，进行词性标注，并打印出分词和标签：
- en: '[PRE50]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The NP/VP chunkings are then calculated and printed out:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算并打印出NP/VP分块：
- en: '[PRE51]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: There is a more comprehensive tutorial at [http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在[http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html)有一个更全面的教程。
- en: Regular expression-based chunking for NER
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于正则表达式的NER分块
- en: '**Named Entity Recognition** (**NER**) is the process of finding mentions of
    specific things in text. Consider a simple name; location-named entity recognizer
    might find `Ford Prefect` and `Guildford` as the name and location mentions, respectively,
    in the following text:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）是找到文本中特定事物提及的过程。考虑一个简单的名字；一个地点命名实体识别器可能会在以下文本中将`Ford Prefect`和`Guildford`分别识别为名字和地点提及：'
- en: '[PRE52]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We will start by building rule-based NER systems and move up to machine-learning
    methods. Here, we'll take a look at building an NER system that can extract e-mail
    addresses from text.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先构建基于规则的NER系统，然后逐步过渡到机器学习方法。在这里，我们将探讨如何构建一个可以从文本中提取电子邮件地址的NER系统。
- en: How to do it…
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Enter the following command into the command prompt:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令提示符中输入以下命令：
- en: '[PRE53]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Interaction with the program proceeds as follows:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与程序的交互过程如下：
- en: '[PRE54]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: You can see that both `foo@bar.com` as well as `foo.bar@gmail.com` were returned
    as valid `e-mail` type chunks. Also, note that the final period in the sentence
    is not part of the second e-mail.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以看到，`foo@bar.com`以及`foo.bar@gmail.com`都被返回为有效的`e-mail`类型分块。此外，请注意，句子中的最后一个句点不是第二个电子邮件的一部分。
- en: How it works…
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'A regular expression chunker finds chunks that match the given regular expression.
    Essentially, the `java.util.regex.Matcher.find()` method is used to iteratively
    find matching text segments, and these are then converted into the Chunk objects.
    The `RegExChunker` class wraps these steps. The code of `src/com/lingpipe/cookbook/chapter5/RegExNer.java`
    is described as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式分块器找到与给定正则表达式匹配的分块。本质上，使用`java.util.regex.Matcher.find()`方法迭代地找到匹配的文本段，然后这些段被转换为Chunk对象。`RegExChunker`类封装了这些步骤。`src/com/lingpipe/cookbook/chapter5/RegExNer.java`的代码描述如下：
- en: '[PRE55]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: All the interesting work was done in the preceding lines of code. The `emailRegex`
    is pulled off of the Internet—see the following for the source, and the remaining
    bits are setting up `chunkType` and `score`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所有有趣的工作都在前面的代码行中完成了。`emailRegex`是从互联网上获取的——见下文以获取来源，其余部分是设置`chunkType`和`score`。
- en: 'The rest of the code reads in the input and prints out the chunking:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的其余部分读取输入并打印出分块：
- en: '[PRE56]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: See also
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The regular expression for the e-mail address match is from [regexlib.com](http://regexlib.com)
    at [http://regexlib.com/DisplayPatterns.aspx?cattabindex=0&categoryId=1](http://regexlib.com/DisplayPatterns.aspx?cattabindex=0&categoryId=1)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件地址匹配的正则表达式来自[regexlib.com](http://regexlib.com)，具体链接为[http://regexlib.com/DisplayPatterns.aspx?cattabindex=0&categoryId=1](http://regexlib.com/DisplayPatterns.aspx?cattabindex=0&categoryId=1)。
- en: Dictionary-based chunking for NER
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于词典的NER分块
- en: In many websites and blogs and certainly on web forums, you might see keyword
    highlighting that links pages you can buy a product from. Similarly, news websites
    also provide topic pages for people, places, and trending events, such as the
    one at [http://www.nytimes.com/pages/topics/](http://www.nytimes.com/pages/topics/).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多网站、博客以及当然是在网络论坛上，你可能会看到关键字高亮显示，链接到你可以购买产品的页面。同样，新闻网站也为人物、地点和热门事件提供主题页面，例如[http://www.nytimes.com/pages/topics/](http://www.nytimes.com/pages/topics/)。
- en: A lot of this is fully automated and is easy to do with a dictionary-based `Chunker`.
    It is straightforward to compile lists of names for entities and their types.
    An exact dictionary chunker extracts chunks based on exact matches of tokenized
    dictionary entries.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的许多操作都是完全自动化的，并且使用基于词典的`Chunker`很容易做到。编译实体及其类型的名称列表非常直接。精确的词典分词器根据标记化词典条目的精确匹配提取分词。
- en: The implementation of the dictionary-based chunker in LingPipe is based on the
    Aho-Corasick algorithm which finds all matches against a dictionary in linear
    time independent of the number of matches or size of the dictionary. This makes
    it much more efficient than the naïve approach of doing substring searches or
    using regular expressions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe中基于词典的分词器实现基于Aho-Corasick算法，该算法在字典中找到所有匹配项，其时间复杂度与匹配项的数量或字典的大小无关。这使得它比使用子字符串搜索或正则表达式的朴素方法要高效得多。
- en: How to do it…
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In the IDE of your choice run the `DictionaryChunker` class in the `chapter5`
    package or type the following using the command line:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你选择的IDE中运行`chapter5`包中的`DictionaryChunker`类，或者使用命令行输入以下内容：
- en: '[PRE57]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Since this particular chunker example is biased (very heavily) towards the
    Hitchhikers Guide, let''s use a sentence that involves some of the characters:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这个特定的分词示例偏向于《银河系漫游指南》（非常严重），让我们使用一个涉及一些角色的句子：
- en: '[PRE58]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Note that we have overlapping chunks from `Heart` and `Heart of Gold`. As we
    will see, this can be configured to behave differently.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，我们从`Heart`和`Heart of Gold`中得到了重叠的分词。正如我们将看到的，这可以配置为以不同的方式行为。
- en: How it works…
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Dictionary-based NER drives a great deal of automatic linking against unstructured
    text data. We can build one using the following steps.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词典的命名实体识别（NER）在针对非结构化文本数据的大量自动链接中发挥着重要作用。我们可以通过以下步骤构建一个：
- en: 'The first step of the code will create `MapDictionary<String>` to store the
    dictionary entries:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一步将创建`MapDictionary<String>`来存储词典条目：
- en: '[PRE59]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Next, we will populate the dictionary with `DictionaryEntry<String>`, which
    includes type information and a score that will be used to create chunks:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`DictionaryEntry<String>`填充词典，它包括类型信息和用于创建分词的分数：
- en: '[PRE60]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In the `DictionaryEntry` constructor, the first argument is the phrase, the
    second string argument is the type, and the final double argument is the score
    for the chunk. Dictionary entries are always case sensitive. There is no limit
    to the number of different entity types in a dictionary. The scores will simply
    be passed along as chunk scores in the dictionary-based chunker.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在`DictionaryEntry`构造函数中，第一个参数是短语，第二个字符串参数是类型，最后一个双精度参数是分词的分数。词典条目总是区分大小写的。词典中不同实体类型的数量没有限制。分数将简单地作为基于词典的分词器中的分词分数传递。
- en: 'Next, we will build `Chunker`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建`Chunker`：
- en: '[PRE61]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: An exact dictionary chunker might be configured either to extract all the matching
    chunks to restrict the results to a consistent set of non-overlapping chunks via
    the `returnAllMatches` boolean. Look at the Javadoc to understand the exact criteria.
    There is also a `caseSensitive` boolean. The chunker requires a tokenizer, as
    it matches tokens as symbols, and whitespaces are ignored in the matching process.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一个精确的词典分词器可能被配置为提取所有匹配的分词，通过`returnAllMatches`布尔值将结果限制为一致的非重叠分词集。查看Javadoc以了解确切的标准。还有一个`caseSensitive`布尔值。分词器需要一个分词器，因为它将标记作为符号进行匹配，并且在匹配过程中忽略空白字符。
- en: 'Next is our standard I/O code for console interaction:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的标准I/O代码，用于控制台交互：
- en: '[PRE62]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The remaining code creates a chunking, goes through the chunks, and prints
    them out:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的代码创建分词，遍历分词并打印它们：
- en: '[PRE63]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Dictionary chunkers are very useful even in machine-learning-based systems.
    There tends to always be a class of entities that are best identified this way.
    The *Mixing the NER sources* recipe addresses how to work with multiple sources
    of named entities.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在基于机器学习的系统中，词典分词器也非常有用。通常总有一类实体最适合以这种方式识别。"混合NER来源"配方解决了如何处理多个命名实体来源的问题。
- en: Translating between word tagging and chunks – BIO codec
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](part0051_split_000.html#page "Chapter 4. Tagging Words and Tokens"),
    *Tagging Words and Tokens*, we used HMMs and CRFs to apply tags to words/tokens.
    This recipe addresses the case of creating chunks from taggings that use the **Begin,
    In, and Out** (**BIO**) tags to encode chunkings that can span multiple words/tokens.
    This, in turn, is the basis of modern named-entity detection systems.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The standard BIO-tagging scheme has the first token in a chunk of type X tagged
    B-X (begin), with all the subsequent tokens in the same chunk tagged I-X (in).
    All the tokens that are not in chunks are tagged O (out). For example, the string
    with character counts:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'It can be tagged as:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The corresponding chunks will be:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: How to do it…
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The program will show the simplest mapping between taggings and chunkings and
    the other way around:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The program first prints out the string that will be tagged with a tagging:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Next, the chunking is printed:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Then, the tagging is created from the chunking just displayed:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: How it works…
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code starts by manually constructing `StringTagging`—we will see HMMs and
    CRFs do the same programmatically, but here it is explicit. It then prints out
    the created `StringTagging`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next, it will construct `BioTagChunkCodec` and convert the tagging just printed
    out to a chunking followed by printing the chunking:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The remaining code reverses the process. First, a different `BioTagChunkCodec`
    is created with `boolean` `enforceConsistency`, which, if `true`, checks that
    the tokens created by the supplied tokenizer align exactly with the chunk begins
    and ends. Without the alignment we end up with a perhaps untenable relationship
    between chunks and tokens depending on the use case:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The last `for` loop simply prints out the tagging returned by the `codec2.toStringTagging()`
    method.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recipe works through the simplest example of mapping between taggings and
    chunkings. `BioTagChunkCodec` also takes the `TagLattice<String>` objects to produce
    n-best output, as will be shown in the HMM and CRF chunkers to follow.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: HMM-based NER
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`HmmChunker` uses an HMM to perform chunking over tokenized character sequences.
    Instances contain an HMM decoder for the model and tokenizer factory. The chunker
    requires the states of the HMM to conform to a token-by-token encoding of a chunking.
    It uses the tokenizer factory to break the chunks down into sequences of tokens
    and tags. Refer to the *Hidden Markov Models (HMM) – part of speech* recipe in
    [Chapter 4](part0051_split_000.html#page "Chapter 4. Tagging Words and Tokens"),
    *Tagging Words and Tokens*.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at training `HmmChunker` and using it for the `CoNLL2002` Spanish
    task. You can and should use your own data, but this recipe assumes that training
    data will be in the `CoNLL2002` format.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Training is done using an `ObjectHandler` which supplies the training instances.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we want to train this chunker, we need to either label some data using the
    **Computational Natural Language Learning** (**CoNLL**) schema or use the one
    that's publicly available. For speed, we'll choose to get a corpus that is available
    in the CoNLL 2002 task.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要训练这个分词器，我们需要使用 **计算自然语言学习**（**CoNLL**）模式标记一些数据，或者使用公开可用的数据。为了提高速度，我们将选择获取
    CoNLL 2002 任务中可用的语料库。
- en: Note
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The ConNLL is an annual meeting that sponsors a bakeoff. In 2002, the bakeoff
    involved Spanish and Dutch NER.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ConNLL 是一个年度会议，它赞助了一个烘焙比赛。在 2002 年，比赛涉及西班牙语和荷兰语的命名实体识别。
- en: The data can be downloaded from [http://www.cnts.ua.ac.be/conll2002/ner.tgz](http://www.cnts.ua.ac.be/conll2002/ner.tgz).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以从 [http://www.cnts.ua.ac.be/conll2002/ner.tgz](http://www.cnts.ua.ac.be/conll2002/ner.tgz)
    下载。
- en: 'Similar to what we showed in the preceding recipe; let''s take a look at what
    this data looks like:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前展示的配方类似；让我们看看这些数据看起来像什么：
- en: '[PRE74]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: With this encoding scheme, the phrases *El Abogado General del Estado* and *Daryl
    Williams* are coded as persons, with their beginning and continuing tokens picked
    out with tags B-PER and I-PER, respectively.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种编码方案中，短语 *El Abogado General del Estado* 和 *Daryl Williams* 被编码为人物，分别用标签
    B-PER 和 I-PER 挑选出它们的起始和持续标记。
- en: Note
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are a few formatting errors in the data that must be fixed before our
    parsers can handle them. After unpacking `ner.tgz` in the `data` directory you
    will have to go to `data/ner/data`, unzip the following files, and modify as indicated:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中存在一些格式错误，在解析器能够处理它们之前必须修复。在 `data` 目录中解压 `ner.tgz` 后，您将需要转到 `data/ner/data`，解压以下文件，并按指示修改：
- en: '[PRE75]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: How to do it…
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Using the command line, type the following:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用命令行，键入以下内容：
- en: '[PRE76]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'It will run the training on the CoNLL training data if the model doesn''t exist.
    It might take a while, so be patient. The output of the training will be:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型不存在，它将在 CoNLL 训练数据上运行训练。这可能需要一段时间，所以请耐心等待。训练的输出将如下：
- en: '[PRE77]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Once the prompt to enter the text is presented, type in some Spanish text from
    the CoNLL test set:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦出现输入文本的提示，请输入 CoNLL 测试集中的西班牙文文本：
- en: '[PRE78]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: What we will see is a number of entities, their confidence score, the span in
    the original sentence, the type of entity, and the phrase that represents this
    entity.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将看到一系列实体，它们的置信度分数，原始句子中的范围，实体的类型，以及代表这个实体的短语。
- en: 'To find out the correct tags, take a look at the annotated `esp.testa` file,
    which contains the following tags for this sentence:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要找出正确的标签，请查看注释过的 `esp.testa` 文件，其中包含以下标签用于这个句子：
- en: '[PRE79]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'This can be read as follows:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可以读作如下：
- en: '[PRE80]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: So, we got all the ones with 1.000 confidence correct and the rest wrong. This
    can help us set up a threshold in production.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们得到了所有 1.000 置信度正确的那些，其余的都是错误的。这可以帮助我们在生产中设置一个阈值。
- en: How it works…
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `CharLmRescoringChunker` provides a long-distance character language model-based
    chunker that operates by rescoring the output of a contained character language
    model HMM chunker. The underlying chunker is an instance of `CharLmHmmChunker`,
    which is configured with the specified tokenizer factory, n-gram length, number
    of characters, and interpolation ratio provided in the constructor.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`CharLmRescoringChunker` 提供了一个基于长距离字符语言模型的分词器，它通过重新评分包含的字符语言模型 HMM 分词器的输出来操作。底层分词器是
    `CharLmHmmChunker` 的一个实例，它使用构造函数中指定的分词器工厂、n-gram 长度、字符数和插值比进行配置。'
- en: 'Let''s start with the `main()` method; here, we will set up the chunker, train
    it if it doesn''t exist, and then allow for some input to get the named entities
    out:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `main()` 方法开始；在这里，我们将设置分词器，如果它不存在，则对其进行训练，然后允许输入一些文本以获取命名实体：
- en: '[PRE81]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The training file will be in the correct place if you unpack the CoNLL data
    (`tar –xvzf ner.tgz`) in the data directory. Remember to correct the annotation
    on line 221619 of `esp.train`. If you use other data, then modify and recompile
    the class.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在数据目录中解压 CoNLL 数据（`tar –xvzf ner.tgz`），训练文件将位于正确的位置。请记住，要更正 `esp.train` 文件中第
    221619 行的注释。如果你使用其他数据，那么请修改并重新编译类。
- en: 'The next bit of code trains the model if it doesn''t exist and then loads the
    serialized version of the chunker. If you have questions about deserialization,
    see the *Deserializing and running a classifier* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*. Consider the following
    code snippet:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 下一段代码在模型不存在时训练模型，然后加载分词器的序列化版本。如果你对反序列化有疑问，请参阅第 1 章 *反序列化和运行分类器* 的配方，*简单分类器*。考虑以下代码片段：
- en: '[PRE82]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The `trainHMMChunker()` method starts with some `File` bookkeeping before setting
    up configuration parameters for `CharLmRescoringChunker`:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainHMMChunker()` 方法在设置 `CharLmRescoringChunker` 的配置参数之前，进行了一些 `File` 记录：'
- en: '[PRE83]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now, let''s take a look at parsing the CoNLL data. The source for this class
    is `src/com/lingpipe/cookbook/chapter5/Conll2002ChunkTagParser`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何解析 CoNLL 数据。这个类的来源是 `src/com/lingpipe/cookbook/chapter5/Conll2002ChunkTagParser`：
- en: '[PRE86]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The statics set up the configuration of the `com.aliasi.tag.LineTaggingParser`
    LingPipe class. CoNLL, like many available data sets, uses a token/tag per line
    format, which is meant to be very easy to parse:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 静态设置 `com.aliasi.tag.LineTaggingParser` LingPipe 类的配置。CoNLL，像许多可用的数据集一样，使用每行一个标记/标记的格式，这种格式旨在非常容易解析：
- en: '[PRE87]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The `LineTaggingParser` constructor requires a regular expression that identifies
    the token and tag strings via grouping. There is additionally a regular expression
    for lines to ignore and finally, a regular expression for sentence ends.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`LineTaggingParser` 构造函数需要一个正则表达式，该正则表达式通过分组识别标记和标记字符串。此外，还有一个用于忽略行的正则表达式，最后是一个用于句子结束的正则表达式。'
- en: 'Next, we set up `TagChunkCodec`; this will handle the mapping from tagged tokens
    in the BIO format to proper chunks. See the previous recipe, *Translating between
    word tagging and chunks – BIO codec*, for more about what is going on here. The
    remaining parameters customize the tags to match those of the CoNLL training data:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置 `TagChunkCodec`；这将处理从 BIO 格式的标记标记到正确分块的映射。有关这里发生的事情的更多信息，请参阅之前的菜谱，*在词标记和分块之间转换
    – BIO codec*。其余参数定制标记以匹配 CoNLL 训练数据：
- en: '[PRE88]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The rest of the class provides methods for `parseString()`, which is immediately
    sent to the `LineTaggingParser` class:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 类的其余部分提供了 `parseString()` 方法，该方法立即发送到 `LineTaggingParser` 类：
- en: '[PRE89]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Next, the `ObjectHandler` parser is properly configured with the codec and
    supplied handler:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用 codec 和提供的处理器正确配置了 `ObjectHandler` 解析器：
- en: '[PRE90]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: It's a lot of odd-looking code, but all this does is set up a parser to read
    the lines from the input file and extract chunkings out of them.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码看起来很奇怪，但它所做的只是设置一个解析器来读取输入文件的行并从中提取分块。
- en: 'Finally, let''s go back to the `main` method and look at the output loop. We
    will set up the `MAX_NBEST` chunkings value as 10 and then invoke the `nBestChunkings`
    method on the chunker. This provides the top 10 chunks and their probabilistic
    scores. Based on an evaluation, we can choose to cut off at a particular score:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们回到 `main` 方法，看看输出循环。我们将设置 `MAX_NBEST` 分块值为 10，然后对分块器调用 `nBestChunkings`
    方法。这提供了前 10 个分块及其概率分数。基于评估，我们可以选择在特定分数处截断：
- en: '[PRE91]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: There's more…
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: For more details on running a complete evaluation, refer to the evaluation section
    of the tutorial at [http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 关于运行完整评估的更多详细信息，请参阅教程中的评估部分，网址为 [http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html)。
- en: See also
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more details on `CharLmRescoringChunker` and `HmmChunker`, refer to:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 `CharLmRescoringChunker` 和 `HmmChunker` 的更多详细信息，请参阅：
- en: '[http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/AbstractCharLmRescoringChunker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/AbstractCharLmRescoringChunker.html)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/AbstractCharLmRescoringChunker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/AbstractCharLmRescoringChunker.html)'
- en: '[http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html)'
- en: Mixing the NER sources
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合 NER 源
- en: Now that we've seen how to build a few different types of NERs, we can look
    at how to combine them. In this recipe, we will take a regular expression chunker,
    a dictionary-based chunker, and an HMM-based chunker and combine their outputs
    and look at overlaps.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何构建几种不同类型的 NER，我们可以看看如何将它们组合起来。在这个菜谱中，我们将使用正则表达式分块器、基于字典的分块器和基于 HMM
    的分块器，并将它们的输出组合起来，查看重叠部分。
- en: 'We will just initialize a few chunkers in the same way we did in the past few
    recipes and then pass the same text through these chunkers. The easiest possibility
    is that each chunker returns a unique output. For example, let''s consider a sentence
    such as "President Obama was scheduled to give a speech at the G-8 conference
    this evening". If we have a person chunker and an organization chunker, we might
    only get two unique chunks out. However, if we add a `Presidents of USA` chunker,
    we will get three chunks: `PERSON`, `ORGANIZATION`, and `PRESIDENT`. This very
    simple recipe will show us one way to handle these cases.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the command line or equivalent in your IDE, type the following:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The usual interactive prompt follows:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We see the output from the three chunkers: `neChunking` is the output of an
    HMM chunker that is trained to return the MUC-6 entities, `pChunking` is a simple
    regular expression that recognizes male pronouns, and `dChunking` is a dictionary
    chunker that recognizes US Presidents.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With overlaps allowed, we will see the chunks for `PRESIDENT` as well as `PERSON`
    in the merged output.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With overlaps disallowed, they will be added to the set overlapped chunks and
    removed from the unique chunks.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We initialized three chunkers that should be familiar to you from the previous
    recipes in this chapter:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Now, we will just chunk our input text via all three chunkers, combine the
    chunks into one set, and pass our `getCombinedChunks` method to it:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The meat of this recipe is in the `getCombinedChunks` method. We will just
    loop through all the chunks and check each pair if they overlap in their starts
    and ends. If they overlap and overlaps are not allowed, they are added to an overlapped
    set; otherwise, they are added to a combined set:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Here is the place to add more rules for overlapping chunks. For example, you
    can make it score based, so if the `PRESIDENT` chunk type has a higher score than
    the HMM-based one, you can choose it instead.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: CRFs for chunking
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CRFs are best known to provide close to state-of-the-art performance for named-entity
    tagging. This recipe will tell us how to build one of these systems. The recipe
    assumes that you have read, understood, and played with the *Conditional r* *andom
    fields – CRF for word/token tagging* recipe in [Chapter 4](part0051_split_000.html#page
    "Chapter 4. Tagging Words and Tokens"), *Tagging Words and Tokens*, which addresses
    the underlying technology. Like HMMs, CRFs treat named entity detection as a word-tagging
    problem, with an interpretation layer that provides chunkings. Unlike HMMs, CRFs
    use a logistic-regression-based classification approach, which, in turn, allows
    for random features to be included. Also, there is an excellent tutorial on CRFs
    that this recipe follows closely (but omits details) at [http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html).
    There is also a lot of information in the Javadoc.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Just as we did earlier, we will use a small hand-coded corpus to serve as training
    data. The corpus is in `src/com/lingpipe/cookbook/chapter5/TinyEntityCorpus.java`.
    It starts with:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所做的那样，我们将使用一个小型手编语料库作为训练数据。语料库位于 `src/com/lingpipe/cookbook/chapter5/TinyEntityCorpus.java`。它从以下内容开始：
- en: '[PRE97]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Since we are only using this corpus to train, the `visitTest()` method does
    nothing. However, the `visitTrain()` method exposes the handler to all the chunkings
    stored in the `CHUNKINGS` constant. This, in turn, looks like the following:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只使用这个语料库进行训练，`visitTest()` 方法不起作用。然而，`visitTrain()` 方法将处理程序暴露给存储在 `CHUNKINGS`
    常量中的所有分块。这反过来又看起来像以下这样：
- en: '[PRE98]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'We are still not done. Given that the creation of `Chunking` is fairly verbose,
    there are static methods to help dynamically create the requisite objects:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。鉴于 `Chunking` 的创建相当冗长，有一些静态方法可以帮助动态创建所需的对象：
- en: '[PRE99]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: This is all the setup; next, we will train and run a CRF on the preceding data.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所有的设置；接下来，我们将对前面的数据进行训练和运行一个条件随机场（CRF）。
- en: How to do it...
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Type the `TrainAndRunSimplCrf` class in the command line or run the equivalent
    in your IDE:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中输入 `TrainAndRunSimplCrf` 类或在您的集成开发环境（IDE）中运行等效命令：
- en: '[PRE100]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'This results in loads of screen output that report on the health and progress
    of the CRF, it is mostly information from the underlying logistic-regression classifier
    that drives the whole show. The fun bit is that we will get an invitation to play
    with the new CRF:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会产生大量的屏幕输出，报告 CRF 的健康和进度，这主要是来自驱动整个过程的底层逻辑回归分类器的信息。有趣的是，我们将收到一个邀请来玩新的 CRF：
- en: '[PRE101]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The chunker reports the first best output:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分块器报告了第一个最佳输出：
- en: '[PRE102]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: The preceding output is the first best analysis by the CRF of what sorts of
    entities are in the sentence. It thinks that `John Smith` is `PER` with `the 0-10:PER@-Infinity`
    output. We know that it applies to the `John Smith` string by taking the substring
    from 0 to 10 in the input text. Ignore `–Infinity`, which is supplied for chunks
    that have no score. The first best chunking does not have scores. The other entity
    that it thinks is in the text is `New York` as an `LOC`.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的输出是 CRF 对句子中存在哪些实体类型的第一次最佳分析。它认为 `John Smith` 是 `PER`，输出为 `the 0-10:PER@-Infinity`。我们知道它适用于
    `John Smith` 字符串，是通过在输入文本中从 0 到 10 取子字符串来实现的。忽略 `–Infinity`，这是为没有得分的分块提供的。第一次最佳分块没有得分。它认为文本中存在的另一个实体是
    `New York` 作为 `LOC`。
- en: 'Immediately, the conditional probabilities follow:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 立即，条件概率随之而来：
- en: '[PRE103]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: The preceding output provides the 10 best analyses of the whole phrase, along
    with their conditional (natural log) probabilities. In this case, we will see
    that the system isn't particularly confident of any of its analyses. For instance,
    the estimated probability of the first best analysis being correct is `exp(-1.66)=0.19`.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的输出提供了整个短语的 10 个最佳分析，以及它们的条件（自然对数）概率。在这种情况下，我们将看到系统对其分析并不特别自信。例如，第一次最佳分析正确的估计概率为
    `exp(-1.66)=0.19`。
- en: 'Next, in the output, we see probabilities for individual chunks:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在输出中，我们可以看到单个分块的概率：
- en: '[PRE104]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: As with the previous conditional output, the probabilities are logs, so we can
    see that the `John Smith` chunk has estimated probability `exp(-0.49) = 0.61`,
    which makes sense because in training the CRF saw `John` at the beginning of `PER`
    and `Smith` at the end of another, but not `John Smith` directly.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与之前的条件输出一样，概率是日志形式，因此我们可以看到 `John Smith` 分块估计的概率为 `exp(-0.49) = 0.61`，这是有道理的，因为在训练
    CRF 时，它看到了 `John` 在 `PER` 的开头，`Smith` 在另一个结尾，但没有直接看到 `John Smith`。
- en: The preceding kind of probability distributions can really improve systems if
    there are sufficient resources to consider a broad range of analyses and ways
    of combining evidence to allow for improbable outcomes to be selected. First best
    analyses tend to be over committed to conservative outcomes that fit what training
    data looks like.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的这种概率分布如果考虑到足够多的资源来考虑广泛的分析和结合证据的方式，以允许选择不太可能的结果，确实可以改善系统。第一次最佳分析往往过于承诺于符合训练数据看起来那样的保守结果。
- en: How it works…
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The code in `src/com/lingpipe/cookbook/chapter5/TrainAndRunSimpleCRF.java`
    resembles our classifier and HMM recipes with a few differences. These differences
    are addressed as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '`src/com/lingpipe/cookbook/chapter5/TrainAndRunSimpleCRF.java` 中的代码与我们的分类器和隐马尔可夫模型（HMM）配方相似，但有几个不同之处。这些差异如下所述：'
- en: '[PRE105]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: When we previously played with CRFs, the inputs were of the `Tagging<String>`
    type. Looking back at `TinyEntityCorpus.java`, the types are of the `Chunking`
    type. The preceding `BioTagChunkCodec` facilitates the translation of `Chunking`
    into `Tagging` via the efforts of a supplied `TokenizerFactory` and `boolean`
    that raise an exception if `TokenizerFactory` does not exactly agree with the
    `Chunk` starts and ends. Look back to the *Translating between word tagging and
    chunks–BIO codec* recipe better understand the role of this class.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前玩CRFs时，输入是 `Tagging<String>` 类型。回顾 `TinyEntityCorpus.java`，类型是 `Chunking`
    类型。前面的 `BioTagChunkCodec` 通过提供的 `TokenizerFactory` 和 `boolean` 将 `Chunking` 转换为
    `Tagging`，如果 `TokenizerFactory` 与 `Chunk` 的开始和结束不完全一致，则会抛出异常。回顾*在单词标记和分块之间转换-BIO编解码器*配方以更好地理解此类的作用。
- en: 'Let''s take a look at the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下内容：
- en: '[PRE106]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'This codec will translate into a tagging:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 此编解码器将转换为以下标记：
- en: '[PRE107]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'All the mechanics are hidden inside a new `ChainCrfChunker` class, and it is
    initialized in a manner similar to logistic regression, which is the underlying
    technology. Refer to the *Logistic regression* recipe of [Chapter 3](part0036_split_000.html#page
    "Chapter 3. Advanced Classifiers"), *Advanced Classifiers*, for more information
    on the configuration:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机制都隐藏在一个新的 `ChainCrfChunker` 类中，并且它的初始化方式类似于逻辑回归，这是其底层技术。有关配置的更多信息，请参阅[第3章](part0036_split_000.html#page
    "第3章. 高级分类器")的*逻辑回归*配方：
- en: '[PRE109]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: The only new thing here is the `tagChunkCodec` parameter, which we just described.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一的新事物是 `tagChunkCodec` 参数，我们刚刚描述了它。
- en: 'Once the training is over, we will access the chunker for first best with the
    following code:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们将使用以下代码访问分块器以获取最佳结果：
- en: '[PRE110]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Conditional chunkings are delivered by:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 条件分块通过以下方式提供：
- en: '[PRE111]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'The individual chunks are accessed with:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下方式访问单个分块：
- en: '[PRE112]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: That's it. You have access to one of the world's finest chunking technologies.
    Next, we will show you how to make it better.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。您已经可以使用世界上最先进的分块技术之一了。接下来，我们将向您展示如何使其变得更好。
- en: NER using CRFs with better features
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CRFs进行NER并具有更好的特征
- en: In this recipe, we'll show you how to create a realistic, though not quite state-of-the-art,
    set of features for CRFs. The features will include normalized tokens, part-of-speech
    tags, word-shape features, position features, and token prefixes and suffixes.
    Substitute it for the `SimpleCrfFeatureExtractor` in the *CRFs for chunking* recipe
    to use it.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将向您展示如何为CRFs创建一组真实但并非最先进的特征。这些特征将包括归一化标记、词性标记、词形特征、位置特征以及标记的前缀和后缀。将其替换为*CRFs
    for chunking*配方中的 `SimpleCrfFeatureExtractor` 以使用它。
- en: How to do it…
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'The source for this recipe is in `src/com/lingpipe/cookbook/chapter5/FancyCrfFeatureExtractor.java`:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的源代码位于 `src/com/lingpipe/cookbook/chapter5/FancyCrfFeatureExtractor.java`：
- en: 'Open up your IDE or command prompt and type:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您的IDE或命令提示符，并输入：
- en: '[PRE113]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Brace yourself for an explosion of features from the console. The data being
    used for feature extraction is `TinyEntityCorpus` of the previous recipe. Luckily,
    the first bit of data is just the node features for the "John" in the sentence
    `John ran.`:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备好从控制台爆炸般涌现出的功能。用于特征提取的数据是之前配方中的 `TinyEntityCorpus`。幸运的是，第一部分数据只是句子 `John ran.`
    中 "John" 的节点特征：
- en: '[PRE114]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'The next word in the sequence adds edge features—we won''t bother showing you
    the node features:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 序列中的下一个词添加了边缘特征——我们不会展示节点特征：
- en: '[PRE115]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: How it works…
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: As with other recipes, we won't bother discussing parts that are very similar
    to previous recipes—the relevant previous recipe here is the *Modifying CRFs*
    recipe in [Chapter 4](part0051_split_000.html#page "Chapter 4. Tagging Words and
    Tokens"), *Tagging Words and Tokens*. This is exactly the same, except for the
    fact that we will add in a lot more features—perhaps, from unexpected sources.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他配方一样，我们不会讨论与之前配方非常相似的部分——这里相关的先前配方是[第4章](part0051_split_000.html#page "第4章.
    标记单词和标记")的*修改CRFs*配方，*标记单词和标记*。这完全一样，只是我们将添加更多功能——也许，来自意想不到的来源。
- en: Note
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The tutorial for CRFs covers how to serialize/deserialize this class. This implementation
    does not cover it.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: CRFs教程涵盖了如何序列化/反序列化此类。此实现不包含它。
- en: 'Object construction is similar to the `Modifying CRFs` recipe in [Chapter 4](part0051_split_000.html#page
    "Chapter 4. Tagging Words and Tokens"), *Tagging Words and Tokens*:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 对象构造类似于[第4章](part0051_split_000.html#page "第4章. 标记单词和标记")的*修改CRFs*配方，*标记单词和标记*：
- en: '[PRE116]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: The constructor sets up a part-of-speech tagger with a cache and shoves it into
    the `mPosTagger` member variable.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数使用缓存设置了一个词性标注器，并将其推入`mPosTagger`成员变量中。
- en: 'The following method does very little, except supplying an inner `ChunkerFeatures`
    class:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方法做得很少，除了提供一个内部的`ChunkerFeatures`类：
- en: '[PRE117]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'The `ChunkerFeatures` class is where things get more interesting:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChunkerFeatures`类是事情变得更有趣的地方：'
- en: '[PRE118]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: The `mPosTagger` function is used to set up `Tagging<String>` for the tokens
    presented on class creation. This will be aligned with the `tag()` and `token()`
    superclass methods and be the source of part-of-speech tags as a node feature.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '`mPosTagger`函数用于在类创建时为提供的标记设置`Tagging<String>`。这将与`tag()`和`token()`超类方法对齐，并作为节点特征的词性标签来源。'
- en: 'Now, we can get on with the feature extraction. We will start with edge features,
    as they are the simplest:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续进行特征提取。我们将从边缘特征开始，因为它们是最简单的：
- en: '[PRE119]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: The new feature is prefixed with `PREV_TAG_TOKEN_CAT_`, and the example is `PREV_TAG_TOKEN_CAT_PN_LET-CAP=1.0`.
    The `tokenCat()` method looks at the word shape feature for the previous token
    and returns it as a string. Look at the Javadoc for `IndoEuropeanTokenCategorizer`
    to see what is going on.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 新特征以`PREV_TAG_TOKEN_CAT_`为前缀，例如`PREV_TAG_TOKEN_CAT_PN_LET-CAP=1.0`。`tokenCat()`方法查看前一个标记的词形特征，并将其作为字符串返回。查看`IndoEuropeanTokenCategorizer`的Javadoc以了解发生了什么。
- en: 'Next comes the node features. There are many of these; each will be presented
    in turn:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是节点特征。这里有许多这样的特征；每个将依次介绍：
- en: '[PRE120]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'The preceding code sets up the method with the appropriate return type. The
    next two lines set up some state to know where the feature extractor is in the
    string:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码设置了具有适当返回类型的方法。接下来的两行设置了某些状态，以了解特征提取器在字符串中的位置：
- en: '[PRE121]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Next, we will compute the token categories, tokens, and part-of-speech tags
    for the current position, previous position, and the next position of the input:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算输入的当前位置、前一个位置和下一个位置的标记类别、标记和词性标签：
- en: '[PRE122]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: The previous and next methods check if we're at the begin or end of the sentence
    and return `null` accordingly. The part-of-speech tagging is taken from the saved
    part-of-speech taggings computed in the constructor.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个和下一个方法检查我们是否位于句子的开始或结束，并相应地返回`null`。词性标注是从构造函数中计算出的已保存的词性标注中获取的。
- en: 'The token methods provide some normalization of tokens to compress all numbers
    to the same kind of value. This method is as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 标记方法提供了一些标记的规范化，以将所有数字压缩到同一类型的值。此方法如下：
- en: '[PRE123]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: This just takes every sequence of numbers and replaces it with `*D...D*`. For
    instance, `12/3/08` is converted to `*DD*/*D*/*DD*`.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是将每个数字序列替换为`*D...D*`。例如，`12/3/08`被转换为`*DD*/*D*/*DD*`。
- en: 'We will then set feature values for the preceding, current, and following tokens.
    First, a flag indicates whether it begins or ends a sentence or an internal node:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将为前一个、当前和后续标记设置特征值。首先，一个标志指示它是否是句子或内部节点的开始或结束：
- en: '[PRE124]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Next, we will include the tokens, token categories, and their parts of speech:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将包括标记、标记类别及其词性：
- en: '[PRE125]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Finally, we will add the prefix and suffix features, which add features for
    each suffix and prefix (up to a prespecified length):'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加前缀和后缀特征，这些特征为每个后缀和前缀（最长可达预定义长度）添加特征：
- en: '[PRE126]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: After this, we will just return the feature mapping generated.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们只需返回生成的特征映射。
- en: 'The `prefix` or `suffix` function is simply implemented with a list:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '`prefix`或`suffix`函数简单地使用列表实现：'
- en: '[PRE127]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: That's a nice feature set for your named-entity detector.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对你的命名实体检测器来说很棒的特征集。
