- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Model Pruning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型剪枝
- en: In this chapter, we’ll explore **model pruning** techniques designed to reduce
    model size while maintaining performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨旨在在保持性能的同时减少模型大小的**模型剪枝**技术。
- en: Model pruning refers to the systematic elimination of unnecessary parameters
    from a neural network while maintaining performance. For LLMs, this typically
    involves identifying and removing redundant or less important weights, neurons,
    or attention heads based on criteria such as magnitude, sensitivity analysis,
    or gradient-based importance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 模型剪枝是指在保持性能的同时，从神经网络中系统地消除不必要的参数。对于LLMs来说，这通常涉及根据幅度、敏感性分析或基于梯度的重要性等标准识别和移除冗余或不重要的权重、神经元或注意力头。
- en: You’ll learn how to implement various pruning methods, from magnitude-based
    pruning to iterative techniques, and the trade-offs involved in size reduction
    versus performance. Additionally, this chapter will help you decide whether to
    prune during or after training, ensuring your LLMs remain efficient and effective.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何实现各种剪枝方法，从基于幅度的剪枝到迭代技术，以及大小缩减与性能之间的权衡。此外，本章将帮助你决定是在训练期间还是训练后进行剪枝，以确保你的LLMs保持高效和有效。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Magnitude-based pruning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于幅度的剪枝
- en: Structured versus unstructured pruning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化与非结构化剪枝
- en: Iterative pruning techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代剪枝技术
- en: Pruning during training versus post-training pruning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练期间剪枝与训练后剪枝
- en: Balancing pruning and model performance
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡剪枝和模型性能
- en: Combining pruning with other compression techniques
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将剪枝与其他压缩技术结合
- en: Magnitude-based pruning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于幅度的剪枝
- en: '**Magnitude-based pruning** is one of the simplest and most widely used pruning
    techniques. The idea behind this method is to remove weights in the neural network
    that contribute least to the model’s overall function, typically, these are weights
    with the smallest magnitude (absolute value). By pruning such weights, the model
    becomes more compact and faster, with minimal impact on accuracy:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于幅度的剪枝**是最简单且最广泛使用的剪枝技术之一。这种方法背后的思想是移除对神经网络整体功能贡献最小的权重，通常，这些是幅度最小（绝对值）的权重。通过剪枝这些权重，模型变得更加紧凑和快速，对准确性的影响最小：'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code example, magnitude-based pruning removes 30% of the lowest-magnitude
    weights in all linear layers of an LLM. The `prune.l1_unstructured` function specifies
    that weights with the smallest L1 norm will be pruned.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码示例中，基于幅度的剪枝移除了LLM所有线性层中幅度最低的30%的权重。`prune.l1_unstructured`函数指定了具有最小L1范数的权重将被剪枝。
- en: 'The following code snippet implements the `prune.l1_unstructured` function
    for unstructured L1-norm-based pruning on a given parameter tensor within a PyTorch
    module by zeroing out weights with the smallest absolute values:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段实现了在PyTorch模块中对给定参数张量进行无结构L1范数剪枝的`prune.l1_unstructured`函数，通过将绝对值最小的权重置零来实现：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, the function begins by extracting the target tensor from the module and
    determining how many of its elements should be pruned based on the specified proportion
    `amount`. It identifies a pruning threshold by computing the *k*-th smallest absolute
    value in the tensor, where `k` corresponds to the number of parameters to prune.
    A binary mask is then created, where values above the threshold are retained while
    those below the threshold are set to zero. This mask is applied to produce a pruned
    version of the tensor, which replaces the original parameter in the module. The
    mask is stored as a buffer to persist across model operations, and a forward pre-hook
    is registered to ensure that pruning is enforced before every forward pass, preserving
    the sparsity pattern even if the underlying weights are updated during training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，函数首先从模块中提取目标张量，并确定根据指定的比例`amount`应该剪枝多少个元素。它通过计算张量中第`k`小的绝对值来确定剪枝阈值，其中`k`对应于要剪枝的参数数量。然后创建一个二进制掩码，其中高于阈值的值被保留，而低于阈值的值被设置为零。这个掩码被应用于生成张量的剪枝版本，它替换了模块中的原始参数。掩码作为缓冲区存储，以在模型操作之间持久化，并注册了一个前向预钩子，以确保在每次前向传递之前强制执行剪枝，即使在训练过程中底层权重被更新，也能保持稀疏模式。
- en: In model pruning, the L1 norm is used to evaluate the importance of weights
    or parameters in a model by summing the absolute values of their components, with
    lower L1 norm values often indicating less significant parameters that can be
    removed to reduce model size while maintaining performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型剪枝中，使用L1范数通过求其成分的绝对值之和来评估模型中权重或参数的重要性，通常L1范数值较低表示不那么重要的参数，可以移除以减小模型大小同时保持性能。
- en: After pruning, the `prune.remove` method is called to remove the pruning reparameterization
    and make the changes permanent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝后，调用`prune.remove`方法来移除剪枝重新参数化并使更改永久化。
- en: Magnitude-based pruning is particularly effective for models with many small
    weights that contribute little to overall performance, but it may not be sufficient
    when applied alone for large-scale pruning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于幅度的剪枝对于具有许多小权重且对整体性能贡献不大的模型特别有效，但单独应用时可能不足以进行大规模剪枝。
- en: Structured versus unstructured pruning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化剪枝与无结构化剪枝
- en: 'When pruning LLMs, you can either prune weights individually (unstructured
    pruning) or remove entire structures, such as filters, channels, or attention
    heads (structured pruning):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在剪枝LLMs时，你可以单独剪除权重（无结构化剪枝）或移除整个结构，如滤波器、通道或注意力头（结构化剪枝）：
- en: '`prune.l1_unstructured` function described earlier.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前描述的`prune.l1_unstructured`函数。
- en: '**Structured pruning**: Entire sections of the model, such as neurons, channels,
    or layers, are pruned. This approach is easier to implement on modern hardware
    and often leads to better speedups in inference time, even though it may have
    a larger immediate effect on model performance.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化剪枝**：剪枝整个模型的部分，如神经元、通道或层。这种方法在现代硬件上更容易实现，并且通常会导致推理时间上的更好加速，尽管它可能对模型性能有更大的即时影响。'
- en: 'Structured pruning in LLMs can be implemented using PyTorch’s built-in utilities,
    as shown in the following code. Here, we apply L2-norm structured pruning to remove
    30% of neurons across linear layers, targeting entire rows of weight matrices
    to effectively eliminate complete neurons rather than just individual connections:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs中，可以使用PyTorch的内置工具实现结构化剪枝，如下面的代码所示。在这里，我们应用L2范数结构化剪枝来移除线性层中30%的神经元，目标是整个权重矩阵的行，以有效地消除完整的神经元而不是仅仅单个连接：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this structured pruning example, the `ln_structured` function removes entire
    neurons from the linear layer based on the L2 norm across all weights in a given
    dimension. The choice of structured pruning can significantly reduce computational
    complexity while also making the model more suitable for deployment on standard
    hardware architectures.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个结构化剪枝示例中，`ln_structured`函数根据给定维度的所有权重中的L2范数从线性层中移除整个神经元。结构化剪枝的选择可以显著降低计算复杂度，同时使模型更适合部署在标准硬件架构上。
- en: Next, we’ll see how to prune a small fraction of weights at a time over multiple
    training steps instead of pruning large portions of the model in a single pass.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何通过在多个训练步骤中每次剪除一小部分权重，而不是一次性剪除模型的大部分内容。
- en: Iterative pruning techniques
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代剪枝技术
- en: Here, we’ll talk about **iterative pruning**, which allows you to prune a small
    fraction of weights at a time over multiple training steps. This method reduces
    the risk of drastic performance drops and provides more opportunities for the
    model to recover and adjust to the pruning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论**迭代剪枝**，它允许你在多个训练步骤中每次剪除一小部分权重。这种方法降低了性能急剧下降的风险，并为模型提供了更多恢复和适应剪枝的机会。
- en: 'The iterative approach also allows for fine-tuning after each pruning step,
    enabling the model to “heal” from the weight reduction:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代方法还允许在每个剪枝步骤后进行微调，使模型能够从权重减少中“恢复”：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, 10% of the weights are pruned after every 10 epochs. The gradual
    removal of weights ensures that the model has enough time to adjust between each
    pruning step. Iterative pruning, combined with validation steps, can help find
    a more optimal balance between model size and performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，每10个epoch后剪除10%的权重。逐渐移除权重确保模型有足够的时间在每次剪枝步骤之间进行调整。迭代剪枝与验证步骤相结合可以帮助找到模型大小和性能之间的更优平衡。
- en: Pruning during training versus post-training pruning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练期间剪枝与训练后剪枝
- en: 'A key decision in applying pruning is whether to prune the model during training
    or after training is complete:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 应用剪枝时的一个关键决策是在训练期间还是训练完成后进行剪枝：
- en: '**Pruning during training**: This approach allows the model to adjust to the
    pruned structure over time by iteratively pruning weights as it learns. The model
    can compensate for pruned weights, potentially resulting in better final performance.
    However, it requires more computational resources and training time.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练过程中的剪枝**：这种方法允许模型通过迭代地剪枝权重来逐渐适应剪枝结构。模型可以补偿剪枝的权重，从而可能带来更好的最终性能。然而，这需要更多的计算资源和训练时间。'
- en: 'Here’s an example of this approach:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是这种方法的一个例子：
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Post-training pruning**: In this approach, pruning is performed after the
    model has been fully trained. This method is computationally efficient since it
    doesn’t require modifications during the training process, and you can optionally
    fine-tune the model afterward. However, it may result in a larger accuracy drop
    compared to pruning during training.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练后剪枝**：在这种方法中，剪枝是在模型完全训练后进行的。这种方法计算效率高，因为它不需要在训练过程中进行修改，并且你可以选择在之后微调模型。然而，与训练过程中的剪枝相比，它可能会导致更大的准确性下降。'
- en: 'Let’s take a look at an example of post-training pruning:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看训练后剪枝的一个例子：
- en: '[PRE5]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The choice between these two depends on your performance constraints and available
    resources. Pruning during training often leads to more stable models, while post-training
    pruning is faster and more resource-efficient.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的选择取决于你的性能约束和可用资源。训练过程中的剪枝通常会导致更稳定的模型，而训练后的剪枝更快且更高效。
- en: Balancing pruning and model performance
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡剪枝和模型性能
- en: Finding the right balance between pruning and model performance is critical.
    Aggressive pruning can lead to significant performance degradation, while too
    little pruning may not yield enough benefits. The key is to identify which parts
    of the model can be pruned with minimal impact on accuracy. This requires careful
    validation after each pruning step and close monitoring of key performance metrics.
    These metrics include parameter reduction rates, inference speed gains, memory
    footprint reduction, changes in perplexity, and task-specific performance. Throughout
    the process, it’s crucial to balance the accuracy-efficiency trade-off to ensure
    the pruned model retains acceptable performance despite having fewer parameters
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在剪枝和模型性能之间找到合适的平衡至关重要。过于激进的剪枝可能导致性能显著下降，而剪枝不足可能不会带来足够的收益。关键在于识别哪些模型部分可以被剪枝而不会对准确性产生太大影响。这需要在每次剪枝步骤后进行仔细验证，并密切监控关键性能指标。这些指标包括参数减少率、推理速度提升、内存占用减少、困惑度变化以及特定任务的性能。在整个过程中，平衡准确性-效率权衡至关重要，以确保剪枝模型在参数减少的情况下仍能保持可接受的性能。
- en: 'A common strategy is to apply fine-tuning after pruning to restore some of
    the lost performance. Fine-tuning allows the model to adjust to the pruned structure
    and recover its original capabilities:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的策略是在剪枝后进行微调以恢复一些丢失的性能。微调允许模型适应剪枝结构并恢复其原始能力：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this example, after pruning a portion of the weights, the model is fine-tuned
    with a lower learning rate to restore performance. A lower learning rate allows
    the model to adjust gradually to the new pruned structure, preventing the destabilization
    of learned features. Validation is performed after each fine-tuning step to monitor
    the model’s progress and ensure that pruning has not significantly degraded performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，在剪枝部分权重之后，模型使用较低的学习率进行微调以恢复性能。较低的学习率允许模型逐渐适应新的剪枝结构，防止学习特征的不稳定。在每次微调步骤后进行验证，以监控模型的进展并确保剪枝没有显著降低性能。
- en: Let’s see how we can combine pruning with other model compression techniques.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将剪枝与其他模型压缩技术相结合。
- en: Combining pruning with other compression techniques
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将剪枝与其他压缩技术相结合
- en: Pruning can be combined with other model compression techniques, such as quantization
    or distillation, to achieve even greater reductions in model size and complexity.
    Combining these techniques often results in more compact models that maintain
    high performance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝可以与其他模型压缩技术相结合，如量化或蒸馏，以实现模型大小和复杂性的更大减少。结合这些技术通常会产生更紧凑的模型，同时保持高性能。
- en: Pruning and quantization
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝和量化
- en: 'Pruning followed by **quantization** can lead to significant reductions in
    model size and faster inference speeds, especially for resource-constrained environments:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在剪枝后进行**量化**可以显著减少模型大小并加快推理速度，尤其是在资源受限的环境中：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Pruning and knowledge distillation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝和知识蒸馏
- en: 'You can also combine pruning with **knowledge distillation**, where a smaller,
    pruned **student model** is trained to mimic the behavior of a larger, well-trained
    **teacher model**:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以将剪枝与**知识蒸馏**相结合，其中较小的、剪枝的**学生模型**被训练来模仿较大的、训练良好的**教师模型**的行为：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This approach allows the student model to achieve high performance with fewer
    parameters. Knowledge distillation helps compensate for accuracy loss caused by
    pruning by transferring high-level representations from the unpruned teacher model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许学生模型在更少的参数下实现高性能。知识蒸馏有助于通过从未剪枝的教师模型中传递高级表示来补偿剪枝造成的精度损失。
- en: These examples illustrate how pruning can be applied during or after training,
    balanced with performance requirements, and combined with other compression techniques
    such as quantization and knowledge distillation to create more efficient LLMs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例说明了剪枝如何在训练期间或之后应用，平衡性能要求，并结合其他压缩技术，如量化和知识蒸馏，以创建更高效的LLMs。
- en: Summary
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored various model pruning techniques for LLMs, including
    magnitude-based pruning, structured versus unstructured pruning, and iterative
    pruning methods. We discussed the trade-offs involved in pruning during training
    versus post-training, and the importance of fine-tuning after pruning to recover
    lost performance. By combining pruning with other compression techniques, such
    as quantization and distillation, you can create more efficient LLMs suitable
    for deployment in resource-constrained environments.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了LLMs的各种模型剪枝技术，包括基于幅度的剪枝、结构化与非结构化剪枝以及迭代剪枝方法。我们讨论了在训练期间与训练后剪枝所涉及的权衡，以及剪枝后微调以恢复丢失性能的重要性。通过结合剪枝与其他压缩技术，如量化和蒸馏，你可以创建更适合在资源受限环境中部署的更高效的LLMs。
- en: In the next chapter, we’ll explore quantization techniques for LLMs, focusing
    on reducing numerical precision to improve model efficiency while maintaining
    performance. You’ll learn how to apply post-training and quantization-aware training
    to optimize your LLMs further.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨用于LLMs的量化技术，重点关注降低数值精度以提高模型效率，同时保持性能。你将学习如何应用训练后和量化感知训练来进一步优化你的LLMs。
