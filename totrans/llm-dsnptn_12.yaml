- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore **model pruning** techniques designed to reduce
    model size while maintaining performance.
  prefs: []
  type: TYPE_NORMAL
- en: Model pruning refers to the systematic elimination of unnecessary parameters
    from a neural network while maintaining performance. For LLMs, this typically
    involves identifying and removing redundant or less important weights, neurons,
    or attention heads based on criteria such as magnitude, sensitivity analysis,
    or gradient-based importance.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll learn how to implement various pruning methods, from magnitude-based
    pruning to iterative techniques, and the trade-offs involved in size reduction
    versus performance. Additionally, this chapter will help you decide whether to
    prune during or after training, ensuring your LLMs remain efficient and effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude-based pruning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured versus unstructured pruning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterative pruning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning during training versus post-training pruning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing pruning and model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining pruning with other compression techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magnitude-based pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Magnitude-based pruning** is one of the simplest and most widely used pruning
    techniques. The idea behind this method is to remove weights in the neural network
    that contribute least to the model’s overall function, typically, these are weights
    with the smallest magnitude (absolute value). By pruning such weights, the model
    becomes more compact and faster, with minimal impact on accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code example, magnitude-based pruning removes 30% of the lowest-magnitude
    weights in all linear layers of an LLM. The `prune.l1_unstructured` function specifies
    that weights with the smallest L1 norm will be pruned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet implements the `prune.l1_unstructured` function
    for unstructured L1-norm-based pruning on a given parameter tensor within a PyTorch
    module by zeroing out weights with the smallest absolute values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, the function begins by extracting the target tensor from the module and
    determining how many of its elements should be pruned based on the specified proportion
    `amount`. It identifies a pruning threshold by computing the *k*-th smallest absolute
    value in the tensor, where `k` corresponds to the number of parameters to prune.
    A binary mask is then created, where values above the threshold are retained while
    those below the threshold are set to zero. This mask is applied to produce a pruned
    version of the tensor, which replaces the original parameter in the module. The
    mask is stored as a buffer to persist across model operations, and a forward pre-hook
    is registered to ensure that pruning is enforced before every forward pass, preserving
    the sparsity pattern even if the underlying weights are updated during training.
  prefs: []
  type: TYPE_NORMAL
- en: In model pruning, the L1 norm is used to evaluate the importance of weights
    or parameters in a model by summing the absolute values of their components, with
    lower L1 norm values often indicating less significant parameters that can be
    removed to reduce model size while maintaining performance.
  prefs: []
  type: TYPE_NORMAL
- en: After pruning, the `prune.remove` method is called to remove the pruning reparameterization
    and make the changes permanent.
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude-based pruning is particularly effective for models with many small
    weights that contribute little to overall performance, but it may not be sufficient
    when applied alone for large-scale pruning.
  prefs: []
  type: TYPE_NORMAL
- en: Structured versus unstructured pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When pruning LLMs, you can either prune weights individually (unstructured
    pruning) or remove entire structures, such as filters, channels, or attention
    heads (structured pruning):'
  prefs: []
  type: TYPE_NORMAL
- en: '`prune.l1_unstructured` function described earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured pruning**: Entire sections of the model, such as neurons, channels,
    or layers, are pruned. This approach is easier to implement on modern hardware
    and often leads to better speedups in inference time, even though it may have
    a larger immediate effect on model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Structured pruning in LLMs can be implemented using PyTorch’s built-in utilities,
    as shown in the following code. Here, we apply L2-norm structured pruning to remove
    30% of neurons across linear layers, targeting entire rows of weight matrices
    to effectively eliminate complete neurons rather than just individual connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this structured pruning example, the `ln_structured` function removes entire
    neurons from the linear layer based on the L2 norm across all weights in a given
    dimension. The choice of structured pruning can significantly reduce computational
    complexity while also making the model more suitable for deployment on standard
    hardware architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll see how to prune a small fraction of weights at a time over multiple
    training steps instead of pruning large portions of the model in a single pass.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative pruning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we’ll talk about **iterative pruning**, which allows you to prune a small
    fraction of weights at a time over multiple training steps. This method reduces
    the risk of drastic performance drops and provides more opportunities for the
    model to recover and adjust to the pruning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iterative approach also allows for fine-tuning after each pruning step,
    enabling the model to “heal” from the weight reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, 10% of the weights are pruned after every 10 epochs. The gradual
    removal of weights ensures that the model has enough time to adjust between each
    pruning step. Iterative pruning, combined with validation steps, can help find
    a more optimal balance between model size and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning during training versus post-training pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A key decision in applying pruning is whether to prune the model during training
    or after training is complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pruning during training**: This approach allows the model to adjust to the
    pruned structure over time by iteratively pruning weights as it learns. The model
    can compensate for pruned weights, potentially resulting in better final performance.
    However, it requires more computational resources and training time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of this approach:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Post-training pruning**: In this approach, pruning is performed after the
    model has been fully trained. This method is computationally efficient since it
    doesn’t require modifications during the training process, and you can optionally
    fine-tune the model afterward. However, it may result in a larger accuracy drop
    compared to pruning during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at an example of post-training pruning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The choice between these two depends on your performance constraints and available
    resources. Pruning during training often leads to more stable models, while post-training
    pruning is faster and more resource-efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing pruning and model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding the right balance between pruning and model performance is critical.
    Aggressive pruning can lead to significant performance degradation, while too
    little pruning may not yield enough benefits. The key is to identify which parts
    of the model can be pruned with minimal impact on accuracy. This requires careful
    validation after each pruning step and close monitoring of key performance metrics.
    These metrics include parameter reduction rates, inference speed gains, memory
    footprint reduction, changes in perplexity, and task-specific performance. Throughout
    the process, it’s crucial to balance the accuracy-efficiency trade-off to ensure
    the pruned model retains acceptable performance despite having fewer parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'A common strategy is to apply fine-tuning after pruning to restore some of
    the lost performance. Fine-tuning allows the model to adjust to the pruned structure
    and recover its original capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, after pruning a portion of the weights, the model is fine-tuned
    with a lower learning rate to restore performance. A lower learning rate allows
    the model to adjust gradually to the new pruned structure, preventing the destabilization
    of learned features. Validation is performed after each fine-tuning step to monitor
    the model’s progress and ensure that pruning has not significantly degraded performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can combine pruning with other model compression techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Combining pruning with other compression techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pruning can be combined with other model compression techniques, such as quantization
    or distillation, to achieve even greater reductions in model size and complexity.
    Combining these techniques often results in more compact models that maintain
    high performance.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning and quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pruning followed by **quantization** can lead to significant reductions in
    model size and faster inference speeds, especially for resource-constrained environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Pruning and knowledge distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also combine pruning with **knowledge distillation**, where a smaller,
    pruned **student model** is trained to mimic the behavior of a larger, well-trained
    **teacher model**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This approach allows the student model to achieve high performance with fewer
    parameters. Knowledge distillation helps compensate for accuracy loss caused by
    pruning by transferring high-level representations from the unpruned teacher model.
  prefs: []
  type: TYPE_NORMAL
- en: These examples illustrate how pruning can be applied during or after training,
    balanced with performance requirements, and combined with other compression techniques
    such as quantization and knowledge distillation to create more efficient LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various model pruning techniques for LLMs, including
    magnitude-based pruning, structured versus unstructured pruning, and iterative
    pruning methods. We discussed the trade-offs involved in pruning during training
    versus post-training, and the importance of fine-tuning after pruning to recover
    lost performance. By combining pruning with other compression techniques, such
    as quantization and distillation, you can create more efficient LLMs suitable
    for deployment in resource-constrained environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll explore quantization techniques for LLMs, focusing
    on reducing numerical precision to improve model efficiency while maintaining
    performance. You’ll learn how to apply post-training and quantization-aware training
    to optimize your LLMs further.
  prefs: []
  type: TYPE_NORMAL
