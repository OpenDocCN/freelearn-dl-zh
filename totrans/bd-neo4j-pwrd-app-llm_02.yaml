- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Demystifying RAG
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭秘 RAG
- en: In the previous chapter, we explored the evolution of LLMs and how they have
    changed the GenAI landscape. We also discussed some of their pitfalls. We will
    explore how we can avoid these pitfalls using **Retrieval-Augmented Generation**
    (**RAG**) in this chapter. We will take a look at what RAG means, what its architecture
    is, and how it fits into the LLM workflow in building improved intelligent applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了 LLM 的演变以及它们如何改变 GenAI 的格局。我们还讨论了一些它们的陷阱。在本章中，我们将探讨如何使用 **Retrieval-Augmented
    Generation** （**RAG**）来避免这些陷阱。我们将探讨 RAG 的含义、其架构以及它在构建改进的智能应用程序的 LLM 工作流程中的位置。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding the power of RAG
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 RAG 的力量
- en: Deconstructing the RAG flow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解构 RAG 流程
- en: Retrieving external information for your RAG
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的 RAG 检索外部信息
- en: Building an end-to-end RAG flow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建端到端 RAG 流程
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires familiarity with the Python programming language (version
    `3.6` or higher is recommended) and basic concepts of deep learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要熟悉 Python 编程语言（建议使用版本 `3.6` 或更高版本）以及深度学习的基本概念。
- en: We will be leveraging popular AI toolkits such as Hugging Face’s Transformers
    library ([https://huggingface.co/docs/transformers/en/index](https://huggingface.co/docs/transformers/en/index))
    to build and experiment with RAG. While not mandatory, having a basic understanding
    of Git version control can be helpful.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用流行的 AI 工具包，如 Hugging Face 的 Transformers 库 ([https://huggingface.co/docs/transformers/en/index](https://huggingface.co/docs/transformers/en/index))
    来构建和实验 RAG。虽然不是强制性的，但具备 Git 版本控制的基本理解可能会有所帮助。
- en: Git allows you to easily clone the code repository for this chapter and track
    any changes you make. Do not worry about finding or typing the code yourself!
    We have created a dedicated public repository on GitHub, [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2),
    allowing you to easily clone it and follow along with the chapter’s hands-on exercises.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Git 允许您轻松地克隆本章的代码存储库并跟踪您所做的任何更改。无需担心自己寻找或输入代码！我们已经在 GitHub 上创建了一个专门的公共存储库，[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2)，您可以通过它轻松地克隆并跟随本章的动手练习。
- en: This repository contains all the necessary scripts, files, and configurations
    required to implement the RAG model and integrate Neo4j with advanced knowledge
    graph capabilities.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本存储库包含实现 RAG 模型以及将 Neo4j 与高级知识图谱功能集成的所有必要脚本、文件和配置。
- en: 'To follow along, make sure you have the following Python libraries installed
    in your environment:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟上进度，请确保您在环境中安装了以下 Python 库：
- en: '**Transformers**: Install the Hugging Face Transformers library for handling
    model-related functionalities: `pip install transformers`.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformers**：安装 Hugging Face Transformers 库以处理模型相关功能：`pip install transformers`。'
- en: '**PyTorch**: Install PyTorch as the backend for computation. Follow the instructions
    at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    to install the appropriate version for your system.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PyTorch**：将 PyTorch 作为计算的后端进行安装。按照 [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    上的说明安装适合您系统的相应版本。'
- en: '**scikit-learn**: For similarity calculations, install `scikit-learn` using
    the `pip install scikit-learn` command.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**：对于相似度计算，使用 `pip install scikit-learn` 命令安装 `scikit-learn`。'
- en: '**NumPy**: Install NumPy for numerical operations: `pip install numpy`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**：安装 NumPy 以进行数值运算：`pip install numpy`。'
- en: '**SentencePiece**: SentencePiece is required for text tokenization with certain
    models. You can install it using the instructions provided in the official GitHub
    repository: [https://github.com/google/sentencepiece#installation](https://github.com/google/sentencepiece#installation).
    For most Python environments, install it via `pip`: `pip install sentencepiece`.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SentencePiece**：某些模型需要 SentencePiece 进行文本分词。您可以使用官方 GitHub 存储库中提供的说明进行安装：[https://github.com/google/sentencepiece#installation](https://github.com/google/sentencepiece#installation)。对于大多数
    Python 环境，您可以通过 `pip` 安装它：`pip install sentencepiece`。'
- en: '**rank_bm25**: The `rank_bm25` library is required to implement the BM25 algorithm
    for keyword-based retrieval. You can install it using `pip`: `pip install rank_bm25`.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rank_bm25**：实现基于关键字的检索的 BM25 算法需要 `rank_bm25` 库。您可以使用 `pip` 安装它：`pip install
    rank_bm25`。'
- en: '**datasets**: The `datasets` library from Hugging Face provides efficient tools
    for loading, processing, and transforming datasets. It supports large-scale datasets
    with minimal memory usage. You can install it using `pip install datasets`.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：Hugging Face的`datasets`库提供了高效的工具，用于加载数据集、处理和转换数据集。它支持使用最小内存使用量处理大规模数据集。您可以使用`pip
    install datasets`进行安装。'
- en: '**pandas**: `pandas` is a powerful data analysis library in Python, used for
    manipulating tabular data. In this example, it helps preprocess the dataset by
    converting it into a DataFrame for easier manipulation. Install it using `pip
    install pandas`.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pandas**：`pandas`是Python中一个强大的数据分析库，用于操作表格数据。在这个例子中，它通过将其转换为DataFrame来帮助预处理数据集，以便更容易地进行操作。使用`pip
    install pandas`进行安装。'
- en: '**faiss-CPU**: `faiss-cpu` is a library for efficient similarity search and
    clustering of dense vectors. It is used in this example for building a retriever
    that fetches relevant passages during inference. Visit the Faiss GitHub repository
    ([https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss))
    for documentation and examples. Install it using `pip`: `pip install faiss-cpu`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**faiss-CPU**：`faiss-cpu`是一个用于高效搜索和聚类密集向量的库。在这个例子中，它用于构建在推理期间检索相关段落的检索器。访问Faiss的GitHub仓库（[https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)）以获取文档和示例。使用`pip`进行安装：`pip
    install faiss-cpu`。'
- en: '**Accelerate**: Accelerate is a library by Hugging Face that simplifies distributed
    training and inference. It ensures optimal hardware utilization across CPUs, GPUs,
    and multi-node setups. Install it using `pip install accelerate`.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加速**：加速是Hugging Face的一个库，它简化了分布式训练和推理。它确保了在CPU、GPU和多节点设置中硬件利用的最优化。使用`pip
    install accelerate`进行安装。'
- en: By ensuring your environment is configured with these tools, you can seamlessly
    explore the hands-on exercises provided in this chapter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过确保您的环境配置了这些工具，您可以无缝探索本章提供的动手练习。
- en: '**Note**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: All the sections in this chapter focus on the relevant code snippets. For the
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有部分都专注于相关的代码片段。
- en: 'complete code, please refer to the book’s GitHub repository: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码，请参阅本书的GitHub仓库：[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2)。
- en: Understanding the power of RAG
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解RAG的力量
- en: RAG was introduced by Meta researchers in 2020 ([https://arxiv.org/abs/2005.11401v4](https://arxiv.org/abs/2005.11401v4))
    as a framework that allows GenAI models to leverage external data that is not
    part of model training to enhance the output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是由Meta研究人员在2020年引入的（[https://arxiv.org/abs/2005.11401v4](https://arxiv.org/abs/2005.11401v4)），作为一个框架，允许GenAI模型利用模型训练之外的外部数据来增强输出。
- en: 'It is a widely known fact that LLMs suffer from hallucinations. One of the
    classic real-world examples of LLMs hallucinating is the case of Levidow, Levidow
    & Oberman, the New York law firm that was fined for submitting a legal brief containing
    fake citations generated by OpenAI’s ChatGPT in a case against Colombian airline
    Avianca. They were subsequently fined thousands of dollars, and they are likely
    to have lost more in reputational damage. You can read more about it here: [https://news.sky.com/story/lawyers-fined-after-citing-bogus-cases-from-chatgpt-research-12908318](https://news.sky.com/story/lawyers-fined-after-citing-bogus-cases-from-chatgpt-research-12908318).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，大型语言模型（LLMs）容易产生幻觉。一个经典的LLMs产生幻觉的真实世界例子是Levidow, Levidow & Oberman律师事务所，该律所在与哥伦比亚航空公司Avianca的案件中提交了一份包含由OpenAI的ChatGPT生成的虚假引用的法律简报，因此被罚款。他们随后被罚款数千美元，并且可能因声誉受损而损失更多。更多关于此事的信息，请参阅此处：[https://news.sky.com/story/lawyers-fined-after-citing-bogus-cases-from-chatgpt-research-12908318](https://news.sky.com/story/lawyers-fined-after-citing-bogus-cases-from-chatgpt-research-12908318)。
- en: 'LLM hallucinations can arise from several factors, such as the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的幻觉可能由以下几个因素引起：
- en: '**Overfitting to training data**: During training, the LLM might overfit to
    statistical patterns in the training data. This can lead the model to prioritize
    replicating those patterns over generating factually accurate content.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度拟合训练数据**：在训练过程中，LLM可能会过度拟合到训练数据中的统计模式。这可能导致模型优先复制这些模式，而不是生成事实准确的内容。'
- en: '**Lack of causal reasoning**: LLMs excel at identifying statistical relationships
    between words but may struggle to understand cause-and-effect relationships. This
    can lead to outputs that are grammatically correct but factually implausible.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏因果推理能力**：大型语言模型在识别词语之间的统计关系方面表现出色，但可能难以理解因果关系。这可能导致输出在语法上正确但在事实上不可信。'
- en: '**Temperature configuration**: LLMs can be configured with a parameter called
    **temperature**, a number between `0` and `1` that controls the randomness in
    text generation. Higher temperatures increase creativity but also the likelihood
    of hallucinations as the model deviates from expected responses.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**温度配置**：大型语言模型可以通过一个名为**温度**的参数进行配置，这是一个介于`0`和`1`之间的数字，它控制文本生成的随机性。较高的温度增加了创造力，但也增加了模型偏离预期响应并产生幻觉的可能性。'
- en: '**Missing information**: If the information required to generate an accurate
    response is not included in the training data, the model might generate plausible
    sounding but incorrect answers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失信息**：如果生成准确响应所需的信息未包含在训练数据中，模型可能会生成听起来合理但实际上错误的答案。'
- en: '**Flawed or biased training data**: The quality of the training process plays
    a significant role. If the dataset contains biases or inaccuracies, the model
    may perpetuate these issues, leading to hallucinations.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有缺陷或存在偏差的训练数据**：训练过程的质量起着重要作用。如果数据集包含偏差或不准确性，模型可能会持续这些问题，导致幻觉。'
- en: 'While hallucinations are a significant challenge, several methods can help
    mitigate them to some extent:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然幻觉是一个重大挑战，但几种方法可以在一定程度上帮助减轻它们：
- en: '**Prompt engineering**: This involves carefully crafting and iteratively refining
    the instructions or queries given to the LLM to elicit consistent and accurate
    responses. For instance, asking an LLM'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示工程**：这涉及精心设计和迭代优化提供给大型语言模型的指令或查询，以产生一致和准确的响应。例如，向一个大型语言模型提问'
- en: '[PRE0]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'provides more structure and precision compared to the broad query like:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于像“**提示工程**”这样的宽泛查询，它提供了更多的结构和精确性：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The former query specifies the expected output, leading the model to focus
    on a concise and relevant list of benefits, while the latter might yield a verbose
    or tangential response. Prompt engineering helps guide the model to stay within
    the desired scope of information and reduces the chances of it producing irrelevant
    or fabricated outputs. For a detailed exploration of prompt engineering techniques
    and best practices, check out this guide: [https://cloud.google.com/discover/what-is-prompt-engineering](https://cloud.google.com/discover/what-is-prompt-engineering).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 前者查询指定了预期的输出，引导模型关注一个简洁且相关的利益列表，而后者可能产生冗长或离题的回答。提示工程有助于引导模型保持在所需的信息范围内，并减少其产生不相关或虚构输出的可能性。有关提示工程技术和最佳实践的详细探讨，请参阅此指南：[https://cloud.google.com/discover/what-is-prompt-engineering](https://cloud.google.com/discover/what-is-prompt-engineering)。
- en: '**In-context learning** (**few-shot prompting**): In this method, examples
    are included within the prompt to guide the LLM toward accurate, task-specific
    responses. For instance, when asking for a product comparison, providing a few
    examples of properly structured comparisons within the prompt helps the model
    mimic the pattern. This approach leverages the model’s capability to infer context
    and adjust its responses based on the given examples, making it effective for
    domain-specific tasks.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境学习**（**少样本提示**）：在此方法中，示例包含在提示中，以引导大型语言模型向准确、特定任务的响应。例如，当要求产品比较时，在提示中提供几个结构良好的比较示例有助于模型模仿该模式。这种方法利用了模型推断情境并根据给定示例调整其响应的能力，使其在特定领域任务中非常有效。'
- en: '**Fine-tuning**: This involves training an already pre-trained LLM further
    on a specific dataset to adapt it to specialized domains or tasks. This process
    enhances the model’s ability to generate domain-specific, relevant, and accurate
    responses. One popular method for fine-tuning is **Reinforcement Learning with
    Human Feedback** (**RLHF**), where human evaluators guide the model by scoring
    its outputs. These scores are used to adjust the model’s behavior, aligning it
    with human expectations. For example, fine-tuning an LLM on a company’s internal
    documentation ensures it produces accurate and relevant outputs tailored to the
    organization’s specific needs. If prompted with'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：这涉及在特定数据集上进一步训练已经预训练的LLM，以适应特定领域或任务。这个过程增强了模型生成特定领域、相关和准确响应的能力。微调的一种流行方法是**强化学习与人反馈（RLHF**），其中人类评估者通过评分模型的输出来引导模型。这些评分用于调整模型的行为，使其与人类期望保持一致。例如，在公司的内部文档上微调LLM可以确保它产生准确且相关的输出，以满足组织的特定需求。如果提示：'
- en: '[PRE2]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: a fine-tuned model might provide a detailed explanation consistent with the
    company’s policies, whereas a general model might offer a vague or unrelated response.
    Let us take another example scenario to understand how using RLHF, you can improve
    responses.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个微调后的模型可能会提供一个与公司政策一致详细解释，而一个通用模型可能会提供模糊或不相关的响应。让我们再举一个例子场景，以了解如何使用RLHF来改进响应。
- en: 'Suppose the LLM was initially asked:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 假设最初LLM被询问：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The response might include generic benefits that do not align with the software’s
    unique features. With RLHF, human evaluators score the response based on accuracy,
    relevance, and completeness. For instance, the initial response could be:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 响应可能包括与软件独特功能不匹配的通用好处。使用RLHF，人类评估者根据准确性、相关性和完整性评分响应。例如，初始响应可能是：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The feedback may be:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈可能包括：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After fine-tuning with human feedback, the result could be a more accurate
    and tailored response, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过人类反馈的微调后，结果可能是一个更准确和定制的响应，如下所示：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: RLHF is especially valuable in reducing hallucinations because it emphasizes
    learning from human-curated feedback.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF在减少幻觉方面特别有价值，因为它强调从人类编辑的反馈中学习。
- en: 'While these methods provide significant improvements, they still fall short
    in one critical area: enabling organizations to use domain-specific knowledge
    to rapidly build accurate, contextual, and explainable GenAI applications. The
    solution lies in **grounding** – a concept that ties the model’s responses to
    real-world facts or data. This approach forms the foundation of a new paradigm
    in text generation called RAG. By dynamically retrieving factual information from
    reliable knowledge sources, RAG ensures outputs are both accurate and contextually
    aligned. RAG tries to address LLM hallucinations by incorporating relevant information
    from factual knowledge repositories.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法提供了显著的改进，但它们在关键领域仍有所不足：使组织能够利用特定领域的知识快速构建准确、上下文相关且可解释的通用人工智能应用。解决方案在于**扎根**——一个将模型的响应与真实世界的事实或数据联系起来的概念。这种方法构成了文本生成新范式的基础，称为RAG（Retrieval-Augmented
    Generation，检索增强生成）。通过从可靠的知识源动态检索事实信息，RAG确保输出既准确又与上下文一致。RAG通过结合来自事实知识库的相关信息来尝试解决大型语言模型（LLM）的幻觉问题。
- en: 'The term retrieval-augmented generation, or RAG for short, was first introduced
    by researchers at **Facebook AI Research** (**FAIR**) in a paper titled *Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks*: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401),
    submitted in May 2020.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 术语检索增强生成（Retrieval-Augmented Generation），简称RAG，首次由**Facebook AI Research（FAIR**）的研究人员在2020年5月提交的一篇题为《Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks》的论文中提出：[https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)。
- en: 'The paper proposed RAG as a hybrid architecture (refer to *Figure 2.1*) that
    combines a neural retriever with a sequence-to-sequence generator. The **retriever**
    fetches relevant documents from an external knowledge base, which are then used
    as context for the generator to produce outputs grounded in factual data. This
    approach was shown to significantly improve performance on knowledge-intensive
    NLP tasks, such as open-domain question-answering and dialogue systems, by reducing
    the reliance on the model’s internal knowledge and enhancing factual accuracy.
    RAG addresses the previously mentioned shortcomings of LLMs by introducing a critical
    element: the ability to retrieve relevant knowledge from supplementary or domain-specific
    data sources.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提出了RAG（检索增强生成）作为一种混合架构（参见图2.1），它结合了一个神经检索器和一个序列到序列生成器。**检索器**从外部知识库中检索相关文档，然后这些文档被用作生成器的上下文，以产生基于事实数据的输出。这种方法已被证明可以显著提高知识密集型NLP任务（如开放域问答和对话系统）的性能，通过减少对模型内部知识的依赖并提高事实准确性。RAG通过引入一个关键元素来解决之前提到的LLM的不足：从补充或特定领域的数据源中检索相关知识的能力。
- en: '![Figure 2.1 — RAG architecture proposed in the Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks research paper by FAIR](img/B31107_02_1.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 — FAIR在“检索增强生成用于知识密集型NLP任务”研究论文中提出的RAG架构](img/B31107_02_1.png)'
- en: Figure 2.1 — RAG architecture proposed in the Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks research paper by FAIR
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 — FAIR在“检索增强生成用于知识密集型NLP任务”研究论文中提出的RAG架构
- en: Additionally, RAG pipelines offer the potential to reduce model size while maintaining
    accuracy. Instead of embedding all knowledge within the model’s parameters—which
    would require extensive resources—RAG allows the model to retrieve information
    dynamically, keeping it lightweight and scalable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RAG管道提供了在保持准确性的同时减少模型大小的潜力。而不是将所有知识嵌入到模型的参数中——这将需要大量资源——RAG允许模型动态检索信息，保持其轻量级和可扩展性。
- en: The next section in this chapter will delve deeper into the inner workings of
    RAG, exploring how it bridges the gap between raw generation and knowledge-grounded
    text production.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本章下一节将深入探讨RAG的内部工作原理，探讨它是如何弥合原始生成和基于知识文本生产之间的差距。
- en: Deconstructing the RAG flow
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解构RAG流程
- en: Let us now deconstruct the building blocks of a RAG model and help you understand
    how it functions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在解构RAG模型的构建块，并帮助您了解它是如何工作的。
- en: First, we will take a look at the regular LLM application flow. *Figure 2.2*
    illustrates this basic flow.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看常规LLM应用流程。*图2.2*展示了这个基本流程。
- en: '![Figure 2.2 — The basic flow of information in a chat application with an
    LLM](img/B31107_02_2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 — 带有LLM的聊天应用中的信息基本流程](img/B31107_02_2.png)'
- en: Figure 2.2 — The basic flow of information in a chat application with an LLM
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 — 带有LLM的聊天应用中的信息基本流程
- en: Here is what happens when a user prompts an LLM
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户向LLM（大型语言模型）提出请求时，会发生什么情况。
- en: '**User sends a prompt**: The process begins with a user sending a prompt to
    an LLM chat API. This prompt could be a question, an instruction, or any other
    request for information or content generation.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**用户发送提示**：这个过程从用户向LLM聊天API发送提示开始。这个提示可能是一个问题、一个指令或任何其他请求信息或内容生成的请求。'
- en: '**LLM API processes the prompt**: The LLM chat API receives the user’s prompt
    and transmits it to an LLM. LLMs are AI models trained on massive amounts of text
    data, allowing them to communicate and generate human-like text in response to
    a wide range of prompts and questions.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM API处理提示**：LLM聊天API接收用户的提示并将其传输给LLM。LLM是经过大量文本数据训练的AI模型，允许它们对广泛的提示和问题进行沟通并生成类似人类的文本。'
- en: '**LLM generates a response**: The LLM then processes the prompt and formulates
    a response. This response is sent back to the LLM chat API, which then transmits
    it to the user.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LLM生成响应**：然后LLM处理提示并制定一个响应。这个响应被发送回LLM聊天API，然后将其传输给用户。'
- en: From this flow, we can see that the LLM is responsible for providing the answer
    and there is no other process in between. This is the most common usage without
    RAG in the request-response flow.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个流程中，我们可以看到LLM负责提供答案，中间没有其他过程。这是没有RAG的请求-响应流程中最常见的用法。
- en: Now let us take a look at where RAG fits into this workflow.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看RAG在这个工作流程中是如何定位的。
- en: '![Figure 2.3 — The flow of information in a chat application with the RAG model](img/B31107_02_3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 — 带有RAG模型的聊天应用中的信息流程](img/B31107_02_3.png)'
- en: Figure 2.3 — The flow of information in a chat application with the RAG model
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 — 带有RAG模型的聊天应用中的信息流
- en: 'We can see from *Figure 2.3* that we have an intermediate data source before
    the actual LLM service invocation that can provide the context for the LLM request:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从*图2.3*中看到，在调用实际的LLM服务之前，我们有一个中间数据源，它可以提供LLM请求的上下文：
- en: '**User sends prompt**: The process starts with the user sending a prompt or
    question through a chat interface. This prompt could be anything the user wants
    information about or needs help with.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**用户发送提示**：该过程从用户通过聊天界面发送提示或问题开始。这个提示可能是用户想要了解的任何信息或需要帮助的内容。'
- en: '**RAG model processes prompt**: The prompt is received by a chat API, which
    then relays it to the RAG model. The RAG model has two main components working
    together: the *retriever* (discussed in Step *3*) and the *encoder-decoder* (discussed
    in Step *4*).'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**RAG模型处理提示**：提示被聊天API接收，然后将其转发给RAG模型。RAG模型有两个主要组件协同工作：*检索器*（在第*3*步中讨论）和*编码器-解码器*（在第*4*步中讨论）。'
- en: '**Retriever**: This component searches through a knowledge repository, which
    may include unstructured documents, passages, or structured data such as tables
    or knowledge graphs. Its role is to locate the most relevant information needed
    to address the user’s prompt.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索器**：该组件在知识库中搜索，可能包括非结构化文档、段落或如表格或知识图谱之类的结构化数据。其作用是定位处理用户提示所需的最相关信息。'
- en: We will cover a simple example of the retriever component. You can review the
    full code available at [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/dpr.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/dpr.py)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖检索器组件的一个简单示例。你可以在[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/dpr.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/dpr.py)查看完整的代码。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let us define a set of documents that we want to store in a document store.
    We are using a few predefined sentences here for demonstration purposes:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一组我们想要存储在文档存储中的文档。这里我们使用一些预定义的句子来演示：
- en: '[PRE8]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will store the content defined previously in a content store. Then
    we will generate an embedding for each of the documents and store them in the
    content store:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将之前定义的内容存储在内容存储中。然后我们将为每个文档生成一个嵌入并将它们存储在内容存储中：
- en: '[PRE9]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let us define a method that retrieves the content from the document store
    based on query input. We will generate an embedding of the request and query the
    content store to retrieve the relevant result. We are leveraging vector search
    here to get the relevant results:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一种方法，根据查询输入从文档存储中检索内容。我们将生成请求的嵌入并查询内容存储以检索相关结果。我们在这里利用向量搜索来获取相关结果：
- en: '[PRE10]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see for a given query what kind of output we would receive as an example.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到对于给定的查询，我们会收到什么样的输出作为示例。
- en: 'The following is sample input:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输入：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And here is the sample output:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是示例输出：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Note**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Retriever implementations can be quite complex. They could involve
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器实现可能相当复杂。它们可能涉及
- en: using efficient search algorithms such as BM25, TF-IDF, or neural retrievers
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高效的搜索算法，如BM25、TF-IDF或神经检索器
- en: such as **Dense Passage Retrieval**. You can read more about it at [https://github.com/facebookresearch/](https://github.com/facebookresearch/).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如**密集段落检索**。你可以在[https://github.com/facebookresearch/](https://github.com/facebookresearch/)了解更多信息。
- en: '**Encoder-decoder/augmented generation**: The encoder part of this component
    processes the prompt along with the retrieved information—whether structured or
    unstructured—to create a comprehensive representation. The decoder then uses this
    representation to generate a response that is accurate, contextually rich, and
    tailored to the user’s prompt.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器-解码器/增强生成**：该组件的编码器部分处理提示以及检索到的信息——无论是结构化还是非结构化——以创建一个全面的表示。然后解码器使用这个表示来生成一个准确、语境丰富且针对用户提示的响应。'
- en: 'This involves invoking the LLM API with the input query and context information.
    Let us take a look at an example of how this works. The following example shows
    how we can invoke a query with contextual information. This example showcases
    the use of T5Tokenizer model:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到使用输入查询和上下文信息调用LLM API。让我们看看一个示例，看看它是如何工作的。以下示例展示了如何使用上下文信息调用查询。这个示例展示了T5Tokenizer模型的使用：
- en: 'Let us define an LLM first. We will be using the T5 model from Hugging Face:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先定义一个LLM。我们将使用Hugging Face的T5模型：
- en: '[PRE13]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define the query and documents for the RAG flow. Normally, we leverage a retriever
    for the RAG flow. We are going to use hardcoded values for demonstration purposes
    here:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义RAG流程的查询和文档。通常，我们利用检索器进行RAG流程。在这里，为了演示目的，我们将使用硬编码的值：
- en: '[PRE14]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will define a method that takes the input query and the retrieved passages
    to use the LLM API to demonstrate the RAG approach:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义一个方法，它接受输入查询和检索到的段落，以使用LLM API来演示RAG方法：
- en: '[PRE15]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Note**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: We are employing the **T5 model**’s beam search decoding to produce an
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用**T5模型**的束搜索解码来生成
- en: accurate and contextually relevant response. **Beam search decoding** is a search
    algorithm used to find the most likely sequence of tokens (words) during text
    generation. Unlike greedy decoding, which selects the most
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 准确且与上下文相关的响应。**束搜索解码**是一种在文本生成过程中寻找最可能序列（单词）的搜索算法。与贪婪解码不同，贪婪解码选择最
- en: probable token at each step, beam search maintains multiple potential
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中，束搜索维护多个潜在的
- en: sequences (called **beams**) and explores them simultaneously. This
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 序列（称为**束**）并同时探索它们。此
- en: 'increases the chances of finding a high-quality result, as it avoids committing
    to suboptimal choices too early in the generation process. You can learn more
    about beam search in Transformers in this article: [https://huggingface.co/blog/constrained-beam-search](https://huggingface.co/blog/constrained-beam-search).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 增加了找到高质量结果的机会，因为它避免了在生成过程中过早地做出次优选择。你可以在本文中了解更多关于Transformers中束搜索的信息：[https://huggingface.co/blog/constrained-beam-search](https://huggingface.co/blog/constrained-beam-search)。
- en: Now, let us invoke this method and review the response.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们调用此方法并审查响应。
- en: '**Chat API delivers response**: The following code will invoke the `generate_response`
    method and deliver the chat response for the input query:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聊天API提供响应**：以下代码将调用`generate_response`方法，并为输入查询提供聊天响应：'
- en: '[PRE16]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When we run this example, the outcome is as follows.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个示例时，结果如下。
- en: 'The following is sample input:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输入：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The retrieved passages are as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 检索到的段落如下：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following is the sample output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输出：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can find the full code for this example at [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/augmented_generation.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/augmented_generation.py).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/augmented_generation.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/augmented_generation.py)找到这个示例的完整代码。
- en: '**Integration and fine-tuning**: Now let us look at a code snippet that combines
    the retriever and the LLM invocation as the full RAG flow. The following code
    demonstrates this:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**集成和微调**：现在让我们看看一个代码片段，它将检索器和LLM调用结合起来，作为完整的RAG流程。以下代码展示了这一点：'
- en: '[PRE20]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: From the code, we can see that the flow is simple. We retrieve the documents
    needed to leverage the RAG flow using the retriever and pass the input query and
    the retrieved documents to the LLM API invocation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中，我们可以看到流程很简单。我们使用检索器检索利用RAG流程所需的文档，并将输入查询和检索到的文档传递给LLM API调用。
- en: In this deep dive into the RAG architecture, we focused on its mechanics and
    demonstrated the functioning of its core components. By combining efficient information
    retrieval with advanced language generation models, RAG produces contextually
    appropriate and knowledge-enriched responses. As we transition to the next section,
    we will discuss the **retrieval process**.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次对RAG架构的深入研究中，我们关注了其机制，并展示了其核心组件的功能。通过结合高效的信息检索和高级语言生成模型，RAG产生了上下文相关且知识丰富的响应。随着我们过渡到下一节，我们将讨论**检索过程**。
- en: Retrieving external information for your RAG
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为你的RAG检索外部信息
- en: Understanding how RAG leverages external knowledge is crucial for appreciating
    its ability to generate factually accurate and informative responses. This section
    discusses various **retrieval techniques**, strategies for integrating retrieved
    information, and practical examples to illustrate these concepts.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 理解RAG如何利用外部知识对于欣赏其生成事实准确和富有信息性的响应的能力至关重要。本节讨论了各种**检索技术**、整合检索信息的策略，以及说明这些概念的实例。
- en: Understanding retrieval techniques and strategies
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解检索技术和策略
- en: 'The success of a RAG model hinges on its ability to retrieve relevant information
    from a vast external knowledge base using one of the commonly used retrieval techniques.
    These retrieval methods are essential for sourcing relevant information from large
    datasets. Common techniques include traditional methods such as BM25 and modern
    neural approaches such as DPR. Broadly speaking, these techniques can be classified
    into three categories: **vector similarity search**, **keyword matching**, and
    **passage retrieval**. We will discuss each of them in the following subsections.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: RAG模型的成功取决于其使用常用检索技术之一从庞大的外部知识库中检索相关信息的能力。这些检索方法对于从大型数据集中获取相关信息至关重要。常见的技术包括传统的BM25方法以及现代的DPR神经网络方法。总的来说，这些技术可以分为三类：**向量相似度搜索**、**关键词匹配**和**段落检索**。我们将在以下小节中讨论每个技术。
- en: Vector similarity search
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量相似度搜索
- en: 'The text or query you pass to the LLM is converted into a vector representation
    called an **embedding**. The vector similarity search compares the vector embeddings
    to retrieve the closest match. The idea is that related and similar text will
    have similar embeddings. This technique works as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您传递给LLM的文本或查询被转换成一个称为**嵌入**的向量表示。向量相似度搜索通过比较向量嵌入来检索最接近的匹配项。其基本思想是相关和相似文本将具有相似的嵌入。该技术的工作原理如下：
- en: 'Build an embedding of the input query. We tokenize the input query and generate
    the vector embedding representation of it:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建输入查询的嵌入。我们对输入查询进行分词，并生成其向量嵌入表示：
- en: '[PRE21]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Build the embeddings of the documents. We generate an embedding for each document
    using the tokenizer and associate each embedding with its document:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建文档的嵌入。我们使用分词器为每个文档生成一个嵌入，并将每个嵌入与其对应的文档关联：
- en: '[PRE22]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Find similar documents using dot product calculation. This step uses the input
    query embedding and searches the document embeddings for results similar to the
    input query:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用点积计算查找相似文档。此步骤使用输入查询嵌入并在文档嵌入中搜索与输入查询相似的文档：
- en: '[PRE23]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Sort the documents by the relevancy score and return the results. The results
    contain the matching documents along with a score representing how similar it
    is to the input query. We will order the results in the order we want, most similar
    to least similar:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按相关性分数对文档进行排序并返回结果。结果包含匹配的文档以及一个表示其与输入查询相似度的分数。我们将按照所需的顺序对结果进行排序，从最相似到最不相似：
- en: '[PRE24]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let us run this example to see what the results would look like.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这个示例，看看结果会是什么样子。
- en: 'The following is a sample input query:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输入查询：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the sample output (ranked documents):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为示例输出（按相关性排序的文档）：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The preceding code demonstrates how to use DPR to encode a query and a set of
    documents into high-dimensional vector representations. By computing similarity
    scores, such as the dot product between the query vector and document vectors,
    the model evaluates the relevance of each document to the query. The documents
    are then ranked based on their similarity scores, with the most relevant ones
    appearing at the top. This process highlights the power of vector-based retrieval
    in effectively identifying contextually relevant information from a diverse set
    of documents, even when they include a mix of related and unrelated content.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码演示了如何使用DPR将查询和一组文档编码成高维向量表示。通过计算相似度分数，例如查询向量与文档向量之间的点积，模型评估每个文档与查询的相关性。然后根据相似度分数对文档进行排序，最相关的文档将出现在顶部。这个过程突出了基于向量的检索在有效识别来自各种文档的上下文相关信息方面的强大功能，即使这些文档包含相关和不相关的混合内容。
- en: 'The full version of this example is available in the GitHub repository: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/vector_similarity_search.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/vector_similarity_search.py).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的完整版本可在GitHub仓库中找到：[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/vector_similarity_search.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/vector_similarity_search.py)。
- en: Keyword matching
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键词匹配
- en: '**Keyword matching** is a simpler approach that identifies documents containing
    keywords present in the user prompt. While efficient, it can be susceptible to
    noise and misses documents with relevant synonyms. BM25 is a keyword-based probabilistic
    retrieval function that scores documents based on the query terms appearing in
    each document, considering term frequency and document length. The flow of this
    approach looks as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词匹配**是一种更简单的方法，它识别包含用户提示中关键词的文档。虽然效率高，但可能容易受到噪声的影响，并错过包含相关同义词的文档。BM25 是一种基于关键词的概率检索函数，它根据每个文档中出现的查询词对文档进行评分，考虑词频和文档长度。这种方法的基本流程如下：'
- en: 'Build the BM25 corpus using the documents. We will tokenize documents and build
    a corpus from this. We will build the BM25 corpus:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文档构建 BM25 语料库。我们将对文档进行分词并从这些文档中构建语料库。我们将构建 BM25 语料库：
- en: '[PRE27]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Tokenize the query to search using it:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将查询分词以使用它进行搜索：
- en: '[PRE28]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Query the BM25 corpus using the tokenized query. This returns the scores for
    matching documents:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分词查询查询 BM25 语料库。这将返回匹配文档的分数：
- en: '[PRE29]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will take these scores, order the documents in the required order, and return
    them:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用这些分数，按所需顺序排列文档，并返回它们：
- en: '[PRE30]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When we run this example, the results would look like this for the given input.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此示例时，对于给定的输入，结果将如下所示。
- en: 'The following is a sample input query:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输入查询：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the sample output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输出：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The BM25 algorithm ranks documents based on their relevance to a query. It relies
    on the term frequency (how often a keyword appears in a document) and document
    length, applying a probabilistic scoring function to evaluate relevance. Unlike
    vector similarity search, which represents both queries and documents as dense
    numerical vectors in high-dimensional space and measures similarity using mathematical
    functions such as the dot product, BM25 operates directly on discrete word matches.
    This means BM25 is efficient and interpretable but can struggle with semantic
    relationships, as it cannot recognize synonyms or contextual meanings. In contrast,
    vector similarity search, such as DPR, excels in identifying conceptual similarities
    even when exact keywords differ, making it more suitable for tasks requiring deep
    semantic understanding. This snippet illustrates BM25’s utility for straightforward
    keyword-matching tasks where efficiency and explainability are critical.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: BM25 算法根据文档与查询的相关性对文档进行排名。它依赖于词频（关键词在文档中出现的频率）和文档长度，应用概率评分函数来评估相关性。与将查询和文档都表示为高维空间中密集数值向量的向量相似度搜索不同，它使用如点积等数学函数来衡量相似度，BM25
    直接在离散单词匹配上操作。这意味着 BM25 效率高且可解释，但可能在处理语义关系方面遇到困难，因为它无法识别同义词或上下文含义。相比之下，向量相似度搜索，如
    DPR，在识别即使精确关键词不同时也能识别概念相似性方面表现出色，这使得它更适合需要深度语义理解的任务。此代码片段说明了 BM25 在简单关键词匹配任务中的实用性，其中效率和可解释性至关重要。
- en: 'The complete example is available in the GitHub repository: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/keyword_matching.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/keyword_matching.py).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 完整示例可在 GitHub 仓库中找到：[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/keyword_matching.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/keyword_matching.py)。
- en: Passage retrieval
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 段落检索
- en: 'Instead of retrieving entire documents, RAG can focus on specific passages
    within documents that directly address the user’s query. This allows for more
    precise information extraction. The initial flow of this approach is very similar
    to the vector search approach. We get the ranked documents using the approach
    shown in vector search and then extract relevant passages as shown in the following
    code snippet:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与检索整个文档不同，RAG 可以专注于文档中直接针对用户查询的具体段落。这允许进行更精确的信息提取。这种方法的基本流程与向量搜索方法非常相似。我们使用向量搜索中显示的方法获取排名靠前的文档，然后如以下代码片段所示提取相关段落：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: When we run this example for the given input query, the results look as follows.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们针对给定的输入查询运行此示例时，结果如下所示。
- en: 'The following is a sample input query:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输入查询：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following is the sample output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输出：
- en: '[PRE35]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The preceding example illustrates the **passage-retrieval approach**, which
    is more granular than document-level retrieval, focusing on extracting specific
    passages that directly address the user’s query. By leveraging a *reader model*
    in combination with a *retriever*, this approach enhances relevance and specificity,
    as it identifies not only the most relevant document but also the exact passage
    within it that best answers the query.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例说明了**段落检索方法**，它比文档级检索更细粒度，专注于提取直接针对用户查询的特定段落。通过结合使用**读者模型**和**检索器**，这种方法增强了相关性和特异性，因为它不仅确定了最相关的文档，还确定了其中最佳回答查询的确切段落。
- en: 'Even if a passage has a slightly lower retriever score, the reader may prioritize
    it because it evaluates relevance more precisely at the word and span levels,
    considering contextual nuances. The retriever typically calculates a similarity
    score using the dot product of the query and passage embeddings:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 即使一个段落的检索器分数略低，读者也可能优先考虑它，因为它在词和跨度级别上更精确地评估相关性，考虑了上下文细微差别。检索器通常使用查询和段落嵌入的点积来计算相似度分数：
- en: '![](img/B21107_02_001.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21107_02_001.png)'
- en: Here, 𝑞 is the query embedding, ![](img/B21107_02_002.png) is the passage embedding
    for the ![](img/B21107_02_003.png) passage, and 𝑑 is the dimensionality of the
    embeddings.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，𝑞是查询嵌入，![](img/B21107_02_002.png)是![](img/B21107_02_003.png)段落的嵌入，𝑑是嵌入的维度。
- en: 'The reader, however, refines this further by analyzing the text content of
    each passage. It assigns a **relevance score** or **logit** (also known as **confidence
    score**) based on the likelihood that a given passage contains the answer. This
    relevance score is computed from the raw outputs (logits) of the reader model,
    which considers word-level and span-level interactions between the query and the
    passage. The formula for the relevance score can be expressed as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，读者通过分析每个段落的文本内容进一步细化这一过程。它根据给定段落包含答案的可能性分配一个**相关性分数**或**logit**（也称为**置信度分数**）。这个相关性分数是从读者模型的原始输出（logits）中计算出来的，该模型考虑了查询与段落之间的词级和跨度级交互。相关性分数的公式可以表示如下：
- en: '![](img/B21107_02_004.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21107_02_004.png)'
- en: 'Here, we have the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: logits(![](img/B21107_02_002.png)) refers to the raw scores assigned to the
    passage, ![](img/B21107_02_002.png), by the reader
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: logits(![](img/B21107_02_002.png))指的是读者分配给段落![](img/B21107_02_002.png)的原始分数。
- en: softmax converts these raw scores into probabilities, emphasizing the passage
    most likely to be relevant ([https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html))
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: softmax将这些原始分数转换为概率，强调最有可能相关的段落([https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html))
- en: By combining both stages, the system can identify passages that are not only
    semantically similar (*retriever stage*) but also contextually aligned with the
    query’s intent (*reader stage*).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合两个阶段，系统可以识别出不仅语义相似（检索器阶段），而且与查询意图上下文对齐的段落（读者阶段）。
- en: This dual-stage process highlights the strength of passage retrieval in generating
    highly targeted responses in information retrieval pipelines.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个双阶段过程突出了段落检索在信息检索管道中生成高度针对性的响应的优势。
- en: 'The complete example is available in the GitHub repository: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/passage_retrieval.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/passage_retrieval.py).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 完整示例可在GitHub仓库中找到：[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/passage_retrieval.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/passage_retrieval.py)。
- en: Integrating the retrieved information
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成检索信息
- en: 'For the last step in the RAG flow, let us look at how we can combine the retriever
    information with the generation model in a way that synthesizes contextually relevant
    and coherent responses. Unlike earlier examples, this approach explicitly integrates
    multiple retrieved passages with the query. By doing so, it creates a single input
    for the generation model. This allows the model to synthesize a unified and enriched
    response that goes beyond merely selecting or ranking passages:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG流程的最后一步，让我们看看我们如何以综合上下文相关且连贯的响应的方式将检索器信息与生成模型相结合。与早期示例不同，这种方法明确地将多个检索到的段落与查询相结合。通过这样做，它为生成模型创建了一个单一输入。这使得模型能够综合出一个统一且丰富的响应，而不仅仅是选择或排序段落：
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is a sample input query:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输入查询：
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following is the sample output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为示例输出：
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/B21107_02_007.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21107_02_007.png)'
- en: Here, ![](img/B21107_02_008.png) is the token at position ![](img/B21107_02_009.png),
    ![](img/B21107_02_010.png) is the hidden state, and ![](img/B21107_02_011.png)
    is the model’s weight matrix. Beam search ensures the selection of the most likely
    sequence by maximizing the overall probability across tokens. Unlike previous
    examples where individual passages were selected or ranked, this code explicitly
    combines multiple retrieved documents into a single input alongside the query.
    This enables the T5 model to process the combined context holistically and produce
    a coherent response that incorporates information from multiple sources, making
    it particularly effective for queries requiring synthesis or summarization across
    multiple passages.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B21107_02_008.png) 是位置 ![](img/B21107_02_009.png) 的标记，![](img/B21107_02_010.png)
    是隐藏状态，而 ![](img/B21107_02_011.png) 是模型的权重矩阵。Beam 搜索通过最大化跨标记的整体概率来确保选择最可能的序列。与前面示例中单独选择或排序段落不同，此代码明确地将多个检索到的文档与查询组合成一个单一输入。这使得
    T5 模型能够全面处理组合上下文，并产生一个包含来自多个来源信息的连贯响应，这使得它在需要跨多个段落综合或总结查询时特别有效。
- en: 'To refer to the full version of this code, please refer: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/integrate_and_generate.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/integrate_and_generate.py)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要参考此代码的完整版本，请参阅：[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/integrate_and_generate.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/integrate_and_generate.py)
- en: By exploring various retrieval techniques and their integration with generation
    models, we have seen how RAG architectures leverage external knowledge to produce
    accurate and informative responses.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探索各种检索技术和它们与生成模型的集成，我们看到了 RAG 架构如何利用外部知识来产生准确和有信息量的响应。
- en: In the next section, let us look at the holistic flow from reading input documents
    from a source and leveraging those documents for a retriever flow, instead of
    the simple hardcoded sentences we looked at in the examples in this section.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们看看从源读取输入文档并利用这些文档进行检索流程的整体流程，而不是本节示例中查看的简单硬编码句子。
- en: Building an end-to-end RAG flow
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建端到端 RAG 流程
- en: In the previous sections, we delved into the various steps in the RAG flow individually
    with simple data to demonstrate the usage. It would be a good idea to take a step
    back and use a real-world dataset, albeit a simple one, to complete the whole
    flow. For this, we will use the GitHub issues dataset ([https://huggingface.co/datasets/lewtun/github-issues](https://huggingface.co/datasets/lewtun/github-issues)).
    We will look at how we can read this data and use it in the RAG flow. This would
    lay the foundation for the full end-to-end RAG flow implementation in later chapters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们通过简单的数据深入探讨了 RAG 流程中的各个步骤以展示用法。退一步使用一个真实世界的数据集（尽管很简单）来完成整个流程是个不错的主意。为此，我们将使用
    GitHub 的问题数据集（[https://huggingface.co/datasets/lewtun/github-issues](https://huggingface.co/datasets/lewtun/github-issues)）。我们将探讨如何读取这些数据并在
    RAG 流程中使用它们。这将为后续章节中完整端到端 RAG 流程的实现奠定基础。
- en: 'In this example, we will load GitHub comments to be able to answer questions,
    such as how we can load data offline. We need to follow these steps to load the
    data and set up the retriever:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将加载 GitHub 注释以回答诸如如何离线加载数据等问题。我们需要遵循以下步骤来加载数据并设置检索器：
- en: '**Preparing the data**: First, we need to prepare our dataset. We will use
    the Hugging Face `datasets` library:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备数据**：首先，我们需要准备我们的数据集。我们将使用 Hugging Face `datasets` 库：'
- en: '[PRE39]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Select the relevant columns**: Keep only the columns required for analysis:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择相关列**：仅保留分析所需列：'
- en: '[PRE40]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '**Convert the dataset into a pandas DataFrame**: Convert the dataset into a
    `pandas` DataFrame for easier manipulation:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将数据集转换为 pandas DataFrame**：将数据集转换为 `pandas` DataFrame 以便于操作：'
- en: '[PRE41]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '**Explode comments, convert them back into a dataset, and process**: Flatten
    the comments into individual rows, convert the DataFrame back into a dataset,
    and compute the length of each comment. This step makes this data amenable to
    use with the retriever flow:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**爆炸注释，将它们转换回数据集，并处理**：将注释展开成单独的行，将 DataFrame 转换回数据集，并计算每条注释的长度。这一步使得数据更适合与检索流程一起使用：'
- en: '[PRE42]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**Concatenate text for embeddings**: Let us prepare the document text by concatenating
    the relevant text fields. We will take individual fields from each row and prepare
    the text that represents the document text for that row. These documents are stored
    in an embedding store for retriever usage purposes:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**拼接文本以生成嵌入**：让我们通过拼接相关文本字段来准备文档文本。我们将从每一行中提取单个字段，并准备代表该行文档文本的文本。这些文档存储在嵌入存储中，用于检索器使用：'
- en: '[PRE43]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '**Load model and tokenizer**: Let us load the LLM that we will use to convert
    the documents into the embeddings and store them in an embedding store for the
    retriever flow:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载模型和分词器**：让我们加载LLM，我们将使用它将文档转换为嵌入并将它们存储在嵌入存储中以用于检索器流程：'
- en: '[PRE44]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Define the embedding function**: Define the embedding function that leverages
    the model we defined previously to generate the embedding. We can invoke this
    method iteratively to generate the embeddings for all the documents we have, one
    document at a time:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义嵌入函数**：定义一个嵌入函数，该函数利用我们之前定义的模型来生成嵌入。我们可以迭代地调用此方法，一次生成一个文档的所有嵌入：'
- en: '[PRE45]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Compute embeddings**: Compute embeddings for the dataset. Now that we have
    the embedding function defined, let us call it for all the documents we have in
    our comments dataset. Note that we are storing the embedding in a new column named
    `embedding` in the same dataset:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算嵌入**：为数据集计算嵌入。现在我们已经定义了嵌入函数，让我们为我们的评论数据集中的所有文档调用它。请注意，我们正在将嵌入存储在同一数据集的新列`embedding`中：'
- en: '[PRE46]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Perform semantic search**: Let us perform the retriever flow for the question.
    This will retrieve all the questions related to the question we have asked. We
    can use these documents to refine the response as needed:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行语义搜索**：让我们为问题执行检索器流程。这将检索与我们所提问题相关的所有问题。我们可以使用这些文档根据需要改进响应：'
- en: '[PRE47]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The preceding code showcases the complete flow, from how we load the data into
    a data store, which can form the basis for the retriever, to retrieving documents,
    which can be used to provide more context for the LLM when it is generating the
    answer.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码展示了完整的流程，从我们将数据加载到数据存储中，这可以成为检索器的基础，到检索文档，这些文档可以用于在LLM生成答案时提供更多上下文。
- en: 'Now let us see how the output looks when we run this application. We have the
    question hardcoded in the example code, and it is:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看运行此应用程序时的输出看起来如何。示例代码中硬编码了问题，它是：
- en: '[PRE48]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following is the sample output:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例输出：
- en: '[PRE49]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This hands-on example demonstrated the practical application of an end-to-end
    RAG architecture, leveraging powerful retrieval techniques to enhance language
    generation. The preceding code is adapted from the Hugging Face NLP course, available
    at [https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf](https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这个动手实验展示了端到端RAG架构的实际应用，利用强大的检索技术来增强语言生成。前面的代码是从Hugging Face NLP课程中改编的，可在[https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf](https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf)找到。
- en: The complete Python file, along with a detailed explanation of how to run it,
    is available at [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/full_rag_pipeline.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/full_rag_pipeline.py).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的Python文件以及如何运行的详细说明可在[https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/full_rag_pipeline.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/full_rag_pipeline.py)找到。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we deep-dived into the world of RAG models. We started by understanding
    the core principles of RAG and how they differ from traditional generative AI
    models. This foundational knowledge is crucial as it sets the stage for appreciating
    the enhanced capabilities that RAG brings to the table.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了RAG模型的世界。我们首先理解了RAG的核心原则以及它们与传统生成式AI模型的不同之处。这种基础性知识至关重要，因为它为欣赏RAG带来的增强功能奠定了基础。
- en: Next, we took a closer look at the architecture of RAG models, deconstructing
    their components through detailed code examples. By examining the encoder, retriever,
    and decoder, you gained insights into the inner workings of these models and how
    they integrate retrieved information to produce more contextually relevant and
    coherent outputs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们更详细地研究了RAG模型的架构，通过详细的代码示例来分解其组件。通过检查编码器、检索器和解码器，你了解了这些模型的内部工作原理以及它们如何整合检索信息以产生更具有上下文相关性和连贯性的输出。
- en: We then explored how RAG harnesses the power of information retrieval. These
    techniques help RAG effectively leverage external knowledge sources to improve
    the quality of a generated text. This is particularly useful for applications
    requiring high accuracy and context awareness. You also learned how to a simple
    RAG model using popular libraries such as Transformers and Hugging Face.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后探讨了RAG如何利用信息检索的力量。这些技术帮助RAG有效地利用外部知识源来提高生成文本的质量。这对于需要高精度和上下文感知的应用尤其有用。你还学习了如何使用像Transformers和Hugging
    Face这样的流行库构建一个简单的RAG模型。
- en: As we move forward to the next chapter, [*Chapter 3*](Preface.xhtml#_idTextAnchor012),
    we will build on this foundation. You will learn about graph data modeling and
    how to create knowledge graphs with Neo4j.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们迈向下一章节[*第3章*](Preface.xhtml#_idTextAnchor012)，我们将在此基础上继续前进。你将了解图数据建模以及如何使用Neo4j创建知识图谱。
