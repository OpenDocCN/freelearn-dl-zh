- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Demystifying RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored the evolution of LLMs and how they have
    changed the GenAI landscape. We also discussed some of their pitfalls. We will
    explore how we can avoid these pitfalls using **Retrieval-Augmented Generation**
    (**RAG**) in this chapter. We will take a look at what RAG means, what its architecture
    is, and how it fits into the LLM workflow in building improved intelligent applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the power of RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deconstructing the RAG flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving external information for your RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an end-to-end RAG flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires familiarity with the Python programming language (version
    `3.6` or higher is recommended) and basic concepts of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: We will be leveraging popular AI toolkits such as Hugging Face‚Äôs Transformers
    library ([https://huggingface.co/docs/transformers/en/index](https://huggingface.co/docs/transformers/en/index))
    to build and experiment with RAG. While not mandatory, having a basic understanding
    of Git version control can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Git allows you to easily clone the code repository for this chapter and track
    any changes you make. Do not worry about finding or typing the code yourself!
    We have created a dedicated public repository on GitHub, [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2),
    allowing you to easily clone it and follow along with the chapter‚Äôs hands-on exercises.
  prefs: []
  type: TYPE_NORMAL
- en: This repository contains all the necessary scripts, files, and configurations
    required to implement the RAG model and integrate Neo4j with advanced knowledge
    graph capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along, make sure you have the following Python libraries installed
    in your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers**: Install the Hugging Face Transformers library for handling
    model-related functionalities: `pip install transformers`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyTorch**: Install PyTorch as the backend for computation. Follow the instructions
    at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    to install the appropriate version for your system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn**: For similarity calculations, install `scikit-learn` using
    the `pip install scikit-learn` command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NumPy**: Install NumPy for numerical operations: `pip install numpy`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SentencePiece**: SentencePiece is required for text tokenization with certain
    models. You can install it using the instructions provided in the official GitHub
    repository: [https://github.com/google/sentencepiece#installation](https://github.com/google/sentencepiece#installation).
    For most Python environments, install it via `pip`: `pip install sentencepiece`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank_bm25**: The `rank_bm25` library is required to implement the BM25 algorithm
    for keyword-based retrieval. You can install it using `pip`: `pip install rank_bm25`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**datasets**: The `datasets` library from Hugging Face provides efficient tools
    for loading, processing, and transforming datasets. It supports large-scale datasets
    with minimal memory usage. You can install it using `pip install datasets`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pandas**: `pandas` is a powerful data analysis library in Python, used for
    manipulating tabular data. In this example, it helps preprocess the dataset by
    converting it into a DataFrame for easier manipulation. Install it using `pip
    install pandas`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**faiss-CPU**: `faiss-cpu` is a library for efficient similarity search and
    clustering of dense vectors. It is used in this example for building a retriever
    that fetches relevant passages during inference. Visit the Faiss GitHub repository
    ([https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss))
    for documentation and examples. Install it using `pip`: `pip install faiss-cpu`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accelerate**: Accelerate is a library by Hugging Face that simplifies distributed
    training and inference. It ensures optimal hardware utilization across CPUs, GPUs,
    and multi-node setups. Install it using `pip install accelerate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By ensuring your environment is configured with these tools, you can seamlessly
    explore the hands-on exercises provided in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: All the sections in this chapter focus on the relevant code snippets. For the
  prefs: []
  type: TYPE_NORMAL
- en: 'complete code, please refer to the book‚Äôs GitHub repository: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch2).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the power of RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG was introduced by Meta researchers in 2020 ([https://arxiv.org/abs/2005.11401v4](https://arxiv.org/abs/2005.11401v4))
    as a framework that allows GenAI models to leverage external data that is not
    part of model training to enhance the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a widely known fact that LLMs suffer from hallucinations. One of the
    classic real-world examples of LLMs hallucinating is the case of Levidow, Levidow
    & Oberman, the New York law firm that was fined for submitting a legal brief containing
    fake citations generated by OpenAI‚Äôs ChatGPT in a case against Colombian airline
    Avianca. They were subsequently fined thousands of dollars, and they are likely
    to have lost more in reputational damage. You can read more about it here: [https://news.sky.com/story/lawyers-fined-after-citing-bogus-cases-from-chatgpt-research-12908318](https://news.sky.com/story/lawyers-fined-after-citing-bogus-cases-from-chatgpt-research-12908318).'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM hallucinations can arise from several factors, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting to training data**: During training, the LLM might overfit to
    statistical patterns in the training data. This can lead the model to prioritize
    replicating those patterns over generating factually accurate content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of causal reasoning**: LLMs excel at identifying statistical relationships
    between words but may struggle to understand cause-and-effect relationships. This
    can lead to outputs that are grammatically correct but factually implausible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature configuration**: LLMs can be configured with a parameter called
    **temperature**, a number between `0` and `1` that controls the randomness in
    text generation. Higher temperatures increase creativity but also the likelihood
    of hallucinations as the model deviates from expected responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing information**: If the information required to generate an accurate
    response is not included in the training data, the model might generate plausible
    sounding but incorrect answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flawed or biased training data**: The quality of the training process plays
    a significant role. If the dataset contains biases or inaccuracies, the model
    may perpetuate these issues, leading to hallucinations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While hallucinations are a significant challenge, several methods can help
    mitigate them to some extent:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt engineering**: This involves carefully crafting and iteratively refining
    the instructions or queries given to the LLM to elicit consistent and accurate
    responses. For instance, asking an LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'provides more structure and precision compared to the broad query like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The former query specifies the expected output, leading the model to focus
    on a concise and relevant list of benefits, while the latter might yield a verbose
    or tangential response. Prompt engineering helps guide the model to stay within
    the desired scope of information and reduces the chances of it producing irrelevant
    or fabricated outputs. For a detailed exploration of prompt engineering techniques
    and best practices, check out this guide: [https://cloud.google.com/discover/what-is-prompt-engineering](https://cloud.google.com/discover/what-is-prompt-engineering).'
  prefs: []
  type: TYPE_NORMAL
- en: '**In-context learning** (**few-shot prompting**): In this method, examples
    are included within the prompt to guide the LLM toward accurate, task-specific
    responses. For instance, when asking for a product comparison, providing a few
    examples of properly structured comparisons within the prompt helps the model
    mimic the pattern. This approach leverages the model‚Äôs capability to infer context
    and adjust its responses based on the given examples, making it effective for
    domain-specific tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning**: This involves training an already pre-trained LLM further
    on a specific dataset to adapt it to specialized domains or tasks. This process
    enhances the model‚Äôs ability to generate domain-specific, relevant, and accurate
    responses. One popular method for fine-tuning is **Reinforcement Learning with
    Human Feedback** (**RLHF**), where human evaluators guide the model by scoring
    its outputs. These scores are used to adjust the model‚Äôs behavior, aligning it
    with human expectations. For example, fine-tuning an LLM on a company‚Äôs internal
    documentation ensures it produces accurate and relevant outputs tailored to the
    organization‚Äôs specific needs. If prompted with'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: a fine-tuned model might provide a detailed explanation consistent with the
    company‚Äôs policies, whereas a general model might offer a vague or unrelated response.
    Let us take another example scenario to understand how using RLHF, you can improve
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the LLM was initially asked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The response might include generic benefits that do not align with the software‚Äôs
    unique features. With RLHF, human evaluators score the response based on accuracy,
    relevance, and completeness. For instance, the initial response could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The feedback may be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After fine-tuning with human feedback, the result could be a more accurate
    and tailored response, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: RLHF is especially valuable in reducing hallucinations because it emphasizes
    learning from human-curated feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 'While these methods provide significant improvements, they still fall short
    in one critical area: enabling organizations to use domain-specific knowledge
    to rapidly build accurate, contextual, and explainable GenAI applications. The
    solution lies in **grounding** ‚Äì a concept that ties the model‚Äôs responses to
    real-world facts or data. This approach forms the foundation of a new paradigm
    in text generation called RAG. By dynamically retrieving factual information from
    reliable knowledge sources, RAG ensures outputs are both accurate and contextually
    aligned. RAG tries to address LLM hallucinations by incorporating relevant information
    from factual knowledge repositories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The term retrieval-augmented generation, or RAG for short, was first introduced
    by researchers at **Facebook AI Research** (**FAIR**) in a paper titled *Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks*: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401),
    submitted in May 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper proposed RAG as a hybrid architecture (refer to *Figure 2.1*) that
    combines a neural retriever with a sequence-to-sequence generator. The **retriever**
    fetches relevant documents from an external knowledge base, which are then used
    as context for the generator to produce outputs grounded in factual data. This
    approach was shown to significantly improve performance on knowledge-intensive
    NLP tasks, such as open-domain question-answering and dialogue systems, by reducing
    the reliance on the model‚Äôs internal knowledge and enhancing factual accuracy.
    RAG addresses the previously mentioned shortcomings of LLMs by introducing a critical
    element: the ability to retrieve relevant knowledge from supplementary or domain-specific
    data sources.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 ‚Äî RAG architecture proposed in the Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks research paper by FAIR](img/B31107_02_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 ‚Äî RAG architecture proposed in the Retrieval-Augmented Generation
    for Knowledge-Intensive NLP Tasks research paper by FAIR
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, RAG pipelines offer the potential to reduce model size while maintaining
    accuracy. Instead of embedding all knowledge within the model‚Äôs parameters‚Äîwhich
    would require extensive resources‚ÄîRAG allows the model to retrieve information
    dynamically, keeping it lightweight and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: The next section in this chapter will delve deeper into the inner workings of
    RAG, exploring how it bridges the gap between raw generation and knowledge-grounded
    text production.
  prefs: []
  type: TYPE_NORMAL
- en: Deconstructing the RAG flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us now deconstruct the building blocks of a RAG model and help you understand
    how it functions.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will take a look at the regular LLM application flow. *Figure 2.2*
    illustrates this basic flow.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 ‚Äî The basic flow of information in a chat application with an
    LLM](img/B31107_02_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 ‚Äî The basic flow of information in a chat application with an LLM
  prefs: []
  type: TYPE_NORMAL
- en: Here is what happens when a user prompts an LLM
  prefs: []
  type: TYPE_NORMAL
- en: '**User sends a prompt**: The process begins with a user sending a prompt to
    an LLM chat API. This prompt could be a question, an instruction, or any other
    request for information or content generation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**LLM API processes the prompt**: The LLM chat API receives the user‚Äôs prompt
    and transmits it to an LLM. LLMs are AI models trained on massive amounts of text
    data, allowing them to communicate and generate human-like text in response to
    a wide range of prompts and questions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**LLM generates a response**: The LLM then processes the prompt and formulates
    a response. This response is sent back to the LLM chat API, which then transmits
    it to the user.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From this flow, we can see that the LLM is responsible for providing the answer
    and there is no other process in between. This is the most common usage without
    RAG in the request-response flow.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us take a look at where RAG fits into this workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 ‚Äî The flow of information in a chat application with the RAG model](img/B31107_02_3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 ‚Äî The flow of information in a chat application with the RAG model
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from *Figure 2.3* that we have an intermediate data source before
    the actual LLM service invocation that can provide the context for the LLM request:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User sends prompt**: The process starts with the user sending a prompt or
    question through a chat interface. This prompt could be anything the user wants
    information about or needs help with.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**RAG model processes prompt**: The prompt is received by a chat API, which
    then relays it to the RAG model. The RAG model has two main components working
    together: the *retriever* (discussed in Step *3*) and the *encoder-decoder* (discussed
    in Step *4*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retriever**: This component searches through a knowledge repository, which
    may include unstructured documents, passages, or structured data such as tables
    or knowledge graphs. Its role is to locate the most relevant information needed
    to address the user‚Äôs prompt.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will cover a simple example of the retriever component. You can review the
    full code available at [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/dpr.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/dpr.py)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us define a set of documents that we want to store in a document store.
    We are using a few predefined sentences here for demonstration purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will store the content defined previously in a content store. Then
    we will generate an embedding for each of the documents and store them in the
    content store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let us define a method that retrieves the content from the document store
    based on query input. We will generate an embedding of the request and query the
    content store to retrieve the relevant result. We are leveraging vector search
    here to get the relevant results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see for a given query what kind of output we would receive as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is sample input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Retriever implementations can be quite complex. They could involve
  prefs: []
  type: TYPE_NORMAL
- en: using efficient search algorithms such as BM25, TF-IDF, or neural retrievers
  prefs: []
  type: TYPE_NORMAL
- en: such as **Dense Passage Retrieval**. You can read more about it at [https://github.com/facebookresearch/](https://github.com/facebookresearch/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder-decoder/augmented generation**: The encoder part of this component
    processes the prompt along with the retrieved information‚Äîwhether structured or
    unstructured‚Äîto create a comprehensive representation. The decoder then uses this
    representation to generate a response that is accurate, contextually rich, and
    tailored to the user‚Äôs prompt.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This involves invoking the LLM API with the input query and context information.
    Let us take a look at an example of how this works. The following example shows
    how we can invoke a query with contextual information. This example showcases
    the use of T5Tokenizer model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us define an LLM first. We will be using the T5 model from Hugging Face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the query and documents for the RAG flow. Normally, we leverage a retriever
    for the RAG flow. We are going to use hardcoded values for demonstration purposes
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will define a method that takes the input query and the retrieved passages
    to use the LLM API to demonstrate the RAG approach:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: We are employing the **T5 model**‚Äôs beam search decoding to produce an
  prefs: []
  type: TYPE_NORMAL
- en: accurate and contextually relevant response. **Beam search decoding** is a search
    algorithm used to find the most likely sequence of tokens (words) during text
    generation. Unlike greedy decoding, which selects the most
  prefs: []
  type: TYPE_NORMAL
- en: probable token at each step, beam search maintains multiple potential
  prefs: []
  type: TYPE_NORMAL
- en: sequences (called **beams**) and explores them simultaneously. This
  prefs: []
  type: TYPE_NORMAL
- en: 'increases the chances of finding a high-quality result, as it avoids committing
    to suboptimal choices too early in the generation process. You can learn more
    about beam search in Transformers in this article: [https://huggingface.co/blog/constrained-beam-search](https://huggingface.co/blog/constrained-beam-search).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us invoke this method and review the response.
  prefs: []
  type: TYPE_NORMAL
- en: '**Chat API delivers response**: The following code will invoke the `generate_response`
    method and deliver the chat response for the input query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we run this example, the outcome is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is sample input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The retrieved passages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can find the full code for this example at [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/augmented_generation.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/augmented_generation.py).
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration and fine-tuning**: Now let us look at a code snippet that combines
    the retriever and the LLM invocation as the full RAG flow. The following code
    demonstrates this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the code, we can see that the flow is simple. We retrieve the documents
    needed to leverage the RAG flow using the retriever and pass the input query and
    the retrieved documents to the LLM API invocation.
  prefs: []
  type: TYPE_NORMAL
- en: In this deep dive into the RAG architecture, we focused on its mechanics and
    demonstrated the functioning of its core components. By combining efficient information
    retrieval with advanced language generation models, RAG produces contextually
    appropriate and knowledge-enriched responses. As we transition to the next section,
    we will discuss the **retrieval process**.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving external information for your RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding how RAG leverages external knowledge is crucial for appreciating
    its ability to generate factually accurate and informative responses. This section
    discusses various **retrieval techniques**, strategies for integrating retrieved
    information, and practical examples to illustrate these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding retrieval techniques and strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The success of a RAG model hinges on its ability to retrieve relevant information
    from a vast external knowledge base using one of the commonly used retrieval techniques.
    These retrieval methods are essential for sourcing relevant information from large
    datasets. Common techniques include traditional methods such as BM25 and modern
    neural approaches such as DPR. Broadly speaking, these techniques can be classified
    into three categories: **vector similarity search**, **keyword matching**, and
    **passage retrieval**. We will discuss each of them in the following subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: Vector similarity search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The text or query you pass to the LLM is converted into a vector representation
    called an **embedding**. The vector similarity search compares the vector embeddings
    to retrieve the closest match. The idea is that related and similar text will
    have similar embeddings. This technique works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build an embedding of the input query. We tokenize the input query and generate
    the vector embedding representation of it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the embeddings of the documents. We generate an embedding for each document
    using the tokenizer and associate each embedding with its document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find similar documents using dot product calculation. This step uses the input
    query embedding and searches the document embeddings for results similar to the
    input query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sort the documents by the relevancy score and return the results. The results
    contain the matching documents along with a score representing how similar it
    is to the input query. We will order the results in the order we want, most similar
    to least similar:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let us run this example to see what the results would look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample input query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the sample output (ranked documents):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code demonstrates how to use DPR to encode a query and a set of
    documents into high-dimensional vector representations. By computing similarity
    scores, such as the dot product between the query vector and document vectors,
    the model evaluates the relevance of each document to the query. The documents
    are then ranked based on their similarity scores, with the most relevant ones
    appearing at the top. This process highlights the power of vector-based retrieval
    in effectively identifying contextually relevant information from a diverse set
    of documents, even when they include a mix of related and unrelated content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full version of this example is available in the GitHub repository: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/vector_similarity_search.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/vector_similarity_search.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Keyword matching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Keyword matching** is a simpler approach that identifies documents containing
    keywords present in the user prompt. While efficient, it can be susceptible to
    noise and misses documents with relevant synonyms. BM25 is a keyword-based probabilistic
    retrieval function that scores documents based on the query terms appearing in
    each document, considering term frequency and document length. The flow of this
    approach looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the BM25 corpus using the documents. We will tokenize documents and build
    a corpus from this. We will build the BM25 corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the query to search using it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Query the BM25 corpus using the tokenized query. This returns the scores for
    matching documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will take these scores, order the documents in the required order, and return
    them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we run this example, the results would look like this for the given input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample input query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The BM25 algorithm ranks documents based on their relevance to a query. It relies
    on the term frequency (how often a keyword appears in a document) and document
    length, applying a probabilistic scoring function to evaluate relevance. Unlike
    vector similarity search, which represents both queries and documents as dense
    numerical vectors in high-dimensional space and measures similarity using mathematical
    functions such as the dot product, BM25 operates directly on discrete word matches.
    This means BM25 is efficient and interpretable but can struggle with semantic
    relationships, as it cannot recognize synonyms or contextual meanings. In contrast,
    vector similarity search, such as DPR, excels in identifying conceptual similarities
    even when exact keywords differ, making it more suitable for tasks requiring deep
    semantic understanding. This snippet illustrates BM25‚Äôs utility for straightforward
    keyword-matching tasks where efficiency and explainability are critical.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete example is available in the GitHub repository: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/keyword_matching.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/keyword_matching.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Passage retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of retrieving entire documents, RAG can focus on specific passages
    within documents that directly address the user‚Äôs query. This allows for more
    precise information extraction. The initial flow of this approach is very similar
    to the vector search approach. We get the ranked documents using the approach
    shown in vector search and then extract relevant passages as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: When we run this example for the given input query, the results look as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample input query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example illustrates the **passage-retrieval approach**, which
    is more granular than document-level retrieval, focusing on extracting specific
    passages that directly address the user‚Äôs query. By leveraging a *reader model*
    in combination with a *retriever*, this approach enhances relevance and specificity,
    as it identifies not only the most relevant document but also the exact passage
    within it that best answers the query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if a passage has a slightly lower retriever score, the reader may prioritize
    it because it evaluates relevance more precisely at the word and span levels,
    considering contextual nuances. The retriever typically calculates a similarity
    score using the dot product of the query and passage embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21107_02_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ùëû is the query embedding, ![](img/B21107_02_002.png) is the passage embedding
    for the ![](img/B21107_02_003.png) passage, and ùëë is the dimensionality of the
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reader, however, refines this further by analyzing the text content of
    each passage. It assigns a **relevance score** or **logit** (also known as **confidence
    score**) based on the likelihood that a given passage contains the answer. This
    relevance score is computed from the raw outputs (logits) of the reader model,
    which considers word-level and span-level interactions between the query and the
    passage. The formula for the relevance score can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21107_02_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: logits(![](img/B21107_02_002.png)) refers to the raw scores assigned to the
    passage, ![](img/B21107_02_002.png), by the reader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: softmax converts these raw scores into probabilities, emphasizing the passage
    most likely to be relevant ([https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining both stages, the system can identify passages that are not only
    semantically similar (*retriever stage*) but also contextually aligned with the
    query‚Äôs intent (*reader stage*).
  prefs: []
  type: TYPE_NORMAL
- en: This dual-stage process highlights the strength of passage retrieval in generating
    highly targeted responses in information retrieval pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete example is available in the GitHub repository: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/passage_retrieval.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/passage_retrieval.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating the retrieved information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the last step in the RAG flow, let us look at how we can combine the retriever
    information with the generation model in a way that synthesizes contextually relevant
    and coherent responses. Unlike earlier examples, this approach explicitly integrates
    multiple retrieved passages with the query. By doing so, it creates a single input
    for the generation model. This allows the model to synthesize a unified and enriched
    response that goes beyond merely selecting or ranking passages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a sample input query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B21107_02_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B21107_02_008.png) is the token at position ![](img/B21107_02_009.png),
    ![](img/B21107_02_010.png) is the hidden state, and ![](img/B21107_02_011.png)
    is the model‚Äôs weight matrix. Beam search ensures the selection of the most likely
    sequence by maximizing the overall probability across tokens. Unlike previous
    examples where individual passages were selected or ranked, this code explicitly
    combines multiple retrieved documents into a single input alongside the query.
    This enables the T5 model to process the combined context holistically and produce
    a coherent response that incorporates information from multiple sources, making
    it particularly effective for queries requiring synthesis or summarization across
    multiple passages.
  prefs: []
  type: TYPE_NORMAL
- en: 'To refer to the full version of this code, please refer: [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/integrate_and_generate.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/integrate_and_generate.py)'
  prefs: []
  type: TYPE_NORMAL
- en: By exploring various retrieval techniques and their integration with generation
    models, we have seen how RAG architectures leverage external knowledge to produce
    accurate and informative responses.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let us look at the holistic flow from reading input documents
    from a source and leveraging those documents for a retriever flow, instead of
    the simple hardcoded sentences we looked at in the examples in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Building an end-to-end RAG flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we delved into the various steps in the RAG flow individually
    with simple data to demonstrate the usage. It would be a good idea to take a step
    back and use a real-world dataset, albeit a simple one, to complete the whole
    flow. For this, we will use the GitHub issues dataset ([https://huggingface.co/datasets/lewtun/github-issues](https://huggingface.co/datasets/lewtun/github-issues)).
    We will look at how we can read this data and use it in the RAG flow. This would
    lay the foundation for the full end-to-end RAG flow implementation in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will load GitHub comments to be able to answer questions,
    such as how we can load data offline. We need to follow these steps to load the
    data and set up the retriever:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparing the data**: First, we need to prepare our dataset. We will use
    the Hugging Face `datasets` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Select the relevant columns**: Keep only the columns required for analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Convert the dataset into a pandas DataFrame**: Convert the dataset into a
    `pandas` DataFrame for easier manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Explode comments, convert them back into a dataset, and process**: Flatten
    the comments into individual rows, convert the DataFrame back into a dataset,
    and compute the length of each comment. This step makes this data amenable to
    use with the retriever flow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Concatenate text for embeddings**: Let us prepare the document text by concatenating
    the relevant text fields. We will take individual fields from each row and prepare
    the text that represents the document text for that row. These documents are stored
    in an embedding store for retriever usage purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Load model and tokenizer**: Let us load the LLM that we will use to convert
    the documents into the embeddings and store them in an embedding store for the
    retriever flow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Define the embedding function**: Define the embedding function that leverages
    the model we defined previously to generate the embedding. We can invoke this
    method iteratively to generate the embeddings for all the documents we have, one
    document at a time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Compute embeddings**: Compute embeddings for the dataset. Now that we have
    the embedding function defined, let us call it for all the documents we have in
    our comments dataset. Note that we are storing the embedding in a new column named
    `embedding` in the same dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Perform semantic search**: Let us perform the retriever flow for the question.
    This will retrieve all the questions related to the question we have asked. We
    can use these documents to refine the response as needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code showcases the complete flow, from how we load the data into
    a data store, which can form the basis for the retriever, to retrieving documents,
    which can be used to provide more context for the LLM when it is generating the
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us see how the output looks when we run this application. We have the
    question hardcoded in the example code, and it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This hands-on example demonstrated the practical application of an end-to-end
    RAG architecture, leveraging powerful retrieval techniques to enhance language
    generation. The preceding code is adapted from the Hugging Face NLP course, available
    at [https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf](https://huggingface.co/learn/nlp-course/chapter5/6?fw=tf).
  prefs: []
  type: TYPE_NORMAL
- en: The complete Python file, along with a detailed explanation of how to run it,
    is available at [https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/full_rag_pipeline.py](https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch2/full_rag_pipeline.py).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we deep-dived into the world of RAG models. We started by understanding
    the core principles of RAG and how they differ from traditional generative AI
    models. This foundational knowledge is crucial as it sets the stage for appreciating
    the enhanced capabilities that RAG brings to the table.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we took a closer look at the architecture of RAG models, deconstructing
    their components through detailed code examples. By examining the encoder, retriever,
    and decoder, you gained insights into the inner workings of these models and how
    they integrate retrieved information to produce more contextually relevant and
    coherent outputs.
  prefs: []
  type: TYPE_NORMAL
- en: We then explored how RAG harnesses the power of information retrieval. These
    techniques help RAG effectively leverage external knowledge sources to improve
    the quality of a generated text. This is particularly useful for applications
    requiring high accuracy and context awareness. You also learned how to a simple
    RAG model using popular libraries such as Transformers and Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward to the next chapter, [*Chapter 3*](Preface.xhtml#_idTextAnchor012),
    we will build on this foundation. You will learn about graph data modeling and
    how to create knowledge graphs with Neo4j.
  prefs: []
  type: TYPE_NORMAL
