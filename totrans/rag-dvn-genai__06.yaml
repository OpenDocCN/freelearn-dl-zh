- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling RAG Bank Customer Data with Pinecone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling up RAG documents, whether text-based or multimodal, isn’t just about
    piling on and accumulating more data—it fundamentally changes how an application
    works. Firstly, scaling is about finding the right amount of data, not just more
    of it. Secondly, as you add more data, the demands on an application can change—it
    might need new features to handle the bigger load. Finally, cost monitoring and
    speed performance will constrain our projects when scaling. Hence, this chapter
    is designed to equip you with cutting-edge techniques for leveraging AI in solving
    the real-world scaling challenges you may face in your projects. For this, we
    will be building a recommendation system based on pattern-matching using Pinecone
    to minimize bank customer churn (customers choosing to leave a bank).
  prefs: []
  type: TYPE_NORMAL
- en: We will start with a step-by-step approach to developing the first program of
    our pipeline. Here, you will learn how to download a Kaggle bank customer dataset
    and perform **exploratory data analysis** (**EDA**). This foundational step is
    crucial as it guides and supports you in preparing your dataset and your RAG strategy
    for the next stages of processing. The second program of our pipeline introduces
    you to the powerful combination of Pinecone—a vector database suited for handling
    large-scale vector search—and OpenAI’s `text-embedding-3-small` model. Here, you’ll
    chunk and embed your data before upserting (updating or inserting records) it
    into a Pinecone index that we will scale up to 1,000,000+ vectors. We will ready
    it for complex query retrieval at a satisfactory speed. Finally, the third program
    of our pipeline will show you how to build RAG queries using Pinecone, augment
    user input, and leverage GPT-4o to generate AI-driven recommendations. The goal
    is to reduce churn in banking by offering personalized, insightful recommendations.
    By the end of this chapter, you’ll have a good understanding of how to apply the
    power of Pinecone and OpenAI technologies to your RAG projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The key aspects of scaling RAG vector stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EDA for data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling with Pinecone vector storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking strategy for customer bank information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding data with OpenAI embedding models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upserting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Pinecone for RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI-driven recommendations with GPT-4o to reduce bank customer churn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining how we will scale with Pinecone.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with Pinecone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be implementing Pinecone’s innovative vector database technology with
    OpenAI’s powerful embedding capabilities to construct data processing and querying
    systems. The goal is to build a recommendation system to encourage customers to
    continue their association with a bank. Once you understand this approach, you
    will be able to apply it to any domain requiring recommendations (leisure, medical,
    or legal). To understand and optimize the complex processes involved, we will
    build the programs from scratch with a minimal number of components. In this chapter,
    we will use the Pinecone vector database and the OpenAI LLM model.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting and designing an architecture depends on a project’s specific goals.
    Depending on your project’s needs, you can apply this methodology to other platforms.
    In this chapter and architecture, the combination of a vector store and a generative
    AI model is designed to streamline operations and facilitate scalability. With
    that context in place, let’s go through the architecture we will be building in
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will implement vector-based similarity search functionality,
    as we did in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*,
    and *Chapter 3*, *Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI*.
    We will take the structure of the three pipelines we designed in those chapters
    and apply them to our recommendation system, as shown in *Figure 6.1*. If necessary,
    take the time to go through those chapters before implementing the code in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](img/B31169_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Scaling RAG-driven generative AI pipelines'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features of the scaled recommendation system we will build can be summarized
    in the three pipelines shown in the preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 1: Collecting and preparing the dataset**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this pipeline, we will perform EDA on the dataset with standard queries and
    k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 2: Scaling a Pinecone index (vector store)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this pipeline, we will see how to chunk, embed, and upsert 1,000,000+ documents
    to a Pinecone index (vector store).
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 3: RAG generative AI**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pipeline will take us to fully scaled RAG when we query a 1,000,000+ vector
    store and augment the input of a GPT-4o model to make targeted recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main theoretical and practical applications of the three programs we will
    explore include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalable and serverless infrastructure**: We begin by understanding Pinecone’s
    serverless architecture, which eliminates the complexities of server management
    and scaling. We don’t need to manage storage resources or machine usage. It’s
    a pay-as-you-go approach based on serverless indexes formed by a cloud and region,
    for example, **Amazon Web Services** (**AWS**) in `us-east-1`. Scaling and billing
    are thus simplified, although we still have to monitor and minimize the costs!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lightweight and simplified development environment**: Our integration strategy
    will minimize the use of external libraries, maintaining a lightweight development
    stack. Directly using OpenAI to generate embeddings and Pinecone to store and
    query these embeddings simplifies the data processing pipeline and increases system
    efficiency. Although this approach can prove effective, other methods are possible
    depending on your project, as implemented in other chapters of this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimized scalability and performance**: Pinecone’s vector database is engineered
    to handle large-scale datasets effectively, ensuring that application performance
    remains satisfactory as the data volume grows. As for all cloud platforms and
    APIs, examine the privacy and security constraints when implementing Pinecone
    and OpenAI. Also, continually monitor the system’s performance and costs, as we
    will see in the *Pipeline 2: Scaling a Pinecone index (vector store)* section
    of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now go to our keyboards to collect and process the `Bank Customer Churn`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 1: Collecting and preparing the dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will focus on handling and analyzing the `Bank Customer Churn`
    dataset. We will guide you through the steps of setting up your environment, manipulating
    data, and applying **machine learning** (**ML**) techniques. It is important to
    get the “feel” of a dataset with human analysis before using algorithms as tools.
    Human insights will always remain critical because of the flexibility of human
    creativity. As such, we will implement data collection and preparation in Python
    in three main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collecting and processing the dataset**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up the Kaggle environment to authenticate and download datasets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting and unzipping the `Bank Customer Churn` dataset
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplifying the dataset by removing unnecessary columns
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploratory data analysis**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing initial data inspections to understand the structure and type of
    data we have
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating relationships between customer complaints and churn (closing accounts)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring how age and salary levels relate to customer churn
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a heatmap to visualize correlations between numerical features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training an ML model**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preparing the data for ML
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying clustering techniques to discover patterns in customer behavior
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing the effectiveness of different cluster configurations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Concluding and moving on to RAG-driven generative AI
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our goal is to analyze the dataset and prepare it for *Pipeline 2: Scaling
    a Pinecone index (vector store)*. To achieve that goal, we need to perform a preliminary
    EDA of the dataset. Moreover, each section is designed to be a hands-on walkthrough
    of the code from scratch, ensuring you gain practical experience and insights
    into data science workflows. Let’s get started by collecting the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Collecting and processing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s first collect the `Bank` `Customer Churn` dataset on Kaggle and process
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn](https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The file `Customer-Churn-Records.csv` contains data on 10,000 records of customers
    from a bank focusing on various aspects that might influence customer churn. The
    dataset was uploaded by Radheshyam Kollipara, who rightly states:'
  prefs: []
  type: TYPE_NORMAL
- en: As we know, it is much more expensive to sign in a new client than keeping an
    existing one. It is advantageous for banks to know what leads a client towards
    the decision to leave the company. Churn prevention allows companies to develop
    loyalty programs and retention campaigns to keep as many customers as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the details of the columns included in the dataset that follow the
    description on Kaggle:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RowNumber—corresponds to the record (row) number and has no effect on the
    output.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`CustomerId—contains random values and has no effect on customers leaving the
    bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Surname—the surname of a customer has no impact on their decision to leave
    the bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`CreditScore—can have an effect on customer churn since a customer with a higher
    credit score is less likely to leave the bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Geography—a customer''s location can affect their decision to leave the bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Gender—it''s interesting to explore whether gender plays a role in a customer
    leaving the bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Age—this is certainly relevant since older customers are less likely to leave
    their bank than younger ones.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tenure—refers to the number of years that the customer has been a client of
    the bank. Normally, older clients are more loyal and less likely to leave a bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Balance—is also a very good indicator of customer churn, as people with a
    higher balance in their accounts are less likely to leave the bank compared to
    those with lower balances.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`NumOfProducts—refers to the number of products that a customer has purchased
    through the bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`HasCrCard—denotes whether or not a customer has a credit card. This column
    is also relevant since people with a credit card are less likely to leave the
    bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`IsActiveMember—active customers are less likely to leave the bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`EstimatedSalary—as with balance, people with lower salaries are more likely
    to leave the bank compared to those with higher salaries.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Exited—whether or not the customer left the bank.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Complain—customer has complained or not.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Satisfaction Score—Score provided by the customer for their complaint resolution.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Card Type—the type of card held by the customer.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Points Earned—the points earned by the customer for using a credit card.`'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what the dataset contains, we need to collect it and process
    it for EDA. Let’s install the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the environment for Kaggle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To collect datasets from Kaggle automatically, you will need to sign up and
    create an API key at [https://www.kaggle.com/](https://www.kaggle.com/). At the
    time of writing this notebook, downloading datasets is free. Follow the instructions
    to save and use your Kaggle API key. Store your key in a safe location. In this
    case, the key is in a file on Google Drive that we need to mount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The program now reads the JSON file and sets environment variables for Kaggle
    authentication using your username and an API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to install Kaggle and authenticate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! That’s all we need. We are now ready to collect the `Bank Customer
    Churn` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now download the zipped dataset, extract the CSV file, upload it into
    a pandas DataFrame, drop columns that we will not use, and display the result.
    Let’s first download the zipped dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the source of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now unzip the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should confirm that the file is unzipped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The CSV file is uploaded to a pandas DataFrame named `data1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now drop the following four columns in this scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RowNumber`: We don’t need these columns because we will be creating a unique
    index for each record.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Surname`: The goal in this scenario is to anonymize the data and not display
    surnames. We will focus on customer profiles and behaviors, such as complaints
    and credit card consumption (points earned).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Gender`: Consumer perceptions and behavior have evolved in the 2020s. It is
    more ethical and just as efficient to leave this information out in the context
    of a sample project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Geography`: This field might be interesting in some cases. For this scenario,
    let’s leave this feature out to avoid overfitting outputs based on cultural clichés.
    Furthermore, including this feature would require more information if we wanted
    to calculate distances for delivery services, for example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output triggered by `data1` shows a simplified yet sufficient dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Triggered output'
  prefs: []
  type: TYPE_NORMAL
- en: This approach’s advantage is that it optimizes the size of the data that will
    be inserted into the Pinecone index (vector store). Optimizing the data size before
    inserting data into Pinecone and reducing the dataset by removing unnecessary
    fields can be very beneficial. It reduces the amount of data that needs to be
    transferred, stored, and processed in the vector store. When scaling, smaller
    data sizes can lead to faster query performance and lower costs, as Pinecone pricing
    can depend on the amount of data stored and the computational resources used for
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now save the new pandas DataFrame in a safe location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You can save it in the location that is best for you. Just make sure to save
    it because we will use it in the *Pipeline 2: Scaling a Pinecone index (vector
    store)* section of this chapter. We will now explore the optimized dataset before
    deciding how to implement it in a vector store.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Exploratory data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will perform EDA using the data that pandas has just defined,
    which contains customer data from a bank. EDA is a critical step before applying
    any RAG techniques with vector stores, as it helps us understand the underlying
    patterns and trends within the data.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, our preliminary analysis shows a direct correlation between customer
    complaints and churn rates, indicating that customers who have lodged complaints
    are more likely to leave the bank. Additionally, our data reveals that customers
    aged 50 and above are less likely to churn compared to younger customers. Interestingly,
    income levels (particularly the threshold of $100,000) do not appear to significantly
    influence churn decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Through the careful examination of these insights, we’ll demonstrate why jumping
    straight into complex ML models, especially deep learning, may not always be necessary
    or efficient for drawing basic conclusions. In scenarios where the relationships
    within the data are evident and the patterns straightforward, simpler statistical
    methods or even basic data analysis techniques might be more appropriate and resource-efficient.
    For example, k-means clustering can be effective, and we will implement it in
    the *Training an ML model* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this is not to understate the power of advanced RAG techniques, which
    we will explore in the *Pipeline 2: Scaling a Pinecone index* *(vector store)*
    section of this chapter. In that section, we will employ deep learning within
    vector stores to uncover more subtle patterns and intricate relationships that
    are not readily apparent through classic EDA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we display the columns of the DataFrame, we can see that it is challenging
    to find patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Age`, `EstimatedSalary`, and `Complain` are possible determining features
    that could be correlated with `Exited`. We can also display the DataFrame to gain
    insights, as shown in the excerpt of `data1` in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31169_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Visualizing the strong correlation between customer complaints
    and bank churning (Exited)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main feature seems to be `Complain`, which leads to `Exited` (churn), as
    shown by running a standard calculation on the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows a very high 100.29% ratio between complaints and customers
    leaving the bank (churning). This means that customers who complained did in fact
    leave the bank, which is a natural market trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can see that only a few exited the bank (six customers) without complaining.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following cells from GitHub; these contain Python functions that are
    variations of the `exited` and `complain` ratios and will produce the following
    outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Age` and `Exited` with a threshold of `age=50` shows that persons over 50
    seem less likely to leave a bank:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Conversely, the output shows that younger customers seem more likely to leave
    a bank if they are dissatisfied. You can explore different age thresholds to analyze
    the dataset further.
  prefs: []
  type: TYPE_NORMAL
- en: '`Salary`and `Exited`with a threshold of `salary_threshold=100000` doesn’t seem
    to be a significant feature, as shown in this output:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Try exploring different thresholds to analyze the dataset to confirm or refute
    this trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a heatmap based on the `data1` pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the highest correlation is between `Complain` and `Exited`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a graph  Description automatically generated](img/B31169_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Excerpt of the heatmap'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding heatmap visualizes the correlation between each pair of features
    (variables) in the dataset. It shows the correlation coefficients between each
    pair of variables, which can range from `-1`(low correlation) to `1`(high correlation),
    with `0` indicating no correlation.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have explored several features. Let’s build an ML model to take
    this exploration further.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Training an ML model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s continue our EDA and drill into the dataset further with an ML model.
    This section implements the training of an ML model using clustering techniques,
    specifically k-means clustering, to explore patterns within our dataset. We’ll
    prepare and process data for analysis, apply clustering, and then evaluate the
    results using different metrics. This approach is valuable for extracting insights
    without immediately resorting to more complex deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering is an unsupervised ML algorithm that partitions a dataset
    into k distinct, non-overlapping clusters by minimizing the variance within each
    cluster. The algorithm iteratively assigns data points to one of the k clusters
    based on the nearest mean (centroid), which is recalculated after each iteration
    until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s break down the code section by section.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation and clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will first copy our chapter’s dataset `data1` to `data2` to be able to go
    back to `data1` if necessary if we wish to try other ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can explore the data with various scenarios of feature sets. In this case,
    we will select `''``CreditScore''`, `''Age''`, `''EstimatedSalary''`, `''Exited''`,
    `''Complain''`, and `''Point Earned''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As in standard practice, let’s scale the features before running an ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The credit score, estimated salary, and points earned (reflecting credit card
    spending) are good indicators of a customer’s financial standing with the bank.
    The age factor, combined with these other factors, might influence older customers
    to remain with the bank. However, the important point to note is that complaints
    may lead any market segment to consider leaving since complaints and churn are
    strongly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now try to find two to four clusters to find the optimal number of
    clusters for this set of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output contains an evaluation of clustering performance using two metrics—the
    silhouette score and the Davies-Bouldin index—across different numbers of clusters
    (ranging from 2 to 4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Silhouette score**: This metric measures the quality of clustering by calculating
    the mean intra-cluster distance (how close each point in one cluster is to points
    in the same cluster) and the mean nearest cluster distance (how close each point
    is to points in the next nearest cluster). The score ranges from -1 to 1, where
    a high value indicates that clusters are well-separated and internally cohesive.
    In this output, the highest silhouette score is 0.6129 for 2 clusters, suggesting
    better cluster separation and cohesion compared to 3 or 4 clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Davies-Bouldin index**: This index evaluates clustering quality by comparing
    the ratio of within-cluster distances to between-cluster distances. Lower values
    of this index indicate better clustering, as they suggest lower intra-cluster
    variance and higher separation between clusters. The smallest Davies-Bouldin index
    in the output is 0.6144 for 2 clusters, indicating that this configuration likely
    provides the most effective separation of data points among the evaluated options.'
  prefs: []
  type: TYPE_NORMAL
- en: For two clusters, the silhouette score and Davies-Bouldin index both suggest
    relatively good clustering performance. But as the number of clusters increases
    to three and four, both metrics indicate a decline in clustering quality, with
    lower silhouette scores and higher Davies-Bouldin indices, pointing to less distinct
    and less cohesive clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation and evaluation of clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since two clusters seem to be the best choice for this dataset and set of features,
    let’s run the model with `n_clusters=2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, as shown in the *2\. Exploratory data analysis* section, the correlation
    between complaints and exiting is established, as shown in the excerpt of the
    pandas DataFrame in *Figure 6.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a game  Description automatically generated](img/B31169_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Excerpt of the output of k-means clustering'
  prefs: []
  type: TYPE_NORMAL
- en: The first cluster is `class=0`, which represents customers who complained (`Complain`)
    and left (`Exited`) the bank.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we count the rows for which `Sum where ''class'' == 0 and ''Exited'' ==
    1`, we will obtain a strong correlation between complaints and customers leaving
    the bank:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that complaints and churn (customers leaving the bank)
    are closely related:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following cell for the second class where `''class'' == 1 and ''Complain''
    == 1` confirms that few customers that complain stay with the bank:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is consistent with the correlations we have observed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We saw that finding the features that could help us keep customers is challenging
    with classical methods that can be effective. However, our strategy will now be
    to transform the customer records into vectors with OpenAI and query a Pinecone
    index to find deeper patterns within the dataset with queries that don’t exactly
    match the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 2: Scaling a Pinecone index (vector store)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this section is to build a Pinecone index with our dataset and scale
    it from 10,000 records up to 1,000,000 records. Although we are building on the
    knowledge acquired in the previous chapters, the essence of scaling is different
    from managing sample datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The clarity of each process of this pipeline is deceptively simple: data preparation,
    embedding, uploading to a vector store, and querying to retrieve documents. We
    have already gone through each of these processes in *Chapters 2* and *3*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, beyond implementing Pinecone instead of Deep Lake and using OpenAI
    models in a slightly different way, we are performing the same functions as in
    *Chapters 2*, *3*, and *4* for the vector store phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**: We will start by preparing our dataset using Python for
    chunking.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chunking and embedding**: We will chunk the prepared data and then embed
    the chunked data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Creating the Pinecone index**: We will create a Pinecone index (vector store).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Upserting**: We will upload the embedded documents (in this case, customer
    records) and the text of each record as metadata.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Querying the Pinecone index**: Finally, we will run a query to retrieve relevant
    documents to prepare *Pipeline 3: RAG generative AI*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take all the time you need, if necessary, to go through *Chapters 2*,*3*, and
    *4* again for the data preparation, chunking, embedding, and querying functions.
  prefs: []
  type: TYPE_NORMAL
- en: We know how to implement each phase because we’ve already done that with Deep
    Lake, and Pinecone is a type of vector store, too. So, what’s the issue here?
    The real issue is the hidden real-life project challenges on which we will focus,
    starting with the size, cost, and operations involved.
  prefs: []
  type: TYPE_NORMAL
- en: The challenges of vector store management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Usually, we begin a section by jumping into the code. That’s fine for small
    volumes, but scaling requires project management decisions before getting started!
    Why? When we run a program with a bad decision or an error on small datasets,
    the consequences are limited. But scaling is a different story! The fundamental
    principle and risk of scaling is that errors are scaled exponentially, too.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s list the pain points you must face before running a single line of code.
    You can apply this methodology to any platform or model. However, we have limited
    the platforms in this chapter to OpenAI and Pinecone to focus on processes, not
    platform management. Using other platforms involves careful risk management, which
    isn’t the objective of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin with OpenAI models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI models for embedding**: OpenAI continually improves and offers new
    models for embedding. Make sure you examine the characteristics of each one before
    embedding, including speed, cost, input limits, and API call rates, at [https://platform.openai.com/docs/models/embeddings](https://platform.openai.com/docs/models/embeddings).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI models for generation**:OpenAI continually releases new models and
    abandons older ones. Google does the same. Think of these models as racing cars.
    Can you win a race today with a 1930 racing car? When scaling, you need the most
    efficient models. Check the speed, cost, input limits, output size, and API call
    rates at [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This means that you must continually take the evolution of models into account
    for speed and cost reasons when scaling. Then, beyond technical considerations,
    you must have a real-time view of the pay-as-you-go billing perspective and technical
    constraints, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Billing management**: [https://platform.openai.com/settings/organization/billing/overview](https://platform.openai.com/settings/organization/billing/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limits including rate limits**: [https://platform.openai.com/settings/organization/limits](https://platform.openai.com/settings/organization/limits)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s examine Pinecone constraints once you have created an account:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud and region**: The choice of the cloud (AWS, Google, or other) and region
    (location of the serverless storage) have pricing implications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage**: This includes read units, write units, and storage costs, including
    cloud backups. Read more at [https://docs.pinecone.io/guides/indexes/back-up-an-index](https://docs.pinecone.io/guides/indexes/back-up-an-index).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You must continually monitor the price and usage of Pinecone as for any other
    cloud environment. You can do so using these links: [https://www.pinecone.io/pricing/](https://www.pinecone.io/pricing/)
    and [https://docs.pinecone.io/guides/operations/monitoring](https://docs.pinecone.io/guides/operations/monitoring).'
  prefs: []
  type: TYPE_NORMAL
- en: The scenario we are implementing is one of many other ways of achieving the
    goals in this chapter with other platforms and frameworks. However, the constraints
    are invariants, including pricing, usage, speed performances, and limits.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement *Pipeline 2* by focusing on the pain points beyond the functionality
    we have already explored in previous chapters. You may open `Pipeline_2_Scaling_a_Pinecone_Index.ipynb`
    in the GitHub repository. The program begins with installing the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the program is limited to Pinecone and OpenAI, which
    has the advantage of avoiding any intermediate software, platforms, and constraints.
    Store your API keys in a safe location. In this case, the API keys are stored
    on Google Drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we install OpenAI and Pinecone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the program initializes the API keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The program now processes the `Bank Customer Churn` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section will focus on preparing the dataset for chunking, which splits
    it into optimized chunks of text to embed. The program first retrieves the `data1.csv`
    dataset that we prepared and saved in the *Pipeline 1: Collecting and preparing
    the dataset* section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we load the dataset in a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We make sure that the 10,000 lines of the dataset are loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that the lines are indeed present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is important in this scenario. Each line that represents
    a customer record will become a line in the `output_lines` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that each line in the `output_lines` list is a separate customer
    record text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We are sure that each line is a separate pre-chunk with a clearly defined customer
    record. Let’s now copy `output_lines` to `lines` for the chunking process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The program runs a quality control on the `lines` list to make sure we haven’t
    lost a line in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that 10,000 lines are present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: And just like that, the data is ready to be chunked.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking and embedding the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will chunk and embed the pre-chunks in the `lines` list.
    Building a pre-chunks list with structured data is not possible every time, but
    when it is, it increases a model’s traceability, clarity, and querying performance.
    The chunking process is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The practice of chunking pre-chunks is important for dataset management. We
    can create our chunks from a list of pre-chunks stored as lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that we have not lost any data during the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: So why bother creating chunks and not just use the lines directly? In many cases,
    lines may require additional quality control and processing, such as data errors
    that somehow slipped through in the previous steps. We might even have a few chunks
    that exceed the input limit (which is continually evolving) of an embedding model
    at a given time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the structure of the chunked data, you can examine the
    length and content of the chunks using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will help a human controller visualize the chunked data, providing
    a snapshot like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The chunks will now be embedded.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will require careful testing and consideration of the issues. We
    will realize that *scaling requires more thinking than doing*. Each project will
    require specific amounts of data through design and testing to provide effective
    responses. We must also take into account the cost and benefit of each component
    of the pipeline. For example, initializing the embedding model is no easy task!
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, OpenAI offers three embedding models that we can test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In this section, we will use `text-embedding-3-small`. However, you can evaluate
    the other models by uncommenting the code. The `embedding` function will accept
    the model you select:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure to check the cost and features of each embedding model before running
    one of your choice: [https://platform.openai.com/docs/guides/embeddings/embedding-models](https://platform.openai.com/docs/guides/embeddings/embedding-models).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The program now embeds the chunks, but the embedding process requires strategic
    choices, particularly to manage large datasets and API rate limits effectively.
    In this case, we will create batches of chunks to embed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We will embed 1,000 chunks at a time with `chunk_start = 0` and `chunk_end =
    1000`. To avoid possible OpenAI API rate limits, `pause_time = 3` was added to
    pause for 3 seconds between each batch. We will store the embeddings in `embeddings
    = []` and count the batches starting with `counter = 1.`
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is divided into three main parts, as explained in the following excerpts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Iterating through all the chunks with batches:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Embedding a batch of `chunks_to_embed`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Updating the start and end values of the chunks to embed for the next batch:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A function was added in case the batches are not perfect multiples of the batch
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the counter and the processing time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The response time may seem long and may vary for each run, but that is what
    scaling is all about! We cannot expect to process large volumes of data in a very
    short time and not face performance challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can display an embedding if we wish to check that everything went well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s verify if we have the same number of text chunks (customer records) and
    vectors (embeddings):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that we are ready to move to Pinecone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We have now chunked and embedded the data. We will duplicate the data to simulate
    scaling in this notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicating data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will duplicate the chunked and embedded data; this way, you can simulate
    volumes without paying for the OpenAI embeddings. The cost of the embedding data
    and the time performances are linear. So we can simulate scaling with a corpus
    of 50,000 data points, for example, and extrapolate the response times and cost
    to any size we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is straightforward. We first determine the number of times we want
    to duplicate the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The program will then duplicate the chunks and the embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The code then checks if the number of chunks fits the number of embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the output confirms that we duplicated the data five times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 50,000 data points is a good volume to begin with, giving us the necessary data
    to populate a vector store. Let’s now create the Pinecone index.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Pinecone index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to make sure our API key is initialized with the name of
    the variable we prefer and then create a Pinecone instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The Pinecone instance, `pc`, has been created. Now, we will choose the index
    name, our cloud, and region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now indicated that we want a serverless cloud instance (`spec`) with
    AWS in the `''us-east-1''` location. We are ready to create the index (the type
    of vector store) named `''bank-index-50000''` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We added the following two parameters to `index_name` and `spec`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dimension=1536` represents the length of the embeddings vector that you can
    adapt to the embedding model of your choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric=''cosine''` is the metric we will use for vector similarity between
    the embedded vectors. You can also choose other metrics, such as Euclidean distance:
    [https://www.pinecone.io/learn/vector-similarity/](https://www.pinecone.io/learn/vector-similarity/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the index is created, the program displays the description of the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The vector count and index fullness are `0` since we haven’t been populating
    the vector store. Great, now we are ready to upsert!
  prefs: []
  type: TYPE_NORMAL
- en: Upserting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The section’s goal is to populate the vector store with our 50,000 embedded
    vectors and their associated metadata (chunks). The objective is to fully understand
    the scaling process and use synthetic data to reach the 50,000+ vector level.
    You can go back to the previous section and duplicate the data up to any value
    you wish. However, bear in mind that the upserting time to a Pinecone index is
    linear. You simply need to extrapolate the performances to the size you want to
    evaluate to obtain the approximate time it would take. Check the Pinecone pricing
    before running the upserting process: [https://www.pinecone.io/pricing/](https://www.pinecone.io/pricing/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will populate (upsert) the vector store with three fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ids`: Contains a unique identifier for each chunk, which will be a counter
    we increment as we upsert the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding`: Contains the vectors (embedded chunks) we created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunks`: Contains the chunks in plain text, which is the metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code will populate the data in batches. Let’s first define the batch upserting
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We will measure the time it takes to process our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a function that will calculate the size of the batches and limit
    them to 4 MB, which is close to the present Pinecone upsert batch size limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our `upsert` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to generate unique IDs for the data we upsert:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create the metadata to upsert the dataset to Pinecone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have everything we need to upsert in `data_for_upsert`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"id": str(ids[i])` contains the IDs we created with the seed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"values": emb` contains the chunks we embedded into vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"metadata": {"text": chunk}` contains the chunks we embedded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We now run the batch upsert process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we measure the response time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The output contains useful information that shows the batch progression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The time shows that it takes just under one minute (56 seconds) per 10,000 data
    points. You can try a larger corpus. The time should remain linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also view the Pinecone index statistics to see how many vectors were
    uploaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that the upserting process was successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The upsert output shows that we upserted 50,000 data points but the output shows
    less, most probably due to duplicates within the data.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the Pinecone index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The task now is to verify the response times with a large Pinecone index. Let’s
    create a function to query the vector store and display the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We need an embedding function for the query using the same embedding model
    as we implemented to embed the chunks of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now query the Pinecone vector store to conduct a unit test and display
    the results and response time. We first initialize the OpenAI client and start
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We then query the vector store with a customer profile that does not exist
    in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The query is embedded with the same model as the one used to embed the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the query and display the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the query response and time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the response quality is satisfactory because it found a similar
    profile. The time is excellent: `0.74 seconds`. When reaching a 1,000,000 vector
    count, for example, the response time should still be constant at less than a
    second. That is the magic of the Pinecone index!'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we go to our organization on Pinecone, [https://app.pinecone.io/organizations/](https://app.pinecone.io/organizations/),
    and click on our index, we can monitor our statistics, analyze our usage, and
    more, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Visualizing the Pinecone index vector count in the Pinecone console'
  prefs: []
  type: TYPE_NORMAL
- en: Our Pinecone index is now ready to augment inputs and generate content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 3: RAG generative AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use RAG generative AI to automate a customized and
    engaging marketing message to the customers of the bank to encourage them to remain
    loyal. We will be building on our programs on data preparation and Pinecone indexing;
    we will leverage the Pinecone vector database for advanced search functionalities.
    We will choose a target vector that represents a market segment to query the Pinecone
    index. The response will be processed to extract the top k similar vectors. We
    will then augment the user input with this target market to ask OpenAI to make
    recommendations to the market segment targeted with customized messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may open `Pipeline-3_RAG_Generative AI.ipynb` on GitHub. The first code
    section in this notebook, *Installing the environment*, is the same as in `2-Pincone_vector_store-1M.ipynb`,
    built in the *Pipeline 2: Scaling a Pinecone index (vector store)* section earlier
    in this chapter. The *Pinecone index* in the second code section is also the same
    as in `2-Pincone_vector_store-1M.ipynb`. However, this time, the Pinecone index
    code checks whether a Pinecone index exists and connects to it if it does, rather
    than creating a new index.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run an example of RAG with GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: RAG with GPT-4o
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section of the code, we will query the Pinecone vector store, augment
    the user input, and generate a response with GPT-4o. It is the same process as
    with Deep Lake and an OpenAI generative model in *Chapter 3*, *Building Index-Based
    RAG with LlamaIndex, Deep Lake, and OpenAI*, for example. However, the nature
    and usage of the Pinecone query is quite different in this case for the following
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Target vector**: The user input is not a question in the classical sense.
    In this case, it is a target vector representing the profile of a market segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage**:The usage isn’t to augment the generative AI in the classical dialog
    sense (questions, summaries). In this case, we expect GPT-4o to write an engaging,
    customized email to offer products and services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query time**:Speed is critical when scaling an application. We will measure
    the query time on the Pinecone index that contains 1,000,000+ vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will need an embedding function to embed the input. We will simplify and
    use the same embedding model we used in the *Embedding* section of *Pipeline 2:
    Scaling a Pinecone index (vector store)* for compatibility reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to query the Pinecone index.
  prefs: []
  type: TYPE_NORMAL
- en: Querying a target vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A target vector represents a market segment that a marketing team wants to focus
    on for recommendations to increase customer loyalty. Your imagination and creativity
    are the only limits! Usually, the marketing team will be part of the design team
    for this pipeline. You might want to organize workshops to try various scenarios
    until the marketing team is satisfied. If you are part of the marketing team,
    then you want to help design target vectors. In any case, human insights into
    our adaptive creativity will lead to many ways of organizing target vectors and
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we will target a market segment of customers around the age of
    42 (`Age 42`). We don’t need the age to be strictly 42 or an age bracket. We’ll
    let AI do the work for us. We are also targeting a customer that has a 100,000+
    (`EstimatedSalary 101348.88`) estimated salary, which would be a loss for the
    bank. We’re choosing a customer who has complained (`Complain 1`) and seems to
    be exiting (`Exited 1`) the bank. Let’s suppose that `Exited 1`, in this scenario,
    means that the customer has made a request to close an account but it hasn’t been
    finalized yet. Let’s also consider that the marketing department chose the target
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: '`query_text` represents the customer profiles we are searching for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We have embedded the query. Let’s now retrieve the top-k customer profiles
    that fit the target vector and parse the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We now print the response and the metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is parsed to find the top-k matches to display their scores and
    content, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We have retrieved valuable information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ranking** through the `top-k` vectors that match the target vector. From
    one to another, depending on the target vector, the ranking will be automatically
    recalculated by the OpenAI generative AI model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Score metric** through the score provided. A score is returned providing
    a metric for the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content** that contains the top-ranked and best scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s an all-in-one automated process! AI is taking us to new heights but we,
    of course, need human control to confirm the output, as described in the previous
    chapter on human feedback.
  prefs: []
  type: TYPE_NORMAL
- en: We now need to extract the relevant information to augment the input.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting relevant texts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following code goes through the top-ranking vectors, searches for the matching
    text metadata, and combines the content to prepare the augmentation phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays `combined_text`, relevant text we need to augment the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to augment the prompt before AI generation.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now engineer our prompt by adding three texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`query_prompt`: The instructions for the generative AI model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_text`: The target vector containing the target profile chosen by the
    marketing team'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combined_context`: The concentrated metadata text of the similar vectors selected
    by the query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`itext` contains these three variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the core input for the generative AI model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: We can now prepare the request for the generative AI model.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will submit the augmented input to an OpenAI generative
    AI model. The goal is to obtain a customized email to send the customers in the
    Pinecone index marketing segment we obtained through the target vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create an OpenAI client and choose GPT-4o as the generative AI
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'We then introduce a time performance measurement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The response time should be relatively constant since we are only sending one
    request at a time in this scenario. We now begin to create our completion request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'The system role provides general instructions to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'The user role contains the engineered `itext` prompt we designed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we set the parameters for the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters are designed to obtain a low random yet “creative” output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`temperature=0`: Low randomness in response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_tokens=300`: Limits response length to 300 tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_p=1`: Considers all possible tokens; full diversity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frequency_penalty=0`: No penalty for frequent word repetition to allow the
    response to remain open'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`presence_penalty=0`: No penalty for introducing new topics to allow the response
    to find ideas for our prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We send the request and display the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is satisfactory for this market segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the goal of the marketing team is to convince customers not to leave
    and to increase their loyalty to the bank, I’d say the email we received as output
    is good enough. Let’s display the time it took to obtain a response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'The response time is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully produced a customized response based on a target vector.
    This approach might be sufficient for some projects, whatever the domain. Let’s
    summarize the RAG-driven generative recommendation system built in this chapter
    and continue our journey.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter aimed to develop a scaled RAG-driven generative AI recommendation
    system using a Pinecone index and OpenAI models tailored to mitigate bank customer
    churn. Using a Kaggle dataset, we demonstrated the process of identifying and
    addressing factors leading to customer dissatisfaction and account closures. Our
    approach involved three key pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: When building *Pipeline 1*, we streamlined the dataset by removing non-essential
    columns, reducing both data complexity and storage costs. Through EDA, we discovered
    a strong correlation between customer complaints and account closures, which a
    k-means clustering model further validated. We then designed *Pipeline 2* to prepare
    our RAG-driven system to generate personalized recommendations. We processed data
    chunks with an OpenAI model, embedding these into a Pinecone index. Pinecone’s
    consistent upsert capabilities ensured efficient data handling, regardless of
    volume. Finally, we built *Pipeline 3* to leverage over 1,000,000 vectors within
    Pinecone to target specific market segments with tailored offers, aiming to boost
    loyalty and reduce attrition. Using GPT-4o, we augmented our queries to generate
    compelling recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: The successful application of a targeted vector representing a key market segment
    illustrated our system’s potential to craft impactful customer retention strategies.
    However, we can improve the recommendations by expanding the Pinecone index into
    a multimodal knowledge base, which we will implement in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does using a Kaggle dataset typically involve downloading and processing real-world
    data for analysis?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is Pinecone capable of efficiently managing large-scale vector storage for AI
    applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can k-means clustering help validate relationships between features such as
    customer complaints and churn?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does leveraging over a million vectors in a database hinder the ability to personalize
    customer interactions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the primary objective of using generative AI in business applications to
    automate and improve decision-making processes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are lightweight development environments advantageous for rapid prototyping
    and application development?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can Pinecone’s architecture automatically scale to accommodate increasing data
    loads without manual intervention?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is generative AI typically employed to create dynamic content and recommendations
    based on user data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the integration of AI technologies like Pinecone and OpenAI require significant
    manual configuration and maintenance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are projects that use vector databases and AI expected to effectively handle
    complex queries and large datasets?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pinecone documentation: [https://docs.pinecone.io/guides/get-started/quickstart](https://docs.pinecone.io/guides/get-started/quickstart)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI embedding and generative models: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Han, Y., Liu, C., & Wang, P. (2023). *A comprehensive survey on vector database:
    Storage and retrieval technique, challenge*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
