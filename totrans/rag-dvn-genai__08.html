<html><head></head><body>
  <div><h1 class="chapterNumber">8</h1>
    <h1 id="_idParaDest-203" class="chapterTitle">Dynamic RAG with Chroma and Hugging Face Llama</h1>
    <p class="normal">This chapter will take you into the pragmatism of dynamic RAG. In today’s rapidly evolving landscape, the ability to make swift, informed decisions is more crucial than ever. Decision-makers across various fields—from healthcare and scientific research to customer service management—increasingly require real-time data that is relevant only within the short period it is needed. A meeting may only require temporary yet highly prepared data. Hence, the concept of data permanence is shifting. Not all information must be stored indefinitely; instead, in many cases, the focus is shifting toward using precise, pertinent data tailored for specific needs at specific times, such as daily briefings or critical meetings.</p>
    <p class="normal">This chapter introduces an innovative and efficient approach to handling such data through the embedding and creation of temporary Chroma collections. Each morning, a new collection is assembled containing just the necessary data for that day’s meetings, effectively avoiding long-term data accumulation and management overhead. This data might include medical reports for a healthcare team discussing patient treatments, customer interactions for service teams strategizing on immediate issues, or the latest scientific research data for researchers making day-to-day experimental decisions. We will then build a Python program to support dynamic and efficient decision-making in daily meetings, applying a methodology using a hard science (any of the natural or physical sciences) dataset for a daily meeting. This approach will highlight the flexibility and efficiency of modern data management. In this case, the team wants to obtain pertinent scientific information without searching the web or interacting with online AI assistants. The constraint is to have a free, open-source assistant that anyone can use, which is why we will use Chroma and Hugging Face resources.</p>
    <p class="normal">The first step is to create a temporary Chroma collection. We will simulate the processing of a fresh dataset compiled daily, tailored to the specific agenda of upcoming meetings, ensuring relevance and conciseness. In this case, we will download the SciQ dataset from Hugging Face, which contains thousands of crowdsourced science questions, such as those related to physics, chemistry, and biology. Then, the program will embed the relevant data required for the day, guaranteeing that all discussion points are backed by the latest, most relevant data. </p>
    <p class="normal">A user might choose to run queries before the meetings to confirm their accuracy and alignment with the day’s objective. Finally, as meetings progress, any arising questions trigger real-time data retrieval, augmented through <strong class="keyWord">Large Language Model Meta AI</strong> (<strong class="keyWord">Llama</strong>) technology to generate dynamic flashcards. These flashcards provide quick and precise responses to ensure discussions are both productive and informed. By the end of this chapter, you will have acquired the skills to implement open-source free dynamic RAG in a wide range of domains.</p>
    <p class="normal">To sum that up, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">The architecture of dynamic RAG</li>
      <li class="bulletList">Preparing a dataset for dynamic RAG</li>
      <li class="bulletList">Creating a Chroma collection</li>
      <li class="bulletList">Embedding and upserting data in a Chroma collection</li>
      <li class="bulletList">Batch-querying a collection</li>
      <li class="bulletList">Querying a collection with a user request</li>
      <li class="bulletList">Augmenting the input with the output of a query</li>
      <li class="bulletList">Configuring Hugging Face’s framework for Meta Llama</li>
      <li class="bulletList">Generating a response based on the augmented input</li>
    </ul>
    <p class="normal">Let’s begin by going through the architecture of dynamic RAG.</p>
    <h1 id="_idParaDest-204" class="heading-1">The architecture of dynamic RAG</h1>
    <p class="normal">Imagine you’re in a<a id="_idIndexMarker497"/> dynamic environment in which information changes daily. Each morning, you gather a fresh batch of 10,000+ questions and validated answers from across the globe. The challenge is to access this information quickly and effectively during meetings without needing long-term storage or complicated infrastructure.</p>
    <p class="normal">This dynamic RAG method allows us to maintain a lean, responsive system that provides up-to-date information without the burden of ongoing data storage. It’s perfect for environments where data relevance is short-lived but critical for decision-making.</p>
    <p class="normal">We will be applying this to a<a id="_idIndexMarker498"/> hard science dataset. However, this dynamic approach isn’t limited to our specific example. It has broad applications across various domains, such as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Customer support</strong>: Daily<a id="_idIndexMarker499"/> updated FAQs can be accessed in real-time to provide quick responses to customer inquiries.</li>
      <li class="bulletList"><strong class="keyWord">Healthcare</strong>: During meetings, medical teams can use the latest research and patient data to answer complex health-related questions.</li>
      <li class="bulletList"><strong class="keyWord">Finance</strong>: Financial analysts can query the latest market data to make informed decisions on investments and strategies.</li>
      <li class="bulletList"><strong class="keyWord">Education</strong>: Educators can access the latest educational resources and research to answer questions and enhance learning.</li>
      <li class="bulletList"><strong class="keyWord">Tech support</strong>: IT teams can use updated technical documentation to solve issues and guide users effectively.</li>
      <li class="bulletList"><strong class="keyWord">Sales and marketing</strong>: Teams can quickly access the latest product information and market trends to answer client queries and strategize.</li>
    </ul>
    <p class="normal">This chapter implements one type of a dynamic RAG ecosystem. Your imagination is the limit, so feel free to apply this ecosystem to your own projects in different ways. For now, let’s see how the dynamic RAG components fit into the ecosystem we described in <em class="chapterRef">Chapter 1</em>, <em class="italic">Why Retrieval Augmented Generation?</em>, in the <em class="italic">RAG ecosystem</em> section.</p>
    <p class="normal">We will streamline the integration and use of dynamic information in real-time decision-making contexts, such as daily meetings, in Python. Here’s a breakdown of this innovative strategy for each component and its ecosystem component label:</p>
    <figure class="mediaobject"><img src="img/B31169_08_01.png" alt="A diagram of a diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.1: The dynamic RAG system</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Temporary Chroma collection creation (D1, D2, D3, E2)</strong>: Every morning, a temporary Chroma collection is set up specifically for that day’s meeting. This collection is not <a id="_idIndexMarker500"/>meant to be saved post-meeting, serving only the day’s immediate needs and ensuring that data does not clutter the system in the long term.</li>
      <li class="bulletList"><strong class="keyWord">Embedding relevant data (D1, D2, D3, E2)</strong>: The collection embeds critical data, such as customer support interactions, medical reports, or scientific facts. This embedding process tailors the content specifically to the meeting agenda, ensuring that all pertinent information is at the fingertips of the meeting participants. The data could include human feedback from documents and possibly other generative AI systems.</li>
      <li class="bulletList"><strong class="keyWord">Pre-meeting data validation (D4)</strong>: Before the meeting begins, a batch of queries is run against this temporary Chroma collection to ensure that all data is accurate and appropriately aligned with the meeting’s objectives, thereby facilitating a smooth and informed discussion.</li>
      <li class="bulletList"><strong class="keyWord">Real-time query handling (G1, G2, G3, G4)</strong>: During the meeting, the system is designed to handle spontaneous queries from participants. A single question can trigger the retrieval of specific information, which is then used to augment Llama’s input, enabling it to generate flashcards dynamically. These flashcards are utilized to provide concise, accurate responses during the meeting, enhancing the efficiency and productivity of the discussion.</li>
    </ul>
    <p class="normal">We will be using Chroma, a powerful, open-source, AI-native vector database designed to store, manage, and search embedded vectors in collections. Chroma contains everything we need to start, and we can run it on our machine. It is also very suitable for applications involving LLMs. Chroma collections are thus suitable for a temporary, cost-effective, and real-time RAG system. The dynamic RAG architecture of this chapter implemented <a id="_idIndexMarker501"/>with Chroma is innovative and practical. Here are some key points to consider in this fast-moving world:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Efficiency and cost-effectiveness</strong>: Using Chroma for temporary storage and Llama for response generation ensures that the system is lightweight and doesn’t incur ongoing storage costs. This makes it ideal for environments where data is refreshed frequently and long-term storage isn’t necessary. It is very convincing for decision-makers who want lean systems.</li>
      <li class="bulletList"><strong class="keyWord">Flexibility</strong>: The system’s ephemeral nature allows for the integration of new data daily, ensuring that the most up-to-date information is always available. This can be particularly valuable in fast-paced environments in which information changes rapidly.</li>
      <li class="bulletList"><strong class="keyWord">Scalability</strong>: The approach is scalable to other similar datasets, provided they can be embedded and queried effectively. This makes it adaptable to various domains beyond the given example. Scaling is not only increasing volumes of data but also the ability to apply a framework to a wide range of domains and situations.</li>
      <li class="bulletList"><strong class="keyWord">User-friendliness</strong>: The system’s design is straightforward, making it accessible to users who may not be deeply technical but need reliable answers quickly. This simplicity can enhance user engagement and satisfaction. Making users happy with cost-effective, transparent, and lightweight AI will surely boost their interest in RAG-driven generative AI.</li>
    </ul>
    <p class="normal">Let’s now begin building a dynamic RAG program.</p>
    <h1 id="_idParaDest-205" class="heading-1">Installing the environment</h1>
    <p class="normal">The environment <a id="_idIndexMarker502"/>focuses on open-source and free resources that we can run on our machine or a free Google Colab account. This chapter will run these resources on Google Colab with Hugging Face and Chroma.</p>
    <p class="normal">We will first install Hugging Face.</p>
    <h2 id="_idParaDest-206" class="heading-2">Hugging Face</h2>
    <p class="normal">We will implement Hugging <a id="_idIndexMarker503"/>Face’s open-source resources to download a dataset for the Llama model. Sign up at <a href="https://huggingface.co/">https://huggingface.co/</a> to obtain your<a id="_idIndexMarker504"/> Hugging Face API token. If you are using Google Colab, you can create a Google Secret in the sidebar and activate it. If so, you<a id="_idIndexMarker505"/> can comment the following cell—<code class="inlineCode"># Save your Hugging Face token in a secure location</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">#1.Uncomment the following lines if you want to use Google Drive to retrieve your token
from google.colab import drive
drive.mount('/content/drive')
f = open("drive/MyDrive/files/hf_token.txt", "r")
access_token=f.readline().strip()
f.close()
#2.Uncomment the following line if you want to enter your HF token manually
#access_token =[YOUR HF_TOKEN]
import os
os.environ['HF_TOKEN'] = access_token
</code></pre>
    <p class="normal">The program first retrieves the Hugging Face API token. Make sure to store it in a safe place. You can choose to use Google Drive or enter it manually. Up to now, the installation seems to have run smoothly. We now install <code class="inlineCode">datasets</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install datasets==2.20.0
</code></pre>
    <p class="normal">However, there are conflicts, such as <code class="inlineCode">pyarrow</code>, with Google Colab’s pre-installed version, which is more recent. These conflicts between fast-moving packages are frequent. When Hugging Face updates its packages, this conflict will not appear anymore. But other conflicts may appear. This conflict will not stop us from downloading datasets. If it did, we would have to uninstall Google Colab packages and reinstall <code class="inlineCode">pyarrow</code>, but other dependencies may possibly create issues. We must accept these challenges, as explained in the <em class="italic">Setting up the environment</em> section in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>.</p>
    <p class="normal">We will now install<a id="_idIndexMarker506"/> Hugging Face’s <code class="inlineCode">transformers</code> package:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install transformers==4.41.2
</code></pre>
    <p class="normal">We also install accelerate to run PyTorch packages on GPUs, which is highly recommended for<a id="_idIndexMarker507"/> this notebook, among other features, such as mixed precision and accelerated processing times:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install accelerate==0.31.0
</code></pre>
    <p class="normal">Finally, we will initialize <code class="inlineCode">meta-llama/Llama-2-7b-chat-hf</code> as the tokenizer and chat model interactions. Llama is a series of transformer-based language models developed by Meta AI (formerly Facebook AI) that we can access through Hugging Face:</p>
    <pre class="programlisting code"><code class="hljs-code">from transformers import AutoTokenizer
import tranformers
import torch
model = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model)
</code></pre>
    <p class="normal">We access the model through Hugging Face’s pipeline:</p>
    <pre class="programlisting code"><code class="hljs-code">pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    torch_dtype=torch.float16,
    device_map="auto",
)
</code></pre>
    <p class="normal">Let’s go through the pipeline:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">transformers.pipeline</code> is the function used to create a pipeline for text generation. This pipeline abstracts away much of the complexity we must avoid in this dynamic RAG ecosystem.</li>
      <li class="bulletList"><code class="inlineCode">text-generation</code> specifies the type of task the pipeline is set up for. In this case, we want text generation.</li>
      <li class="bulletList"><code class="inlineCode">model</code> specifies the model we selected.</li>
      <li class="bulletList"><code class="inlineCode">torch_dtype=torch.float16</code> sets the data type for PyTorch tensors to <code class="inlineCode">float16</code>. This is a key<a id="_idIndexMarker508"/> factor for dynamic RAG, which reduces memory consumption and can speed up computation, particularly on GPUs that support half-precision computations. Half-precision computations use 16 bits: half of the standard 32-bit precision, for faster, lighter processing. This is exactly what we need.</li>
      <li class="bulletList"><code class="inlineCode">device_map="auto"</code> instructs the pipeline to automatically determine the best device to run the<a id="_idIndexMarker509"/> model on (CPU, GPU, multi-GPU, etc.). This parameter is particularly important for optimizing performance and automatically distributing the model’s layers across available devices (like GPUs) in the most efficient manner possible. If multiple GPUs are available, it will distribute the load across them to maximize parallel processing. If you have access to a GPU, activate it to speed up the configuration of this pipeline.</li>
    </ul>
    <p class="normal">Hugging Face is ready; Chroma is required next.</p>
    <h2 id="_idParaDest-207" class="heading-2">Chroma</h2>
    <p class="normal">The following line installs <a id="_idIndexMarker510"/>Chroma, our open-<a id="_idIndexMarker511"/>source vector database:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install chromadb==0.5.3
</code></pre>
    <p class="normal">Take a close look <a id="_idIndexMarker512"/>at the following excerpt output, which displays the packages installed and, in particular, <strong class="keyWord">Open Neural Network Exchange</strong> (<strong class="keyWord">ONNX</strong>):</p>
    <pre class="programlisting con"><code class="hljs-con">Successfully installed asgiref-3…onnxruntime-1.18.0…
</code></pre>
    <p class="normal">ONNX (<a href="https://onnxruntime.ai/">https://onnxruntime.ai/</a>) is a key<a id="_idIndexMarker513"/> component in this chapter’s dynamic RAG scenario because it is fully integrated with Chroma. ONNX is a standard format for representing <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) models<a id="_idIndexMarker514"/> designed to enable models to be used across different frameworks and hardware without being locked into one ecosystem.</p>
    <p class="normal">We will be using ONNX Runtime, which is a performance-focused engine for running ONNX models. It acts as a cross-platform accelerator for ML models, providing a flexible interface that allows integration with hardware-specific libraries. This makes it possible to optimize the models for various hardware configurations (CPUs, GPUs, and other accelerators). As for Hugging Face, it is recommended to activate a GPU if you have access to one for the program in this chapter. Also, we will select a model included within ONNX Runtime installation packages.</p>
    <p class="normal">We have now installed the Hugging Face and Chroma resources we need, including ONNX Runtime. Hugging Face’s framework is used throughout the model life cycle, from accessing and deploying<a id="_idIndexMarker515"/> pre-trained models to training and fine-tuning them within its ecosystem. ONNX, among its many features, can intervene in the post-training phase to ensure a model’s compatibility and efficient execution across different hardware and software setups. Models <a id="_idIndexMarker516"/>might be developed and fine-tuned using Hugging Face’s tools and then converted to the ONNX format for broad, optimized deployment using ONNX Runtime.</p>
    <p class="normal">We will now use spaCy to compute the accuracy between the response we obtain when querying our vector store and the original completion text. The following command installs a medium-sized English language model from spaCy, tailored for general NLP tasks:</p>
    <pre class="programlisting code"><code class="hljs-code">!python -m spacy download en_core_web_md
</code></pre>
    <p class="normal">This model, labeled <code class="inlineCode">en_core_web_md</code>, originates from web text in English and is balanced for speed and accuracy, which we need for dynamic RAG. It is efficient for computing text similarity. You may need to restart the session once the package is installed.</p>
    <p class="normal">We have now successfully installed the open-source, optimized, cost-effective resources we need for dynamic RAG and are ready to start running the program’s core.</p>
    <h1 id="_idParaDest-208" class="heading-1">Activating session time</h1>
    <p class="normal">When working in real-life<a id="_idIndexMarker517"/> dynamic RAG projects, such as in this scenario, time is <a id="_idIndexMarker518"/>essential! For example, if the daily decision-making meeting is at 10 a.m., the RAG preparation team might have to start preparing for this meeting at 8 a.m. to gather the data online, in processed company data batches, or in any other way necessary for the meeting’s goal.</p>
    <p class="normal">First, activate a GPU if one is available. On Google Colab, for example, go to <strong class="screenText">Runtime</strong> | <strong class="screenText">Change runtime type</strong> and select a GPU if possible and available. If not, the notebook will take a bit longer but will run on a CPU. Then, go through each section in this chapter, running the notebook cell by cell to understand the process in depth.</p>
    <p class="normal">The following <a id="_idIndexMarker519"/>code activates a measure of the session time once the environment is<a id="_idIndexMarker520"/> installed all the way to the end of the notebook:</p>
    <pre class="programlisting code"><code class="hljs-code"># Start timing before the request
session_start_time = time.time()
</code></pre>
    <p class="normal">Finally, restart the session, go to <strong class="screenText">Runtime</strong> again, and click on <strong class="screenText">Run all</strong>. Once the program is finished, go to <strong class="screenText">Total session time</strong>, the last section of the notebook. You will have an estimate of how long it takes for a preparation run. With the time left before a daily meeting, you can tweak the data, queries, and model parameters for your needs a few times.</p>
    <p class="normal">This on-the-fly dynamic RAG approach will make any team that has these skills a precious asset in this fast-moving world. We will start the core of the program by downloading and preparing the dataset.</p>
    <h1 id="_idParaDest-209" class="heading-1">Downloading and preparing the dataset</h1>
    <p class="normal">We will use the SciQ dataset created by Welbl, Liu, and Gardner (2017) with a method for generating high-quality, domain-specific multiple-choice science questions via <em class="italic">crowdsourcing</em>. The <a id="_idIndexMarker521"/>SciQ dataset consists of 13,679 multiple-choice questions crafted to aid the training of NLP models for science exams. The<a id="_idIndexMarker522"/> creation process involves two main steps: selecting relevant passages and generating questions with plausible distractors.</p>
    <p class="normal">In the context of using this dataset for an augmented generation of questions through a Chroma collection, we will implement the <code class="inlineCode">question</code>, <code class="inlineCode">correct_answer</code>, and <code class="inlineCode">support</code> columns. The dataset also contains <code class="inlineCode">distractor</code> columns with wrong answers, which we will drop.</p>
    <p class="normal">We will integrate the prepared dataset into a retrieval system that utilizes query augmentation techniques to enhance the retrieval of relevant questions based on specific scientific topics or question<a id="_idIndexMarker523"/> formats for Hugging Face’s Llama model. This will allow for the dynamic <a id="_idIndexMarker524"/>generation of augmented, real-time completions for Llama, as implemented in the chapter’s program. The program loads the training data from the <code class="inlineCode">sciq</code> dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"># Import required libraries
from datasets import load_dataset
import pandas as pd
# Load the SciQ dataset from HuggingFace
dataset = load_dataset("sciq", split="train")
</code></pre>
    <p class="normal">The dataset is filtered to detect the non-empty <code class="inlineCode">support</code> and <code class="inlineCode">correct_answer</code> columns:</p>
    <pre class="programlisting code"><code class="hljs-code"># Filter the dataset to include only questions with support and correct answer
filtered_dataset = dataset.filter(lambda x: x["support"] != "" and x["correct_answer"] != "")
</code></pre>
    <p class="normal">We will now display the number of rows filtered:</p>
    <pre class="programlisting code"><code class="hljs-code"># Print the number of questions with support
print("Number of questions with support: ", len(filtered_dataset))
</code></pre>
    <p class="normal">The output shows that we have 10,481 documents:</p>
    <pre class="programlisting con"><code class="hljs-con">Number of questions with support:  10481
</code></pre>
    <p class="normal">We need to clean the DataFrame to focus on the columns we need. Let’s drop the distractors (wrong answers to the questions):</p>
    <pre class="programlisting code"><code class="hljs-code"># Convert the filtered dataset to a pandas DataFrame
df = pd.DataFrame(filtered_dataset)
# Columns to drop
columns_to_drop = ['distractor3', 'distractor1', 'distractor2']
# Dropping the columns from the DataFrame
df.drop(columns=columns_to_drop, inplace=True)
</code></pre>
    <p class="normal">We have the correct answer and the support content that we will now merge:</p>
    <pre class="programlisting code"><code class="hljs-code"># Create a new column 'completion' by merging 'correct_answer' and 'support'
df['completion'] = df['correct_answer'] + " because " + df['support']
# Ensure no NaN values are in the 'completion' column
df.dropna(subset=['completion'], inplace=True)
df
</code></pre>
    <p class="normal">The output shows the columns we<a id="_idIndexMarker525"/> need to prepare the data for retrieval in the completion columns, as <a id="_idIndexMarker526"/>shown in the excerpt of the DataFrame for a completion field in which <code class="inlineCode">aerobic</code> is the correct answer because it is the connector and the rest of the text is the support content for the correct answer:</p>
    <pre class="programlisting con"><code class="hljs-con">aerobic because "Cardio" has become slang for aerobic exercise that raises your heart rate for an extended amount of time. Cardio can include biking, running, or swimming. Can you guess one of the main organs of the cardiovascular system? Yes, your heart.
</code></pre>
    <p class="normal">The program now displays the shape of the DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">df.shape
</code></pre>
    <p class="normal">The output shows we still have all the initial lines and four columns:</p>
    <pre class="programlisting con"><code class="hljs-con">(10481, 4)
</code></pre>
    <p class="normal">The following code will display the names of the columns:</p>
    <pre class="programlisting code"><code class="hljs-code"># Assuming 'df' is your DataFrame
print(df.columns)
</code></pre>
    <p class="normal">As a result, the output displays the four columns we need:</p>
    <pre class="programlisting con"><code class="hljs-con">Index(['question', 'correct_answer', 'support', 'completion'], dtype='object')
</code></pre>
    <p class="normal">The data is now ready to be embedded and upserted.</p>
    <h1 id="_idParaDest-210" class="heading-1">Embedding and upserting the data in a Chroma collection</h1>
    <p class="normal">We will begin by creating the Chroma client and defining a collection name:</p>
    <pre class="programlisting code"><code class="hljs-code"># Import Chroma and instantiate a client. The default Chroma client is ephemeral, meaning it will not save to disk.
import chromadb
client = chromadb.Client()
collection_name="sciq_supports6"
</code></pre>
    <p class="normal">Before creating the collection and upserting the data to the collection, we need to verify whether the collection already exists or not:</p>
    <pre class="programlisting code"><code class="hljs-code"># List all collections
collections = client.list_collections()
# Check if the specific collection exists
collection_exists = any(collection.name == collection_name for collection in collections)
print("Collection exists:", collection_exists)
</code></pre>
    <p class="normal">The output will return <code class="inlineCode">True</code> if the collection exists and <code class="inlineCode">False</code> if it doesn’t:</p>
    <pre class="programlisting con"><code class="hljs-con">Collection exists: False
</code></pre>
    <p class="normal">If the collection doesn’t exist, we will create a collection with <code class="inlineCode">collection_name</code> defined earlier:</p>
    <pre class="programlisting code"><code class="hljs-code"># Create a new Chroma collection to store the supporting evidence. We don't need to specify an embedding function, and the default will be used.
if collection_exists!=True:
  collection = client.create_collection(collection_name)
else:
  print("Collection ", collection_name," exists:", collection_exists)
</code></pre>
    <p class="normal">Let’s peek into the structure of the dictionary of the collection we created:</p>
    <pre class="programlisting code"><code class="hljs-code">#Printing the dictionary
results = collection.get()
for result in results:
    print(result)  # This will print the dictionary for each item
</code></pre>
    <p class="normal">The output displays the dictionary of each item of the collection:</p>
    <pre class="programlisting con"><code class="hljs-con">ids
embeddings
metadatas
documents
uris
data
included
</code></pre>
    <p class="normal">Let’s briefly go through the three key fields for our scenario:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ids</code>: This field represents the unique identifiers for each item in the collection.</li>
      <li class="bulletList"><code class="inlineCode">embeddings</code>: Embeddings are the embedded vectors of the documents.</li>
      <li class="bulletList"><code class="inlineCode">documents</code>: This refers to the <code class="inlineCode">completion</code> column in which we merged the correct answer and the support content.</li>
    </ul>
    <p class="normal">We now need a lightweight rapid LLM model for our dynamic RAG environment.</p>
    <h2 id="_idParaDest-211" class="heading-2">Selecting a model</h2>
    <p class="normal">Chroma will initialize a default model, which can be <code class="inlineCode">all-MiniLM-L6-v2</code>. However, let’s make sure we are using this model and initialize it:</p>
    <pre class="programlisting code"><code class="hljs-code">model_name = "all-MiniLM-L6-v2"  # The name of the model to use for embedding and querying
</code></pre>
    <p class="normal">The <code class="inlineCode">all-MiniLM-L6-v2</code> model was designed with an optimal, enhanced method by Wang et al. (2021) for model compression, focusing on distilling self-attention relationships between components of transformer models. This approach is flexible in the number of attention heads between teacher and student models, improving compression efficiency. The model is fully integrated into Chroma with ONNX, as explained in the <em class="italic">Installing the environment</em> section of this chapter.</p>
    <p class="normal">The magic of this <code class="inlineCode">MiniLM</code> model is based on compression and knowledge distillation through a teacher model and the student model:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Teacher model</strong>: This is the original, typically larger and more complex model such as BERT, RoBERTa, and XLM-R, in our case, that has been pre-trained on a comprehensive dataset. The teacher model possesses high accuracy and a deep understanding of the tasks it has been trained on. It serves as the source of knowledge that we aim to transfer.</li>
      <li class="bulletList"><strong class="keyWord">Student model</strong>: This is our smaller, less complex model, <code class="inlineCode">all-MiniLM-L6-v2</code>, which is trained to mimic the teacher model’s behavior, which will prove very effective for our dynamic RAG architecture. The goal is to have the student model replicate the performance of the teacher model as closely as possible but with significantly fewer parameters or computational expense.</li>
    </ul>
    <p class="normal">In our case, <code class="inlineCode">all-MiniLM-L6-v2</code> will accelerate the embedding and querying process. We can see that in the age of superhuman LLM models, such as GPT-4o, we can perform daily tasks with smaller compressed and distilled models. Let’s embed the data next.</p>
    <h2 id="_idParaDest-212" class="heading-2">Embedding and storing the completions</h2>
    <p class="normal">Embedding and upserting data in a Chroma collection is seamless and concise. In this scenario, we’ll embed and upsert the whole <code class="inlineCode">df</code> completions in a <code class="inlineCode">completion_list</code> extracted from our <code class="inlineCode">df</code> dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">ldf=len(df)
nb=ldf  # number of questions to embed and store
import time
start_time = time.time()  # Start timing before the request
# Convert Series to list of strings
completion_list = df["completion"][:nb].astype(str).tolist()
</code></pre>
    <p class="normal">We use the <code class="inlineCode">collection_exists</code> status we defined when creating the collection to avoid loading the data twice. In this scenario, the collection is temporary; we just want to load it once and use it once. If you try to load the data in this temporary scenario a second time, you will get warnings. However, you can modify the code if you wish to try different datasets and methods, such as preparing a prototype at full speed for another project.</p>
    <p class="normal">In any case, in this scenario, we first check if the collection exists and then upsert the <code class="inlineCode">ids</code> and <code class="inlineCode">documents</code> in the <code class="inlineCode">complete_list</code> and store the <code class="inlineCode">type</code> of data, which is <code class="inlineCode">completion</code>, in the <code class="inlineCode">metadatas</code> field:</p>
    <pre class="programlisting code"><code class="hljs-code"># Avoiding trying to load data twice in this one run dynamic RAG notebook
if collection_exists!=True:
  # Embed and store the first nb supports for this demo
  collection.add(
      ids=[str(i) for i in range(0, nb)],  # IDs are just strings
      documents=completion_list,
      metadatas=[{"type": "completion"} for _ in range(0, nb)],
  )
</code></pre>
    <p class="normal">Finally, we measure the response time:</p>
    <pre class="programlisting code"><code class="hljs-code">response_time = time.time() - start_time  # Measure response time
print(f"Response Time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The output shows that, in this case, Chroma activated the default model through <code class="inlineCode">onnx</code>, as explained in the introduction of this section and also in the <em class="italic">Installing the environment</em> section of this chapter:</p>
    <pre class="programlisting con"><code class="hljs-con">/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02&lt;00:00, 31.7MiB/s]
</code></pre>
    <p class="normal">The output also shows that the processing time for 10,000+ documents is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">Response Time: 234.25 seconds
</code></pre>
    <p class="normal">The response time might vary and depends on whether you are using a GPU. When using an accessible GPU, the time fits the needs required for dynamic RAG scenarios.</p>
    <p class="normal">With that, the Chroma vector store is now populated. Let’s take a peek at the embeddings.</p>
    <h2 id="_idParaDest-213" class="heading-2">Displaying the embeddings</h2>
    <p class="normal">The program now fetches the embeddings and displays the first one:</p>
    <pre class="programlisting code"><code class="hljs-code"># Fetch the collection with embeddings included
result = collection.get(include=['embeddings'])
# Extract the first embedding from the result
first_embedding = result['embeddings'][0]
# If you need to work with the length or manipulate the first embedding:
embedding_length = len(first_embedding)
print("First embedding:", first_embedding)
print("Embedding length:", embedding_length)
</code></pre>
    <p class="normal">The output shows that our completions have been vectorized, as we can see in the first embedding:</p>
    <pre class="programlisting con"><code class="hljs-con">First embedding: [0.03689068928360939, -0.05881563201546669, -0.04818134009838104,…
</code></pre>
    <p class="normal">The output also displays the embedding length, which is interesting:</p>
    <pre class="programlisting con"><code class="hljs-con">Embedding length: 384
</code></pre>
    <p class="normal">The <code class="inlineCode">all-MiniLM-L6-v2</code> model reduces the complexity of text data by mapping sentences and paragraphs into a 384-dimensional space. This is significantly lower than the typical dimensionality of one-hot encoded vectors, such as the 1,526 dimensions of the OpenAI <code class="inlineCode">text-embedding-ada-002</code>. This shows that <code class="inlineCode">all-MiniLM-L6-v2</code> uses dense vectors, which use all dimensions of the vector space to encode information to produce nuanced semantic relationships between different documents as opposed to sparse vectors.</p>
    <p class="normal">Sparse vector models, such as the <strong class="keyWord">bag-of-words</strong> (<strong class="keyWord">BoW</strong>) model, can be effective in some cases. However, their main limitation is that they don’t capture the order of words or the context around them, which can be crucial for understanding the meaning of text when training LLMs.</p>
    <p class="normal">We have now embedded the documents into dense vectors in a smaller dimensional space than full-blown LLMs and will produce satisfactory results.</p>
    <h1 id="_idParaDest-214" class="heading-1">Querying the collection</h1>
    <p class="normal">The code in this section <a id="_idIndexMarker527"/>executes a query against the Chroma vector store using its integrated semantic search functionality. It queries the vector representations of all the vectors in the Chroma collection questions in the initial dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">dataset["question"][:nbq].
</code></pre>
    <p class="normal">The query requests one most relevant or similar document for each question with <code class="inlineCode">n_results=1</code>, which you can modify if you wish.</p>
    <p class="normal">Each question text is converted into a vector. Then, Chroma runs a vector similarity search by comparing the embedded vectors against our database of document vectors to find the closest <a id="_idIndexMarker528"/>match based on vector similarity:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
start_time = time.time()  # Start timing before the request
# number of retrievals to write
results = collection.query(
    query_texts=df["question"][:nb],
    n_results=1)
response_time = time.time() - start_time  # Measure response time
print(f"Response Time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The output displays a satisfactory response time for the 10,000+ queries:</p>
    <pre class="programlisting con"><code class="hljs-con">Response Time: 199.34 seconds
</code></pre>
    <p class="normal">We will now analyze the 10,000+ queries. We will use spaCy to evaluate a query’s result and compare it with the original completion. We first load the spaCy model we installed in the <em class="italic">Installing the environment</em> section of this chapter:</p>
    <pre class="programlisting code"><code class="hljs-code">import spacy
import numpy as np
# Load the pre-trained spaCy language model
nlp = spacy.load('en_core_web_md')  # Ensure that you've installed this model with 'python -m spacy download en_core_web_md'
</code></pre>
    <p class="normal">The program then creates a similarity function that takes two arguments (the original completion, <code class="inlineCode">text1</code>, and the retrieved text, <code class="inlineCode">text2</code>) and returns the similarity value:</p>
    <pre class="programlisting code"><code class="hljs-code">def simple_text_similarity(text1, text2):
    # Convert the texts into spaCy document objects
    doc1 = nlp(text1)
    doc2 = nlp(text2)
   
    # Get the vectors for each document
    vector1 = doc1.vector
    vector2 = doc2.vector
   
    # Compute the cosine similarity between the two vectors
    # Check for zero vectors to avoid division by zero
    if np.linalg.norm(vector1) == 0 or np.linalg.norm(vector2) == 0:
        return 0.0  # Return zero if one of the texts does not have a vector representation
    else:
        similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))
        return similarity
</code></pre>
    <p class="normal">We will now perform a full validation run on the 10,000 queries. As can be seen in the following code block, the validation begins by defining the variables we will need:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">nbqd</code> to only display the first 100 and last 100 results.</li>
      <li class="bulletList"><code class="inlineCode">acc_counter</code> measures the results with a similarity score superior to 0.5, which you can <a id="_idIndexMarker529"/>modify to fit your needs.</li>
      <li class="bulletList"><code class="inlineCode">display_counter</code> to count the number of results we have displayed:</li>
    </ul>
    <pre class="programlisting code"><code class="hljs-code">nbqd = 100  # the number of responses to display, supposing there are more than 100 records
# Print the question, the original completion, the retrieved document, and compare them
acc_counter=0
display_counter=0
</code></pre>
    <p class="normal">The program goes through <code class="inlineCode">nb</code> results, which, in our case, is the total length of our dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">for i, q in enumerate(df['question'][:nb]):
    original_completion = df['completion'][i]  # Access the original completion for the question
    retrieved_document = results['documents'][i][0]  # Retrieve the corresponding document
    similarity_score = simple_text_similarity(original_completion, retrieved_document)
</code></pre>
    <p class="normal">The code accesses the original completion and stores it in <code class="inlineCode">original_completion</code>. Then, it retrieves the result and stores it in <code class="inlineCode">retrieved_document</code>. Finally, it calls the similarity function we defined, <code class="inlineCode">simple_text_similarity</code>. The original completion and the retrieved document store the similarity score in <code class="inlineCode">similarity_score</code>.</p>
    <p class="normal">Now, we introduce an accuracy metric. In this scenario, the threshold of the similarity score is set to <code class="inlineCode">0.7</code>, which is reasonable:</p>
    <pre class="programlisting code"><code class="hljs-code">    if similarity_score &gt; 0.7:
      acc_counter+=1
</code></pre>
    <p class="normal">If <code class="inlineCode">similarity_score &gt; 0.7</code>, then the accuracy counter, <code class="inlineCode">acc_counter</code>, is incremented. The display counter, <code class="inlineCode">display_counter</code>, is also incremented to only the first and last <code class="inlineCode">nbqd</code> (maximum results <a id="_idIndexMarker530"/>to display) defined at the beginning of this function:</p>
    <pre class="programlisting code"><code class="hljs-code">    display_counter+=1
    if display_counter&lt;=nbqd or display_counter&gt;nb-nbqd:
</code></pre>
    <p class="normal">The information displayed provides insights into the performance of the system:</p>
    <pre class="programlisting code"><code class="hljs-code">      print(i," ", f"Question: {q}")
      print(f"Retrieved document: {retrieved_document}")
      print(f"Original completion: {original_completion}")
      print(f"Similarity Score: {similarity_score:.2f}")
      print()  # Blank line for better readability between entries
</code></pre>
    <p class="normal">The output displays four key variables:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">{q}</code> is the question asked, the query.</li>
      <li class="bulletList"><code class="inlineCode">{retrieved_document}</code> is the document retrieved.</li>
      <li class="bulletList"><code class="inlineCode">{original_completion}</code> is the original document in the dataset.</li>
      <li class="bulletList"><code class="inlineCode">{similarity_score:.2f}</code> is the similarity score between the original document and the document retrieved to measure the performance of each response.</li>
    </ul>
    <p class="normal">The first output provides the information required for a human observer to control the result of the query and trace it back to the source.</p>
    <p class="normal">The first part of the output is the question, the query:</p>
    <pre class="programlisting con"><code class="hljs-con">Question: What type of organism is commonly used in preparation of foods such as cheese and yogurt?
</code></pre>
    <p class="normal">The second part of the output is the retrieved document:</p>
    <pre class="programlisting con"><code class="hljs-con">Retrieved document: lactic acid because Bacteria can be used to make cheese from milk. The bacteria turn the milk sugars into lactic acid. The acid is what causes the milk to curdle to form cheese. Bacteria are also involved in producing other foods. Yogurt is made by using bacteria to ferment milk ( Figure below ). Fermenting cabbage with bacteria produces sauerkraut.
</code></pre>
    <p class="normal">The third part of <a id="_idIndexMarker531"/>the output is the original completion. In this case, we can see that the retrieved document provides relevant information but not the exact original completion:</p>
    <pre class="programlisting con"><code class="hljs-con">Original completion: mesophilic organisms because Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.
</code></pre>
    <p class="normal">Finally, the output displays the similarity score calculated by spaCy:</p>
    <pre class="programlisting con"><code class="hljs-con">Similarity Score: 0.73
</code></pre>
    <p class="normal">The score shows that although the original completion was not selected, the completion selected is relevant.</p>
    <p class="normal">When all the results have been analyzed, the program calculates the accuracy obtained for the 10,000+ queries:</p>
    <pre class="programlisting code"><code class="hljs-code">if nb&gt;0:
  acc=acc_counter/nb
</code></pre>
    <p class="normal">The calculation is based on the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Acc</code> is the overall accuracy obtained</li>
      <li class="bulletList"><code class="inlineCode">acc_counter</code> is the total of <code class="inlineCode">Similarity</code> <code class="inlineCode">scores &gt; 0.7</code></li>
      <li class="bulletList"><code class="inlineCode">nb</code> is the number of queries. In this case, <code class="inlineCode">nb=len(df)</code> </li>
      <li class="bulletList"><code class="inlineCode">acc=acc_counter/nb</code> calculates the overall accuracy of all the results</li>
    </ul>
    <p class="normal">The code then displays the number of documents measured and the overall similarity score:</p>
    <pre class="programlisting code"><code class="hljs-code">  print(f"Number of documents: {nb:.2f}")
  print(f"Overall similarity score: {acc:.2f}")
</code></pre>
    <p class="normal">The output shows that all the questions returned relevant results:</p>
    <pre class="programlisting con"><code class="hljs-con">Number of documents: 10481.00
Overall similarity score: 1.00
</code></pre>
    <p class="normal">This satisfactory overall similarity score shows that the system works in a closed environment. But <a id="_idIndexMarker532"/>we need to go further and see what happens in the open environment of heated discussions in a meeting!</p>
    <h1 id="_idParaDest-215" class="heading-1">Prompt and retrieval</h1>
    <p class="normal">This section is the one to use during real-time querying meetings. You can adapt the interface to your needs. We’ll focus on functionality.</p>
    <p class="normal">Let’s look at the first<a id="_idIndexMarker533"/> prompt:</p>
    <pre class="programlisting code"><code class="hljs-code"># initial question
prompt = "Millions of years ago, plants used energy from the sun to form what?"
# variant 1 similar
#prompt = "Eons ago, plants used energy from the sun to form what?"
# variant 2 divergent
#prompt = "Eons ago, plants used sun energy to form what?"
</code></pre>
    <p class="normal">You will notice that there are two commented variants under the first prompt. Let’s clarify this:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">initial question</code> is the exact text that comes from the initial dataset. It isn’t likely that an attendee in the meeting or a user will ask the question that way. But we can use it to verify if the system is working.</li>
      <li class="bulletList"><code class="inlineCode">variant 1</code> is similar to the initial question and could be asked.</li>
      <li class="bulletList"><code class="inlineCode">variant 2</code> diverges and may prove challenging.</li>
    </ul>
    <p class="normal">We will select <code class="inlineCode">variant 1</code> for this section and we should obtain a satisfactory result.</p>
    <p class="normal">We can see that, as for all AI programs, human control is mandatory! The more <code class="inlineCode">variant 2</code> diverges with spontaneous questions, the more challenging it becomes for the system to remain stable and respond as we expect. This limit explains why, even if a dynamic RAG system can adapt rapidly, designing a solid system will require careful and continual improvements.</p>
    <p class="normal">If we query the collection as we did in the previous section with one prompt only this time, we will obtain a response rapidly:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
import textwrap
# Start timing before the request
start_time = time.time()
# Query the collection using the prompt
results = collection.query(
    query_texts=[prompt],  # Use the prompt in a list as expected by the query method
    n_results=1  # Number of results to retrieve
)
# Measure response time
response_time = time.time() - start_time
# Print response time
print(f"Response Time: {response_time:.2f} seconds\n")
# Check if documents are retrieved
if results['documents'] and len(results['documents'][0]) &gt; 0:
    # Use textwrap to format the output for better readability
    wrapped_question = textwrap.fill(prompt, width=70)  # Wrap text at 70 characters
    wrapped_document = textwrap.fill(results['documents'][0][0], width=70)
    # Print formatted results
    print(f"Question: {wrapped_question}")
    print("\n")
    print(f"Retrieved document: {wrapped_document}")
    print()
else:
    print("No documents retrieved."
</code></pre>
    <p class="normal">The response <a id="_idIndexMarker534"/>time is rapid:</p>
    <pre class="programlisting con"><code class="hljs-con">Response Time: 0.03 seconds
</code></pre>
    <p class="normal">The output shows that the retrieved document is relevant:</p>
    <pre class="programlisting con"><code class="hljs-con">Response Time: 0.03 seconds
Question: Millions of years ago, plants used energy from the sun to form what?
Retrieved document: chloroplasts because When ancient plants underwent photosynthesis,
they changed energy in sunlight to stored chemical energy in food. The
plants used the food and so did the organisms that ate the plants.
After the plants and other organisms died, their remains gradually
changed to fossil fuels as they were covered and compressed by layers
of sediments. Petroleum and natural gas formed from ocean organisms
and are found together. Coal formed from giant tree ferns and other
swamp plants.
</code></pre>
    <p class="normal">We have successfully retrieved the result of our query. This semantic vector search might even be enough if the <a id="_idIndexMarker535"/>attendees of the meeting are satisfied with it. You will always have time to improve the configuration of RAG with Llama.</p>
    <p class="normal">Hugging Face Llama will now take this response and write a brief NLP summary.</p>
    <h1 id="_idParaDest-216" class="heading-1">RAG with Llama</h1>
    <p class="normal">We initialized <code class="inlineCode">meta-llama/Llama-2-7b-chat-hf</code> in the <em class="italic">Installing the environment</em> section. We must now <a id="_idIndexMarker536"/>create a function to configure Llama 2’s<a id="_idIndexMarker537"/> behavior:</p>
    <pre class="programlisting code"><code class="hljs-code">def LLaMA2(prompt):
    sequences = pipeline(
        prompt,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        max_new_tokens=100, # Control the output length more granularly
        temperature=0.5,  # Slightly higher for more diversity
        repetition_penalty=2.0,  # Adjust based on experimentation
        truncation=True
    )
    return sequences
</code></pre>
    <p class="normal">You can tweak each parameter to your expectations:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">prompt</code>: The input text that the model uses to generate the output. It’s the starting point for the model’s response.</li>
      <li class="bulletList"><code class="inlineCode">do_sample</code>: A Boolean value (<code class="inlineCode">True</code> or <code class="inlineCode">False</code>). When set to <code class="inlineCode">True</code>, it enables stochastic sampling, meaning the model will pick tokens randomly based on their probability distribution, allowing for more varied outputs.</li>
      <li class="bulletList"><code class="inlineCode">top_k</code>: This parameter limits the<a id="_idIndexMarker538"/> number of highest-probability vocabulary tokens to consider when selecting tokens in the sampling process. Setting it to <code class="inlineCode">10</code> means the model will <a id="_idIndexMarker539"/>choose from the top 10 most likely next tokens.</li>
      <li class="bulletList"><code class="inlineCode">num_return_sequences</code>: Specifies the number of independently generated responses to return. Here, it is set to <code class="inlineCode">1</code>, meaning the function will return one sequence for each prompt.</li>
      <li class="bulletList"><code class="inlineCode">eos_token_id</code>: This token marks the end of a sequence in tokenized form. Once it is generated, the model stops generating further tokens. The end-of-sequence token is an <code class="inlineCode">id</code> that points to Llama’s <code class="inlineCode">eos_token</code>.</li>
      <li class="bulletList"><code class="inlineCode">max_new_tokens</code>: Limits the number of new tokens the model can generate. Set to <code class="inlineCode">100</code> here, it constrains the output to a maximum length of 100 tokens beyond the input prompt length.</li>
      <li class="bulletList"><code class="inlineCode">temperature</code>: This controls randomness in the sampling process. A temperature of <code class="inlineCode">0.5</code> makes the model’s responses less random and more focused than a higher temperature but still allows for some diversity.</li>
      <li class="bulletList"><code class="inlineCode">repetition_penalty</code>: A modifier that discourages the model from repeating the same token. A penalty of <code class="inlineCode">2.0</code> means any token already used is less likely to be chosen again, promoting more diverse and less repetitive text.</li>
      <li class="bulletList"><code class="inlineCode">truncation</code>: When enabled, it ensures the output does not exceed the maximum length specified by <code class="inlineCode">max_new_tokens</code> by cutting off excess tokens.</li>
    </ul>
    <p class="normal">The prompt will contain the instruction for Llama in <code class="inlineCode">iprompt</code> and the result obtained in the <em class="italic">Prompt and retrieval</em> section of the notebook. The result is appended to <code class="inlineCode">iprompt</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">iprompt='Read the following input and write a summary for beginners.'
lprompt=iprompt + " " + results['documents'][0][0]
</code></pre>
    <p class="normal">The augmented input for the Llama call is <code class="inlineCode">lprompt</code>. The code will measure the time it takes and make the completion request:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
start_time = time.time()  # Start timing before the request
response=LLaMA2(lprompt)
</code></pre>
    <p class="normal">We now retrieve the generated text from the response and display the time it took for Llama to respond:</p>
    <pre class="programlisting code"><code class="hljs-code">for seq in response:
    generated_part = seq['generated_text'].replace(iprompt, '')  # Remove the input part from the output
  
response_time = time.time() - start_time  # Measure response time
print(f"Response Time: {response_time:.2f} seconds")  # Print response timeLe
</code></pre>
    <p class="normal">The output shows that<a id="_idIndexMarker540"/> Llama returned the completion in a reasonable<a id="_idIndexMarker541"/> time:</p>
    <pre class="programlisting con"><code class="hljs-con">Response Time: 5.91 seconds
</code></pre>
    <p class="normal">Let’s wrap the response in a nice format to display it:</p>
    <pre class="programlisting code"><code class="hljs-code">wrapped_response = textwrap.fill(response[0]['generated_text'], width=70)
print(wrapped_response)
</code></pre>
    <p class="normal">The output displays a technically reasonable completion:</p>
    <pre class="programlisting con"><code class="hljs-con">chloroplasts because When ancient plants underwent photosynthesis,
they changed energy in sunlight to stored chemical energy in food. The
plants used the food and so did the organisms that ate the plants.
After the plants and other organisms died, their remains gradually
changed to fossil fuels as they were covered and compressed by layers
of sediments. Petroleum and natural gas formed from ocean organisms
and are found together. Coal formed from giant tree ferns and other
swamp plants. Natural Gas: 10% methane (CH4) - mostly derived from
anaerobic decomposition or fermentation processes involving
microorganism such As those present In wetlands; also contains smaller
amounts Of ethene(C2H6), propiene/propadiene/( C3 H5-7). This is where
most petrol comes frm! But there're more complex hydrocarbons like
pentanes &amp; hexans too which can come
</code></pre>
    <p class="normal">The summary produced by Llama is technically acceptable. To obtain another, possibly better result, as long as the session is not closed, the user can run a query and an augmented generation several times with different Llama parameters.</p>
    <p class="normal">You can even try another LLM. Dynamic RAG doesn’t necessarily have to be 100% open-source. If necessary, we must be pragmatic and introduce whatever it takes. For example, the following prompt <a id="_idIndexMarker542"/>was submitted to ChatGPT with GPT-4o, which is the result of the query we <a id="_idIndexMarker543"/>used for Llama:</p>
    <pre class="programlisting code"><code class="hljs-code">Write a nice summary with this text: Question: Millions of years ago, plants used energy from the sun to form what?
Retrieved document: chloroplasts because When ancient plants underwent photosynthesis,
they changed energy in sunlight to stored chemical energy in food. The plants used the food and so did the organisms that ate the plants. After the plants and other organisms died, their remains gradually
changed to fossil fuels as they were covered and compressed by layers of sediments. Petroleum and natural gas formed from ocean organisms and are found together. Coal formed from giant tree ferns and other swamp plants.
</code></pre>
    <p class="normal">The output of OpenAI GPT-4o surpasses Llama 2 in this case and produces a satisfactory output:</p>
    <pre class="programlisting con"><code class="hljs-con">Millions of years ago, plants harnessed energy from the sun through photosynthesis to produce food, storing chemical energy. This energy was vital for the plants themselves and for the organisms that consumed them. Over time, the remains of these plants and animals, buried under sediment, transformed into fossil fuels. Ocean organisms' remains contributed to the formation of petroleum and natural gas, often found together, while the remains of giant tree ferns and swamp plants formed coal.
</code></pre>
    <p class="normal">If necessary, you can replace <code class="inlineCode">meta-llama/Llama-2-7b-chat-hf</code> with GPT-4o, as implemented in <em class="chapterRef">Chapter 4</em>, <em class="italic">Multimodal Modular RAG for Drone Technology</em>, and configure it to obtain this level of output. The only rule in dynamic RAG is performance. With that, we’ve seen that there are many ways to implement dynamic RAG.</p>
    <p class="normal">Once the session is over, we can delete it.</p>
    <h2 id="_idParaDest-217" class="heading-2">Deleting the collection</h2>
    <p class="normal">You can manually<a id="_idIndexMarker544"/> delete the collection with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">#client.delete_collection(collection_name)
</code></pre>
    <p class="normal">You can also close the session to delete the temporary dynamic RAG collection created. We can check and see whether the collection we created, <code class="inlineCode">collection_name</code>, still exists or not:</p>
    <pre class="programlisting code"><code class="hljs-code"># List all collections
collections = client.list_collections()
# Check if the specific collection exists
collection_exists = any(collection.name == collection_name for collection in collections)
print("Collection exists:", collection_exists)
</code></pre>
    <p class="normal">If we are still working on a collection in a session, the response will be <code class="inlineCode">True</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">Collection exists: True
</code></pre>
    <p class="normal">If we delete the<a id="_idIndexMarker545"/> collection with code or by closing the session, the response will be <code class="inlineCode">False</code>. Let’s take a look at the total session time.</p>
    <h1 id="_idParaDest-218" class="heading-1">Total session time</h1>
    <p class="normal">The following code measures <a id="_idIndexMarker546"/>the time between the beginning of the session and immediately after the <em class="italic">Installing the environment</em> section:</p>
    <pre class="programlisting code"><code class="hljs-code">end_time = time.time() - session_start_time  # Measure response time
print(f"Session preparation time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The output can have two meanings:</p>
    <ul>
      <li class="bulletList">It can measure the time we worked on the preparation of the dynamic RAG scenario with the daily dataset for the Chroma collection, querying, and summarizing by Llama.</li>
      <li class="bulletList">It can measure the time it took to run the whole notebook without intervening at all.</li>
    </ul>
    <p class="normal">In this case, the session time is the result of a full run with no human intervention:</p>
    <pre class="programlisting con"><code class="hljs-con">Session preparation time: 780.35 seconds
</code></pre>
    <p class="normal">The whole process takes less than 15 minutes, which fits the constraints of the preparation time in a dynamic RAG scenario. It leaves room for a few runs to tweak the system before the<a id="_idIndexMarker547"/> meeting. With that, we have successfully walked through a dynamic RAG process and will now summarize our journey.</p>
    <h1 id="_idParaDest-219" class="heading-1">Summary</h1>
    <p class="normal">In a fast-evolving world, gathering information rapidly for decision-making provides a competitive advantage. Dynamic RAG is one way to bring AI into meeting rooms with rapid and cost-effective AI. We built a system that simulated the need to obtain answers to hard science questions in a daily meeting. After installing and analyzing the environment, we downloaded and prepared the SciQ dataset, a science question-and-answer dataset, to simulate a daily meeting during which hard science questions would be asked. The attendees don’t want to spend their time searching the web and wasting their time when decisions must be made. This could be for a marketing campaign, fact-checking an article, or any other situation in which hard science knowledge is required.</p>
    <p class="normal">We created a Chroma collection vector store. We then embedded 10,000+ documents and inserted data and vectors into the Chroma vector store on our machine with <code class="inlineCode">all-MiniLM-L6-v2</code>. The process proved cost-effective and sufficiently rapid. The collection was created locally, so there is no storage cost. The collection is temporary, so there is no useless space usage or cluttering. We then queried the collection to measure the accuracy of the system we set up. The results were satisfactory, so we processed the full dataset to confirm. Finally, we created the functionality for a user prompt and query function to use in real time during a meeting. The result of the query augmented the user’s input for <code class="inlineCode">meta-llama/Llama-2-7b-chat-hf</code>, which transformed the query into a short summary.</p>
    <p class="normal">The dynamic RAG example we implemented would require more work before being released into production. However, it provides a path to open-source, lightweight, RAG-driven generative AI for rapid data collection, embedding, and querying. If we need to store the retrieval data and don’t want to create large vector stores, we can integrate our datasets in an OpenAI GPT-4o-mini model, for example, through fine-tuning, as we will see in the next chapter.</p>
    <h1 id="_idParaDest-220" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with <em class="italic">Yes</em> or <em class="italic">No</em>:</p>
    <ol>
      <li class="numberedList" value="1">Does the script ensure that the Hugging Face API token is never hardcoded directly into the notebook for security reasons?</li>
      <li class="numberedList">In the chapter’s program, is the <code class="inlineCode">accelerate</code> library used here to facilitate the deployment of ML models on cloud-based platforms?</li>
      <li class="numberedList">Is user authentication separate from the API token required to access the Chroma database in this script?</li>
      <li class="numberedList">Does the notebook use Chroma for temporary storage of vectors during the dynamic retrieval process?</li>
      <li class="numberedList">Is the notebook configured to use real-time acceleration of queries through GPU optimization?</li>
      <li class="numberedList">Can this notebook’s session time measurements help in optimizing the dynamic RAG process?</li>
      <li class="numberedList">Does the script demonstrate Chroma’s capability to integrate with ML models for enhanced retrieval performance?</li>
      <li class="numberedList">Does the script include functionality for adjusting the parameters of the Chroma database based on session performance metrics?</li>
    </ol>
    <h1 id="_idParaDest-221" class="heading-1">References</h1>
    <ul>
      <li class="bulletList"><em class="italic">Crowdsourcing Multiple Choice Science Questions</em> by Johannes Welbl, Nelson F. Liu, Matt Gardner: <a href="http://arxiv.org/abs/1707.06209">http://arxiv.org/abs/1707.06209</a>.</li>
      <li class="bulletList"><em class="italic">MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers </em>by Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, Furu Wei: <a href="https://arxiv.org/abs/2012.15828">https://arxiv.org/abs/2012.15828</a>.</li>
      <li class="bulletList">Hugging Face Llama model documentation: <a href="https://huggingface.co/docs/transformers/main/en/model_doc/llama">https://huggingface.co/docs/transformers/main/en/model_doc/llama</a>.</li>
      <li class="bulletList">ONNX: <a href="https://onnxruntime.ai/">https://onnxruntime.ai/</a>.</li>
    </ul>
    <h1 id="_idParaDest-222" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList"><em class="italic">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</em> by Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou: <a href="https://arxiv.org/abs/2002.10957">https://arxiv.org/abs/2002.10957</a>.</li>
      <li class="bulletList"><em class="italic">LLaMA: Open and Efficient Foundation Language Models</em> by Hugo Touvron, Thibaut Lavril, Gautier Lzacard, et al.: <a href="https://arxiv.org/abs/2302.13971">https://arxiv.org/abs/2302.13971</a>.</li>
      <li class="bulletList">Building an ONNX Runtime package: <a href="https://onnxruntime.ai/docs/build/custom.html#custom-build-packages">https://onnxruntime.ai/docs/build/custom.html#custom-build-packages</a>.</li>
    </ul>
    <h1 id="_idParaDest-223" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>