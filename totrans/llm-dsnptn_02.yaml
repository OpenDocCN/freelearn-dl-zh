- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Cleaning for LLM Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll dive into the **data cleaning** pattern for LLM training.
  prefs: []
  type: TYPE_NORMAL
- en: Clean, high-quality data is the foundation of robust and reliable language models.
    We’ll explore common data quality issues, preprocessing techniques, and strategies
    for handling diverse data types. *Figure 2**.1* depicts a data cleaning pipeline
    specifically designed for processing raw text data before it’s used to train language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Data cleaning pipeline](img/B31249_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Data cleaning pipeline
  prefs: []
  type: TYPE_NORMAL
- en: The process begins with an initial data quality check to assess the raw data’s
    suitability. Following this, text preprocessing and deduplication steps are applied
    to refine and streamline the dataset. If the data fails to meet the required standards
    at any point, it is rerouted through an automated cleaning pipeline for additional
    processing. Successful completion of this stage leads to data validation to ensure
    the dataset’s integrity and compliance with training standards. If the data passes
    validation, it is marked as clean and ready for use in language model training,
    ensuring high-quality input for effective model development.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be equipped with practical tools and techniques
    to clean your data for LLM training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the importance of clean data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common data quality issues in language datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text preprocessing techniques for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling multilingual and code-mixed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deduplication strategies for large text corpora
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated data cleaning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data validation and quality assurance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the importance of clean data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The quality of data used in training LLMs directly impacts their performance
    and reliability. When we train LLMs on noisy or inconsistent data, we risk introducing
    bias, errors, and inconsistency into the model’s learned representations and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the impact of data quality on LLM performance, we can use a simple
    Python script to compare the perplexity scores of models trained on clean and
    noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the necessary packages and import them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, define the initial part of the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `calculate_perplexity` function tokenizes the input text into PyTorch tensors
    using the provided tokenizer. It then passes the tokenized input to the model
    with `input_ids` also used as labels, allowing the model to compute a loss representing
    prediction error. This loss is exponentiated to derive a scalar perplexity score
    and returned as a Python float.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second part of the code initializes a language model and tokenizer using
    `GPT4LMHeadModel.from_pretrained("GPT4")` and `GPT4Tokenizer.from_pretrained("GPT4")`,
    which load the model and tokenizer weights from a pre-trained source identified
    as `"GPT4"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Perplexity
  prefs: []
  type: TYPE_NORMAL
- en: '**Perplexity** is a measure used to evaluate language models. It quantifies
    how well a probability model predicts a sample.'
  prefs: []
  type: TYPE_NORMAL
- en: Lower perplexity indicates that the model is more confident in its predictions
    and considers the text more likely or “normal”. Higher perplexity suggests that
    the model finds the text more surprising or unusual.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are example texts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, calculate perplexity and print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This script demonstrates how even small amounts of noise in the input data can
    significantly impact the model’s perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: The perplexity score is calculated as the exponential of the cross-entropy loss.
    In this code, it’s computed using `torch.exp(outputs.loss).item()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are our possible outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The quick brown fox jumps over the lazy dog` is a common, grammatically correct
    English sentence. The clean text perplexity might be something like `10.25`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g` contains numbers and symbols
    in place of letters, making it less common and more difficult for the model to
    predict. The noisy text perplexity might be something like `52.87`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exact numbers will depend on the specific model and tokenizer used, but
    the noisy text should consistently have a higher perplexity score than the clean
    text.
  prefs: []
  type: TYPE_NORMAL
- en: This difference in scores demonstrates the model’s ability to distinguish between
    standard, easily predictable text and unusual, harder-to-predict text. It’s a
    useful metric for tasks such as detecting machine-generated or tampered text,
    as such text often has higher perplexity scores compared to human-written text.
  prefs: []
  type: TYPE_NORMAL
- en: Common data quality issues in language datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Language datasets often contain various quality issues that can negatively
    impact LLM training:'
  prefs: []
  type: TYPE_NORMAL
- en: Spelling and grammatical errors can introduce noise and inconsistencies in the
    learned representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inconsistent formatting can lead to unnecessary complexity in the model’s learned
    patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redundant data can cause models to overfit to specific patterns or bias present
    in the duplicates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irrelevant or low-quality content can dilute the useful information in the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incomplete or truncated sentences can lead to models learning incomplete language
    structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code-switching and mixed languages can confuse models trained for specific languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personally identifiable information** (**PII**) raises privacy concerns and
    can lead to the memorization of sensitive data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To detect these issues, we can use various Python libraries and techniques.
    Here’s an example using spaCy for basic text quality checks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Provide the imports and an overall function definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check for spelling errors (using spaCy’s built-in spell checker):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check for grammatical issues (a simplistic approach using parts of speech (pos)
    tags):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Parts of speech** (**POS**) tags are labels assigned to each word in a sentence
    to indicate its grammatical role. These tags help systems to understand the syntactic
    structure of sentences and are used in tasks such as parsing, machine translation,
    sentiment analysis, and information extraction. Each tag corresponds to a POS
    such as a noun, verb, or adjective, often with finer-grained distinctions to capture
    tense, number, or function.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check for sentence completeness:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s an example usage of the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The script provided in *steps 1 to 5* illustrates a basic framework for identifying
    some common text quality issues. We will address other quality issues in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Text preprocessing techniques for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective text preprocessing is crucial for preparing data for LLM training.
    We employ various techniques, including lowercasing, punctuation handling, whitespace
    normalization, special character handling, **tokenization**, number normalization,
    and contraction expansion. Tokenization is the process of breaking text into smaller
    units for further analysis or processing. Tokens are the smallest meaningful units
    of text in natural language processing. They can be words, but they could also
    include punctuation, numbers, or other elements depending on the tokenization
    strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, **subword tokenization** is an advanced text processing technique
    that breaks words into smaller meaningful units (subwords), enabling more efficient
    handling of rare words, compound words, and morphological variations in natural
    language processing tasks. Unlike traditional word-level tokenization, subword
    tokenization can identify common prefixes, suffixes, and root words, allowing
    models to understand and process previously unseen words by recognizing their
    familiar components.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider the word “unbelievably”. Traditional word-level tokenization
    would treat this as a single token. If the model has never seen this word before,
    it may struggle to interpret it correctly. In contrast, subword tokenization would
    break it down into smaller components such as “un”, “believ”, and “ably”. These
    subwords are more likely to appear across different contexts—“un-” in “unlikely”,
    “believ” in “believe”, “ably” in “capably”—allowing the model to derive meaning
    even if it encounters “unbelievably” for the first time. This decomposition enhances
    generalization, reduces vocabulary size, and improves the model’s ability to handle
    rare or morphologically complex words.
  prefs: []
  type: TYPE_NORMAL
- en: Popular subword tokenization algorithms include **byte pair encoding** (**BPE**),
    WordPiece, and SentencePiece, which learn to identify frequently occurring character
    sequences in a training corpus and create a vocabulary of subword tokens. This
    approach is particularly valuable for handling morphologically rich languages,
    reducing vocabulary size while maintaining semantic meaning, and has become fundamental
    in modern language models such as Gemini, Claude, GPT, and other transformer-based
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'These methods help clean and standardize the text data, reducing noise and
    improving the model’s ability to generalize. Here’s a Python script demonstrating
    these preprocessing techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the necessary Python packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, define the overall preprocessing function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove stopwords (stopwords are common words such as “the”, “is”, and “at”)
    that are often removed in text processing as they carry little semantic meaning):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s an example usage of the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This script demonstrates basic text preprocessing techniques. For LLM training,
    we might need to adapt these techniques based on the specific requirements of
    the model and the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Handling multilingual and code-mixed data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs often encounter multilingual and code-mixed data, which is text that blends
    two or more languages within a single sentence or conversation. This presents
    a challenge as LLMs must interpret linguistic nuances, grammar, and semantic connections
    across multiple languages. To handle code-mixed data, LLMs need to learn language
    switching, vocabulary and syntax variations, and maintain coherent responses,
    which demands strong language modeling and multilingual training data.
  prefs: []
  type: TYPE_NORMAL
- en: We need to implement strategies to handle these scenarios effectively. The following
    steps are needed because they create cleaner, more consistent training data that
    helps LLMs better understand and process text across different languages and mixed-language
    scenarios, ultimately improving their performance in real-world applications where
    language mixing is common.
  prefs: []
  type: TYPE_NORMAL
- en: 'For multilingual data, certain tasks are crucial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Language identification**: Detects the primary language of each text sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Script normalization**: Converts text to a consistent script (e.g., transliteration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language-specific preprocessing**: Applies language-specific tokenization
    and normalization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meanwhile, you should carry out the following steps for code-mixed data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Token-level language identification**: Identifies the language of individual
    tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency enforcement**: Ensures consistent handling of code-switching
    patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s a Python script demonstrating language detection and script normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s provide the imports and the overall function definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize (using NLTK for simplicity, but consider language-specific tokenizers):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the example usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code iterates through a list of multilingual text strings, including English,
    German, Japanese, and a code-mixed example, and for each string, it calls a `handle_multilingual_text`
    function (presumably defined elsewhere) to process the text, returning a dictionary
    containing the original text, detected language, transliterated text (if applicable),
    and tokenized words, which are then printed to the console.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Putting the preceding three code blocks together, we provide a basic framework
    for handling multilingual text. For more advanced scenarios, we would use specialized
    libraries such as Polyglot for language-specific processing and code-mixing analysis
    when multiple languages are used in the same conversation ([https://dl.acm.org/doi/10.1145/3544548.3581445](https://dl.acm.org/doi/10.1145/3544548.3581445)).
  prefs: []
  type: TYPE_NORMAL
- en: For example, Polyglot includes built-in language detection, named entity recognition,
    sentiment analysis, and transliteration capabilities across multiple languages,
    all while maintaining a relatively lightweight footprint compared to larger multilingual
    frameworks. The library is particularly valuable for projects dealing with international
    text data, as it provides consistent APIs across languages and comes with pre-trained
    models, making it an efficient choice for multilingual text analysis tasks without
    the complexity of managing multiple language-specific tools.
  prefs: []
  type: TYPE_NORMAL
- en: Deduplication strategies for large text corpora
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deduplication is a critical step in preparing large text corpora for LLM training.
    Duplicate content can lead to biased models and wasted computational resources.
    We employ various strategies to identify and remove duplicates efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exact match deduplication**: Remove identical text samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Near-duplicate detection**: Identify and remove highly similar text samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shingling**: Create small overlapping sequences of words for comparison.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locality sensitive hashing**: Efficiently find similar items in large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following sections show examples of each strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Exact match deduplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scenario**: You have a list of customer addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “123 Main St, Anytown, CA 91234”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “456 Oak Ave, Somecity, NY 56789”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “123 Main St, Anytown, CA 91234”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Result**: The third entry, “123 Main St, Anytown, CA 91234”, is removed because
    it is an exact duplicate of the first entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remaining data**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “123 Main St, Anytown, CA 91234”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “456 Oak Ave, Somecity, NY 56789”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Near-duplicate detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scenario**: You have a collection of news articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article 1: “The company reported a significant increase in quarterly profits.”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article 2: “Quarterly profits saw a large increase, the company reports.”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Result**: A near-duplicate detection algorithm determines that these articles
    are highly similar in content, even though the wording is slightly different.
    One of the articles is removed, based on a similarity threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remaining data**: “The company reported a significant increase in quarterly
    profits.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shingling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scenario**: You want to compare the similarity of text documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Document 1: “The quick brown fox jumps over the lazy dog.”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: k=3 word shingle.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Result**: The shingles generated are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “The quick brown”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “quick brown fox”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “brown fox jumps”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “fox jumps over”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “jumps over the”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “over the lazy”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: “the lazy dog”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The document is then represented by the set of those shingles. Then another
    document could be turned into shingles, and the sets of shingles can be compared.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Locality Sensitive Hashing (LSH)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scenario**: You have a very large database of online product descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Process**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSH is used to hash the product descriptions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar product descriptions are more likely to be hashed into the same “buckets.”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Only the descriptions within the same buckets are then compared in detail to
    find near duplicates.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Result**: Instead of comparing every product description to every other description,
    LSH narrows down the comparisons to only those descriptions within the same buckets,
    greatly increasing the efficiency of finding near duplicates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Deduplicating is computationally very expensive, so techniques such as minhashing
    or parallel processing can be used to scale the deduplicating with the increase
    in corpus data.
  prefs: []
  type: TYPE_NORMAL
- en: Minhashing efficiently approximates the similarity between documents using smaller,
    more manageable representations, reducing the computational load. Parallel processing
    further distributes the deduplication task across multiple processors or machines,
    allowing for simultaneous comparisons and significantly speeding up the overall
    process, thus enabling effective deduplication of massive corpora.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a Python script demonstrating basic deduplication techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the overall function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, find duplicates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a deduplicated corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This script demonstrates a basic near-duplicate detection approach using TF-IDF
    and **cosine similarity**. TF-IDF is a numerical statistic used to reflect the
    importance of words in documents within a collection. It combines how often a
    word appears in a document (TF) with how unique it is across all documents (IDF).
    TF-IDF converts text into numerical vectors, enabling mathematical comparisons
    between documents, which is crucial for the similarity calculations used in the
    deduplication process. For large-scale deduplication, we would use more efficient
    algorithms and distributed computing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the similarity threshold of `0.9` used in the deduplication function code
    determines how similar documents must be to be considered duplicates, with 90%
    similarity required by default. This value can be adjusted based on specific use
    cases—a higher threshold (e.g., `0.95` or `1`, which is maximum) is stricter and
    reduces false positives, while a lower threshold (e.g., `0` which is minimum or
    `0.8`) is more lenient and catches more potential duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss automated data cleaning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Automated data cleaning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To handle the massive datasets required for LLM training, we need to implement
    automated data cleaning pipelines. These pipelines should be scalable, efficient,
    and capable of handling various data quality issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key components of an automated data cleaning pipeline are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion**: Efficiently load and parse large text corpora.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality assessment**: Automatically detect and flag data quality issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preprocessing**: Apply text cleaning and normalization techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deduplication**: Remove exact and near-duplicate content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering**: Remove low-quality or irrelevant samples based on predefined
    criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation**: Ensure the cleaned data meets quality standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: Save the cleaned data in an appropriate format for LLM training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a Python script outlining a basic automated data cleaning pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by defining the overall class structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will define a preprocess function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`preprocess`: This method takes a text string as input, converts it to lowercase,
    removes punctuation, splits it into words, filters out common stop words, and
    then joins the remaining words into a string, effectively cleaning and normalizing
    the text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_by_length`: This method takes a pandas DataFrame containing a `text`
    column and filters the DataFrame to include only rows where the length of the
    `text` column falls within a specified minimum and maximum length, allowing the
    selection of text samples within a desired character range.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then define the deduplication function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This `deduplicate` method takes a pandas DataFrame as input and removes near-duplicate
    text entries based on their similarity. It first transforms the `text` column
    of the DataFrame into a TF-IDF matrix using a vectorizer, representing each text
    sample as a numerical vector. Then, it calculates the cosine similarity between
    all pairs of text samples using the TF-IDF matrix, resulting in a similarity matrix.
    The code iterates through the similarity matrix, and if the similarity between
    two text samples exceeds a defined `similarity_threshold`, the index of the second
    sample is added to a set of duplicates. Finally, it removes the rows corresponding
    to the identified duplicate indices from the DataFrame and returns the deduplicated
    DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Putting all the functions together, we can now define a `clean` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This `clean` method orchestrates a series of data cleaning steps on a CSV file.
    It begins by reading the input CSV file into a pandas DataFrame. Then, the `preprocess`
    method is applied to each text entry in the `text` column, normalizing and cleaning
    the text. Subsequently, it filters the DataFrame using the `filter_by_length`
    method to retain only text entries within a specified length range. After length
    filtering, near-duplicate entries are removed using the `deduplicate` method.
    Finally, it saves the cleaned DataFrame to a new CSV file specified by `output_file`,
    excluding the index, and prints a confirmation message indicating the output file’s
    location. Essentially, this method performs a complete text cleaning pipeline,
    encompassing preprocessing, length filtering, and deduplication.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following is an example usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Overall, this script provides a basic framework for an automated data cleaning
    pipeline. In practice, we would extend this pipeline with more sophisticated cleaning
    techniques, error handling, and parallel processing capabilities to handle large-scale
    datasets efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The values `10` and `1000` in the code represent the minimum and maximum allowed
    lengths for text documents in the data cleaning pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_length=10`: This sets the minimum number of characters a document must
    have to be included in the cleaned dataset. It helps to filter out very short
    texts that might not contain meaningful information, such as single words or brief
    phrases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length=1000`: This establishes the maximum number of characters allowed
    for a document. It excludes extremely long texts that might be atypical or potentially
    problematic for processing, such as entire books or very large documents that
    could skew the analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These length constraints help ensure that the cleaned dataset contains documents
    of a reasonable and consistent size range, which can improve the quality and efficiency
    of subsequent text analysis or machine learning tasks. You can adjust the length
    based on your use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Data validation and quality assurance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After cleaning the data, you need to validate the results and ensure that the
    cleaned dataset meets the required quality standards for LLM training. We implement
    various validation checks and quality assurance measures to verify the effectiveness
    of our cleaning process.
  prefs: []
  type: TYPE_NORMAL
- en: Key aspects include performing statistical analyses, sampling and manual reviews,
    automated tests, consistency verifications, and performance impact assessments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a Python script demonstrating basic data validation techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the basic function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, check for empty or very short texts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sample for a manual review:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the impact on the model’s perplexity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The script defines a function called `validate_cleaned_data` that’s designed
    to perform a basic quality assessment on a text dataset stored in a CSV file (presumably
    after some initial cleaning steps). It loads the data, calculates some basic statistics,
    checks for specific potential issues in the text content, provides a sample for
    manual inspection, and uses a pre-trained language model (hypothetically GPT-4)
    to evaluate the naturalness or quality of a sample of the text via perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following issues are being checked for:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset size and basic properties:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`len(df)`: Checks the total number of samples (rows) in the CSV.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`df[''text''].str.len().mean()`: Calculates the average length of the text
    entries, which is useful to see if texts are generally long or short.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`df[''text''].nunique()`: Counts the number of unique text entries. A low number
    compared to the total number of samples might indicate many duplicates.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`df[df[''text''].str.len() < 10]`: Filters the DataFrame to find rows where
    the length of the string in the `text` column is less than 10 characters*   `len(short_texts)`:
    Counts how many such short texts were found*   `df[''text''].str.contains(r''[^a-zA-Z0-9\s]'')`:
    Uses a regular expression (`r''[^a-zA-Z0-9\s]''`) with pandas’ `.str.contains()`
    method. The regex pattern [`^...`] matches any character *not* in the specified
    set (a-z, A-Z, 0-9, whitespace \s).*   `mask.sum()`: Sums the resulting Boolean
    series (true=`1`, false=0) to count how many texts contain at least one such special
    character.*   `df[''text''].str.contains(r''\d'')`: Uses the regular expression
    `\d` (which matches any digit) with `.str.contains()`*   `mask.sum()`: Counts
    how many texts contain at least one digit*   `df[''text''].str.isupper()`: Uses
    the pandas `.str.isupper()` string method, which returns `True` if all cased characters
    in the string are uppercase and there is at least one alphabetic character (i.e.,
    a letter) that is uppercase and not just symbols or digits. If the string is all
    non-alphabetic (like numbers or punctuation), it will return `False`—even though
    those characters aren’t lowercase either.*   `mask.sum()`: Counts how many texts
    are entirely in uppercase*   `df.sample(...)`). Perplexity calculations can be
    computationally expensive, so they are often done on a representative sample rather
    than the whole dataset.*   `GPT4LMHeadModel`) and its corresponding tokenizer
    (`GPT4Tokenizer`) are loaded. (Note: `''GPT4''` here is illustrative; you’d use
    actual model identifiers such as `''gpt2''` or `''bert-base-uncased''` from libraries
    such as Hugging Face Transformers).*   `calculate_perplexity` function tokenizes
    the text, feeds it to the model, obtains the loss (a measure of how surprised
    the model was by the text), and calculates perplexity using `torch.exp(outputs.loss)`.*   `sample_perplexities.mean()`)
    to get a single score representing the sample’s average quality according to the
    model.*   `sample = df.sample(...)`: Takes a random sample of the data*   `print(sample[''text''].head())`:
    Prints the first few text entries from that random sample, making it easy for
    a user running the script to quickly eyeball some examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To ensure comprehensive quality assurance, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement more sophisticated automated tests tailored to your specific data
    characteristics and cleaning rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a systematic process for manual review, including guidelines for human
    annotators to assess data quality consistently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a known synthetic dataset with known issues to benchmark and assess the
    performance of the pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare the cleaned dataset against the original dataset to verify that no unintended
    data loss or alteration occurred during the cleaning process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct regular audits of your data cleaning pipeline to identify any emerging
    issues or bias introduced during cleaning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain detailed logs of the cleaning process, including any decisions made
    and their rationale, to ensure reproducibility and facilitate future improvements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By implementing these measures, you can ensure that your cleaned dataset is
    of high quality and suitable for training robust LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the critical process of data cleaning for LLM training.
    We discussed the importance of clean data in developing robust and reliable language
    models and covered common data quality issues specific to language datasets. We
    provided techniques to address these issues, including text preprocessing, handling
    multilingual and code-mixed data, and deduplication strategies for large text
    corpora.
  prefs: []
  type: TYPE_NORMAL
- en: We also delved into the implementation of automated data cleaning pipelines,
    which are essential for handling the massive datasets used in LLM training. Finally,
    we discussed data validation and quality assurance measures to ensure the effectiveness
    of the cleaning process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on the data augmentation pattern for LLMs.
  prefs: []
  type: TYPE_NORMAL
