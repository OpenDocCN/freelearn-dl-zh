- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introducing Stable Diffusion
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Stable Diffusion
- en: Stable Diffusion is a deep learning model that utilizes diffusion processes
    to generate high-quality artwork from guided instructions and images.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 是一个深度学习模型，它利用扩散过程从引导指令和图像中生成高质量的美术作品。
- en: In this chapter, we will introduce you to AI image generation technology, namely
    Stable Diffusion, and see how it evolved into what it is now.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您介绍 AI 图像生成技术，即 Stable Diffusion，并了解它是如何发展到现在的。
- en: Unlike other deep learning image generation models, such as OpenAI’s DALL-E
    2, Stable Diffusion works by starting with a random-noise latent tensor and then
    gradually adding detailed information to it. The amount of detail that is added
    is determined by a diffusion process, governed by a mathematical equation (we
    will delve into the details in [*Chapter 5*](B21263_05.xhtml#_idTextAnchor097)).
    In the final stage, the model decodes the latent tensor into the pixel image.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他深度学习图像生成模型不同，例如 OpenAI 的 DALL-E 2，Stable Diffusion 通过从随机的噪声潜在张量开始，然后逐渐向其中添加详细信息来工作。添加的细节量由一个扩散过程决定，该过程受数学方程式控制（我们将在[*第五章*](B21263_05.xhtml#_idTextAnchor097)中深入探讨细节）。在最终阶段，模型将潜在张量解码成像素图像。
- en: Since its creation in 2022, Stable Diffusion has been used widely to generate
    impressive images. For example, it can generate images of people, animals, objects,
    and scenes that are indistinguishable from real photographs. Images are generated
    using specific instructions, such as *A cat running on the moon’s surface* or
    *a photograph of an astronaut riding* *a horse.*
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2022 年创建以来，Stable Diffusion 已被广泛用于生成令人印象深刻的图像。例如，它可以生成与真实照片难以区分的人物、动物、物体和场景的图像。图像是通过特定的指令生成的，例如
    *“在月球表面奔跑的猫”* 或 *“一位宇航员骑马的照片”。*
- en: 'Here is a sample of a prompt to use with Stable Diffusion to generate an image
    using the given description:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个用于与 Stable Diffusion 一起生成图像的提示示例：
- en: '`"a photograph of an astronaut riding` `a horse".`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`"一位宇航员骑马的照片"。`'
- en: 'Stable Diffusion will generate an image like the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 将生成以下图像：
- en: '![Figure 1.1: A photograph of an astronaut riding a horse, generated by Stable
    Diffusion](img/B21263_01_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1：由 Stable Diffusion 生成的宇航员骑马的图片](img/B21263_01_01.jpg)'
- en: 'Figure 1.1: A photograph of an astronaut riding a horse, generated by Stable
    Diffusion'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：由 Stable Diffusion 生成的宇航员骑马的图片
- en: This image didn’t exist before I hit the *Enter* button. It was created collaboratively
    by me and Stable Diffusion. Stable Diffusion not only understands the descriptions
    we give it, but also adds more detail to the image.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我按下 *Enter* 按钮之前，这张图片并不存在。它是通过我和 Stable Diffusion 的协作创建的。Stable Diffusion 不仅理解我们给出的描述，还会为图像添加更多细节。
- en: Apart from text-to-image generation, Stable Diffusion also facilitates editing
    photos using natural language. To illustrate, consider the preceding image again.
    We can replace the space background with a blue sky and mountains using an automatically
    generated mask and prompts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了文本到图像的生成之外，Stable Diffusion 还通过自然语言促进照片的编辑。为了说明这一点，再次考虑前面的图像。我们可以使用自动生成的蒙版和提示将太空背景替换为蓝天和山脉。
- en: 'The `background` prompt can be used to generate the background mask, and the
    `blue sky and mountains` prompt is used to guide Stable Diffusion to transform
    the initial image into the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `background` 提示生成背景蒙版，而 `blue sky and mountains` 提示用于指导 Stable Diffusion
    将初始图像转换为以下形式：
- en: '![Figure 1.2: Replace the background with a blue sky and mountains](img/B21263_01_02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2：将背景替换为蓝天和山脉](img/B21263_01_02.jpg)'
- en: 'Figure 1.2: Replace the background with a blue sky and mountains'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：将背景替换为蓝天和山脉
- en: No mouse-clicking or dragging is required, and there's no need for additional
    paid software such as Photoshop. You can achieve this using pure Python together
    with Stable Diffusion. Stable Diffusion can perform many other tasks using only
    Python code, which will be covered later in this book.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无需鼠标点击或拖动，也不需要额外的付费软件，如 Photoshop。您可以使用纯 Python 和 Stable Diffusion 实现这一点。Stable
    Diffusion 可以仅使用 Python 代码执行许多其他任务，这些内容将在本书的后续章节中介绍。
- en: Stable Diffusion is a powerful tool that has the potential to revolutionize
    the way we create and interact with images. It can be used to create realistic
    images for movies, video games, and other applications. It can also be used to
    generate personalized images for marketing, advertising, and decoration.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散是一个强大的工具，有可能彻底改变我们创建和交互图像的方式。它可以用于创建电影、视频游戏和其他应用中的逼真图像。它还可以用于生成用于营销、广告和装饰的个性化图像。
- en: 'Here are some of the key features of Stable Diffusion:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是稳定扩散的一些关键特性：
- en: It can generate high-quality images from text descriptions
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以从文本描述中生成高质量的图像
- en: It is based on a diffusion process, which is a more stable and reliable way
    to generate images than other methods
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它基于扩散过程，这是一种比其他方法更稳定、更可靠的生成图像的方式
- en: Many massive pre-trained publicly accessible models are available (10,000+),
    and keep on growing
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在有大量可公开访问的预训练模型（10,000+），并且还在不断增长
- en: New research and applications are building on Stable Diffusion
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的研究和应用正在建立在稳定扩散的基础上
- en: It is open source and can be used by anyone
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是开源的，任何人都可以使用
- en: Before we proceed, let me provide a brief introduction to the evolution of the
    Diffusion model in recent years.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我简要介绍一下近年来扩散模型的演变。
- en: Evolution of the Diffusion model
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型的演变
- en: Diffusion hasn’t always been available, just as Rome was not built in a day.
    To have a high-level bird’s view of this technology, in this section, we will
    discuss the overall evolution of the Diffusion model in recent years.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型并非一直可用，正如罗马不是一天建成的。为了从高层次上了解这项技术，在本节中，我们将讨论近年来扩散模型的整体演变。
- en: Before Transformer and Attention
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Transformer和Attention之前
- en: Not too long ago, **Convolutional Neural Networks** (**CNNs**) and **Residual
    Neural Networks** (**ResNets**) dominated the field of computer vision in machine
    learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不久前，**卷积神经网络**（**CNNs**）和**残差神经网络**（**ResNets**）主导了机器学习中的计算机视觉领域。
- en: CNNs and ResNets have proven to be highly effective in tasks such as guided
    object detection and face recognition. These models have been widely adopted across
    various industries, including self-driving cars and AI-driven agriculture.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）和残差网络（ResNets）在引导对象检测和面部识别等任务中已被证明是非常有效的。这些模型已在各个行业得到广泛应用，包括自动驾驶汽车和人工智能驱动的农业。
- en: 'However, there is a significant drawback to CNNs and ResNets: they can only
    recognize objects that are part of their training set. To detect a completely
    new object, a new category label must be added to the training dataset, followed
    by retraining or fine-tuning the pre-trained models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CNNs和ResNets存在一个显著的缺点：它们只能识别其训练集中的对象。要检测一个完全新的对象，必须将新的类别标签添加到训练数据集中，然后重新训练或微调预训练模型。
- en: This limitation stems from the models themselves, as well as the constraints
    imposed by hardware and the availability of training data at that time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种限制源于模型本身，以及当时硬件的限制和训练数据的可用性。
- en: Transformer transforms machine learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer改变了机器学习
- en: The Transformer model, developed by Google, has revolutionized the field of
    computer vision, starting with its impact on **Natural Language** **Processing**
    (**NLP**).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由谷歌开发的Transformer模型彻底改变了计算机视觉领域，其影响始于对**自然语言处理**（**NLP**）的影响。
- en: Unlike traditional approaches that rely on predefined labels to calculate loss
    and update neural network weights through backpropagation, the Transformer model,
    along with the Attention mechanism, introduced a pioneering concept. They utilize
    the training data itself for both training and labeling purposes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖于预定义标签来计算损失并通过反向传播更新神经网络权重的传统方法不同，Transformer模型以及注意力机制引入了一个开创性的概念。它们利用训练数据本身进行训练和标记。
- en: 'Let’s consider the following sentence as an example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以下面的句子为例：
- en: '*“Stable Diffusion can generate images* *using text”*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*“稳定扩散可以使用文本来生成图像”*'
- en: 'Let’s say we input the sequence of words into the neural network, excluding
    the last word *text*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将单词序列输入到神经网络中，但不包括最后一个单词 *text*：
- en: '*“Stable Diffusion can generate* *images using”*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*“稳定扩散可以使用* *来生成图像”*'
- en: Using this prompt, the model can predict the next word based on its current
    weights. Let’s say it predicts *apple*. The encoded embedding of the word *apple*
    is significantly different from *text* in terms of vector space, much like two
    numbers with a large gap between them. This gap value can be used as the loss
    value, which is then backpropagated to update the weights.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个提示，模型可以根据其当前权重预测下一个单词。比如说它预测了 *apple*。单词 *apple* 的编码嵌入在向量空间中与 *text* 有显著差异，就像两个之间有很大差距的数字一样。这个差距值可以用作损失值，然后通过反向传播更新权重。
- en: By repeating this process millions or even billions of times during training
    and updating, the model’s weights gradually learn to produce the next reasonable
    words in a sentence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在训练和更新过程中重复这个过程数百万次甚至数十亿次，模型的权重逐渐学会在句子中产生下一个合理的单词。
- en: Machine learning models can now learn a wide range of tasks with a properly
    designed loss function.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型现在可以通过设计适当的损失函数学习各种任务。
- en: CLIP from OpenAI makes a big difference
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenAI 的 CLIP 发生了重大变化
- en: 'Researchers and engineers quickly recognized the potential of the Transformer
    model, as mentioned in the concluding remarks of the well-known machine learning
    paper titled *Attention Is All You Need* [2]. The author states the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员和工程师迅速认识到 Transformer 模型的潜力，正如著名机器学习论文《Attention Is All You Need》的结论部分所提到的。作者陈述如下：
- en: '*We are excited about the future of Attention-based models and plan to apply
    them to other tasks. We plan to extend the Transformer to problems involving input
    and output modalities other than text and to investigate local, restricted Attention
    mechanisms to efficiently handle large inputs and outputs such as images, audio,*
    *and video*.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们对基于注意力的模型未来的发展感到兴奋，并计划将其应用于其他任务。我们计划将 Transformer 扩展到涉及除文本之外的其他输入和输出模态的问题，并研究局部、受限的注意力机制，以有效地处理图像、音频、*以及视频*等大型输入和输出。'
- en: If you have read the paper and grasped the remarkable capabilities of Transformer-
    and Attention-based models, you might also be inspired to reimagine your own work
    and harness this extraordinary power.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读了这篇论文并掌握了基于 Transformer 和注意力机制的模型的卓越能力，你可能会受到启发，重新构想自己的工作并利用这种非凡的力量。
- en: Researchers from OpenAI grasped this power and created a model called CLIP [1]
    that uses the Attention mechanism and Transformer model architecture to train
    an image classification model. The model has the ability to classify a wide range
    of images with no need for labeled data. It is the first large-scale image classification
    model trained on 400 million image-text pairs extracted from the internet.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的研究人员抓住了这种力量，创建了一个名为 CLIP [1] 的模型，该模型使用注意力机制和 Transformer 模型架构来训练图像分类模型。该模型能够无需标记数据对广泛的图像进行分类。它是第一个在从互联网上提取的
    4 亿个图像-文本对上训练的大型规模图像分类模型。
- en: 'Although there were similar efforts prior to OpenAI’s CLIP model, the results
    were not deemed satisfactory according to the authors of the CLIP paper [1]:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 OpenAI 的 CLIP 模型之前已有类似的研究，但根据 CLIP 论文作者 [1] 的观点，这些结果并不令人满意：
- en: '*A crucial difference between these weakly supervised models and recent explorations
    of learning image representations directly from natural language* *is scale.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*这些弱监督模型与最近直接从自然语言学习图像表示的探索之间的一个关键区别是规模。*'
- en: Indeed, scale plays a pivotal role in unlocking the remarkable superpower of
    universal image recognition. While other models utilized 200,000 images, the CLIP
    team trained their model using a staggering 400,000,000 images combined with text
    data from the public internet.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，规模在解锁通用图像识别的非凡超级能力中起着关键作用。当其他模型使用了 20 万张图像时，CLIP 团队使用令人敬畏的 4 亿张图像与来自公共互联网的文本数据一起训练了他们的模型。
- en: The results are astonishing. CLIP enables image recognition and segmentation
    without the limitations of predefined labels. It can detect objects that previous
    models struggled with. CLIP has brought about a significant change through its
    large-scale model. Given the immense weight of CLIP, researchers have pondered
    whether it could also be employed for image generation from text.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人震惊。CLIP 使图像识别和分割摆脱了预定义标签的限制。它可以检测到先前模型难以处理的对象。CLIP 通过其大规模模型带来了重大变革。鉴于 CLIP
    的巨大影响力，研究人员在思考它是否也可以用于从文本生成图像。
- en: Generate images
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成图像
- en: Using only CLIP, we still cannot generate a realistic image based on a text
    description. For instance, if we ask CLIP to draw an apple, the model merges various
    types of apples, different shapes, colors, backgrounds, and so on. CLIP might
    generate an apple that is half green and half red, which might not be what we
    intended.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用CLIP，我们仍然无法根据文本描述生成逼真的图像。例如，如果我们要求CLIP画一个苹果，模型会合并各种类型的苹果，不同的形状、颜色、背景等等。CLIP可能会生成一个一半绿色一半红色的苹果，这可能不是我们想要的。
- en: You may be familiar with **Generative Adversarial Networks** (**GANs**), which
    are capable of generating highly photorealistic images. However, text prompts
    cannot be utilized in the generation process. GANs have become a sophisticated
    solution for image processing tasks such as face restoration and image upscaling.
    Nevertheless, a new innovative approach was needed to leverage models for image
    generation based on guided descriptions or prompts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能熟悉**生成对抗网络**（**GANs**），它能够生成高度逼真的图像。然而，在生成过程中无法使用文本提示。GANs已经成为图像处理任务（如人脸修复和图像上采样）的复杂解决方案。尽管如此，仍需要一种新的创新方法来利用基于引导描述或提示的图像生成模型。
- en: In June 2020, a paper titled *Denoising Diffusion Probabilistic Models* [3]
    by Jonathan Ho et al. introduced a diffusion-based probabilistic model for image
    generation. The term **diffusion** is borrowed from thermodynamics. The original
    meaning of diffusion is the movement of particles from a region of high concentration
    to a region of low concentration. This idea of diffusion inspired machine learning
    researchers to apply it to denoising and sampling processes. In other words, we
    can start with a noisy image and gradually refine it by removing noise. The denoising
    process gradually transforms an image with high levels of noise into a clearer
    version of the original image. Therefore, this generative model is referred to
    as a **denoising diffusion** **probabilistic model**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年6月，Jonathan Ho等人发表了一篇题为《基于扩散的概率模型去噪》的论文[3]，介绍了一种基于扩散的图像生成概率模型。术语**扩散**借自热力学。扩散的原始含义是粒子从高浓度区域向低浓度区域的运动。这种扩散的想法启发了机器学习研究人员将其应用于去噪和采样过程。换句话说，我们可以从一个噪声图像开始，通过去除噪声逐渐细化它。去噪过程逐渐将高噪声水平的图像转换成原始图像的更清晰版本。因此，这种生成模型被称为**去噪扩散****概率模型**。
- en: The idea behind this approach is ingenious. For any given image, a limited number
    of normally distributed noise images are added to the original image, effectively
    transforming it into a fully noisy image. What if we train a model that can reverse
    this diffusion process, guided by the CLIP model? Surprisingly, this approach
    works [4].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法背后的想法是巧妙的。对于任何给定的图像，我们向原始图像添加有限数量的正态分布的噪声图像，有效地将其转换为一个完全噪声的图像。如果我们训练一个模型，该模型可以在CLIP模型的引导下逆转这种扩散过程，会怎样呢？令人惊讶的是，这种方法是有效的[4]。
- en: DALL-E 2 and Stable Diffusion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DALL-E 2和Stable Diffusion
- en: In April 2022, OpenAI released DALL-E 2, accompanied by its paper titled *Hierarchical
    Text-Conditional Image Generation with CLIP Latents* [4]. DALL-E 2 garnered significant
    attention worldwide. It generated a massive collection of astonishing images that
    spread across social networks and mainstream media. People were not only amazed
    by the quality of the generated images but also by its ability to create images
    that had never existed before. DALL-E 2 was effectively producing works of art.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年4月，OpenAI发布了DALL-E 2，并附带了一篇题为《基于CLIP潜力的分层文本条件图像生成》的论文[4]。DALL-E 2在全球范围内引起了广泛关注。它生成了一大批令人惊叹的图像，在社交网络和主流媒体中广泛传播。人们不仅对生成的图像质量感到惊讶，也对它能够创造从未存在过的图像的能力感到惊讶。DALL-E
    2实际上在创作艺术作品。
- en: Perhaps coincidentally, in April 2022, a paper titled *High-Resolution Image
    Synthesis with Latent Diffusion Models* [5] was published by CompVis, introducing
    another diffusion-based model for text-guided image generation. Building upon
    CompVis’s work, researchers and engineers from CompVis, Stability AI, and LAION
    collaborated to release an open source counterpart of DALL-E 2 called Stable Diffusion
    in August 2022.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是巧合，2022年4月，CompVis发表了一篇题为《基于潜在扩散模型的超高分辨率图像合成》的论文[5]，介绍了一种基于扩散的文本引导图像生成模型。在CompVis的工作基础上，CompVis、Stability
    AI和LAION的研究人员和工程师合作，于2022年8月发布了DALL-E 2的开源版本，名为Stable Diffusion。
- en: Why Stable Diffusion
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择Stable Diffusion
- en: While DALL-E 2 and other commercial image generation models such as Midjourney
    can produce remarkable images without requiring complex environment setups or
    hardware preparation, these models are closed-source. Consequently, users have
    limited control over the generation process, cannot use their own customized models,
    and are unable to add custom functions to the platform.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 DALL-E 2 和其他商业图像生成模型如Midjourney可以在不要求复杂的环境设置或硬件准备的情况下生成令人瞩目的图像，但这些模型是闭源的。因此，用户对生成过程有限制，无法使用自己的定制模型，也无法向平台添加自定义功能。
- en: On the other hand, Stable Diffusion is an open source model released under the
    CreativeML Open RAIL-M license. Users not only have the freedom to utilize the
    model but can also read the source code, add features, and benefit from the countless
    custom models shared by the community.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Stable Diffusion是一个在CreativeML Open RAIL-M许可下发布的开源模型。用户不仅有权使用该模型，还可以阅读源代码，添加功能，并从社区共享的无数自定义模型中受益。
- en: Which Stable Diffusion to use
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用哪个Stable Diffusion
- en: 'When we say Stable Diffusion, which Stable Diffusion are we really referring
    to? Here’s a list of the different Stable Diffusion tools and the differences
    between them:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说Stable Diffusion时，我们真正指的是哪个Stable Diffusion？以下是一个不同Stable Diffusion工具及其差异的列表：
- en: '**Stable Diffusion GitHub repo** ([https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)):
    This is the original implementation of Stable Diffusion from CompVis, contributed
    to by many great engineers and researchers. It is a PyTorch implementation that
    can be used to train and generate images, text, and other creative content. The
    library is now less active at the time of writing in 2023\. Its README page also
    recommends users use Diffusers from Hugging Face to use and train Diffusion models.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stable Diffusion GitHub仓库** ([https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)):
    这是CompVis提供的Stable Diffusion的原始实现，由许多杰出的工程师和研究人员贡献。这是一个PyTorch实现，可用于训练和生成图像、文本和其他创意内容。截至2023年写作时，该库的活跃度较低。其README页面还建议用户使用Hugging
    Face的Diffusers来使用和训练扩散模型。'
- en: '**Diffusers from Hugging Face**: Diffusers is a library for training and using
    diffusion models developed by Hugging Face. It is the go-to library for state-of-the-art,
    pre-trained diffusion models for generating images, audio, and even the 3D structures
    of molecules. The library is well maintained and being actively developed at the
    time of writing. New code is added to its GitHub repository almost every day.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face的Diffusers**: Diffusers是由Hugging Face开发的用于训练和使用扩散模型的库。它是生成图像、音频甚至分子3D结构的最新、预训练扩散模型的首选库。截至写作时，该库得到了良好的维护，并正在积极开发。几乎每天都有新代码添加到其GitHub仓库中。'
- en: '**Stable Diffusion WebUI from AUTOMATIC1111**: This might be the most popular
    web-based application currently that allows users to generate images and text
    using Stable Diffusion. It provides a GUI interface that makes it easy to experiment
    with different settings and parameters.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AUTOMATIC1111的Stable Diffusion WebUI**: 这可能是目前最受欢迎的基于Web的应用程序，允许用户使用Stable
    Diffusion生成图像和文本。它提供了一个GUI界面，使得用户可以轻松地实验不同的设置和参数。'
- en: '**InvokeAI**: InvokeAI was originally developed as a fork of the Stable Diffusion
    project, but it has since evolved into its own unique platform. InvokeAI offers
    a number of features that make it a powerful tool for creatives.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InvokeAI**: InvokeAI最初是作为Stable Diffusion项目的分支开发的，但后来已经发展成为一个独特的平台。InvokeAI提供了一系列功能，使其成为创意人士的强大工具。'
- en: '**ComfyUI**: ComfyUI is a node-based UI that utilizes Stable Diffusion. It
    allows users to construct tailored workflows, including image post-processing
    and conversions. It is a potent and adaptable graphical user interface for Stable
    Diffusion, characterized by its node-based design.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ComfyUI**: ComfyUI是一个基于节点的UI，利用Stable Diffusion。它允许用户构建定制的流程，包括图像后处理和转换。它是一个强大且灵活的图形用户界面，用于Stable
    Diffusion，以其基于节点的设计为特点。'
- en: In this book, when I use Stable Diffusion, I am referring to the Stable Diffusion
    model, not the GUI tools just listed. The focus of this book will be on using
    Stable Diffusion with plain Python. Our example code will use Diffusers’ pipelines
    and will leverage the code from Stable Diffusion WebUI and open source code from
    academic papers, et cetera.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，当我提到Stable Diffusion时，我指的是Stable Diffusion模型，而不是刚刚列出的GUI工具。本书的重点将集中在使用纯Python的Stable
    Diffusion。我们的示例代码将使用Diffusers的管道，并利用来自Stable Diffusion WebUI和学术论文开源代码等。
- en: Why this book
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择这本书
- en: While the Stable Diffusion GUI tool can generate fantastic images driven by
    the Diffusion model, its usability is limited. The presence of dozens of knobs
    (more sliders and buttons are being added) and specific terms sometimes makes
    generating high-quality images a guessing game. On the other hand, the open source
    Diffusers package from Hugging Face gives users full control over Stable Diffusion
    using Python. However, it lacks many key features such as loading custom LoRA
    and textual inversion, utilizing community-shared models/checkpoints, scheduling,
    and weighted prompts, unlimited prompt tokens, and high-resolution image fixing
    and upscaling (The Diffusers package does keep improving over time, however).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Stable Diffusion GUI工具可以由扩散模型驱动生成令人惊叹的图像，但其可用性有限。存在数十个调节旋钮（正在添加更多滑块和按钮）和特定术语，有时使得生成高质量图像变成了一场猜谜游戏。另一方面，来自Hugging
    Face的开源Diffusers包使用Python让用户对Stable Diffusion拥有完全控制权。然而，它缺少许多关键特性，如加载自定义LoRA和文本反转，利用社区共享的模型/检查点，调度，加权提示，无限提示令牌，以及高分辨率图像修复和放大（然而，Diffusers包仍在不断改进）。
- en: This book aims to help you understand all the complex terms and knobs from the
    internal view of the Diffusion model. The book will also assist you in overcoming
    the limitations of Diffusers and implementing the missing functions and advanced
    features to create a fully customized Stable Diffusion application.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在帮助您从扩散模型的内部视角理解所有复杂的术语和调节旋钮。本书还将协助您克服扩散器的局限性，实现缺失的功能和高级特性，以创建一个完全定制的Stable
    Diffusion应用程序。
- en: Considering the rapid pace of AI technology evolution, this book also aims to
    enable you to quickly adapt to the upcoming changes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到人工智能技术发展的快速步伐，本书还旨在使您能够快速适应即将到来的变化。
- en: By the end of this book, you will not only be able to use Python to generate
    and edit images but also leverage the solutions provided in the book to build
    Stable Diffusion applications for your business and users.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到本书结束时，您不仅能够使用Python生成和编辑图像，还能利用本书中提供的解决方案构建适合您业务和用户的Stable Diffusion应用程序。
- en: Let’s start the journey.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这段旅程。
- en: References
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Learning Transferable Visual Models From Natural Language* *Supervision*:
    [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*从自然语言监督学习可迁移的视觉模型*：[https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
- en: '*Attention Is All You* *Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*注意力即所需*：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
- en: '*Denoising Diffusion Probabilistic* *Models*: [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*去噪扩散概率模型*：[https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)'
- en: '*Hierarchical Text-Conditional Image Generation with CLIP* *Latents*: [https://arxiv.org/abs/2204.06125v1](https://arxiv.org/abs/2204.06125v1)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用CLIP潜变量进行分层文本条件图像生成*：[https://arxiv.org/abs/2204.06125v1](https://arxiv.org/abs/2204.06125v1)'
- en: '*High-Resolution Image Synthesis with Latent Diffusion* *Models*: [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用潜在扩散模型进行高分辨率图像合成*：[https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)'
- en: 'DALL-E 2: [https://openai.com/dall-e-2](https://openai.com/dall-e-2)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DALL-E 2：[https://openai.com/dall-e-2](https://openai.com/dall-e-2)
