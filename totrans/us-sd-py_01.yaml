- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Stable Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stable Diffusion is a deep learning model that utilizes diffusion processes
    to generate high-quality artwork from guided instructions and images.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to AI image generation technology, namely
    Stable Diffusion, and see how it evolved into what it is now.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other deep learning image generation models, such as OpenAI’s DALL-E
    2, Stable Diffusion works by starting with a random-noise latent tensor and then
    gradually adding detailed information to it. The amount of detail that is added
    is determined by a diffusion process, governed by a mathematical equation (we
    will delve into the details in [*Chapter 5*](B21263_05.xhtml#_idTextAnchor097)).
    In the final stage, the model decodes the latent tensor into the pixel image.
  prefs: []
  type: TYPE_NORMAL
- en: Since its creation in 2022, Stable Diffusion has been used widely to generate
    impressive images. For example, it can generate images of people, animals, objects,
    and scenes that are indistinguishable from real photographs. Images are generated
    using specific instructions, such as *A cat running on the moon’s surface* or
    *a photograph of an astronaut riding* *a horse.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample of a prompt to use with Stable Diffusion to generate an image
    using the given description:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"a photograph of an astronaut riding` `a horse".`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stable Diffusion will generate an image like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: A photograph of an astronaut riding a horse, generated by Stable
    Diffusion](img/B21263_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: A photograph of an astronaut riding a horse, generated by Stable
    Diffusion'
  prefs: []
  type: TYPE_NORMAL
- en: This image didn’t exist before I hit the *Enter* button. It was created collaboratively
    by me and Stable Diffusion. Stable Diffusion not only understands the descriptions
    we give it, but also adds more detail to the image.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from text-to-image generation, Stable Diffusion also facilitates editing
    photos using natural language. To illustrate, consider the preceding image again.
    We can replace the space background with a blue sky and mountains using an automatically
    generated mask and prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `background` prompt can be used to generate the background mask, and the
    `blue sky and mountains` prompt is used to guide Stable Diffusion to transform
    the initial image into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2: Replace the background with a blue sky and mountains](img/B21263_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Replace the background with a blue sky and mountains'
  prefs: []
  type: TYPE_NORMAL
- en: No mouse-clicking or dragging is required, and there's no need for additional
    paid software such as Photoshop. You can achieve this using pure Python together
    with Stable Diffusion. Stable Diffusion can perform many other tasks using only
    Python code, which will be covered later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion is a powerful tool that has the potential to revolutionize
    the way we create and interact with images. It can be used to create realistic
    images for movies, video games, and other applications. It can also be used to
    generate personalized images for marketing, advertising, and decoration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the key features of Stable Diffusion:'
  prefs: []
  type: TYPE_NORMAL
- en: It can generate high-quality images from text descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is based on a diffusion process, which is a more stable and reliable way
    to generate images than other methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many massive pre-trained publicly accessible models are available (10,000+),
    and keep on growing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New research and applications are building on Stable Diffusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is open source and can be used by anyone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we proceed, let me provide a brief introduction to the evolution of the
    Diffusion model in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of the Diffusion model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diffusion hasn’t always been available, just as Rome was not built in a day.
    To have a high-level bird’s view of this technology, in this section, we will
    discuss the overall evolution of the Diffusion model in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Before Transformer and Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not too long ago, **Convolutional Neural Networks** (**CNNs**) and **Residual
    Neural Networks** (**ResNets**) dominated the field of computer vision in machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs and ResNets have proven to be highly effective in tasks such as guided
    object detection and face recognition. These models have been widely adopted across
    various industries, including self-driving cars and AI-driven agriculture.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a significant drawback to CNNs and ResNets: they can only
    recognize objects that are part of their training set. To detect a completely
    new object, a new category label must be added to the training dataset, followed
    by retraining or fine-tuning the pre-trained models.'
  prefs: []
  type: TYPE_NORMAL
- en: This limitation stems from the models themselves, as well as the constraints
    imposed by hardware and the availability of training data at that time.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer transforms machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Transformer model, developed by Google, has revolutionized the field of
    computer vision, starting with its impact on **Natural Language** **Processing**
    (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional approaches that rely on predefined labels to calculate loss
    and update neural network weights through backpropagation, the Transformer model,
    along with the Attention mechanism, introduced a pioneering concept. They utilize
    the training data itself for both training and labeling purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the following sentence as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Stable Diffusion can generate images* *using text”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we input the sequence of words into the neural network, excluding
    the last word *text*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Stable Diffusion can generate* *images using”*'
  prefs: []
  type: TYPE_NORMAL
- en: Using this prompt, the model can predict the next word based on its current
    weights. Let’s say it predicts *apple*. The encoded embedding of the word *apple*
    is significantly different from *text* in terms of vector space, much like two
    numbers with a large gap between them. This gap value can be used as the loss
    value, which is then backpropagated to update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: By repeating this process millions or even billions of times during training
    and updating, the model’s weights gradually learn to produce the next reasonable
    words in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models can now learn a wide range of tasks with a properly
    designed loss function.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP from OpenAI makes a big difference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Researchers and engineers quickly recognized the potential of the Transformer
    model, as mentioned in the concluding remarks of the well-known machine learning
    paper titled *Attention Is All You Need* [2]. The author states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We are excited about the future of Attention-based models and plan to apply
    them to other tasks. We plan to extend the Transformer to problems involving input
    and output modalities other than text and to investigate local, restricted Attention
    mechanisms to efficiently handle large inputs and outputs such as images, audio,*
    *and video*.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have read the paper and grasped the remarkable capabilities of Transformer-
    and Attention-based models, you might also be inspired to reimagine your own work
    and harness this extraordinary power.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers from OpenAI grasped this power and created a model called CLIP [1]
    that uses the Attention mechanism and Transformer model architecture to train
    an image classification model. The model has the ability to classify a wide range
    of images with no need for labeled data. It is the first large-scale image classification
    model trained on 400 million image-text pairs extracted from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there were similar efforts prior to OpenAI’s CLIP model, the results
    were not deemed satisfactory according to the authors of the CLIP paper [1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A crucial difference between these weakly supervised models and recent explorations
    of learning image representations directly from natural language* *is scale.*'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, scale plays a pivotal role in unlocking the remarkable superpower of
    universal image recognition. While other models utilized 200,000 images, the CLIP
    team trained their model using a staggering 400,000,000 images combined with text
    data from the public internet.
  prefs: []
  type: TYPE_NORMAL
- en: The results are astonishing. CLIP enables image recognition and segmentation
    without the limitations of predefined labels. It can detect objects that previous
    models struggled with. CLIP has brought about a significant change through its
    large-scale model. Given the immense weight of CLIP, researchers have pondered
    whether it could also be employed for image generation from text.
  prefs: []
  type: TYPE_NORMAL
- en: Generate images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using only CLIP, we still cannot generate a realistic image based on a text
    description. For instance, if we ask CLIP to draw an apple, the model merges various
    types of apples, different shapes, colors, backgrounds, and so on. CLIP might
    generate an apple that is half green and half red, which might not be what we
    intended.
  prefs: []
  type: TYPE_NORMAL
- en: You may be familiar with **Generative Adversarial Networks** (**GANs**), which
    are capable of generating highly photorealistic images. However, text prompts
    cannot be utilized in the generation process. GANs have become a sophisticated
    solution for image processing tasks such as face restoration and image upscaling.
    Nevertheless, a new innovative approach was needed to leverage models for image
    generation based on guided descriptions or prompts.
  prefs: []
  type: TYPE_NORMAL
- en: In June 2020, a paper titled *Denoising Diffusion Probabilistic Models* [3]
    by Jonathan Ho et al. introduced a diffusion-based probabilistic model for image
    generation. The term **diffusion** is borrowed from thermodynamics. The original
    meaning of diffusion is the movement of particles from a region of high concentration
    to a region of low concentration. This idea of diffusion inspired machine learning
    researchers to apply it to denoising and sampling processes. In other words, we
    can start with a noisy image and gradually refine it by removing noise. The denoising
    process gradually transforms an image with high levels of noise into a clearer
    version of the original image. Therefore, this generative model is referred to
    as a **denoising diffusion** **probabilistic model**.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind this approach is ingenious. For any given image, a limited number
    of normally distributed noise images are added to the original image, effectively
    transforming it into a fully noisy image. What if we train a model that can reverse
    this diffusion process, guided by the CLIP model? Surprisingly, this approach
    works [4].
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E 2 and Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In April 2022, OpenAI released DALL-E 2, accompanied by its paper titled *Hierarchical
    Text-Conditional Image Generation with CLIP Latents* [4]. DALL-E 2 garnered significant
    attention worldwide. It generated a massive collection of astonishing images that
    spread across social networks and mainstream media. People were not only amazed
    by the quality of the generated images but also by its ability to create images
    that had never existed before. DALL-E 2 was effectively producing works of art.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps coincidentally, in April 2022, a paper titled *High-Resolution Image
    Synthesis with Latent Diffusion Models* [5] was published by CompVis, introducing
    another diffusion-based model for text-guided image generation. Building upon
    CompVis’s work, researchers and engineers from CompVis, Stability AI, and LAION
    collaborated to release an open source counterpart of DALL-E 2 called Stable Diffusion
    in August 2022.
  prefs: []
  type: TYPE_NORMAL
- en: Why Stable Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While DALL-E 2 and other commercial image generation models such as Midjourney
    can produce remarkable images without requiring complex environment setups or
    hardware preparation, these models are closed-source. Consequently, users have
    limited control over the generation process, cannot use their own customized models,
    and are unable to add custom functions to the platform.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Stable Diffusion is an open source model released under the
    CreativeML Open RAIL-M license. Users not only have the freedom to utilize the
    model but can also read the source code, add features, and benefit from the countless
    custom models shared by the community.
  prefs: []
  type: TYPE_NORMAL
- en: Which Stable Diffusion to use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we say Stable Diffusion, which Stable Diffusion are we really referring
    to? Here’s a list of the different Stable Diffusion tools and the differences
    between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stable Diffusion GitHub repo** ([https://github.com/CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)):
    This is the original implementation of Stable Diffusion from CompVis, contributed
    to by many great engineers and researchers. It is a PyTorch implementation that
    can be used to train and generate images, text, and other creative content. The
    library is now less active at the time of writing in 2023\. Its README page also
    recommends users use Diffusers from Hugging Face to use and train Diffusion models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diffusers from Hugging Face**: Diffusers is a library for training and using
    diffusion models developed by Hugging Face. It is the go-to library for state-of-the-art,
    pre-trained diffusion models for generating images, audio, and even the 3D structures
    of molecules. The library is well maintained and being actively developed at the
    time of writing. New code is added to its GitHub repository almost every day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stable Diffusion WebUI from AUTOMATIC1111**: This might be the most popular
    web-based application currently that allows users to generate images and text
    using Stable Diffusion. It provides a GUI interface that makes it easy to experiment
    with different settings and parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InvokeAI**: InvokeAI was originally developed as a fork of the Stable Diffusion
    project, but it has since evolved into its own unique platform. InvokeAI offers
    a number of features that make it a powerful tool for creatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ComfyUI**: ComfyUI is a node-based UI that utilizes Stable Diffusion. It
    allows users to construct tailored workflows, including image post-processing
    and conversions. It is a potent and adaptable graphical user interface for Stable
    Diffusion, characterized by its node-based design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, when I use Stable Diffusion, I am referring to the Stable Diffusion
    model, not the GUI tools just listed. The focus of this book will be on using
    Stable Diffusion with plain Python. Our example code will use Diffusers’ pipelines
    and will leverage the code from Stable Diffusion WebUI and open source code from
    academic papers, et cetera.
  prefs: []
  type: TYPE_NORMAL
- en: Why this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the Stable Diffusion GUI tool can generate fantastic images driven by
    the Diffusion model, its usability is limited. The presence of dozens of knobs
    (more sliders and buttons are being added) and specific terms sometimes makes
    generating high-quality images a guessing game. On the other hand, the open source
    Diffusers package from Hugging Face gives users full control over Stable Diffusion
    using Python. However, it lacks many key features such as loading custom LoRA
    and textual inversion, utilizing community-shared models/checkpoints, scheduling,
    and weighted prompts, unlimited prompt tokens, and high-resolution image fixing
    and upscaling (The Diffusers package does keep improving over time, however).
  prefs: []
  type: TYPE_NORMAL
- en: This book aims to help you understand all the complex terms and knobs from the
    internal view of the Diffusion model. The book will also assist you in overcoming
    the limitations of Diffusers and implementing the missing functions and advanced
    features to create a fully customized Stable Diffusion application.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the rapid pace of AI technology evolution, this book also aims to
    enable you to quickly adapt to the upcoming changes.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this book, you will not only be able to use Python to generate
    and edit images but also leverage the solutions provided in the book to build
    Stable Diffusion applications for your business and users.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start the journey.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learning Transferable Visual Models From Natural Language* *Supervision*:
    [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Attention Is All You* *Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Denoising Diffusion Probabilistic* *Models*: [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Hierarchical Text-Conditional Image Generation with CLIP* *Latents*: [https://arxiv.org/abs/2204.06125v1](https://arxiv.org/abs/2204.06125v1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*High-Resolution Image Synthesis with Latent Diffusion* *Models*: [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'DALL-E 2: [https://openai.com/dall-e-2](https://openai.com/dall-e-2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
