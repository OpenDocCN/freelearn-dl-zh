<html><head></head><body>
<div><h1 class="chapternumber">7</h1>
<h1 class="chaptertitle" id="_idParaDest-96">Search and Recommendation Engines with LLMs</h1>
<p class="normal">In the previous chapter, we covered the core steps involved in building conversational applications. We started with a plain vanilla chatbot, then added more complex components, such as memory, non-parametric knowledge, and external tools. All of this was made straightforward with the pre-built components of LangChain, as well as Streamlit for UI rendering. Even though conversational applications are often seen as the “comfort zone” of generative AI and LLMs, those models do embrace a wider spectrum of applications.</p>
<p class="normal1">In this chapter, we are going to cover how LLMs can enhance recommendation systems, using both embeddings and generative models. We will learn how to create our own recommendation system application leveraging state-of-the-art LLMs using LangChain as the framework.</p>
<p class="normal1">Throughout this chapter, we will cover the following topics:</p>
<ul class="calibre14">
<li class="bulletlist">Definition and evolutions of recommendation systems</li>
<li class="bulletlist1">How LLMs are impacting this field of research</li>
<li class="bulletlist1">Building recommendation systems with LangChain</li>
</ul>
<h1 class="heading" id="_idParaDest-97">Technical requirements</h1>
<p class="normal">To complete the tasks in this book, you will need the following:</p>
<ul class="calibre14">
<li class="bulletlist">Hugging Face account and a user access token.</li>
<li class="bulletlist1">OpenAI account and a user access token.</li>
<li class="bulletlist1">Python version 3.7.1 or later.</li>
<li class="bulletlist1">Make sure to have the following Python packages installed: <code class="inlinecode">langchain</code>, <code class="inlinecode">python-dotenv</code>, <code class="inlinecode"> huggingface_hub</code>, <code class="inlinecode">streamlit</code>, <code class="inlinecode">lancedb</code>, <code class="inlinecode">openai</code>, and <code class="inlinecode">tiktoken</code>. These can be easily installed via <code class="inlinecode">pip install</code> in your terminal.</li>
</ul>
<p class="normal1">You’ll find the code for this chapter in the book’s GitHub repository at <a href="Chapter_07.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>.</p>
<h1 class="heading" id="_idParaDest-98">Introduction to recommendation systems</h1>
<p class="normal">A recommendation system is a computer<a id="_idIndexMarker457" class="calibre3"/> program that recommends items for users of digital platforms such as e-commerce websites and social networks. It uses large datasets to develop models of users’ likes and interests, and then recommends similar items to individual users.</p>
<p class="normal1">There are different types of recommendation systems, depending on the methods and data they use. Some of the common types are:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Collaborative filtering</strong>: This type of recommendation<a id="_idIndexMarker458" class="calibre3"/> system uses the ratings or feedback of other users<a id="_idIndexMarker459" class="calibre3"/> who have similar preferences to the target user. It assumes that users who liked certain items in the past will like similar items in the future. For example, if user A and user B both liked movies X and Y, then the algorithm may recommend movie Z to user A if user B also liked it.</li>
</ul>
<p class="normal-one">Collaborative filtering can be further divided into two subtypes: user-based and item-based:</p>
<ul class="calibre14">
<li class="bulletlist2"><strong class="screentext">User-based collaborative filtering</strong> finds similar users to the target<a id="_idIndexMarker460" class="calibre3"/> user and recommends items that they liked.</li>
<li class="bulletlist3"><strong class="screentext">Item-based collaborative filtering</strong> finds similar items to the ones<a id="_idIndexMarker461" class="calibre3"/> that the target user liked and recommends them.</li>
</ul>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Content-based filtering</strong>: This type of recommendation system<a id="_idIndexMarker462" class="calibre3"/> uses the features or attributes <a id="_idIndexMarker463" class="calibre3"/>of the items themselves to recommend items that are similar to the ones that the target user has liked or interacted with before. It assumes that users who liked certain features of an item will like other items with similar features. The main difference with item-based collaborative filtering is that, while this latter item-based uses patterns of user behavior to make recommendations, content-based filtering uses information about the items themselves. For example, if user A liked movie X, which is a comedy with actor Y, then the algorithm may recommend movie Z, which is also a comedy<a id="_idIndexMarker464" class="calibre3"/> with actor Y.</li>
<li class="bulletlist1"><strong class="screentext">Hybrid filtering</strong>: This type of recommendation<a id="_idIndexMarker465" class="calibre3"/> system combines both collaborative<a id="_idIndexMarker466" class="calibre3"/> and content-based filtering methods to overcome some of their limitations and provide more accurate and diverse recommendations. For example, YouTube uses hybrid filtering to recommend videos based on both the ratings and views of other users who have watched similar videos, and the features and categories of the videos themselves.</li>
<li class="bulletlist1"><strong class="screentext">Knowledge-based filtering</strong>: This type of recommendation<a id="_idIndexMarker467" class="calibre3"/> system uses explicit knowledge<a id="_idIndexMarker468" class="calibre3"/> or rules about the domain and the user’s needs or preferences to recommend items that satisfy certain criteria or constraints. It does not rely on ratings or feedback from other users, but rather on the user’s input or query. For example, if user A wants to buy a laptop with certain specifications and budget, then the algorithm may recommend a laptop that satisfies those criteria. Knowledge-based recommender systems work well when there is no or little rating history available, or when the items are complex and customizable.</li>
</ul>
<p class="normal1">Within the above frameworks, there are then various machine learning techniques that can be used, which we will cover in the next section.</p>
<h1 class="heading" id="_idParaDest-99">Existing recommendation systems</h1>
<p class="normal">Modern recommendation systems use <strong class="screentext">machine learning</strong> (<strong class="screentext">ML</strong>) techniques to make<a id="_idIndexMarker469" class="calibre3"/> better predictions about users’ preferences, based on the available data such as the following:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">User behavior data</strong>:<strong class="screentext"> </strong>Insights about user interaction<a id="_idIndexMarker470" class="calibre3"/> with a product. This data can be acquired from factors like user ratings, clicks, and purchase records.</li>
<li class="bulletlist1"><strong class="screentext">User demographic data</strong>: This refers to personal information<a id="_idIndexMarker471" class="calibre3"/> about users, including details like age, educational background, income level, and geographical location.</li>
<li class="bulletlist1"><strong class="screentext">Product attribute data</strong>: This involves information<a id="_idIndexMarker472" class="calibre3"/> about the characteristics of a product, such as genres of books, casts of movies, or specific cuisines in the context of food.</li>
</ul>
<p class="normal1">As of today, some of the most popular ML techniques are K-nearest neighbors, dimensionality reduction, and neural networks. Let’s look at these methods in detail.</p>
<h2 class="heading1" id="_idParaDest-100">K-nearest neighbors</h2>
<p class="normal"><strong class="screentext">K-nearest neighbors</strong> (<strong class="screentext">KNN</strong>) is an ML algorithm that can be used<a id="_idIndexMarker473" class="calibre3"/> for both classification and regression problems. It works by finding the <em class="italic">k</em> closest data points (where <em class="italic">k</em> refers to the number of nearest data point you want to find, and is set by the user before initializing the algorithm) to a new data point and using their labels or values to make a prediction. KNN is based on the assumption that similar data points are likely to have similar labels or values.</p>
<p class="normal1">KNN can be applied to recommendation systems in the context of collaborative filtering, both user-based and item-based:</p>
<ul class="calibre14">
<li class="bulletlist">User-based KNN is a type of collaborative<a id="_idIndexMarker474" class="calibre3"/> filtering, which uses the ratings or feedback of other users who have similar tastes or preferences to the target user.</li>
</ul>
<p class="normal-one">For example, let’s say we have three users: Alice, Bob, and Charlie. They all buy books online and rate them. Alice and Bob both liked (rated highly) the series, <em class="italic">Harry Potter</em>, and the book, <em class="italic">The Hobbit</em>. The system sees this pattern and considers Alice and Bob to be similar.</p>
<p class="normal-one">Now, if Bob also liked the book <em class="italic">A Game of Thrones</em>, which Alice hasn’t read yet, the system will recommend <em class="italic">A Game of Thrones</em> to Alice. This is because it assumes that since Alice and Bob have similar tastes, Alice might also like <em class="italic">A Game of Thrones</em>.</p>
<ul class="calibre14">
<li class="bulletlist">Item-based KNN is another type of collaborative<a id="_idIndexMarker475" class="calibre3"/> filtering, which uses the attributes or features of the items to recommend similar items to the target user.</li>
</ul>
<p class="normal-one">For example, let’s consider the same users and their ratings for the books. The system notices that the <em class="italic">Harry Potter</em> series and the book, <em class="italic">The Hobbit</em> are both liked by Alice and Bob. So, it considers these two books to be similar.</p>
<p class="normal-one">Now, if Charlie reads and likes <em class="italic">Harry Potter</em>, the system will recommend <em class="italic">The Hobbit</em> to Charlie. This is because<a id="_idIndexMarker476" class="calibre3"/> it assumes that since <em class="italic">Harry Potter</em> and <em class="italic">The Hobbit</em> are similar (both liked by the same users), Charlie might also like <em class="italic">The Hobbit</em>.</p>
<p class="normal1">KNN is a popular technique in recommendation systems, but it has<a id="_idIndexMarker477" class="calibre3"/> some pitfalls:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Scalability</strong>: KNN can become computationally expensive and slow when dealing with large datasets, as it requires calculating distances between all pairs of items or users.</li>
<li class="bulletlist1"><strong class="screentext">Cold-start problem</strong>: KNN struggles with new items or users that have limited or no interaction history, as it relies on finding neighbors based on historical data.</li>
<li class="bulletlist1"><strong class="screentext">Data sparsity</strong>: KNN performance can degrade in sparse datasets where there are many missing values, making it challenging to find meaningful neighbors.</li>
<li class="bulletlist1"><strong class="screentext">Feature relevance</strong>: KNN treats all features equally and assumes that all features contribute equally to similarity calculations. This may not hold true in scenarios where some features are more relevant than others.</li>
<li class="bulletlist1"><strong class="screentext">Choice of K</strong>: Selecting the appropriate value of K (number of neighbors) can be subjective and impact the quality of recommendations. A small K may result in noise, while a large K may lead to overly broad recommendations.</li>
</ul>
<p class="normal1">Generally speaking, KNN is recommended in scenarios with small datasets with minimal noise (so that outliers, missing values and other noises do not impact the distance metric) and dynamic data (KNN is an instance-based method that doesn’t require retraining and can adapt to changes quickly).</p>
<p class="normal1">Additionally, further techniques are widely used in the file of recommendation systems, such as matrix factorization.</p>
<h2 class="heading1" id="_idParaDest-101">Matrix factorization</h2>
<p class="normal">Matrix factorization is a technique<a id="_idIndexMarker478" class="calibre3"/> used in recommendation systems to analyze and predict user preferences or behaviors based on historical data. It involves decomposing a large matrix into two or more smaller matrices to uncover latent features that contribute to the observed data patterns and address the so-called “curse of dimensionality.”</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">The curse of dimensionality<a id="_idIndexMarker479" class="calibre3"/> refers to challenges that arise when dealing with high-dimensional data. It leads to increased complexity, sparse data, and difficulties in analysis and modeling due to the exponential growth of data requirements and potential overfitting.</p>
</div>
<p class="normal1">In the context of recommendation <a id="_idIndexMarker480" class="calibre3"/>systems, this technique is employed to predict missing values in the user-item interaction matrix, which represents users’ interactions with various items (such as movies, products, or books).</p>
<p class="normal1">Let’s consider the following example. Imagine you have a matrix where rows represent users, columns represent movies, and the cells contain ratings (from 1 as lowest to 5 as highest). However, not all users have rated all movies, resulting in a matrix with many missing entries:</p>
<table class="table-container" id="table001-2">
<tbody class="calibre18">
<tr class="calibre19">
<td class="table-cell"/>
<td class="table-cell">
<p class="normal1">Movie 1</p>
</td>
<td class="table-cell">
<p class="normal1">Movie 2</p>
</td>
<td class="table-cell">
<p class="normal1">Movie 3</p>
</td>
<td class="table-cell">
<p class="normal1">Movie 4</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1">User 1</p>
</td>
<td class="table-cell">
<p class="normal1">4</p>
</td>
<td class="table-cell">
<p class="normal1">-</p>
</td>
<td class="table-cell">
<p class="normal1">5</p>
</td>
<td class="table-cell">
<p class="normal1">-</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1">User 2</p>
</td>
<td class="table-cell">
<p class="normal1">-</p>
</td>
<td class="table-cell">
<p class="normal1">3</p>
</td>
<td class="table-cell">
<p class="normal1">-</p>
</td>
<td class="table-cell">
<p class="normal1">2</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1">User 3</p>
</td>
<td class="table-cell">
<p class="normal1">5</p>
</td>
<td class="table-cell">
<p class="normal1">4</p>
</td>
<td class="table-cell">
<p class="normal1">-</p>
</td>
<td class="table-cell">
<p class="normal1">3</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 7.1: Example of a dataset with missing data</p>
<p class="normal1">Matrix factorization aims to break down this matrix into two matrices: one for users and another for movies, with a reduced number of dimensions (latent factors). These latent factors could represent attributes like genre preferences or specific movie characteristics. By multiplying these matrices, you can predict the missing ratings and recommend movies that the users might enjoy.</p>
<p class="normal1">There are different algorithms<a id="_idIndexMarker481" class="calibre3"/> for matrix factorization, including the following:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Singular value decomposition </strong>(<strong class="screentext">SVD</strong>) decomposes a matrix<a id="_idIndexMarker482" class="calibre3"/> into three separate matrices, where the middle matrix contains singular values that represent the importance of different components in the data. It’s widely used in data compression, dimensionality reduction, and collaborative filtering<a id="_idIndexMarker483" class="calibre3"/> in recommendation systems.</li>
<li class="bulletlist1"><strong class="screentext">Principal component analysis </strong>(<strong class="screentext">PCA</strong>) is a technique to reduce the dimensionality<a id="_idIndexMarker484" class="calibre3"/> of data by transforming it into a new coordinate system aligned with the principal components. These components capture the most significant variability in the data, allowing efficient analysis and visualization.</li>
<li class="bulletlist1"><strong class="screentext">Non-negative matrix factorization </strong>(<strong class="screentext">NMF</strong>) decomposes a matrix into two matrices with non-negative<a id="_idIndexMarker485" class="calibre3"/> values. It’s often used for topic modeling, image processing, and feature extraction, where the components represent non-negative<a id="_idIndexMarker486" class="calibre3"/> attributes.</li>
</ul>
<p class="normal1">In the context of recommendation<a id="_idIndexMarker487" class="calibre3"/> systems, probably the most popular technique is SVD (thanks to its interpretability, flexibility, and ability to handle missing values and performance), so let’s use this one to go on with our example. We will use the Python <code class="inlinecode">numpy</code> module to apply SVD as follows:</p>
<pre class="programlisting"><code class="hljs-code">import numpy as np
# Your user-movie rating matrix (replace with your actual data)
user_movie_matrix = np.array([
    [4, 0, 5, 0],
    [0, 3, 0, 2],
    [5, 4, 0, 3]
])
# Apply SVD
U, s, V = np.linalg.svd(user_movie_matrix, full_matrices=False)
# Number of latent factors (you can choose this based on your preference)
num_latent_factors = 2
# Reconstruct the original matrix using the selected latent factors
reconstructed_matrix = U[:, :num_latent_factors] @ np.diag(s[:num_latent_factors]) @ V[:num_latent_factors, :]
# Replace negative values with 0
reconstructed_matrix = np.maximum(reconstructed_matrix, 0)
print("Reconstructed Matrix:")
print(reconstructed_matrix)
</code></pre>
<p class="normal1">The following is the output:</p>
<pre class="programlisting1"><code class="hljs-con">Reconstructed Matrix:
[[4.2972542  0.         4.71897811 0.        ]
 [1.08572801 2.27604748 0.         1.64449028]
 [4.44777253 4.36821972 0.52207171 3.18082082]]
</code></pre>
<p class="normal1">In this example, the <code class="inlinecode">U</code> matrix contains user-related information, the <code class="inlinecode">s</code> matrix contains singular values, and the <code class="inlinecode">V</code> matrix contains movie-related information. By selecting a certain number of latent factors (<code class="inlinecode">num_latent_factors</code>), you can reconstruct the original matrix with reduced dimensions, while setting the <code class="inlinecode">full_matrices=False</code> parameter in the <code class="inlinecode">np.linalg.svd</code> function ensures that the decomposed matrices are truncated to have dimensions consistent with the selected number of latent factors.</p>
<p class="normal1">These predicted ratings can then be used to recommend movies with higher predicted ratings to users. Matrix factorization enables recommendation systems to uncover hidden patterns in user preferences and make personalized recommendations based on those patterns.</p>
<p class="normal1">Matrix factorization has been a widely<a id="_idIndexMarker488" class="calibre3"/> used technique in recommendation systems, especially when dealing with large datasets containing a substantial number of users and items, since it efficiently captures latent factors even in such scenarios; or when you want personalized recommendations based on latent factors, since it learns unique latent representations for each user and item. However, it has some pitfalls (some similar to the KNN’s technique):</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Cold-start problem</strong>: Similar to KNN, matrix factorization<a id="_idIndexMarker489" class="calibre3"/> struggles with new items or users that have limited or no interaction history. Since it relies on historical data, it can’t effectively provide recommendations for new items or users.</li>
<li class="bulletlist1"><strong class="screentext">Data sparsity</strong>: As the number of users and items grows, the user-item interaction matrix becomes increasingly sparse, leading to challenges in accurately predicting missing values.</li>
<li class="bulletlist1"><strong class="screentext">Scalability</strong>: For large datasets, performing matrix factorization can be computationally expensive and time-consuming.</li>
<li class="bulletlist1"><strong class="screentext">Limited context</strong>: Matrix factorization typically only considers user-item interactions, ignoring contextual information like time, location, or additional<a id="_idIndexMarker490" class="calibre3"/> user attributes.</li>
</ul>
<p class="normal1">Hence, <strong class="screentext">neural networks </strong>(<strong class="screentext">NNs</strong>) have been explored as an alternative<a id="_idIndexMarker491" class="calibre3"/> to mitigate these pitfalls in recent years.</p>
<h2 class="heading1" id="_idParaDest-102">Neural networks</h2>
<p class="normal">NNs are used in recommendation<a id="_idIndexMarker492" class="calibre3"/> systems to improve the accuracy and personalization of recommendations by learning intricate patterns from data. Here’s how neural networks are commonly applied in this context:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Collaborative filtering with neural networks</strong>: Neural networks can model<a id="_idIndexMarker493" class="calibre3"/> user-item interactions by embedding users and items into continuous vector spaces. These embeddings capture latent features that represent user preferences and item characteristics. Neural collaborative filtering models combine these embeddings with neural network architectures to predict ratings or interactions between users and items.</li>
<li class="bulletlist1"><strong class="screentext">Content-based recommendations</strong>: In content-based recommendation<a id="_idIndexMarker494" class="calibre3"/> systems, neural networks can learn representations of item content, such as text, images, or audio. These representations capture<a id="_idIndexMarker495" class="calibre3"/> item characteristics and user preferences. Neural networks like <strong class="screentext">convolutional neural networks</strong> (<strong class="screentext">CNNs</strong>) and <strong class="screentext">recurrent neural networks</strong> (<strong class="screentext">RNNs</strong>) are used to process and learn<a id="_idIndexMarker496" class="calibre3"/> from item content, enabling personalized content-based recommendations.</li>
<li class="bulletlist1"><strong class="screentext">Sequential models</strong>: In scenarios where user interactions<a id="_idIndexMarker497" class="calibre3"/> have a temporal sequence, such as clickstreams or browsing history, RNNs or variants such as <strong class="screentext">long short-term memory </strong>(<strong class="screentext">LSTM</strong>) networks can capture temporal<a id="_idIndexMarker498" class="calibre3"/> dependencies in the user behavior and make sequential recommendations.</li>
<li class="bulletlist1"><strong class="screentext">Autoencoders and variational autoencoders </strong>(<strong class="screentext">VAEs</strong>) can be used to learn low-dimensional<a id="_idIndexMarker499" class="calibre3"/> representations<a id="_idIndexMarker500" class="calibre3"/> of users and items.</li>
</ul>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Autoencoders are a type of neural<a id="_idIndexMarker501" class="calibre3"/> network architecture used for unsupervised learning and dimensionality reduction. They consist of an encoder and a decoder. The encoder maps the input data into a lower-dimensional latent space representation, while the decoder attempts to reconstruct the original input data from the encoded representation.</p>
<p class="normal1">VAEs<a id="_idIndexMarker502" class="calibre3"/> are an extension of traditional autoencoders that introduce probabilistic elements. VAEs not only learn to encode the input data into a latent space but also model the distribution of this latent space using probabilistic methods. This allows for the generation of new data samples from the learned latent space. VAEs are used for generative tasks like image synthesis, anomaly detection, and data imputation.</p>
</div>
<p class="normal1">In both autoencoders and VAEs, the idea is to learn<a id="_idIndexMarker503" class="calibre3"/> a compressed and meaningful representation of the input data in the latent space, which can be useful for various tasks including feature extraction, data generation, and dimensionality reduction.</p>
<p class="normal1">These representations can then be used to make recommendations by identifying similar users and items in the latent space. In fact, the unique architecture that features<a id="_idIndexMarker504" class="calibre3"/> NNs allows for the following techniques:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Side information integration</strong>: NNs can incorporate additional user and item attributes, such as demographic information, location, or social connections, to improve recommendations by learning from diverse data sources.</li>
<li class="bulletlist1"><strong class="screentext">Deep reinforcement learning</strong>: In certain scenarios, deep reinforcement learning can be used to optimize recommendations over time, learning from user feedback to suggest actions that maximize long-term rewards.</li>
</ul>
<p class="normal1">NNs offer flexibility and the ability to capture complex patterns in data, making them well suited for recommendation systems. However, they also require careful design, training, and tuning to achieve optimal performance. NNs also bring their own challenges, including<a id="_idIndexMarker505" class="calibre3"/> the following:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Increased complexity</strong>: NNs, especially <strong class="screentext">deep neural networks</strong> (<strong class="screentext">DNNs</strong>), can become incredibly complex<a id="_idIndexMarker506" class="calibre3"/> due to their layered architecture. As we add more hidden layers and neurons, the model’s capacity to learn intricate patterns increases.</li>
<li class="bulletlist1"><strong class="screentext">Training requirements</strong>: NNs are heavy models whose training requires special hardware requirements including GPUs, which might be very expensive.</li>
<li class="bulletlist1"><strong class="screentext">Potential overfitting</strong>: Overfitting occurs when an ANN learns to perform exceptionally<a id="_idIndexMarker507" class="calibre3"/> well on the training data but fails to generalize to unseen data</li>
</ul>
<p class="normal1">Selecting appropriate architectures, handling large datasets, and tuning hyperparameters are essential to effectively use NNs in recommendation systems.</p>
<p class="normal1">Even though relevant advancements have been made in recent years, the aforementioned techniques still suffer from some pitfalls, primarily their being task-specific. For example, a rating-prediction recommendation system will not be able to tackle a task where we need to recommend the top <em class="italic">k</em> items that likely match the user’s taste. Actually, if we extend this limitation to other “pre-LLMs” AI solutions, we might see some similarities: it is indeed the task-specific situation that LLMs and, more generally, Large Foundation Models are revolutionizing, being highly generalized and adaptable to various tasks, depending on user’s prompts and instructions. Henceforth, extensive research in the field of recommendation systems is being done into what extent LLMs can enhance<a id="_idIndexMarker508" class="calibre3"/> the current models. In the following sections, we will cover the theory behind these new approaches referring to recent papers and blogs about this emerging domain.</p>
<h1 class="heading" id="_idParaDest-103">How LLMs are changing recommendation systems</h1>
<p class="normal">We saw in previous chapters<a id="_idIndexMarker509" class="calibre3"/> how LLMs can be customized in three main ways: pre-training, fine-tuning, and prompting. According to the paper <em class="italic">Recommender systems in the Era of Large Language Models (LLMs)</em> from Wenqi Fan et al., these techniques can also be used to tailor an LLM to be a recommender system:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Pre-training</strong>: Pre-training LLMs for recommender systems<a id="_idIndexMarker510" class="calibre3"/> is an important step to enable LLMs to acquire extensive world knowledge and user preferences, and to adapt to different recommendation tasks with zero or few shots.</li>
</ul>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">An example of a recommendation<a id="_idIndexMarker511" class="calibre3"/> system LLM is P5, introduced by Shijie Gang et al. in their paper <em class="italic">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5)</em>.</p>
<p class="normal1">P5 is a unified text-to-text paradigm<a id="_idIndexMarker512" class="calibre3"/> for building recommender systems using <strong class="screentext">large language models</strong> (<strong class="screentext">LLMs</strong>). It consists of three steps:</p>
<ul class="calibre14">
<li class="bulletlist">Pretrain: A foundation language model based on T5 architecture is pretrained on a large-scale web corpus and fine-tuned on recommendation tasks.</li>
<li class="bulletlist1">Personalized prompt: A personalized prompt is generated for each user based on their behavior data and contextual features.</li>
<li class="bulletlist1">Predict: The personalized prompt is fed into the pretrained language model to generate recommendations.</li>
</ul>
<p class="normal1">P5 is based on the idea that LLMs can encode extensive world knowledge and user preferences and can be adapted to different recommendation tasks with zero or few shots.</p>
</div>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Fine-tuning</strong>: Training an LLM from scratch<a id="_idIndexMarker513" class="calibre3"/> is a highly computational-intensive activity. An alternative and less intrusive approach to customize an LLM for recommendation systems might be fine-tuning.</li>
</ul>
<p class="normal-one">More specifically, the authors of the paper review two main strategies for fine-tuning LLMs:</p>
<ul class="calibre14">
<li class="bulletlist2"><strong class="screentext">Full-model fine-tuning</strong> involves changing<a id="_idIndexMarker514" class="calibre3"/> the entire model’s weights based on task-specific recommendation datasets.</li>
<li class="bulletlist3"><strong class="screentext">Parameter-efficient fine-tuning</strong> aims to change<a id="_idIndexMarker515" class="calibre3"/> only a small part of weights or develop trainable adapters to fit specific tasks.</li>
</ul>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Prompting</strong>: The third and “lightest” way of tailoring<a id="_idIndexMarker516" class="calibre3"/> LLMs to be recommender systems is prompting. According to the authors, there are three main techniques for prompting LLMs:<ul class="calibre17">
<li class="bulletlist2"><strong class="screentext">Conventional prompting </strong>aims to unify downstream tasks into language generation tasks by designing text templates or providing a few input-output examples.</li>
<li class="bulletlist3"><strong class="screentext">In-context learning </strong>enables LLMs to learn new tasks based on contextual information without fine-tuning.</li>
<li class="bulletlist3"><strong class="screentext">Chain-of-thought</strong> enhances the reasoning abilities of LLMs by providing multiple demonstrations to describe the chain of thought as examples within the prompt. The authors also discuss the advantages and challenges of each technique and provide some examples of existing methods that adopt them.</li>
</ul>
</li>
</ul>
<p class="normal-one">Regardless of the typology, prompting is the fastest way to test whether a general-purpose LLM<a id="_idIndexMarker517" class="calibre3"/> can tackle recommendation systems’ tasks.</p>
<p class="normal1">The application of LLMs within the recommendation system domain is raising interest in the research field, and there is already some interesting evidence of the results as seen above.</p>
<p class="normal1">In the next section, we are going to implement our own recommendation application using the prompting approach and leveraging the capabilities of LangChain as an AI orchestrator.</p>
<h1 class="heading" id="_idParaDest-104">Implementing an LLM-powered recommendation system</h1>
<p class="normal">Now that we have covered some theory<a id="_idIndexMarker518" class="calibre3"/> about recommendation systems and emerging research on how LLMs can enhance them, let’s start building our recommendation app, which will be a movie<a id="_idIndexMarker519" class="calibre3"/> recommender system called MovieHarbor. The goal will be to make it as general as possible, meaning that we want our app to be able to address various recommendations tasks with a conversational interface. The scenario we are going to simulate<a id="_idIndexMarker520" class="calibre3"/> will be that of the so-called “cold start,” concerning the first interaction of a user with the recommendation system where we do not have the user’s preference history. We will leverage a movie database with textual descriptions.</p>
<p class="normal1">For this purpose, we will <a id="_idIndexMarker521" class="calibre3"/>use the <em class="italic">Movie recommendation data</em> dataset, available on Kaggle at <a href="https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data" class="calibre3">https://www.kaggle.com/datasets/rohan4050/movie-recommendation-data</a>.</p>
<p class="normal1">The reason for using a dataset with a textual description of each movie (alongside information such as ratings and movie titles) is so that we can get the embeddings of the text. So let’s start building our MovieHarbor application.</p>
<h2 class="heading1" id="_idParaDest-105">Data preprocessing</h2>
<p class="normal">In order to apply LLMs<a id="_idIndexMarker522" class="calibre3"/> to our dataset, we first need to preprocess the data. The initial dataset included several columns; however, the ones we are interested in are the following:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Genres</strong>: A list of applicable genres for the movie.</li>
<li class="bulletlist1"><strong class="screentext">Title</strong>: The movie’s title.</li>
<li class="bulletlist1"><strong class="screentext">Overview</strong>: Textual description of the plot.</li>
<li class="bulletlist1"><strong class="screentext">Vote_average</strong>: A rating from 1 to 10 for a given movie</li>
<li class="bulletlist1"><strong class="screentext">Vote_count</strong>: The number of votes for a given movie.</li>
</ul>
<p class="normal1">I won’t report here the whole code (you can find it in the GitHub repo of this book at <a href="Chapter_07.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>), however, I will share the main steps of data preprocessing:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">First, we format the <code class="inlinecode">genres</code> column into a <code class="inlinecode">numpy</code> array, which is easier to handle than the original dictionary format in the dataset:
        <pre class="programlisting2"><code class="hljs-code">import pandas as pd
import ast
# Convert string representation of dictionaries to actual dictionaries
md['genres'] = md['genres'].apply(ast.literal_eval)
# Transforming the 'genres' column
md['genres'] = md['genres'].apply(lambda x: [genre['name'] for genre in x])
</code></pre>
</li>
<li class="bulletlist1">Next, we merge the <code class="inlinecode">vote_average</code> and <code class="inlinecode">vote_count</code> columns into a single column, which is the weighted ratings with respect to the number of votes. I’ve also limited the rows to the 95<sup class="superscript">th</sup> percentile of the number of votes, so that we can get rid of minimum vote counts to prevent skewed results:
        <pre class="programlisting2"><code class="hljs-code"># Calculate weighted rate (IMDb formula)
def calculate_weighted_rate(vote_average, vote_count, min_vote_count=10):
    return (vote_count / (vote_count + min_vote_count)) * vote_average + (min_vote_count / (vote_count + min_vote_count)) * 5.0
# Minimum vote count to prevent skewed results
vote_counts = md[md['vote_count'].notnull()]['vote_count'].astype('int')
min_vote_count = vote_counts.quantile(0.95)
# Create a new column 'weighted_rate'
md['weighted_rate'] = md.apply(lambda row: calculate_weighted_rate(row['vote_average'], row['vote_count'], min_vote_count), axis=1)
</code></pre>
</li>
<li class="bulletlist1">Next, we create<a id="_idIndexMarker523" class="calibre3"/> a new column called <code class="inlinecode">combined_info</code> where we are going to merge all the elements that will be provided as context to the LLMs. Those elements are the movie title, overview, genres, and ratings:
        <pre class="programlisting2"><code class="hljs-code">md_final['combined_info'] = md_final.apply(lambda row: f"Title: {row['title']}. Overview: {row['overview']} Genres: {', '.join(row['genres'])}. Rating: {row['weighted_rate']}", axis=1).astype(str)
</code></pre>
</li>
<li class="bulletlist1">We tokenize the movie <code class="inlinecode">combined_info</code> so that we will get better results while embedding:
        <pre class="programlisting2"><code class="hljs-code">import pandas as pd
import tiktoken
import os
import openai
openai.api_key = os.environ["OPENAI_API_KEY"]
from openai.embeddings_utils import get_embedding
embedding_encoding = "cl100k_base" # this the encoding for text-embedding-ada-002
max_tokens = 8000 # the maximum for text-embedding-ada-002 is 8191
encoding = tiktoken.get_encoding(embedding_encoding)
# omit reviews that are too long to embed
md_final["n_tokens"] = md_final.combined_info.apply(lambda x: len(encoding.encode(x)))
md_final = md_final[md_final.n_tokens &lt;= max_tokens]
</code></pre>
</li>
</ol>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1"><code class="inlinecode">cl100k_base</code> is the name of a tokenizer used by OpenAI’s embeddings API. A tokenizer is a tool that splits a text<a id="_idIndexMarker524" class="calibre3"/> string into units called tokens, which can then be processed by a neural network. Different tokenizers have different rules and vocabularies for how to split the text and what tokens to use.</p>
<p class="normal1">The <code class="inlinecode">cl100k_base</code> tokenizer is based on the <strong class="screentext">byte pair encoding</strong> (<strong class="screentext">BPE</strong>) algorithm, which learns<a id="_idIndexMarker525" class="calibre3"/> a vocabulary of subword units from a large corpus of text. The <code class="inlinecode">cl100k_base</code> tokenizer has a vocabulary of 100,000 tokens, which are mostly common words and word pieces, but also include some special tokens for punctuation, formatting, and control. It can handle texts in multiple languages and domains, and can encode up to 8,191 tokens per input.</p>
</div>
<ol class="calibre15">
<li class="bulletlist1" value="5">We embed the text<a id="_idIndexMarker526" class="calibre3"/> with <code class="inlinecode">text-embedding-ada-002</code>:
        <pre class="programlisting2"><code class="hljs-code">md_final["embedding"] = md_final.overview.apply(lambda x: get_embedding(x, engine=embedding_model))
</code></pre>
</li>
</ol>
<p class="normal-one">After changing some columns’ names and dropping unnecessary columns, the final dataset looks as follows:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_07_01.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 7.1: Sample of the final movies dataset</p>
<p class="normal-one">Let’s have a look at a random row of text:</p>
<pre class="programlisting2"><code class="hljs-code">md['text'][0]
</code></pre>
<p class="normal-one">The following output is obtained:</p>
<pre class="programlisting3"><code class="hljs-con">'Title: GoldenEye. Overview: James Bond must unmask the mysterious head of the Janus Syndicate and prevent the leader from utilizing the GoldenEye weapons system to inflict devastating revenge on Britain. Genres: Adventure, Action, Thriller. Rating: 6.173464373464373'
</code></pre>
<p class="normal-one">The last change we will make is modifying some naming conventions and data types as follows:</p>
<pre class="programlisting2"><code class="hljs-code">md_final.rename(columns = {'embedding': 'vector'}, inplace = True)
md_final.rename(columns = {'combined_info': 'text'}, inplace = True)
md_final.to_pickle('movies.pkl')
</code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="6">Now that we have our final dataset, we need to store it in a VectorDB. For this purpose, we<a id="_idIndexMarker527" class="calibre3"/> are going to leverage <strong class="screentext">LanceDB</strong>, an open-source database for vector-search built with persistent storage, which greatly simplifies the retrieval, filtering, and management of embeddings and also offers a native integration with LangChain. You can easily install LanceDB via <code class="inlinecode">pip install lancedb</code>:
        <pre class="programlisting2"><code class="hljs-code">import lancedb
uri = "data/sample-lancedb"
db = lancedb.connect(uri)
table = db.create_table("movies", md)
</code></pre>
</li>
</ol>
<p class="normal1">Now that we have all our ingredients, we can start working with those embeddings and start building our recommendation<a id="_idIndexMarker528" class="calibre3"/> system. We will start with a simple task in a cold-start scenario, adding progressive layers of complexity with LangChain components. Afterwards, we will also try a content-based scenario to challenge our LLMs with diverse tasks.</p>
<h2 class="heading1" id="_idParaDest-106">Building a QA recommendation chatbot in a cold-start scenario</h2>
<p class="normal">In previous sections, we saw<a id="_idIndexMarker529" class="calibre3"/> how the cold-start<a id="_idIndexMarker530" class="calibre3"/> scenario – that means interacting with a user for the first time without their backstory – is a problem often encountered by recommendation systems. The less information we have about a user, the harder it is to match the recommendations to their preferences.</p>
<p class="normal1">In this section, we are going to simulate a cold-start scenario with LangChain and OpenAI’s LLMs with the following high-level architecture:</p>
<figure class="mediaobject"><img alt="A diagram of a computer  Description automatically generated" src="img/B21714_07_02.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 7.2: High-level architecture of recommendation system in a cold-start scenario</p>
<p class="normal1">In the previous section, we’ve already saved our embeddings in LanceDB. Now, we are going to build a LangChain RetrievalQA retriever, a chain component designed for question-answering against an index. In our case, we will use the vector store as our index retriever. The idea is that the chain returns the top <em class="italic">k</em> most similar movies upon the user’s query, using cosine similarity as the distance metric (which is the default).</p>
<p class="normal1">So, let’s start building the chain:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">We are using only the movie overview as information input:
        <pre class="programlisting2"><code class="hljs-code">from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import LanceDB
os.environ["OPENAI_API_KEY"]
embeddings = OpenAIEmbeddings()
docsearch = LanceDB(connection = table, embedding = embeddings)
query = "I'm looking for an animated action movie. What could you suggest to me?"
docs = docsearch.similarity_search(query)
docs
</code></pre>
</li>
</ol>
<p class="normal-one">The following is the corresponding<a id="_idIndexMarker531" class="calibre3"/> output (I will display a truncated version of the output, showing<a id="_idIndexMarker532" class="calibre3"/> only the first out of four document sources):</p>
<pre class="programlisting3"><code class="hljs-con">[Document(page_content='Title: Hitman: Agent 47. Overview: An assassin teams up with a woman to help her find her father and uncover the mysteries of her ancestry. Genres: Action, Crime, Thriller. Rating: 5.365800865800866', metadata={'genres': array(['Action', 'Crime', 'Thriller'], dtype=object), 'title': 'Hitman: Agent 47', 'overview': 'An assassin teams up with a woman to help her find her father and uncover the mysteries of her ancestry.', 'weighted_rate': 5.365800865800866, 'n_tokens': 52, 'vector': array([-0.00566491, -0.01658553, […]
</code></pre>
<p class="normal-one">As you can see, alongside each <code class="inlinecode">Document</code>, all variables are reported as metadata, plus the distance is also reported as a score. The lower the distance, the greater the proximity between the user’s query and the movie’s text embedding.</p>
<ol class="calibre15">
<li class="bulletlist1" value="2">Once we have gathered the most similar documents, we want a conversational response. For this goal, in addition to the embedding models, we will also use OpenAI’s completion model GPT-3 and combine it in RetrievalQA:
        <pre class="programlisting2"><code class="hljs-code">qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=docsearch.as_retriever(), return_source_documents=True)
query = "I'm looking for an animated action movie. What could you suggest to me?"
result = qa({"query": query})
result['result']
</code></pre>
</li>
</ol>
<p class="normal-one">Let’s look at the output:</p>
<pre class="programlisting3"><code class="hljs-con">' I would suggest Transformers. It is an animated action movie with genres of Adventure, Science Fiction, and Action, and a rating of 6.'
</code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="3">Since we set<a id="_idIndexMarker533" class="calibre3"/> the <code class="inlinecode">return_source_documents=True</code> parameter, we can<a id="_idIndexMarker534" class="calibre3"/> also retrieve the document sources:
        <pre class="programlisting2"><code class="hljs-code">result['source_documents'][0]
</code></pre>
</li>
</ol>
<p class="normal-one">The following is the output:</p>
<pre class="programlisting3"><code class="hljs-con">Document(page_content='Title: Hitman: Agent 47. Overview: An assassin teams up with a woman to help her find her father and uncover the mysteries of her ancestry. Genres: Action, Crime, Thriller. Rating: 5.365800865800866', metadata={'genres': array(['Action', 'Crime', 'Thriller'], dtype=object), 'title': 'Hitman: Agent 47', 'overview': 'An assassin teams up with a woman to help her find her father and uncover the mysteries of her ancestry.', 'weighted_rate': 5.365800865800866, 'n_tokens': 52, 'vector': array([-0.00566491, -0.01658553, -0.02255735, ..., -0.01242317,
       -0.01303058, -0.00709073], dtype=float32), '_distance': 0.42414575815200806})
</code></pre>
<p class="normal-one">Note that the first document reported is not the one the model suggested. This occurred probably because of the rating, which is lower than Transformers (which was only the third result). This is a great example of how the LLM was able to consider multiple factors, on top of similarity, to suggest a movie to the user.</p>
<ol class="calibre15">
<li class="bulletlist1" value="4">The model was able to generate a conversational answer, however, it is still using only a part of the available information – the textual overview. What if we want our MovieHarbor system to also leverage the other variables? We can approach the task in two ways:<ul class="calibre17">
<li class="bulletlist2"><strong class="screentext">The “filter” way</strong>: This approach consists <a id="_idIndexMarker535" class="calibre3"/>of adding some filters as <strong class="screentext">kwargs</strong> to our retriever, which<a id="_idIndexMarker536" class="calibre3"/> might be required by the application before responding to the user. Those questions might be, for example, about the genre of a movie.</li>
</ul>
<p class="normal1">For example, let’s say<a id="_idIndexMarker537" class="calibre3"/> we want to provide<a id="_idIndexMarker538" class="calibre3"/> results featuring only those movies for which the genre is tagged as comedy. You can achieve this with the following code:</p>
<pre class="programlisting"><code class="hljs-code">df_filtered = md[md['genres'].apply(lambda x: 'Comedy' in x)]
qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff",
    retriever=docsearch.as_retriever(search_kwargs={'data': df_filtered}), return_source_documents=True)
query = "I'm looking for a movie with animals and an adventurous plot."
result = qa({"query": query})
</code></pre>
<p class="normal1">The filter can also operate at the metadata level, as shown in the following example, where<a id="_idIndexMarker539" class="calibre3"/> we want to filter only results with a rating above 7:</p>
<pre class="programlisting"><code class="hljs-code">qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff",
    retriever=docsearch.as_retriever(search_kwargs={'filter': {weighted_rate__gt:7}}), return_source_documents=True)
</code></pre>
<ul class="calibre17">
<li class="bulletlist2"><strong class="screentext">The “agentic” way</strong>: This is probably the most<a id="_idIndexMarker540" class="calibre3"/> innovative way to approach the problem. Making our chain agentic means converting the retriever to a tool that the agent can leverage if needed, including the additional variables. By doing so, it would be sufficient for the user to provide their preferences in natural language so that the agent can retrieve the most promising recommendation if needed.</li>
</ul>
<p class="normal1">Let’s see how to implement this with code, asking specifically for an action movie (thus filtering on the <code class="inlinecode">genre</code> variable):</p>
<pre class="programlisting"><code class="hljs-code">from langchain.agents.agent_toolkits import create_retriever_tool
from langchain.agents.agent_toolkits import create_conversational_retrieval_agent
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(temperature = 0)
retriever = docsearch.as_retriever(return_source_documents = True)
tool = create_retriever_tool(
    retriever,
    "movies",
    "Searches and returns recommendations about movies."
)
tools = [tool]
agent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True)
result = agent_executor({"input": "suggest me some action movies"})
</code></pre>
</li>
</ol>
<p class="normal-one">Let’s see a glimpse<a id="_idIndexMarker541" class="calibre3"/> of the chain of thoughts<a id="_idIndexMarker542" class="calibre3"/> and the output produced (always based on the four most similar movies according to cosine similarity):</p>
<pre class="programlisting3"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
Invoking: `movies` with `{'genre': 'action'}`
[Document(page_content='The action continues from [REC], […]
Here are some action movies that you might enjoy:
1. [REC]² - The action continues from [REC], with a medical officer and a SWAT team sent into a sealed-off apartment to control the situation. It is a thriller/horror movie.
2. The Boondock Saints - Twin brothers Conner and Murphy take swift retribution into their own hands to rid Boston of criminals. It is an action/thriller/crime movie.
3. The Gamers - Four clueless players are sent on a quest to rescue a princess and must navigate dangerous forests, ancient ruins, and more. It is an action/comedy/thriller/foreign movie.
4. Atlas Shrugged Part III: Who is John Galt? - In a collapsing economy, one man has the answer while others try to control or save him. It is a drama/science fiction/mystery movie.
Please note that these recommendations are based on the genre "action" and may vary in terms of availability and personal preferences.
&gt; Finished chain.
</code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="5">Finally, we might also want<a id="_idIndexMarker543" class="calibre3"/> to make our application<a id="_idIndexMarker544" class="calibre3"/> more tailored toward its goal of being a recommender system. To do so, we need to do some prompt engineering.</li>
</ol>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">One of the advantages<a id="_idIndexMarker545" class="calibre3"/> of using LangChain’s pre-built components, such as the RetrievalQA chain, is that they come with a pre-configured, well-curated prompt template. Before overriding the existing prompt, it’s a good practice to inspect it, so that you can also see which variables (within <code class="inlinecode">{}</code>) are already expected from the component.</p>
</div>
<p class="normal-one">To explore the existing prompt, you can run the following code:</p>
<pre class="programlisting2"><code class="hljs-code">print(qa.combine_documents_chain.llm_chain.prompt.template)
</code></pre>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}
Question: {question}
Helpful Answer:
</code></pre>
<p class="normal-one">Let’s say, for example, that we want<a id="_idIndexMarker546" class="calibre3"/> our system to return<a id="_idIndexMarker547" class="calibre3"/> three suggestions for each user’s request, with a short description of the plot and the reason why the user might like it. The following is a sample prompt that could match this goal:</p>
<pre class="programlisting2"><code class="hljs-code">from langchain.prompts import PromptTemplate
template = """You are a movie recommender system that help users to find movies that match their preferences.
Use the following pieces of context to answer the question at the end.
For each question, suggest three movies, with a short description of the plot and the reason why the user migth like it.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}
Question: {question}
Your response:"""
 
PROMPT = PromptTemplate(
    template=template, input_variables=["context", "question"])
</code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="6">Now we need to pass it into our chain:
        <pre class="programlisting2"><code class="hljs-code">PROMPT = PromptTemplate(
    template=template, input_variables=["context", "question"])
chain_type_kwargs = {"prompt": PROMPT}
qa = RetrievalQA.from_chain_type(llm=OpenAI(),
    chain_type="stuff",
    retriever=docsearch.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs)
query = "I'm looking for a funny action movie, any suggestion?"
result = qa({'query':query})
print(result['result'])
</code></pre>
</li>
</ol>
<p class="normal-one">The following<a id="_idIndexMarker548" class="calibre3"/> output is <a id="_idIndexMarker549" class="calibre3"/>obtained:</p>
<pre class="programlisting3"><code class="hljs-con">1. A Good Day to Die Hard: An action-packed comedy directed by John Moore, this movie follows Iconoclastic, take-no-prisoners cop John McClane as he travels to Moscow to help his wayward son Jack. With the Russian underworld in pursuit, and battling a countdown to war, the two McClanes discover that their opposing methods make them unstoppable heroes.
2. The Hidden: An alien is on the run in America and uses the bodies of anyone in its way as a hiding place. With lots of innocent people dying in the chase, this action-packed horror movie is sure to keep you laughing.
3. District B13: Set in the ghettos of Paris in 2010, this action-packed science fiction movie follows an undercover cop and ex-thug as they try to infiltrate a gang in order to defuse a neutron bomb. A thrilling comedy that will keep you laughing.
</code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="7">Another thing that we might<a id="_idIndexMarker550" class="calibre3"/> want to implement in our prompt is the information gathered with the conversational<a id="_idIndexMarker551" class="calibre3"/> preliminary questions that we might want to set as a welcome page. For example, before letting the user input their natural language question, we might want to ask their age, gender, and favorite movie genre. To do so, we can insert in our prompt a section where we can format the input variables with those shared by the user, and then combine this prompt chunk in the final prompt we are going to pass to the chain. Below you can find an example (for simplicity, we are going to set the variables without asking the user):
        <pre class="programlisting2"><code class="hljs-code">from langchain.prompts import PromptTemplate
template_prefix = """You are a movie recommender system that help users to find movies that match their preferences.
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}"""
user_info = """This is what we know about the user, and you can use this information to better tune your research:
Age: {age}
Gender: {gender}"""
template_suffix= """Question: {question}
Your response:"""
user_info = user_info.format(age = 18, gender = 'female')
COMBINED_PROMPT = template_prefix +'\n'+ user_info +'\n'+ template_suffix
print(COMBINED_PROMPT)
</code></pre>
</li>
</ol>
<p class="normal-one">Here is<a id="_idIndexMarker552" class="calibre3"/> the <a id="_idIndexMarker553" class="calibre3"/>output:</p>
<pre class="programlisting3"><code class="hljs-con">You are a movie recommender system that help users to find movies that match their preferences.
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}
This is what we know about the user, and you can use this information to better tune your research:
Age: 18
Gender: female
Question: {question}
Your response:
</code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="8">Now let’s format the prompt and pass it into our chain:
        <pre class="programlisting2"><code class="hljs-code">PROMPT = PromptTemplate(
    template=COMBINED_PROMPT, input_variables=["context", "question"])
chain_type_kwargs = {"prompt": PROMPT}
qa = RetrievalQA.from_chain_type(llm=OpenAI(),
    chain_type="stuff",
    retriever=docsearch.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs)
result = qa({'query':query})
result['result']
</code></pre>
</li>
</ol>
<p class="normal-one">We receive the following output:</p>
<pre class="programlisting3"><code class="hljs-con">' Sure, I can suggest some action movies for you. Here are a few examples: A Good Day to Die Hard, Goldfinger, Ong Bak 2, and The Raid 2. All of these movies have high ratings and feature thrilling action elements. I hope you find something that you enjoy!'
</code></pre>
<p class="normal1">As you can see, the system considered<a id="_idIndexMarker554" class="calibre3"/> the user’s information<a id="_idIndexMarker555" class="calibre3"/> provided. When we build the front-end of MovieHarbor, we will make this information dynamic as preliminary questions proposed to the user.</p>
<h2 class="heading1" id="_idParaDest-107">Building a content-based system</h2>
<p class="normal">In the previous section, we covered <a id="_idIndexMarker556" class="calibre3"/>the cold-start scenario where the system knew nothing about the user. Sometimes, recommender systems already have some backstory about users, and it is extremely useful to embed this knowledge in our application. Let’s imagine, for example, that we have a users database where the system has stored all the registered user’s information (such as age, gender, country, etc.) as well as the movies the user has already watched alongside their rating.</p>
<p class="normal1">To do so, we will need to set a custom prompt that is able to retrieve this information from a source. For simplicity, we will create a sample dataset with users’ information with just two records, corresponding to two users. Each user will exhibit the following variables: username, age, gender, and a dictionary containing movies already watched alongside with the rating they gave to them.</p>
<p class="normal1">The high-level architecture is represented by the following diagram:</p>
<figure class="mediaobject"><img alt="A diagram of a computer flowchart  Description automatically generated" src="img/B21714_07_03.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 7.3: High-level architecture of a content-based recommendation system</p>
<p class="normal1">Let’s break down this architecture<a id="_idIndexMarker557" class="calibre3"/> and examine each step to build the final chat for this content-based system, starting from the available users’ data:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">As discussed earlier, we now have a bit of information about our users’ preferences. More specifically, imagine we have a dataset containing users’ attributes (name, age, gender) along with their reviews (a score from 1 to 10) of some movies. The following is the code used to create the dataset:
        <pre class="programlisting2"><code class="hljs-code">import pandas as pd
data = {
    "username": ["Alice", "Bob"],
    "age": [25, 32],
    "gender": ["F", "M"],
    "movies": [
        [("Transformers: The Last Knight", 7), ("Pokémon: Spell of the Unknown", 5)],
        [("Bon Cop Bad Cop 2", 8), ("Goon: Last of the Enforcers", 9)]
    ]
}
# Convert the "movies" column into dictionaries
for i, row_movies in enumerate(data["movies"]):
    movie_dict = {}
    for movie, rating in row_movies:
        movie_dict[movie] = rating
    data["movies"][i] = movie_dict
# Create a pandas DataFrame
df = pd.DataFrame(data)
df.head()
</code></pre>
</li>
</ol>
<p class="normal-one">The following output is obtained:</p>
<figure class="mediaobject"><img alt="A black and white screen with white text  Description automatically generated" src="img/B21714_07_04.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 7.4: Sample users dataset</p>
<ol class="calibre15">
<li class="bulletlist1" value="2">What we want to do now is apply<a id="_idIndexMarker558" class="calibre3"/> the same logic of the prompt of the cold start with the formatting with variables. The difference here is that, rather than asking the user to provide the values for those variables, we will directly collect them from our user dataset. So, we first define our prompt chunks:
        <pre class="programlisting2"><code class="hljs-code">template_prefix = """You are a movie recommender system that help users to find movies that match their preferences.
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}"""
user_info = """This is what we know about the user, and you can use this information to better tune your research:
Age: {age}
Gender: {gender}
Movies already seen alongside with rating: {movies}"""
template_suffix= """Question: {question}
Your response:"""
</code></pre>
</li>
<li class="bulletlist1">We then format the <code class="inlinecode">user_info</code> chunk<a id="_idIndexMarker559" class="calibre3"/> as follows (assuming that the user interacting with the system is <code class="inlinecode">Alice</code>):
        <pre class="programlisting2"><code class="hljs-code">age = df.loc[df['username']=='Alice']['age'][0]
gender = df.loc[df['username']=='Alice']['gender'][0]
movies = ''
# Iterate over the dictionary and output movie name and rating
for movie, rating in df['movies'][0].items():
    output_string = f"Movie: {movie}, Rating: {rating}" + "\n"
    movies+=output_string
    #print(output_string)
user_info = user_info.format(age = age, gender = gender, movies = movies)
COMBINED_PROMPT = template_prefix +'\n'+ user_info +'\n'+ template_suffix
print(COMBINED_PROMPT)
</code></pre>
</li>
</ol>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting3"><code class="hljs-con">You are a movie recommender system that help users to find movies that match their preferences.
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}
This is what we know about the user, and you can use this information to better tune your research:
Age: 25
Gender: F
Movies already seen alongside with rating: Movie: Transformers: The Last Knight, Rating: 7
Movie: Pokémon: Spell of the Unknown, Rating: 5
Question: {question}
Your response:
</code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="4">Let’s now use this prompt<a id="_idIndexMarker560" class="calibre3"/> within our chain:
        <pre class="programlisting2"><code class="hljs-code">PROMPT = PromptTemplate(
    template=COMBINED_PROMPT, input_variables=["context", "question"])
chain_type_kwargs = {"prompt": PROMPT}
qa = RetrievalQA.from_chain_type(llm=OpenAI(),
    chain_type="stuff",
    retriever=docsearch.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs)
query = "Can you suggest me some action movie based on my background?"
result = qa({'query':query})
result['result']
</code></pre>
</li>
</ol>
<p class="normal-one">We then obtain the following output:</p>
<pre class="programlisting3"><code class="hljs-con">" Based on your age, gender, and the movies you've already seen, I would suggest the following action movies: The Raid 2 (Action, Crime, Thriller; Rating: 6.71), Ong Bak 2 (Adventure, Action, Thriller; Rating: 5.24), Hitman: Agent 47 (Action, Crime, Thriller; Rating: 5.37), and Kingsman: The Secret Service (Crime, Comedy, Action, Adventure; Rating: 7.43)."
'
</code></pre>
<p class="normal-one">As you can see, the model is now able to recommend a list of movies to Alice based on the user’s information about past preferences, retrieved as context within the model’s metaprompt.</p>
<p class="normal1">Note that, in this scenario, we used as dataset a simple pandas dataframe. In production scenarios, a best practice for storing variables related to a task to be addressed (such as a recommendation task) is that of using a feature store. Feature stores are data systems that are designed to support machine learning workflows. They allow data teams to store, manage, and access features<a id="_idIndexMarker561" class="calibre3"/> that are used for training and deploying machine learning models.</p>
<p class="normal1">Furthermore, LangChain offers native integrations towards some of the most popular features stores:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Feast:</strong> This is an open-source feature<a id="_idIndexMarker562" class="calibre3"/> store for machine learning. It allows teams to define, manage, discover, and serve features. Feast supports batch and streaming data sources and integrates with various data processing and storage systems. Feast uses BigQuery for offline features and BigTable or Redis for online features.</li>
<li class="bulletlist1"><strong class="screentext">Tecton: </strong>This is a managed feature<a id="_idIndexMarker563" class="calibre3"/> platform that provides a complete solution for building, deploying, and using features for machine learning. Tecton allows users to define features in code, version control them, and deploy them to production with best practices. Furthermore, it integrates with existing data infrastructure and ML platforms like SageMaker and Kubeflow, and it uses Spark for feature transformations and DynamoDB for online feature serving.</li>
<li class="bulletlist1"><strong class="screentext">Featureform: </strong>This is a virtual feature<a id="_idIndexMarker564" class="calibre3"/> store that transforms existing data infrastructure into a feature store. Featureform allows users to create, store, and access features using standard feature definitions and a Python SDK. It orchestrates and manages the data pipelines required for feature engineering and materialization, and it is compatible with a wide range of data systems, such as Snowflake, Redis, Spark, and Cassandra.</li>
<li class="bulletlist1"><strong class="screentext">AzureML Managed Feature Store: </strong>This is a new type of workspace<a id="_idIndexMarker565" class="calibre3"/> that lets users discover, create, and operationalize features. This service integrates with existing data stores, feature pipelines, and ML platforms like Azure Databricks and Kubeflow. Plus, it uses SQL, PySpark, SnowPark, or Python for feature transformations and Parquet/S3 or Cosmos DB for feature storage.</li>
</ul>
<p class="normal1">You can read more<a id="_idIndexMarker566" class="calibre3"/> about LangChain’s integration with features at <a href="https://blog.langchain.dev/feature-stores-and-llms/" class="calibre3">https://blog.langchain.dev/feature-stores-and-llms/</a>.</p>
<h1 class="heading" id="_idParaDest-108">Developing the front-end with Streamlit</h1>
<p class="normal">Now that we have seen the logic<a id="_idIndexMarker567" class="calibre3"/> behind an LLM-powered recommendation<a id="_idIndexMarker568" class="calibre3"/> system, it is time to give a GUI to our MovieHarbor. To do so, we will once again leverage Streamlit, and we will assume the cold-start scenario. As always, you can find the whole Python code in the GitHub book repository at <a href="Chapter_07.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>.</p>
<p class="normal1">As per the Globebotter application in <em class="italic">Chapter 6</em>, in this case also you need to create a <code class="inlinecode">.py</code> file to run in your terminal via <code class="inlinecode">streamlit run file.py</code>. In our case, the file will be named <code class="inlinecode">movieharbor.py</code>.</p>
<p class="normal1">Let’s now summarize the key steps to build the app with the front-end:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">Configure the application webpage:
        <pre class="programlisting2"><code class="hljs-code">import streamlit as st
st.set_page_config(page_title="GlobeBotter", page_icon="<img alt="" role="presentation" src="img/Icon.png" class="calibre4"/>")
st.header('<img alt="" role="presentation" src="img/Icon.png" class="calibre4"/> Welcome to MovieHarbor, your favourite movie recommender')
</code></pre>
</li>
<li class="bulletlist1">Import the credentials and establish the connection to LanceDB:
        <pre class="programlisting2"><code class="hljs-code">load_dotenv()
#os.environ["HUGGINGFACEHUB_API_TOKEN"]
openai_api_key = os.environ['OPENAI_API_KEY']
embeddings = OpenAIEmbeddings()
uri = "data/sample-lancedb"
db = lancedb.connect(uri)
table = db.open_table('movies')
docsearch = LanceDB(connection = table, embedding = embeddings)
# Import the movie dataset
md = pd.read_pickle('movies.pkl')
</code></pre>
</li>
<li class="bulletlist1">Create some widgets for the user to define their features and movies preferences:
        <pre class="programlisting2"><code class="hljs-code"># Create a sidebar for user input
st.sidebar.title("Movie Recommendation System")
st.sidebar.markdown("Please enter your details and preferences below:")
# Ask the user for age, gender and favourite movie genre
age = st.sidebar.slider("What is your age?", 1, 100, 25)
gender = st.sidebar.radio("What is your gender?", ("Male", "Female", "Other"))
genre = st.sidebar.selectbox("What is your favourite movie genre?", md.explode('genres')["genres"].unique())
# Filter the movies based on the user input
df_filtered = md[md['genres'].apply(lambda x: genre in x)]
</code></pre>
</li>
<li class="bulletlist1">Define the<a id="_idIndexMarker569" class="calibre3"/> parametrized<a id="_idIndexMarker570" class="calibre3"/> prompt chunks:
        <pre class="programlisting2"><code class="hljs-code">template_prefix = """You are a movie recommender system that help users to find movies that match their preferences.
Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
{context}"""
user_info = """This is what we know about the user, and you can use this information to better tune your research:
Age: {age}
Gender: {gender}"""
template_suffix= """Question: {question}
Your response:"""
user_info = user_info.format(age = age, gender = gender)
COMBINED_PROMPT = template_prefix +'\n'+ user_info +'\n'+ template_suffix
print(COMBINED_PROMPT)
</code></pre>
</li>
<li class="bulletlist1">Set up the <code class="inlinecode">RetrievalQA</code> chain:
        <pre class="programlisting2"><code class="hljs-code">#setting up the chain
qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff",
    retriever=docsearch.as_retriever(search_kwargs={'data': df_filtered}), return_source_documents=True)
</code></pre>
</li>
<li class="bulletlist1">Insert the search bar for the user:
        <pre class="programlisting2"><code class="hljs-code">query = st.text_input('Enter your question:', placeholder = 'What action movies do you suggest?')
if query:
    result = qa({"query": query})
    st.write(result['result'])
</code></pre>
</li>
</ol>
<p class="normal1">And that’s it! You can run the final result in your terminal with <code class="inlinecode">streamlit run movieharbor.py</code>. It looks like the following:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_07_05.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 7.5: Sample front-end for Movieharbor with Streamlit</p>
<p class="normal1">So, you can see, in just few lines<a id="_idIndexMarker571" class="calibre3"/> of code we were able<a id="_idIndexMarker572" class="calibre3"/> to set up a webapp for our MovieHarbor. Starting from this template, you can customize your layout with Streamlit’s components, as well as tailor it to content-based scenarios. Plus, you can customize your prompts in such a way that the recommender acts as you prefer.</p>
<h1 class="heading" id="_idParaDest-109">Summary</h1>
<p class="normal">In this chapter, we explored how LLMs could change the way we approach a recommendation system task. We started from the analysis of the current strategies and algorithms for building recommendation applications, differentiating between various scenarios (collaborative filtering, content-based, cold start, etc.) as well as different techniques (KNN, matrix factorization, and NNs).</p>
<p class="normal1">We then moved to the new, emerging field of research into how to apply the power of LLMs to this field, and explored the various experiments that have been done in recent months.</p>
<p class="normal1">Leveraging this knowledge, we built a movie recommender application powered by LLMs, using LangChain as the AI orchestrator and Streamlit as the front-end, showing how LLMs can revolutionize this field thanks to their reasoning capabilities as well as their generalization. This was just one example of how LLMs not only can open new frontiers, but can also enhance existing fields of research.</p>
<p class="normal1">In the next chapter, we will see what these powerful models can do when working with structured data.</p>
<h1 class="heading" id="_idParaDest-110">References</h1>
<ul class="calibre16">
<li class="bulletlist"><strong class="screentext">Recommendation as Language Processing</strong> (<strong class="screentext">RLP</strong>): A Unified <strong class="screentext">Pretrain, Personalized Prompt &amp; Predict Paradigm</strong> (<strong class="screentext">P5</strong>). <a href="https://arxiv.org/abs/2203.13366" class="calibre3">https://arxiv.org/abs/2203.13366</a></li>
<li class="bulletlist1">LangChain’s blog about featurestores. <a href="https://blog.langchain.dev/feature-stores-and-llms/" class="calibre3">https://blog.langchain.dev/feature-stores-and-llms/</a></li>
<li class="bulletlist1">Feast. <a href="https://docs.feast.dev/" class="calibre3">https://docs.feast.dev/</a></li>
<li class="bulletlist1">Tecton. <a href="https://www.tecton.ai/" class="calibre3">https://www.tecton.ai/</a></li>
<li class="bulletlist1">FeatureForm. <a href="https://www.featureform.com/" class="calibre3">https://www.featureform.com/</a></li>
<li class="bulletlist1">Azure Machine Learning feature store. <a href="https://learn.microsoft.com/en-us/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2" class="calibre3">https://learn.microsoft.com/en-us/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2</a></li>
</ul>
<h1 class="heading">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3">https://packt.link/llm</a></p>
<p class="normal1"><img alt="" role="presentation" src="img/QR_Code214329708533108046.png" class="calibre4"/></p>
</div>
</body></html>