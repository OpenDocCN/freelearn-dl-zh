<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Agent Awareness</h1>
                </header>
            
            <article>
                
<p><em>You can run, but you can't hide!</em></p>
<p>Oh, you're back? That's great, because it means that your eyes are capturing some light information that your brain is perceiving in an act that it is usually called reading. Everything we do, every decision we make, is based on what we perceive, and biologically we make short decision-making processes because time is crucial (e.g. you see a snake and your amygdala processes that information much faster and quicker than your visual cortex!).</p>
<p>With the same very concept, AI needs to base their decisions from facts by gathering information they need to perceive first. This chapter is all about perception, and how AI can get this information from the environment, so that it can be aware of its surroundings. We looked at EQS in the previous chapter, which gathers a lot of information about the surrounding environment and processes. Here, we will just limit ourselves to the simple act of perception. What the AI will do with that information is a topic for other chapters (and some we have already covered).</p>
<p>Here is a quick rundown of the topics we will face in this chapter:</p>
<ul>
<li>Perception and awareness in existing video games</li>
<li>Overview of the Sensing System within Unreal</li>
<li>Perception Component</li>
<li>Senses for Sight and Hear</li>
<li><span>Perception Stimuli</span></li>
<li>Implementing sight in an agent (both in Blueprint and in C++)</li>
<li>Implementing hearing in an agent (both in Blueprint and C++)</li>
</ul>
<p>So, let's start by looking at some examples of AI awareness in video games, and then we will see how we can set up a Perceiving system in Unreal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Perception of AI in video games</h1>
                </header>
            
            <article>
                
<p>It's all a matter of perception, right? But when it comes to artificial intelligence—and AI in games, in particular—perception can make all the difference between winning and losing. In other words, <em>how</em> an AI character is able to perceive a player during gameplay can create a range of different experiences, thus creating environments full of tension and suspense while you turn every corner with slow, tentative footsteps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sound</h1>
                </header>
            
            <article>
                
<p>Have you ever tried to sneak past a guard in a game, trying not to make a noise or get detected? This is one of the most common ways that AI perceive a player and respond accordingly (often not in your favor!). However, one of the benefits of using sounds to influence an AI's perception of a player is that it gives the player the opportunity to initiate surprise attacks (e.g. Hitman, Assassin's Creed). For example, a player can sneak up on an enemy and stun or attack them from behind, therefore providing the player with an advantage. This can be particularly helpful when enemies are challenging to defeat, or a player is low in resources (e.g. ammunition, health packs/potions, etc).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Footsteps</h1>
                </header>
            
            <article>
                
<p>Just like the preceding example suggests, one of the most common ways that AI can perceive characters via sound is through footsteps. No surprises here about how, but the proximity of detection here can depend on many factors. For example, some characters can walk while crouching to avoid detection, or simply by sneaking (e.g. <em>Abe's Oddyssey</em>); other games allow some characters to be undetectable while moving around, unless visually spotted by an enemy (e.g. Natalia in <em>Resident Evil: Revelations 2</em>). Another key ingredient in using footsteps as a trigger for an AI's perception is the type of ground material that a player is walking on. For example, a player walking through a forest, crunching on leaves and bark, is going to be a lot more obvious (and loud) than a player who is walking on sand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Knocking over objects</h1>
                </header>
            
            <article>
                
<p>While sneaking through a level or walking while crouching, or even in the prone position (e.g. <em>Battlefield</em>), will not trigger an enemy, chances are if you knock something over (e.g. a bottle, box, random item) it's going to alert them. In this case, environmental objects play an important role in an AI's ability to perceive a player's location by simply the player themselves fumbling around an environment. In some cases, certain objects are likely to attract more attention than others, depending on how much noise they make. Of course, as game designers, you have the power to determine this!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Position</h1>
                </header>
            
            <article>
                
<p>Similar to sound, AI have the ability to see you based on your proximity to them. This one is a little more obvious and a bit harder to avoid when you are in plain sight of an enemy. Imagine that you're sneaking past an enemy and as soon as you come close enough to them, that's it, it's all over, you've been spotted! This is the unfortunate peril that many players face, but one that has many pay-offs, especially in terms of satisfaction to having outsmarted the enemy.</p>
<p>Let's examine this concept a little further with some examples. To begin, we have games like <em>Assassin's Creed</em>, <em>Hitman: Absolution</em>, and <em>Thief</em>, where the art of eluding your enemy through manoeuvring is paramount to the player's success in completing a mission. Often, this requires that the play leverages the environmental surroundings such as NPCs, walls, haystacks, plants (trees, bushes), rooftops, and utilizing the element of surprise.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Zone of proximity</h1>
                </header>
            
            <article>
                
<p>In other cases, there is an explicit zone of proximity that a player can remain out of before they are detected. Often, in games, this is articulated by a light source such as a flashlight, forcing the player to dance between shadow and light to avoid detection. An excellent example of games that have adopted this approach are <em>Monaco: What's Yours Is Mine</em> and <em>Metal Gear Solid</em>, where certain AI characters have a proximity of visibility via the use of torches or by simply facing you for an extended period of time.</p>
<p>You can see an example of this in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3352ac11-2ea1-4ee9-9d47-71ffbc4ba88d.png" style="width:48.08em;height:22.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Screenshot from the game <em>Monaco: What's Yours Is Mine</em></div>
<p>Here (in <em>Monaco: What's Yours Is Mine</em>), you can see the radius of the flashlights, and as soon as a player enters it, they have a limited amount of time before they grab the attention of the guards.</p>
<p>Since <em>Monaco: What's Yours Is Mine</em><span> is entirely based on this mechanic, let's look at some more screenshots to get a better feeling of how sight perception works in this game.</span></p>
<p>In the following screenshot, we can see how the perception changes when the player changes room:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/42ef940a-7398-4ab6-981a-06388b9875ab.png" style="width:56.33em;height:48.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot from the game </span><em>Monaco: What's Yours Is Mine</em></div>
<p>In the following screenshot, we have a close-up of the player's perception:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/58bf00d1-89d8-48fa-befb-e153919fb408.png" style="width:63.00em;height:34.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot from the game </span><em>Monaco: What's Yours Is Mine</em></div>
<p>Then, we have a close-up of a guard's flashlight:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a6d333a7-f573-4b7d-965a-8f3b6ffa6ad2.png" style="width:51.08em;height:44.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Screenshot from the game </span><em>Monaco: What's Yours Is Mine</em></div>
<p>Changing game, In <em>Metal Gear Solid</em>, perception is used in a similar way with enemies (red dots) patrolling the environment around the player (white dot). In the following screenshot, you can see a camera (represented as a red dot in the <em>minimap</em>) with a yellow cone of view in the <em>minimap</em> (guards have a blue cone, instead):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/89aa9ea2-407f-46cb-8e95-6444a386fbb2.png" style="width:39.17em;height:22.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Screenshot from the game <em>Metal Gear Solid</em></div>
<p>The Metal Gear Solid game series is entirely based on perception, and it is worthwhile exploring more and learning about the game if you are interested in developing game AIs with this mechanic.</p>
<p>Wrapping up, if you get too close to NPCs (e.g. within their range of visibility) you will be noticed, and they will try to interact with your character, whether it be good (beggars in <em>Assassin's Creed</em>) or bad (enemy attacks you), which unlocks many interesting mechanics based on perception.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interacting with other enemies</h1>
                </header>
            
            <article>
                
<p>An AI's perception about your position isn't necessarily related to when you enter their zone of visibility. In other cases (e.g. first-person shooters), this may happen when you start to shoot an enemy. This creates a ripple effect in that many AIs within your initial proximity will then target you (e.g. Metal Gear Solid, Army of Two, Battlefield, etc.).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">It isn't all about the "enemy"</h1>
                </header>
            
            <article>
                
<p>In many sporting games, AI has to be perceptive in order to respond accordingly, e.g. from preventing a goal, hitting a ball, or shooting hoops. AI within sports must be perceptive (and competitive) when it comes to playing <em>against</em> you. They need to know your location and the location of the ball (or any other object) so that they can respond (e.g. kicking the ball away from the goal posts).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Perceptive AI isn't just humanoid or animalistic</h1>
                </header>
            
            <article>
                
<p>Perceptive AI can also include machines, such as cars and other vehicles. Take the games <em>Grand Theft Auto</em>, <em>Driver</em>, and <em>The Getaway</em>, into account, these games require that a player navigates around a 3D world space at some point inside of a car. In some instances, there are NPCs inside, but for the most part, the cars themselves respond to your driving. This is also the case in more sport oriented games such as <em>Grand Turismo</em>, <em>Need for Speed</em>, and <em>Ridge Racer</em> (to name a few).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Impact of Players</h1>
                </header>
            
            <article>
                
<p>As we have seen, there are many ways in which AI can detect players. But one thing that a game designer must consider among all of this is how this will influence the game's experience; how will it drive gameplay? While the use of perceptive AI is quite a nice addition to any game, it also impacts how a game is played. For example, if you want to have gameplay that is heavily focused on skill, player dexterity, and more environmentally aware, then the perception of AI needs to be quite sensitive, with the player being a lot more vulnerable (e.g. Thief). But if, on the other hand, you want a fast-paced action game, you will need to have perceptive AI with the balance of allowing the player to respond accordingly. For example, they have a level playing field to fight against the AI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of the Sensing System</h1>
                </header>
            
            <article>
                
<p>Coming back to Unreal, as you would expect, there is a subsystem of the AI Framework that implements AI Perception. Once again, you are free to implement your own system, especially if you have particular needs…</p>
<p>With <strong><em>Sensing and Perception</em></strong>, we are collocating at a lower level than <strong><em>Decision-Making</em></strong> (like <em>Behavior Trees</em> and <em>EQS</em>). In fact, there is no decision to take, no place to select, but just a passage/flow of information.</p>
<p class="mce-root"/>
<p>If the Sensing System perceives something "interesting" (we will define what this means later), then it notifies the AI controller, which will decide what to do about the received stimuli (which is its perception, in Unreal terminology).</p>
<p>Therefore, in this chapter, we will focus on how to properly set up the Sensing System so that our AI can perceive, but we won't deal with what to do once we have received the stimuli (e.g. the player is in sight, so start chasing them). After all, if you already have the behavior ready (e.g. a <em>Behavior Tree</em> that chases the player; we will build such a tree later in this book), the logic behind the sensing is simple as "if the player is in sight (the AI controller received a stimuli from the Sensing system), then execute the Chasing Behavior tree".</p>
<p>In practical terms, the built-in sensing system of Unreal is based mainly on the use of two components: <strong>AIPerceptionComponent</strong> and <strong>AIPerceptionStimuliSourceComponent</strong>. The first is able to perceive stimuli, whereas the latter is able to produce one (but it is not the only way we can produce stimuli, as we will soon see).</p>
<div class="packt_infobox">As odd as it might seem, the system believes that the AIPerceptionComponent is attached to the AI Controller (and not the Pawn/Character that they control). In fact, it's the AI Controller that will make a decision based on the stimuli received, not the mere Pawn. As a result, the AIPerceptionComponent needs to be attached directly to the AI Controller.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The AIPerceptionComponent</h1>
                </header>
            
            <article>
                
<p>Let's break down how the <strong>AIPerceptionComponent</strong> works. We are going to do this both in Blueprint and C++.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AIPerceptionComponent in Blueprint</h1>
                </header>
            
            <article>
                
<p>If we open our Blueprint AI Controller, we are able to add the <strong>AIPerceptionComponent</strong> like we would any other component: from the Components tab, click on <strong>Add Component</strong> and select the <strong>AIPerceptionComponent</strong>, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2367df79-29df-45cf-a653-8361ab6d60d3.png" style="width:20.75em;height:11.00em;"/></p>
<p>When you select the component, you will see how it appears in the <em>Details</em> panel, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/11de9f01-0203-4253-be9c-f3aaee0d4d45.png" style="width:28.83em;height:39.67em;"/></p>
<p>It only has two parameters. One defines the dominant senses. In fact, <strong>AIPerceptionComponent</strong> can have more than one sense, and when it comes to retrieving the location of the target that's been sensed, which one should the AI use? The <strong>Dominant Sense</strong> removes ambiguity by giving one sense priority over the others. The other parameter is an Array of senses. As you fill the Array with the different senses, you will be able to customize each one of them, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/71c2ecc9-71b8-4232-b45b-2f316f5bdb20.png"/></p>
<div class="packt_infobox"> Keep in mind that you can have more than one sense of each kind. Suppose that your enemy has two heads, facing a different direction: you might want to have two sight senses, one for each head. Of course, in this case, it requires a bit more setup to make them work correctly since you need to modify how the sight component works since, let's say, the AI always watches from its forward vector.</div>
<p>Each sense has its own properties and parameters. Let's go through the two main ones: sight and hearing.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sense – Sight</h1>
                </header>
            
            <article>
                
<p>The Sight sense works as you would expect, and it comes pretty ready out of the box (this might not be true for other senses, but sight and hearing are the most common). This is what it looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0c3fee81-8fd0-45b1-8358-ac6b1cf19ea2.png"/></p>
<p>Let's break down the main parameters that control sense of Sight:</p>
<ul>
<li><strong>Sight Radius</strong>: If a target (an object that can be seen) enters within this range, and it is not occluded, then the target is detected. In this sense, it is the "<em>Maximus sight distance to notice the target</em>".</li>
<li><strong>Lose Sight Radius</strong>: If the target has already been seen, then the target will be still seen within this range, if not occluded. This value is greater than <em>Sight Radius</em>, meaning that the AI is able to perceive the target at a greater distance if it is already seen. In this sense, it is the "<em>Maximus sight distance to notice a target that has already been seen</em>".</li>
<li><strong>PeripheralVisionHalfAngleDegrees</strong>: As the name suggests, it specifies how far (in degrees) the AI can look. A value of 90 means (since this value is just half of the angle) that the AI is able to see everything that is in front of it up to 180 degrees. A value of 180 would mean that the AI can look in any direction; it has 360-degree vision. Also, it is important to note that this half angle is measured from the forward vector. The following diagram illustrates this:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eedbf74b-20da-4d5c-9437-70784e032a1f.png" style="width:32.17em;height:21.58em;"/></p>
<ul>
<li><strong>Auto Success Range From Last Seen</strong>: By default, this is set to an invalid value (-1.0f), meaning that it isn't used. This specifies a range from the last seen location of a target, and if it is within this range, then the target is always visible.</li>
</ul>
<p>There are other settings that are more general, and can be applied to many senses (including hearing, so they will not be repeated in the next section):</p>
<ul>
<li><strong>Detection By Affiliation</strong>: <em>See the Different Teams</em> section.</li>
<li><strong>Debug Color</strong>: As the name suggests, it is the color with which this sense should be displayed in the visual debugger (see <a href="de51b2fe-fb19-4347-8de9-a31b2c2a6f2f.xhtml">Chapter 11</a>, <em>Debugging methods for AI <span>–</span> Logging</em>, for more information).</li>
<li><strong>Max Age</strong>: It indicates the time (expressed in seconds) that a stimulus is recorded for. Imagine that a target exits from the vision of the AI; its last location is still recorded, and it is assigned an age (how old is that data). If the Age gets bigger than the Max Age, then the stimuli are erased. For instance, an AI is chasing the player, who escapes from his/her sight. Now, the AI should first check the last position where the player has been seen to try to bring him/her back into their sight. If it fails, or the position was recorded many minutes ago, that data is not relevant anymore, and it can be erased. In summary, this specifies the age limit after the stimuli that's generated by this sense is forgotten. Moreover, a value of 0 means never.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sense – Hearing</h1>
                </header>
            
            <article>
                
<p>The Hearing sense has just one proper parameter, which is the Hearing Range. This sets the distance at which the AI is able to hear. The others are the general ones we already have seen for Sight (e.g. <em>Max Age</em>, <em>Debug Color</em>, and <em>Detection By Affiliation</em>). This is what it looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1c6deb86-c037-44d0-9ca6-308076ac2239.png"/></p>
<div class="packt_infobox">To make this book complete, it's worth mentioning that there is another option, called <em>LoSHearing</em>. To the best of my knowledge, and by looking at the Unreal Source code (version 4.20), this parameter doesn't seem to affect anything (except debugging). As a result, we leave it as not enabled.</div>
<p>In any case, there are other options to control how sound is produced. Actually, the Hearing events need to be manually triggered with a special function/Blueprint node.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AIPerceptionComponent and Senses in C++</h1>
                </header>
            
            <article>
                
<p>If you skipped the previous sections, please read them first. In fact, all the concepts are the same, and in this section, I'm just going to show the use of the component in C++ without re-explaining all the concepts.</p>
<p>Here are the <kbd>#include</kbd> statements for the classes we would like to use (I only included Sight and Hearing):</p>
<pre>#include "Perception/AIPerceptionComponent.h"<br/>#include "Perception/AISense_Sight.h"<br/>#include "Perception/AISenseConfig_Sight.h"<br/>#include "Perception/AISense_Hearing.h"<br/>#include "Perception/AISenseConfig_Hearing.h"</pre>
<p>To add the component to a C++ AI Controller, you do so like any other component, with a variable in the <kbd>.h</kbd> file. So, to keep track of it, you can use the <kbd>inerith</kbd> variable from the base class, which is declared as follows:</p>
<pre>  UPROPERTY(VisibleDefaultsOnly, Category = AI)<br/>  UAIPerceptionComponent* PerceptionComponent;</pre>
<div class="packt_infobox">As a result, you are able to use this variable in any <em>AIController</em>, without declaring it in the header (<kbd>.h</kbd>) file.</div>
<p> </p>
<p>Then, with the <kbd>CreateDefaultSubobject()</kbd> function in the constructor in the <kbd>.cpp</kbd> file, we can create the component:</p>
<pre>PerceptionComponent = CreateDefaultSubobject&lt;UAIPerceptionComponent&gt;(TEXT("SightPerceptionComponent"));</pre>
<p>Moreover, you will need extra variables, one for each of the senses you want to configure. For example, for the Sight and Hearing senses, you will need the following variables:</p>
<pre>UAISenseConfig_Sight* SightConfig;<br/>UAISenseConfig_Hearing* HearingConfig;</pre>
<p>To configure a Sense, you need to create it first, and you have access to all its properties and can set what you need:</p>
<pre>//Create the Senses<br/>SightConfig = CreateDefaultSubobject&lt;UAISenseConfig_Sight&gt;(FName("Sight Config"));<br/>HearingConfig = CreateDefaultSubobject&lt;UAISenseConfig_Hearing&gt;(FName("Hearing Config"));<br/><br/>//Configuring the Sight Sense<br/>SightConfig-&gt;SightRadius = 600;<br/>SightConfig-&gt;LoseSightRadius = 700;<br/><br/>//Configuration of the Hearing Sense<br/>HearingConfig-&gt;HearingRange = 900;</pre>
<p>Finally, you need to bind the senses to the <strong>AIPerceptionComponent</strong>:</p>
<pre>//Assigning the Sight and Hearing Sense to the AI Perception Component<br/>PerceptionComponent-&gt;ConfigureSense(*SightConfig);<br/>PerceptionComponent-&gt;ConfigureSense(*HearingConfig);<br/>PerceptionComponent-&gt;SetDominantSense(SightConfig-&gt;GetSenseImplementation());</pre>
<p>In case you need to call back for the events, you can do so by bypassing the callback function (it has to have the same signature, not necessarily the same name):</p>
<pre>//Binding the OnTargetPerceptionUpdate function<br/>PerceptionComponent-&gt;OnTargetPerceptionUpdated.AddDynamic(this, &amp;ASightAIController::OnTargetPerceptionUpdate);</pre>
<p>This concludes the use of the <strong>AIPerceptionComponent</strong> and <strong>Senses</strong> in C++.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Different Teams</h1>
                </header>
            
            <article>
                
<p>As the AI perception system that's built into Unreal goes, AIs and anything that can be detected can have a team. Some teams are against each other, whereas some are just neutral. As a result, when an AI comes to perceive something, that something can be Friendly (it is in the same team), Neutral, or an Enemy. For instance, if an AI is patrolling a camp, we can ignore Friendly and Neutral entities, and focus only on Enemies. By the way, the default settings are to perceive only enemies.</p>
<p>The way in which you can change which kind of entities an AI can perceive is through the <em>Detecting for Affiliation</em> settings of <em>Sense</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5c50f213-fa1a-4343-80e6-aabd93154d5c.png" style="width:30.17em;height:21.00em;"/></p>
<p>This provides three checkboxes where we can choose what we would like that AI to perceive.</p>
<p>There are 255 teams in total, and by default, every entity is within team 255 (the only special team). Whoever is in team 255 is perceived as Neutral (even if both the entities are in the same team). Otherwise, if two entities are in the same team (different than 255), they "see" each other as Friendly. On the other hand, two entities in two different teams (different than 255) the "see" each other as Enemies.</p>
<p>Now the question is, how can we change teams? Well, at the moment, this is only possible in C++. Moreover, we have talked about entities, but who can actually be in a team? Everything that implements the <strong>IGenericTeamAgentInterface</strong> can be part of a team. <em>AIControllers</em> already implements it. As a result, changing teams on an AI Controller is easy, as shown in the following snippet:</p>
<pre>    // Assign to Team 1<br/>    SetGenericTeamId(FGenericTeamId(1));</pre>
<p>For other entities, once they implement the <strong>IGenericTeamAgentInterface</strong>, they can override the <kbd>GetGenericTeamId()</kbd> function, which provides a way for the AI to check in which team that entity is.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The AIStimuliSourceComponent</h1>
                </header>
            
            <article>
                
<p>We have seen how an AI can perceive through a Sense, but how are the stimuli generated in the first place?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">All Pawns are automatically detected</h1>
                </header>
            
            <article>
                
<p>In the case of the Sight sense, by default, all the Pawns are already a stimuli source. In fact, later in this chapter, we will use the Player character, who will be detected by the AI without having an <strong>AIStimuliSourceComponent</strong>. In case you are interested in disabling this default behavior, you can do so by going into your project directory, and then going inside the <strong>Config</strong> folder. There, you will find a file named <strong>DefaultGame.ini</strong>, in which you can set a series of configuration variables. If you add the following two lines at the end of the file, Pawns will not produce Sight stimuli by default, and they will need the <strong>AIStimuliSourceComponent</strong> as well as everything else:</p>
<pre>[/Script/AIModule.AISense_Sight]<br/>bAutoRegisterAllPawnsAsSources=false</pre>
<p class="mce-root"/>
<p>In our project, we are not going to add these lines, since we want the Pawns to be detected without using having to add more components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AIStimuliSourceComponent in Blueprint</h1>
                </header>
            
            <article>
                
<p>Like any other component, it can be added to a Blueprint:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/57288860-895c-44b1-8f27-48c4ea2fea33.png" style="width:30.33em;height:39.92em;"/></p>
<p>If you select it, you will see in the <em>Details</em> panel that it has just two parameters:</p>
<ul>
<li><strong>Auto Register as a Source</strong>: As the name suggests, if checked, the source automatically registers inside the Perception system, and it will start proving stimuli from the start</li>
<li><strong>Register as Source for Senses</strong>: This is an array of all the senses that this component provides stimuli for</li>
</ul>
<p>There is not much more to say about this component. It is very simple to use, but important (your AI might not perceive any stimuli!). Thus, remember to add it to the non-Pawn entities when you want them to generate a stimulus (which can be as simple as being seen from the AI).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AIStimuliSourceComponent in C++</h1>
                </header>
            
            <article>
                
<p>Using this component in C++ is easy since you just create it, configure it, and it is ready to go.</p>
<p>This is the <kbd>#include</kbd> statement you need to use so that you have access to the class of the component:</p>
<pre>#include "Perception/AIPerceptionStimuliSourceComponent.h"</pre>
<p>Like any other component, you need to add a variable in the <kbd>.h</kbd> file to keep track of it:</p>
<pre>UAIPerceptionStimuliSourceComponent* PerceptionStimuliSourceComponent;</pre>
<p>Next, you need to generate it in the constructor with the <span><kbd>CreateDefaultSubobject()</kbd> function</span>:</p>
<pre>PerceptionStimuliSourceComponent = CreateDefaultSubobject&lt;UAIPerceptionStimuliSourceComponent&gt;(TEXT("PerceptionStimuliComponent"));</pre>
<p>Then, you need to register a source Sense, as follows (in this case, <em>Sight</em>, but you can change <kbd>TSubClassOf&lt;UAISense&gt;()</kbd> to the Sense you need):</p>
<pre>PerceptionStimuliSourceComponent-&gt;RegisterForSense(TSubclassOf&lt;UAISense_Sight&gt;());</pre>
<div class="packt_infobox">The <strong>Auto Register as a Source</strong> bool is protected and true by default.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hands-on with the perception system – Sight AI Controller</h1>
                </header>
            
            <article>
                
<p>The best way we to learn about something is by using it. So, let's start by creating a simple perception system in which we print on the screen when something enters or leave the perception field of the AIs, along with the number of currently seen objects (including/excluding the one that just entered/exited).</p>
<p>Once again, we will do this twice, once with Blueprint and another time with C++, so that we can get to know about both methods of creation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A Blueprint perception system</h1>
                </header>
            
            <article>
                
<p>First of all, we need to create a new AI controller (unless you want to use the one we've already been using). In this example, I'm going to call it "SightAIController". Open up the Blueprint editor, add the AIPerception component, and feel free to rename it (if you like) to something like "SightPerceptionComponent".</p>
<p>Select this component. In the <em>Details</em> panel, we need to add this as a sense to <em>Sight</em>, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9e782229-d2b9-4538-a23d-b97a187574e4.png" style="width:20.42em;height:10.83em;"/></p>
<p>We can set the <strong>Sight Radius</strong> and the <strong>Lose Sight Radius</strong> to something reasonable, such as <em>600</em> and <em>700</em>, respectively, so that we have something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5d351646-69df-40d5-8ca4-8a1b2d15f792.png"/></p>
<p>We can leave the angle untouched, but we need to change the <strong>Detection By Affiliation</strong>. In fact, it isn't possible to change the Team from Blueprint, so the player will be in the same 255th team, which is neutral. Since we are just getting our hands dirty on how the system works, we can check all three checkboxes. Now, we should have something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bac0a4d6-a13b-40a3-a34c-b278ada0d7f6.png" style="width:31.00em;height:32.08em;"/></p>
<p>At the bottom of the component, we should have all the different events. In particular, we will need <strong>On Target Perception</strong> <strong>Updated</strong>, which is called every time a target enters or exits the perception field—exactly what we need:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/48f8450c-3c7e-4028-87d7-4f57ba8edb47.png" style="width:29.67em;height:11.58em;"/></p>
<p>Click on the "<strong>+</strong>" sign to add the event in the graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2b352ecc-ed31-4a25-99dc-c36c5365aa0e.png" style="width:31.00em;height:10.25em;"/></p>
<p>This event will provide us with the Actor that caused the update and created the stimuli (it's worth remembering that a Perception component might have more than one perception at the time, and this variable tells you which stimuli caused the update). In our case, we have only Sight, so it can't be anything else. The next step is to understand how many targets we have insight and which one left or entered the field of view.</p>
<p>So, drag the <strong>SightPerceptionComponent</strong> into the graph. From there, we can drag a pin to get all the "<strong>Currently Perceived Actors</strong>", which will give us back an array of Actors. Don't forget to set the <em>Sense Class</em> to <em>Sight</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7b2cf273-5140-456a-9672-7f11dcc3925d.png" style="width:37.25em;height:22.00em;"/></p>
<p>By measuring the length of this array, we can get the number of currently perceived actors at the moment. Moreover, by checking whether the Actor that was passed from the event is in the currently "<em>seen Actors</em>" array, we can determine whether such an actor has left or entered the field of view:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/42ca93c2-6540-44c1-a5e9-9d44809b2b64.png"/></p>
<p>The last step is to format all of this information in a nice formatted string so that it can be shown on-screen. We will use the Append node to build the string, along with a select for the "<em>entered</em>" or "<em>left</em>" Actor. Finally, we will plug the end result into a <em>Print String</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/545cd541-2591-466c-9780-c691b041834e.png"/></p>
<div class="packt_infobox">The <em>Print String</em> is just for debugging purposes and it is not available when shipping games, but we are just testing and understanding how the perception system works.<br/>
Also, I know that when the number of perceived actors is one, the string will produce "<em>1 objects</em>", which is incorrect, but correcting plurals (although possible, both with an if statement or in a more complex fashion to take care of language(s) structure(s)) is outside the scope of this book. This is why I am using this expression.</div>
<p>Save the AI controller and go back to the level. If you don't want to do the same in C++, skip the next section, and go directly to "<em>Test it all"</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A C++ perception system</h1>
                </header>
            
            <article>
                
<p>Again, if you are more on the C++ side, or want to experiment with how we can build the same AI Controller in C++, this is the section for you. We will follow the exact same steps (more or less), and instead of images, we will have code!</p>
<p>Let's start by creating a new AIController class (if you don't remember how to, have a look at <a href="42076317-364e-4824-a8b7-cc2c418f021e.xhtml">Chapter 2</a>, <em>Moving the first steps in the AI world</em>). We will name it SightAIController and place it within the <kbd>AIControllers</kbd> folder.</p>
<p>Let's start editing the <kbd>SightAIController.h</kbd> file, in which we need to include some other <kbd>.h</kbd> files so that our compiler knows where the implementations of the class we need are. In fact, we will need access to the <strong>AIPerception</strong> and <strong>AISense_Config</strong> classes. So, at the top of your code file, you should have the following <kbd>#include</kbd> statements:</p>
<pre>#pragma once<br/>#include "CoreMinimal.h"<br/>#include "AIController.h"<br/>#include "Perception/AIPerceptionComponent.h"<br/>#include "Perception/AISense_Sight.h"<br/>#include "Perception/AISenseConfig_Sight.h"<br/>#include "SightAIController.generated.h"</pre>
<p>Then, in our class, we need to keep a reference to the <kbd>AIPerception</kbd> Component and an extra variable that will hold the configuration for the Sight sense:</p>
<pre> //Components Variables<br/> UAIPerceptionComponent* PerceptionComponent;<br/> UAISenseConfig_Sight* SightConfig;</pre>
<p>Moreover, we need to add the <kbd>Constructor</kbd> function, as well as a callback for the <kbd>OnTargetPerceptionUpdate</kbd> event. In order to work, this last one has to be a <kbd>UFUNCTION()</kbd>, and needs to have an <strong>Actor</strong> and a <strong>AIStimulus</strong> as inputs. In this way, the reflection system will work as excepted:</p>
<pre>//Constructor<br/> ASightAIController();<br/><br/>//Binding function<br/> UFUNCTION()<br/> void OnTargetPerceptionUpdate(AActor* Actor, FAIStimulus Stimulus);</pre>
<p>Let's move into our <kbd>.cpp</kbd> file. First, we need to create the <kbd>AIPerception</kbd> Component, as well as a Sight configuration:</p>
<pre>ASightAIController::ASightAIController() {<br/><strong>  //Creating the AI Perception Component</strong><br/><strong>  PerceptionComponent = CreateDefaultSubobject&lt;UAIPerceptionComponent&gt;(TEXT("SightPerceptionComponent"));</strong><br/><strong>  SightConfig = CreateDefaultSubobject&lt;UAISenseConfig_Sight&gt;(FName("Sight Config"));</strong><br/><br/>}</pre>
<p>Then, we can configure the <em>Sight Sense</em> with the same parameters: <strong>Sight Radius</strong> to <em>600</em> and <strong>Lose Sight Radius</strong> to <em>700</em>:</p>
<pre>ASightAIController::ASightAIController() {<br/>  //Creating the AI Perception Component<br/>  PerceptionComponent = CreateDefaultSubobject&lt;UAIPerceptionComponent&gt;(TEXT("SightPerceptionComponent"));<br/>  SightConfig = CreateDefaultSubobject&lt;UAISenseConfig_Sight&gt;(FName("Sight Config"));<br/><br/><strong>  //Configuring the Sight Sense</strong><br/><strong>  SightConfig-&gt;SightRadius = 600;</strong><br/><strong>  SightConfig-&gt;LoseSightRadius = 700;</strong><br/>}</pre>
<p>Next, we need to check all the flags for the <strong>DetectionByAffiliation</strong> so that we detect our Player (since, at the moment, they both are in the 255th team; look at the <em>Exercise</em> section to learn how to improve this):</p>
<pre>ASightAIController::ASightAIController() {<br/>  //Creating the AI Perception Component<br/>  PerceptionComponent = CreateDefaultSubobject&lt;UAIPerceptionComponent&gt;(TEXT("SightPerceptionComponent"));<br/>  SightConfig = CreateDefaultSubobject&lt;UAISenseConfig_Sight&gt;(FName("Sight Config"));<br/><br/>  //Configuring the Sight Sense<br/>  SightConfig-&gt;SightRadius = 600;<br/>  SightConfig-&gt;LoseSightRadius = 700;<br/><strong>  SightConfig-&gt;DetectionByAffiliation.bDetectEnemies = true;</strong><br/><strong>  SightConfig-&gt;DetectionByAffiliation.bDetectNeutrals = true;</strong><br/><strong>  SightConfig-&gt;DetectionByAffiliation.bDetectFriendlies = true;</strong><br/>}</pre>
<p>Finally, we associate the Sight configuration with the <kbd>AIPerception</kbd> Component, and bind the <kbd>OnTargetPerceptionUpdate</kbd> function to the homonym event on the <kbd>AIPerceptionComponent</kbd>:</p>
<pre>ASightAIController::ASightAIController() {<br/>  //Creating the AI Perception Component<br/>  PerceptionComponent = CreateDefaultSubobject&lt;UAIPerceptionComponent&gt;(TEXT("SightPerceptionComponent"));<br/>  SightConfig = CreateDefaultSubobject&lt;UAISenseConfig_Sight&gt;(FName("Sight Config"));<br/><br/>  //Configuring the Sight Sense<br/>  SightConfig-&gt;SightRadius = 600;<br/>  SightConfig-&gt;LoseSightRadius = 700;<br/>  SightConfig-&gt;DetectionByAffiliation.bDetectEnemies = true;<br/>  SightConfig-&gt;DetectionByAffiliation.bDetectNeutrals = true;<br/>  SightConfig-&gt;DetectionByAffiliation.bDetectFriendlies = true;<br/><br/><strong>  //Assigning the Sight Sense to the AI Perception Component</strong><br/><strong>  PerceptionComponent-&gt;ConfigureSense(*SightConfig);</strong><br/><strong>  PerceptionComponent-&gt;SetDominantSense(SightConfig-&gt;GetSenseImplementation());</strong><br/><br/><strong>  //Binding the OnTargetPerceptionUpdate function</strong><br/><strong>  PerceptionComponent-&gt;OnTargetPerceptionUpdated.AddDynamic(this, &amp;ASightAIController::OnTargetPerceptionUpdate);</strong><br/>}</pre>
<p>This concludes the <em>Constructor</em>, but we still need to implement the <kbd>OnTargetPerceptionUpdate()</kbd> function. First of all, we need to retrieve all the <strong>Currently Perceived Actors</strong>. This function requires an array of actors that it can fill, along with the implementation of the Sense to use.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As a result, we will have our array filled up with the Perceived Actors:</p>
<pre>void ASightAIController::OnTargetPerceptionUpdate(AActor* Actor, FAIStimulus Stimulus)<br/>{<br/><strong>  //Retrieving Perceived Actors</strong><br/><strong>  TArray&lt;AActor*&gt; PerceivedActors;</strong><br/><strong>  PerceptionComponent-&gt;GetPerceivedActors(TSubclassOf&lt;UAISense_Sight&gt;(), PerceivedActors);</strong><br/><br/>}</pre>
<p>By measuring the length of this array, we can get the number of currently perceived actors at the moment. Moreover, by checking if the Actor that was passed from the event (the parameter of the function) is in the currently "<em>seen Actors</em>" array, we can determine whether such an actor has left or entered the field of view:</p>
<pre>void ASightAIController::OnTargetPerceptionUpdate(AActor* Actor, FAIStimulus Stimulus)<br/>{<br/>  //Retrieving Perceived Actors<br/>  TArray&lt;AActor*&gt; PerceivedActors;<br/>  PerceptionComponent-&gt;GetPerceivedActors(TSubclassOf&lt;UAISense_Sight&gt;(), PerceivedActors);<br/><br/><strong>  //Calculating the Number of Perceived Actors and if the current target Left or Entered the field of view.</strong><br/><strong>  bool isEntered = PerceivedActors.Contains(Actor);</strong><br/><strong>  int NumberObjectSeen = PerceivedActors.Num();</strong><br/><br/>}</pre>
<p>Finally, we need to pack this information into a formatted string, and then print it on the screen:</p>
<pre>void ASightAIController::OnTargetPerceptionUpdate(AActor* Actor, FAIStimulus Stimulus)<br/>{<br/>  //Retrieving Perceived Actors<br/>  TArray&lt;AActor*&gt; PerceivedActors;<br/>  PerceptionComponent-&gt;GetPerceivedActors(TSubclassOf&lt;UAISense_Sight&gt;(), PerceivedActors);<br/><br/>  //Calculating the Number of Perceived Actors and if the current target Left or Entered the field of view.<br/>  bool isEntered = PerceivedActors.Contains(Actor);<br/>  int NumberObjectSeen = PerceivedActors.Num();<br/><br/><strong>  //Formatting the string and printing it</strong><br/><strong>  FString text = FString(Actor-&gt;GetName() + " has just " + (isEntered ? "Entered" : "Left") + " the field of view. Now " + FString::FromInt(NumberObjectSeen) + " objects are visible.");</strong><br/><strong>  if (GEngine) {</strong><br/><strong>    GEngine-&gt;AddOnScreenDebugMessage(-1, 5.0f, FColor::Turquoise, text);</strong><br/><strong>  }</strong><br/><strong>  UE_LOG(LogTemp, Warning, TEXT("%s"), *text);</strong><br/><br/>}</pre>
<div class="packt_infobox">Once again, I know that "1 objects" is incorrect, but correcting plurals (although possible) is outside the scope of this book; let's keep it simple.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing it all</h1>
                </header>
            
            <article>
                
<p>Now, you should have an AI controller with the perception system implemented (whether it is in Blueprint or C++ <span>–</span> it doesn't matter, they should behave identically).</p>
<p>Create another <kbd>ThirdPersonCharacter</kbd> by <em>Alt</em> + <em>Dragging</em> the player into the level (if you want to use an AI that we created in the previous chapters, you can do so):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/df7f7507-b26d-4568-a1a2-f47c14969dee.png" style="width:42.83em;height:23.17em;"/></p>
<p>In the <em>Details</em> panel, we make it be controlled by our AI controller, and not a player (this should be a process that's easy to you by now):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8c134131-d15e-4d83-9e7c-60f7cd77f2f3.png" style="width:32.83em;height:16.42em;"/></p>
<p>Alternatively, if you are going with a C++ setup, choose the following settings:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/50b3295c-6d69-4dc9-b9f4-63982f12d0a9.png" style="width:32.67em;height:15.58em;"/></p>
<p>Before pressing play, it would be nice to create some other objects that can be detected. We know that all the Pawns are detected (unless disabled), so let's try something that isn't a Pawn <span>– </span>maybe a moving platform. As a result, if we want to detect it, we need to use the <strong>AIPerceptionStimuliSourceComponent</strong>.</p>
<p>First, let's create the floating platform (which can be easily pushed by our character). If you are in the default level of the <em>ThirdPersonCharacter Example</em>, you can duplicate with <em>Alt + Drag</em> this big mesh, which is highlighted in the following screenshot (otherwise, if you are using a custom level, a cube that you can squash will work fine):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b681cee9-abf5-45c6-b6c5-75d0a6a19f0b.png"/></p>
<p>So far, it's way too big, so let's scale it down to (1, 1, 0.5). Also, to be on the same page, you can move it to (-500, 310, 190). Finally, we need to change the mobility to Movable, since it needs to move:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4cc272c4-56b5-4a87-8d07-fb1be8ddb556.png"/></p>
<p>Next, we want to be able to push such a platform, so we need to enable Physics simulation. To keep it pushable by our character, let's give it a mass of <em>100 Kg</em> (I know, it seems like a lot, but with little friction and with the fact that the platform floats, it's the right amount). Moreover, we don't want the platform to rotate, so we need to block all the three rotational axes inside <strong>Constraints</strong>. The same goes if we want the platform to float <span>–</span> if we lock the z-axis, the platform can only move along the <em>XY plane</em> with no rotation. This will ensure a nice, pushable platform. This is what the Physics part should look like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e6752a91-6659-41af-a376-858798e8c904.png"/></p>
<p>Finally, we need to add a <strong>AIPerceptionStimuliSourceComponent</strong>, from the <strong>Add Component</strong> green button near the name of the Actor:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e5a6ec6f-30c6-48a5-85bd-527d4e6bbeee.png"/></p>
<p>Once the component has been added, we can select it from the preceding menu. As a result, the <em>Details</em> panel will allow us to change <strong>AIPerceptionStimuliSourceComponent</strong> settings. In particular, we want to add the <em>Sight Sense</em>, and automatically register the component as a source. This is how we should set it up:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/67575c9b-2175-4d6d-a9ff-f7be80327656.png"/></p>
<p>As an optional step, you can convert this into a blueprint so that you can reuse it, and maybe assign a more meaningful name. Also, you can duplicate it a few times if you want to have several objects be tracked by the <em>Sight Perception System</em>.</p>
<p class="mce-root"/>
<p>Finally, you can hit play and test what we have achieved so far. If you pass our <em>AI controlled Character</em>, you will get a notification on the top of the screen. We get the same output if we push a platform inside or out of the AI's field of view. In the following screenshot, you can see the C++ implementation, but it works very similarly with the Blueprint one (just the color of the print changes):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0baa2090-609d-4055-ab47-a299a33f8607.png"/></p>
<p>Also, as anticipation, it is possible to see the AI field of view with the visual debugger, which we will explore in <a href="a8cbf52e-71e1-4f9d-a2bd-913a1e8bd8e1.xhtml">Chapter 13</a>, <em>Debugging Methods for AI - The Gameplay Debugger</em>. The following screenshot is a reference of the field of view of the AI Character we have created. For details on how to display it and understand what all this information means, hang on until <a href="a8cbf52e-71e1-4f9d-a2bd-913a1e8bd8e1.xhtml">Chapter 13</a>, <em>Debugging Methods for AI - The Gameplay Debugger</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c0ffc59c-02df-4c6c-b515-6a9b19be5fc2.png"/></p>
<p>It's time to pat yourself on the back because it might seem like you've only done a little, but actually, you managed to learn about a complex system. Also, if you tried one way (Blueprint or C++), try the other one if you want to be able to master the system both in Blueprint and in C++.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>And that was a lot of information to perceive, wasn't it?</p>
<p>We started by understanding how the different pieces of the built-in perception system in Unreal works within the AI framework. From there, we explored how we can actually use those components (both in C++ and Blueprint), and learned how to properly configure them as well.</p>
<p>We concluded with a practical example of setting up a <em>Sight</em> perception system, and once again did so both in Blueprint and C++.</p>
<p>In the next chapter, we will see how we can simulate large <em>Crowds</em>.</p>


            </article>

            
        </section>
    </body></html>