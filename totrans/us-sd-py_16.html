<html><head></head><body>
		<div id="_idContainer105">
			<h1 id="_idParaDest-188" class="chapter-number"><a id="_idTextAnchor309"/>16</h1>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor310"/>Exploring Stable Diffusion XL</h1>
			<p>After the not-very-successful Stable Diffusion 2.0 and Stable Diffusion 2.1, July 2023 saw the launch of Stability AI’s latest release, <strong class="bold">Stable Diffusion XL</strong> (<strong class="bold">SDXL</strong>) [1]. I eagerly applied the model weights data as soon as<a id="_idIndexMarker465"/> registration was open. Both my tests and those conducted by the community indicate that SDXL has made significant strides forward. It now allows us to generate higher-quality images at increased resolutions, vastly outperforming the Stable Diffusion V1.5 base model. Another notable enhancement is the ability to use more intuitive “natural language” prompts to generate images, eliminating the need to cobble together a multitude of “words” to form a meaningful prompt. Furthermore, we can now generate desired images with more <span class="No-Break">concise prompts.</span></p>
			<p>SDXL has improved in almost every aspect compared to the previous versions, and it is worth the time and effort to start using it for better and stable image generation. In this chapter, we will discuss in detail what’s new in SDXL and explain why the aforementioned changes led to its improvements. For example, we will explore what is new in <a id="_idIndexMarker466"/>the <strong class="bold">Variational Autoencoder</strong> (<strong class="bold">VAE</strong>), UNet, and TextEncoder design compared to Stable Diffusion V1.5. In a nutshell, this chapter will cover <span class="No-Break">the following:</span></p>
			<ul>
				<li>What’s new <span class="No-Break">in SDXL?</span></li>
				<li><span class="No-Break">Using SDXL</span></li>
			</ul>
			<p>Then, we will use Python code to demonstrate the latest SDXL base and community models in action. We will cover basic usage and also advanced usage, such as loading multiple LoRA models and using unlimited <span class="No-Break">weighted prompts.</span></p>
			<p><span class="No-Break">Let’s begin<a id="_idTextAnchor311"/>.</span></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor312"/>What’s new in SDXL?</h1>
			<p>SDXL is still a latent diffusion<a id="_idIndexMarker467"/> model, maintaining the same overall architecture used in Stable Diffusion v1.5. According to the original paper behind SDXL [2], SDXL expands every component, making them wider and bigger. The SDXL backbone UNet is three times larger, there are two text encoders in the SDXL base model, and a separate diffusion-based refinement model is included. The overall architecture is shown in <span class="No-Break"><em class="italic">Figure 16</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B21263_16_01.jpg" alt="Figure 16.1: SDXL architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.1: SDXL architecture</p>
			<p>Note that the refiner is optional; we can decide whether to use the refiner model or not. Next, let’s drill down to each component one <span class="No-Break">by one<a id="_idTextAnchor313"/>.</span></p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor314"/>The VAE of the SDXL</h2>
			<p>A <strong class="bold">VAE</strong> is a<a id="_idIndexMarker468"/> pair of <a id="_idIndexMarker469"/>encoder and decoder neural networks. A VAE encoder encodes an image into a latent space, and its paired decoder can decode a latent image to a pixel image. Many articles on the web tell us that a VAE is a technique used to improve the quality of images; however, this is not the whole picture. The core responsibility of VAE in Stable Diffusion is converting pixel images to and from the latent space. Of course, a good VAE can improve the image quality by adding <span class="No-Break">high-frequency details.</span></p>
			<p>The VAE used in SDXL is a retrained one, using the same autoencoder architecture but with an increased batch size (256 versus 9) and, additionally, tracking the weights with an exponential moving average [2]. The new VAE outperforms the original model in all <span class="No-Break">evaluated metrics.</span></p>
			<p>Because of these implementation differences, instead of reusing the VAE code introduced in <a href="B21263_05.xhtml#_idTextAnchor097"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we will need to write new code if we decide to use VAE independently. Here, we will provide an example of some common usage of the <span class="No-Break">SDXL VAE:</span></p>
			<ol>
				<li>Initialize a <span class="No-Break">VAE model:</span><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers.models import AutoencoderKL</pre><pre class="source-code">
vae_model = AutoencoderKL.from_pretrained(</pre><pre class="source-code">
    "stabilityai/stable-diffusion-xl-base-1.0",</pre><pre class="source-code">
    subfolder = "vae"</pre><pre class="source-code">
).to("cuda:0")</pre></li>
				<li>Encode an<a id="_idIndexMarker470"/> image using the <a id="_idIndexMarker471"/>VAE model. Before executing the following code, replace the <strong class="source-inline">cat.png</strong> file with a validated accessible <span class="No-Break">image path:</span><pre class="source-code">
from diffusers.utils import load_image</pre><pre class="source-code">
from diffusers.image_processor import VaeImageProcessor</pre><pre class="source-code">
image = load_image("/path/to/cat.png")</pre><pre class="source-code">
image_processor = VaeImageProcessor()</pre><pre class="source-code">
prep_image = image_processor.preprocess(image)</pre><pre class="source-code">
prep_image = prep_image.to("cuda:0")</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    image_latent = vae_model.encode(prep_image</pre><pre class="source-code">
        ).latent_dist.sample()</pre><pre class="source-code">
image_latent.shape</pre></li>
				<li>Decode an image from <span class="No-Break">latent space:</span><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    decode_image = vae_model.decode(</pre><pre class="source-code">
        image_latent,</pre><pre class="source-code">
        return_dict = False</pre><pre class="source-code">
    )[0]</pre><pre class="source-code">
image = image_processor.postprocess(image = decode_image)[0]</pre><pre class="source-code">
image</pre></li>
			</ol>
			<p>In the preceding code, you first encode an image to latent space. An image in the latent space is invisible to our eyes, but it captures the features of an image in the latent space (in other words, in a high-dimensional vector space). Then, the decode part of the code decodes the image in the<a id="_idIndexMarker472"/> latent space to pixel space. From the preceding code, we know what the core<a id="_idIndexMarker473"/> functionality of a <span class="No-Break">VAE is.</span></p>
			<p>You might be curious as to why knowledge about the VAE is necessary. It has numerous applications. For instance, it allows you to save the generated latent image in a database and decode it only when needed. This method can reduce image storage by up to 90% without much loss <span class="No-Break">of infor<a id="_idTextAnchor315"/>mation.</span></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor316"/>The UNet of SDXL</h2>
			<p>The <a id="_idIndexMarker474"/>UNet model is the backbone neural network of SDXL. The UNet in <a id="_idIndexMarker475"/>SDXL is almost three times larger than the previous Stable Diffusion models. SDXL’s UNet is a 2.6 GB billion parameter neural network, while the Stable Diffusion V1.5’s UNet has 860 million parameters. Although the current open source LLM model is much larger in terms of neural network size, SDXL’s UNet is, so far, the largest among those open source Diffusion models at the time of writing (October 2023), which directly leads to higher VRAM demands. 8 GB of VRAM can meet most of the use cases when using SD V1.5. For SDXL, 15 GB of VRAM is commonly required; otherwise, we will need to reduce the <span class="No-Break">image resolution.</span></p>
			<p>Besides the<a id="_idIndexMarker476"/> model size expansion, SDXL rearranges its <a id="_idIndexMarker477"/>Transformer block’s position, which is crucial for better and more precise natural <span class="No-Break">language-to-imag<a id="_idTextAnchor317"/>e guidance.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor318"/>Two text encoders in SDXL</h2>
			<p>One of the <a id="_idIndexMarker478"/>most significant changes in SDXL is the text encoder. SDXL uses two text<a id="_idIndexMarker479"/> encoders together, CLIP ViT-L [5] and OpenCLIP ViT-bigG (also named OpenCLIP G/14). Furthermore, SDXL uses pooled embeddings from <span class="No-Break">OpenCLIP ViT-bigG.</span></p>
			<p>CLIP ViT-L is one <a id="_idIndexMarker480"/>of the most widely used models from OpenAI, which is also the text encoder or embedding model used in Stable Diffusion V1.5. What is the OpenCLIP ViT-bigG model? OpenCLIP<a id="_idIndexMarker481"/> is an open <a id="_idIndexMarker482"/>source implementation of <strong class="bold">CLIP</strong> (<strong class="bold">Contrastive Language-Image Pre-Training</strong>). OpenCLIP G/14 is the largest and best OpenClip model trained on the LAION-2B dataset [9], a 100 TB dataset containing 2 billion images. While the OpenAI CLIP model generates a 768-dimensional embedding vector, OpenClip G/14 outputs a 1,280-dimensional embedding. By concatenating the two embeddings (of the same length), a 2,048-dimensional embedding is output. This is much larger than the previous 768-dimensional embedding from Stable <span class="No-Break">Diffusion v1.5.</span></p>
			<p>To illustrate the text encoding process, let’s take the sentence <strong class="source-inline">a running dog</strong> as input; the ordinary text tokenizer will first convert the sentence into tokens, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
input_prompt = "a running dog"
from transformers import CLIPTokenizer,CLIPTextModel
import torch
# initialize tokenizer 1
clip_tokenizer = CLIPTokenizer.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    subfolder = "tokenizer",
    dtype = torch.float16
)
input_tokens = clip_tokenizer(
    input_prompt,
    return_tensors="pt"
)["input_ids"]
print(input_tokens)
clip_tokenizer_2 = CLIPTokenizer.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    subfolder = "tokenizer_2",
    dtype = torch.float16
)
input_tokens_2 = clip_tokenizer_2(
    input_prompt,
    return_tensors="pt"
)["input_ids"]
print(input_tokens_2)</pre>
			<p>The <a id="_idIndexMarker483"/>preceding code<a id="_idIndexMarker484"/> will return the <span class="No-Break">following result:</span></p>
			<pre class="source-code">
tensor([[49406,   320,  2761,  1929, 49407]])
tensor([[49406,   320,  2761,  1929, 49407]])</pre>
			<p>In the preceding result, <strong class="source-inline">49406</strong> is the beginning token and <strong class="source-inline">49407</strong> is the <span class="No-Break">end token.</span></p>
			<p>Next, the<a id="_idIndexMarker485"/> following <a id="_idIndexMarker486"/>code uses the CLIP text encoder to convert the tokens into <span class="No-Break">embedding vectors:</span></p>
			<pre class="source-code">
clip_text_encoder = CLIPTextModel.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    subfolder = "text_encoder",
    torch_dtype =torch.float16
).to("cuda")
# encode token ids to embeddings
with torch.no_grad():
    prompt_embeds = clip_text_encoder(
        input_tokens.to("cuda")
    )[0]
print(prompt_embeds.shape)</pre>
			<p>The result embedding tensor includes five 768 <span class="No-Break">dimension vectors:</span></p>
			<pre class="source-code">
torch.Size([1, 5, 768])</pre>
			<p>The previous code used OpenAI’s CLIP to convert the prompt text to 768-dimension embeddings. The following code uses the OpenClip G/14 model to encode the tokens into five <span class="No-Break">1,280-dimension embeddings:</span></p>
			<pre class="source-code">
clip_text_encoder_2 = CLIPTextModel.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    subfolder = "text_encoder_2",
    torch_dtype =torch.float16
).to("cuda")
# encode token ids to embeddings
with torch.no_grad():
    prompt_embeds_2 = clip_text_encoder_2(input_tokens.to("cuda"))[0]
print(prompt_embeds_2.shape)</pre>
			<p>The result embedding tensor includes five <span class="No-Break">1,280-dimension vectors:</span></p>
			<pre class="source-code">
torch.Size([1, 5, 1280])</pre>
			<p>Now, the next question is, what are <strong class="bold">pooled embeddings</strong>? Embedding pooling is the process of converting a sequence of tokens <a id="_idIndexMarker487"/>into one embedding vector. In other words, pooling embedding is a lossy compression <span class="No-Break">of information.</span></p>
			<p>Unlike the embedding process we used before, which encodes each token into an embedding vector, a pooled embedding is one vector that represents the whole input text. We can generate the pooled embedding from OpenClip using the following <span class="No-Break">Python code:</span></p>
			<pre class="source-code">
from transformers import CLIPTextModelWithProjection
clip_text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    subfolder = "text_encoder_2",
    torch_dtype =torch.float16
).to("cuda")
# encode token ids to embeddings
with torch.no_grad():
    pool_embed = clip_text_encoder_2(input_tokens.to("cuda"))[0]
print(pool_embed.shape)</pre>
			<p>The preceding code will return a <strong class="source-inline">torch.Size([1, 1280])</strong> pooled embedding vector from the text encoder. The maximum token size for a pooled embedding is <strong class="source-inline">77</strong>. In SDXL, the pooled embedding is provided to the UNet together with the token-level embedding from both CLIP and OpenCLIP, guiding the <span class="No-Break">image generation.</span></p>
			<p>Don’t worry – you won’t need to manually provide these embeddings before using SDXL. <strong class="source-inline">StableDiffusionXLPipeline</strong> from the <strong class="source-inline">Diffusers</strong> package does everything for us. All we need to do is provide the prompt and negative prompt text. We will provide the sample code in th<a id="_idTextAnchor319"/>e <em class="italic">Using </em><span class="No-Break"><em class="italic">SDXL</em></span><span class="No-Break"> section.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor320"/>The two-stage design</h2>
			<p>Another design addition in <a id="_idIndexMarker488"/>SDXL is its refiner model. According to the SDXL paper [2], the refiner model is used to enhance an image by adding more details and making it better, especially during the last <span class="No-Break">10 steps.</span></p>
			<p>The refiner model is just another image-to-image model that can help fix broken images and add more elements to the images generated by the <span class="No-Break">base model.</span></p>
			<p>Based on my observations, for community-shared checkpoint models, the refiner model may not <span class="No-Break">be necessary.</span></p>
			<p>Next, we are going to use SDXL<a id="_idTextAnchor321"/> for common <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor322"/>Using SDXL</h1>
			<p>We briefly covered <a id="_idIndexMarker489"/>loading the SDXL model in <a href="B21263_06.xhtml#_idTextAnchor117"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> and SDXL ControlNet usage in <a href="B21263_13.xhtml#_idTextAnchor257"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>. You can find the sample codes there. In this section, we will cover more common SDXL usages, including loading community-shared SDXL models and how to use the image-to-image pipeline to enhance the model, using SDXL with community-shared LoRA models, and the unlimited length prompt pipeline from Diffuser (provided by th<a id="_idTextAnchor323"/>e author of <span class="No-Break">this book).</span></p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor324"/>Use SDXL community models</h2>
			<p>Just months after the release of<a id="_idIndexMarker490"/> SDXL, the open source community has released countless fine-tuned SDXL models based on the base model from Stability AI. We can find these models on Hugging Face and CIVITAI (<a href="https://civitai.com/">https://civitai.com/</a>), and <a id="_idIndexMarker491"/>the number <span class="No-Break">keeps growing.</span></p>
			<p>Here, let’s load one model from HuggingFace, using the SDXL <span class="No-Break">model ID:</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionXLPipeline
base_pipe = StableDiffusionXLPipeline.from_pretrained(
    "RunDiffusion/RunDiffusion-XL-Beta",
    torch_dtype = torch.float16
)
base_pipe.watermark = None</pre>
			<p>Note that in the preceding code, <strong class="source-inline">base_pipe.watermark = None</strong> will remove the invisible watermark from the <span class="No-Break">generated image.</span></p>
			<p>Next, move the model to CUDA, generate an image, and then offload the model <span class="No-Break">from CUDA:</span></p>
			<pre class="source-code">
prompt = "realistic photo of astronaut cat in fighter cockpit, detailed, 8k"
sdxl_pipe.to("cuda")
image = sdxl_pipe(
    prompt = prompt,
    width = 768,
    height = 1024,
    generator = torch.Generator("cuda").manual_seed(1)
).images[0]
sdxl_pipe.to("cpu")
torch.cuda.empty_cache()
image</pre>
			<p>With just one line <a id="_idIndexMarker492"/>prompt and not needing to provide any negative prompt, SDXL generates an amazing image, as shown in <span class="No-Break"><em class="italic">Figure 16</em></span><span class="No-Break"><em class="italic">.2:</em></span></p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B21263_16_02.jpg" alt="Figure 16.2: A cat pilot generated by SDXL"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.2: A cat pilot generated by SDXL</p>
			<p>You may want to <a id="_idIndexMarker493"/>use the refiner model to enhance the image, but the refiner model doesn’t make a significant difference. Instead, we will use the image-to-image pipeline with the same model <a id="_idTextAnchor325"/>data to upscale <span class="No-Break">the image.</span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor326"/>Using SDXL image-to-image to enhance an image</h2>
			<p>Let’s first<a id="_idIndexMarker494"/> upscale the image <span class="No-Break">to twice:</span></p>
			<pre class="source-code">
from diffusers.image_processor import VaeImageProcessor
img_processor = VaeImageProcessor()
# get the size of the image
(width, height) = image.size
# upscale image
image_x = img_processor.resize(
    image = image,
    width = int(width * 1.5),
    height = int(height * 1.5)
)
image_x</pre>
			<p>Then, start an image-to-image pipeline by reusing the model data from the previous text-to-image pipeline, saving the RAM and <span class="No-Break">VRAM usage:</span></p>
			<pre class="source-code">
from diffusers import StableDiffusionXLImg2ImgPipeline
img2img_pipe = StableDiffusionXLImg2ImgPipeline(
    vae = sdxl_pipe.vae,
    text_encoder = sdxl_pipe.text_encoder,
    text_encoder_2 = sdxl_pipe.text_encoder_2,
    tokenizer = sdxl_pipe.tokenizer,
    tokenizer_2 = sdxl_pipe.tokenizer_2,
    unet = sdxl_pipe.unet,
    scheduler = sdxl_pipe.scheduler,
    add_watermarker = None
)
img2img_pipe.watermark = None</pre>
			<p>Now, it is time<a id="_idIndexMarker495"/> to call the pipeline to further enhance <span class="No-Break">the image:</span></p>
			<pre class="source-code">
img2img_pipe.to("cuda")
refine_image_2x = img2img_pipe(
    image = image_x,
    prompt = prompt,
    strength = 0.3,
    num_inference_steps = 30,
    guidance_scale = 4.0
).images[0]
img2img_pipe.to("cpu")
torch.cuda.empty_cache()
refine_image_2x</pre>
			<p>Note that we set the strength to <strong class="source-inline">0.3</strong> to preserve most of the original input image information. We will get a new, better image, as shown in <span class="No-Break"><em class="italic">Figure 16</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B21263_16_03.jpg" alt="Figure 16.3: The refined cat pilot image from an image-to-image pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.3: The refined cat pilot image from an image-to-image pipeline</p>
			<p>While you might <a id="_idIndexMarker496"/>not notice many differences in this book at first glance, upon closer inspection of the image on a computer monitor, you will discover numerous <span class="No-Break">additional details.</span></p>
			<p>Now, let’s explore how to utilize LoRA with Diffusers. If you’re unfamiliar with LoRA, I recommend turning back to <a href="B21263_08.xhtml#_idTextAnchor153"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, which delves into the usage of Stable Diffusion LoRA in greater detail, and <a href="B21263_21.xhtml#_idTextAnchor405"><span class="No-Break"><em class="italic">Chapter 21</em></span></a>, which provides comprehensiv<a id="_idTextAnchor327"/>e coverage of <span class="No-Break">LoRA training.</span></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor328"/>Using SDXL LoRA models</h2>
			<p>Not long ago, it was impossible to load <a id="_idIndexMarker497"/>LoRA using Diffusers, not to mention loading multiple LoRA models into one pipeline. With the massive work that has been done by the Diffusers team and community contributors, we can now load multiple LoRA models into the SDXL pipeline with the LoRA scale <span class="No-Break">number specified.</span></p>
			<p>And its usage is also extremely simple. It takes just two lines of code to add one LoRA to <span class="No-Break">the pipeline:</span></p>
			<pre class="source-code">
sdxl_pipe.load_lora_weights("path/to/lora.safetensors")
sdxl_pipe.fuse_lora(lora_scale = 0.5)</pre>
			<p>To add two <span class="No-Break">LoRA models:</span></p>
			<pre class="source-code">
sdxl_pipe.load_lora_weights("path/to/lora1.safetensors")
sdxl_pipe.fuse_lora(lora_scale = 0.5)
sdxl_pipe.load_lora_weights("path/to/lora2.safetensors")
sdxl_pipe.fuse_lora(lora_scale = 0.5)</pre>
			<p>As we discussed in <a href="B21263_08.xhtml#_idTextAnchor153"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, there are two ways to use LoRA – one is merging with the backbone model weights, and the other is dynamic monkey patching. Here, for SDXL, the method is model merging, which means unloading a LoRA from the pipeline. To unload a LoRA model, we will need to load the LoRA again but with a negative <strong class="source-inline">lora_scale</strong>. For example, if we want to unload <strong class="source-inline">lora2.safetensors</strong> from the pipeline, we can use the following code to <span class="No-Break">achieve it:</span></p>
			<pre class="source-code">
sdxl_pipe.load_lora_weights("path/to/lora2.safetensors")
sdxl_pipe.fuse_lora(lora_scale = -0.5)</pre>
			<p>Besides using <strong class="source-inline">fuse_lora</strong> to load a LoRA model, we can also use PEFT-integrated LoRA loading. The code is very similar to the one we just used, but we add one more parameter called <strong class="source-inline">adapter_name</strong>, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
sdxl_pipe.load_lora_weights("path/to/lora1.safetensors",
    adapter_name="lora1")
sdxl_pipe.load_lora_weights("path/to/lora2.safetensors", ,
    adapter_name="lora2")</pre>
			<p>We can adjust the LoRA scale dynamically with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
sdxl_pipe.set_adapters(["lora1", "lora2"], adapter_weights=[0.5, 1.0])</pre>
			<p>And we can also disable LoRA <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
sdxl_pipe.disable_lora()</pre>
			<p>Alternatively, we can disable one of the two loaded LoRA models, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
sdxl_pipe.set_adapters(["lora1", "lora2"], adapter_weights=[0.0, 1.0])</pre>
			<p>In the preceding code, we disabled <strong class="source-inline">lora1</strong> while continuing to <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">lora2</strong></span><span class="No-Break">.</span></p>
			<p>With proper<a id="_idIndexMarker498"/> LoRA management code, you can use SDXL with an unlimited number of LoRA models. Speaking of “unlimited,” next, we will cover the “unli<a id="_idTextAnchor329"/>mited” length prompt <span class="No-Break">for SDXL.</span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor330"/>Using SDXL with an unlimited prompt</h2>
			<p>By default, SDXL, like<a id="_idIndexMarker499"/> previous versions, supports only a maximum of 77 tokens for one-time image generation. In <a href="B21263_10.xhtml#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we delved deep into implementing a text embedding encoder that supports weighted prompts without length limitation. For SDXL, the idea is similar but more complex and a bit harder to implement; after all, there are now two <span class="No-Break">text encoders.</span></p>
			<p>I built a long-weighted SDXL pipeline, <strong class="source-inline">lpw_stable_diffusion_xl</strong>, which is merged with the official <strong class="source-inline">Diffusers</strong> package. In this section, I will introduce the usage of this pipeline to enable a long-weighted and <span class="No-Break">unlimited pipeline.</span></p>
			<p>Make sure you have updated your <strong class="source-inline">Diffusers</strong> package to the latest version with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
pip install -U diffusers</pre>
			<p>Then, use the following code to use <span class="No-Break">the pipeline:</span></p>
			<pre class="source-code">
from diffusers import DiffusionPipeline
import torch
pipe = DiffusionPipeline.from_pretrained(
    "RunDiffusion/RunDiffusion-XL-Beta",
    torch_dtype = torch.float16,
    use_safetensors = True,
    variant = "fp16",
    custom_pipeline = "lpw_stable_diffusion_xl",
)
prompt = """
glamour photography, (full body:1.5) photo of young man,
white blank background,
wear sweater, with scarf,
wear jean pant,
wear nike run shoes,
wear sun glass,
wear leather shoes,
holding a umbrella in hand
""" * 2
prompt = prompt + " a (cute cat:1.5) aside"
neg_prompt = """
(worst quality:1.5),(low quality:1.5), paint, cg, spots, bad hands,
three hands, noise, blur
"""
pipe.to("cuda")
image = pipe(
    prompt = prompt,
    negative_prompt = neg_prompt,
    width = 832,
    height = 1216,
    generator = torch.Generator("cuda").manual_seed(7)
).images[0]
pipe.to("cpu")
torch.cuda.empty_cache()
image</pre>
			<p>The preceding<a id="_idIndexMarker500"/> code uses <strong class="source-inline">DiffusionPipeline</strong> to load a custom pipeline, <strong class="source-inline">lpw_stable_diffusion_xl</strong>, contributed by an open source community member (<span class="No-Break">i.e., me).</span></p>
			<p>Note that in the code, the prompt is multiplied by 2, making it definitely longer than 77 tokens. At the end of the prompt, <strong class="source-inline">a (cute cat:1.5) aside</strong> is appended. If the pipeline supports prompts longer than 77 tokens, there should be a cat in the <span class="No-Break">generated result.</span></p>
			<p>The image generated from the preceding code is shown in <span class="No-Break"><em class="italic">Figure 16</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B21263_16_04.jpg" alt="Figure 16.4: A man with a cat, generated using an unlimited prompt length pipeline – lpw_stable_diffusion_xl"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 16.4: A man with a cat, generated using an unlimited prompt length pipeline – lpw_stable_diffusion_xl</p>
			<p>From the <a id="_idIndexMarker501"/>image, we can see that all elements in the prompt are reflected, and there is now a cut<a id="_idTextAnchor331"/>e cat sitting alongside <span class="No-Break">the man.</span></p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor332"/>Summary</h1>
			<p>This chapter covers the newest and best Stable Diffusion model – SDXL. We first introduced the basics of SDXL and why it is powerful and efficient, and then we drilled down into each component of the newly released model, covering VAE, UNet, text encoders, and the new <span class="No-Break">two-stage design.</span></p>
			<p>We provided a sample code for each of the components to help you understand SDXL inside out. These code samples can also be used to leverage the power of the individual components. For example, we can use VAE to compress images and a text encoder to generate text embeddings <span class="No-Break">for images.</span></p>
			<p>In the second half of this chapter, we covered some common use cases of SDXL, such as loading community-shared checkpoint models, using the image-to-image pipeline to enhance and upscale images, and introducing a simple and effective solution to load multiple LoRA models into one pipeline. Finally, we provided an end-to-end solution to use unlimited length-weighted prompts <span class="No-Break">for SDXL.</span></p>
			<p>With the help of SDXL, we can generate amazing images with short prompts and achieve much <span class="No-Break">better results.</span></p>
			<p>In the next chapter, we are going to discuss how to write Stable Diffusion prompts and leverage LLM to help produce an<a id="_idTextAnchor333"/>d enhance <span class="No-Break">prompts automatically.</span></p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor334"/>References</h1>
			<ol>
				<li><span class="No-Break">SDXL: </span><a href="https://stability.ai/stable-diffusion"><span class="No-Break">https://stability.ai/stable-diffusion</span></a></li>
				<li>SDXL: Improving Latent Diffusion Models for High-Resolution Image <span class="No-Break">Synthesis: </span><a href="https://arxiv.org/abs/2307.01952"><span class="No-Break">https://arxiv.org/abs/2307.01952</span></a></li>
				<li>Stable Diffusion XL <span class="No-Break">Diffusers: </span><a href="https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl"><span class="No-Break">https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl</span></a></li>
				<li>CLIP from <span class="No-Break">OpenAI: </span><a href="https://openai.com/research/clip"><span class="No-Break">https://openai.com/research/clip</span></a></li>
				<li>CLIP VIT Large <span class="No-Break">model: </span><a href="https://huggingface.co/openai/clip-vit-large-patch14"><span class="No-Break">https://huggingface.co/openai/clip-vit-large-patch14</span></a></li>
				<li>REACHING 80% ZERO-SHOT ACCURACY WITH OPENCLIP: VIT-G/14 TRAINED ON <span class="No-Break">LAION-2B: </span><a href="https://laion.ai/blog/giant-openclip/"><span class="No-Break">https://laion.ai/blog/giant-openclip/</span></a></li>
				<li><span class="No-Break">CLIP-ViT-bigG-14-laion2B-39B-b160k: </span><a href="https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"><span class="No-Break">https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</span></a></li>
				<li>OpenCLIP GitHub <span class="No-Break">repository: </span><a href="https://github.com/mlfoundations/open_clip"><span class="No-Break">https://github.com/mlfoundations/open_clip</span></a></li>
				<li>LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL <span class="No-Break">DATASETS: </span><a href="https://laion.ai/blog/laion-5b/"><span class="No-Break">https://laion.ai/blog/laion-5b/</span></a></li>
			</ol>
		</div>
	</body></html>