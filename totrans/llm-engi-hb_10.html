<html><head></head><body>
  <div id="_idContainer208" class="Basic-Text-Frame">
    <h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-233" class="chapterTitle">Inference Pipeline Deployment</h1>
    <p class="normal">Deploying the inference pipeline for <a id="_idIndexMarker889"/>the <strong class="keyWord">large language model</strong> (<strong class="keyWord">LLM</strong>) Twin application is a critical<a id="_idIndexMarker890"/> stage in the <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) application life cycle. It’s where the most value is added to your business, making your models accessible to your end users. However, successfully deploying AI models can be challenging, as the models require expensive computing power and access to up-to-date features to run the inference. To overcome these constraints, it’s crucial to carefully design your deployment strategy. This ensures that it meets the application’s requirements, such as latency, throughput, and costs. As we work with LLMs, we must consider the inference optimization techniques presented in <em class="italic">Chapter 8</em>, such as model quantization. Also, to automate the deployment processes, we must leverage MLOps best practices, such as model registries that version and share our models across our infrastructure.</p>
    <p class="normal">To understand how to design the deployment architecture of the LLM Twin, we will first look at three deployment types we can choose from: online real-time inference, asynchronous inference, and offline batch transform. Also, to better understand which option to choose for our LLM Twin use case, we will quickly walk you through a set of critical criteria we must consider before making an architectural decision, such as latency, throughput, data, and infrastructure. Also, we’ll weigh the pros and cons of monolithic and microservices architecture in model serving, a decision that can significantly influence the scalability and maintainability of your service.Once we’ve grasped the various design choices available, we’ll focus on understanding the deployment strategy for the LLM Twin’s inference pipeline. Subsequently, we will walk you through an end-to-end tutorial on deploying the LLM Twin service, including deploying our custom fine-tuned LLM to AWS SageMaker endpoints and implementing a FastAPI server as the central entry point for our users. We will then wrap up this chapter with a short discussion on autoscaling strategies and how to use them on SageMaker.</p>
    <p class="normal">Hence, in this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Criteria for choosing deployment types</li>
      <li class="bulletList">Understanding inference deployment types</li>
      <li class="bulletList">Monolithic versus microservices architecture in model serving</li>
      <li class="bulletList">Exploring the LLM Twin’s inference pipeline deployment strategy</li>
      <li class="bulletList">Deploying the LLM Twin service</li>
      <li class="bulletList">Autoscaling capabilities to handle spikes in usage</li>
    </ul>
    <h1 id="_idParaDest-234" class="heading-1">Criteria for choosing deployment types</h1>
    <p class="normal">When it comes to deploying ML models, the first step is to understand the four requirements present in every ML application: throughput, latency, data, and infrastructure.</p>
    <p class="normal">Understanding them and their interaction is essential. When designing the deployment architecture for your models, there is always a trade-off between the four that will directly impact the user’s experience. For example, should your model deployment be optimized for low latency or high throughput?</p>
    <h2 id="_idParaDest-235" class="heading-2">Throughput and latency</h2>
    <p class="normal"><strong class="keyWord">Throughput </strong>refers<a id="_idIndexMarker891"/> to the<a id="_idIndexMarker892"/> number of inference requests a system can process in a given period. It is typically measured<a id="_idIndexMarker893"/> in <strong class="keyWord">requests per second</strong> (<strong class="keyWord">RPS</strong>). Throughput is crucial when deploying ML models when you expect to process many requests. It ensures the system can handle many requests efficiently without becoming a bottleneck.</p>
    <p class="normal">High throughput often requires scalable and robust infrastructure, such as machines or clusters with multiple high-end GPUs.<strong class="keyWord">Latency</strong> is the<a id="_idIndexMarker894"/> time it<a id="_idIndexMarker895"/> takes for a system to process a single inference request from when it is received until the result is returned. Latency is critical in real-time applications where quick response times are essential, such as in live user interactions, fraud detection, or any system requiring immediate feedback. For example, the average latency of OpenAI’s API is the average response time from when a user sends a request, and the service provides a result that is accessible within your application.</p>
    <p class="normal">The latency <a id="_idIndexMarker896"/>is the sum of the <a id="_idIndexMarker897"/>network I/O, serialization and deserialization, and the LLM’s inference time. Meanwhile, the throughput is the average number of requests the API processes and serves a second.</p>
    <p class="normal">Low-latency systems require optimized and often more costly infrastructure, such as faster processors, lower network latency, and possibly edge computing to reduce the distance data needs to travel.</p>
    <p class="normal">A lower latency translates to higher <a id="_idIndexMarker898"/>throughput when the service processes multiple queries in parallel successfully. For example, if the <a id="_idIndexMarker899"/>service takes 100 ms to process requests, this translates to a throughput of 10 requests per second. If the latency reaches 10 ms per request, the throughput rises to 100 requests per second.</p>
    <p class="normal">However, to complicate things, most ML applications adopt a batching strategy to simultaneously pass multiple data samples to the model. In this case, a lower latency can translate into lower throughput; in other words, a higher latency maps to a higher throughput. For example, if you process 20 batched requests in 100 ms, the latency is 100 ms, while the throughput is 200 requests per second. If you process 60 requests in 200 ms, the latency is 200 ms, while the throughput rises to 300 requests per second. Thus, even when batching requests at serving time, it’s essential to consider the minimum latency accepted for a good user experience.</p>
    <h2 id="_idParaDest-236" class="heading-2">Data</h2>
    <p class="normal">As we know, data<a id="_idIndexMarker900"/> is everywhere in an<a id="_idIndexMarker901"/> ML system. But when talking about model serving, we mostly care about the model’s input and output. This includes the format, volume, and complexity of the processed data. Data is the foundation of the inference process. The characteristics of the data, such as its size and type, determine how the system needs to be configured and optimized for efficient processing.</p>
    <p class="normal">The type and size of the data directly impact latency and throughput, as more complex or extensive data can take longer to process. For example, designing a model that takes input structured data and outputs a probability differs entirely from an LLM that takes input text (or even images) and outputs an array of characters.</p>
    <p class="normal">Infrastructure</p>
    <p class="normal">Infrastructure<a id="_idIndexMarker902"/> refers to the<a id="_idIndexMarker903"/> underlying hardware, software, networking, and system architecture that supports the deployment and operation of the ML models. The infrastructure provides the necessary resources for deploying, scaling, and maintaining ML models. It includes computing resources, memory, storage, networking components, and the software stack:</p>
    <ul>
      <li class="bulletList">For <strong class="keyWord">high throughput</strong>, the <a id="_idIndexMarker904"/>systems require scalable infrastructure to manage large data volumes and high request rates, possibly through parallel processing, distributed systems, and high-end GPUs.</li>
      <li class="bulletList">Infrastructure must be optimized to reduce processing time to achieve <strong class="keyWord">low latency</strong>, such as <a id="_idIndexMarker905"/>using faster CPUs, GPUs, or specialized hardware. While optimizing your system for low latency while batching your requests, you often have to sacrifice high throughput in favor of lower latency, which can result in your hardware not being utilized at total capacity. As you process fewer requests per second, it results in idle computing, which increases the overall cost of processing a request. Thus, picking the suitable machine for your requirements is critical in optimizing costs.</li>
    </ul>
    <p class="normal">It is crucial to design infrastructure to meet specific data requirements. This includes selecting storage solutions to handle large datasets and implementing fast retrieval mechanisms to ensure efficient data access. For example, we mostly care about optimizing throughput for offline training, while for online inference, we generally care about latency.</p>
    <p class="normal">With this in mind, before picking a specific deployment type, you should ask yourself questions such as:</p>
    <ul>
      <li class="bulletList">What are the throughput requirements? You should make this decision based on the throughput’s required minimum, average, and maximum statistics.</li>
      <li class="bulletList">How many requests the system must handle simultaneously? (1, 10, 1,000, 1 million, etc.)</li>
      <li class="bulletList">What are the latency requirements? (1 millisecond, 10 milliseconds, 1 second, etc.)</li>
      <li class="bulletList">How should the system scale? For example, we should look at the CPU workload, number of requests, queue size, data size, or a combination of them.</li>
      <li class="bulletList">What are the cost requirements?With what data do we work with? For example, do we work with images, text, or tabular data?</li>
      <li class="bulletList">What<a id="_idIndexMarker906"/> is the size of the data <a id="_idIndexMarker907"/>we work with? (100 MB, 1 GB, 10 GB)</li>
    </ul>
    <p class="normal">Deeply thinking about these questions directly impacts the user experience of your application, which ultimately makes the difference between a successful product and not. Even if you ship a mind-blowing model, if the user needs to wait too long for a response or it often crashes, the user will switch your production to something less accurate that works reliably. For example, Google found in a 2016 study that 53% of visits are abandoned if a mobile site takes longer than three seconds to load: <a href="https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/mobile-site-load-time-statistics/"><span class="url">https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/mobile-site-load-time-statistics/</span></a>.</p>
    <p class="normal">Let’s move on to the three deployment architectures we can leverage to serve our models.</p>
    <h1 id="_idParaDest-237" class="heading-1">Understanding inference deployment types</h1>
    <p class="normal">As illustrated in <em class="italic">Figure 10.1</em>, you can <a id="_idIndexMarker908"/>choose from three fundamental deployment types when serving models:</p>
    <ul>
      <li class="bulletList">Online real-time inference</li>
      <li class="bulletList">Asynchronous inference</li>
      <li class="bulletList">Offline batch transform</li>
    </ul>
    <p class="normal">When selecting one design over the other, there is a trade-off between latency, throughput, and costs. You must consider how the data is accessed and the infrastructure you are working with. Another criterion you have to consider is how the user will interact with the model. For example, will the user use it directly, like a chatbot, or will it be hidden within your system, like a classifier that checks if an input (or output) is safe?</p>
    <p class="normal">You have to consider the<a id="_idIndexMarker909"/> freshness of the predictions as well. For example, serving your model in offline batch mode might be easier to implement if, in your use case, it is OK to consume delayed predictions. Otherwise, you have to serve your model in real-time, which is more infrastructure-demanding. Also, you have to consider your application’s traffic. Ask yourself questions such as, “Will the application be constantly used, or will there be spikes in traffic and then flatten out?”</p>
    <p class="normal">With that in mind, let’s explore the three major ML deployment types.</p>
    <figure class="mediaobject"><img src="../Images/B31105_10_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.1: The three fundamental architectures of inference deployment types</p>
    <h2 id="_idParaDest-238" class="heading-2">Online real-time inference</h2>
    <p class="normal">In real-time inference, we have<a id="_idIndexMarker910"/> a simple architecture based on a server that can be accessed through HTTP requests. The most popular options are to implement a REST <a id="_idIndexMarker911"/>API or gRPC server. The REST API is more accessible but slower, using JSON to pass data between the client and server. </p>
    <p class="normal">This approach is usually taken when serving models outside your internal network to the broader public. For example, OpenAI’s API implements a REST API protocol.</p>
    <p class="normal">On the other hand, implementing a gRPC makes your ML server faster, though it may reduce its flexibility and general applicability. You have to implement <code class="inlineCode">protobuf</code> schemas in your client application, which are more tedious to work with than JSON structures. The benefit, however, is that <code class="inlineCode">protobuf</code> objects can be compiled into bites, making the network transfers much faster. Thus, this protocol is often adopted for internal services within the same ML system.</p>
    <p class="normal">Using the real-time inference approach, the client sends an HTTP request to the ML service, which immediately processes the request and returns the result in the same response. This synchronous interaction means the client waits for the result before moving on.</p>
    <p class="normal">To make this work efficiently, the infrastructure must support low-latency, highly responsive ML services, often deployed on fast, scalable servers. Load balancing is crucial to evenly distribute incoming traffic evenly, while autoscaling ensures the system can handle varying loads. High availability is also essential to keeping the service operational at all times.</p>
    <p class="normal">For example, this architecture is often present when interacting with LLMs, as when sending a request to a chatbot or API (powered by LLMs), you expend the predictions right ahead. LLM services, such as ChatGPT or Claude, often use WebSockets to stream each token individually to the end user, making the interaction more responsive. Other famous examples are AI services such as embedding or reranking models used for <strong class="keyWord">retrieval-augmented generation</strong> (<strong class="keyWord">RAG</strong>) or online recommendation engines in platforms like TikTok.</p>
    <p class="normal">The simplicity of<a id="_idIndexMarker912"/> real-time inference, with its direct client-server interaction, makes <a id="_idIndexMarker913"/>it an attractive option for applications that require immediate responses, like chatbots or real-time recommendations. However, this approach can be challenging to scale and may lead to underutilized resources during low-traffic periods.</p>
    <h2 id="_idParaDest-239" class="heading-2">Asynchronous inference</h2>
    <p class="normal">In <a id="_idIndexMarker914"/>asynchronous<a id="_idIndexMarker915"/> inference, the client sends a request to the ML service, which acknowledges the request and places it in a queue for processing. Unlike real-time inference, the client doesn’t wait for an immediate response. Instead, the ML service processes the request asynchronously. This requires a robust infrastructure that queues the messages to be processed by the ML service later on.</p>
    <p class="normal">When the results are ready, you can leverage multiple techniques to send them to the client. For example, depending on the size of the result, you can put it either in a different queue or an object storage dedicated to storing the results. </p>
    <p class="normal">The client can either adopt a polling mechanism that checks on a schedule if there are new results or adopt a push strategy and implement a notification system to inform the client when the results are ready.</p>
    <p class="normal">Asynchronous inference uses resources more efficiently. It doesn’t have to process all the requests simultaneously but can define a maximum number of machines that run in parallel to process the messages. This is possible because the requests are stored in the queue until a machine can process them. Another huge benefit is that it can handle spikes in requests without any timeouts. For example, let’s assume that on an e-shop site, we usually have 10 requests per second handled by two machines. Because of a promotion, many people started to visit the site, and the number of requests spiked to 100 requests per second. Instead of scaling the number of <strong class="keyWord">virtual machines </strong>(<strong class="keyWord">VMs</strong>) by 10, which can add drastic costs, the requests are queued, and the same two VMs can process them in their rhythm without any failures.</p>
    <p class="normal">Another popular advantage for asynchronous architectures is when the requested job takes significant time to complete. For example, if the job takes over five minutes, you don’t want to block the client waiting for a response.</p>
    <p class="normal">While asynchronous inference offers significant benefits, it does come with trade-offs. It introduces higher latency, making it less suitable for time-sensitive applications. Additionally, it adds complexity to the implementation and infrastructure. Depending on your design choices, this architecture type falls somewhere between online and offline, offering a balance of benefits and trade-offs.</p>
    <p class="normal">For example, this<a id="_idIndexMarker916"/> is a robust design where you don’t care too much about the latency<a id="_idIndexMarker917"/> of the inference but want to optimize costs heavily. Thus, it is a popular choice for problems such as extracting keywords from documents, summarizing them using LLMs, or running deep-fake models on top of videos. But suppose you carefully design the autoscaling system to process the requests from the queue at decent speeds. In that case, you can leverage this design for other use cases, such as online recommendations for e-commerce. In the end, it sums up how much computing power you are willing to pay to meet the expectations of your application.</p>
    <h2 id="_idParaDest-240" class="heading-2">Offline batch transform</h2>
    <p class="normal">Batch transform is about<a id="_idIndexMarker918"/> processing large volumes of data simultaneously, either on a schedule or triggered manually. In a batch transform architecture, the ML service pulls<a id="_idIndexMarker919"/> data from a storage system, processes it in a single operation, and then stores the results in storage. The storage system can be implemented as an object storage like AWS S3 or a data warehouse like GCP BigQuery. </p>
    <p class="normal">Unlike the asynchronous inference architecture, a batch transform design is optimized for high throughput with permissive latency requirements. When real-time predictions are unnecessary, this approach can significantly reduce costs, as processing data in big batches is the most economical method. Moreover, the batch transform architecture is the simplest way to serve a model, accelerating development time.</p>
    <p class="normal">The client pulls the results directly from data storage, decoupling its interaction with the ML service. Taking this approach, the client never has to wait for the ML service to process its input, but at the same time, it doesn’t have the flexibility to ask for new results at any time. You can see the data storage, where the results are stored as a large cache, from where the client can take what is required. If you want to make your application more responsive, the client can be notified when the processing is complete and can retrieve the results.</p>
    <p class="normal">Unfortunately, this approach will always introduce a delay between the time the predictions were computed and consumed. That’s why not all applications can leverage this design choice. For example, if we implement a recommender system for a video streaming application, having a delay of one day for the predicted movies and TV shows might work because you don’t consume these products at a high frequency. But suppose you make a recommender system for a social media platform. In that case, delaying one day or even one hour is unacceptable, as you constantly want to provide fresh content to the user.</p>
    <p class="normal">Batch transform shines in scenarios where high throughput is needed, like data analytics or periodic reporting. However, it’s unsuitable for real-time applications due to its high latency and requires careful planning and scheduling to manage large datasets effectively. That’s why it is an offline serving method.</p>
    <p class="normal">To conclude, we examined the three most common architectures for serving ML models. We started with online real-time inference, which serves clients when they request a prediction. Then, we looked at the asynchronous inference method, which sits between online and offline. Ultimately, we presented the offline batch transform, which is used to process large amounts of data and store them in data storage, from where the client later consumes them.</p>
    <h1 id="_idParaDest-241" class="heading-1">Monolithic versus microservices architecture in model serving</h1>
    <p class="normal">In the previous section, we saw three different methods of deploying the ML service. The differences in architecture were mainly based on the interaction between the client and the ML service, such as the communication protocol, the ML service responsiveness, and prediction freshness. </p>
    <p class="normal">But another aspect to consider is the architecture of the ML service itself, which can be implemented as a monolithic server or as multiple microservices. This will impact how the ML service is implemented, maintained, and scaled. Let’s explore the two options.</p>
    <figure class="mediaobject"><img src="../Images/B31105_10_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.2: Monolithic versus microservices architecture in model serving</p>
    <h2 id="_idParaDest-242" class="heading-2">Monolithic architecture</h2>
    <p class="normal">The LLM (or any other ML model) and the <a id="_idIndexMarker920"/>associated business logic (preprocessing and post-processing steps) are bundled into a single service in a monolithic architecture. This approach is straightforward to implement at the beginning of a project, as everything is placed within one code base. Simplicity makes maintenance easy when working on small to medium projects, as updates and changes can be made within a unified system.</p>
    <p class="normal">One key challenge of a monolithic architecture is the difficulty of scaling components independently. The LLM typically requires GPU power, while the rest of the business logic is CPU and I/O-bound. As a result, the infrastructure must be optimized for both GPU and CPU. This can lead to inefficient resource use, with the GPU being idle when the business logic is executed and vice versa. Such inefficiency can result in additional costs that could be avoided.</p>
    <p class="normal">Moreover, this <a id="_idIndexMarker921"/>architecture can limit flexibility, as all components must share the same tech stack and runtime environment. For example, you might want to run the LLM using Rust or C++ or compile it with ONNX or TensorRT while keeping the business logic in Python. Having all the code in one system makes this differentiation difficult. Finally, splitting the work across different teams is complex, often leading to bottlenecks and reduced agility.</p>
    <h2 id="_idParaDest-243" class="heading-2">Microservices architecture</h2>
    <p class="normal">A microservices architecture<a id="_idIndexMarker922"/> breaks down the inference pipeline into separate, independent services—typically splitting the LLM service and the business logic into distinct components. These services communicate over a network using protocols such as REST or gRPC.</p>
    <p class="normal">As illustrated in <em class="italic">Figure 10.3</em>, the main advantage of this approach is the ability to scale each component independently. For instance, since the LLM service might require more GPU resources than the business logic, it can be scaled horizontally without impacting the other components. This optimizes resource usage and reduces costs, as different types of machines (e.g., GPU versus CPU) can be used according to each service’s needs.</p>
    <p class="normal">For example, let’s assume that the LLM inference takes longer, so you will need more ML service replicas to meet the demand. But remember that GPU VMs are expensive. By decoupling the two components, you will run only what is required on the GPU machine and not block the GPU VM with other computing that can be done on a much cheaper machine. </p>
    <p class="normal">Thus, by decoupling the components, you can scale horizontally as required, with minimal costs, providing a cost-effective solution to your system’s needs.</p>
    <figure class="mediaobject"><img src="../Images/B31105_10_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.3: Scaling microservices independently based on compute requirements</p>
    <p class="normal">Additionally, each <a id="_idIndexMarker923"/>microservice can adopt the most suitable technology stack, allowing teams to innovate and optimize independently.</p>
    <p class="normal">However, microservices introduce complexity in deployment and maintenance. Each service must be deployed, monitored, and maintained separately, which can be more challenging than managing a monolithic system. </p>
    <p class="normal">The increased network communication between services can also introduce latency and potential points of failure, necessitating robust <a id="_idIndexMarker924"/>monitoring and resilience mechanisms.</p>
    <p class="normal">Note that the proposed design for decoupling the ML model and business logic into two services can be extended if necessary. For example, you can have one service for preprocessing the data, one for the model, and another for post-processing the data. Depending on the four pillars (latency, throughput, data, and infrastructure), you can get creative and design the most optimal architecture for your application needs.</p>
    <h2 id="_idParaDest-244" class="heading-2">Choosing between monolithic and microservices architectures</h2>
    <p class="normal">The choice<a id="_idIndexMarker925"/> between monolithic and microservices architectures for serving ML models largely depends on the application’s specific needs. A monolithic approach might be ideal for smaller teams or more straightforward applications where ease of development and maintenance is a priority. It’s also a good starting point for projects without frequent scaling requirements. Also, if the ML models are smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, the trade-off between reducing costs and complicating your infrastructure is worth considering.</p>
    <p class="normal">On the other hand, microservices, with their adaptability and scalability, are well suited for larger, more complex systems where different components have varying scaling needs or require distinct tech stacks. This architecture is particularly advantageous when scaling specific system parts, such as GPU-intensive LLM services. As LLMs require powerful machines with GPUs, such as Nvidia A100, V100, or A10g, which are incredibly costly, microservices offer the flexibility to optimize the system for keeping these machines busy all the time or quickly scaling down when the GPU is idle. However, this flexibility comes at the cost of increased complexity in both development and operations.</p>
    <p class="normal">A common strategy is to start with a monolithic design and further decouple it into multiple services as the project grows. However, to successfully do so without making the transition too complex and costly, you must design the monolith application with this in mind. For instance, even if all the code runs on a single machine, you can completely decouple the modules of the application at the software level. This makes it easier to move these modules to different microservices when the time comes. When working with Python, for example, you can implement the ML and business logic into two different Python modules that don’t interact with each other. Then, you can glue these two modules at a higher level, such as through a service class, or directly into the framework you use to expose your application over the internet, such as FastAPI. </p>
    <p class="normal">Another option is to write the ML and business logic as two different Python packages that you glue together in the same ways as before. This is better because it completely enforces a separation between the two but adds extra complexity at development time. The main idea, therefore, is that if you start with a monolith and down the line you want to move to a microservices architecture, it’s essential to design your software with modularity in mind. Otherwise, if the logic is mixed, you will probably have to rewrite everything from <a id="_idIndexMarker926"/>scratch, adding tons of development time, which translates into wasted resources.</p>
    <p class="normal">In summary, monolithic architectures offer simplicity and ease of maintenance but at the cost of flexibility and scalability. At the same time, microservices provide the agility to scale and innovate but require more sophisticated management and operational practices.</p>
    <h1 id="_idParaDest-245" class="heading-1">Exploring the LLM Twin’s inference pipeline deployment strategy</h1>
    <p class="normal">Now<a id="_idIndexMarker927"/> that we’ve understood all the design choices available for implementing the deployment strategy of the LLM Twin’s inference pipeline, let’s explore the concrete decisions we made to actualize it.</p>
    <p class="normal">Our primary objective is to develop a chatbot that facilitates content creation. To achieve this, we will process requests sequentially, with a strong emphasis on low latency. This necessitates the selection of an online real-time inference deployment architecture.</p>
    <p class="normal">On the monolith versus microservice aspect, we will split the ML service between a REST API server containing the business logic and an LLM microservice optimized for running the given LLM. As the LLM requires a powerful machine to run the inference, and we can further optimize it with various engines to speed up the latency and memory usage, it makes the most sense to go with the microservice architecture. By doing so, we can quickly adapt the infrastructure based on various LLM sizes. For example, if we run an 8B parameter model, the model can run on a single machine with a Nivida A10G GPU after quantization. But if we want to run a 30B model, we can upgrade to an Nvidia A100 GPU. Doing so allows us to upgrade only the LLM microservice while keeping the REST API untouched.</p>
    <p class="normal">As illustrated in<em class="italic"> Figure 10.4</em>, most business logic is centered around RAG in our particular use case. Thus, we will perform RAG’s retrieval and augmentation parts within the business microservice. It will also include all the advanced RAG techniques presented in the previous chapter to optimize the pre-retrieval, retrieval, and post-retrieval steps. </p>
    <p class="normal">The LLM microservice is strictly optimized for the RAG generation component. Ultimately, the business layer will send the prompt trace consisting of the user query, prompt, answer, and other intermediary steps to the prompt monitoring pipeline, which we will detail in <em class="chapterRef">Chapter 11</em>.</p>
    <p class="normal">In summary, our <a id="_idIndexMarker928"/>approach involves implementing an online real-time ML service using a microservice architecture, which effectively splits the LLM and business logic into two distinct services.</p>
    <figure class="mediaobject"><img src="../Images/B31105_10_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.4: Microservice deployment architecture of the LLM Twin’s inference pipeline</p>
    <p class="normal">Let’s review <a id="_idIndexMarker929"/>the interface of the<a id="_idIndexMarker930"/> inference pipeline, which is defined by the <strong class="keyWord">feature/training/inference</strong> (<strong class="keyWord">FTI</strong>) architecture. For the pipeline to run, it needs two things:</p>
    <ul>
      <li class="bulletList">Real-time features used for RAG, generated by the feature pipeline, which is queried from our online feature store, more concretely from the Qdrant vector database (DB)</li>
      <li class="bulletList">A fine-tuned LLM generated by the training pipeline, which is pulled from our model registry</li>
    </ul>
    <p class="normal">With that in mind, the flow of the ML service looks as follows, as illustrated in <em class="italic">Figure 10.4</em>:</p>
    <ol>
      <li class="numberedList" value="1">A user sends a query through an HTTP request.</li>
      <li class="numberedList">The user’s input retrieves the proper context by leveraging the advanced RAG retrieval module implemented in <em class="italic">Chapter 4</em>.</li>
      <li class="numberedList">The user’s input and retrieved context are packed into the final prompt using a dedicated prompt template.</li>
      <li class="numberedList">The prompt is sent to the LLM microservice through an HTTP request.</li>
      <li class="numberedList">The business microservices wait for the generated answer.</li>
      <li class="numberedList">After the answer is generated, it is sent to the prompt monitoring pipeline along with the user’s input and other vital information to monitor.</li>
      <li class="numberedList">Ultimately, the generated answer is sent back to the user.</li>
    </ol>
    <p class="normal">Now, let’s explore what tech stack we used to implement the architecture presented in <em class="italic">Figure 10.4</em>. As we know, we use Qdrant for the vector DB. We will leverage Hugging Face for the model registry. By doing so, we can publicly share our model with everyone who is testing the code from this book. Thus, you can easily use the model we provided if you don’t want to run the training pipeline, which can cost up to 100 dollars. As you can see, shareability and accessibility are some of the most beautiful aspects of storing your model in a model registry.</p>
    <p class="normal">We will<a id="_idIndexMarker931"/> implement the business microservice in FastAPI because it’s popular, easy to use, and fast. The LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s integration with Hugging Face’s <strong class="keyWord">Deep Learning Containers</strong> (<strong class="keyWord">DLCs</strong>) to deploy the model. We will discuss Hugging Face’s DLCs in the next section, but intuitively, it is an inference engine used to optimize LLMs at serving time. The prompt monitoring pipeline is implemented using Comet, but we will look over that module only in <em class="italic">Chapter 11</em>.</p>
    <p class="normal">The SageMaker Inference deployment<a id="_idIndexMarker932"/> is composed of the following components that we will show you how to implement:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">SageMaker endpoint</strong>: An endpoint<a id="_idIndexMarker933"/> is a scalable and secure API that SageMaker hosts to enable real-time predictions from deployed models. It’s essentially the interface through which applications interact with your model. Once deployed, an application can make HTTP requests to the endpoint to receive real-time predictions.</li>
      <li class="bulletList"><strong class="keyWord">SageMaker model</strong>: In SageMaker, a model<a id="_idIndexMarker934"/> is an artifact that results from training an algorithm. It contains the information required to make predictions, including the weights and computation logic. You can create multiple models and use them in different configurations or for various predictions.</li>
      <li class="bulletList"><strong class="keyWord">SageMaker configuration</strong>: This configuration<a id="_idIndexMarker935"/> specifies the hardware and software set up to host the model. It defines the resources required for the endpoint, such as the type and number of ML compute instances. Endpoint configurations are used when creating or updating an endpoint. They allow for flexibility in the deployment and scalability of the hosted models.</li>
      <li class="bulletList"><strong class="keyWord">SageMaker Inference component</strong>: This is the <a id="_idIndexMarker936"/>last piece of the puzzle that connects the model and configuration to an<strong class="keyWord"> </strong>endpoint. You can deploy multiple models to an endpoint, each with its resource configuration. Once deployed, models are easily accessible via the InvokeEndpoint API in Python.</li>
    </ul>
    <p class="normal">Together, these components create a robust infrastructure for deploying and managing ML models in SageMaker, enabling scalable, secure, and efficient real-time predictions.</p>
    <p class="normal">Other popular cloud platforms offer the exact solutions. For example, you have Azure OpenAI instead of Bedrock and Azure ML instead of SageMaker on Azure. The list of ML deployment tools, such as Hopsworks, Modal, Vertex AI, Seldon, BentoML, and many more, is endless and will probably change. What is essential though is to understand your use case requirements and find a tool that fits your needs.</p>
    <h2 id="_idParaDest-246" class="heading-2">The training versus the inference pipeline</h2>
    <p class="normal">Understanding <a id="_idIndexMarker937"/>the nuances between the training and inference <a id="_idIndexMarker938"/>pipelines is crucial before we deploy the inference pipeline. While it might seem straightforward that the training pipeline is for training and the inference pipeline is for inference, there are significant differences that we need to grasp to comprehend the technical aspects of our discussion fully.</p>
    <p class="normal">One key difference lies in how data is handled and accessed within each pipeline. During training, data is typically accessed from offline storage in batch mode, optimized for throughput and ensuring data lineage. For example, our LLM Twin architecture uses ZenML artifacts to access, version, and track data fed to the training loop in batches. In contrast, the inference pipeline requires an online DB optimized for low latency. We will leverage the Qdrant vector DB to grab the necessary context for RAG. In this context, the focus shifts from data lineage and versioning to quick data access, ensuring a seamless user experience. Additionally, the outputs of these pipelines also differ significantly. The training pipeline outputs trained model weights stored in the model registry. Meanwhile, the inference pipeline outputs predictions served directly to the user.</p>
    <p class="normal">Also, the infrastructure required for each pipeline is different. The training pipeline demands more powerful machines equipped with as many GPUs as possible. This is because training involves batching data and holding all the necessary gradients in memory for optimization steps, making it highly compute-intensive. More computational power and VRAM allow larger batches (or throughput), reducing training time and enabling more extensive experimentation. On the other hand, the inference pipeline typically <a id="_idIndexMarker939"/>requires less computation. Inference<a id="_idIndexMarker940"/> often involves passing a single sample or smaller batches to the model without the need for optimization steps.</p>
    <p class="normal">Despite these differences, there is some overlap between the two pipelines, particularly regarding preprocessing and post-processing steps. Applying the same preprocessing and post-processing functions and hyperparameters during training and inference is crucial. Any discrepancies can lead to what is known as training-serving skew, where the model’s performance during inference deviates from its performance during training.</p>
    <h1 id="_idParaDest-247" class="heading-1">Deploying the LLM Twin service</h1>
    <p class="normal">The last step is implementing the<a id="_idIndexMarker941"/> architecture presented in the previous section. More concretely, we will deploy the LLM microservice using AWS SageMaker and the business microservice using FastAPI. Within the business microservice, we will glue the RAG logic written in <em class="italic">Chapter 9</em> with our fine-tuned LLM Twin, ultimately being able to test out the inference pipeline end to end.</p>
    <p class="normal">Serving the ML model is one of the most critical steps in any ML application’s life cycle, as users can only interact with our model after this phase is completed. If the serving architecture isn’t designed correctly or if the infrastructure isn’t working properly, it doesn’t matter that you have implemented a powerful and excellent model. As long as the user cannot appropriately interact with it, it has near zero value from a business point of view. For example, if you have the best code assistant on the market, but the latency to use it is too high, or the API calls keep crashing, the user will probably switch to a less performant code<a id="_idIndexMarker942"/> assistant that works faster and is more stable.</p>
    <p class="normal">Thus, in this section, we will show you how to:</p>
    <ul>
      <li class="bulletList">Deploy our fined-tuned LLM Twin model to AWS SageMaker</li>
      <li class="bulletList">Write an inference client to interact with the deployed model</li>
      <li class="bulletList">Write the business service in FastAPI</li>
      <li class="bulletList">Integrate our RAG logic with our fine-tuned LLM</li>
      <li class="bulletList">Implement autoscaling rules for the LLM microservice</li>
    </ul>
    <h2 id="_idParaDest-248" class="heading-2">Implementing the LLM microservice using AWS SageMaker</h2>
    <p class="normal">We aim to deploy the LLM Twin model, stored in Hugging Face’s model registry, to Amazon SageMaker as an online real-time inference endpoint. We will leverage Hugging Face’s specialized inference container, known as the Hugging Face LLM<strong class="keyWord"> DLC</strong>, to deploy our LLM.</p>
    <h3 id="_idParaDest-249" class="heading-3">What are Hugging Face’s DLCs?</h3>
    <p class="normal">DLCs are<a id="_idIndexMarker943"/> specialized Docker images that come pre-loaded with essential deep-learning frameworks and libraries, including popular tools like transformers, datasets, and tokenizers from Hugging Face. These containers are designed to simplify the process of training and deploying models by eliminating the need for complex environment setup and optimization. The Hugging Face Inference DLC, in particular, includes a fully integrated serving stack, significantly simplifying the deployment process and reducing the technical expertise needed to serve deep learning models in production.</p>
    <p class="normal">When it comes<a id="_idIndexMarker944"/> to serving models, the DLC is powered by the <strong class="keyWord">Text Generation Inference</strong> (<strong class="keyWord">TGI</strong>) engine, made by Hugging Face: <a href="https://github.com/huggingface/text-generation-inference"><span class="url">https://github.com/huggingface/text-generation-inference</span></a>.</p>
    <p class="normal">TGI is an open-source solution for deploying and serving LLMs. It offers high-performance text generation using tensor parallelism and dynamic batching for the most popular open-source LLMs available on Hugging Face, such as Mistral, Llama, and Falcon. To sum up, the most powerful features the DLC image<a id="_idIndexMarker945"/> provides are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Tensor parallelism</strong>, thus enhancing the computational efficiency of model inference</li>
      <li class="bulletList"><strong class="keyWord">Optimized transformers code for inference</strong>, leveraging flash-attention to maximize performance across the most widely used architectures: <a href="https://github.com/Dao-AILab/flash-attention "><span class="url">https://github.com/Dao-AILab/flash-attention</span></a></li>
      <li class="bulletList"><strong class="keyWord">Quantization with </strong><code class="inlineCode">bitsandbytes</code> that reduces the model size while maintaining performance, making deployments more efficient: <a href="https://github.com/bitsandbytes-foundation/bitsandbytes "><span class="url">https://github.com/bitsandbytes-foundation/bitsandbytes</span></a></li>
      <li class="bulletList"><strong class="keyWord">Continuous batching of incoming requests</strong>, thus improving throughput by dynamically batching requests as they arrive</li>
      <li class="bulletList"><strong class="keyWord">Accelerated weight loading</strong> by utilizing <code class="inlineCode">safetensors</code> for faster model initialization, reducing start-up time: <a href="https://github.com/huggingface/safetensors "><span class="url">https://github.com/huggingface/safetensors</span></a></li>
      <li class="bulletList"><strong class="keyWord">Token streaming</strong> that supports real-time <a id="_idIndexMarker946"/>interactions through <strong class="keyWord">Server-Sent Events</strong> (<strong class="keyWord">SSE</strong>)</li>
    </ul>
    <p class="normal">To summarize, our LLM Twin model will run inside DLC Docker images, listening to requests, optimizing the LLM for inference, and serving the results in real time. The DLC’s Docker images will be hosted on AWS SageMaker under inference endpoints that can be accessed through HTTP requests. With that in mind, let’s move on to the implementation. We will start by deploying the LLM and then writing a wrapper class to interact with the SageMaker Inference endpoint.</p>
    <h3 id="_idParaDest-250" class="heading-3">Configuring SageMaker roles</h3>
    <p class="normal">The first step is to create the proper<a id="_idIndexMarker947"/> AWS <strong class="keyWord">Identity and Access Management</strong> (<strong class="keyWord">IAM</strong>) users and roles to access and deploy the SageMaker infrastructure. AWS IAM controls who can authenticate and what any actor has access to. You can create new users (assigned to people) and new roles (assigned to other actors within your infrastructure, such as EC2 VMs) through IAM.</p>
    <p class="normal">The whole deployment process is automated. We will have to run a few CLI commands, but first, ensure that you have correctly configured the <code class="inlineCode">AWS_ACCESS_KEY</code>, <code class="inlineCode">AWS_SECRET_KEY</code>, and <code class="inlineCode">AWS_REGION</code> environmental variables in the <code class="inlineCode">.env</code> file. At this step, the easiest way is to use the credentials attached to an admin role as, in the following steps, we will create a set of narrower IAM roles used in the rest of the chapter.</p>
    <p class="normal">After you configured your <code class="inlineCode">.env</code> file, we have to:</p>
    <ol>
      <li class="numberedList" value="1">Create an IAM user restricted to creating and deleting only the resources we need for the deployment, such as SageMaker itself, <strong class="keyWord">Elastic Container Registry</strong> (<strong class="keyWord">ECR</strong>), and S3. To make it, run the following:
        <pre class="programlisting con-one"><code class="hljs-con">poetry poe create-sagemaker-role
</code></pre>
      </li>
    </ol>
    <p class="normal-one">This command will generate a JSON file called <code class="inlineCode">sagemaker_user_credentials.json</code> that contains a new AWS access and secret key. From now on, we will use these credentials to deploy everything related to SageMaker to ensure we modify only the resources associated with SageMaker. Otherwise, we could accidentally modify other AWS resources using an admin account, resulting in additional costs or altering other existing projects. Thus, having a narrow role only to your use case is good practice. </p>
    <p class="normal-one">The <a id="_idIndexMarker948"/>last step is to take the new credentials from the JSON file and update the <code class="inlineCode">AWS_ACCESS_KEY</code> and <code class="inlineCode">AWS_SECRET_KEY</code> variables in your <code class="inlineCode">.env</code> file. You can check out the implementation at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_sagemaker_role.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_sagemaker_role.py</span></a>.</p>
    <ol>
      <li class="numberedList" value="2">Create an IAM execution role. We will attach this role to the SageMaker deployment, empowering it to access other AWS resources on our behalf. This is standard practice for cloud deployments, as instead of authenticating every machine within your credentials, you attach a role that allows them to access only what is necessary from your infrastructure. In our case, we will provide SageMaker access to AWS S3, CloudWatch, and ECR. To create the role, run the following:
        <pre class="programlisting con-one"><code class="hljs-con">poetry poe create-sagemaker-execution-role
</code></pre>
      </li>
    </ol>
    <p class="normal-one">This command will generate a JSON file called <code class="inlineCode">sagemaker_execution_role.json</code> that contains the <strong class="keyWord">Amazon Resource Name</strong> (<strong class="keyWord">ARN</strong>) of the <a id="_idIndexMarker949"/>newly created role. The ARN is an ID attached to any AWS resource to identify it across your cloud infrastructure. Take the ARN value from the JSON file and update the <code class="inlineCode">AWS_ARN_ROLE</code> variable<a id="_idIndexMarker950"/> from your <code class="inlineCode">.env</code> file with it. You can check out the implementation at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_execution_role.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_execution_role.py</span></a>.</p>
    <div class="note">
      <p class="normal">If you have issues, configure the AWS CLI with the same AWS credentials as in the <code class="inlineCode">.env</code> file and repeat the process. Official documentation for installing the AWS CLI: <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html"><span class="url">https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html</span></a>.</p>
    </div>
    <p class="normal">By setting the IAM user and role in your <code class="inlineCode">.env</code> file, we will automatically load them in the settings Python object and use them throughout the following steps. Now, let’s move on to the actual deployment.</p>
    <h3 id="_idParaDest-251" class="heading-3">Deploying the LLM Twin model to AWS SageMaker</h3>
    <p class="normal">The<a id="_idIndexMarker951"/> deployment of AWS SageMaker is fully<a id="_idIndexMarker952"/> automated through a set of Python classes, which we will cover in this chapter. This section aims to understand how we configure the SageMaker infrastructure directly from Python. Thus, you don’t have to run everything step by step, as in a standard tutorial, but only to understand the code.</p>
    <p class="normal">We can initiate and finalize the entire SageMaker deployment using a simple CLI command: <code class="inlineCode">poe deploy-inference-endpoint</code>. This command will initialize all the steps presented in <em class="italic">Figure 10.5</em>, except for creating the SageMaker AWS IAMs we created and configured in the previous step. </p>
    <p class="normal">In this section, we will walk you through the code presented in <em class="italic">Figure 10.5</em> that helps us fully automate the deployment process, starting with the <code class="inlineCode">create_endpoint()</code> function. Ultimately, we will test the CLI command and check the AWS console to see whether the deployment was successful. The SageMaker deployment code is available at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy</span></a>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_10_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.5: AWS SageMaker deployment steps</p>
    <p class="normal">We will take a <a id="_idIndexMarker953"/>top-down approach to walk<a id="_idIndexMarker954"/> you through the implementation, starting with the main function that deploys the LLM Twin model to AWS SageMaker. In the function below, we first take the latest version of the Docker DLC image using the <code class="inlineCode">get_huggingface_llm_image_uri()</code> function, which is later passed to the deployment strategy class, along with an instance of the resource manager and deployment service:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">create_endpoint</span>(<span class="hljs-params">endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED</span>):
    llm_image = get_huggingface_llm_image_uri(<span class="hljs-string">"huggingface"</span>, version=<span class="hljs-literal">None</span>)
    resource_manager = ResourceManager()
    deployment_service = DeploymentService(resource_manager=resource_manager)
    SagemakerHuggingfaceStrategy(deployment_service).deploy(
        role_arn=settings.ARN_ROLE,
        llm_image=llm_image,
        config=hugging_face_deploy_config,
        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE,
        endpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_INFERENCE,
        gpu_instance_type=settings.GPU_INSTANCE_TYPE,
        resources=model_resource_config,
        endpoint_type=endpoint_type,
    )
</code></pre>
    <p class="normal">We must <a id="_idIndexMarker955"/>review the three classes used <a id="_idIndexMarker956"/>in the <code class="inlineCode">create_endpoint()</code> function to fully understand the deployment process. Let’s start with the <code class="inlineCode">ResourceManager</code> class. The class begins with the initialization method, establishing the connection to AWS SageMaker using boto3, the AWS SDK for Python, which provides the necessary functions to interact with various AWS services, including SageMaker.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">ResourceManager</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-variable">self</span>.sagemaker_client = boto3.client(
            <span class="hljs-string">"sagemaker"</span>,
            region_name=settings.AWS_REGION,
            aws_access_key_id=settings.AWS_ACCESS_KEY,
            aws_secret_access_key=settings.AWS_SECRET_KEY,
        )
</code></pre>
    <p class="normal">Next, we implement the <code class="inlineCode">endpoint_config_exists</code> method, checking whether a specific SageMaker endpoint configuration exists:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">endpoint_config_exists</span>(<span class="hljs-params">self, endpoint_config_name: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-built_in">bool</span>:
        <span class="hljs-keyword">try</span>:
            <span class="hljs-variable">self</span>.sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)
            logger.info(<span class="hljs-string">f"Endpoint configuration '</span><span class="hljs-subst">{endpoint_config_name}</span><span class="hljs-string">' exists."</span>)
            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
        <span class="hljs-keyword">except</span> ClientError:
            logger.info(<span class="hljs-string">f"Endpoint configuration '</span><span class="hljs-subst">{endpoint_config_name}</span><span class="hljs-string">' does not exist."</span>)
            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
</code></pre>
    <p class="normal">The class also includes the <code class="inlineCode">endpoint_exists</code> method, which checks the existence of a specific SageMaker endpoint:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">endpoint_exists</span>(<span class="hljs-params">self, endpoint_name: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-built_in">bool</span>:
        <span class="hljs-keyword">try</span>:
            <span class="hljs-variable">self</span>.sagemaker_client.describe_endpoint(EndpointName=endpoint_name)
            logger.info(<span class="hljs-string">f"Endpoint '</span><span class="hljs-subst">{endpoint_name}</span><span class="hljs-string">' exists."</span>)
            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
        <span class="hljs-keyword">except</span> <span class="hljs-variable">self</span>.sagemaker_client.exceptions.ResourceNotFoundException:
            logger.info(<span class="hljs-string">f"Endpoint '</span><span class="hljs-subst">{endpoint_name}</span><span class="hljs-string">' does not exist."</span>)
            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
</code></pre>
    <p class="normal">Let’s move to <a id="_idIndexMarker957"/>the <code class="inlineCode">DeploymentService</code>. Within <a id="_idIndexMarker958"/>the constructor, we set up the <code class="inlineCode">sagemaker_client</code>, which will interface with AWS SageMaker and an instance of the <code class="inlineCode">ResourceManager</code> class we talked about earlier:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">DeploymentService</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, resource_manager</span>):
        <span class="hljs-variable">self</span>.sagemaker_client = boto3.client(
            <span class="hljs-string">"sagemaker"</span>,
            region_name=settings.AWS_REGION,
            aws_access_key_id=settings.AWS_ACCESS_KEY,
            aws_secret_access_key=settings.AWS_SECRET_KEY,
        )
        <span class="hljs-variable">self</span>.resource_manager = resource_manager
</code></pre>
    <p class="normal">The <code class="inlineCode">deploy()</code> method<a id="_idIndexMarker959"/> is the heart of the <code class="inlineCode">DeploymentService</code> class. This method orchestrates the entire process of deploying a model to a <a id="_idIndexMarker960"/>SageMaker endpoint. It checks whether the necessary configurations are already in place and, if not, it triggers the deployment:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">deploy</span>(
<span class="hljs-params">    self,</span>
<span class="hljs-params">    role_arn: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    llm_image: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    config: </span><span class="hljs-built_in">dict</span><span class="hljs-params">,</span>
<span class="hljs-params">    endpoint_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    endpoint_config_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    gpu_instance_type: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    resources: </span><span class="hljs-type">Optional</span><span class="hljs-params">[</span><span class="hljs-built_in">dict</span><span class="hljs-params">] = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,</span>
) -&gt; <span class="hljs-literal">None</span>:
    <span class="hljs-keyword">try</span>:
        <span class="hljs-keyword">if</span> <span class="hljs-variable">self</span>.resource_manager.endpoint_config_exists(endpoint_config_name=endpoint_config_name):
            logger.info(<span class="hljs-string">f"Endpoint configuration </span><span class="hljs-subst">{endpoint_config_name}</span><span class="hljs-string"> exists. Using existing configuration..."</span>)
        <span class="hljs-keyword">else</span>:
            logger.info(<span class="hljs-string">f"Endpoint configuration</span><span class="hljs-subst">{endpoint_config_name}</span><span class="hljs-string"> does not exist."</span>)
        <span class="hljs-variable">self</span>.prepare_and_deploy_model(
            role_arn=role_arn,
            llm_image=llm_image,
            config=config,
            endpoint_name=endpoint_name,
            update_endpoint=<span class="hljs-literal">False</span>,
            resources=resources,
            endpoint_type=endpoint_type,
            gpu_instance_type=gpu_instance_type,
        )
        logger.info(<span class="hljs-string">f"Successfully deployed/updated model to endpoint </span><span class="hljs-subst">{endpoint_name}</span><span class="hljs-string">."</span>)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        logger.error(<span class="hljs-string">f"Failed to deploy model to SageMaker: </span><span class="hljs-subst">{e}</span><span class="hljs-string">"</span>)
        <span class="hljs-keyword">raise</span>
</code></pre>
    <p class="normal">The deploy <a id="_idIndexMarker961"/>method begins by checking whether<a id="_idIndexMarker962"/> the endpoint configuration already exists using the <code class="inlineCode">resource_manager</code>. This step is crucial because it avoids unnecessary redeployment if the configuration is already set up. The deployment itself is handled by calling the <code class="inlineCode">prepare_and_deploy_model()</code> method, which is responsible for the actual deployment of the model to the specified SageMaker endpoint.</p>
    <p class="normal">The <code class="inlineCode">prepare_and_deploy_model()</code> method is a static method within the <code class="inlineCode">DeploymentService</code> class. This method is focused on setting up and deploying the Hugging Face model to SageMaker:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@staticmethod</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">prepare_and_deploy_model</span>(
<span class="hljs-params">    role_arn: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    llm_image: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    config: </span><span class="hljs-built_in">dict</span><span class="hljs-params">,</span>
<span class="hljs-params">    endpoint_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    update_endpoint: </span><span class="hljs-built_in">bool</span><span class="hljs-params">,</span>
<span class="hljs-params">    gpu_instance_type: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    resources: </span><span class="hljs-type">Optional</span><span class="hljs-params">[</span><span class="hljs-built_in">dict</span><span class="hljs-params">] = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,</span>
) -&gt; <span class="hljs-literal">None</span>:
    huggingface_model = HuggingFaceModel(
        role=role_arn,
        image_uri=llm_image,
        env=config,
        transformers_version=<span class="hljs-string">"4.6"</span>,
        pytorch_version=<span class="hljs-string">"1.13"</span>,
        py_version=<span class="hljs-string">"py310"</span>,
    )
    huggingface_model.deploy(
        instance_type=gpu_instance_type,
        initial_instance_count=<span class="hljs-number">1</span>,
        endpoint_name=endpoint_name,
        update_endpoint=update_endpoint,
        resources=resources,
        tags=[{<span class="hljs-string">"Key"</span>: <span class="hljs-string">"task"</span>, <span class="hljs-string">"Value"</span>: <span class="hljs-string">"model_task"</span>}],
        endpoint_type=endpoint_type,
    )
</code></pre>
    <p class="normal">This method begins by creating an instance of HuggingFaceModel, a specialized model class from SageMaker designed to handle Hugging Face models. The constructor for HuggingFaceModel takes several essential parameters, such as the role ARN (which gives SageMaker the necessary permissions), the URI of the LLM DLC Docker image, and the LLM configuration that specifies what LLM to load from Hugging Face and its inference parameters, such as the maximum total of tokens.</p>
    <p class="normal">Once HuggingFaceModel is instantiated, the method deploys it to SageMaker using the deploy function. This deployment process involves specifying the type of instance used, the number of instances, and whether to update an existing endpoint or create a new one. The method also includes optional resources for more complex deployments, such as the <code class="inlineCode">initial_instance_count</code> parameter for multi-model endpoints and tags for tracking and categorization.</p>
    <p class="normal">The last step is to walk you through the <code class="inlineCode">SagemakerHuggingfaceStrategy</code> class, which aggregates everything we have shown. The class is initialized only with an instance of a deployment service, such as the one shown above.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">SagemakerHuggingfaceStrategy</span>(<span class="hljs-title">DeploymentStrategy</span>):
<span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, deployment_service</span>):
    <span class="hljs-variable">self</span>.deployment_service = deployment_service
</code></pre>
    <p class="normal">The core <a id="_idIndexMarker963"/>functionality of the <code class="inlineCode">SagemakerHuggingfaceStrategy</code> class is encapsulated in its <code class="inlineCode">deploy()</code> method. This method <a id="_idIndexMarker964"/>orchestrates the deployment process, taking various parameters that define how the Hugging Face model should be deployed to AWS SageMaker:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">deploy</span>(
<span class="hljs-params">    self,</span>
<span class="hljs-params">    role_arn: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    llm_image: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    config: </span><span class="hljs-built_in">dict</span><span class="hljs-params">,</span>
<span class="hljs-params">    endpoint_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    endpoint_config_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    gpu_instance_type: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">    resources: </span><span class="hljs-type">Optional</span><span class="hljs-params">[</span><span class="hljs-built_in">dict</span><span class="hljs-params">] = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,</span>
) -&gt; <span class="hljs-literal">None</span>:
    logger.info(<span class="hljs-string">"Starting deployment using Sagemaker Huggingface Strategy..."</span>)
    logger.info(
        <span class="hljs-string">f"Deployment parameters: nb of replicas: </span><span class="hljs-subst">{settings.COPIES}</span><span class="hljs-string">, nb of gpus:</span><span class="hljs-subst">{settings.GPUS}</span><span class="hljs-string">, instance_type:</span><span class="hljs-subst">{settings.GPU_INSTANCE_TYPE}</span><span class="hljs-string">"</span>
    )
</code></pre>
    <p class="normal">The parameters<a id="_idIndexMarker965"/> passed into the method are crucial <a id="_idIndexMarker966"/>to the deployment process:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">role_arn</code>: The AWS IAM role that provides permissions for the SageMaker deployment.</li>
      <li class="bulletList"><code class="inlineCode">llm_image</code>: The URI of the DLC Docker image</li>
      <li class="bulletList"><code class="inlineCode">config</code>: A dictionary containing configuration settings for the model environment.</li>
      <li class="bulletList"><code class="inlineCode">endpoint_name</code> and <code class="inlineCode">endpoint_config_name</code>: Names for the SageMaker endpoint and its configuration, respectively.</li>
      <li class="bulletList"><code class="inlineCode">gpu_instance_type</code>: The type of the GPU EC2 instances used for the deployment.</li>
      <li class="bulletList"><code class="inlineCode">resources</code>: Optional resources dictionary used for multi-model endpoint deployments.</li>
      <li class="bulletList"><code class="inlineCode">endpoint_type</code>: This can either be <code class="inlineCode">MODEL_BASED</code> or <code class="inlineCode">INFERENCE_COMPONENT</code>, determining whether the endpoint includes an inference component.</li>
    </ul>
    <p class="normal">The method delegates the actual deployment process to the <code class="inlineCode">deployment_service</code>. This delegation is a critical aspect of the strategy pattern, allowing for flexibility in how the deployment is carried out without altering the high-level deployment logic.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">try</span>:
    <span class="hljs-variable">self</span>.deployment_service.deploy(
        role_arn=role_arn,
        llm_image=llm_image,
        config=config,
        endpoint_name=endpoint_name,
        endpoint_config_name=endpoint_config_name,
        gpu_instance_type=gpu_instance_type,
        resources=resources,
        endpoint_type=endpoint_type,
    )
    logger.info(<span class="hljs-string">"Deployment completed successfully."</span>)
<span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
    logger.error(<span class="hljs-string">f"Error during deployment: </span><span class="hljs-subst">{e}</span><span class="hljs-string">"</span>)
    <span class="hljs-keyword">raise</span>
</code></pre>
    <p class="normal">Also, let’s review the resource configuration to understand the infrastructure better. These resources are leveraged when setting up multi-endpoint configurations that use multiple replicas to serve clients while respecting the latency and throughput requirements of the application. The <code class="inlineCode">ResourceRequirements</code> object is initialized with a dictionary that<a id="_idIndexMarker967"/> specifies various resource<a id="_idIndexMarker968"/> parameters. These parameters include the number of replicas (copies) of the model to be deployed, the number of GPUs required, the number of CPU cores, and the memory allocation in megabytes. Each of these parameters plays a crucial role in the performance and scalability of the deployed model.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sagemaker.compute_resource_requirements.resource_requirements <span class="hljs-keyword">import</span> ResourceRequirements
    model_resource_config = ResourceRequirements(
    requests={
        <span class="hljs-string">"copies"</span>: settings.COPIES,
        <span class="hljs-string">"num_accelerators"</span>: settings.GPUS
        <span class="hljs-string">"num_cpus"</span>: settings.CPUS,
        <span class="hljs-string">"memory"</span>: <span class="hljs-number">5</span> * <span class="hljs-number">1024</span>
    },
)
</code></pre>
    <p class="normal">In the preceding snippet, <code class="inlineCode">ResourceRequirements</code> is configured with four key parameters:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">copies</strong>: This parameter determines how many instances or replicas of the model should be deployed. Having multiple replicas can help in reducing latency and increasing throughput.</li>
      <li class="bulletList"><strong class="keyWord">num_accelerators</strong>: This parameter specifies the number of GPUs to allocate. Since LLMs are computationally intensive, multiple GPUs are typically required to accelerate inference processes.</li>
      <li class="bulletList"><strong class="keyWord">num_cpus:</strong> This defines the number of CPU cores the deployment should have. The number of CPUs impacts the model’s ability to handle data preprocessing, post-processing, and other tasks that are less GPU-dependent but still essential.</li>
      <li class="bulletList"><strong class="keyWord">memory</strong>: The memory parameter sets the minimum amount of RAM required for the deployment. Adequate memory is necessary to ensure the model can load and operate without running into memory shortages.</li>
    </ul>
    <p class="normal">By setting these parameters, the class ensures that it has sufficient resources to operate efficiently when the model is deployed to a SageMaker endpoint. The precise tuning of these values will vary depending on the LLM’s specific requirements, such as its size, the complexity of the tasks it will perform, and the expected load. To get a better understanding of how to use them, after deploying the endpoint, we suggest modifying them and seeing how the performance of the LLM microservice changes.</p>
    <p class="normal">Ultimately, let’s review <a id="_idIndexMarker969"/>the settings configuring the <a id="_idIndexMarker970"/>LLM engine. The <code class="inlineCode">HF_MODEL_ID</code> identifies which Hugging Face model to deploy. For example, in the settings class, we set it to <code class="inlineCode">mlabonne/TwinLlama-3.1-8B-13</code> to load our custom LLM Twin model stored in Hugging Face. <code class="inlineCode">SM_NUM_GPUS</code> specifies the number of GPUs allocated per model replica, which is crucial for fitting your model into the GPU’s VRAM. <code class="inlineCode">HUGGING_FACE_HUB_TOKEN</code> provides access to the Hugging Face Hub for model retrieval. <code class="inlineCode">HF_MODEL_QUANTIZE</code> specifies what quantization technique to use, while the rest of the variables control the LLM token generation process.</p>
    <pre class="programlisting code"><code class="hljs-code">hugging_face_deploy_config = {
    <span class="hljs-string">"HF_MODEL_ID"</span>: settings.HF_MODEL_ID,
    <span class="hljs-string">"SM_NUM_GPUS"</span>: json.dumps(settings.SM_NUM_GPUS),  <span class="hljs-comment"># Number of GPU used per replica</span>
    <span class="hljs-string">"MAX_INPUT_LENGTH"</span>: json.dumps(settings.MAX_INPUT_LENGTH),  <span class="hljs-comment"># Max length of input text</span>
    <span class="hljs-string">"</span><span class="hljs-string">MAX_TOTAL_TOKENS"</span>: json.dumps(settings.MAX_TOTAL_TOKENS),  <span class="hljs-comment"># Max length of the generation (including input text)</span>
    <span class="hljs-string">"MAX_BATCH_TOTAL_TOKENS"</span>: json.dumps(settings.MAX_BATCH_TOTAL_TOKENS),
    <span class="hljs-string">"HUGGING_FACE_HUB_TOKEN"</span>: settings.HUGGINGFACE_ACCESS_TOKEN,
    <span class="hljs-string">"MAX_BATCH_PREFILL_TOKENS"</span>: <span class="hljs-string">"10000"</span>,
    "HF_MODEL_QUANTIZE": <span class="hljs-string">"bitsandbytes"</span>,
}
</code></pre>
    <p class="normal">Using these two configurations, we fully control our infrastructure, what LLM to use, and how it behaves. To start the SageMaker deployment with the configuration shown above, call the <code class="inlineCode">create_endpoint()</code> function (presented at the beginning of the section) as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">create_endpoint(endpoint_type=EndpointType.MODEL_BASED)
</code></pre>
    <p class="normal">For convenience, we also wrapped it up under a <code class="inlineCode">poe</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe deploy-inference-endpoint
</code></pre>
    <p class="normal">That’s all you <a id="_idIndexMarker971"/>need to deploy an inference pipeline to<a id="_idIndexMarker972"/> AWS SageMaker. The hardest part is finding the correct configuration to fit your needs while reducing your infrastructure’s costs. Depending on AWS, this will take up to 15-30 minutes to deploy. You can always change any value directly from your <code class="inlineCode">.env</code> file and deploy the model with a different configuration without touching the code. For example, our default values use a single GPU instance of type <code class="inlineCode">ml.g5.xlargeGPU</code>. If you want more replicas, you can tweak the <code class="inlineCode">GPUS</code> and <code class="inlineCode">SM_NUM_GPUS</code> settings or change your instance type by changing the <code class="inlineCode">GPU_INSTANCE_TYPE</code> variable.</p>
    <div class="note">
      <p class="normal">Before deploying the LLM microservice to AWS SageMaker, ensure that you’ve generated a user role by running <code class="inlineCode">poetry poe create-sagemaker-role</code> and an execution role by running <code class="inlineCode">poetry poe create-sagemaker-execution-role</code>. Also, ensure you update your <code class="inlineCode">AWS_*</code> environment variables in your <code class="inlineCode">.env</code> file with the credentials generated by the two steps. You can find more details on this aspect in the repository’s README file.</p>
    </div>
    <p class="normal">After deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker dashboard in AWS to visualize it. First, in the left panel, click on <strong class="keyWord">SageMaker dashboard</strong>, and then in the <strong class="keyWord">Inference</strong> column, click on the <strong class="keyWord">Endpoints</strong> button, as illustrated in <em class="italic">Figure 10.6</em>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_10_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.6: AWS SageMaker Inference endpoints example</p>
    <p class="normal">After clicking the <strong class="screenText">Endpoints</strong> button, you will see your <strong class="keyWord">twin</strong> endpoint in a <strong class="keyWord">Creating</strong> or <strong class="keyWord">Created </strong>status, as seen in <em class="italic">Figure 10.7</em>. After clicking on it, you can look at the endpoint’s logs in CloudWatch and<a id="_idIndexMarker973"/> monitor the CPU, memory, disk, and<a id="_idIndexMarker974"/> GPU utilization. </p>
    <p class="normal">Also, they provide an excellent way to monitor all the HTTP errors, such as <code class="inlineCode">4XX</code> and <code class="inlineCode">5XX</code>, in one place.</p>
    <figure class="mediaobject"><img src="../Images/B31105_10_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.7: AWS SageMaker twin inference endpoint example</p>
    <h3 id="_idParaDest-252" class="heading-3">Calling the AWS SageMaker Inference endpoint</h3>
    <p class="normal">Now that our LLM service has <a id="_idIndexMarker975"/>been deployed on AWS SageMaker, let’s learn how to call the service. To do so, we will write two classes that will help us prepare the prompt for SageMaker, call the inference endpoint through HTTP requests, and decode the results in a way the client can work with. All the AWS SageMaker Inference code is available on GitHub at <code class="inlineCode">llm_engineering/model/inference</code>. It all starts with the following example:</p>
    <pre class="programlisting code"><code class="hljs-code">text = <span class="hljs-string">"Write me a post about AWS SageMaker inference endpoints."</span>
llm = LLMInferenceSagemakerEndpoint(
        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE
    )
Answer = InferenceExecutor(llm, text).execute()
</code></pre>
    <p class="normal">As before, we will walk you through the <code class="inlineCode">LLMInferenceSagemakerEndpoint</code> and <code class="inlineCode">InferenceExecutor</code> classes. Let’s start with the <code class="inlineCode">LLMInferenceSagemakerEndpoint</code> class, which directly interacts with SageMaker. The constructor initializes all the essential attributes necessary to interact with the SageMaker endpoint:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">LLMInferenceSagemakerEndpoint</span>(<span class="hljs-title">Inference</span>):
<span class="hljs-keyword">    def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-keyword">    </span><span class="hljs-params">    self,</span>
<span class="hljs-params"> </span><span class="hljs-keyword">    </span><span class="hljs-params">   endpoint_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params"> </span><span class="hljs-keyword">    </span><span class="hljs-params">   default_payload: </span><span class="hljs-type">Optional</span><span class="hljs-params">[</span><span class="hljs-type">Dict</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">, </span><span class="hljs-type">Any</span><span class="hljs-params">]] = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-keyword">    </span><span class="hljs-params">    inference_component_name: </span><span class="hljs-type">Optional</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">] = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-keyword">    </span>) -&gt; <span class="hljs-literal">None</span>:
 <span class="hljs-keyword">    </span>   <span class="hljs-built_in">super</span>().__init__()
   <span class="hljs-keyword">    </span> <span class="hljs-variable">self</span>.client = boto3.client(
   <span class="hljs-keyword">    </span>     <span class="hljs-string">"sagemaker-runtime"</span>,
      <span class="hljs-keyword">    </span>  region_name=settings.AWS_REGION,
    <span class="hljs-keyword">    </span>    aws_access_key_id=settings.AWS_ACCESS_KEY,
   <span class="hljs-keyword">    </span>     aws_secret_access_key=settings.AWS_SECRET_KEY,
  <span class="hljs-keyword">    </span>  )
   <span class="hljs-keyword">    </span> <span class="hljs-variable">self</span>.endpoint_name = endpoint_name
   <span class="hljs-keyword">    </span> <span class="hljs-variable">self</span>.payload = default_payload <span class="hljs-keyword">if</span> default_payload <span class="hljs-keyword">else</span> <span class="hljs-variable">self</span>._default_payload()
 <span class="hljs-keyword">    </span>   <span class="hljs-variable">self</span>.inference_component_name = inference_component_name
</code></pre>
    <p class="normal"><code class="inlineCode">endpoint_name</code> is crucial for identifying the SageMaker endpoint we want to request. Additionally, the method initializes the payload using a provided value or by calling a method that generates a default payload if none is provided.</p>
    <p class="normal">One of the key features of the class is its ability to generate a default payload for inference requests. This<a id="_idIndexMarker976"/> is handled by the <code class="inlineCode">_default_payload()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">_default_payload</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">"inputs"</span>: <span class="hljs-string">""</span>,
        <span class="hljs-string">"parameters"</span>: {
            <span class="hljs-string">"max_new_tokens"</span>: settings.MAX_NEW_TOKENS_INFERENCE,
            <span class="hljs-string">"top_p"</span>: settings.TOP_P_INFERENCE,
            <span class="hljs-string">"temperature"</span>: settings.TEMPERATURE_INFERENCE,
            <span class="hljs-string">"return_full_text"</span>: <span class="hljs-literal">False</span>,
        },
    }
</code></pre>
    <p class="normal">This method returns a dictionary that represents the default structure of the payload to be sent for inference. The parameters section includes settings that influence the model’s behavior during inference, such as the number of tokens to generate, the sampling strategy (<code class="inlineCode">top_p</code>), and the temperature setting, which controls randomness in the output. These parameters are fetched from the application’s settings, ensuring consistency across different inference tasks.</p>
    <p class="normal">The class allows customization of the payload through the <code class="inlineCode">set_payload()</code> method, which enables the user to modify the inputs and parameters before sending an inference request:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">set_payload</span>(<span class="hljs-params">self, inputs: </span><span class="hljs-built_in">str</span><span class="hljs-params">, parameters: </span><span class="hljs-type">Optional</span><span class="hljs-params">[</span><span class="hljs-type">Dict</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">, </span><span class="hljs-type">Any</span><span class="hljs-params">]] = </span><span class="hljs-literal">None</span>) -&gt; <span class="hljs-literal">None</span>:
    <span class="hljs-variable">self</span>.payload[<span class="hljs-string">"inputs"</span>] = inputs
    <span class="hljs-keyword">if</span> parameters:
        <span class="hljs-variable">self</span>.payload[<span class="hljs-string">"parameters"</span>].update(parameters)
</code></pre>
    <p class="normal">This method updates the <code class="inlineCode">inputs</code> field of the payload with the new input text provided by the user. Additionally, it allows for modifying inference parameters if any are provided.</p>
    <p class="normal">Ultimately, we leverage<a id="_idIndexMarker977"/> the <code class="inlineCode">inference()</code> method to call the SageMaker endpoint with the customized payload:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">inference</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
    <span class="hljs-keyword">try</span>:
        logger.info(<span class="hljs-string">"Inference request sent."</span>)
        invoke_args = {
            <span class="hljs-string">"EndpointName"</span>: <span class="hljs-variable">self</span>.endpoint_name,
            <span class="hljs-string">"ContentType"</span>: <span class="hljs-string">"application/json"</span>,
            <span class="hljs-string">"Body"</span>: json.dumps(<span class="hljs-variable">self</span>.payload),
        }
        <span class="hljs-keyword">if</span> <span class="hljs-variable">self</span>.inference_component_name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">"None"</span>, <span class="hljs-literal">None</span>]:
            invoke_args[<span class="hljs-string">"InferenceComponentName"</span>] = <span class="hljs-variable">self</span>.inference_component_name
        response = <span class="hljs-variable">self</span>.client.invoke_endpoint(**invoke_args)
        response_body = response[<span class="hljs-string">"Body"</span>].read().decode(<span class="hljs-string">"utf8"</span>)
        <span class="hljs-keyword">return</span> json.loads(response_body)
    <span class="hljs-keyword">except</span> Exception:
        logger.exception(<span class="hljs-string">"SageMaker inference failed."</span>)
        <span class="hljs-keyword">raise</span>
</code></pre>
    <p class="normal">In this method, the inference method constructs the request to be sent to the SageMaker endpoint. The method packages the payload and other necessary details into a format SageMaker expects. If an <code class="inlineCode">inference_component_name</code> is specified, it is included in the request, allowing for more granular control over the inference process if needed. The request is sent using the <code class="inlineCode">invoke_endpoint()</code> function, and the response is read, decoded, and returned as a JSON object.</p>
    <p class="normal">Let’s understand how the <code class="inlineCode">InferenceExecutor</code> uses the <code class="inlineCode">LLMInferenceSagemakerEndpoint</code> class we previously presented to send HTTP requests to the AWS SageMaker endpoint.</p>
    <p class="normal">The <code class="inlineCode">InferenceExecutor</code> class<a id="_idIndexMarker978"/> begins with the constructor, which inputs key parameters for calling the LLM. The <code class="inlineCode">llm</code> parameter accepts any instance that implements the Inference interface, such as the <code class="inlineCode">LLMInferenceSagemakerEndpoint</code> class, which is used to perform the inference. </p>
    <p class="normal">Also, it accepts the query parameter, which represents the user input. Ultimately, it takes an optional context field if you want to do RAG, and you can customize the prompt template. If no prompt template is provided, it will default to a generic version that is not specialized in any LLM:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">InferenceExecutor</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-params">  </span>    <span class="hljs-params">  self,</span>
<span class="hljs-params">   </span>    <span class="hljs-params"> llm: Inference,</span>
<span class="hljs-params">   </span>    <span class="hljs-params"> query: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">  </span>    <span class="hljs-params">  context: </span><span class="hljs-built_in">str</span><span class="hljs-params"> | </span><span class="hljs-literal">None</span><span class="hljs-params"> = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">   </span>    <span class="hljs-params"> prompt: </span><span class="hljs-built_in">str</span><span class="hljs-params"> | </span><span class="hljs-literal">None</span><span class="hljs-params"> = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
    ) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-variable">self</span>.llm = llm
        <span class="hljs-variable">self</span>.query = query
        <span class="hljs-variable">self</span>.context = context <span class="hljs-keyword">if</span> context <span class="hljs-keyword">else</span> <span class="hljs-string">""</span>
        <span class="hljs-keyword">if</span> prompt <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-variable">self</span>.prompt = <span class="hljs-string">"""</span>
    <span class="hljs-string">You are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.</span>
<span class="hljs-string">User query: {query}</span>
<span class="hljs-string">Context: {context}</span>
<span class="hljs-string">   </span>    <span class="hljs-string">     """</span>
        <span class="hljs-keyword">else</span>:
            <span class="hljs-variable">self</span>.prompt = prompt
</code></pre>
    <p class="normal">The <code class="inlineCode">execute()</code> method is the key component of the <code class="inlineCode">InferenceExecutor</code> class. This method is responsible for actually performing the inference. When execute is called, it prepares the payload sent to the LLM by formatting the prompt with the user’s query and context.</p>
    <p class="normal">Then, it configures several parameters that influence the behavior of the LLM, such as the maximum number of new tokens the model is allowed to generate, a repetition penalty to discourage the model from generating repetitive text, and the temperature setting that controls the randomness of the output.</p>
    <p class="normal">Once the payload and<a id="_idIndexMarker979"/> parameters are set, the method calls the <code class="inlineCode">inference</code> function from <code class="inlineCode">LLMInferenceSagemakerEndpoint</code> and waits for the generated answer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">execute</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:
    <span class="hljs-variable">self</span>.llm.set_payload(
        inputs=<span class="hljs-variable">self</span>.prompt.<span class="hljs-built_in">format</span>(query=<span class="hljs-variable">self</span>.query, context=<span class="hljs-variable">self</span>.context),
        parameters={
            <span class="hljs-string">"max_new_tokens"</span>: settings.MAX_NEW_TOKENS_INFERENCE,
            <span class="hljs-string">"repetition_penalty"</span>: <span class="hljs-number">1.1</span>,
            <span class="hljs-string">"temperature"</span>: settings.TEMPERATURE_INFERENCE,
        },
    )
    answer = <span class="hljs-variable">self</span>.llm.inference()[<span class="hljs-number">0</span>][<span class="hljs-string">"generated_text"</span>]
    <span class="hljs-keyword">return</span> answer
</code></pre>
    <p class="normal">By making the inference through an object that implements the Inference interface we decouple, we can easily inject other Inference strategies and the <code class="inlineCode">LLMInferenceSagemakerEndpoint</code> implementation presented above without modifying different parts of the code.</p>
    <p class="normal">Running a test example is straightforward. Simply call the following Python file, as shown below:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry run python -m llm_engineering.model.inference.test
</code></pre>
    <p class="normal">Also, for convenience, we wrap it under a <code class="inlineCode">poe</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe test-sagemaker-endpoint
</code></pre>
    <p class="normal">Now, we must understand how we implement the business microservice using FastAPI. This microservice will send HTTP requests to the LLM microservice defined above and call the RAG retrieval module implemented in <em class="italic">Chapter 9</em>.</p>
    <h2 id="_idParaDest-253" class="heading-2">Building the business microservice using FastAPI</h2>
    <p class="normal">To<a id="_idIndexMarker980"/> implement a simple FastAPI application that<a id="_idIndexMarker981"/> proves our deployment strategy, we first have to define a FastAPI instance as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> fastapi <span class="hljs-keyword">import</span> FastAPI
app = FastAPI()
</code></pre>
    <p class="normal">Next, we define the <code class="inlineCode">QueryRequest</code> and <code class="inlineCode">QueryResponse</code> classes using Pydantic’s <code class="inlineCode">BaseModel</code>. These classes represent the request and response structure for the FastAPI endpoints:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">QueryRequest</span>(<span class="hljs-title">BaseModel</span>):
    query: <span class="hljs-built_in">str</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">QueryResponse</span>(<span class="hljs-title">BaseModel</span>):
    answer: <span class="hljs-built_in">str</span>
</code></pre>
    <p class="normal">Now that we’ve defined our FastAPI components and have all the SageMaker elements in place, let’s reiterate over the <code class="inlineCode">call_llm_service()</code> and <code class="inlineCode">rag()</code> functions we’ve presented in <em class="chapterRef">Chapter 9</em> and couldn’t run because we haven’t deployed our fine-tuned LLM. Thus, as a refresher, the <code class="inlineCode">call_llm_service()</code> function wraps the inference logic used to call the SageMaker LLM microservice:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">call_llm_service</span>(<span class="hljs-params">query: </span><span class="hljs-built_in">str</span><span class="hljs-params">, context: </span><span class="hljs-built_in">str</span><span class="hljs-params"> | </span><span class="hljs-literal">None</span>) -&gt; <span class="hljs-built_in">str</span>:
    llm = LLMInferenceSagemakerEndpoint(
        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_component_name=<span class="hljs-literal">None</span>
    )
    answer = InferenceExecutor(llm, query, context).execute()
    <span class="hljs-keyword">return</span> answer
</code></pre>
    <p class="normal">Next, we define the <code class="inlineCode">rag()</code> function that implements all the RAG business logic. To avoid repeating ourselves, check <em class="chapterRef">Chapter 9</em> for the complete function explanation. What is important to highlight is that the <code class="inlineCode">rag()</code> function only implements the business steps required to do RAG, which are CPU- and I/O-bounded. For example, the <code class="inlineCode">ContextRetriever</code> class makes API calls to OpenAI and Qdrant, which are network I/O bounded, and calls the embedding model, which runs directly on the CPU. Also, as the LLM inference logic is moved to a different microservice, the <code class="inlineCode">call_llm_service()</code> function is only network I/O bounded. To conclude, the whole function is light to run, where the heavy computing is done on other services, which allows us to host the FastAPI server on a light and cheap machine that doesn’t need a GPU to run at low latencies:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">rag</span>(<span class="hljs-params">query: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-built_in">str</span>:
    retriever = ContextRetriever(mock=<span class="hljs-literal">False</span>)
    documents = retriever.search(query, k=<span class="hljs-number">3</span> * <span class="hljs-number">3</span>)
    context = EmbeddedChunk.to_context(documents)
    answer = call_llm_service(query, context)
    <span class="hljs-keyword">return</span> answer
</code></pre>
    <p class="normal">Ultimately, we define the <code class="inlineCode">rag_endpoint()</code> function, used to expose our RAG logic over the internet as an HTTP endpoint. We use a Python decorator to expose it as a POST endpoint in the FastAPI application. This endpoint is mapped to the <code class="inlineCode">/rag</code> route and expects a <code class="inlineCode">QueryRequest</code> as input. The function processes the request by calling the rag function with the<a id="_idIndexMarker982"/> user’s query. If successful, it returns the <a id="_idIndexMarker983"/>answer wrapped in a <code class="inlineCode">QueryResponse</code> object. If an exception occurs, it raises an HTTP <em class="chapterRef">500</em> error with the exception details:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@app.post(</span><span class="hljs-string">"/rag"</span><span class="hljs-params">, response_model=QueryResponse</span><span class="hljs-meta">)</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title">rag_endpoint</span>(<span class="hljs-params">request: QueryRequest</span>):
    <span class="hljs-keyword">try</span>:
        answer = rag(query=request.query)
        <span class="hljs-keyword">return</span> {<span class="hljs-string">"answer"</span>: answer}
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        <span class="hljs-keyword">raise</span> HTTPException(status_code=<span class="hljs-number">500</span>, detail=<span class="hljs-built_in">str</span>(e)) <span class="hljs-keyword">from</span> e
</code></pre>
    <p class="normal">This FastAPI application demonstrates how to effectively integrate an LLM hosted on AWS SageMaker into a web service, utilizing RAG to enhance the relevance of the model’s responses. The code’s modular design, leveraging custom classes like <code class="inlineCode">ContextRetriever</code>, <code class="inlineCode">InferenceExecutor</code>, and <code class="inlineCode">LLMInferenceSagemakerEndpoint</code>, allows for easy customization and scalability, making it a powerful tool for deploying ML models in production environments.</p>
    <p class="normal">We will leverage the <code class="inlineCode">uvicorn</code> web server, the go-to method for FastAPI applications, to start the server. To do so, you have to run the following:</p>
    <pre class="programlisting con"><code class="hljs-con">uvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload
</code></pre>
    <p class="normal">Also, you can run the following <code class="inlineCode">poe</code> command to achieve the same:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-inference-ml-service
</code></pre>
    <p class="normal">To call the <code class="inlineCode">/rag</code> endpoint, we can leverage the <code class="inlineCode">curl</code> CLI command to make a POST HTTP request to our FastAPI server, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">curl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/json' -d '{\"query\": \"your_query \"}'
</code></pre>
    <p class="normal">As usual, we provided an example using a <code class="inlineCode">poe</code> command that contains an actual user query:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe call-inference-ml-service
</code></pre>
    <p class="normal">This <a id="_idIndexMarker984"/>FastAPI server runs only locally. The next step <a id="_idIndexMarker985"/>would be to deploy it to AWS <strong class="keyWord">Elastic Kubernetes Service</strong> (<strong class="keyWord">EKS</strong>), a <a id="_idIndexMarker986"/>self-hosted version of Kubernetes by AWS. Another option would be to deploy it to AWS <strong class="keyWord">Elastic Container Service</strong> (<strong class="keyWord">ECS</strong>), which<a id="_idIndexMarker987"/> is similar to AWS EKS but doesn’t use Kubernetes under the hood but AWS’s implementation. Unfortunately, this is not specific to LLMs or LLMOps. Hence, we won’t go through these steps in this book. But to get an idea of what you must do, you must create an <a id="_idIndexMarker988"/>AWS EKS/ECS cluster from the dashboard or leverage an <strong class="keyWord">infrastructure-as-code</strong> (<strong class="keyWord">IaC</strong>) tool such as<a id="_idIndexMarker989"/> Terraform. After that, you will have to Dockerize the FastAPI code presented above. Ultimately, you would have to push the Docker image to AWS ECR and create an ECS/EKR deployment using the Docker image hosted on ECR. If this sounds like a lot, the good news is that we will walk you through a similar example in <em class="italic">Chapter 11</em>, where we will deploy the ZenML pipelines to AWS.</p>
    <div class="note">
      <p class="normal">Once you’re done testing your inference pipeline deployment, deleting all your AWS SageMaker resources used to deploy the LLM is essential. As almost all AWS resources use a pay-as-you-go strategy, using SageMaker for a few hours wouldn’t break your wallet, but if you forget and leave it open, in a few days, the costs can grow exponentially. Thus, a good rule of thumb is to always delete everything after you’re done testing your SageMaker infrastructure (or any AWS resource). Luckily, we have provided a script that deletes all the AWS SageMaker resources for you:</p>
      <pre class="programlisting con"><code class="hljs-con">poetry poe delete-inference-endpoint
</code></pre>
      <p class="normal">To ensure everything was correctly deleted, go to your SageMaker dashboard and check it yourself.</p>
    </div>
    <h1 id="_idParaDest-254" class="heading-1">Autoscaling capabilities to handle spikes in usage</h1>
    <p class="normal">So far, the <a id="_idIndexMarker990"/>SageMaker LLM microservice has used a static number of replicas to serve our users, which means that all the time, regardless of the traffic, it has the same number of instances up and running. As we highlighted throughout this book, machines with GPUs are expensive. Thus, we lose a lot of money during downtime when most replicas are idle. Also, if our application has sudden spikes in traffic, the application will perform poorly as the server cannot handle the number of requests. This is a massive problem for the user experience of our application, as in those spikes, we bring in the majority of new users. Thus, if they have a terrible impression of our product, we significantly reduce their chance of returning to our platform.</p>
    <p class="normal">Previously, we configured our multi-endpoint service using the <code class="inlineCode">ResourceRequirements</code> class from SageMaker. For example, let’s assume we requested four copies (replicas) with the following compute requirements:</p>
    <pre class="programlisting code"><code class="hljs-code">model_resource_config = ResourceRequirements(
    requests={
        <span class="hljs-string">"copies"</span>: <span class="hljs-number">4</span>,  <span class="hljs-comment"># Number of replicas.</span>
        <span class="hljs-string">"num_accelerators"</span>: <span class="hljs-number">4</span>, <span class="hljs-comment"># Number of GPUs required.</span>
        <span class="hljs-string">"num_cpus"</span>: <span class="hljs-number">8</span>, <span class="hljs-comment"># Number of CPU cores required.</span>
        <span class="hljs-string">"memory"</span>: <span class="hljs-number">5</span> * <span class="hljs-number">1024</span>,  <span class="hljs-comment"># Minimum memory required in Mb (required)</span>
    },
)
</code></pre>
    <p class="normal">Using this configuration, we always have four replicas serving the clients, regardless of idle time or spikes in traffic. The solution is to implement an autoscaling strategy that scales the number of replicas up and down dynamically based on various metrics, such as the number of requests.</p>
    <p class="normal">For example, <em class="italic">Figure 10.8</em> shows a standard architecture where the SageMaker Inference endpoints scale in and out based on the number of requests. When there is no traffic, we can have one<a id="_idIndexMarker991"/> online replica so the server remains responsive to new user requests or even scales down to zero if the latency is not super critical. Then, let’s assume that when we have around 10 requests per second, we have to keep two replicas online, and when the number of requests spikes to 100 per second, the autoscaling service should spin up to 20 replicas to keep up with the demand. Note that these are fictional numbers that should be adapted to your specific use case.</p>
    <figure class="mediaobject"><img src="../Images/B31105_10_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.8: Autoscaling possible use cases</p>
    <p class="normal">Without going into the little details of cloud networking, when working with multi-replica systems, between the client and the replicas sits an <strong class="keyWord">Application Load Balancer</strong> (<strong class="keyWord">ALB</strong>) or another <a id="_idIndexMarker992"/>type of load balancer. </p>
    <p class="normal">All the requests first go to the ALB, which knows to route them to a replica. The ALB can adopt various routing strategies, where the simplest one is called round robin, which sequentially sends a request to each replica. For example, the first request is routed to replica one, the second to replica two, and so on. Taking this approach, regardless of how many replicas you have online, the endpoint that the client calls is always represented by the load balancer that acts as an entry point into your cluster. Thus, adding or removing new replicas doesn’t affect the server and client communication protocol.</p>
    <p class="normal">Let’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker Inference endpoint. SageMaker provides a feature called <strong class="keyWord">Application Auto Scaling </strong>that allows you to scale resources dynamically based on pre-defined policies. Two foundational steps are involved in effectively leveraging this functionality: registering a scalable target and creating a scalable policy.</p>
    <h2 id="_idParaDest-255" class="heading-2">Registering a scalable target</h2>
    <p class="normal">The first step in enabling autoscaling for your <a id="_idIndexMarker993"/>resources is to register a scalable target with the <strong class="keyWord">Application Auto Scaling</strong> feature<a id="_idIndexMarker994"/> AWS provides. Think of this as informing AWS about the specific resource you intend to scale, as well as setting the boundaries within which the scaling should occur. However, this <a id="_idIndexMarker995"/>step does not dictate how or when the scaling should happen.</p>
    <p class="normal">For instance, when working with SageMaker Inference components, you’ll define the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Resource ID</strong>: This serves as a unique identifier for the resource you want to scale, typically including the name of the SageMaker Inference component.</li>
      <li class="bulletList"><strong class="keyWord">Service namespace</strong>: This identifies the AWS service the resource belongs to, which, in this case, is <strong class="keyWord">SageMaker</strong>.</li>
      <li class="bulletList"><strong class="keyWord">Scalable dimension</strong>: This specifies the resources to be scaled, such as the desired number of copies.</li>
      <li class="bulletList"><strong class="keyWord">MinCapacity and MaxCapacity</strong>: These parameters define the boundaries of the autoscaling strategies, such as minimum and maximum limits of the number of replicas.</li>
    </ul>
    <p class="normal">By registering a scalable target, you prepare your SageMaker Inference component for future scaling actions without determining when or how these actions should occur.</p>
    <h2 id="_idParaDest-256" class="heading-2">Creating a scalable policy</h2>
    <p class="normal">Once your scalable target is registered, the<a id="_idIndexMarker996"/> next step is defining how the scaling should occur. This is where creating a scaling policy comes in. A scaling policy defines specific rules that trigger scaling events. When creating policies, you have to define metrics to <a id="_idIndexMarker997"/>know what to monitor and thresholds to know when to emit scaling events.</p>
    <p class="normal">In the context of our SageMaker Inference component, the scalable policy might include the following elements:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Policy type</strong>: For instance, you might select <strong class="keyWord">TargetTrackingScaling</strong>, a policy that adjusts the resource’s capacity to maintain a specific target value for a chosen metric.</li>
      <li class="bulletList"><strong class="keyWord">Target tracking configuration</strong>: This involves selecting the metric to monitor (such as <em class="italic">SageMakerInferenceComponentInvocationsPerCopy</em>), setting the desired target value, and specifying cooldown periods that control how quickly scaling actions can occur after previous ones.</li>
    </ul>
    <p class="normal">The scaling policy defines the rules for your scaling-in and scaling-out strategy. It constantly monitors the specified metric, and depending on whether the metric exceeds or falls below the target value, it triggers actions to scale the number of inference component copies up or down, always within the limits defined by the registered scalable target.</p>
    <p class="normal">Let’s explain in more depth how<a id="_idIndexMarker998"/> the <strong class="keyWord">TargetTrackingScaling </strong>policy works. Imagine you have a metric that represents the ideal average utilization or throughput level for your application. With target tracking, you select this metric and set a target value that reflects the optimal state for your application. Once defined, <strong class="keyWord">Application Auto Scaling</strong> creates<a id="_idIndexMarker999"/> and manages the necessary CloudWatch alarms to monitor this metric. When deviations occur, scaling actions are triggered, similar to how a thermostat adjusts to maintain a consistent room temperature.</p>
    <p class="normal">For instance, consider an application running on SageMaker. Let’s assume we set a target of keeping GPU utilization around 70 percent. This target allows you to maintain enough headroom to manage sudden traffic spikes while preventing the unnecessary cost of idle resources. When GPU usage exceeds the target, the system scales out, adding resources to manage the increased load. Conversely, when GPU usage drops below the target, the system scales in, reducing capacity to minimize costs during quieter periods.</p>
    <p class="normal">One significant advantage of setting up target tracking policies using Application Auto Scaling is that they simplify the scaling process. You no longer need to configure CloudWatch alarms and define scaling adjustments manually.</p>
    <h2 id="_idParaDest-257" class="heading-2">Minimum and maximum scaling limits</h2>
    <p class="normal">When setting up autoscaling for your SageMaker Inference endpoints, it’s crucial to establish your maximum and <a id="_idIndexMarker1000"/>minimum scaling limits before creating your scaling<a id="_idIndexMarker1001"/> policy. The minimum value represents the least resources your model can operate with. This value must be at least 1, ensuring that your model always has some capacity.</p>
    <p class="normal">Next, configure the maximum value, which defines the upper limit of resources your model can scale up to. While the maximum must be equal to or greater than the minimum value, it doesn’t impose any upper limit. Thus, you can scale up as much as your application needs within the boundaries of what AWS can provide.</p>
    <h3 id="_idParaDest-258" class="heading-3">Cooldown period</h3>
    <p class="normal">Another important aspect of a scaling policy is the <a id="_idIndexMarker1002"/>cooldown period, during which it’s crucial to maintain a balance between responsiveness and stability. This cooldown period acts as a safeguard, ensuring that your system doesn’t overreact during scaling events—whether it’s reducing capacity (scaling in) or increasing it (scaling out). By introducing a calculated pause, the cooldown period prevents rapid fluctuations in the number of instances. Specifically, it delays the removal of instances during scale-in requests and restricts the creation of new replicas during scale-out requests. This strategy helps maintain a stable and efficient environment for LLM service.</p>
    <p class="normal">These practical basics are used in autoscaling most web servers, including online real-time ML servers. Once you understand how to configure scaling policies for SageMaker, you can immediately apply the strategies you’ve learned to other popular deployment tools like Kubernetes or AWS ECS.</p>
    <div class="note">
      <p class="normal">For a step-by-step guideline on how to configure autoscaling for the AWS SagaMaker endpoint implemented in this chapter, you can follow this official tutorial from AWS: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html"><span class="url">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html</span></a>.</p>
    </div>
    <p class="normal">Autoscaling<a id="_idIndexMarker1003"/> is a critical component in any cloud architecture, but there are some pitfalls you should be aware of. The first and most dangerous one is over-scaling, which directly impacts the costs of your infrastructure. If your scaling policy or cooldown period is too sensitive, you may be uselessly spinning up new machines that will remain idle or with the resources underused. The second reason is on the other side of the spectrum, where your system doesn’t scale enough, resulting in a bad user experience for the end user.</p>
    <p class="normal">That’s why a good practice is to understand the requirements of your system. Based on them, you should tweak and experiment with the autoscaling parameters in a dev or test environment until you find the sweet spot (similar to hyperparameter tuning when training models). Let’s suppose, for instance, that you expect your system to support an average of 100 users per minute and scale up to 10,000 users per minute in case of an outlier event such as a holiday. Using this spec, you can stress test your system and monitor your resources to find the best trade-off between costs, latency, and throughput that supports standard and outlier use cases.</p>
    <h1 id="_idParaDest-259" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we learned what design decisions to make before serving an ML model, whether an LLM or not, by walking you through the three fundamental deployment types for ML models: online real-time inference, asynchronous inference, and offline batch transform. Then, we considered whether building our ML-serving service as a monolith application made sense or splitting it into two microservices, such as an LLM microservice and a business microservice. To do this, we weighed the pros and cons of a monolithic versus microservices architecture in model-serving.</p>
    <p class="normal">Next, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker Inference endpoint. We also saw how to implement the business microservice using FastAPI, which consists of all the RAG steps based on the retrieval module implemented in <em class="italic">Chapter 9</em> and the LLM microservice deployed on AWS SageMaker. Ultimately, we explored why we have to implement an autoscaling strategy. We also reviewed a popular autoscaling strategy that scales in and out based on a given set of metrics and saw how to implement it in AWS SageMaker.</p>
    <p class="normal">In the next chapter, we will learn about the fundamentals of MLOps and LLMOps and then explore how to deploy the ZenML pipelines to AWS and implement a <strong class="keyWord">continuous training</strong>, <strong class="keyWord">continuous integration</strong>, and <strong class="keyWord">continuous delivery</strong> (<strong class="keyWord">CT</strong>/<strong class="keyWord">CI</strong>/<strong class="keyWord">CD</strong>) and monitoring pipeline.</p>
    <h1 id="_idParaDest-260" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">AWS Developers. (2023, September 22). <em class="italic">Machine Learning in 15: Amazon SageMaker High-Performance Inference at Low Cost</em> [Video]. YouTube. <a href="https://www.youtube.com/watch?v=FRbcb7CtIOw "><span class="url">https://www.youtube.com/watch?v=FRbcb7CtIOw</span></a></li>
      <li class="bulletList">bitsandbytes-foundation. (n.d.). GitHub—bitsandbytes-foundation/bitsandbytes: Accessible large language models via k-bit quantization for PyTorch. GitHub. <a href="https://github.com/bitsandbytes-foundation/bitsandbytes "><span class="url">https://github.com/bitsandbytes-foundation/bitsandbytes</span></a></li>
      <li class="bulletList"><em class="italic">Difference between IAM role and IAM user in AWS</em>. (n.d.). Stack Overflow. <a href="https://stackoverflow.com/questions/46199680/difference-between-iam-role-and-iam-user-in-aws "><span class="url">https://stackoverflow.com/questions/46199680/difference-between-iam-role-and-iam-user-in-aws</span></a></li>
      <li class="bulletList">Huggingface. (n.d.-a). GitHub—huggingface/safetensors: Simple, safe way to store and distribute tensors. GitHub. <a href="https://github.com/huggingface/safetensors "><span class="url">https://github.com/huggingface/safetensors</span></a></li>
      <li class="bulletList">Huggingface. (n.d.-b). GitHub—huggingface/text-generation-inference: Large Language Model Text Generation Inference. GitHub. <a href="https://github.com/huggingface/text-generation-inference "><span class="url">https://github.com/huggingface/text-generation-inference</span></a></li>
      <li class="bulletList">Huyen, C. (n.d.). <em class="italic">Designing machine learning systems</em>. O’Reilly Online Learning. <a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/ "><span class="url">https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/</span></a></li>
      <li class="bulletList">Iusztin, P. (2024, August 20). Architect LLM &amp; RAG inference pipelines | Decoding ML. <em class="italic">Medium</em>. <a href="https://medium.com/decodingml/architect-scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99 "><span class="url">https://medium.com/decodingml/architect-scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99</span></a></li>
      <li class="bulletList">Lakshmanan, V., Robinson, S., and Munn, M. (n.d.). <em class="italic">Machine Learning design patterns</em>. O’Reilly Online Learning. <a href="https://www.oreilly.com/library/view/machine-learning-design/9781098115777/ "><span class="url">https://www.oreilly.com/library/view/machine-learning-design/9781098115777/</span></a></li>
      <li class="bulletList">Mendoza, A. (2024, August 21). <em class="italic">Best tools for ML model Serving</em>. neptune.ai. <a href="https://neptune.ai/blog/ml-model-serving-best-tools  "><span class="url">https://neptune.ai/blog/ml-model-serving-best-tools</span></a></li>
    </ul>
    <p class="normal"><a href="https://neptune.ai/blog/ml-model-serving-best-tools  "/></p>
    <h1 id="_idParaDest-261" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng"><span class="url">https://packt.link/llmeng</span></a></p>
    <p class="normal"><span class="url"><img src="../Images/QR_Code79969828252392890.png" alt=""/></span></p>
  </div>
</body></html>