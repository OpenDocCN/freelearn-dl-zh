- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: 'RAGs to Riches: Elevating AI with External Data'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'RAGs to Riches: 利用外部数据提升 AI'
- en: LLMs such as GPT have certain limitations. They may not have up-to-date information
    due to their knowledge cutoff date for training. This poses a significant challenge
    when we want our AI models to provide accurate, context-aware, and timely responses.
    Imagine asking an LLM a question about the latest technology trends or seeking
    real-time updates on a breaking news event; traditional language models might
    fall short in these scenarios.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如 GPT 这样的 LLM 存在某些局限性。由于它们的训练知识截止日期，它们可能没有最新的信息。当我们希望我们的 AI 模型提供准确、上下文感知和及时响应时，这构成了一个重大挑战。想象一下，当您向
    LLM 提出一个关于最新技术趋势的问题或寻求关于突发新闻事件的实时更新时，传统的语言模型在这些情况下可能会不足。
- en: In this chapter, we’re going to introduce you to a game-changing technique called
    **retrieval-augmented generation** (**RAG**), an outcome of the work carried out
    by researchers at Facebook AI (now Meta). It’s the secret sauce that empowers
    language models such as GPT to bridge the gap between their static knowledge and
    the dynamic real world. With RAG, we’ll show you how to equip your generative
    AI applications with the ability to pull in fresh information, ground your organizational
    data, cross-reference facts to address hallucinations, and stay contextually aware,
    all in real time. We will also discuss the fundaments of vector databases, a new,
    hot, and emerging database that is designed for storing, indexing, and querying
    vectors that represent highly dimensional data; they are typically used for similarity
    search and machine learning applications and are important in building RAG applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您介绍一种颠覆性的技术，称为**检索增强生成**（**RAG**），这是 Facebook AI（现在称为 Meta）的研究人员所进行工作的成果。这是赋予
    GPT 等语言模型连接其静态知识与动态现实之间差距的秘密配方。有了 RAG，我们将向您展示如何使您的生成式 AI 应用程序具备获取新信息、定位组织数据、交叉验证事实以解决幻觉，并保持上下文意识的能力，这一切都在实时进行。我们还将讨论向量数据库的基础知识，这是一种新的、热门的、新兴的数据库，旨在存储、索引和查询代表高维数据的向量；它们通常用于相似性搜索和机器学习应用，并在构建
    RAG 应用程序中至关重要。
- en: Let’s understand how RAG can turn your language model into an information-savvy
    conversational assistant, ensuring that it’s always in the know, no matter when
    you ask the question.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解 RAG 如何将您的语言模型转变为信息丰富的对话助手，确保无论何时提问，它都能始终了解情况。
- en: 'We will cover the following main topics in the chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要内容：
- en: A deep dive into vector DB essentials
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解向量数据库的基本知识
- en: Vector stores
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量存储
- en: The role of vector DBs in retrieval-augmented generation (RAG)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据库在检索增强生成（RAG）中的作用
- en: Chunking strategies
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块策略
- en: Evaluation of RAG using Azure Prompt Flow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Azure Prompt Flow 评估 RAG
- en: Case study – Global chat application deployment by a multinational organization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究 – 一家跨国组织部署的全球聊天应用程序
- en: '![Figure 4.1 – Benefits of RAG](img/B21443_04_1.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – RAG 的优势](img/B21443_04_1.jpg)'
- en: Figure 4.1 – Benefits of RAG
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – RAG 的优势
- en: A deep dive into vector DB essentials
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解向量数据库的基本知识
- en: To fully comprehend RAG, it’s imperative to understand vector DBs because RAG
    relies heavily on its efficient data retrieval for query resolution. A vector
    DB is a database designed to store and efficiently query highly dimensional vectors
    and is often used in similarity searches and machine learning tasks. The design
    and mechanics of vector DBs directly influence the effectiveness and accuracy
    of RAG answers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全理解 RAG，理解向量数据库是至关重要的，因为 RAG 严重依赖于其高效的数据检索以解决查询。向量数据库是一种设计用于存储和高效查询高维向量的数据库，常用于相似性搜索和机器学习任务。向量数据库的设计和机制直接影响
    RAG 答案的效率和准确性。
- en: In this section, we will cover the fundamental components of vector DBs (vectors
    and vector embeddings), and in the next section, we will dive deeper into the
    important characteristics of vector DBs that enable a RAG-based generative AI
    solution. We will also explain how it differs from regular databases and then
    tie it all back to explain RAG.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍向量数据库（向量及其嵌入）的基本组件，在下一节中，我们将深入探讨向量数据库的重要特性，这些特性使得基于 RAG 的生成式 AI 解决方案成为可能。我们还将解释它与常规数据库的不同之处，并将其全部联系起来解释
    RAG。
- en: Vectors and vector embeddings
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量和向量嵌入
- en: 'A vector is a mathematical object that has both magnitude and direction and
    can be represented by an ordered list of numbers. In a more general sense, especially
    in computer science and machine learning, a vector can be thought of as an array
    or list of numbers that represents a point in a certain dimensional space. For
    instances depicted in the following image, in 2D space (on the left), a vector
    might be represented as [x, y], whereas in 3D space (on the right), it might be
    [x, y, z]:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是一个具有大小和方向的数学对象，可以用一组有序的数字来表示。在更广泛的意义上，尤其是在计算机科学和机器学习领域，向量可以被看作是一个表示一定维度空间中点的数字数组或列表。以下图像中所示，在二维空间（左侧），一个向量可能表示为[x,
    y]，而在三维空间（右侧），它可能表示为[x, y, z]：
- en: '![Figure 4.2 – Representation of vectors in 2D and 3D space](img/B21443_04_2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – 2D和3D空间中向量的表示](img/B21443_04_2.jpg)'
- en: Figure 4.2 – Representation of vectors in 2D and 3D space
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 2D和3D空间中向量的表示
- en: Vector embedding refers to the representation of objects, such as words, sentences,
    or even entire documents, as vectors in a highly dimensional space. A highly dimensional
    space denotes a mathematical space with more than three dimensions, frequently
    used in data analysis and machine learning to represent intricate data structures.
    Think of it as a room where you can move in more than three directions, facilitating
    the description and analysis of complex data. The embedding process converts words,
    sentences, or documents into vector representations, capturing the intricate semantic
    relationships between them. Hence, words with similar meanings tend to be close
    to each other in the highly dimensional space. Now, you must be wondering how
    this plays a role in designing generative AI solutions consisting of LLMs. Vector
    embeddings provide the foundational representation of data. They are a standardized
    numerical representation for diverse types of data, which LLMs use to process
    and generate information. Such an embedding process to convert words and sentences
    to a numerical representation is initiated by embedding models such as OpenAI’s
    text-embedding-ada-002\. Let’s explain this with an example.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 向量嵌入指的是将对象，如单词、句子，甚至整个文档，在高度维度空间中表示为向量的过程。高度维度空间指的是具有超过三个维度的数学空间，常用于数据分析机器学习中表示复杂的数据结构。想象一下，这是一个你可以向超过三个方向移动的房间，便于描述和分析复杂数据。嵌入过程将单词、句子或文档转换为向量表示，捕捉它们之间复杂的语义关系。因此，具有相似意义的单词在高度维度空间中往往彼此靠近。现在，你可能想知道这如何在设计包含LLMs的生成式AI解决方案中发挥作用。向量嵌入提供了数据的基础表示。它们是多种类型数据的标准化数值表示，LLMs使用它来处理和生成信息。这种将单词和句子转换为数值表示的嵌入过程由OpenAI的text-embedding-ada-002等嵌入模型启动。让我们用一个例子来解释这一点。
- en: 'The following image visually represents the clustering of mammals and birds
    in a two-dimensional vector embedding space, differentiating between their realistic
    and cartoonish portrayals. This image depicts a spectrum between “REALISTIC” and
    “CARTOON” representations, further categorized into “MAMMAL” and “BIRD.” On the
    realistic side, there’s a depiction of a mammal (elk) and three birds (an owl,
    an eagle, and a small bird). On the cartoon side, there are stylized and whimsical
    cartoon versions of mammals and birds, including a comically depicted deer, an
    owl, and an exaggerated bird character. LLMs use such vector embedding spaces,
    which are numerical representations of objects in highly dimensional spaces, to
    understand, process, and generate information. For example, imagine an educational
    application designed to teach children about wildlife. If a student prompts the
    chatbot to provide images of birds in a cartoon representation, the LLM will search
    and generate information from the bottom right quadrant:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像直观地表示了在二维向量嵌入空间中哺乳动物和鸟类的聚类，区分了它们的现实和卡通表现。这幅图像描绘了“现实”和“卡通”表示之间的光谱，进一步分为“哺乳动物”和“鸟类”。在现实方面，有一只哺乳动物（麋鹿）和三只鸟（一只猫头鹰、一只老鹰和一只小鸟）。在卡通方面，有风格化和异想天开的卡通版本，包括一个滑稽的鹿、一只猫头鹰和夸张的鸟类角色。LLMs使用这样的向量嵌入空间，这些是高度维度空间中对象的数值表示，来理解、处理和生成信息。例如，想象一个旨在教育儿童了解野生动物的教育应用程序。如果学生提示聊天机器人提供卡通风格的鸟类图像，LLM将搜索并从右下象限生成信息：
- en: '![Figure 4.3 – Location of animals with similar characteristics in a highly
    dimensional space, demonstrating “relatedness”](img/B21443_04_3.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 在高维空间中具有相似特征的动物的位置，展示了“相关性”](img/B21443_04_3.jpg)'
- en: Figure 4.3 – Location of animals with similar characteristics in a highly dimensional
    space, demonstrating “relatedness”
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 在高维空间中具有相似特征的动物的位置，展示了“相关性”
- en: Now, let’s delve into the evolution of embedding models that produce embeddings,
    a.k.a numerical representations of objects, within highly dimensional spaces.
    Embedding models have experienced significant evolution, transitioning from the
    initial methods that mapped discrete words to dense vectors, such as word-to-vector
    (Word2Vec), global vectors for word representation (GloVe), and FastText to more
    sophisticated contextual embeddings using deep learning architectures. These newer
    models, such as embeddings from language models (ELMos), utilize long short-term
    memory (LSTM)-based structures to offer context-specific representations. The
    newer transformer architecture-based embedding models, which underpin models such
    as bidirectional encoder representations from transformers (BERT), generative
    pre-trained transformer (GPT), and their subsequent iterations, marked a revolutionary
    leap over predecessor models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解嵌入模型的演变，这些模型在高度维度的空间中产生嵌入，即对象的数值表示。嵌入模型经历了显著的演变，从最初将离散单词映射到密集向量的方法（如词向量（Word2Vec）、全局单词表示向量（GloVe）和FastText）过渡到更复杂的基于深度学习架构的上下文嵌入。这些较新的模型，如来自语言模型的嵌入（ELMos），利用基于长短期记忆（LSTM）的结构来提供上下文特定的表示。基于新变压器架构的嵌入模型，如双向编码器表示的变压器（BERT）、生成预训练变压器（GPT）及其后续迭代，在先前的模型上实现了革命性的飞跃。
- en: These models capture contextual information in unparalleled depth, enabling
    embeddings to represent nuances in word meanings based on the surrounding context,
    thereby setting new standards in various natural language processing tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型以无与伦比的程度捕捉上下文信息，使嵌入能够根据周围上下文表示单词含义的细微差别，从而在各种自然语言处理任务中设定新的标准。
- en: 'Important note:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：
- en: In Jan 2024, OpenAI announced two third-generation embedding models, **text-embedding-3-small**
    and **text-embedding-3-large**, which are the newest models that have better performance,
    lower costs, and better multi-lingual retrieval and parameters to reduce the overall
    size of dimensions when compared to predecessor second-generation model, **text-embedding-ada-002**.
    Another key difference is the number of dimensions between the two generations.
    The third-generation models come in different dimensions, and the highest they
    can go up to is 3,072\. As of Jan 2024, we have seen more production workloads
    using text-embedding-ada-002 in production, which has 1,536 dimensions. OpenAI
    recommends using the third-generation models going forward for improved performance
    and reduced costs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年1月，OpenAI宣布了两种第三代嵌入模型，**text-embedding-3-small**和**text-embedding-3-large**，这是性能更好、成本更低、多语言检索更好以及参数减少，以减少与前辈第二代模型**text-embedding-ada-002**相比的维度总体大小的新模型。另一个关键区别是两代之间的维度数。第三代模型有不同的维度，最高可达3,072。截至2024年1月，我们已经看到更多使用1,536维度的text-embedding-ada-002在生产中的工作负载，OpenAI建议未来使用第三代模型以获得更好的性能和降低成本。
- en: We also wanted you to know that while OpenAI’s embedding model is one of the
    most popular choices when it comes to text embeddings, you can find the list of
    leading embedding models on Hugging Face ([https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望让您知道，虽然OpenAI的嵌入模型在文本嵌入方面是最受欢迎的选择之一，但您可以在Hugging Face上找到领先嵌入模型的列表（[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)）。
- en: 'The following snippet of code gives an example of generating Azure OpenAI endpoints:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段给出了生成Azure OpenAI端点的示例：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this section, we highlighted the significance of vector embeddings. However,
    their true value emerges when used effectively. Hence, we’ll now dive deep into
    indexing and vector search strategies, which are crucial for optimal data retrieval
    in the RAG workflow.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们强调了向量嵌入的重要性。然而，它们真正的价值在于有效使用时。因此，我们现在将深入探讨索引和向量搜索策略，这对于RAG工作流程中的最佳数据检索至关重要。
- en: Vector search strategies
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量搜索策略
- en: 'Vector search strategies are crucial because they determine how efficiently
    and accurately highly dimensional data (such as embeddings) can be queried and
    retrieved. Optimal strategies ensure that the most relevant and contextually appropriate
    results are returned. In vector-based searching, there are primarily two main
    strategies: **exact search** and **approximate search**.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 向量搜索策略至关重要，因为它们决定了如何高效且准确地查询和检索高维数据（如嵌入）。最佳策略确保返回最相关和上下文适当的搜索结果。在基于向量的搜索中，主要有两种主要策略：**精确搜索**和**近似搜索**。
- en: Exact search
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精确搜索
- en: The exact search method, as the term suggests, directly matches a query vector
    with vectors in the database. It uses an exhaustive approach to identify the closest
    neighbors, allowing minimal to no errors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如术语所暗示的，精确搜索方法直接将查询向量与数据库中的向量相匹配。它使用穷举法来识别最近的邻居，允许最小化或没有错误。
- en: This is typically what the traditional KNN method employs. Traditional KNNs
    utilize brute force methods to find the K-nearest neighbors, which demands a thorough
    comparison of the input vector with every other vector in the dataset. Although
    computing the similarity for each vector is typically quick, the process becomes
    time-consuming and resource-intensive over extensive datasets because of the vast
    number of required comparisons. For instance, if you had a dataset of one million
    vectors and wanted to find the nearest neighbors for a single input vector, the
    traditional KNN would require one million distance computations. This can be thought
    of as looking up a friend’s phone number in a phone book by checking each entry
    one by one rather than using a more efficient search strategy that speeds up the
    process, which we will discuss in the next section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是传统KNN方法所采用的。传统的KNNs使用穷举法来找到K个最近的邻居，这需要对输入向量与数据集中的每个其他向量进行彻底的比较。尽管通常对每个向量的相似性计算很快，但由于需要进行的比较数量庞大，这个过程在大型数据集中会变得耗时且资源密集。例如，如果您有一个包含一百万个向量的数据集，并且想要找到一个输入向量的最近邻居，传统的KNN将需要一百万次距离计算。这可以想象为在电话簿中通过逐个检查条目来查找朋友的电话号码，而不是使用更有效的搜索策略来加快这个过程，我们将在下一节中讨论。
- en: Approximate nearest neighbors (ANNs)
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 近似最近邻（ANNs）
- en: In modern vector DBs, the search strategy known as ANN stands out as a powerful
    technique that quickly finds the near-closest data points in highly dimensional
    spaces, potentially trading off a bit of accuracy for speed. Unlike KNN, ANN prioritizes
    search speed at the expense of slight accuracy. Additionally, for it to function
    effectively, a vector index must be built beforehand.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代向量数据库中，被称为ANN的搜索策略因其快速找到高维空间中近似的最近数据点而脱颖而出，这是一种强大的技术，可能会以牺牲一点准确性为代价来换取速度。与KNN不同，ANN优先考虑搜索速度，而牺牲了轻微的准确性。此外，为了有效地运行，必须事先构建一个向量索引。
- en: The process of vector indexing
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量索引的过程
- en: The process of vector indexing involves the organization of embeddings in a
    data structure called an index, which can be traversed quickly for retrieval purposes.
    Many ANN algorithms aid in forming a vector index, all aiming for rapid querying
    by creating an efficiently traversable data structure. Typically, they compress
    the original vector representation to enhance the search process.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 向量索引的过程涉及将嵌入组织到一个称为索引的数据结构中，该结构可以快速遍历以进行检索。许多ANN算法有助于形成向量索引，所有这些算法都旨在通过创建一个高效可遍历的数据结构来实现快速查询。通常，它们会压缩原始向量表示，以增强搜索过程。
- en: 'There are numerous indexing algorithms, and this is an active research area.
    ANNs can be broadly classified into **tree-based indexes**, **graph-based indexes**,
    **hash-based indexes**, and **quantization-based indexes**. In this section, we
    will cover the two most popular indexing algorithms. When creating an LLM application,
    you don’t need to dive deep into the indexing process since many vector databases
    provide this as a service to you. But it’s important to choose the right type
    of index for your specific needs to ensure efficient data retrieval:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着许多索引算法，这是一个活跃的研究领域。ANNs可以广泛地分为**基于树的索引**、**基于图的索引**、**基于哈希的索引**和**基于量化的索引**。在本节中，我们将介绍两种最流行的索引算法。在创建LLM应用程序时，您不需要深入研究索引过程，因为许多向量数据库已经为您提供了这项服务。但是，选择适合您特定需求的正确类型的索引对于确保高效的数据检索非常重要：
- en: '**Hierarchical navigable small world** (**HNSW**): This is a method for approximate
    similarity search in highly dimensional spaces. HNSW is a graph-based index that
    works by creating a hierarchical graph structure, where each node represents a
    data point, and the edges connect similar data points. This hierarchical structure
    allows for efficient search operations, as it narrows down the search space quickly.
    HNSW is well suited for similarity search use cases, such as content-based recommendation
    systems and text search.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层可导航小世界**（**HNSW**）：这是一种在高度维空间中进行近似相似性搜索的方法。HNSW是一种基于图的索引，通过创建一个分层图结构来工作，其中每个节点代表一个数据点，边连接相似的数据点。这种分层结构允许进行高效的搜索操作，因为它可以快速缩小搜索空间。HNSW非常适合相似性搜索用例，例如基于内容的推荐系统和文本搜索。'
- en: 'If you wish to dive deeper into its workings, we recommend checking out this
    research paper: [https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320).'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想深入了解其工作原理，我们建议查看这篇研究论文：[https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320)。
- en: 'The following image is a representation of the HNSW index:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图像是HNSW索引的表示：
- en: '![Figure 4.4 – Representation of HNSW index](img/B21443_04_4.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – HNSW索引的表示](img/B21443_04_4.jpg)'
- en: Figure 4.4 – Representation of HNSW index
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – HNSW索引的表示
- en: The image illustrates the HNSW graph structure used for efficient similarity
    searches. The graph is constructed in layers, with decreasing density from the
    bottom to the top. Each layer’s characteristic radius reduces as we ascend, creating
    sparser connections. The depicted search path, using the red dotted lines, showcases
    the algorithm’s strategy; it starts from the sparsest top layer, quickly navigating
    vast data regions, and then refines its search in the denser lower layers, minimizing
    the overall comparisons and enhancing search efficiency.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像说明了用于高效相似性搜索的HNSW图结构。图是分层构建的，从底部到顶部密度逐渐降低。随着我们向上移动，每一层的特征半径减小，创建更稀疏的连接。用红色虚线表示的搜索路径展示了算法的策略；它从最稀疏的顶层开始，快速导航大量数据区域，然后在更密集的底层细化搜索，最小化总体比较并提高搜索效率。
- en: '**Facebook AI Similarity Search** (**FAISS**): FAISS, developed by Facebook
    AI Research, is a library designed for the efficient similarity search and clustering
    of highly dimensional vectors. It uses product quantization to compress data during
    indexing, accelerating similarity searches in vast datasets. This method divides
    the vector space into regions known as Voronoi cells, each symbolized by a centroid.
    The primary purpose is to minimize storage needs and expedite searches, though
    it may slightly compromise accuracy. To visualize this, consider the following
    image. The Voronoi cells denote regions from quantization, and the labeled points
    within these cells are the centroids or representative vectors. When indexing
    a new vector, it’s aligned with its closest centroid. For searches, FAISS pinpoints
    the probable Voronoi cell containing the nearest neighbors and then narrows down
    the search within that cell, significantly cutting down distance calculations:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Facebook AI相似性搜索**（**FAISS**）：由Facebook AI Research开发的FAISS是一个用于高效相似性搜索和聚类高度维向量的库。它在索引过程中使用产品量化来压缩数据，加速大量数据集中的相似性搜索。这种方法将向量空间划分为称为Voronoi单元的区域，每个单元由一个质心表示。主要目的是最小化存储需求并加快搜索速度，尽管这可能会略微降低准确性。为了可视化这一点，请考虑以下图像。Voronoi单元表示量化区域，这些单元内的标记点是质心或代表性向量。在索引新向量时，它与其最近的质心对齐。对于搜索，FAISS确定可能包含最近邻的Voronoi单元，然后在单元内缩小搜索范围，显著减少距离计算：'
- en: '![Figure 4.5 – Representation of FAISS index](img/B21443_04_5.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – FAISS索引的表示](img/B21443_04_5.jpg)'
- en: Figure 4.5 – Representation of FAISS index
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – FAISS索引的表示
- en: It excels in applications such as image and video search, recommendation systems,
    and any task that involves searching for nearest neighbors in highly dimensional
    spaces because of its performance optimizations and built-in GPU optimization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其性能优化和内置GPU优化，它在图像和视频搜索、推荐系统以及任何涉及在高度维空间中搜索最近邻的任务中表现出色。
- en: In this section, we covered indexing and the role of ANNs in index creation.
    Next, we’ll explore similarity measures, how they differ from indexing, and their
    impact on improving data retrieval.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了索引和ANN在索引创建中的作用。接下来，我们将探讨相似性度量，它们与索引的不同之处，以及它们对改进数据检索的影响。
- en: When to Use HNSW vs. FAISS
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用HNSW与FAISS
- en: 'Use HNSW when:'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用HNSW的情况：
- en: High precision in similarity search is crucial.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相似度搜索中实现高精度至关重要。
- en: The dataset size is large but not at the scale where managing it becomes impractical
    for HNSW.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集规模很大，但尚未达到HNSW管理起来变得不切实际的程度。
- en: Real-time or near-real-time search performance is required.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要实时或接近实时的搜索性能。
- en: The dataset is dynamic, with frequent updates or insertions.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集是动态的，经常更新或插入。
- en: Apt for use cases involving text like article recommendation systems
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于涉及文本的用例，如文章推荐系统
- en: 'Use FAISS when:'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当使用FAISS时：
- en: Managing extremely large datasets (e.g., billions of vectors).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理极其庞大的数据集（例如，数十亿个向量）。
- en: Batch processing and GPU optimization can significantly benefit the application.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量处理和GPU优化可以显著提高应用性能。
- en: There’s a need for flexible trade-offs between search speed and accuracy.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要在搜索速度和准确性之间进行灵活的权衡。
- en: The dataset is relatively static, or batch updates are acceptable.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集相对静态，或批量更新是可以接受的。
- en: Apt for use cases like image and video search.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于图像和视频搜索等用例。
- en: Note
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Choosing the right indexing strategy hinges on several critical factors, including
    the nature and structure of the data, the types of queries (e.g. range queries,
    nearest neighbors, exact search) to be supported, and the volume and growth of
    the data. Additionally, the frequency of data updates (e.g., static vs dynamic)
    the dimensionality of the data, performance requirements (real-time, batch), and
    resource constraints play significant roles in the decision-making process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的索引策略取决于几个关键因素，包括数据的性质和结构、要支持的查询类型（例如范围查询、最近邻查询、精确搜索）、数据的体积和增长、数据更新的频率（例如静态与动态）、数据的维度、性能要求（实时、批量）以及资源限制。此外，数据更新的频率、数据的维度、性能要求（实时、批量）和资源限制在决策过程中也发挥着重要作用。
- en: Similarity measures
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相似度度量
- en: 'Similarity measures dictate how the index is organized, and this makes sure
    that the retrieved data are highly relevant to the query. For instance, in a system
    designed to retrieve similar images, the index might be built around the feature
    vectors of images, and the similarity measure would determine which images are
    “close” or “far” within that indexed space. The importance of these concepts is
    two-fold: indexing significantly speeds up data retrieval, and similarity measures
    ensure that the retrieved data is relevant to the query, together enhancing the
    efficiency and efficacy of data retrieval systems. Selecting an appropriate distance
    metric greatly enhances the performance of classification and clustering tasks.
    The optimal similarity measure is chosen based on the nature of the data input.
    In other words, similarity measures define how closely two items or data points
    are related. They can be broadly classified into **distance metrics** and **similarity
    metrics**. Next, we’ll explore the three top similarity metrics for building AI
    applications: cosine similarity and Euclidean and Manhattan distance.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度度量决定了索引的组织方式，并确保检索到的数据与查询高度相关。例如，在一个旨在检索相似图像的系统设计中，索引可能围绕图像的特征向量构建，而相似度度量将决定哪些图像在该索引空间内是“接近”或“远离”的。这些概念的重要性有两方面：索引显著加快了数据检索速度，相似度度量确保检索到的数据与查询相关，共同提高了数据检索系统的效率和效果。选择合适的距离度量可以显著提高分类和聚类任务的性能。最优的相似度度量是根据数据输入的性质选择的。换句话说，相似度度量定义了两个项目或数据点之间关系的紧密程度。它们可以广泛地分为**距离度量**和**相似度度量**。接下来，我们将探讨构建AI应用时前三个最常用的相似度度量：余弦相似度、欧几里得距离和曼哈顿距离。
- en: '**Similarity metrics – Cosine similarity**: Cosine similarity, a type of similarity
    metric, calculates the cosine value of the angle between two vectors, and OpenAI
    suggests using it for its models to measure the distance between two embeddings
    obtained from text-embedding-ada-002\. The higher the metric, the more similar
    they are:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似度度量 – 余弦相似度**：余弦相似度是一种相似度度量，它计算两个向量之间角度的余弦值。OpenAI建议使用它来衡量从text-embedding-ada-002中获得的两个嵌入之间的距离。度量值越高，它们越相似：'
- en: '![Figure 4.6 – Illustration of relatedness through cosine similarity between
    two words](img/B21443_04_6.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 通过两个单词之间的余弦相似度展示相关性的插图](img/B21443_04_6.jpg)'
- en: Figure 4.6 – Illustration of relatedness through cosine similarity between two
    words
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 通过两个单词之间的余弦相似度展示相关性的插图
- en: The preceding image shows a situation where the cosine similarity is 1 for India
    and the USA because they are related, as both are countries. In the other image,
    the similarity is 0 because football is not similar to a lion.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像显示了当印度和美国相关联时，余弦相似度为 1 的情况，因为它们都是国家。在另一幅图像中，相似度为 0，因为足球与狮子不相似。
- en: '**Distance metrics – Euclidean (L2)**: Euclidean distance computes the straight-line
    distance between two points in Euclidean space. The higher the metric, the less
    similar the two points are:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**距离度量 – 欧几里得距离（L2）**：欧几里得距离计算欧几里得空间中两点之间的直线距离。度量值越高，两点之间的相似度越低：'
- en: '![Figure 4.7 – Illustration of Euclidean distance](img/B21443_04_7.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 欧几里得距离示意图](img/B21443_04_7.jpg)'
- en: Figure 4.7 – Illustration of Euclidean distance
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 欧几里得距离示意图
- en: 'The image illustrates the Euclidean distance formula in a 2D space. It shows
    two points: (x1,y1) and (x2,y2). The preceding formula calculates the straight-line
    distance between the two points in a plane.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像展示了二维空间中的欧几里得距离公式。它显示了两个点：(x1,y1) 和 (x2,y2)。前面的公式计算了平面上两点之间的直线距离。
- en: '**Distance metrics – Manhattan (L1)**: Manhattan distance calculates the sum
    of absolute differences along each dimension. The higher the metric, the less
    similar the differences. The following image depicts the Manhattan distance (or
    L1 distance) between two points in a 2D space, where the distance is measured
    along the axes at right angles, similar to navigating city blocks in a grid-like
    street layout:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**距离度量 – 曼哈顿距离（L1）**：曼哈顿距离计算每个维度上绝对差异的总和。度量值越高，差异的相似度越低。以下图像展示了二维空间中两点之间的曼哈顿距离（或
    L1 距离），距离是沿着直角轴测量的，类似于在网格状街道布局中导航城市街区：'
- en: '![Figure 4.8 – Illustration of Manhattan distance](img/B21443_04_8.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 曼哈顿距离示意图](img/B21443_04_8.jpg)'
- en: Figure 4.8 – Illustration of Manhattan distance
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 曼哈顿距离示意图
- en: You might be wondering when to select one metric over another during the development
    of generative AI applications. The decision on which similarity measure to use
    hinges on various elements, such as the type of data, the context of the application,
    and the bespoke demands of the analysis results.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道在开发生成式 AI 应用程序时，何时选择一种度量标准而不是另一种。关于使用哪种相似度测量的决定取决于各种元素，例如数据的类型、应用的上下文以及分析结果的定制需求。
- en: Cosine similarity is preferred over Manhattan and Euclidean distances when the
    magnitude of the data vectors is less relevant than the direction or orientation
    of the data. In text analysis, for example, two documents might be represented
    by highly dimensional vectors of word frequencies. If one document is a longer
    version of the other, their word frequency vectors will point in the same direction,
    but the magnitude (length) of one vector will be larger due to the higher word
    count. Using Euclidean or Manhattan distance would highlight these differences
    in magnitude, suggesting the documents are different. However, using cosine similarity
    would capture their similarity in content (the direction of the vectors), de-emphasizing
    the differences in word count. In this context, cosine similarity is more appropriate,
    as it focuses on the angle between the vectors, reflecting the content overlap
    of the documents rather than their length or magnitude.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据向量的幅度相对于方向或数据取向不那么重要时，余弦相似度比曼哈顿和欧几里得距离更受欢迎。例如，在文本分析中，两篇文档可能由表示单词频率的高维向量表示。如果一篇文档是另一篇文档的较长版本，它们的单词频率向量将指向同一方向，但由于单词计数更高，一个向量的幅度（长度）会更大。使用欧几里得或曼哈顿距离会突出这些幅度差异，表明文档不同。然而，使用余弦相似度将捕捉它们在内容（向量的方向）上的相似性，淡化单词计数上的差异。在这种情况下，余弦相似度更合适，因为它关注向量之间的角度，反映了文档的内容重叠，而不是它们的长度或幅度。
- en: Euclidean and Manhattan distances are more apt than cosine similarity when the
    magnitude and absolute differences between data vectors are crucial, such as with
    consistent scaled numerical data (e.g., age, height, weight, and so on) or in
    spatial applications such as grid-based pathfinding. While cosine similarity emphasizes
    the orientation or pattern of data vectors, which is especially useful in highly
    dimensional, sparse datasets, Euclidean and Manhattan distances capture the actual
    differences between data points, making them preferable in scenarios where absolute
    value deviations are significant such as when comparing the medical test results
    of patients or finding the distance between geographical co-ordinates on earth.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据向量的幅度和绝对差异至关重要时，例如在具有一致缩放数值数据（例如年龄、身高、体重等）或空间应用（如基于网格的路径查找）中，欧几里得距离和曼哈顿距离比余弦相似度更合适。虽然余弦相似度强调数据向量的方向或模式，这在高维、稀疏数据集中特别有用，但欧几里得距离和曼哈顿距离捕捉数据点之间的实际差异，因此在绝对值偏差显著的场景中更受欢迎，例如比较患者的医学检测结果或找到地球上地理坐标之间的距离。
- en: 'The following is a snippet of code that uses Azure OpenAI endpoints to calculate
    the similarity between two sentences: “What number of countries do you know?”
    and “How many countries are you familiar with?” by using embedding model text-embedding-ada-002\.
    It gives a score of 0.95:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用 Azure OpenAI 端点通过嵌入模型 text-embedding-ada-002 计算两个句子“你知道多少个国家？”和“你熟悉多少个国家？”之间相似度的代码片段。它给出了
    0.95 的分数：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now let us walkthrough a scenario where Cosine Similarity will be preferred
    over Manhattan distance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个场景来了解为什么在余弦相似度和曼哈顿距离之间，余弦相似度会被优先考虑。
- en: Recommendation System for Articles
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**文章推荐系统**'
- en: Let's consider a scenario where a news aggregation platform aims to recommend
    articles similar to what a user is currently reading, enhancing user engagement
    by suggesting relevant content.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景，一个新闻聚合平台旨在推荐与用户当前阅读的文章相似的文章，通过建议相关内容来增强用户参与度。
- en: 'How It Works:'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**工作原理**：'
- en: '**Preprocessing and Indexing**: Articles in the platform’s database are processed
    to extract textual features, often converted into high-dimensional vectors using
    LDA or transformer based embeddings like text-ada-embedding-002\. These vectors
    are then indexed using HNSW, an algorithm suitable for high-dimensional spaces
    due to its hierarchical structure that facilitates efficient navigation and search.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理和索引**：平台数据库中的文章经过处理以提取文本特征，通常使用 LDA 或基于 transformer 的嵌入（如 text-ada-embedding-002）将其转换为高维向量。然后，使用
    HNSW 索引对这些向量进行索引，HNSW 是一种适合高维空间的算法，由于其层次结构，它促进了高效的导航和搜索。'
- en: '**Retrieval Time**: When a user reads an article, the system generates a feature
    vector for this article and queries the HNSW index to find vectors (and thus articles)
    that are close in the high-dimensional space. Cosine similarity can be used to
    evaluate the similarity between the query article’s vector and those in the index,
    identifying articles with similar content.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索时间**：当用户阅读一篇文章时，系统会为这篇文章生成一个特征向量，并查询 HNSW 索引以找到在高维空间中与之接近的向量（以及相应的文章）。可以使用余弦相似度来评估查询文章的向量与索引中的向量之间的相似度，从而识别出内容相似的文章。'
- en: '**Outcome**: The system recommends a list of articles ranked by their relevance
    to the currently viewed article. Thanks to the efficient indexing and similarity
    search, these recommendations are generated quickly, even from a vast database
    of articles, providing the user with a seamless experience.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：系统根据与当前查看文章的相关性推荐一系列文章。得益于高效的索引和相似度搜索，即使是从庞大的文章数据库中，这些推荐也能快速生成，为用户提供无缝的体验。'
- en: Now let us walkthrough a scenario where Manhattan Distance will be preferred
    over Cosine Similarity.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个场景来了解为什么在曼哈顿距离和余弦相似度之间，曼哈顿距离会被优先考虑。
- en: Ride-Sharing App Matchmaking
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**拼车应用匹配**'
- en: Let's consider a scenario where a ride-sharing application needs to match passengers
    with nearby drivers efficiently. The system must quickly find the closest available
    drivers to a passenger’s location to minimize wait times and optimize routes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景，一个拼车应用需要高效地将乘客与附近的司机匹配。系统必须快速找到离乘客位置最近的可用司机，以最小化等待时间并优化路线。
- en: 'How It Works:'
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**工作原理**：'
- en: '**Preprocessing and Indexing**: Drivers’ current locations are constantly being
    updated and stored as points in a 2D space representing a map. These points can
    be indexed using a tree based spatial indexing techniques or data structures optimized
    for geospatial data, such as R-trees.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理和索引**：司机的当前位置不断更新并存储为表示地图的二维空间中的点。这些点可以使用基于树的空間索引技术或针对地理空间数据优化的数据结构（如R树）进行索引。'
- en: '**Retrieval Time**: When a passenger requests a ride, the application uses
    the passenger’s current location as a query point. Manhattan distance (L1 norm)
    is particularly suitable for urban environments, where movement is constrained
    by a grid-like structure of streets and avenues, mimicking the actual paths a
    car would take along city blocks.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索时间**：当乘客请求乘车时，应用程序使用乘客的当前位置作为查询点。曼哈顿距离（L1范数）特别适合城市环境，因为街道和大道的网格状结构限制了移动，模仿了汽车在城市街区中实际行驶的路径。'
- en: '**Outcome**: The system quickly identifies the nearest available drivers using
    the indexed data and Manhattan distance calculations, considering the urban grid’s
    constraints. This process ensures a swift matchmaking process, improving the user
    experience by reducing wait times.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：系统快速识别最近的可用司机，使用索引数据和曼哈顿距离计算，考虑到城市网格的约束。这个过程确保了快速匹配过程，通过减少等待时间来提高用户体验。'
- en: Vector stores
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量存储
- en: As generative AI applications continue to push the boundaries of what’s possible
    in tech, vector stores have emerged as a crucial component, streamlining and optimizing
    the search and retrieval of relevant data. In our previous discussions, we’ve
    delved into the advantages of vector DBs over traditional databases, unraveling
    the concepts of vectors, embeddings, vector search strategies, approximate nearest
    neighbors (ANNs), and similarity measures. In this section, we aim to provide
    an integrative understanding of these concepts within the realm of vector DBs
    and libraries.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成式人工智能应用不断推动技术在可能性的边界，向量存储已成为一个关键组件，简化并优化了相关数据的搜索和检索。在我们之前的讨论中，我们深入探讨了向量数据库相对于传统数据库的优势，解开了向量、嵌入、向量搜索策略、近似最近邻（ANNs）和相似度度量的概念。在本节中，我们旨在提供对向量数据库和库中这些概念的综合性理解。
- en: The image illustrates a workflow for transforming different types of data—Audio,
    Text, and Videos—into vector embeddings.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像展示了将不同类型的数据——音频、文本和视频——转换为向量嵌入的工作流程。
- en: '**Audio**: An audio input is processed through an “Audio Embedding model,”
    resulting in “Audio vector embeddings.”'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音频**：音频输入通过“音频嵌入模型”进行处理，产生“音频向量嵌入”。'
- en: '**Text**: Textual data undergoes processing in a “Text Embedding model,” leading
    to “Text vector embeddings.”'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本**：文本数据在“文本嵌入模型”中进行处理，导致“文本向量嵌入”。'
- en: '**Videos**: Video content is processed using a “Video Embedding model,” generating
    “Video vector embeddings.”'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频**：视频内容使用“视频嵌入模型”进行处理，生成“视频向量嵌入”。'
- en: Once these embeddings are created, they are subsequently utilized (potentially
    in an enterprise vector database system) to perform “Similarity Search” operations.
    This implies that the vector embeddings can be compared to find similarities,
    making them valuable for tasks such as content recommendations, data retrieval,
    and more.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了这些嵌入，它们随后（可能在企业向量数据库系统中）被用于执行“相似度搜索”操作。这意味着向量嵌入可以进行比较以找到相似性，这使得它们对于内容推荐、数据检索等任务非常有价值。
- en: '![Figure 4.9 – Multimodal embeddings process in an AI application](img/B21443_04_9.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图4.9 – 人工智能应用中的多模态嵌入过程](img/B21443_04_9.jpg)'
- en: Figure 4.9 – Multimodal embeddings process in an AI application
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 人工智能应用中的多模态嵌入过程
- en: What is a vector database?
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是向量数据库？
- en: A **vector database** (**vector DB**) is a specialized database designed to
    handle highly dimensional vectors primarily generated from embeddings of complex
    data types such as text, images, or audio. It provides capabilities to store and
    index unstructured data and enhance searches, as well as retrieval capabilities
    as a service.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量数据库**（**向量数据库**）是一种专门设计的数据库，用于处理高度维度的向量，这些向量主要来自文本、图像或音频等复杂数据类型的嵌入。它提供了存储和索引非结构化数据以及增强搜索和检索功能的能力。'
- en: 'Modern vector databases that are brimming with advancements empower you to
    architect resilient enterprise solutions. Here, we list 15 key features to consider
    when choosing a vector DB. Every feature may not be important for your use case,
    but it might be a good place to start. Keep in mind that this area is changing
    fast, so there might be more features emerging in the future:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 满载着先进技术的现代向量数据库能够帮助您构建弹性的企业级解决方案。在此，我们列出在选择向量数据库时需要考虑的15个关键特性。并非每个特性对您的用例都至关重要，但这是一个良好的起点。请记住，这个领域发展迅速，未来可能会有更多特性出现：
- en: '**Indexing**: As mentioned earlier, indexing refers to the process of organizing
    highly dimensional vectors in a way that allows for efficient similarity searches
    and retrievals. A vector DB offers built-in indexing features designed to arrange
    highly dimensional vectors for swift and effective similarity-based searches and
    retrievals. Previously, we discussed indexing algorithms such as FAISS and HNSW.
    Many vector DBs incorporate such features natively. For instance, Azure AI Search
    integrates the HNSW indexing service directly.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引**：如前所述，索引是指以允许高效进行相似度搜索和检索的方式组织高维向量的过程。向量数据库提供内置的索引功能，旨在为快速和有效的基于相似度的搜索和检索排列高维向量。以前，我们讨论了FAISS和HNSW等索引算法。许多向量数据库原生地集成了这些功能。例如，Azure
    AI Search直接集成了HNSW索引服务。'
- en: '**Search and retrieval**: Instead of relying on exact matches, as traditional
    databases do, vector DBs provide vector search capabilities as a service, such
    as approximate nearest neighbors (ANNs), to quickly find vectors that are roughly
    the closest to a given input. To quantify the closeness or similarity between
    vectors, they utilize similarity measures such as cosine similarity or Euclidean
    distance, enabling efficient and nuanced similarity-based searches in large datasets.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索和检索**：与传统的数据库依赖精确匹配不同，向量数据库提供向量搜索功能作为服务，例如近似最近邻（ANNs），以快速找到与给定输入大致最接近的向量。为了量化向量之间的接近度或相似度，它们利用诸如余弦相似度或欧几里得距离等相似度度量，从而在大型数据集中实现高效且细致的基于相似度的搜索。'
- en: '**Create, read, update, and delete**: A vector DB manages highly dimensional
    vectors and offers create, read, update, and delete (CRUD) operations tailored
    to vectorized data. When vectors are created, they’re indexed for efficient retrieval.
    Reading often means performing similarity searches to retrieve vectors closest
    to a given query vector, typically using methods such as ANNs. Vectors can be
    updated, necessitating potential re-indexing, and they can also be deleted, with
    the database adjusting its internal structures accordingly to maintain efficiency
    and consistency.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建、读取、更新和删除**：向量数据库管理高维向量，并提供针对向量数据定制的创建、读取、更新和删除（CRUD）操作。当创建向量时，它们会被索引以实现高效的检索。读取通常意味着执行相似度搜索以检索与给定查询向量最接近的向量，通常使用ANNs等方法。向量可以被更新，可能需要重新索引，并且它们也可以被删除，数据库会相应地调整其内部结构以保持效率和一致性。'
- en: '**Security**: This meets GDPR, SOC2 Type II, and HIPAA rules to easily manage
    access to the console and use SSO. Data is encrypted when stored and in transit,
    which also provides more granular identity and access management features.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：这符合GDPR、SOC2 Type II和HIPAA规则，可以轻松管理对控制台的访问并使用SSO。数据在存储和传输过程中都进行加密，这还提供了更细粒度的身份和访问管理功能。'
- en: '**Serverless**: A high-quality vector database is designed to gracefully autoscale
    with low management overhead as data volumes soar into millions or billions of
    entries, distributing seamlessly across several nodes. **Optimal vector** databases
    grant users the flexibility to adjust the system in response to shifts in data
    insertion, query frequencies, and underlying hardware configurations.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器**：高质量的向量数据库设计为随着数据量激增至数百万或数十亿条记录时优雅地自动扩展，以低管理开销无缝分布在多个节点上。**最佳向量**数据库授予用户根据数据插入、查询频率和底层硬件配置的变化调整系统的灵活性。'
- en: '**Hybrid search**: Hybrid search combines traditional keyword-based search
    methods with other search mechanisms, such as semantic or contextual search, to
    retrieve results from both the exact term matches and by understanding the underlying
    intent or context of the query, ensuring a more comprehensive and relevant set
    of results.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合搜索**：混合搜索结合了传统的基于关键词的搜索方法和其他搜索机制，例如语义或上下文搜索，以从精确匹配的术语和通过理解查询的潜在意图或上下文来检索结果，确保更全面和相关的结果集。'
- en: '**Semantic re-ranking**: This is a secondary ranking step to improve the relevance
    of search results. It re-ranks the search results that were initially scored by
    state-of-the-art ranking algorithms such as BM25 and RRF based on language understanding.
    For instance, Azure AI search employs secondary ranking that uses multi-lingual,
    deep learning models derived from Microsoft Bing to elevate the results that are
    most relevant in terms of meaning.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义重排序**：这是一个次要的排序步骤，用于提高搜索结果的相关性。它根据语言理解对最初由最先进的排序算法（如BM25和RRF）评分的搜索结果进行重新排序。例如，Azure
    AI搜索采用基于从Microsoft Bing衍生出的多语言、深度学习模型的二级排序，以提高在意义上最相关的结果。'
- en: '**Auto vectorization/embedding**: Auto-embedding in a vector database refers
    to the automatic process of converting data items into vector representations
    for efficient similarity searches and retrieval, with access to multiple embedding
    models.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动向量化/嵌入**：向量数据库中的自动嵌入是指将数据项自动转换为向量表示的过程，以实现高效的相似性搜索和检索，并访问多个嵌入模型。'
- en: '**Data replication**: This ensures data availability, redundancy, and recovery
    in case of failures, safeguarding business continuity and reducing data loss risks.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据复制**：这确保了数据可用性、冗余和恢复，以防发生故障，保护业务连续性并降低数据丢失风险。'
- en: '**Concurrent user access and data isolation**: Vector databases support a large
    number of users concurrently and ensure robust data isolation to ensure updates
    remain private unless deliberately shared.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发用户访问和数据隔离**：向量数据库支持大量用户同时访问，并确保强大的数据隔离，以确保更新保持私密，除非故意共享。'
- en: '**Auto-chunking**: Auto-chunking is the automated process of dividing a larger
    set of data or content into smaller, manageable pieces or chunks for easier processing
    or understanding. This process helps preserve the semantic relevance of texts
    and addresses the token limitations of embedding models. We will learn more about
    chunking strategies in the upcoming sections in this chapter.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动分块**：自动分块是将较大的数据集或内容自动分割成更小、更易于管理的块或片段的过程，以便更容易处理或理解。这个过程有助于保留文本的语义相关性，并解决嵌入模型的标记限制。我们将在本章接下来的部分中了解更多关于分块策略的内容。'
- en: '**Extensive interaction tools**: Prominent vector databases, such as Pinecone,
    offer versatile APIs and SDKs across languages, ensuring adaptability in integration
    and management.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛的交互工具**：突出的向量数据库，如Pinecone，提供跨语言的灵活API和SDK，确保在集成和管理方面的适应性。'
- en: '**Easy integration**: Vector DBs provide seamless integration with LLM orchestration
    frameworks and SDKs, such as Langchain and Semantic Kernel, and leading cloud
    providers, such as Azure, GCP, and AWS.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于集成**：向量数据库提供与LLM编排框架和SDK（如Langchain和Semantic Kernel）以及领先的云服务提供商（如Azure、GCP和AWS）的无缝集成。'
- en: '**User-friendly interface**: This ensures an intuitive platform with simple
    navigation and direct feature access, streamlining the user experience.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户友好的界面**：这确保了一个直观的平台，具有简单的导航和直接的功能访问，简化了用户体验。'
- en: '**Flexible pricing models**: Provides flexible pricing models as per user needs
    to keep the costs low for the user.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的定价模型**：根据用户需求提供灵活的定价模型，以保持用户成本较低。'
- en: '**Low downtime and high resiliency**: Resiliency in a vector database (or any
    database) refers to its ability to recover quickly from failures, maintain data
    integrity, and ensure continuous availability even in the face of adverse conditions,
    such as hardware malfunctions, software bugs, or other unexpected disruptions.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低停机时间和高弹性**：向量数据库（或任何数据库）的弹性是指其从故障中快速恢复的能力、维护数据完整性和确保即使在不利条件下（如硬件故障、软件错误或其他意外中断）也能持续可用。'
- en: As of early 2024, a few prominent open source vector databases include Chroma,
    Milvus, Quadrant, and Weaviate, while Pinecone and Azure AI search are among the
    leading proprietary solutions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2024年初，一些突出的开源向量数据库包括Chroma、Milvus、Quadrant和Weaviate，而Pinecone和Azure AI搜索则是领先的专有解决方案之一。
- en: Vector DB limitations
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库局限性
- en: '**Accuracy vs. speed trade-off**: When dealing with highly dimensional data,
    vector DBs often face a trade-off between speed and accuracy for similarity searches.
    The core challenge stems from the computational expense of searching for the exact
    nearest neighbors in large datasets. To enhance search speed, techniques such
    as ANNs are employed, which quickly identify “close enough” vectors rather than
    the exact matches. While ANN methods can dramatically boost query speeds, they
    may sometimes sacrifice pinpoint accuracy, potentially missing the true nearest
    vectors. Certain vector index methods, such as product quantization, enhance storage
    efficiency and accelerate queries by condensing and consolidating data at the
    expense of accuracy.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性与速度的权衡**：在处理高维数据时，向量数据库在相似性搜索中经常面临速度和准确性的权衡。核心挑战来自于在大数据集中搜索精确最近邻的计算成本。为了提高搜索速度，采用了诸如ANN等技术，这些技术快速识别“足够接近”的向量而不是精确匹配。虽然ANN方法可以显著提高查询速度，但它们有时可能会牺牲精确度，可能错过真正的最近向量。某些向量索引方法，如产品量化，通过压缩和合并数据来提高存储效率并加速查询，但以牺牲准确性为代价。'
- en: '**Quality of embedding**: The effectiveness of a vector database is dependent
    on the quality of the vector embedding used. Poorly designed embeddings can lead
    to inaccurate search results or missed connections.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入质量**：向量数据库的有效性取决于所使用的向量嵌入的质量。设计不当的嵌入可能导致搜索结果不准确或错过连接。'
- en: '**Complexity**: Implementing and managing vector databases can be complex,
    requiring specialized knowledge about vector search strategy indexing and chunking
    strategies to optimize for specific use cases.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：实现和管理向量数据库可能很复杂，需要关于向量搜索策略、索引和分块策略的专门知识，以优化特定用例。'
- en: Vector libraries
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量库
- en: Vector databases may not always be necessary. Small-scale applications may not
    require all the advanced features that vector DBs provide. In those instances,
    vector libraries become very valuable. Vector libraries are usually sufficient
    for small, static data and provide the ability to store in memory, index, and
    use similarity search strategies. However, they may not provide features such
    as CRUD support, data replication, and being able to store data on disk, and hence,
    the user will have to wait for a full import before they can query. Facebook’s
    FAISS is a popular example of a vector library.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库不一定总是必要的。小型应用可能不需要向量数据库提供的所有高级功能。在这些情况下，向量库变得非常有价值。向量库通常足够用于小型、静态数据，并提供在内存中存储、索引和使用相似性搜索策略的能力。然而，它们可能不提供诸如CRUD支持、数据复制以及能够在磁盘上存储数据等功能，因此用户在能够查询之前必须等待完整导入。Facebook的FAISS是向量库的一个流行例子。
- en: As a rule of thumb, if you are dealing with millions/billions of records and
    storing data that are changing frequently, require millisecond response times,
    and more long-term storage capabilities on disk, it is recommended to use vector
    DBs over vector libraries.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，如果你正在处理数百万/数十亿条记录，存储频繁变化的数据，需要毫秒级响应时间，以及更多长期存储能力在磁盘上，建议使用向量数据库而不是向量库。
- en: Vector DBs vs. traditional databases – Understanding the key differences
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库与传统数据库的比较 – 理解关键差异
- en: As stated earlier, vector databases have become pivotal, especially in the era
    of generative AI, because they facilitate efficient storage, querying, and retrieval
    of highly dimensional vectors that are nothing but numerical representations of
    words or sentences often produced by deep learning models. Traditional scalar
    databases are designed to handle discrete and simple data types, making them ill-suited
    for the complexities of large-scale vector data. In contrast, vector databases
    are optimized for similar searches in the vector space, enabling the rapid identification
    of vectors that are “close” or “similar” in highly dimensional spaces. Unlike
    conventional data models such as relational databases, where queries commonly
    resemble “retrieve the books borrowed by a particular member” or “identify the
    items currently discounted,” vector queries primarily seek similarities among
    vectors based on one or more reference vectors. In other words, queries might
    look like “identify the top 10 images of dogs similar to the dog in this photo”
    or “locate the best cafes near my current location.” At retrieval time, vector
    databases are crucial, as they facilitate the swift and precise retrieval of relevant
    document embeddings to augment the generation process. This technique is also
    called RAG, and we will learn more about it in the later sections.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，向量数据库已成为关键，尤其是在生成式人工智能时代，因为它们促进了高度维向量的高效存储、查询和检索，而这些向量不过是深度学习模型经常产生的单词或句子的数值表示。传统的标量数据库旨在处理离散和简单的数据类型，这使得它们不适合大规模向量数据的复杂性。相比之下，向量数据库针对向量空间中的相似搜索进行了优化，能够在高维空间中快速识别“接近”或“相似”的向量。与关系数据库等传统数据模型不同，查询通常类似于“检索特定成员借阅的书籍”或“识别当前打折的商品”，向量查询主要基于一个或多个参考向量在向量之间寻找相似性。换句话说，查询可能看起来像“识别与这张照片中的狗最相似的顶级
    10 张狗的图像”或“定位我当前位置附近最好的咖啡馆。”在检索时间，向量数据库至关重要，因为它们促进了相关文档嵌入的快速精确检索，以增强生成过程。这种技术也称为
    RAG，我们将在后面的章节中了解更多关于它的内容。
- en: Imagine you have a database of fruit images, and each image is represented by
    a vector (a list of numbers) that describes its features. Now, let’s say you have
    a photo of an apple, and you want to find similar fruits in your database. Instead
    of going through each image individually, you convert your apple photo into a
    vector using the same method you used for the other fruits. With this apple vector
    in hand, you search the database to find vectors (and therefore images) that are
    most similar or closest to your apple vector. The result would likely be other
    apple images or fruits that look like apples based on the vector representation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个水果图像数据库，每个图像都由一个向量（一组数字）表示，该向量描述了其特征。现在，假设你有一张苹果的照片，你想要在数据库中找到类似的水果。你不需要逐个检查每张图片，而是使用与其他水果相同的方法将你的苹果照片转换为向量。有了这个苹果向量，你可以在数据库中搜索与你的苹果向量最相似或最接近的向量（因此是图像）。结果很可能是其他苹果图像或根据向量表示看起来像苹果的水果。
- en: '![Figure 4.10 – Vector represenation](img/B21443_04_10.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 向量表示](img/B21443_04_10.jpg)'
- en: Figure 4.10 – Vector represenation
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 向量表示
- en: Vector DB sample scenario – Music recommendation system using a vector database
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库示例场景 – 使用向量数据库的音乐推荐系统
- en: Let’s consider a music streaming platform aiming to provide song recommendations
    based on a user’s current listening. Imagine a user who is listening to “Song
    X” on the platform.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个音乐流媒体平台，该平台旨在根据用户的当前收听习惯提供歌曲推荐。想象一下，一个用户正在平台上收听“歌曲 X”。
- en: Behind the scenes, every song in the platform’s library is represented as a
    highly dimensional vector based on its musical features and content, using embeddings.
    “Song X” also has its vector representation. When the system aims to recommend
    songs similar to “Song X,” it doesn’t look for exact matches (as traditional databases
    might). Instead, it leverages a vector DB to search for songs with vectors closely
    resembling that of “Song X.” Using an ANN search strategy, the system quickly
    sifts through millions of song vectors to find those that are approximately nearest
    to the vector of “Song X.” Once potential song vectors are identified, the system
    employs similarity measures, such as cosine similarity, to rank these songs based
    on how close their vectors are to “Song X’s” vector. The top-ranked songs are
    then recommended to the user.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，平台库中的每首歌曲都根据其音乐特征和内容，使用嵌入表示为一个高度维度的向量。“歌曲X”也有其向量表示。当系统旨在推荐与“歌曲X”相似的歌曲时，它不会寻找精确匹配（如传统数据库可能做的那样）。相反，它利用向量数据库来搜索与“歌曲X”的向量非常相似的歌曲。使用ANN搜索策略，系统快速筛选数百万首歌曲向量，以找到那些与“歌曲X”的向量大致最近的向量。一旦确定了潜在的曲目向量，系统就使用相似度度量，如余弦相似度，根据这些歌曲的向量与“歌曲X”的向量接近程度对它们进行排名。然后，排名最高的歌曲被推荐给用户。
- en: Within milliseconds, the user gets a list of songs that musically resemble “Song
    X,” providing a seamless and personalized listening experience. All this rapid,
    similarity-based recommendation magic is powered by the vector database’s specialized
    capabilities.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在毫秒内，用户就能获得一系列与“歌曲X”音乐上相似的歌曲列表，提供无缝且个性化的听觉体验。所有这些基于快速相似性推荐的魔法都是由向量数据库的专用功能驱动的。
- en: Common vector DB applications
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的向量数据库应用
- en: '**Image and video similarity search**: In the context of image and video similarity
    search, a vector DB specializes in efficiently storing and querying highly dimensional
    embeddings derived from multimedia content. By processing images through deep
    learning models, they are converted into feature vectors, a.k.a embeddings, that
    capture their essential characteristics. When it comes to videos, an additional
    step may need to be carried out to extract frames and then convert them into vector
    embeddings. Contrastive language-image pre-training (CLIP) from OpenAI is a very
    popular choice for embedding videos and images. These vector embeddings are indexed
    in the vector DB, allowing for rapid and precise retrieval when a user submits
    a query. This mechanism powers applications such as reverse image and video search,
    content recommendations, and duplicate detection by comparing and ranking content
    based on the proximity of their embeddings.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像和视频相似度搜索**：在图像和视频相似度搜索的背景下，一个向量数据库专门用于高效存储和查询从多媒体内容中提取的高度维度的嵌入。通过深度学习模型处理图像，它们被转换为特征向量，即嵌入，这些向量捕捉了它们的本质特征。对于视频来说，可能需要额外一步来提取帧，然后将它们转换为向量嵌入。来自OpenAI的对比语言-图像预训练（CLIP）是嵌入视频和图像的一个非常流行的选择。这些向量嵌入在向量数据库中索引，允许在用户提交查询时快速准确地检索。这种机制为反向图像和视频搜索、内容推荐和基于嵌入相似度比较和排名的重复检测等应用提供了动力。'
- en: '**Voice recognition**: Voice recognition with vectors is akin to video vectorization.
    Analog audio is digitized into short frames, each representing an audio segment.
    These frames are processed and stored as feature vectors, with the entire audio
    sequence representing things such as spoken sentences or songs. For user authentication,
    a vectorized spoken key phrase might be compared to stored recordings. In conversational
    agents, these vector sequences can be inputted into neural networks to recognize
    and classify spoken words in speech and generate responses, similar to ChatGPT.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：使用向量的语音识别类似于视频向量化。模拟音频被数字化为短帧，每帧代表一个音频段。这些帧被处理并存储为特征向量，整个音频序列代表诸如说话句子或歌曲之类的事物。对于用户身份验证，可能将向量化的说话短语与存储的录音进行比较。在对话代理中，这些向量序列可以被输入到神经网络中，以识别和分类语音中的说话词并生成响应，类似于ChatGPT。'
- en: '**Long-term memory for chatbots**: Virtual database management systems (VDBMs)
    can be employed to enhance the long-term memory capabilities of chatbots or generative
    models. Many generative models can only process a limited amount of preceding
    text in prompt responses, which results in their inability to recall details from
    prolonged conversations. As these models don’t have inherent memory of past interactions
    and can’t differentiate between factual data and user-specific details, using
    VDBMs can provide a solution for storing, indexing, and referencing previous interactions
    to improve consistency and context-awareness in responses.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长期记忆对于聊天机器人**：虚拟数据库管理系统（VDBMs）可以被用来增强聊天机器人或生成模型的长期记忆能力。许多生成模型在提示响应中只能处理有限的前置文本，这导致它们无法回忆起长时间对话中的细节。由于这些模型没有过去交互的内在记忆，无法区分事实数据和用户特定细节，使用VDBMs可以为存储、索引和引用以前的交互提供解决方案，从而提高响应的一致性和上下文感知性。'
- en: This is a very important use case and plays a key role in implementing RAG,
    which we will discuss in the next section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的用例，在实现RAG中扮演着关键角色，我们将在下一节中讨论。
- en: The role of vector DBs in retrieval-augmented generation (RAG)
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量数据库在检索增强生成（RAG）中的作用
- en: To fully understand RAG and the pivotal role of vector DBs within it, we must
    first acknowledge the inherent constraints of LLMs, which paved the way for the
    advent of RAG techniques powered by vector DBs. This section sheds light on the
    specific LLM challenges that RAG aims to overcome and the importance of vector
    DBs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全理解RAG以及向量数据库在其中的关键作用，我们首先必须承认LLMs的固有限制，这为基于向量数据库的RAG技术的出现铺平了道路。本节将阐明RAG旨在克服的具体LLM挑战以及向量数据库的重要性。
- en: First, the big question – Why?
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首先，最大的问题——为什么？
- en: 'In [*Chapter 1*](B21443_01.xhtml#_idTextAnchor015), we delved into the limitations
    of LLMs, which include the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B21443_01.xhtml#_idTextAnchor015)中，我们探讨了LLMs的限制，包括以下内容：
- en: LLMs possess a fixed knowledge base determined by their training data; as of
    February 2024, ChatGPT’s knowledge is limited to information up until April 2023.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs拥有由其训练数据决定的固定知识库；截至2024年2月，ChatGPT的知识仅限于截至2023年4月的信息。
- en: LLMs can occasionally produce false narratives, spinning tales or facts that
    aren’t real.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs can occasionally produce false narratives, spinning tales or facts that
    aren’t real.
- en: They lack personal memory, relying solely on the input context length. For example,
    take GPT4-32K; it can only process up to 32K tokens between prompts and completions
    (we’ll dive deeper into prompts, completions, and tokens in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098)).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们缺乏个人记忆，完全依赖于输入上下文的长度。例如，以GPT4-32K为例；它只能在提示和完成之间处理最多32K个标记（我们将在[*第五章*](B21443_05.xhtml#_idTextAnchor098)中更深入地探讨提示、完成和标记）。
- en: To counter these challenges, a promising avenue is enhancing LLM generation
    with retrieval components. These components can extract pertinent data from external
    knowledge bases—a process termed RAG, which we’ll explore further in this section.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，一个有前景的途径是增强LLM生成中的检索组件。这些组件可以从外部知识库中提取相关数据——这个过程被称为RAG，我们将在本节中进一步探讨。
- en: So, what is RAG, and how does it help LLMs?
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，RAG是什么，它如何帮助LLMs？
- en: Retrieval-augmented generation (RAG) was first introduced in a paper ti[tled
    *Retrieval-Augmented Generation*](https://arxiv.org/pdf/2005.11401.pdf) *for Knowledge-Intensive
    NLP Tasks* ([https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf))
    in November 2020 by Facebook AI Research (now Meta). RAG is an approach that combines
    the generative capabilities of LLMs with retrieval mechanisms to extract relevant
    information from vast datasets. LLMs, such as the GPT variants, have the ability
    to generate human-like text based on patterns in their training data but lack
    the means to perform real-time external lookups or reference specific external
    knowledge bases post-training. RAG addresses this limitation by using a retrieval
    model to query a dataset and fetch relevant information, which then serves as
    the context for the generative model to produce a detailed and informed response.
    This also helps in grounding the LLM queries with relevant information that reduces
    the chances of hallucinations.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）首次在2020年11月由Facebook AI Research（现Meta）发表的一篇题为 *检索增强生成*（[Retrieval-Augmented
    Generation](https://arxiv.org/pdf/2005.11401.pdf)）的论文中提出，用于知识密集型自然语言处理任务（[https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)）。RAG是一种结合了大型语言模型（LLMs）的生成能力和检索机制，从大量数据集中提取相关信息的方法。LLMs，如GPT变体，能够根据其训练数据中的模式生成类似人类的文本，但缺乏进行实时外部查找或参考特定外部知识库的手段。RAG通过使用检索模型查询数据集并获取相关信息来解决这个问题，这些信息随后作为生成模型的上下文，以生成详细和有信息量的响应。这也帮助将LLM查询与相关信息联系起来，减少了幻觉的可能性。
- en: The critical role of vector DBs
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库的关键作用
- en: A vector DB plays a crucial role in facilitating the efficient retrieval aspect
    of RAG. In this setup, each piece of information, such as text, video, or audio,
    in the dataset is represented as a highly dimensional vector and indexed in a
    vector DB. When a query from a user comes in, it’s also converted into a similar
    vector representation. The vector DB then rapidly searches for vectors (documents)
    in the dataset that are closest to the query vector, leveraging techniques such
    as ANN search. Then, it attaches the query with relevant content and sends it
    to the LLMs to generate a response. This ensures that the most relevant information
    is retrieved quickly and efficiently, providing a foundation for the generative
    model to build upon.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库在促进RAG的高效检索方面发挥着关键作用。在这个设置中，数据集中的每一项信息，如文本、视频或音频，都表示为一个高维向量，并在向量数据库中进行索引。当用户查询到来时，它也被转换为类似的向量表示。然后，向量数据库快速搜索与查询向量最接近的数据集中的向量（文档），利用如ANN搜索等技术。然后，它将查询与相关内容相关联，并将其发送到LLMs以生成响应。这确保了最相关的信息能够快速有效地检索到，为生成模型提供基础。
- en: Example of an RAG workflow
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RAG工作流程的示例
- en: 'Let’s walk through as an example step by step, as shown in the image. Imagine
    a platform where users can ask about ongoing cricket matches, including recent
    performances, statistics, and trivia:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以图像中所示为例，一步一步地走一遍。想象一个平台，用户可以询问正在进行的板球比赛，包括最近的表现、统计数据和趣闻：
- en: Suppose the user asks, “How did Virat Kohli perform in the last match, and what’s
    an interesting fact from that game?” Since the LLM was trained until April 2023,
    the LLM may not have this answer.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设用户问，“维拉特·科赫利在上一场比赛中的表现如何，这场比赛有什么有趣的事实？”由于LLM的训练截止到2023年4月，LLM可能没有这个答案。
- en: The retrieval model will embed the query and send it to a vector DB.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索模型将嵌入查询并发送到向量数据库。
- en: All the latest cricket news is stored in a vector DB in a properly indexed format
    using ANN strategies such as HNSW. The vector DB performs a cosine similarity
    with the indexed information and provides a few relevant results or contexts.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有最新的板球新闻都存储在一个使用HNSW等ANN策略进行适当索引的向量数据库中。向量数据库与索引信息执行余弦相似度，并提供一些相关结果或上下文。
- en: The retrieved context is then sent to the LLM along with the query to synthesize
    the information and provide a relevant answer.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将检索到的上下文与查询一起发送到LLM，以综合信息并提供相关答案。
- en: 'The LLM provides the relevant answer: “Virat Kohli scored 85 runs off 70 balls
    in the last match. An intriguing detail from that game is that it was the first
    time in three years that he hit more than seven boundaries in an ODI inning.”'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM提供了相关的答案：“维拉特·科赫利在上一场比赛中70球得到85分。这场比赛的一个有趣细节是，这是他三年以来在ODI比赛中首次击中七个以上的边界。”
- en: 'The following image illustrates the preceding points:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像说明了前面的要点：
- en: '![Figure 4.11 – Representation of RAG workflow with vector database](img/B21443_04_11.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – RAG 工作流程的向量数据库表示](img/B21443_04_11.jpg)'
- en: Figure 4.11 – Representation of RAG workflow with vector database
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – RAG 工作流程的向量数据库表示
- en: Business applications of RAG
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG 的商业应用
- en: 'In the following list, we have mentioned a few popular business applications
    of RAG based on what we’ve seen in the industry:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们根据我们在行业中所见，提到了一些基于 RAG 的流行商业应用：
- en: '**Enterprise search engines**: One of the most prominent applications of RAG
    is in the realm of enterprise learning and development, serving as a search engine
    for employee upskilling. Employees can pose questions about the company, its culture,
    or specific tools, and RAG swiftly delivers accurate and relevant answers.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**企业搜索引擎**：RAG 在企业学习和发展的领域中最显著的应用之一是作为员工技能提升的搜索引擎。员工可以就公司、其文化或特定工具提出问题，RAG
    迅速提供准确和相关的答案。'
- en: '**Legal and compliance**: RAG fetches relevant case laws or checks business
    practices against regulations.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律和合规**：RAG 检索相关的案例法或检查商业实践是否符合法规。'
- en: '**Ecommerce**: RAG suggests products or summarizes reviews based on user behavior
    and queries.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电子商务**：RAG 根据用户行为和查询建议产品或总结评论。'
- en: '**Customer support**: RAG provides precise answers to customer queries by pulling
    information from the company’s knowledge base and providing solutions in real
    time.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户支持**：RAG 通过从公司的知识库中提取信息并提供实时解决方案来对客户查询提供精确的答案。'
- en: '**Medical and healthcare**: RAG retrieves pertinent medical research or provides
    preliminary symptom-based suggestions.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗和保健**：RAG 检索相关的医学研究或基于症状的初步建议。'
- en: Chunking strategies
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分块策略
- en: 'In our last discussion, we delved into vector DBs and RAG. Before diving into
    RAG, we need to efficiently house our embedded data. While we touched upon indexing
    methods to speed up data fetching, there’s another crucial step to take even before
    that: chunking.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上次的讨论中，我们深入探讨了向量数据库和 RAG。在深入 RAG 之前，我们需要有效地存储我们的嵌入数据。虽然我们提到了索引方法以加快数据检索，但在那之前还有一个至关重要的步骤：分块。
- en: What is chunking?
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是分块？
- en: In the context of building LLM applications with embedding models, chunking
    involves dividing a long piece of text into smaller, manageable pieces or “chunks”
    that fit within the model’s token limit. The process involves breaking text into
    smaller segments before sending these to the embedding models. As shown in the
    following image, chunking happens before the embedding process. Different documents
    have different structures, such as free-flowing text, code, or HTML. So, different
    chunking strategies can be applied to attain optimal results. Tools such as Langchain
    provide you with functionalities to chunk your data efficiently based on the nature
    of the text.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用嵌入模型构建 LLM 应用程序的情况下，分块涉及将一段长文本分割成更小、更易于管理的片段或“块”，这些块适合模型令牌限制。这个过程涉及在将文本发送到嵌入模型之前将其分割成更小的片段。如图所示，分块发生在嵌入过程之前。不同的文档有不同的结构，例如自由流动的文本、代码或
    HTML。因此，可以应用不同的分块策略以获得最佳结果。Langchain 等工具为您提供基于文本性质高效分块数据的函数。
- en: 'The diagram below depicts a data processing workflow, highlighting the chunking
    step, starting with raw “Data sources” that are converted into “Documents.” Central
    to this workflow is the “Chunk” stage, where a “TextSplitter” breaks the data
    into smaller segments. These chunks are then transformed into numerical representations
    using an “Embedding model” and are subsequently indexed into a “Vector DB” for
    efficient search and retrieval. The text associated with the retrieved chunks
    is then sent as context to the LLMs, which then generate a final response:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示描绘了一个数据处理工作流程，突出了分块步骤，从原始的“数据源”开始，这些数据源被转换成“文档”。此工作流程的核心是“分块”阶段，其中“文本分割器”将数据分割成更小的片段。然后，这些片段通过“嵌入模型”转换成数值表示，并随后被索引到“向量数据库”中，以实现高效的搜索和检索。检索到的片段相关的文本随后作为上下文发送给大型语言模型（LLMs），然后生成最终响应：
- en: '![Fig 4.12 – Chunking Process](img/B21443_04_12.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 分块过程](img/B21443_04_12.jpg)'
- en: Fig 4.12 – Chunking Process
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 分块过程
- en: But why is it needed?
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但为什么需要它呢？
- en: 'Chunking is vital for two main reasons:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 分块有两个主要原因至关重要：
- en: Chunking strategically divides document text to enhance its comprehension by
    embedding models, and it boosts the relevance of the content retrieved from a
    vector DB. Essentially, it refines the accuracy and context of the results sourced
    from the database.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分块策略有策略地划分文档文本，通过嵌入模型增强其理解，并提高从向量数据库检索的内容的相关性。本质上，它提高了从数据库中获取的结果的准确性和上下文。
- en: It tackles the token constraints of embedding models. For instance, Azure’s
    OpenAI embedding models like text-embedding-ada-002 can handle up to 8,191 tokens,
    which is about 6,000 words, given each token averages four characters. So, for
    optimal embeddings, it’s crucial our text stays within this limit.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它解决了嵌入模型的标记约束问题。例如，Azure的OpenAI嵌入模型text-embedding-ada-002可以处理多达8,191个标记，这大约是6,000个单词，因为每个标记平均有四个字符。因此，为了获得最佳的嵌入效果，确保我们的文本保持在限制之内至关重要。
- en: Popular chunking strategies
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流行分块策略
- en: '**Fixed-size chunking**: This is a very common approach that defines a fixed
    size (200 words), which is enough to capture the semantic meaning of a paragraph,
    and it incorporates an overlap of about 10–15% as an input to the vector embedding
    generation model. Chunking data with a slight overlap between text ensures context
    preservation. It’s advisable to begin with a roughly 10% overlap. Below is a snippet
    of code that demonstrates the use of fixed-size chunking with LangChain:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定大小分块**：这是一种非常常见的方法，它定义了一个固定的大小（200个单词），这足以捕捉段落的意义，并且它将大约10-15%的重叠作为输入到向量嵌入生成模型。在文本之间有轻微重叠的分块数据确保了上下文的保留。建议开始时使用大约10%的重叠。以下是一个使用LangChain演示固定大小分块的代码片段：'
- en: '[PRE2]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE3]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Variable-size chunking**: Variable-size chunking refers to the dynamic segmentation
    of data or text into varying-sized components, as opposed to fixed-size divisions.
    This approach accommodates the diverse structures and characteristics present
    in different types of data.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可变大小分块**：可变大小分块是指将数据或文本动态分割成不同大小的组件，而不是固定大小的分割。这种方法适应了不同类型数据中存在的各种结构和特征。'
- en: '**Sentence splitting**: Sentence transformer models are neural architectures
    optimized for embedding at the sentence level. For example, BERT works best when
    chunked at the sentence level. Tools such as NLTK and SpaCy provide functions
    to split the sentences within a text.'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子分割**：句子转换器模型是针对句子级别嵌入优化的神经网络架构。例如，BERT在句子级别分块时表现最佳。NLTK和SpaCy等工具提供了分割文本中句子的函数。'
- en: '**Specialized chunking**: Documents, such as research papers, possess a structured
    organization of sections, and the Markdown language, with its unique syntax, necessitates
    specialized chunking, resulting in the proper separation between sections/pages
    to yield contextually relevant chunks.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专用分块**：例如研究论文等文档具有结构化的章节组织，Markdown语言独特的语法要求进行专用分块，从而在章节/页面之间实现适当的分离，以产生上下文相关的分块。'
- en: '**Code Chunking**: When embedding code into your vector DB, this technique
    can be invaluable. Langchain supports code chunking for numerous languages. Below
    is a snippet code to chunk your Python code:'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码分块**：当将代码嵌入到你的向量数据库中时，这种技术可能非常有价值。Langchain支持多种语言的代码分块。以下是一个分块Python代码的代码片段：'
- en: '[PRE4]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE5]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Chunking considerations
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分块注意事项
- en: Chunking strategies vary based on **data type** and **format** and the **chosen
    embedding model**. For instance, code requires a distinct chunking approach compared
    to unstructured text. While models such as text-embedding-ada-002 excel with 256-
    and 512-token-sized chunks, our understanding of chunking is ever-evolving. Moreover,
    preprocessing plays a crucial role before chunking, where you can optimize your
    content by removing unnecessary text content, such as stop words, special symbols,
    etc., that add noise. For the latest techniques, we suggest regularly checking
    the text splitters section in the LangChain documentation, ensuring you employ
    the best strategy for your needs
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 分块策略根据**数据类型**、**格式**和**选择的嵌入模型**而有所不同。例如，与无结构化文本相比，代码需要一种不同的分块方法。虽然像text-embedding-ada-002这样的模型在256和512个标记大小的分块上表现出色，但我们对分块的理解是不断发展的。此外，预处理在分块之前起着至关重要的作用，你可以通过删除不必要的文本内容（如停用词、特殊符号等）来优化你的内容，这些内容会增加噪声。对于最新的技术，我们建议定期检查LangChain文档中的文本分割器部分，确保你使用最适合你需求的最佳策略。
- en: '(Split by tokens from Langchain: [https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: （由 Langchain 的标记分割：[https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)）。
- en: Evaluation of RAG using Azure Prompt Flow
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Azure Prompt Flow 对 RAG 的评估
- en: 'Up to this point, we have discussed the development of resilient RAG applications.
    However, the question arises: How can we determine whether these applications
    are functioning as anticipated and if the context they retrieve is pertinent?
    While manual validation—comparing the responses generated by LLMs against ground
    truth—is possible, this method proves to be labor-intensive, costly, and challenging
    to execute on a large scale. Consequently, it’s essential to explore methodologies
    that facilitate automated evaluation on a vast scale. Recent research has delved
    into the concept of utilizing “LLM as a judge” to assess output, a strategy that
    Azure Prompt Flow incorporates within its offerings.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了弹性 RAG 应用的开发。然而，问题随之而来：我们如何确定这些应用程序是否按预期运行，以及它们检索的上下文是否相关？虽然手动验证——将
    LLM 生成的响应与事实真相进行比较——是可能的，但这种方法证明是劳动密集型、成本高昂且难以在大规模上执行。因此，探索便于大规模自动评估的方法至关重要。最近的研究深入探讨了利用“LLM
    作为评判者”来评估输出的概念，这是 Azure Prompt Flow 在其服务中采用的一种策略。
- en: 'Azure Prompt Flow has built-in and structured metaprompt templates with comprehensive
    guardrails to evaluate your output against ground truth. The following mentions
    four metrics that can help you evaluate your RAG solution in Prompt Flow:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Prompt Flow 内置了结构和元提示模板，具有全面的防护措施，以评估您的输出与事实真相的一致性。以下提到四个可以帮助您在 Prompt
    Flow 中评估 RAG 解决方案的指标：
- en: '**Groundedness**: Measures the alignment of the model’s answers with the input
    source, making sure the model’s generated response is not fabricated. The model
    must always extract information from the provided “context” while responding to
    user’s query.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础性**：衡量模型答案与输入源的一致性，确保模型生成的响应不是虚构的。在回答用户查询时，模型必须始终从提供的“上下文”中提取信息。'
- en: '**Relevance**: Measures the degree to which the model’s generated response
    is closely connected to the context and user query.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性**：衡量模型生成的响应与上下文和用户查询的紧密程度。'
- en: '**Retrieval score**: Measures the extent to which the model’s retrieved documents
    are pertinent and directly related to the given questions.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索分数**：衡量模型检索的文档与给定问题的相关性和直接相关性。'
- en: '**Custom metrics**: While the above three are the most important for evaluating
    RAG applications, Prompt Flow allows you to use custom metrics, too. Bring your
    own LLM as a judge and define your own metrics by modifying the existing metaprompts.
    This also allows you to use open source models such as Llama and to build your
    own metrics from code with Python functions. The above evaluations are more no-code
    or low-code friendly; however, for a more pro-code friendly approach, azureml-metrics
    SDK, such as ROUGE, BLEU, F1-Score, Precision, and Accuracy, can be utilized as
    well.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义指标**：虽然上述三个指标对于评估 RAG 应用程序最为重要，但 Prompt Flow 允许您使用自定义指标。将您自己的 LLM 作为评判者，并通过修改现有的
    metaprompts 定义自己的指标。这还允许您使用如 Llama 这样的开源模型，并使用 Python 函数从代码中构建自己的指标。上述评估更适合无代码或低代码环境；然而，对于更专业的代码环境，可以使用
    azureml-metrics SDK，例如 ROUGE、BLEU、F1 分数、精确度和准确度等。'
- en: The field is advancing quickly, so we recommend regularly checking Azure ML
    Prompt Flow’s latest updates on evaluation metrics. Start with the “Manual Evaluation”
    feature in Prompt Flow to gain a basic understanding of LLM performance. It’s
    important to use a mix of metrics for a thorough evaluation that captures both
    semantic and syntactic essence rather than relying on just one metric to compare
    the responses with the actual ground truth.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域正在快速发展，因此我们建议定期检查 Azure ML Prompt Flow 的最新评估指标更新。从 Prompt Flow 中的“手动评估”功能开始，以获得对
    LLM 性能的基本理解。使用多种指标进行彻底评估，以捕捉语义和句法本质，而不仅仅依赖于单一指标来比较响应与实际的事实真相。
- en: Case study – Global chat application deployment by a multinational organization
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 - 跨国组织部署的全球聊天应用程序
- en: A global firm recently launched an advanced internal chat application featuring
    a Q&A support chatbot. This innovative tool, deployed across various Azure regions,
    integrates several large language models, including the specialized finance model,
    BloombergGPT. To meet specific organizational requirements, bespoke plugins were
    developed. It had an integration with Service Now, empowering the chatbot to streamline
    ticket generation and oversee incident actions.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一家全球公司最近推出了一款具有问答支持聊天机器人的先进内部聊天应用程序。这个创新工具部署在多个Azure区域，集成了包括专门金融模型BloombergGPT在内的几个大型语言模型。为了满足特定的组织要求，开发了定制插件。它集成了Service
    Now，使聊天机器人能够简化票据生成并监督事件操作。
- en: In terms of data refinement, the company meticulously preprocessed its knowledge
    base (KB) information, eliminating duplicates, special symbols, and stop words.
    The KB consisted of answers to frequently asked questions and general information
    to various support-related questions. They employed fixed chunking approaches,
    exploring varied chunk sizes, before embedding these data into the Azure AI search.
    Their methodology utilized Azure OpenAI’s text-ada-embedding-002 models in tandem
    with the cosine similarity metric and Azure AI search’s vector search capabilities.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据精炼方面，公司仔细预处理了其知识库（KB）信息，消除了重复项、特殊符号和停用词。KB包括常见问题的答案以及各种支持相关问题的通用信息。他们在将数据嵌入到Azure
    AI搜索之前，采用了固定的分块方法，探索了不同的分块大小。他们的方法利用了Azure OpenAI的text-ada-embedding-002模型，与余弦相似度指标和Azure
    AI搜索的向量搜索功能相结合。
- en: From their extensive testing, they discerned optimal results with a chunk size
    of 512 tokens and a 10% overlap. Moreover, they adopted an ANN vector search methodology
    using cosine similarity. They also incorporated hybrid search that included keyword
    and semantic search with Semantic Reranker. Their RAG workflow, drawing context
    from Azure Vector Search and the GPT 3.5 Turbo-16K models, proficiently generated
    responses to customer support inquiries. They implemented caching techniques using
    Azure Cache Redis and rate-limiting strategies using Azure API Management to optimize
    the costs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在广泛的测试中，他们发现512个标记大小和10%重叠的最佳结果。此外，他们采用了基于余弦相似度的ANN向量搜索方法。他们还结合了包含关键词和语义搜索的混合搜索，并使用语义重排器。他们的RAG工作流程从Azure向量搜索和GPT
    3.5 Turbo-16K模型中提取上下文，有效地生成对客户支持查询的响应。他们使用Azure Cache Redis进行缓存技术，并使用Azure API
    Management进行速率限制策略，以优化成本。
- en: The integration of the support Q&A chatbot significantly streamlined the multinational
    firm’s operations, offering around-the-clock, consistent, and immediate responses
    to queries, thereby enhancing user satisfaction. This not only brought about substantial
    cost savings by reducing human intervention but also ensured scalability to handle
    global demands. By automating tasks such as ticket generation, the firm gained
    deeper insights in[to user interactions, allowing for continuous improvement and
    refinement of their services.](https://arxiv.org/pdf/2309.11322.pdf)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 支持问答聊天机器人的集成显著简化了跨国公司的运营，提供全天候、一致性和即时的查询响应，从而提高了用户满意度。这不仅通过减少人工干预实现了显著的成本节约，还确保了可扩展性以应对全球需求。通过自动化如票据生成等任务，公司对用户互动有了更深入的了解，从而实现了服务的持续改进和优化。[详情](https://arxiv.org/pdf/2309.11322.pdf)
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the RAG approach, a powerful method for leveraging
    your data to craft personalized experiences, reduce hallucinations while also
    addressing the training limitations inherent in LLMs. Our journey began with an
    examination of foundational concepts such as vectors and databases, with a special
    focus on Vector Databases. We understood the critical role that Vector DBs play
    in the development of RAG-based applications, also highlighting how they can enhance
    LLM responses through effective chunking strategies. The discussion also covered
    practical insights on building engaging RAG experiences, evaluating them through
    prompt flow, and included a hands-on lab available on GitHub to apply what we’ve
    learned.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了RAG方法，这是一种强大的利用数据来创造个性化体验的方法，在减少LLM固有的训练限制的同时，也解决了幻觉问题。我们的旅程从检查基础概念，如向量和数据库开始，特别关注向量数据库。我们理解了向量数据库在基于RAG的应用开发中的关键作用，并强调了它们如何通过有效的分块策略增强LLM的响应。讨论还涵盖了构建引人入胜的RAG体验的实用见解，通过提示流进行评估，并包括GitHub上可用的动手实验室，以应用我们所学的知识。
- en: In the next chapter we will introduce another popular technique designed to
    minimize hallucinations and more easily steer the responses of LLMs. We will cover
    prompt engineering strategies, empowering you to fully harness the capabilities
    of your LLMs and engage more effectively with AI. This exploration will provide
    you with the tools and knowledge to enhance your interactions with AI, ensuring
    more reliable and contextually relevant outputs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一种旨在最小化幻觉并更容易引导LLM响应的流行技术。我们将涵盖提示工程策略，让您充分利用LLM的能力，并更有效地与AI互动。这次探索将为您提供工具和知识，以增强您与AI的互动，确保更可靠和上下文相关的输出。
- en: References
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Vector database management systems: Fundamental concepts, use-cases, and current
    challenges : [https://arxiv.org/pdf/2309.11322.pdf](https://arxiv.org/pdf/2309.11322.pdf)'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量数据库管理系统：基本概念、用例和当前挑战：[https://arxiv.org/pdf/2309.11322.pdf](https://arxiv.org/pdf/2309.11322.pdf)
- en: Two minutes NLP — 11 word embeddings models you should know | by Fabio Chiusano
    | NLPlanet | Medium - [https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9](https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9)
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两分钟NLP — 您应该了解的11个词嵌入模型 | 作者：Fabio Chiusano | NLPlanet | Medium - [https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9](https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9)
- en: 'How To Choose The Right Embedding Model For You | by Chebbah Mehdi | Medium:
    [https://medium.com/@mehdi_chebbah/how-to-choose-the-right-embedding-model-for-you-1fc917d14517](mailto:https://medium.com/@mehdi_chebbah/how-to-choose-the-right-embedding-model-for-you-1fc917d14517)'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何为您选择合适的嵌入模型 | 作者：Chebbah Mehdi | Medium：[https://medium.com/@mehdi_chebbah/how-to-choose-the-right-embedding-model-for-you-1fc917d14517](mailto:https://medium.com/@mehdi_chebbah/how-to-choose-the-right-embedding-model-for-you-1fc917d14517)
- en: A Gentle Introduction to Vector Databases | Weaviate - vector database - [https://weaviate.io/blog/what-is-a-vector-database](https://weaviate.io/blog/what-is-a-vector-database)
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量数据库简介 | Weaviate - 向量数据库 - [https://weaviate.io/blog/what-is-a-vector-database](https://weaviate.io/blog/what-is-a-vector-database)
- en: Vector Library versus Vector Database | Weaviate - vector database - [https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database](https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database)
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量库与向量数据库 | Weaviate - 向量数据库 - [https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database](https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database)
- en: Efficient and robust approximate nearest neighbor search using Hierarchical
    Navigable Small World graphs - [https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分层可导航小世界图进行高效且鲁棒的近似最近邻搜索 - [https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320)
- en: 'Introduction Milvus v2.0.x documentation: [https://milvus.io/docs/v2.0.x/overview.md](https://milvus.io/docs/v2.0.x/overview.md)'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Milvus v2.0.x 文档介绍：[https://milvus.io/docs/v2.0.x/overview.md](https://milvus.io/docs/v2.0.x/overview.md)
- en: The 5 Best Vector Databases | A List With Examples | DataCamp - [https://www.datacamp.com/blog/the-top-5-vector-databases](https://www.datacamp.com/blog/the-top-5-vector-databases)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5个最佳向量数据库 | 带有示例的列表 | DataCamp - [https://www.datacamp.com/blog/the-top-5-vector-databases](https://www.datacamp.com/blog/the-top-5-vector-databases)
- en: Vector Library versus Vector Database | Weaviate - vector database - [https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database](https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database)
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量库与向量数据库 | Weaviate - 向量数据库 - [https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database](https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database)
- en: 'RAG: [https://milvus.io/docs/v2.0.x/overview.md](https://milvus.io/docs/v2.0.x/overview.md)'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG：[https://milvus.io/docs/v2.0.x/overview.md](https://milvus.io/docs/v2.0.x/overview.md)
- en: Chunk documents in vector search - Azure Cognitive Search | Microsoft Learn
    - [https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents)
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在向量搜索中分块文档 - Azure认知搜索 | Microsoft Learn - [https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents)
- en: Chunking Strategies for LLM Applications | Pinecone - [https://www.pinecone.io/learn/chunking-strategies/](https://www.pinecone.io/learn/chunking-strategies/)
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM应用中的分块策略 | Pinecone - [https://www.pinecone.io/learn/chunking-strategies/](https://www.pinecone.io/learn/chunking-strategies/)
- en: 'Product Quantization: Compressing high-dimensional vectors by 97% | Pinecone:
    [https://www.pinecone.io/learn/series/faiss/product-quantization/](https://www.pinecone.io/learn/series/faiss/product-quantization/)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 产品量化：通过压缩高维向量实现97%的压缩率 | Pinecone：[https://www.pinecone.io/learn/series/faiss/product-quantization/](https://www.pinecone.io/learn/series/faiss/product-quantization/)
- en: Evaluation and monitoring metrics for generative AI - Azure AI Studio | Microsoft
    Learn - [https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in)
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成式AI的评估和监控指标 - Azure AI Studio | Microsoft Learn - [https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in)
- en: 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于知识密集型NLP任务的检索增强生成：[https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)
