- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RAGs to Riches: Elevating AI with External Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs such as GPT have certain limitations. They may not have up-to-date information
    due to their knowledge cutoff date for training. This poses a significant challenge
    when we want our AI models to provide accurate, context-aware, and timely responses.
    Imagine asking an LLM a question about the latest technology trends or seeking
    real-time updates on a breaking news event; traditional language models might
    fall short in these scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’re going to introduce you to a game-changing technique called
    **retrieval-augmented generation** (**RAG**), an outcome of the work carried out
    by researchers at Facebook AI (now Meta). It’s the secret sauce that empowers
    language models such as GPT to bridge the gap between their static knowledge and
    the dynamic real world. With RAG, we’ll show you how to equip your generative
    AI applications with the ability to pull in fresh information, ground your organizational
    data, cross-reference facts to address hallucinations, and stay contextually aware,
    all in real time. We will also discuss the fundaments of vector databases, a new,
    hot, and emerging database that is designed for storing, indexing, and querying
    vectors that represent highly dimensional data; they are typically used for similarity
    search and machine learning applications and are important in building RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand how RAG can turn your language model into an information-savvy
    conversational assistant, ensuring that it’s always in the know, no matter when
    you ask the question.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A deep dive into vector DB essentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of vector DBs in retrieval-augmented generation (RAG)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of RAG using Azure Prompt Flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study – Global chat application deployment by a multinational organization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Benefits of RAG](img/B21443_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Benefits of RAG
  prefs: []
  type: TYPE_NORMAL
- en: A deep dive into vector DB essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To fully comprehend RAG, it’s imperative to understand vector DBs because RAG
    relies heavily on its efficient data retrieval for query resolution. A vector
    DB is a database designed to store and efficiently query highly dimensional vectors
    and is often used in similarity searches and machine learning tasks. The design
    and mechanics of vector DBs directly influence the effectiveness and accuracy
    of RAG answers.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover the fundamental components of vector DBs (vectors
    and vector embeddings), and in the next section, we will dive deeper into the
    important characteristics of vector DBs that enable a RAG-based generative AI
    solution. We will also explain how it differs from regular databases and then
    tie it all back to explain RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors and vector embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A vector is a mathematical object that has both magnitude and direction and
    can be represented by an ordered list of numbers. In a more general sense, especially
    in computer science and machine learning, a vector can be thought of as an array
    or list of numbers that represents a point in a certain dimensional space. For
    instances depicted in the following image, in 2D space (on the left), a vector
    might be represented as [x, y], whereas in 3D space (on the right), it might be
    [x, y, z]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Representation of vectors in 2D and 3D space](img/B21443_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Representation of vectors in 2D and 3D space
  prefs: []
  type: TYPE_NORMAL
- en: Vector embedding refers to the representation of objects, such as words, sentences,
    or even entire documents, as vectors in a highly dimensional space. A highly dimensional
    space denotes a mathematical space with more than three dimensions, frequently
    used in data analysis and machine learning to represent intricate data structures.
    Think of it as a room where you can move in more than three directions, facilitating
    the description and analysis of complex data. The embedding process converts words,
    sentences, or documents into vector representations, capturing the intricate semantic
    relationships between them. Hence, words with similar meanings tend to be close
    to each other in the highly dimensional space. Now, you must be wondering how
    this plays a role in designing generative AI solutions consisting of LLMs. Vector
    embeddings provide the foundational representation of data. They are a standardized
    numerical representation for diverse types of data, which LLMs use to process
    and generate information. Such an embedding process to convert words and sentences
    to a numerical representation is initiated by embedding models such as OpenAI’s
    text-embedding-ada-002\. Let’s explain this with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image visually represents the clustering of mammals and birds
    in a two-dimensional vector embedding space, differentiating between their realistic
    and cartoonish portrayals. This image depicts a spectrum between “REALISTIC” and
    “CARTOON” representations, further categorized into “MAMMAL” and “BIRD.” On the
    realistic side, there’s a depiction of a mammal (elk) and three birds (an owl,
    an eagle, and a small bird). On the cartoon side, there are stylized and whimsical
    cartoon versions of mammals and birds, including a comically depicted deer, an
    owl, and an exaggerated bird character. LLMs use such vector embedding spaces,
    which are numerical representations of objects in highly dimensional spaces, to
    understand, process, and generate information. For example, imagine an educational
    application designed to teach children about wildlife. If a student prompts the
    chatbot to provide images of birds in a cartoon representation, the LLM will search
    and generate information from the bottom right quadrant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Location of animals with similar characteristics in a highly
    dimensional space, demonstrating “relatedness”](img/B21443_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Location of animals with similar characteristics in a highly dimensional
    space, demonstrating “relatedness”
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s delve into the evolution of embedding models that produce embeddings,
    a.k.a numerical representations of objects, within highly dimensional spaces.
    Embedding models have experienced significant evolution, transitioning from the
    initial methods that mapped discrete words to dense vectors, such as word-to-vector
    (Word2Vec), global vectors for word representation (GloVe), and FastText to more
    sophisticated contextual embeddings using deep learning architectures. These newer
    models, such as embeddings from language models (ELMos), utilize long short-term
    memory (LSTM)-based structures to offer context-specific representations. The
    newer transformer architecture-based embedding models, which underpin models such
    as bidirectional encoder representations from transformers (BERT), generative
    pre-trained transformer (GPT), and their subsequent iterations, marked a revolutionary
    leap over predecessor models.
  prefs: []
  type: TYPE_NORMAL
- en: These models capture contextual information in unparalleled depth, enabling
    embeddings to represent nuances in word meanings based on the surrounding context,
    thereby setting new standards in various natural language processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important note:'
  prefs: []
  type: TYPE_NORMAL
- en: In Jan 2024, OpenAI announced two third-generation embedding models, **text-embedding-3-small**
    and **text-embedding-3-large**, which are the newest models that have better performance,
    lower costs, and better multi-lingual retrieval and parameters to reduce the overall
    size of dimensions when compared to predecessor second-generation model, **text-embedding-ada-002**.
    Another key difference is the number of dimensions between the two generations.
    The third-generation models come in different dimensions, and the highest they
    can go up to is 3,072\. As of Jan 2024, we have seen more production workloads
    using text-embedding-ada-002 in production, which has 1,536 dimensions. OpenAI
    recommends using the third-generation models going forward for improved performance
    and reduced costs.
  prefs: []
  type: TYPE_NORMAL
- en: We also wanted you to know that while OpenAI’s embedding model is one of the
    most popular choices when it comes to text embeddings, you can find the list of
    leading embedding models on Hugging Face ([https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet of code gives an example of generating Azure OpenAI endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we highlighted the significance of vector embeddings. However,
    their true value emerges when used effectively. Hence, we’ll now dive deep into
    indexing and vector search strategies, which are crucial for optimal data retrieval
    in the RAG workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Vector search strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vector search strategies are crucial because they determine how efficiently
    and accurately highly dimensional data (such as embeddings) can be queried and
    retrieved. Optimal strategies ensure that the most relevant and contextually appropriate
    results are returned. In vector-based searching, there are primarily two main
    strategies: **exact search** and **approximate search**.'
  prefs: []
  type: TYPE_NORMAL
- en: Exact search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The exact search method, as the term suggests, directly matches a query vector
    with vectors in the database. It uses an exhaustive approach to identify the closest
    neighbors, allowing minimal to no errors.
  prefs: []
  type: TYPE_NORMAL
- en: This is typically what the traditional KNN method employs. Traditional KNNs
    utilize brute force methods to find the K-nearest neighbors, which demands a thorough
    comparison of the input vector with every other vector in the dataset. Although
    computing the similarity for each vector is typically quick, the process becomes
    time-consuming and resource-intensive over extensive datasets because of the vast
    number of required comparisons. For instance, if you had a dataset of one million
    vectors and wanted to find the nearest neighbors for a single input vector, the
    traditional KNN would require one million distance computations. This can be thought
    of as looking up a friend’s phone number in a phone book by checking each entry
    one by one rather than using a more efficient search strategy that speeds up the
    process, which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Approximate nearest neighbors (ANNs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In modern vector DBs, the search strategy known as ANN stands out as a powerful
    technique that quickly finds the near-closest data points in highly dimensional
    spaces, potentially trading off a bit of accuracy for speed. Unlike KNN, ANN prioritizes
    search speed at the expense of slight accuracy. Additionally, for it to function
    effectively, a vector index must be built beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: The process of vector indexing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The process of vector indexing involves the organization of embeddings in a
    data structure called an index, which can be traversed quickly for retrieval purposes.
    Many ANN algorithms aid in forming a vector index, all aiming for rapid querying
    by creating an efficiently traversable data structure. Typically, they compress
    the original vector representation to enhance the search process.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are numerous indexing algorithms, and this is an active research area.
    ANNs can be broadly classified into **tree-based indexes**, **graph-based indexes**,
    **hash-based indexes**, and **quantization-based indexes**. In this section, we
    will cover the two most popular indexing algorithms. When creating an LLM application,
    you don’t need to dive deep into the indexing process since many vector databases
    provide this as a service to you. But it’s important to choose the right type
    of index for your specific needs to ensure efficient data retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchical navigable small world** (**HNSW**): This is a method for approximate
    similarity search in highly dimensional spaces. HNSW is a graph-based index that
    works by creating a hierarchical graph structure, where each node represents a
    data point, and the edges connect similar data points. This hierarchical structure
    allows for efficient search operations, as it narrows down the search space quickly.
    HNSW is well suited for similarity search use cases, such as content-based recommendation
    systems and text search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you wish to dive deeper into its workings, we recommend checking out this
    research paper: [https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following image is a representation of the HNSW index:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Representation of HNSW index](img/B21443_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Representation of HNSW index
  prefs: []
  type: TYPE_NORMAL
- en: The image illustrates the HNSW graph structure used for efficient similarity
    searches. The graph is constructed in layers, with decreasing density from the
    bottom to the top. Each layer’s characteristic radius reduces as we ascend, creating
    sparser connections. The depicted search path, using the red dotted lines, showcases
    the algorithm’s strategy; it starts from the sparsest top layer, quickly navigating
    vast data regions, and then refines its search in the denser lower layers, minimizing
    the overall comparisons and enhancing search efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '**Facebook AI Similarity Search** (**FAISS**): FAISS, developed by Facebook
    AI Research, is a library designed for the efficient similarity search and clustering
    of highly dimensional vectors. It uses product quantization to compress data during
    indexing, accelerating similarity searches in vast datasets. This method divides
    the vector space into regions known as Voronoi cells, each symbolized by a centroid.
    The primary purpose is to minimize storage needs and expedite searches, though
    it may slightly compromise accuracy. To visualize this, consider the following
    image. The Voronoi cells denote regions from quantization, and the labeled points
    within these cells are the centroids or representative vectors. When indexing
    a new vector, it’s aligned with its closest centroid. For searches, FAISS pinpoints
    the probable Voronoi cell containing the nearest neighbors and then narrows down
    the search within that cell, significantly cutting down distance calculations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Representation of FAISS index](img/B21443_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Representation of FAISS index
  prefs: []
  type: TYPE_NORMAL
- en: It excels in applications such as image and video search, recommendation systems,
    and any task that involves searching for nearest neighbors in highly dimensional
    spaces because of its performance optimizations and built-in GPU optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered indexing and the role of ANNs in index creation.
    Next, we’ll explore similarity measures, how they differ from indexing, and their
    impact on improving data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use HNSW vs. FAISS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use HNSW when:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: High precision in similarity search is crucial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset size is large but not at the scale where managing it becomes impractical
    for HNSW.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time or near-real-time search performance is required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is dynamic, with frequent updates or insertions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apt for use cases involving text like article recommendation systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use FAISS when:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Managing extremely large datasets (e.g., billions of vectors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch processing and GPU optimization can significantly benefit the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s a need for flexible trade-offs between search speed and accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is relatively static, or batch updates are acceptable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apt for use cases like image and video search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right indexing strategy hinges on several critical factors, including
    the nature and structure of the data, the types of queries (e.g. range queries,
    nearest neighbors, exact search) to be supported, and the volume and growth of
    the data. Additionally, the frequency of data updates (e.g., static vs dynamic)
    the dimensionality of the data, performance requirements (real-time, batch), and
    resource constraints play significant roles in the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarity measures dictate how the index is organized, and this makes sure
    that the retrieved data are highly relevant to the query. For instance, in a system
    designed to retrieve similar images, the index might be built around the feature
    vectors of images, and the similarity measure would determine which images are
    “close” or “far” within that indexed space. The importance of these concepts is
    two-fold: indexing significantly speeds up data retrieval, and similarity measures
    ensure that the retrieved data is relevant to the query, together enhancing the
    efficiency and efficacy of data retrieval systems. Selecting an appropriate distance
    metric greatly enhances the performance of classification and clustering tasks.
    The optimal similarity measure is chosen based on the nature of the data input.
    In other words, similarity measures define how closely two items or data points
    are related. They can be broadly classified into **distance metrics** and **similarity
    metrics**. Next, we’ll explore the three top similarity metrics for building AI
    applications: cosine similarity and Euclidean and Manhattan distance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarity metrics – Cosine similarity**: Cosine similarity, a type of similarity
    metric, calculates the cosine value of the angle between two vectors, and OpenAI
    suggests using it for its models to measure the distance between two embeddings
    obtained from text-embedding-ada-002\. The higher the metric, the more similar
    they are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Illustration of relatedness through cosine similarity between
    two words](img/B21443_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Illustration of relatedness through cosine similarity between two
    words
  prefs: []
  type: TYPE_NORMAL
- en: The preceding image shows a situation where the cosine similarity is 1 for India
    and the USA because they are related, as both are countries. In the other image,
    the similarity is 0 because football is not similar to a lion.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance metrics – Euclidean (L2)**: Euclidean distance computes the straight-line
    distance between two points in Euclidean space. The higher the metric, the less
    similar the two points are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Illustration of Euclidean distance](img/B21443_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Illustration of Euclidean distance
  prefs: []
  type: TYPE_NORMAL
- en: 'The image illustrates the Euclidean distance formula in a 2D space. It shows
    two points: (x1,y1) and (x2,y2). The preceding formula calculates the straight-line
    distance between the two points in a plane.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance metrics – Manhattan (L1)**: Manhattan distance calculates the sum
    of absolute differences along each dimension. The higher the metric, the less
    similar the differences. The following image depicts the Manhattan distance (or
    L1 distance) between two points in a 2D space, where the distance is measured
    along the axes at right angles, similar to navigating city blocks in a grid-like
    street layout:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Illustration of Manhattan distance](img/B21443_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Illustration of Manhattan distance
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering when to select one metric over another during the development
    of generative AI applications. The decision on which similarity measure to use
    hinges on various elements, such as the type of data, the context of the application,
    and the bespoke demands of the analysis results.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity is preferred over Manhattan and Euclidean distances when the
    magnitude of the data vectors is less relevant than the direction or orientation
    of the data. In text analysis, for example, two documents might be represented
    by highly dimensional vectors of word frequencies. If one document is a longer
    version of the other, their word frequency vectors will point in the same direction,
    but the magnitude (length) of one vector will be larger due to the higher word
    count. Using Euclidean or Manhattan distance would highlight these differences
    in magnitude, suggesting the documents are different. However, using cosine similarity
    would capture their similarity in content (the direction of the vectors), de-emphasizing
    the differences in word count. In this context, cosine similarity is more appropriate,
    as it focuses on the angle between the vectors, reflecting the content overlap
    of the documents rather than their length or magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean and Manhattan distances are more apt than cosine similarity when the
    magnitude and absolute differences between data vectors are crucial, such as with
    consistent scaled numerical data (e.g., age, height, weight, and so on) or in
    spatial applications such as grid-based pathfinding. While cosine similarity emphasizes
    the orientation or pattern of data vectors, which is especially useful in highly
    dimensional, sparse datasets, Euclidean and Manhattan distances capture the actual
    differences between data points, making them preferable in scenarios where absolute
    value deviations are significant such as when comparing the medical test results
    of patients or finding the distance between geographical co-ordinates on earth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a snippet of code that uses Azure OpenAI endpoints to calculate
    the similarity between two sentences: “What number of countries do you know?”
    and “How many countries are you familiar with?” by using embedding model text-embedding-ada-002\.
    It gives a score of 0.95:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now let us walkthrough a scenario where Cosine Similarity will be preferred
    over Manhattan distance.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation System for Articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's consider a scenario where a news aggregation platform aims to recommend
    articles similar to what a user is currently reading, enhancing user engagement
    by suggesting relevant content.
  prefs: []
  type: TYPE_NORMAL
- en: 'How It Works:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Preprocessing and Indexing**: Articles in the platform’s database are processed
    to extract textual features, often converted into high-dimensional vectors using
    LDA or transformer based embeddings like text-ada-embedding-002\. These vectors
    are then indexed using HNSW, an algorithm suitable for high-dimensional spaces
    due to its hierarchical structure that facilitates efficient navigation and search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval Time**: When a user reads an article, the system generates a feature
    vector for this article and queries the HNSW index to find vectors (and thus articles)
    that are close in the high-dimensional space. Cosine similarity can be used to
    evaluate the similarity between the query article’s vector and those in the index,
    identifying articles with similar content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outcome**: The system recommends a list of articles ranked by their relevance
    to the currently viewed article. Thanks to the efficient indexing and similarity
    search, these recommendations are generated quickly, even from a vast database
    of articles, providing the user with a seamless experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let us walkthrough a scenario where Manhattan Distance will be preferred
    over Cosine Similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Ride-Sharing App Matchmaking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let's consider a scenario where a ride-sharing application needs to match passengers
    with nearby drivers efficiently. The system must quickly find the closest available
    drivers to a passenger’s location to minimize wait times and optimize routes.
  prefs: []
  type: TYPE_NORMAL
- en: 'How It Works:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Preprocessing and Indexing**: Drivers’ current locations are constantly being
    updated and stored as points in a 2D space representing a map. These points can
    be indexed using a tree based spatial indexing techniques or data structures optimized
    for geospatial data, such as R-trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval Time**: When a passenger requests a ride, the application uses
    the passenger’s current location as a query point. Manhattan distance (L1 norm)
    is particularly suitable for urban environments, where movement is constrained
    by a grid-like structure of streets and avenues, mimicking the actual paths a
    car would take along city blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outcome**: The system quickly identifies the nearest available drivers using
    the indexed data and Manhattan distance calculations, considering the urban grid’s
    constraints. This process ensures a swift matchmaking process, improving the user
    experience by reducing wait times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector stores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As generative AI applications continue to push the boundaries of what’s possible
    in tech, vector stores have emerged as a crucial component, streamlining and optimizing
    the search and retrieval of relevant data. In our previous discussions, we’ve
    delved into the advantages of vector DBs over traditional databases, unraveling
    the concepts of vectors, embeddings, vector search strategies, approximate nearest
    neighbors (ANNs), and similarity measures. In this section, we aim to provide
    an integrative understanding of these concepts within the realm of vector DBs
    and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: The image illustrates a workflow for transforming different types of data—Audio,
    Text, and Videos—into vector embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Audio**: An audio input is processed through an “Audio Embedding model,”
    resulting in “Audio vector embeddings.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text**: Textual data undergoes processing in a “Text Embedding model,” leading
    to “Text vector embeddings.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Videos**: Video content is processed using a “Video Embedding model,” generating
    “Video vector embeddings.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once these embeddings are created, they are subsequently utilized (potentially
    in an enterprise vector database system) to perform “Similarity Search” operations.
    This implies that the vector embeddings can be compared to find similarities,
    making them valuable for tasks such as content recommendations, data retrieval,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Multimodal embeddings process in an AI application](img/B21443_04_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Multimodal embeddings process in an AI application
  prefs: []
  type: TYPE_NORMAL
- en: What is a vector database?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **vector database** (**vector DB**) is a specialized database designed to
    handle highly dimensional vectors primarily generated from embeddings of complex
    data types such as text, images, or audio. It provides capabilities to store and
    index unstructured data and enhance searches, as well as retrieval capabilities
    as a service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modern vector databases that are brimming with advancements empower you to
    architect resilient enterprise solutions. Here, we list 15 key features to consider
    when choosing a vector DB. Every feature may not be important for your use case,
    but it might be a good place to start. Keep in mind that this area is changing
    fast, so there might be more features emerging in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Indexing**: As mentioned earlier, indexing refers to the process of organizing
    highly dimensional vectors in a way that allows for efficient similarity searches
    and retrievals. A vector DB offers built-in indexing features designed to arrange
    highly dimensional vectors for swift and effective similarity-based searches and
    retrievals. Previously, we discussed indexing algorithms such as FAISS and HNSW.
    Many vector DBs incorporate such features natively. For instance, Azure AI Search
    integrates the HNSW indexing service directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search and retrieval**: Instead of relying on exact matches, as traditional
    databases do, vector DBs provide vector search capabilities as a service, such
    as approximate nearest neighbors (ANNs), to quickly find vectors that are roughly
    the closest to a given input. To quantify the closeness or similarity between
    vectors, they utilize similarity measures such as cosine similarity or Euclidean
    distance, enabling efficient and nuanced similarity-based searches in large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create, read, update, and delete**: A vector DB manages highly dimensional
    vectors and offers create, read, update, and delete (CRUD) operations tailored
    to vectorized data. When vectors are created, they’re indexed for efficient retrieval.
    Reading often means performing similarity searches to retrieve vectors closest
    to a given query vector, typically using methods such as ANNs. Vectors can be
    updated, necessitating potential re-indexing, and they can also be deleted, with
    the database adjusting its internal structures accordingly to maintain efficiency
    and consistency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: This meets GDPR, SOC2 Type II, and HIPAA rules to easily manage
    access to the console and use SSO. Data is encrypted when stored and in transit,
    which also provides more granular identity and access management features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serverless**: A high-quality vector database is designed to gracefully autoscale
    with low management overhead as data volumes soar into millions or billions of
    entries, distributing seamlessly across several nodes. **Optimal vector** databases
    grant users the flexibility to adjust the system in response to shifts in data
    insertion, query frequencies, and underlying hardware configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid search**: Hybrid search combines traditional keyword-based search
    methods with other search mechanisms, such as semantic or contextual search, to
    retrieve results from both the exact term matches and by understanding the underlying
    intent or context of the query, ensuring a more comprehensive and relevant set
    of results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic re-ranking**: This is a secondary ranking step to improve the relevance
    of search results. It re-ranks the search results that were initially scored by
    state-of-the-art ranking algorithms such as BM25 and RRF based on language understanding.
    For instance, Azure AI search employs secondary ranking that uses multi-lingual,
    deep learning models derived from Microsoft Bing to elevate the results that are
    most relevant in terms of meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto vectorization/embedding**: Auto-embedding in a vector database refers
    to the automatic process of converting data items into vector representations
    for efficient similarity searches and retrieval, with access to multiple embedding
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data replication**: This ensures data availability, redundancy, and recovery
    in case of failures, safeguarding business continuity and reducing data loss risks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrent user access and data isolation**: Vector databases support a large
    number of users concurrently and ensure robust data isolation to ensure updates
    remain private unless deliberately shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-chunking**: Auto-chunking is the automated process of dividing a larger
    set of data or content into smaller, manageable pieces or chunks for easier processing
    or understanding. This process helps preserve the semantic relevance of texts
    and addresses the token limitations of embedding models. We will learn more about
    chunking strategies in the upcoming sections in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extensive interaction tools**: Prominent vector databases, such as Pinecone,
    offer versatile APIs and SDKs across languages, ensuring adaptability in integration
    and management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy integration**: Vector DBs provide seamless integration with LLM orchestration
    frameworks and SDKs, such as Langchain and Semantic Kernel, and leading cloud
    providers, such as Azure, GCP, and AWS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-friendly interface**: This ensures an intuitive platform with simple
    navigation and direct feature access, streamlining the user experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible pricing models**: Provides flexible pricing models as per user needs
    to keep the costs low for the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low downtime and high resiliency**: Resiliency in a vector database (or any
    database) refers to its ability to recover quickly from failures, maintain data
    integrity, and ensure continuous availability even in the face of adverse conditions,
    such as hardware malfunctions, software bugs, or other unexpected disruptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As of early 2024, a few prominent open source vector databases include Chroma,
    Milvus, Quadrant, and Weaviate, while Pinecone and Azure AI search are among the
    leading proprietary solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Vector DB limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Accuracy vs. speed trade-off**: When dealing with highly dimensional data,
    vector DBs often face a trade-off between speed and accuracy for similarity searches.
    The core challenge stems from the computational expense of searching for the exact
    nearest neighbors in large datasets. To enhance search speed, techniques such
    as ANNs are employed, which quickly identify “close enough” vectors rather than
    the exact matches. While ANN methods can dramatically boost query speeds, they
    may sometimes sacrifice pinpoint accuracy, potentially missing the true nearest
    vectors. Certain vector index methods, such as product quantization, enhance storage
    efficiency and accelerate queries by condensing and consolidating data at the
    expense of accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality of embedding**: The effectiveness of a vector database is dependent
    on the quality of the vector embedding used. Poorly designed embeddings can lead
    to inaccurate search results or missed connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity**: Implementing and managing vector databases can be complex,
    requiring specialized knowledge about vector search strategy indexing and chunking
    strategies to optimize for specific use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vector databases may not always be necessary. Small-scale applications may not
    require all the advanced features that vector DBs provide. In those instances,
    vector libraries become very valuable. Vector libraries are usually sufficient
    for small, static data and provide the ability to store in memory, index, and
    use similarity search strategies. However, they may not provide features such
    as CRUD support, data replication, and being able to store data on disk, and hence,
    the user will have to wait for a full import before they can query. Facebook’s
    FAISS is a popular example of a vector library.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, if you are dealing with millions/billions of records and
    storing data that are changing frequently, require millisecond response times,
    and more long-term storage capabilities on disk, it is recommended to use vector
    DBs over vector libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Vector DBs vs. traditional databases – Understanding the key differences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As stated earlier, vector databases have become pivotal, especially in the era
    of generative AI, because they facilitate efficient storage, querying, and retrieval
    of highly dimensional vectors that are nothing but numerical representations of
    words or sentences often produced by deep learning models. Traditional scalar
    databases are designed to handle discrete and simple data types, making them ill-suited
    for the complexities of large-scale vector data. In contrast, vector databases
    are optimized for similar searches in the vector space, enabling the rapid identification
    of vectors that are “close” or “similar” in highly dimensional spaces. Unlike
    conventional data models such as relational databases, where queries commonly
    resemble “retrieve the books borrowed by a particular member” or “identify the
    items currently discounted,” vector queries primarily seek similarities among
    vectors based on one or more reference vectors. In other words, queries might
    look like “identify the top 10 images of dogs similar to the dog in this photo”
    or “locate the best cafes near my current location.” At retrieval time, vector
    databases are crucial, as they facilitate the swift and precise retrieval of relevant
    document embeddings to augment the generation process. This technique is also
    called RAG, and we will learn more about it in the later sections.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a database of fruit images, and each image is represented by
    a vector (a list of numbers) that describes its features. Now, let’s say you have
    a photo of an apple, and you want to find similar fruits in your database. Instead
    of going through each image individually, you convert your apple photo into a
    vector using the same method you used for the other fruits. With this apple vector
    in hand, you search the database to find vectors (and therefore images) that are
    most similar or closest to your apple vector. The result would likely be other
    apple images or fruits that look like apples based on the vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Vector represenation](img/B21443_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Vector represenation
  prefs: []
  type: TYPE_NORMAL
- en: Vector DB sample scenario – Music recommendation system using a vector database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider a music streaming platform aiming to provide song recommendations
    based on a user’s current listening. Imagine a user who is listening to “Song
    X” on the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, every song in the platform’s library is represented as a
    highly dimensional vector based on its musical features and content, using embeddings.
    “Song X” also has its vector representation. When the system aims to recommend
    songs similar to “Song X,” it doesn’t look for exact matches (as traditional databases
    might). Instead, it leverages a vector DB to search for songs with vectors closely
    resembling that of “Song X.” Using an ANN search strategy, the system quickly
    sifts through millions of song vectors to find those that are approximately nearest
    to the vector of “Song X.” Once potential song vectors are identified, the system
    employs similarity measures, such as cosine similarity, to rank these songs based
    on how close their vectors are to “Song X’s” vector. The top-ranked songs are
    then recommended to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Within milliseconds, the user gets a list of songs that musically resemble “Song
    X,” providing a seamless and personalized listening experience. All this rapid,
    similarity-based recommendation magic is powered by the vector database’s specialized
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Common vector DB applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Image and video similarity search**: In the context of image and video similarity
    search, a vector DB specializes in efficiently storing and querying highly dimensional
    embeddings derived from multimedia content. By processing images through deep
    learning models, they are converted into feature vectors, a.k.a embeddings, that
    capture their essential characteristics. When it comes to videos, an additional
    step may need to be carried out to extract frames and then convert them into vector
    embeddings. Contrastive language-image pre-training (CLIP) from OpenAI is a very
    popular choice for embedding videos and images. These vector embeddings are indexed
    in the vector DB, allowing for rapid and precise retrieval when a user submits
    a query. This mechanism powers applications such as reverse image and video search,
    content recommendations, and duplicate detection by comparing and ranking content
    based on the proximity of their embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voice recognition**: Voice recognition with vectors is akin to video vectorization.
    Analog audio is digitized into short frames, each representing an audio segment.
    These frames are processed and stored as feature vectors, with the entire audio
    sequence representing things such as spoken sentences or songs. For user authentication,
    a vectorized spoken key phrase might be compared to stored recordings. In conversational
    agents, these vector sequences can be inputted into neural networks to recognize
    and classify spoken words in speech and generate responses, similar to ChatGPT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-term memory for chatbots**: Virtual database management systems (VDBMs)
    can be employed to enhance the long-term memory capabilities of chatbots or generative
    models. Many generative models can only process a limited amount of preceding
    text in prompt responses, which results in their inability to recall details from
    prolonged conversations. As these models don’t have inherent memory of past interactions
    and can’t differentiate between factual data and user-specific details, using
    VDBMs can provide a solution for storing, indexing, and referencing previous interactions
    to improve consistency and context-awareness in responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a very important use case and plays a key role in implementing RAG,
    which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The role of vector DBs in retrieval-augmented generation (RAG)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To fully understand RAG and the pivotal role of vector DBs within it, we must
    first acknowledge the inherent constraints of LLMs, which paved the way for the
    advent of RAG techniques powered by vector DBs. This section sheds light on the
    specific LLM challenges that RAG aims to overcome and the importance of vector
    DBs.
  prefs: []
  type: TYPE_NORMAL
- en: First, the big question – Why?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 1*](B21443_01.xhtml#_idTextAnchor015), we delved into the limitations
    of LLMs, which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs possess a fixed knowledge base determined by their training data; as of
    February 2024, ChatGPT’s knowledge is limited to information up until April 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can occasionally produce false narratives, spinning tales or facts that
    aren’t real.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They lack personal memory, relying solely on the input context length. For example,
    take GPT4-32K; it can only process up to 32K tokens between prompts and completions
    (we’ll dive deeper into prompts, completions, and tokens in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To counter these challenges, a promising avenue is enhancing LLM generation
    with retrieval components. These components can extract pertinent data from external
    knowledge bases—a process termed RAG, which we’ll explore further in this section.
  prefs: []
  type: TYPE_NORMAL
- en: So, what is RAG, and how does it help LLMs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval-augmented generation (RAG) was first introduced in a paper ti[tled
    *Retrieval-Augmented Generation*](https://arxiv.org/pdf/2005.11401.pdf) *for Knowledge-Intensive
    NLP Tasks* ([https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf))
    in November 2020 by Facebook AI Research (now Meta). RAG is an approach that combines
    the generative capabilities of LLMs with retrieval mechanisms to extract relevant
    information from vast datasets. LLMs, such as the GPT variants, have the ability
    to generate human-like text based on patterns in their training data but lack
    the means to perform real-time external lookups or reference specific external
    knowledge bases post-training. RAG addresses this limitation by using a retrieval
    model to query a dataset and fetch relevant information, which then serves as
    the context for the generative model to produce a detailed and informed response.
    This also helps in grounding the LLM queries with relevant information that reduces
    the chances of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: The critical role of vector DBs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vector DB plays a crucial role in facilitating the efficient retrieval aspect
    of RAG. In this setup, each piece of information, such as text, video, or audio,
    in the dataset is represented as a highly dimensional vector and indexed in a
    vector DB. When a query from a user comes in, it’s also converted into a similar
    vector representation. The vector DB then rapidly searches for vectors (documents)
    in the dataset that are closest to the query vector, leveraging techniques such
    as ANN search. Then, it attaches the query with relevant content and sends it
    to the LLMs to generate a response. This ensures that the most relevant information
    is retrieved quickly and efficiently, providing a foundation for the generative
    model to build upon.
  prefs: []
  type: TYPE_NORMAL
- en: Example of an RAG workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s walk through as an example step by step, as shown in the image. Imagine
    a platform where users can ask about ongoing cricket matches, including recent
    performances, statistics, and trivia:'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the user asks, “How did Virat Kohli perform in the last match, and what’s
    an interesting fact from that game?” Since the LLM was trained until April 2023,
    the LLM may not have this answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The retrieval model will embed the query and send it to a vector DB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the latest cricket news is stored in a vector DB in a properly indexed format
    using ANN strategies such as HNSW. The vector DB performs a cosine similarity
    with the indexed information and provides a few relevant results or contexts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The retrieved context is then sent to the LLM along with the query to synthesize
    the information and provide a relevant answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The LLM provides the relevant answer: “Virat Kohli scored 85 runs off 70 balls
    in the last match. An intriguing detail from that game is that it was the first
    time in three years that he hit more than seven boundaries in an ODI inning.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following image illustrates the preceding points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Representation of RAG workflow with vector database](img/B21443_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Representation of RAG workflow with vector database
  prefs: []
  type: TYPE_NORMAL
- en: Business applications of RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following list, we have mentioned a few popular business applications
    of RAG based on what we’ve seen in the industry:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enterprise search engines**: One of the most prominent applications of RAG
    is in the realm of enterprise learning and development, serving as a search engine
    for employee upskilling. Employees can pose questions about the company, its culture,
    or specific tools, and RAG swiftly delivers accurate and relevant answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal and compliance**: RAG fetches relevant case laws or checks business
    practices against regulations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ecommerce**: RAG suggests products or summarizes reviews based on user behavior
    and queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer support**: RAG provides precise answers to customer queries by pulling
    information from the company’s knowledge base and providing solutions in real
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical and healthcare**: RAG retrieves pertinent medical research or provides
    preliminary symptom-based suggestions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our last discussion, we delved into vector DBs and RAG. Before diving into
    RAG, we need to efficiently house our embedded data. While we touched upon indexing
    methods to speed up data fetching, there’s another crucial step to take even before
    that: chunking.'
  prefs: []
  type: TYPE_NORMAL
- en: What is chunking?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of building LLM applications with embedding models, chunking
    involves dividing a long piece of text into smaller, manageable pieces or “chunks”
    that fit within the model’s token limit. The process involves breaking text into
    smaller segments before sending these to the embedding models. As shown in the
    following image, chunking happens before the embedding process. Different documents
    have different structures, such as free-flowing text, code, or HTML. So, different
    chunking strategies can be applied to attain optimal results. Tools such as Langchain
    provide you with functionalities to chunk your data efficiently based on the nature
    of the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram below depicts a data processing workflow, highlighting the chunking
    step, starting with raw “Data sources” that are converted into “Documents.” Central
    to this workflow is the “Chunk” stage, where a “TextSplitter” breaks the data
    into smaller segments. These chunks are then transformed into numerical representations
    using an “Embedding model” and are subsequently indexed into a “Vector DB” for
    efficient search and retrieval. The text associated with the retrieved chunks
    is then sent as context to the LLMs, which then generate a final response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.12 – Chunking Process](img/B21443_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fig 4.12 – Chunking Process
  prefs: []
  type: TYPE_NORMAL
- en: But why is it needed?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chunking is vital for two main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Chunking strategically divides document text to enhance its comprehension by
    embedding models, and it boosts the relevance of the content retrieved from a
    vector DB. Essentially, it refines the accuracy and context of the results sourced
    from the database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It tackles the token constraints of embedding models. For instance, Azure’s
    OpenAI embedding models like text-embedding-ada-002 can handle up to 8,191 tokens,
    which is about 6,000 words, given each token averages four characters. So, for
    optimal embeddings, it’s crucial our text stays within this limit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular chunking strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Fixed-size chunking**: This is a very common approach that defines a fixed
    size (200 words), which is enough to capture the semantic meaning of a paragraph,
    and it incorporates an overlap of about 10–15% as an input to the vector embedding
    generation model. Chunking data with a slight overlap between text ensures context
    preservation. It’s advisable to begin with a roughly 10% overlap. Below is a snippet
    of code that demonstrates the use of fixed-size chunking with LangChain:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Variable-size chunking**: Variable-size chunking refers to the dynamic segmentation
    of data or text into varying-sized components, as opposed to fixed-size divisions.
    This approach accommodates the diverse structures and characteristics present
    in different types of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence splitting**: Sentence transformer models are neural architectures
    optimized for embedding at the sentence level. For example, BERT works best when
    chunked at the sentence level. Tools such as NLTK and SpaCy provide functions
    to split the sentences within a text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specialized chunking**: Documents, such as research papers, possess a structured
    organization of sections, and the Markdown language, with its unique syntax, necessitates
    specialized chunking, resulting in the proper separation between sections/pages
    to yield contextually relevant chunks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code Chunking**: When embedding code into your vector DB, this technique
    can be invaluable. Langchain supports code chunking for numerous languages. Below
    is a snippet code to chunk your Python code:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Chunking considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chunking strategies vary based on **data type** and **format** and the **chosen
    embedding model**. For instance, code requires a distinct chunking approach compared
    to unstructured text. While models such as text-embedding-ada-002 excel with 256-
    and 512-token-sized chunks, our understanding of chunking is ever-evolving. Moreover,
    preprocessing plays a crucial role before chunking, where you can optimize your
    content by removing unnecessary text content, such as stop words, special symbols,
    etc., that add noise. For the latest techniques, we suggest regularly checking
    the text splitters section in the LangChain documentation, ensuring you employ
    the best strategy for your needs
  prefs: []
  type: TYPE_NORMAL
- en: '(Split by tokens from Langchain: [https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token)).'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of RAG using Azure Prompt Flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up to this point, we have discussed the development of resilient RAG applications.
    However, the question arises: How can we determine whether these applications
    are functioning as anticipated and if the context they retrieve is pertinent?
    While manual validation—comparing the responses generated by LLMs against ground
    truth—is possible, this method proves to be labor-intensive, costly, and challenging
    to execute on a large scale. Consequently, it’s essential to explore methodologies
    that facilitate automated evaluation on a vast scale. Recent research has delved
    into the concept of utilizing “LLM as a judge” to assess output, a strategy that
    Azure Prompt Flow incorporates within its offerings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Prompt Flow has built-in and structured metaprompt templates with comprehensive
    guardrails to evaluate your output against ground truth. The following mentions
    four metrics that can help you evaluate your RAG solution in Prompt Flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Groundedness**: Measures the alignment of the model’s answers with the input
    source, making sure the model’s generated response is not fabricated. The model
    must always extract information from the provided “context” while responding to
    user’s query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevance**: Measures the degree to which the model’s generated response
    is closely connected to the context and user query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval score**: Measures the extent to which the model’s retrieved documents
    are pertinent and directly related to the given questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom metrics**: While the above three are the most important for evaluating
    RAG applications, Prompt Flow allows you to use custom metrics, too. Bring your
    own LLM as a judge and define your own metrics by modifying the existing metaprompts.
    This also allows you to use open source models such as Llama and to build your
    own metrics from code with Python functions. The above evaluations are more no-code
    or low-code friendly; however, for a more pro-code friendly approach, azureml-metrics
    SDK, such as ROUGE, BLEU, F1-Score, Precision, and Accuracy, can be utilized as
    well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The field is advancing quickly, so we recommend regularly checking Azure ML
    Prompt Flow’s latest updates on evaluation metrics. Start with the “Manual Evaluation”
    feature in Prompt Flow to gain a basic understanding of LLM performance. It’s
    important to use a mix of metrics for a thorough evaluation that captures both
    semantic and syntactic essence rather than relying on just one metric to compare
    the responses with the actual ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – Global chat application deployment by a multinational organization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A global firm recently launched an advanced internal chat application featuring
    a Q&A support chatbot. This innovative tool, deployed across various Azure regions,
    integrates several large language models, including the specialized finance model,
    BloombergGPT. To meet specific organizational requirements, bespoke plugins were
    developed. It had an integration with Service Now, empowering the chatbot to streamline
    ticket generation and oversee incident actions.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of data refinement, the company meticulously preprocessed its knowledge
    base (KB) information, eliminating duplicates, special symbols, and stop words.
    The KB consisted of answers to frequently asked questions and general information
    to various support-related questions. They employed fixed chunking approaches,
    exploring varied chunk sizes, before embedding these data into the Azure AI search.
    Their methodology utilized Azure OpenAI’s text-ada-embedding-002 models in tandem
    with the cosine similarity metric and Azure AI search’s vector search capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: From their extensive testing, they discerned optimal results with a chunk size
    of 512 tokens and a 10% overlap. Moreover, they adopted an ANN vector search methodology
    using cosine similarity. They also incorporated hybrid search that included keyword
    and semantic search with Semantic Reranker. Their RAG workflow, drawing context
    from Azure Vector Search and the GPT 3.5 Turbo-16K models, proficiently generated
    responses to customer support inquiries. They implemented caching techniques using
    Azure Cache Redis and rate-limiting strategies using Azure API Management to optimize
    the costs.
  prefs: []
  type: TYPE_NORMAL
- en: The integration of the support Q&A chatbot significantly streamlined the multinational
    firm’s operations, offering around-the-clock, consistent, and immediate responses
    to queries, thereby enhancing user satisfaction. This not only brought about substantial
    cost savings by reducing human intervention but also ensured scalability to handle
    global demands. By automating tasks such as ticket generation, the firm gained
    deeper insights in[to user interactions, allowing for continuous improvement and
    refinement of their services.](https://arxiv.org/pdf/2309.11322.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the RAG approach, a powerful method for leveraging
    your data to craft personalized experiences, reduce hallucinations while also
    addressing the training limitations inherent in LLMs. Our journey began with an
    examination of foundational concepts such as vectors and databases, with a special
    focus on Vector Databases. We understood the critical role that Vector DBs play
    in the development of RAG-based applications, also highlighting how they can enhance
    LLM responses through effective chunking strategies. The discussion also covered
    practical insights on building engaging RAG experiences, evaluating them through
    prompt flow, and included a hands-on lab available on GitHub to apply what we’ve
    learned.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will introduce another popular technique designed to
    minimize hallucinations and more easily steer the responses of LLMs. We will cover
    prompt engineering strategies, empowering you to fully harness the capabilities
    of your LLMs and engage more effectively with AI. This exploration will provide
    you with the tools and knowledge to enhance your interactions with AI, ensuring
    more reliable and contextually relevant outputs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vector database management systems: Fundamental concepts, use-cases, and current
    challenges : [https://arxiv.org/pdf/2309.11322.pdf](https://arxiv.org/pdf/2309.11322.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Two minutes NLP — 11 word embeddings models you should know | by Fabio Chiusano
    | NLPlanet | Medium - [https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9](https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'How To Choose The Right Embedding Model For You | by Chebbah Mehdi | Medium:
    [https://medium.com/@mehdi_chebbah/how-to-choose-the-right-embedding-model-for-you-1fc917d14517](mailto:https://medium.com/@mehdi_chebbah/how-to-choose-the-right-embedding-model-for-you-1fc917d14517)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Gentle Introduction to Vector Databases | Weaviate - vector database - [https://weaviate.io/blog/what-is-a-vector-database](https://weaviate.io/blog/what-is-a-vector-database)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vector Library versus Vector Database | Weaviate - vector database - [https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database](https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Efficient and robust approximate nearest neighbor search using Hierarchical
    Navigable Small World graphs - [https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Introduction Milvus v2.0.x documentation: [https://milvus.io/docs/v2.0.x/overview.md](https://milvus.io/docs/v2.0.x/overview.md)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The 5 Best Vector Databases | A List With Examples | DataCamp - [https://www.datacamp.com/blog/the-top-5-vector-databases](https://www.datacamp.com/blog/the-top-5-vector-databases)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vector Library versus Vector Database | Weaviate - vector database - [https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database](https://weaviate.io/blog/vector-library-vs-vector-database#feature-comparison---library-versus-database)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RAG: [https://milvus.io/docs/v2.0.x/overview.md](https://milvus.io/docs/v2.0.x/overview.md)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunk documents in vector search - Azure Cognitive Search | Microsoft Learn
    - [https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunking Strategies for LLM Applications | Pinecone - [https://www.pinecone.io/learn/chunking-strategies/](https://www.pinecone.io/learn/chunking-strategies/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Product Quantization: Compressing high-dimensional vectors by 97% | Pinecone:
    [https://www.pinecone.io/learn/series/faiss/product-quantization/](https://www.pinecone.io/learn/series/faiss/product-quantization/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation and monitoring metrics for generative AI - Azure AI Studio | Microsoft
    Learn - [https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
