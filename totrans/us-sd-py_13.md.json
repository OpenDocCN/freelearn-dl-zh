["```py\n    import torch\n    ```", "```py\n    from diffusers import StableDiffusionPipeline\n    ```", "```py\n    # load model\n    ```", "```py\n    text2img_pipe = StableDiffusionPipeline.from_pretrained(\n    ```", "```py\n        \"stablediffusionapi/deliberate-v2\",\n    ```", "```py\n         torch_dtype = torch.float16\n    ```", "```py\n    ).to(\"cuda:0\")\n    ```", "```py\n    # generate sample image\n    ```", "```py\n    prompt = \"\"\"\n    ```", "```py\n    high resolution photo,best quality, masterpiece, 8k\n    ```", "```py\n    A cute cat stand on the tree branch, depth of field, detailed body\n    ```", "```py\n    \"\"\"\n    ```", "```py\n    neg_prompt = \"\"\"\n    ```", "```py\n    paintings,ketches, worst quality, low quality, normal quality, lowres,\n    ```", "```py\n    monochrome, grayscale\n    ```", "```py\n    \"\"\"\n    ```", "```py\n    image = text2img_pipe(\n    ```", "```py\n        prompt = prompt,\n    ```", "```py\n        negative_prompt = neg_prompt,\n    ```", "```py\n        generator = torch.Generator(\"cuda\").manual_seed(7)\n    ```", "```py\n    ).images[0]\n    ```", "```py\n    image\n    ```", "```py\n    pip install opencv-contrib-python\n    ```", "```py\n    pip install controlnet_aux\n    ```", "```py\n    from controlnet_aux import CannyDetector\n    ```", "```py\n    canny = CannyDetector()\n    ```", "```py\n    image_canny = canny(image, 30, 100)\n    ```", "```py\n    from diffusers import ControlNetModel\n    ```", "```py\n    canny_controlnet = ControlNetModel.from_pretrained(\n    ```", "```py\n        'takuma104/control_v11',\n    ```", "```py\n        subfolder='control_v11p_sd15_canny',\n    ```", "```py\n        torch_dtype=torch.float16\n    ```", "```py\n    )\n    ```", "```py\n    from diffusers import StableDiffusionControlNetImg2ImgPipeline\n    ```", "```py\n    cn_pipe = \\\n    ```", "```py\n        StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n    ```", "```py\n        \"stablediffusionapi/deliberate-v2\",\n    ```", "```py\n        torch_dtype = torch.float16,\n    ```", "```py\n        controlnet = canny_controlnet\n    ```", "```py\n    )\n    ```", "```py\n    prompt = \"\"\"\n    ```", "```py\n    high resolution photo,best quality, masterpiece, 8k\n    ```", "```py\n    A cute dog stand on the tree branch, depth of field, detailed body\n    ```", "```py\n    \"\"\"\n    ```", "```py\n    neg_prompt = \"\"\"\n    ```", "```py\n    paintings,ketches, worst quality, low quality, normal quality, lowres,\n    ```", "```py\n    monochrome, grayscale\n    ```", "```py\n    \"\"\"\n    ```", "```py\n    image_from_canny = single_cn_pipe(\n    ```", "```py\n        prompt = prompt,\n    ```", "```py\n        negative_prompt = neg_prompt,\n    ```", "```py\n        image = canny_image,\n    ```", "```py\n        generator = torch.Generator(\"cuda\").manual_seed(2),\n    ```", "```py\n        num_inference_steps = 30,\n    ```", "```py\n        guidance_scale = 6.0\n    ```", "```py\n    ).images[0]\n    ```", "```py\n    image_from_canny\n    ```", "```py\nfrom controlnet_aux import NormalBaeDetector\nnormal_bae = \\\n    NormalBaeDetector.from_pretrained(\"lllyasviel/Annotators\")\nimage_canny = normal_bae(image)\nimage_canny\n```", "```py\nfrom diffusers import ControlNetModel\ncanny_controlnet = ControlNetModel.from_pretrained(\n    'takuma104/control_v11',\n    subfolder='control_v11p_sd15_canny',\n    torch_dtype=torch.float16\n)\nbae_controlnet = ControlNetModel.from_pretrained(\n    'takuma104/control_v11',\n    subfolder='control_v11p_sd15_normalbae',\n    torch_dtype=torch.float16\n)\ncontrolnets = [canny_controlnet, bae_controlnet]\n```", "```py\nfrom diffusers import StableDiffusionControlNetPipeline\ntwo_cn_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"stablediffusionapi/deliberate-v2\",\n    torch_dtype = torch.float16,\n    controlnet = controlnets\n).to(\"cuda\")\n```", "```py\nprompt = \"\"\"\nhigh resolution photo,best quality, masterpiece, 8k\nA cute dog on the tree branch, depth of field, detailed body,\n\"\"\"\nneg_prompt = \"\"\"\npaintings,ketches, worst quality, low quality, normal quality, lowres,\nmonochrome, grayscale\n\"\"\"\nimage_from_2cn = two_cn_pipe(\n    prompt = prompt,\n    image = [canny_image,bae_image],\n    controlnet_conditioning_scale = [0.5,0.5],\n    generator = torch.Generator(\"cuda\").manual_seed(2),\n    num_inference_steps = 30,\n    guidance_scale = 5.5\n).images[0]\nimage_from_2cn\n```", "```py\ncontrol_v11p_sd15_canny\ncontrol_v11p_sd15_mlsd\ncontrol_v11f1p_sd15_depth\ncontrol_v11p_sd15_normalbae\ncontrol_v11p_sd15_seg\ncontrol_v11p_sd15_inpaint\ncontrol_v11p_sd15_lineart\ncontrol_v11p_sd15s2_lineart_anime\ncontrol_v11p_sd15_openpose\ncontrol_v11p_sd15_scribble\ncontrol_v11p_sd15_softedge\ncontrol_v11e_sd15_shuffle\ncontrol_v11e_sd15_ip2p\ncontrol_v11f1e_sd15_tile\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nsdxl_pipe = StableDiffusionXLPipeline.from_pretrained(\n    \"RunDiffusion/RunDiffusion-XL-Beta\",\n    torch_dtype = torch.float16,\n    load_safety_checker = False\n)\nsdxl_pipe.watermark = None\n```", "```py\nfrom diffusers import EulerDiscreteScheduler\nprompt = \"\"\"\nfull body photo of young man, arms spread\nwhite blank background,\nglamour photography,\nupper body wears shirt,\nwears suit pants,\nwears leather shoes\n\"\"\"\nneg_prompt = \"\"\"\nworst quality,low quality, paint, cg, spots, bad hands,\nthree hands, noise, blur, bad anatomy, low resolution, blur face, bad face\n\"\"\"\nsdxl_pipe.to(\"cuda\")\nsdxl_pipe.scheduler = EulerDiscreteScheduler.from_config(\n    sdxl_pipe.scheduler.config)\nimage = sdxl_pipe(\n    prompt = prompt,\n    negative_prompt = neg_prompt,\n    width = 832,\n    height = 1216\n).images[0]\nsdxl_pipe.to(\"cpu\")\ntorch.cuda.empty_cache()\nimage\n```", "```py\nfrom controlnet_aux import OpenposeDetector\nopen_pose = \\\n    OpenposeDetector.from_pretrained(\"lllyasviel/Annotators\")\npose = open_pose(image)\npose\n```", "```py\nfrom diffusers import StableDiffusionXLControlNetPipeline\nfrom diffusers import ControlNetModel\nsdxl_pose_controlnet = ControlNetModel.from_pretrained(\n    \"thibaud/controlnet-openpose-sdxl-1.0\",\n    torch_dtype=torch.float16,\n)\nsdxl_cn_pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"RunDiffusion/RunDiffusion-XL-Beta\",\n    torch_dtype = torch.float16,\n    load_safety_checker = False,\n    add_watermarker = False,\n    controlnet = sdxl_pose_controlnet\n)\nsdxl_cn_pipe.watermark = None\n```", "```py\nfrom diffusers import EulerDiscreteScheduler\nprompt = \"\"\"\nfull body photo of young woman, arms spread\nwhite blank background,\nglamour photography,\nwear sunglass,\nupper body wears shirt,\nwears suit pants,\nwears leather shoes\n\"\"\"\nneg_prompt = \"\"\"\nworst quality,low quality, paint, cg, spots, bad hands,\nthree hands, noise, blur, bad anatomy, low resolution,\nblur face, bad face\n\"\"\"\nsdxl_cn_pipe.to(\"cuda\")\nsdxl_cn_pipe.scheduler = EulerDiscreteScheduler.from_config(\n    sdxl_cn_pipe.scheduler.config)\ngenerator = torch.Generator(\"cuda\").manual_seed(2)\nimage = sdxl_cn_pipe(\n    prompt = prompt,\n    negative_prompt = neg_prompt,\n    width = 832,\n    height = 1216,\n    image = pose,\n    generator = generator,\n    controlnet_conditioning_scale = 0.5,\n    num_inference_steps = 30,\n    guidance_scale = 6.0\n).images[0]\nsdxl_cn_pipe.to(\"cpu\")\ntorch.cuda.empty_cache()\nimage\n```"]