- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the main chapters of this book, we explored the power of OpenAI’s models
    through the lens of ChatGPT, diving into its conversational interface and understanding
    how it can revolutionize the way we interact with AI. However, the world of OpenAI
    extends beyond ChatGPT’s familiar chat-based experience. To fully harness the
    potential of these models, it’s crucial to understand the broader tools and interfaces
    OpenAI provides.
  prefs: []
  type: TYPE_NORMAL
- en: 'This appendix is dedicated to exploring one such tool: the **OpenAI Playground**.
    The Playground offers a versatile environment to experiment with OpenAI’s models,
    granting more control over parameters, outputs, and behaviors. Whether you want
    to fine-tune responses, test different use cases, or simply gain a deeper understanding
    of the models’ capabilities, the Playground is an invaluable resource.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this appendix, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Walk through the Playground interface and its key features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illustrate how to interact with OpenAI models directly from the Playground.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offer tips and best practices to maximize your outcomes when using the Playground.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this appendix, you’ll have the knowledge and confidence to use
    OpenAI’s Playground and its models, going beyond ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Trying OpenAI models in the Playground
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To access an OpenAI Playground, you need to create an OpenAI account and navigate
    through [to https://platform.openai.com/playgro](https://platform.openai.com/playground)und.
    This is how the landing page looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](img/Appendix_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: OpenAI Playground at https://platform.openai.com/playground'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from *Figure 1*, the Playground offers a UI where the user can
    start interacting with the model, which you can select at the top of your chat
    interface. Note that, whenever consuming models via the OpenAI Playground, you
    will be charged a fee depending on the amount of interactions. You can find the
    pricing page at https://openai.com/api/pricing/.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving deeper into the main sections of the Playground, let’s first
    define some jargon you will see in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokens**: Tokens can be considered as word fragments or segments that are
    used by the API to process input prompts. Unlike complete words, tokens may contain
    trailing spaces or even word segments. As a general rule of thumb, one token in
    English is approximately equivalent to four characters, or three-quarters of a
    word (you can refer to the following link to convert words to tokens in the context
    of OpenAI models: https://platform.openai.com/tokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt**: In the context of **natural language processing** (**NLP**) and
    Generative AI, a prompt refers to a piece of text that is given as input to an
    AI language model to generate a response or output. The prompt can be a question,
    a statement, or a sentence, and it is used to provide context and direction to
    the language model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context**: In the field of GPT, context refers to the words and sentences
    that come before the user’s prompt. This context is used by the language model
    to generate the most probable next word or phrase, based on the patterns and relationships
    found in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model confidence**: Model confidence refers to the level of certainty or
    probability that an AI model assigns to a particular prediction or output. In
    the context of NLP, model confidence is often used to indicate how confident the
    AI model is in the correctness or relevance of its generated response to a given
    input prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools**: With tools, we provide the model with an extra skill that it can
    invoke to accomplish the user’s task. A function will always have a description
    in natural language so that the model knows when to invoke it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the Playground, there are four main sections to interact with the models.
    Let’s explore them in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Chat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, you can test all the chat models available today, including both text-only
    models (like GPT-3.5) and multimodal models (like GPT-4o). You can provide a system
    message – the set of instructions that you provide your model with – all in natural
    language.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, the system message is an instruction provided at the
    beginning of a conversation to establish the model’s role, behavior, and response
    guidelines. This message sets the overarching context, guiding the model’s interactions
    to align with specific objectives or constraints. For example, a system message
    might specify that the model should act as a friendly travel advisor or maintain
    a formal tone. This configuration can be set at the backend level by the AI developer,
    so that the end user will not have access to it and, henceforth, will not be able
    to “force” the model to behave differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also compare the output of two different models, given the same question.
    The following is an example of how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](img/Appendix_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An example of comparison between two models'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each model, you can also play with some parameters that you can configure.
    Here is a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temperature** (ranging from 0 to 2): This controls the randomness of the
    model’s response. A low-level temperature makes your model more deterministic,
    meaning that it will tend to give the same output to the same question. For example,
    if I ask my model multiple times *What is OpenAI?* with the temperature set as
    0, it will give, most of the time, the same answer. On the other hand, if I do
    the same with a temperature greater than 0, it will try to modify its answers
    each time, in terms of wording and style.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max tokens**: This controls the length (in terms of tokens) of the model’s
    response to the user’s prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop sequences** (user input): This makes responses end at the desired point,
    such as the end of a sentence or list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top probabilities** (ranging from 0 to 1): This controls which tokens the
    model will consider when generating a response. This means that the model will
    select from the smallest set of tokens whose cumulative probability adds up to
    90% of the distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequency penalty** (ranging from 0 to 1): This controls the repetition of
    the same tokens in the generated response. The higher the penalty, the lower the
    probability of seeing the same tokens more than once in the same response. The
    penalty reduces the chance proportionally, based on how often a token has appeared
    in the text so far (this is the key difference from the following parameter).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Presence penalty** (ranging from 0 to 2): This is similar to the previous
    parameter but stricter. It reduces the chance of repeating any token that has
    appeared in the text at all so far. As it is stricter than the frequency penalty,
    the presence penalty also increases the likelihood of introducing new topics in
    a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assistants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**OpenAI Assistants** can be seen as a way to develop AI agents faster and
    more easily. In fact, Assistants can be defined as entities powered by an LLM,
    with a set of instructions to follow and a set of tools or plugins to use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of OpenAI Assistants, they come with three pre-built tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**File Search**: This allows the user to upload custom documents so that the
    Assistant can navigate through them to accomplish the user’s query. It operates
    with a RAG-based framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Function Calling**: This allows the user to define a set of custom functions
    that can be invoked by the Assistant to accomplish a given task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code Interpreter**: This refers to the capability of the Assistant to run
    code either against provided documents (for example, in the case of spreadsheets
    or analytical papers that require mathematical computations) or simply to solve
    complex tasks provided by the user (for example, complex mathematical problems).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see an example of an Assistant called
    **Chat with PDF**, which specializes in responding to provided documents (in my
    case, I uploaded the paper *LLaMA: Open and Efficient Foundation Language Models*
    by Hugo Touvron et al.).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/Appendix_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Example of an OpenAI Assistant'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, the Assistant was able to answer
    my question, retrieving knowledge from the provided document. In fact, my question
    was pretty vague, since the term *toxicity* can refer to multiple domains; nevertheless,
    the Assistant knows to watch over the provided documents as the primary source
    of information.
  prefs: []
  type: TYPE_NORMAL
- en: Completions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section refers to a class of models called **base models**, like GPT-3\.
    They are the basis on which the so-called “assistant models” (or chat models,
    as we saw previously) are built. For example, the chat model GPT-3.5 Turbo (the
    model behind ChatGPT) is a fine-tuned version of the base model GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Completions (base) models are designed for generating single responses to prompts,
    making them suitable for tasks like text generation and summarization without
    maintaining context over multiple interactions. Chat (assistant) models, on the
    other hand, are optimized for interactive conversations, capable of maintaining
    context across multiple turns, and are ideal for applications like chatbots and
    virtual assistants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below you can see an example of a typical completion task in the Playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/Appendix_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Example of completion task in OpenAI Playground'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, using my words “Today I went to a grocery store and” the model
    completed the sentence with the most likely words.
  prefs: []
  type: TYPE_NORMAL
- en: Today, completion models are rarely used as they are outperformed by chat models,
    yet they can be further fine-tuned to tailored use cases (we will cover fine-tuning
    later on in this section).
  prefs: []
  type: TYPE_NORMAL
- en: Text to speech
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to *Whisper*, the aforementioned speech-to-text model, OpenAI also
    released a **text-to-speech** (**TTS**) model that can be tested directly in the
    Playground.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a video chat  Description automatically generated](img/Appendix_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Example of using OpenAI’s TTS models in the Playground'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the above screenshot, you can select the voice, model, speed,
    and format of the generated audio.
  prefs: []
  type: TYPE_NORMAL
- en: All the previous models come pre-built, in the sense that they have already
    been pre trained on a huge knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some ways you can make your model more customized and tailored
    for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing your model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first method of tailoring your model for your use case is embedded in the
    way the model is designed, and it involves providing your model with the context
    in the few-shot learning approach.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you could ask the model to generate an article whose template and
    lexicon recall another one you have already written. For this, you can provide
    the model with your query of generating an article and also with the former article
    as a reference or context, so that the model is better prepared for your request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](img/Appendix_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An example of a conversation within the OpenAI Playground with the
    few-shot learning approach'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, I instructed the model to output only the label of
    the tweet’s sentiment, providing it with three examples of how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: The second method of customizing your model is more sophisticated and is called
    **fine-tuning**. Fine-tuning is the process of adapting a pre trained model to
    a new task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fine-tuning, the parameters of the pre trained model are altered, either
    by adjusting the existing parameters or by adding new parameters, to better fit
    the data for the new task. This is done by training the model on a smaller labeled
    dataset that is specific to the new task. The key idea behind fine-tuning is to
    leverage the knowledge learned from the pre trained model and fine-tune it to
    the new task, rather than training a model from scratch. Have a look at the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Appendix_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Model fine-tuning'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding figure, you can see a schema on how fine-tuning works on OpenAI
    pre-built models. The idea is that you have available a pre trained model with
    general-purpose weights or parameters. Then, you feed your model with custom data,
    typically in the form of *key-value* prompts and completions, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once the training is done, you will have a customized model that performs particularly
    well for a given task, for example, the classification of your company’s documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The nice thing about fine-tuning is that you can make pre-built models tailored
    to your use cases, without the need to re-train them from scratch, yet leveraging
    smaller training datasets and hence needing less training time and computing.
    At the same time, the model keeps its generative power and accuracy learned via
    the original training, the one that was carried out on the massive dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenAI Playground presents a powerful tool for experimenting with advanced
    AI models through zero- or few-shot learning and fine-tuning techniques. The Playground
    allows users to interact directly with pre trained models, making it easier to
    customize and enhance them for specific tasks, such as sentiment analysis or document
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: For developers looking to build AI applications that leverage OpenAI’s API,
    mastering these techniques is crucial to ascertain whether a specific model’s
    configuration will meet a specific application’s requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the focus of this book being mainly on ChatGPT, enterprise-scale scenarios
    (which we covered in *Chapter 10*) require more customized approaches when it
    comes to AI use cases; that’s why familiarizing yourself with the concept of the
    Playground and OpenAI models’ APIs is of a great value to embrace the mindset
    of this new wave of AI-powered application development.
  prefs: []
  type: TYPE_NORMAL
- en: Join our communities on Discord and Reddit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have questions about the book or want to contribute to discussions on Generative
    AI and LLMs? Join our Discord server at [https://packt.link/I1tSU](Appendix.xhtml)
    and our Reddit channel at [https://packt.link/jwAmA](Appendix.xhtml) to connect,
    share, and collaborate with like-minded enthusiasts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Discord.png) ![](img/QR_Code757615820155951000.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/New_Packt_Logo1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[packt.com](http://packt.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve your learning with Skill Plans built especially for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At [www.packt.com](http://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  prefs: []
  type: TYPE_NORMAL
- en: Other Books You May Enjoy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/9781835087718.png)](https://www.packtpub.com/en-in/product/generating-creative-images-with-dall-e-3-9781835089903)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating Creative Images With DALL-E 3**'
  prefs: []
  type: TYPE_NORMAL
- en: Holly Picano
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 9781835087718'
  prefs: []
  type: TYPE_NORMAL
- en: Master DALL-E 3’s architecture and training methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create fine prints and other AI-generated art with precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seamlessly blend AI with traditional artistry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Address ethical dilemmas in AI art
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the future of digital creativity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement practical optimization techniques for your artistic endeavors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/9781835884003.png)](https://www.packtpub.com/en-in/product/building-ai-applications-with-openai-apis-9781835884010)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Building AI Applications with OpenAI APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: Martin Yanev
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 9781835884003'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a solid foundation in using the OpenAI API for NLP tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build, deploy, and integrate payments into various desktop and SaaS AI applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate ChatGPT with frameworks such as Flask, Django, and Microsoft Office
    APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unleash your creativity by integrating DALL-E APIs to generate stunning AI art
    within your desktop apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experience the power of Whisper API’s speech recognition and text-to-speech
    features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find out how to fine-tune ChatGPT models for your specific use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master AI embeddings to measure the relatedness of text strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packt is searching for authors like you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](http://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you’ve finished *Practical Generative AI with ChatGPT, Second Edition*,
    we’d love to hear your thoughts! If you purchased the book from Amazon, please
    [click here to go straight to the Amazon review page](https://packt.link/r/1836647859)
    for this book and share your feedback or leave a review on the site that you purchased
    it from.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
