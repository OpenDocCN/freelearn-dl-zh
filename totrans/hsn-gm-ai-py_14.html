<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">DRL Frameworks</h1>
                </header>
            
            <article>
                
<p class="mce-root">Working through and exploring the code in this book is meant to be a learning exercise in how <strong>Reinforcement Learning</strong> (<strong>RL</strong>) algorithms work but also how difficult it can be to get them to work. It is because of this difficulty that so many open source RL frameworks seem to pop up every day. In this chapter, we will explore a couple of the more popular frameworks. We will start with why you would want to use a framework and then move on to exploring the more popular frameworks such as Dopamine, Keras-RL, TF-Agents, and RL Lib. </p>
<p>Here is a quick summary of the main topics we will cover in this chapter:</p>
<ul>
<li>Choosing a framework</li>
<li>Introducing Google Dopamine</li>
<li>Playing with Keras-RL</li>
<li>Exploring RL Lib</li>
<li>Using TF agents</li>
</ul>
<p>We will use a combination of notebook environments on Google Colab and virtual environments depending on the complexity of the examples in this chapter. Jupyter Notebooks, which Colab is based on, is an excellent way to demonstrate code. It is generally not the preferred way to develop code and this is the reason we avoided this method until now.</p>
<p>In the next section, we look at why you would want to choose a framework.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing a framework</h1>
                </header>
            
            <article>
                
<p><span>As you may have surmised by now, writing your own RL algorithms and functions on top of a deep learning framework, such as PyTorch, is not trivial. It is also important to remember that the algorithms in this book go back about 30 years over the development of RL. That means that any serious new advances in RL take substantial effort and time—yes, for both development and especially training. Unless you have the time, resources, and incentive for developing your own framework, then it is highly recommended to graduate using a mature framework. However, there is an ever-increasing number of new and comparable frameworks out there, so you may find that you are unable to choose just one. Until one of these frameworks achieves true AGI, then you may also need separate frameworks for different environments or even different tasks. </span></p>
<div class="packt_infobox">Remember, <strong>AGI</strong> stands for <strong>Artificial General Intelligence</strong> and it really is the goal of any RL framework to be AGI. An AGI framework can be trained on any environment. An advanced AGI framework may be able to transfer learning across tasks. Transfer learning is where an agent can learn one task and then use those learnings to accomplish another similar task.</div>
<p>We are going to look at the current most popular frameworks that have the most promise, in later sections. It is important to compare the various current frameworks to see whether one may be a better choice for you and your team. Therefore, we will look at a comparison of the various RL frameworks currently available in the following list.</p>
<p>This list features the current most popular frameworks ordered by current popularity (by Google), but this list is expected to change with time:</p>
<ul>
<li><strong>OpenAI Gym and Baselines</strong>: OpenAI Gym is the framework we have used for most of the environments we have explored in this book. This library also has a companion called Baselines that provides several agents for, you guessed it, baselining the Gym environments. Baselines is also a very popular and good RL framework but we have omitted it here in favor of looking at other libraries. </li>
<li><strong>Google Dopamine</strong>: This is a relatively new framework that has gained popularity very quickly. This is likely due, in part, to its implementation of the RainbowDQN agent. The framework is well developed but has been described as being clunky and not very modular. We showcase it here because it is popular and you will likely want a closer look at it anyway.</li>
<li><strong>ML-Agents</strong>: We have more or less already covered a whole chapter on this framework, so we won't need to explore it here. Unity has developed a very solid but not very modular framework. The implementation currently only supports PG methods such as PPO and Soft Actor-Critic. ML-Agents on its own, however, it can be a great and recommended way to demonstrate RL to development teams or even introduce concepts to clients.</li>
<li><strong>RL Lib with the ray-project</strong>: This has strange origins in that it started as a parallelization project for Python and evolved into a training platform for RL. As such, it tends to favor training regimes that use asynchronous agents such as A3C, and it is well suited to complex environments. Not to mention, this project is based on PyTorch so it will be worth a look.</li>
<li><strong>Keras-RL</strong>: Keras itself is another deep learning framework that is very popular on its own. The deep learning library itself is quite concise and easy to use—perhaps in some ways, too easy. However, it can be an excellent way to prototype an RL concept or environment and deserves a closer look by us.</li>
<li><strong>TRFL</strong>: This library, not unlike Keras-RL, is an extension of the TensorFlow framework to incorporate RL. TensorFlow is another low-level deep learning framework. As such, the code to build any working agent also needs to be quite a low level and using this library likely won't be for you, especially if you enjoy PyTorch.</li>
<li><strong>Tensorforce</strong>: This is another library focused on extending TensorFlow for RL. The benefit of using a TF-based solution is cross-compatibility and even the ability to port your code to web or mobile. However, building low-level computational graphs is not for everyone and does require a higher level of mathematics than we covered in this book.</li>
<li><strong>Horizon</strong>: This framework is from Facebook and is developed on top of PyTorch. Unfortunately, the benefits of this framework fall short in several areas including not having a <kbd>pip</kbd> installer. It also lacks tight integration with Gym environments so, unless you work at Facebook, you will likely want to avoid this framework.</li>
<li><strong>Coach</strong>: This is one of those sleeper frameworks that could build a substantial following of its own someday. There are many useful and powerful features to Coach, including a dedicated dashboard and direct support for Kubernetes. This framework also currently boasts the largest implementation of RL algorithms and will give you plenty of room to explore. Coach is a framework worth exploring on your own.</li>
<li><strong>MAgent</strong>: This project is similar to RLLib (Ray) in that it specializes in training multiple agents asynchronously or in various configurations. It is developed on top of TensorFlow and uses its own grid-world designed environments for what is coined as real-life simulations. This is a very specialized framework for developers or real-life RL solutions.</li>
<li><strong>TF-Agents</strong>: This is another RL implementation from Google developed on top of TensorFlow. As such, it is a more low-level framework but is more robust and capable than the other TF frameworks mentioned here. This framework appears to be a strong contender for more serious research and/or production implementations and worth a further look from readers looking to do such work.</li>
<li><strong>SLM-Lab</strong>: This is another PyTorch-based framework that is actually based on top of Ray (RLLib), although it is designed more for pure research. As such, it lacks a <kbd>pip</kbd> installer and assumes the user is pulling code directly from a repository. It is likely best to leave this framework to the researchers for now.</li>
<li><strong>DeeR</strong>: This is another library that is integrated with Keras and is intended to be more accessible. The library is well kept and the documentation is clear. However, this framework is intended for those learning RL and if you made it this far, you likely already need something more advanced and robust.</li>
<li><strong>Garage</strong>: This is another TF-based framework that has some excellent functionality but lacks documentation and any good installation procedures, which makes this another good research framework but may be better avoided by those interested in developing working agents.</li>
<li><strong>Surreal</strong>: This framework is designed more for robotics applications and, as such, is more closed. Robotics RL with environments such as Mujoco have been shown to be commercially viable. As such, this branch of RL is seeing the impact of those trying to take their share. This means that this framework is currently free but not open source and the free part is likely to change soon. Still, if you are specializing in robotics applications, this may be worth a serious look.</li>
<li><strong>RLgraph</strong>: This is perhaps another sleeper project to keep your eye on. This library is currently absorbing a ton of commits and changing quickly. It is also built with both PyTorch and TensorFlow mappings. We will spend some time looking at using this framework in a later section.</li>
<li><strong>Simple RL</strong>: This is perhaps as simple as you can get with an RL framework. The project is intended to be very accessible and examples with multiple agents can be developed in less than eight lines of code. It can actually be as simple as the following block of code taken from the example documentation<span>:</span></li>
</ul>
<pre style="padding-left: 60px"><span>from</span> <span>simple_rl.agents</span> <span>import</span> QLearningAgent, RandomAgent, RMaxAgent
<span>from</span> <span>simple_rl.tasks</span> <span>import</span> GridWorldMDP
<span>from</span> <span>simple_rl.run_experiments</span> <span>import</span> run_agents_on_mdp

<span># Setup MDP.</span>
mdp <span>=</span> GridWorldMDP(width<span>=</span><span>4</span>, height<span>=</span><span>3</span>, init_loc<span>=</span>(<span>1</span>, <span>1</span>), goal_locs<span>=</span>[(<span>4</span>, <span>3</span>)], lava_locs<span>=</span>[(<span>4</span>, <span>2</span>)], gamma<span>=</span><span>0.95</span>, walls<span>=</span>[(<span>2</span>, <span>2</span>)], slip_prob<span>=</span><span>0.05</span>)

<span># Setup Agents.</span>
ql_agent <span>=</span> QLearningAgent(actions<span>=</span>mdp<span>.</span>get_actions())
rmax_agent <span>=</span> RMaxAgent(actions<span>=</span>mdp<span>.</span>get_actions())
rand_agent <span>=</span> RandomAgent(actions<span>=</span>mdp<span>.</span>get_actions())

<span># Run experiment and make plot.</span>
run_agents_on_mdp([ql_agent, rmax_agent, rand_agent], mdp, instances<span>=</span><span>5</span>, episodes<span>=</span><span>50</span>, steps<span>=</span><span>10</span>)</pre>
<p>With so many frameworks to choose from, we only have time to go over the most popular frameworks in this chapter. While frameworks become popular because they are well written and tend to work well in a wide variety of environments, until we reach AGI, you may still need to explore various frameworks to find an algorithm/agent that works for you and your problem. </p>
<p>To see how this has evolved over time, we can use Google Trends to perform a search comparison analysis. Doing this can often give us an indication of how popular a particular framework is trending in search terms. More search terms means more interest in the framework, which in turn, leads to more development and better software. </p>
<p>The following Google Trends plot shows a comparison of the top five listed frameworks:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-662 image-border" src="assets/212f143f-e9d7-466a-b94a-e2f6c6d214f3.png" style="width:99.50em;height:59.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Google trends comparison of RL frameworks</div>
<p class="mce-root"/>
<p>You can see in the plot the trending increase for RL Lib and Google Dopamine. It is also interesting to note that the primary interest in RL development is the current greatest in the US and Japan, with Japan taking a special interest in ML-Agents. </p>
<p>ML-Agents' popularity lends itself to several factors, one of which being the VP of AI and ML at Unity, Dr. Danny Lange. Dr. Lange lived in Japan for several years and is fluent in Japanese and this has likely contributed to this specific popularity.</p>
<p>It is interesting to note the absence of China in this area, at least for these types of frameworks. China's interest in RL is currently very specific to planning applications popularized by the defeat of the game of Go by an RL agent. That RL agent was developed using an algorithm called Monte Carlo Tree Search, which is intended to do a full exploration of complex but finite state spaces. We started looking at finite state spaces but took a turn to explore continuous or infinite state spaces. These types of agents also transition well to general games and robotics, which is not a major interest by the Chinese. Therefore, it remains to be seen how or what interest China shows in this area but that will most likely influence this space as well when that happens.</p>
<p>In the next section, we look at our first framework and one that we may find the most familiar, Google Dopamine.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Google Dopamine</h1>
                </header>
            
            <article>
                
<p>Dopamine was developed at Google as a platform to showcase the company's latest advances in DRL. Of course, there are also other groups at Google doing the same thing, so it is perhaps a testament to how varied these platforms still are and need to be. In the next exercise, we will use Google Colab to build an example that uses Dopamine on the cloud to train an agent. </p>
<div class="packt_tip">To access all of the features on Colab, you will likely need to create a Google account with payment authorized. This likely means entering a credit or debit card. The plus here is that Google provides $300 US in credits to use the GCP platform, of which Colab is one small part.</div>
<p><span>Open your browser to </span><a href="http://colab.research.google.com">colab.research.google.com</a><span> and follow the next exercise:</span></p>
<ol>
<li>We will first start by creating a new <strong>Python 3 Notebook</strong>. Be sure to choose this by the prompt dialog or through the Colab <strong>File</strong> menu.</li>
</ol>
<div class="packt_infobox"><span>This notebook is based on a variation by the Dopamine authors and the original may be found in the following link</span><span>: <a href="https://github.com/google/dopamine/blob/master/dopamine/colab/agents.ipynb">https://github.com/google/dopamine/blob/master/dopamine/colab/agents.ipynb</a>.</span></div>
<ol start="2">
<li>We first need to install several packages to support the training. On a Colab notebook, we can pass any command to the underlying shell by prefixing it with <kbd>!</kbd>. Enter the following code in a cell and then run the cell:</li>
</ol>
<pre style="padding-left: 60px"><strong><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">cache</span><span class="o">-</span><span class="nb">dir</span> <span class="n">dopamine</span><span class="o">-</span><span class="n">rl</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">cmake</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">atari_py</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">gin</span><span class="o">-</span><span class="n">config</span></strong></pre>
<ol start="3">
<li>Then, we do some imports and set up some global strings in a new cell:</li>
</ol>
<pre style="padding-left: 60px"><strong><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">dopamine.agents.dqn</span> <span class="k">import</span> <span class="n">dqn_agent</span>
<span class="kn">from</span> <span class="nn">dopamine.discrete_domains</span> <span class="k">import</span> <span class="n">run_experiment</span>
<span class="kn">from</span> <span class="nn">dopamine.colab</span> <span class="k">import</span> <span class="n">utils</span> <span class="k">as</span> <span class="n">colab_utils</span>
<span class="kn">from</span> <span class="nn">absl</span> <span class="k">import</span> <span class="n">flags</span>
<span class="kn">import</span> <span class="nn">gin.tf</span>

<span class="n">BASE_PATH</span> <span class="o">=</span> <span class="s1">'/tmp/colab_dope_run'</span>  <span class="c1"># @param</span>
<span class="n">GAME</span> <span class="o">=</span> <span class="s1">'Asterix'</span>  <span class="c1"># @param</span></strong></pre>
<ol start="4">
<li>The <kbd>@param</kbd> function denotes the value as a parameter and this provides a helpful textbox on the interface for changing this parameter later. This is a cool notebook feature:</li>
</ol>
<pre style="padding-left: 60px"><span class="err">!</span><span class="n">gsutil</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">m</span> <span class="n">cp</span> <span class="o">-</span><span class="n">R</span> <span class="n">gs</span><span class="p">:</span><span class="o">//</span><span class="n">download</span><span class="o">-</span><span class="n">dopamine</span><span class="o">-</span><span class="n">rl</span><span class="o">/</span><span class="n">preprocessed</span><span class="o">-</span><span class="n">benchmarks</span><span class="o">/*</span> <span class="o">/</span><span class="n">content</span><span class="o">/</span>
<span class="n">experimental_data</span> <span class="o">=</span> <span class="n">colab_utils</span><span class="o">.</span><span class="n">load_baselines</span><span class="p">(</span><span class="s1">'/content'</span><span class="p">)</span></pre>
<ol start="5">
<li>Then, we run the preceding command and code in another new cell. This loads the data we will use to run the agent on:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">LOG_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">BASE_PATH</span><span class="p">,</span> <span class="s1">'random_dqn'</span><span class="p">,</span> <span class="n">GAME</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MyRandomDQNAgent</span><span class="p">(</span><span class="n">dqn_agent</span><span class="o">.</span><span class="n">DQNAgent</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sess</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">):</span>
    <span class="sd">"""This maintains all the DQN default argument values."""</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyRandomDQNAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="sd">"""Calls the step function of the parent class, but returns a random action.</span>
<span class="sd">    """</span>
    <span class="n">_</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">MyRandomDQNAgent</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">observation</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_random_dqn_agent</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">environment</span><span class="p">,</span> <span class="n">summary_writer</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">"""The Runner class will expect a function of this type to create an agent."""</span>
  <span class="k">return</span> <span class="n">MyRandomDQNAgent</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">num_actions</span><span class="o">=</span><span class="n">environment</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>

<span class="n">random_dqn_config</span> <span class="o">=</span> <span class="s2">"""</span>
<span class="s2">import dopamine.discrete_domains.atari_lib</span>
<span class="s2">import dopamine.discrete_domains.run_experiment</span>
<span class="s2">atari_lib.create_atari_environment.game_name = '</span><span class="si">{}</span><span class="s2">'</span>
<span class="s2">atari_lib.create_atari_environment.sticky_actions = True</span>
<span class="s2">run_experiment.Runner.num_iterations = 200</span>
<span class="s2">run_experiment.Runner.training_steps = 10</span>
<span class="s2">run_experiment.Runner.max_steps_per_episode = 100</span>
<span class="s2">"""</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">GAME</span><span class="p">)</span>
<span class="n">gin</span><span class="o">.</span><span class="n">parse_config</span><span class="p">(</span><span class="n">random_dqn_config</span><span class="p">,</span> <span class="n">skip_unknown</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">random_dqn_runner</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="o">.</span><span class="n">TrainRunner</span><span class="p">(</span><span class="n">LOG_PATH</span><span class="p">,</span> <span class="n">create_random_dqn_agent</span><span class="p">)</span></pre>
<ol start="6">
<li>Create a new cell and enter the preceding code and run it. This creates a random DQN agent for more or less blindly exploring an environment.</li>
<li>Next, we want to train the agent by creating a new cell and entering the following code:</li>
</ol>
<pre style="padding-left: 60px"><span class="nb">print</span><span class="p">(</span><span class="s1">'Will train agent, please be patient, may be a while...'</span><span class="p">)</span>
<span class="n">random_dqn_runner</span><span class="o">.</span><span class="n">run_experiment</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Done training!'</span><span class="p">)</span></pre>
<ol start="8">
<li>This can take a while, so if you have authorized payment enabled, you can run this example on a GPU instance by just changing the notebook type. You can do this by selecting from the menu <strong>Runtime</strong> | <strong>Change runtime type</strong>. A dialog will pop up; change the runtime type and close the dialog, as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-663 image-border" src="assets/10cffd3e-db75-433f-adb6-ec4a4fef591f.png" style="width:26.33em;height:16.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Changing the runtime type on Colab</div>
<ol start="9">
<li>After you change the runtime type, you will need to run the whole notebook again. To do this, select from the menu <strong>Runtime | Run all</strong> to run all of the cells again. You will still need to wait a while for the training to run; it is, after all, running an Atari environment but that is the point.</li>
</ol>
<p>The agent we just built is using a random DQN agent running on the classic Atari game, <em>Asterix</em>. Dopamine is a powerful framework that is easy to use as we have just seen. You can find much more about the library from the source itself, including how to output the results from the last example exercise. </p>
<p>In the next section, we will move away from Colab and explore another framework, Keras-RL with regular Python.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing with Keras-RL</h1>
                </header>
            
            <article>
                
<p>Keras is a very popular deep learning framework on its own and it is heavily used by newcomers looking to learn about the basics of constructing networks. The framework is considered very high-level and abstracts most of the inner details of constructing networks. It only goes to assume that an RL framework built with Keras would attempt to do the same thing. </p>
<p class="mce-root"/>
<div class="packt_tip">This example is dependent on the version of Keras and TensorFlow and may not work correctly unless the two can work together. If you encounter trouble, try installing a different version of TensorFlow and try again.</div>
<p>To <span>run this example, we will start by doing the installation and all of the setup in this exercise:</span></p>
<ol>
<li>To install Keras, you should create a new virtual environment using Python 3.6 and use <kbd>pip</kbd> to install it along with the <kbd>keras-rl</kbd> framework. The commands to do all of this on Anaconda are shown here:</li>
</ol>
<pre style="padding-left: 60px"><strong>conda create -n kerasrl python=3.6</strong><br/><strong>conda activate kerasrl</strong><br/><strong>pip install tensorflow==1.7.1  #not TF 2.0 at time of writing</strong><br/><strong>pip install keras</strong><br/><strong>pip install keras-rl</strong><br/><strong>pip install gym</strong></pre>
<ol start="2">
<li>After all of the packages are installed, open the sample code file, <kbd>Chapter_12_Keras-RL.py</kbd>, as shown here:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import gym<br/><br/>from keras.models import Sequential<br/>from keras.layers import Dense, Activation, Flatten<br/>from keras.optimizers import Adam<br/><br/>from rl.agents.dqn import DQNAgent<br/>from rl.policy import BoltzmannQPolicy<br/>from rl.memory import SequentialMemory<br/><br/>ENV_NAME = 'CartPole-v0'<br/><br/>env = gym.make(ENV_NAME)<br/>np.random.seed(123)<br/>env.seed(123)<br/>nb_actions = env.action_space.n<br/><br/>model = Sequential()<br/>model.add(Flatten(input_shape=(1,) + env.observation_space.shape))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(nb_actions))<br/>model.add(Activation('linear'))<br/>print(model.summary())<br/><br/>memory = SequentialMemory(limit=50000, window_length=1)<br/>policy = BoltzmannQPolicy()<br/>dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,<br/>               target_model_update=1e-2, policy=policy)<br/>dqn.compile(Adam(lr=1e-3), metrics=['mae'])<br/>dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)<br/><br/>dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)<br/><br/>dqn.test(env, nb_episodes=5, visualize=True)</pre>
<ol start="3">
<li>We haven't covered any Keras code but hopefully, the simple nature of the code makes it fairly self-explanatory. If anything, the code should feel quite familiar, although missing a training loop. </li>
<li>Notice in the proceeding code block how the model is constructed using something called a <kbd>Sequential</kbd> class. The class is a container for network layers, which we then add next interspersed with appropriate activation functions. Note at the end of the network, how the last layer uses a linear activation function.</li>
<li>Next, we will take a closer look at the construction of memory, policy, and agent itself. See the following code:</li>
</ol>
<pre style="padding-left: 60px">memory = SequentialMemory(limit=50000, window_length=1)<br/>policy = BoltzmannQPolicy()<br/>dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,<br/>               target_model_update=1e-2, policy=policy)</pre>
<ol start="6">
<li>The interesting thing to note here is how we construct the network model outside the agent and feed it as an input to the agent along with the memory and policy. This is very powerful and provides for some interesting extensibility. </li>
<li>At the end of the file, we can find the training code. The training function called <kbd>fit</kbd> is used to iteratively train the agent. All of the code to do this is encapsulated in the <kbd>fit</kbd> function, as the following code shows:</li>
</ol>
<pre style="padding-left: 60px"><span>dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)</span></pre>
<ol start="8">
<li>The last section of code saves the model and then runs a test on the agent with the following code:</li>
</ol>
<pre style="padding-left: 60px">dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)<br/><br/>dqn.test(env, nb_episodes=5, visualize=True)</pre>
<ol start="9">
<li>Run the code as you normally would and watch the visual training output and testing as shown in the following diagram:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-664 image-border" src="assets/31a25484-5cd9-43cc-880e-88369ad378b9.png" style="width:18.75em;height:12.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output from Chapter_12_Keras-RL.py</div>
<p>Keras-RL is a light powerful framework for testing concepts or other ideas quickly. The performance of Keras itself is not as powerful as TensorFlow or PyTorch, so any serious development should be done using one of those platforms. In the next section, we will look at another RL platform based on PyTorch called RLLib.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring RL Lib</h1>
                </header>
            
            <article>
                
<p>RL Lib is based on the Ray project, which is essentially a Python job-based system. RL Lib is more like ML-Agents, where it exposes functionality using config files although, in the case of ML-Agents, the structure is completely run on their platform. Ray is very powerful but requires a detailed understanding of the configuration parameters and setup. As such, the exercise we show here is just to demonstrate the power and flexibility of Ray but you are directed to the full online documentation for further discovery on your own. </p>
<p><span>Open your browser to </span><a href="http://colab.research.google.com">colab.research.google.com</a><span> and follow the next exercise:</span></p>
<ol>
<li>The great thing about using Colab is it can be quite easy to run and set up. Create a new Python 3 notebook and enter the following commands:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>!</span><span>pip uninstall -y pyarrow<br/></span><span>!</span><span>pip install tensorflow ray[rllib] &gt; /dev/null </span><span>2</span><span>&gt;&amp;</span><span>1</span></strong></pre></div>
<ol start="2">
<li>These commands install the framework on the Colab instance. After this is installed, you need to restart the runtime by selecting from the menu: <strong>Runtime | Restart runtime</strong>.</li>
<li class="mce-root"><span>After the runtime restarts, create a new cell and enter the following code:</span></li>
</ol>
<div style="color: black;font-size: 1em">
<pre style="padding-left: 60px">import ray<br/>from ray import tune<br/><br/>ray.init()</pre></div>
<ol start="4">
<li>That block of code imports the framework and the tune class for hyperparameter tuning.</li>
<li>Create a new cell and enter the following code:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>tune.run(</span><span>"DQN"</span><span>, </span><span>stop={</span><span>"episode_reward_mean"</span><span>: </span><span>100</span><span>},<br/></span><span>    config={<br/></span><span>            </span><span>"env"</span><span>: </span><span>"CartPole-v0"</span><span>,<br/>            <span>"num_gpus"</span><span>: </span><span>0</span><span>,<br/></span></span><span>           </span><span>"num_workers"</span><span>: </span><span>1</span><span>,<br/></span><span>           </span><span>"lr"</span><span>: tune.grid_search([</span><span>0.01</span><span>, </span><span>0.001</span><span>, </span><span>0.0001</span><span>]),<br/>           <span>"monitor"</span><span>: </span><span>False</span><span>,</span></span><span>    },</span><span>)</span></pre></div>
<ol start="6">
<li>Believe it or not, that's it. That is the remainder of the code to build a DQN agent to run and train on the <kbd>CartPole</kbd> environment. Not to mention the <kbd>tune</kbd> class is set to tune the learning rate hyperparameter, <kbd>lr</kbd> (<kbd>alpha</kbd>), using the <kbd>tune.grid_search</kbd> function.</li>
</ol>
<ol start="7">
<li>Run the last cell and observe the output. The output is extremely comprehensive and an example is shown here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-665 image-border" src="assets/ab7b843c-19da-41af-85b0-f6cf8a7dd1ff.png" style="width:129.92em;height:82.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Training RLLib on Google Colab</div>
<p>As you can see in the preceding screenshot, this is a very powerful framework designed to optimize hyperparameter tuning and it provides plenty of options to do so. It also allows for multiagent training in various configurations. This framework is a must-study for anyone doing serious work or research in RL. In the next section, we will look at the last framework, TF-Agents.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using TF-Agents</h1>
                </header>
            
            <article>
                
<p>The last framework we are going to look at is TF-Agents, a relatively new but up-and-coming tool, again, from Google. It seems Google's approach to building RL frameworks is a bit like RL itself. They are trying multiple trial and error attempts/actions to get the best reward—not entirely a bad idea for Google, and considering the resources they are throwing at RL, it may not unexpected to see more RL libraries come out.</p>
<p class="mce-root"/>
<p>TF-Agents, while newer, is typically seen as more robust and mature. It is a framework designed for notebooks and that makes it perfect for trying out various configurations, hyperparameters, or environments. The framework is developed on TensorFlow 2.0 and works beautifully on Google Colab. It will likely become the de-facto platform to teach basic RL concepts and demo RL in the future.</p>
<p>There are plenty of notebook examples to show how to use TF-Agents at the TF-Agents Colab repository (<a href="https://github.com/tensorflow/agents/tree/master/tf_agents/colabs">https://github.com/tensorflow/agents/tree/master/tf_agents/colabs</a>). The whole repository is a great resource but this section itself can be especially useful for those of us that want to see working code examples.</p>
<p>Open your browser up to the TF-Agents Colab page at the preceding link and follow the next exercise:</p>
<ol>
<li>For this exercise, we are going to modify the training environment for one of the samples. That should give us enough of an overview of what the code looks like and how to make changes yourself later on. Locate <kbd>1_dqn_tutorial.ipynb</kbd> and click on it to open the page. Note that <kbd>.ipynb</kbd> stands for <strong>I-Python Notebook</strong>; I-Python is a server platform for hosting notebooks.</li>
<li>Click the link at the top that says <strong>Run in Google Colab</strong>. This will open the notebook in Colab.</li>
<li>From the menu, select <strong>Runtime</strong> | <strong>Change runtime type</strong> <strong>to GPU</strong> and then click <strong>Save</strong>. We are going to convert this example to use the Lunar Lander from Cart Pole. As we know, this will take more compute cycles.</li>
<li>First, we will want to modify the initial <kbd>pip install</kbd> commands to import the full <kbd>gym</kbd> package by updating the commands in the first cell to the following:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>!</span><span>apt-get install xvfb<br/></span><span>!</span><span>pip install gym[all]<br/></span><span>!</span><span>pip install </span><span>'imageio==2.4.0'<br/></span><span>!</span><span>pip install PILLOW<br/></span><span>!</span><span>pip install </span><span>'pyglet==1.3.2'<br/></span><span>!</span><span>pip install pyvirtualdisplay<br/></span><span>!</span><span>pip install tf-agents-nightly<br/></span><span>try</span><span>:<br/></span><span>  %</span><span>%tensorflow_version </span><span>2</span><span>.x<br/></span><span>except</span><span>:<br/></span><span>  </span><span>pass</span></strong></pre></div>
<ol start="5">
<li>Next, we will want to locate the two cells that refer to the <strong>CartPole</strong> environment. We want to change all mentions of <strong>CartPole</strong> to <strong>LunarLander</strong>, as shown in the following code:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>env_name = </span><span>'LunarLander-v2'<br/></span><span>env = suite_gym.load(env_name)<br/><br/># -- and --<br/><br/>example_environment = tf_py_environment.TFPyEnvironment(</span>uite_gym.load(<span><strong><span>'LunarLander-v2'</span></strong></span><span>))</span><span> </span></pre></div>
<ol start="6">
<li>The algorithm this example uses is a simple DQN model. As we know from experience, we can't just run the same hyperparameters for <kbd>LunarLander</kbd>; therefore, we will change them to the following:
<ul>
<li><kbd>num_iterations</kbd>: 500000 from 20000</li>
<li><kbd>initial_collect_steps</kbd>: 20000 from 1000</li>
<li><kbd>collect_steps_per_iteration</kbd>: 5 from 1</li>
<li><kbd>replay_buffer_max_length</kbd>: 250000 from 100000</li>
<li><kbd>batch_size</kbd>: 32 from 64</li>
<li><kbd>learning_rate</kbd>: 1e-35 from 1e-3</li>
<li><kbd>log_interval</kbd>: 2000 from 200</li>
<li><kbd>num_eval_episodes</kbd>: 15 from 10</li>
<li><kbd>eval_interval</kbd>: 500 from 1000</li>
</ul>
</li>
<li>Let's move on to adjusting the network size. Locate the following line of code and change it as shown:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>fc_layer_params = (</span><span>100</span><span>,)<br/><br/># change to<br/><br/>fc_layer_params = (256,)</span></pre></div>
<ol start="8">
<li>Feel free to change other parameters as you like. If you have done your homework, working with this example should be very straightforward. One of the great things about TF-Agents and Google Colab, in general, is how interactive sample and the training output is. </li>
</ol>
<div class="packt_infobox">This book was almost entirely written with Google Colab notebooks. <span>However, as good as they are, notebooks still lack a few good elements needed for larger samples. They also make it difficult for several reasons to use later in other examples. Therefore a preference was given to keep the samples in Python files.</span></div>
<ol start="9">
<li>From the menu, select <strong>Runtime</strong> | <strong>Run all</strong>, to run the sample and then wait patiently for the output. This may take a while so grab a beverage and relax for a while. </li>
</ol>
<p>On the page, you will be able to see several other algorithm forms that we have covered and that we did not get time to cover in this book. The following is a list of the agent types TF-Agents currently supports and a brief description about each:</p>
<ul>
<li><strong>DQN</strong>: This is the standard deep Q-learning network agent we have already looked at plenty of times. There isn't a DDQN agent so it looks like you may need to just put two DQN agents together.</li>
<li><strong>REINFORCE</strong>: This is the first policy gradient method we looked at.</li>
<li><strong>DDPG</strong>: This is a PG method, more specifically, the deep deterministic policy gradient method.</li>
<li><strong>TD3</strong>: This is best described as a clipped double Q-learning model that uses Actor-Critic to better describe the advantages in discrete action spaces. Typically, PG methods can perform poorly in discrete action spaces.</li>
<li><strong>PPO</strong>: This is our old friend proximal policy optimization, another PG method.</li>
<li><strong>SAC</strong>: This is based on soft Actor-Critic—an off-policy maximum entropy deep reinforcement learning with a stochastic actor. The basic reasoning here is the agent maximizes expected rewards by being as random as possible.</li>
</ul>
<p>TF-Agents is a nice stable platform that allows you to build up intuitive samples that you can train in the cloud very easily. This will likely make it a very popular framework for building proof-of-concept models for a variety of problems. In the next section, we will wrap up this chapter with the usual additional exercises.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>The exercises in this section are a bit wider in scope in this chapter in hopes you look through several frameworks on your own:</p>
<ol>
<li>Take some time and look at one of the frameworks listed earlier but not reviewed in this chapter. </li>
<li>Use SimpleRL to solve a grid-world MDP that is different than the one in the example. Be sure to take the time to tune hyperparameters.</li>
<li>Use Google Dopamine to train an agent to play the LunarLander environment. The best choice is likely RainbowDQN or a variation of that.</li>
<li>Use Keras-RL to train an agent to play the Lunar Lander environment; make sure to spend time tuning hyperparameters.</li>
<li>Use RL Lib to train an agent to play the Lunar Lander environment; make sure to spend time tuning hyperparameters.</li>
<li>Modify the Keras-RL example and modify the network structure. Change the number of neurons and layers.</li>
<li>Modify the RL Lib example and change some of the hyperparameters such as the <kbd>num</kbd> workers and the number of GPUs, as shown in the following <kbd>tune</kbd> code:</li>
</ol>
<pre style="padding-left: 60px"><span>tune.run(</span><span>"DQN"</span><span>, </span><span>stop={</span><span>"episode_reward_mean"</span><span>: </span><span>100</span><span>},<br/></span><span>    config={<br/></span><span>            </span><span>"env"</span><span>: </span><span>"CartPole-v0"</span><span>,<br/>            "num_gpus": 0,<br/></span><span>           </span><span>"num_workers"</span><span>: </span><span>1</span><span>,<br/></span><span>           </span><span>"lr"</span><span>: tune.grid_search([</span><span>0.01</span><span>, </span><span>0.001</span><span>, </span><span>0.0001</span><span>]),<br/>           "monitor": False,</span><span>    },</span><span>)</span></pre>
<ol start="8">
<li>Modify the RLLib example and use a different agent type. You will likely have to check the documentation for RLLib to see what other agents are supported.</li>
<li>Use TD3 from TF-Agents to train an agent to complete the Lunar Lander environment.</li>
<li>Use SAC from TF-Agents and use it train the Lunar Lander environment.</li>
</ol>
<p>Feel free to perform these exercises with Google Colab or in your favorite IDE. If you do use an IDE, you may need to take extra care to install some dependencies. In the next and last section of this chapter, we will finish up with the summary.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This was a short but intense chapter in which we spent time looking at various third-party DRL frameworks. Fortunately, all of these frameworks are all still free and open source, and let's hope they stay that way. We started by looking at the many growing frameworks and some pros and cons. Then, we looked at what are currently the most popular or promising libraries. Starting with Google Dopamine, which showcases RainbowDQN, we looked at how to run a quick sample of Google Colab. After that, Keras-RL was next, and we introduced ourselves to the Keras framework as well as how to use the Keras-RL library. Moving on to RLLib, we looked at the powerful automation of the DRL framework that has many capabilities. Finally, we finished up this chapter with another entry from Google, TF-Agents, where we ran a complete DQN agent using TF-Agents on a Google Colab notebook.</p>
<p>We have spent plenty of time learning about and using RL and DRL algorithms. So much so, we should be fairly comfortable with training and looking to move on to more challenging environments. </p>
<p>In the next chapter, we will move on to training agents in more complex environments such as the real world. However, instead of the real world, we are going to use the next best thing: 3D worlds.</p>


            </article>

            
        </section>
    </body></html>