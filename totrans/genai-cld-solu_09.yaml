- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Responsible Development of AI Solutions: Building with Integrity and Care'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the realm of modern technology, **artificial intelligence** (**AI**) has
    emerged as a transformative force, reshaping industries, improving efficiency,
    and enhancing user experiences. As cloud and AI architects, we stand at the forefront
    of this AI revolution, wielding the power to shape the future of AI-driven solutions.
    However, with great power comes great responsibility. The integration of responsible
    AI practices into the design and deployment of AI solutions is not merely a moral
    or ethical imperative; it is a strategic imperative that directly impacts the
    success, reputation, and sustainability of organizations in the AI landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Neglecting responsible AI (RAI) principles can have a profound impact on human
    lives. A thought-provoking article from MIT titled *AI is sending people to jail—and
    getting it wrong*, explores the application of AI and algorithms in the criminal
    justice system. It highlights how facial recognition systems and predictive algorithms
    used by police and judges can exhibit bias due to their training data, leading
    to incorrect decisions that affect human lives. Researchers have consistently
    shown that facial recognition systems are particularly prone to failure in identifying
    individuals with dark skin. Prediction models used in the justice system can be
    skewed towards a certain group of people, leading to incorrect judgments. Instances
    such as these (among others that we will explore in this book) underscore the
    urgent need for AI solutions developed with integrity and care.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we delve into the essentials of **responsible artificial intelligence**
    (**RAI**), starting with the key principles of AI design and addressing the unique
    challenges presented by **large language models** (**LLMs**). As we explore the
    rising concerns around Deepfakes, which are hyper-realistic digital manipulations
    often used to create fake videos or images, the importance of robust AI architecture
    and proactive leadership becomes evident, highlighting the need for ethical and
    responsible AI development. The chapter also examines the relationship between
    AI, cloud computing, and legal frameworks, emphasizing the significance of legal
    compliance and ethical considerations. Additionally, we provide insights into
    the most popular RAI tools, offering practical guidance for their application.
    By the end of this chapter, you will have a comprehensive understanding of the
    principles guiding RAI, strategies to combat LLM challenges, an awareness of the
    impact of Deepfakes, knowledge of AI’s role in cloud computing and legal contexts,
    and familiarity with essential RAI tools, empowering you to navigate and contribute
    to the field of AI responsibly and ethically.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding responsible AI design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key principles of RAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing LLM challenges with RAI principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rising Deepfake concern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building applications using a responsible AI-first approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI, the cloud, and the law – Understanding compliance and regulations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Startup ecosystem in RAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding responsible AI design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the true meaning of responsible AI and delve
    into the fundamental design principles that should be considered while architecting
    generative AI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: What is responsible AI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As stated by Microsoft public documentation, “*Responsible Artificial Intelligence*
    *(Responsible AI) is an approach to developing, assessing, and deploying AI systems
    in a safe, trustworthy, and ethical way.*” It is like building and using smart
    computer programs (AI systems) in a way that is safe, fair, and ethical. Think
    of AI systems as tools created by people who make a lot of choices about how these
    tools should work. Responsible AI is about making these choices carefully to make
    sure AI acts in a way that is good and fair for everyone. It’s like guiding AI
    to always consider what is best for people and their needs. This includes making
    sure AI is reliable, fair, and transparent about how it works. Here are a few
    examples of the types of tools being developed in this space:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fair hiring tools**: An AI tool used by a company to help choose job candidates.
    Responsible AI would ensure this AI doesn’t favor one group of people over another,
    making the hiring process fair for all applicants. For example, **BeApplied**,
    a startup in the RAI space, has developed a piece of ethical recruitment software
    designed to enhance hiring quality and increase diversity by reducing bias. It
    stands apart from traditional applicant tracking systems by incorporating fairness,
    inclusivity, and diversity as its core principles. The platform, underpinned by
    behavioral science, offers anonymized applications and predictive, skill-based
    assessments to ensure unbiased hiring. Its features include sourcing analysis
    tools to diversify talent pools, inclusive job description creation, anonymized
    skills testing for objective assessments, and data-driven shortlisting to focus
    purely on skills. BeApplied aims to create a fairer recruitment world, one hire
    at a time. They currently have some notable customers, such as UNICEF and England
    and Wales Cricket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparent recommendation systems**: Think of a streaming service that suggests
    movies. Responsible AI would make this system clear about why it recommends certain
    movies, ensuring it’s not just promoting certain movies for unfair reasons. For
    example, **LinkedIn** is a notable example of a company that focuses on transparent
    and explainable AI systems, especially in its recommendation systems. Their approach
    ensures that AI system behavior and any related components are understandable,
    explainable, and interpretable. They prioritize transparency in AI to make their
    systems trustworthy and to avoid harmful bias while respecting privacy. For instance,
    they developed **CrystalCandle**, a customer-facing model explainer that creates
    digestible interpretations and insights reflecting the rationale behind model
    predictions. This tool is integrated with business predictive models, aiding sales
    and marketing by converting complex machine learning outputs into clear, actionable
    narratives for users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**: In the healthcare industry, there’s a growing focus on developing
    ethical AI tools to ensure fairness, transparency, and accountability within AI-driven
    decisions. These tools are designed to minimize biases, safeguard patient data
    privacy, and enhance the explainability and reliability of AI algorithms. Ethical
    AI is pivotal in healthcare as it aids in delivering personalized care, improving
    patient outcomes, and maintaining high ethical standards. Embedding ethical considerations
    into AI systems helps prevent potential negative impacts, address health inequalities,
    and build trust with patients and the community, thereby positively influencing
    public health and well-being. One prominent example of such an ethical AI tool
    in healthcare is **Merative** (formerly IBM Watson Health). It supports healthcare
    professionals by offering evidence-based, personalized treatment recommendations
    with a focus on transparency and explainability. The platform also prioritizes
    patient data protection in compliance with healthcare regulations such as HIPAA
    and aims to reduce bias by employing diverse datasets for training its AI models.
    This approach by IBM Watson Health demonstrates the potential of AI to improve
    healthcare decision-making processes while emphasizing patient safety, data privacy,
    and equity across diverse patient populations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Finance**: In the finance industry, ethical AI tools are being developed
    to navigate complex ethical considerations such as data privacy and algorithmic
    bias and ensure transparency and accountability in AI-driven processes. In the
    finance industry, ethical AI tools such as **Zest AI** are revolutionizing how
    financial institutions approach lending by enhancing fairness and transparency
    in credit decisions. Zest AI leverages machine learning to improve credit scoring
    accuracy and reduce biases, thus promoting financial inclusivity. Its focus on
    explainability ensures that lenders can comprehend and justify AI-driven decisions,
    aligning with regulatory compliance and bolstering borrower trust. This example
    underscores the finance sector’s commitment to integrating responsible AI practices
    that benefit both institutions and customers, adhering to ethical standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Criminal justice**: In the criminal justice system, the development of ethical
    AI tools is a growing focus aimed at enhancing fairness, reducing bias, and improving
    the accuracy of legal outcomes. These tools are designed to support decision-making
    processes in areas such as predictive policing, risk assessment for bail and sentencing,
    and evidence analysis. One example of an ethical AI tool in criminal justice is
    **Correctional Offender Management Profiling for Alternative Sanctions (COMPAS)**.
    COMPAS is a risk assessment tool used by courts to evaluate the likelihood of
    a defendant reoffending. COMPAS considers elements such as past arrests, age,
    and employment status to generate risk scores for reoffending, which judges then
    use to decide on sentencing short-term jail or long-term prison. It was found
    that Black defendants are mistakenly classified as “high-risk” for future crimes
    at twice the rate of white defendants. These claims were refuted by the company,
    which stated that the algorithms worked as designed ([https://tinyurl.com/bdejxubh](https://tinyurl.com/bdejxubh)).
    However, continuous improvements have been made since then. While its implementation
    has sparked debate over potential biases, it highlights the sector’s attempt to
    apply AI in making informed, data-driven decisions regarding bail, sentencing,
    and parole. In response to ethical concerns, efforts are being made to improve
    such tools by incorporating fairness algorithms, enhancing transparency, and conducting
    regular audits to identify and mitigate biases. These advancements reflect the
    broader commitment to developing AI in criminal justice that upholds ethical standards
    and contributes to a more equitable legal system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key principles of RAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Responsible AI principles](img/B21443_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Responsible AI principles
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft has established a **Responsible AI Standard**, presenting a comprehensive
    framework that guides the development of AI systems. This framework is grounded
    in six key principles: **fairness**, **reliability and safety**, **privacy and
    security**, **inclusiveness**, **transparency**, and **accountability**, as depicted
    in the preceding above. They follow two guiding principles: **ethical and explainable**.
    These principles form the bedrock of Microsoft’s commitment to a responsible and
    trustworthy approach to AI. This approach is increasingly vital as AI becomes
    more integrated into the products and services we use daily. In my opinion, this
    framework from Microsoft is exceptionally well-rounded for the design of generative
    AI solutions and should always be a primary consideration when architecting such
    solutions. A good mnemonic to remember these principles by is “**F**riendly **R**obots
    **S**afeguard **P**rivacy, **I**nspire **T**rust, **A**ssure **S**afety,” or **FAST-P**a**IRS**.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive deep into each of these principles with the help of examples.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical and explainable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From an ethical standpoint, AI ought to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure fairness and inclusiveness in its statements and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hold responsibility/accountability for its choices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid discrimination against various races, disabilities, or backgrounds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainability in AI provides clarity on decision-making processes for data
    scientists, auditors, and business leaders, enabling them to understand and justify
    the system’s conclusions. It also ensures adherence to corporate policies, industry
    norms, and regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness and inclusiveness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This principle ensures that AI systems do not discriminate, are not biased against
    certain groups or individuals, and provide equal opportunities for all.
  prefs: []
  type: TYPE_NORMAL
- en: For example, designing AI systems with features that accommodate users with
    disabilities, such as voice-activated assistants that can understand and respond
    to users with speech impairments or AI-driven web interfaces that are navigable
    by people with visual impairments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This article from *The New York Times*, titled *Thousands of Dollars for Something
    I Didn’t Do* discusses the case of an African American individual who was wrongfully
    charged and fined due to an erroneous facial recognition match. This incident
    highlights the limitations of AI-based facial recognition systems in accurately
    identifying individuals with darker skin tones. Such incidents necessitate the
    need for fairness and inclusiveness principles in AI systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability and safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This focuses on the AI system being dependable and not posing any harm to users.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an AI system used in a self-driving car must be reliable and safe.
    It should consistently make correct driving decisions, such as stopping at red
    lights and avoiding obstacles, to ensure the safety of passengers and pedestrians.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This principle demands clarity on how AI systems make decisions or reach conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a credit scoring AI system should be transparent about the factors
    it uses to determine someone’s credit score. This means a user should be able
    to understand which financial behaviors are impacting their score, whether positively
    or negatively.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This ensures that the personal data used by AI systems are protected and not
    misused.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an AI-powered health app that tracks users’ physical activities
    and health metrics must safeguard this sensitive and personal information. The
    app should have robust security measures to prevent data breaches and should be
    clear about how it uses and shares user data.
  prefs: []
  type: TYPE_NORMAL
- en: Accountability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This principle is about taking responsibility for the outcomes of AI systems,
    including addressing any negative impacts.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if an AI-powered news recommendation system inadvertently spreads
    fake news, the creators of the system must take responsibility. They should identify
    the failure in their algorithm, rectify the issue, and take steps to prevent such
    occurrences in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing LLM challenges with RAI principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed previously, there are three major challenges we face with LLM
    outputs: hallucinations, toxicity, and intellectual property issues. Now let’s
    double-click into each of these challenges and see how we can use RAI principles
    to address them.'
  prefs: []
  type: TYPE_NORMAL
- en: Intellectual property issues (Transparency and Accountability)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The RAI principle that addresses **intellectual property** (**IP**) issues is
    referred to as “Transparency and Accountability.” This principle ensures that
    AI systems are transparent in their operations and that their creators and operators
    are accountable for their design and use. This includes the prevention of plagiarism
    and ensuring compliance with copyright laws.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency involves the clear disclosure of the data sources, algorithms,
    and training methods used, which can have implications for IP rights.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if an AI system is trained on copyrighted materials or incorporates
    proprietary algorithms, it’s crucial to have proper permissions and to acknowledge
    these sources to avoid IP infringements. We believe new regulations will emerge
    in the upcoming years to prevent IP issues in generative AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, research is being carried out on ways to filter out or block responses
    that are very similar to protected content. For instance, if a user requests a
    generative AI to produce a narrative that is like a popular fantasy novel, the
    AI will analyze the request and either alter the output significantly to avoid
    direct similarities or deny the request altogether, ensuring it does not infringe
    on the novel’s intellectual property rights.
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine unlearning** is a relatively recent concept in the field of machine
    learning and artificial intelligence, which involves the ability to effectively
    remove specific data from a trained model’s knowledge without retraining it from
    scratch. This process is particularly relevant in the context of privacy and data
    protection, especially under regulations such as the GDPR, which advocates for
    the “right to be forgotten.” Traditional machine learning embeds the training
    data into a model’s parameters, making selective data removal challenging. Machine
    unlearning addresses this by developing methods to diminish or reverse the influence
    of certain data points on the model, thus allowing for compliance with privacy
    laws and providing greater flexibility in data management. However, implementing
    this efficiently without compromising the model’s performance is a complex and
    ongoing area of research.'
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations (Reliability and Safety)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The responsible AI principle that addresses the problem of hallucinations in
    AI models is typically “Reliability and Safety.” This principle focuses on ensuring
    that AI systems operate reliably and safely under a wide range of conditions and
    do not produce unintended, harmful, or misleading outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations in AI refer to instances where AI models generate false or nonsensical
    information, often because of training on noisy, biased, or insufficient data.
    Ensuring reliability and safety means rigorously testing AI systems to detect
    and mitigate such issues, ensuring that they perform as expected and do not produce
    erroneous outputs, such as hallucinations, which could lead to misinformation
    or harmful decisions. We have discussed ways to mitigate hallucinations by using
    prompt engineering, RAG techniques, and fine-tuning in *Chapters* *3*, *4*, and
    *5*.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the users must be educated on hallucination possibilities via
    generative AI applications. Additionally, the augmentation of source citations
    in LLM responses should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Toxicity (Fairness and Inclusiveness)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Toxicity in AI can manifest as biased, offensive, or harmful outputs that may
    disproportionately affect certain groups based on race, gender, sexual orientation,
    or other characteristics. The responsible AI principle that specifically addresses
    toxicity in AI systems is “Fairness and Inclusiveness.” This principle ensures
    that AI systems do not perpetuate, amplify, or introduce biases and discriminatory
    practices, including the generation or reinforcement of toxic content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following methods can be used to mitigate toxicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Diverse and representative data collection**: Leverage large language models
    (LLMs) to generate a broad spectrum of training data, ensuring it encompasses
    various groups for a more inclusive representation. This approach helps minimize
    biases and mitigate toxic outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global annotator workforce**: Engage a global team of human annotators from
    diverse races and backgrounds. Such human annotators provide comprehensive guidelines
    on accurately labeling training data, emphasizing the importance of inclusivity
    and unbiased judgment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proactive bias detection and remediation**: Implement systematic processes
    to actively identify and address biases in AI systems. This ongoing effort is
    crucial to prevent and reduce instances of toxic behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inclusive design and rigorous testing**: Involve a wide array of stakeholders
    in both the design and testing phases of AI systems. This inclusive approach is
    key to uncovering and addressing potential issues related to toxicity and bias
    early in the development process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supplemental guardrail models**: Develop and train additional models specifically
    designed to filter out inappropriate or unwanted content. These models act as
    an extra layer of defense, ensuring the overall AI system maintains high standards
    of content quality and appropriateness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, the principle of “Transparency and Accountability” plays a role
    in addressing toxicity. By making AI systems more transparent, stakeholders can
    better understand how and why certain outputs are generated, which aids in identifying
    and correcting toxic behaviors. Accountability ensures that those who design and
    deploy AI systems are responsible for addressing any toxic outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Rising Deepfake concern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deepfake technology has become a rising concern in recent times, primarily due
    to advancements in AI and machine learning, making it easier and more convincing
    than ever before. These technological improvements have enabled the creation of
    highly realistic and difficult-to-detect fake videos and images. This growing
    realism and accessibility heighten the risks of misinformation, privacy violations,
    and the potential for malicious use in politics, personal attacks, and fraud.
    In this section, we will discuss what Deepfake is, some real-world examples, its
    detrimental impact on society, and what we can do to mitigate it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – A face covered by a wireframe, which is used to create Deepfake
    content](img/B21443_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – A face covered by a wireframe, which is used to create Deepfake
    content
  prefs: []
  type: TYPE_NORMAL
- en: What is Deepfake?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deepfake is a technology that uses artificial intelligence to create or alter
    video, images, and audio recordings, making it seem as if someone said or did
    something they did not. It typically involves manipulating someone’s likeness
    or voice.
  prefs: []
  type: TYPE_NORMAL
- en: Some real-world examples of Deepfake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are some early real-world examples of Deepfakes that have raised
    significant concerns and exacerbated the need for their prevention:'
  prefs: []
  type: TYPE_NORMAL
- en: In 2019, a UK-based energy firm’s CEO was tricked into transferring EUR 220,000
    after receiving a phone call from what he believed was his boss. The caller used
    Deepfake technology to imitate the boss’s voice, convincing the CEO of the legitimacy
    of the request ([https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-Deepfake-was-used-to-scam-a-ceo-out-of-243000/?sh=4721eb412241](https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-Deepfake-was-used-to-scam-a-ceo-out-of-243000/?sh=4721eb412241)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edited videos and speeches have also been Deepfaked. For instance, a manipulated
    video of Facebook’s Mark Zuckerberg talking about the power of having billions
    of people’s data and a fake speech by Belgium’s prime minister linking the coronavirus
    pandemic to climate change are examples of Deepfake usage ([https://www.cnn.com/2019/06/11/tech/zuckerberg-Deepfake/index.html](https://www.cnn.com/2019/06/11/tech/zuckerberg-Deepfake/index.html)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concerns regarding the objectification of women due to Deepfake adult videos
    have been rising. The prevalence of AI-generated pornographic content that unlawfully
    uses the faces of women without their consent is increasingly troubling, particularly
    in the online world of notable influencers and streamers. This issue came to light
    in January when “Sweet Anita,” a prominent British live streamer with 1.9 million
    Twitch followers, discovered that a collection of fake explicit videos, which
    illegitimately featured the faces of various Twitch streamers, was being shared
    online. Sweet Anita is well-known on Twitch for her gaming content and interactive
    sessions with her audience ([https://www.nbcnews.com/tech/internet/Deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna69372](https://www.nbcnews.com/tech/internet/Deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna69372)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In early 2024, AI-generated Deepfake images of Taylor Swift, some of which were
    sexually explicit, spread across social media platforms, leading platforms such
    as X (formerly Twitter) to block searches for her name and renew calls for stronger
    AI legislation. The images, seen by millions, prompted actions from social media
    companies and discussions about the need for legal and regulatory responses to
    the misuse of AI technologies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detrimental effects on society
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are some negative consequences of Deepfake that can have harmful
    effects on society:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Misinformation and erosion of trust**: Deepfakes can create highly convincing
    but false representations of individuals saying or doing things they never did,
    leading to misinformation and eroding public trust in media and institutions.
    For example, Deepfakes have been used to create fake videos of politicians, which
    can mislead voters and disrupt democratic processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploitation and harassment**: Deepfakes can be used to create non-consensual
    explicit content or defamatory material, targeting individuals for harassment
    or blackmail. There have been instances where Deepfake technology was used to
    superimpose faces of celebrities or private individuals onto explicit content
    without their consent, causing personal distress and reputational damage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security threats**: Deepfakes pose a security threat by enabling fraud and
    impersonation. They can be used to mimic voices or faces to bypass biometric security
    measures or to create convincing scams. An example was provided earlier, regarding
    a real-world case, where Deepfakes were used to mimic a CEO’s voice to trick a
    manager into transferring a significant sum of money, as reported by Forbes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal and ethical challenges**: The rise of Deepfakes creates legal and ethical
    dilemmas, challenging existing laws on consent, privacy, and free speech. Technology
    blurs the line between truth and fiction, making it difficult to discern real
    from fake and raising questions about the legality of such content creation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In my opinion, the biggest threat to human lives is a nuclear war between countries
    that can lead to suffering and death on a ginormous scale. Imagine a scenario
    where a Deepfake video falsely shows a world leader declaring war or making inflammatory
    statements, leading to international tensions or even conflicts. This highlights
    the potential geopolitical impact of Deepfakes when used maliciously and the need
    for education on how to spot Deepfakes and other mitigation strategies.
  prefs: []
  type: TYPE_NORMAL
- en: How to spot a Deepfake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The identification of Deepfake is an area of growing research. Here, we mention
    a few techniques you can use to identify Deepfake content:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Facial inconsistencies**: Look for anomalies in facial expressions, such
    as awkward blinking, unusual lip movements, or facial features that appear distorted
    or don’t align correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audio-visual mismatch**: Check for mismatches between the audio and visual
    elements. For example, the voice may not sync perfectly with the lip movements,
    or the tone and accent might not match the person’s known speech patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unnatural skin tone or texture**: Deepfakes may exhibit issues with skin
    tone or texture. This can include overly smooth skin, a lack of natural blemishes,
    or inconsistent lighting on the face compared to the surroundings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Background anomalies**: Pay attention to the background of the video. Look
    for strange artifacts, inconsistencies in lighting, or other elements that seem
    out of place or distorted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of blinking or excessive blinking**: In early Deepfakes, the blinking
    was often irregular or missing. Although newer Deepfakes have improved, anomalies
    in blinking can still be a giveaway.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use of detection software**: There are various software tools and apps designed
    to detect Deepfakes by analyzing videos for subtle inconsistencies that are not
    easily noticeable to the human eye. Popular Deepfake detection tools include products
    from Sentinel ([https://thesentinel.ai/](https://thesentinel.ai/)) and Intel’s
    FakeCatcher.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Checking source credibility**: Verify the source of the video or audio. If
    it comes from an unverified or suspicious source, it warrants further scrutiny.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigation strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will explore several key mitigation strategies to tackle
    the risks associated with Deepfake technology. Understanding these techniques
    is a crucial aspect of leadership education, equipping leaders, as well as the
    general public, with the necessary tools to address and counter the challenges
    posed by this advanced technology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Public awareness and education**: Educating the public about the existence
    and potential misuse of Deepfakes can make people more critical of the media they
    consume. This can include campaigns to raise awareness about how to spot Deepfakes,
    which we have discussed in the earlier section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deepfake detection technologies**: Developing and implementing advanced detection
    algorithms that can identify Deepfakes is crucial. These technologies often use
    machine learning to analyze videos or audio for inconsistencies or anomalies that
    are not perceptible to the human eye. Some popular Deepfake detection tools include
    Sentinel and Intel’s Deepfake detector tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal and regulatory measures**: Governments and regulatory bodies can enact
    laws and regulations to penalize the creation and distribution of malicious Deepfakes.
    This includes defining legal frameworks that address consent, privacy, and the
    misuse of Deepfake technology. US President Biden’s office published an Executive
    Order (EO) on Oct. 30, 2023, which is a major step toward implementing safety
    standards and regulations in AI. We will discuss this EO in the upcoming section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blockchain and digital watermarking**: Implementing technologies such as
    blockchain and digital watermarking can help verify the authenticity of digital
    content. This can create a traceable, tamper-evident record of the media, ensuring
    its integrity. For instance, in August 2023, Google’s DeepMind launched a watermarking
    tool for AI-generated images. In November 2023, Google reported that they would
    be using inaudible watermarks in its AI-generated music, so it’s possible to detect
    if Google’s AI tech has been used in the creation of a track ([https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks](https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform responsibility**: Social media platforms and content distributors
    play a crucial role and should implement policies and algorithms to detect and
    remove Deepfake content from their platforms. In November 2023, Meta announced
    that they would be implementing strict policies that would require political advertisers
    to flag AI-generated content as a step towards mitigating the proliferation of
    misinformation through Deepfakes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining these strategies, society can better mitigate the risks associated
    with Deepfake technology, protecting individuals and maintaining trust in digital
    media.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Deepfake detection is a rapidly expanding field of research, primarily driven
    by advancements in generative adversarial networks (GANs). These sophisticated
    AI algorithms consist of two parts: the generator, which is responsible for creating
    synthetic data, and the discriminator, which assesses its authenticity. The discriminator’s
    role is particularly crucial in Deepfake detection. As the cutting-edge in producing
    realistic fake images and videos, understanding and analyzing the discriminator
    aspect of GANs is pivotal for developing effective strategies to identify and
    counter Deepfake content. The deeper our grasp of GAN mechanisms, the more adept
    we become at crafting systems capable of detecting the increasingly intricate
    Deepfakes they generate. While delving into the intricacies of GANs is beyond
    the scope of this book, we strongly recommend monitoring developments in this
    field, as they are likely to play a significant role in shaping future Deepfake
    detection techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Building applications using a responsible AI-first approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the development of generative AI applications
    with a responsible AI-first approach. In [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117),
    we delved into the lifecycle of large language models (LLMs); however, we will
    now examine this through the lens of responsible AI. We aim to discuss how to
    integrate these principles into the various stages of development, namely ideating/exploring,
    building/augmenting, and operationalizing. Achieving this integration demands
    tight collaboration among research, compliance, and engineering teams, effectively
    bringing people, processes, and technology together. This ensures ethical data
    use, eliminating biases from LLM responses and safety and maintaining transparency
    from the initial design stage to deployment and production and beyond. Continuous
    monitoring and observability post-deployment ensure these models remain relevant
    and ethically compliant over time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – LLM Application Development Lifecycle](img/B21443_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – LLM Application Development Lifecycle
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already discussed the **Large Language Model Application Development
    Lifecycle** (**LLMADL**), as shown in [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117).
    Therefore, we won’t delve into its details again. The following image illustrates
    the mitigation layers in the application and platform layers, which are essential
    for building a safe AI system. In this section, we will explore how we can incorporate
    these mitigation layers into the LLMADL process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Mitigation layers of gen AI applications](img/B21443_09_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Mitigation layers of gen AI applications
  prefs: []
  type: TYPE_NORMAL
- en: Ideating/exploration loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first loop involves ideation and exploration, focusing on identifying a
    use case, formulating hypotheses, selecting appropriate LLMs, and creating prompt
    variants that adhere to safety and ethical standards. This stage emphasizes the
    importance of aligning the LLM’s use case with ethical guidelines to prevent bias
    or harm. For example, in developing an LLM-powered chatbot for mental health support,
    it’s crucial to use diverse and inclusive datasets, avoid stereotypes and biases,
    and implement mechanisms to prevent harmful advice. Hypotheses formulated during
    this phase should prioritize fairness, accountability, transparency, and ethics,
    such as ensuring balanced and fair responses by training the LLM with datasets
    that have equal representation of gender and minority group dialogues:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Layer**: The decision to implement a mitigation layer in the model
    layer is made at this stage. This process includes identifying models that comply
    with RAI principles. Often, these safety mitigations are incorporated into models
    through fine-tuning and reinforcement learning from human feedback (RLHF); additionally,
    some benchmarks can provide guidance in making this decision. We covered RLHF
    and benchmarks in [*Chapter 3*](B21443_03.xhtml#_idTextAnchor052), highlighting
    them as potent techniques for developing models that are honest, helpful, and
    harmless. For instance, a benchmark holistic evaluation of language models (HELMs)
    from Stanford Research evaluates models for different tasks using seven key metrics:
    **accuracy**, **calibration**, **robustness, fairness**, **bias**, **toxicity**,
    and **efficiency**. Metrics for different models can be found using the following
    link; these can be a potential first step in the initial assessment when shortlisting
    models based on RAI principles: [https://crfm.stanford.edu/helm/classic/latest/#/leaderboard](https://crfm.stanford.edu/helm/classic/latest/#/leaderboard).
    Model cards associated with LLMs provided by **Hugging Face** and also **Azure
    AI Model Catalog** can also help you do your initial RAI assessment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety system**: For many applications, depending solely on the safety mechanisms
    integrated within the model is insufficient. Large language models can make errors
    and are vulnerable to attacks, such as jailbreak attempts. Hence, it is important
    to implement a robust content filtering system in your application to prevent
    the generation and dissemination of harmful or biased content. Once this safety
    system is activated, it becomes crucial to apply the red team testing approaches
    featuring human involvement, as outlined in [*Chapter 8*](B21443_08.xhtml#_idTextAnchor163).
    This is to guarantee the robustness of this security layer and its freedom from
    vulnerabilities. Red teaming specialists play a vital role in detecting potential
    harm and subsequently facilitate deployment of measurement strategies to confirm
    the effectiveness of the implemented mitigations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Content Safety** is a content filtering application that can help you
    detect and filter out toxic user-generated or AI-generated content, which could
    be text or images. It can also provide protection from jailbreaking attempts.
    Additionally, it can provide severity levels in terms of toxicity along with categorizations
    such as violence, self-harm, sexual, and hate. You can also enable batch evaluations
    of large datasets of prompts and completions for your applications. For example,
    as seen in *Figure 9**.4*, when testing the prompt Painfully twist his arm and
    then punch him in the face, the content was rejected because of the strong filter
    set out on the right side to filter out violent content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Results from Azure content safety](img/B21443_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Results from Azure content safety
  prefs: []
  type: TYPE_NORMAL
- en: Building/augmenting loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This stage is part of the second loop. After the team identifies the desired
    models, in this stage, the goal is to tailor the models based on business requirements
    through prompt engineering and grounding the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metaprompting and grounding**: As outlined in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098),
    prompt engineering and metaprompts can enhance retrieval accuracy. At this stage,
    it’s important to incorporate metaprompts that address four key components: harmful
    content, grounding, copyright issues, and jailbreaking prevention to improve safety.
    We have already explored these metaprompt components with examples in [*Chapter
    5*](B21443_05.xhtml#_idTextAnchor098), so we will not delve into details here.
    However, this area is continuously evolving, and you can expect to see more templates
    emerge over time. When addressing grounding, it’s crucial to ensure that the data
    retrieved from Vector DB complies with responsible AI principles. This means not
    only should the data be unbiased, but there should also be transparency regarding
    the sources of data utilized in the retrieval system, ensuring they are ethically
    sourced. In the case of customer data, data privacy is accorded the highest priority.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**: It is important to evaluate LLM models before deploying into
    production. Metrics such as groundedness, relevance, and retrieval score can help
    you determine the performance of models. Additionally, you can create custom metrics
    with LLMs such as GPT-4 and use them to evaluate your models. Azure Prompt Flow
    helps you achieve this with out-of-the-box metrics and also enables you create
    custom metrics. The following figure captures a snapshot from an experiment carried
    out using Prompt Flow, along with the associated evaluation scores. *Figure 9**.6*
    offers a visualization of the test conducted on an evaluation dataset. The LLM
    responses were assessed against the actual answers, and an average rating of 4
    or higher for groundedness, the retrieval score, and relevance suggests that the
    application is performing effectively:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Azure Prompt Flow evaluation metrics (visualization)](img/B21443_09_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Azure Prompt Flow evaluation metrics (visualization)
  prefs: []
  type: TYPE_NORMAL
- en: Operationalizing/deployment loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This stage marks the final loop, transitioning from development into production,
    and includes designing monitoring processes that continuously evaluate metrics.
    These metrics provide a clearer indication of specific types of drifts. For instance,
    the model’s groundedness could diminish over time if the data were grounded or
    become outdated. This phase also involves integrating continuous integration/continuous
    deployment (CI/CD) processes to facilitate automation. Additionally, collaboration
    with the user experience (UX) team is crucial to ensure the creation of a safe
    user experience:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User experience**: In this layer, incorporating a human feedback loop to
    assess the responses of LLM models is crucial. This can be achieved through simple
    mechanisms such as a thumbs up and thumbs down system. Additionally, setting up
    predefined responses for inappropriate inquiries adds significant value. For instance,
    if a user enquires about constructing a bomb, the system automatically intercepts
    this and delivers a preset response. Furthermore, offering a prompt guide that
    integrates RAI principles and includes citations with responses is an effective
    strategy to guarantee the reliability of the responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: Continuous model monitoring is a crucial component of LLMOps,
    guaranteeing that AI systems stay pertinent in the face of changing societal norms
    and data trends over time. Azure Prompt Flow offers advanced tools for monitoring
    the safety and performance of your application in a production environment. This
    setup facilitates straightforward monitoring using predefined metrics such as
    groundedness, relevance, coherence, fluency, and similarity or custom metrics
    relevant to your use case. We have already conducted a lab in [*Chapter 4*](B21443_04.xhtml#_idTextAnchor070),
    focusing on evaluating RAG workflows where we discussed these metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout all these stages, it’s important to engage with stakeholders, including
    diverse user groups, to understand the impact of the LLM and to ensure that it’s
    being used responsibly. Additionally, documenting the processes and decisions
    made at each stage for accountability and transparency is a key part of responsible
    AI practices.
  prefs: []
  type: TYPE_NORMAL
- en: Role of AI architects and leadership
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI architects and leaders play a pivotal role in building responsible AI practices
    within an organization. Their actions and decisions can set the tone for how AI
    is developed, deployed, and managed. Here are some key roles and actions they
    can take:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Establishing ethical guidelines and standards**: Architects and leaders should
    develop and enforce ethical guidelines for AI development and use within the organization.
    This includes principles around fairness, transparency, privacy, and accountability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Promoting transparency and explainability**: They should advocate for transparency
    in AI systems, ensuring that stakeholders understand how AI decisions are made.
    This involves promoting the development of explainable AI models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensuring data privacy and security**: Leaders must prioritize data privacy
    and security, implement robust policies and practices to protect sensitive information,
    and comply with relevant data protection regulations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fostering an inclusive and diverse AI culture**: Encouraging diversity in
    AI teams and in datasets is crucial. Diverse perspectives help to reduce biases
    in AI systems and make them more equitable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementing continuous monitoring and evaluation**: Regularly monitoring
    AI systems for performance, fairness, and unintended consequences is essential.
    Leaders should establish protocols for the ongoing evaluation and auditing of
    AI systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Investing in responsible AI education and training**: Providing training
    and resources for employees on responsible AI practices helps to create a culture
    of ethical AI use. This includes educating teams about potential biases and how
    to mitigate them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encouraging collaboration and stakeholder engagement**: Engaging with various
    stakeholders, including users, ethicists, and industry experts, can provide diverse
    insights into the potential impacts of AI solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk assessment and management**: Conducting thorough risk assessments to
    understand the potential negative impacts of AI and implementing strategies to
    mitigate these risks is vital.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creating accountability structures**: Setting up clear lines of accountability
    within the organization for AI decision-making helps to maintain ethical standards
    and address any issues that arise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Promoting sustainable AI practices**: Ensuring that AI practices are sustainable
    and do not adversely affect the environment or society is an important consideration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supporting regulation and compliance**: Keeping abreast of and complying
    with international, national, and industry-specific AI regulations and standards
    is crucial for responsible AI deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By taking these actions, architects and leaders can guide their organizations
    toward responsible AI practices, ensuring that AI technologies are used in a way
    that is ethical, fair, reliable, inclusive, safe, secure, and beneficial for all
    stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: AI, the cloud, and the law – understanding compliance and regulations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss compliance in the context of building AI solutions
    on the cloud responsibly, as it ensures that AI systems align with legal, ethical,
    and societal norms. Compliance acts as a safeguard against risks such as bias,
    privacy breaches, and unintended consequences, fostering trust among users and
    stakeholders. It promotes transparency and accountability in AI operations, encouraging
    the adoption of best practices and standardization across the industry. Moreover,
    by addressing public concerns and anticipating future challenges, compliance discussions
    help in shaping AI technologies that are not only technologically advanced but
    also socially responsible and beneficial. This is particularly important in a
    global context where AI’s impact crosses borders and cultural divides.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When architecting generative AI solutions on the cloud, there are several compliance
    considerations to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data privacy regulations**: These comply with global data protection laws
    such as GDPR (Europe), CCPA (California), and others, depending on the geographical
    location and scope of your service or industry. The **General Data Protection
    Regulation** (**GDPR**) is a comprehensive data protection law in the European
    Union that sets guidelines for the collection and processing of personal information
    from individuals in the EU. Adhering to GDPR is crucial, as it ensures the protection
    of personal data, builds trust with customers, and avoids significant fines for
    non-compliance, thereby maintaining a company’s reputation and legal standing
    in the global market. The **California Consumer Privacy Act** (**CCPA**) is a
    state statute in California, USA, designed to enhance privacy rights and consumer
    protection for residents of California. Adhering to CCPA laws is important because
    it ensures compliance with California’s stringent privacy regulations, builds
    consumer trust by protecting personal data, and helps avoid significant financial
    penalties for non-compliance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Industry-specific regulations**: Some examples of industry-specific regulations
    are **Health Insurance Portability and Accountability Act** (**HIPAA**) for healthcare
    data in the US and Canada, **Payment Card Industry Data Security Standard** (**PCI
    DSS**) for payment card information, and FERPA for educational records. **FERPA**
    stands for the **Family Educational Rights and Privacy Act**. It’s a US federal
    law that protects the privacy of student education records and gives parents specific
    rights with respect to their children’s education records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service organization control (SOC) reports**: Ensure compliance with SOC
    2, which focuses on security, availability, processing integrity, confidentiality,
    and the privacy of a system. SOC 2 compliance is more about trust and assurance
    than legal obligation, but its implications are significant in terms of security,
    business relationships, and overall reputation in the market.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud security measures**: Cloud solutions must be secure to protect sensitive
    data against breaches. This involves enabling encryption, access controls, and
    regular security audits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auditability and reporting**: Being able to track and report on how the AI
    system makes decisions can be important for regulatory compliance and transparency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data localization/residency laws**: Some jurisdictions require that data
    be stored within the country of origin, which can affect cloud service choices
    and architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business continuity and disaster recovery**: Adhere to standards that ensure
    business continuity and disaster recovery, such as ISO/IEC 22301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Top cloud providers, such as Microsoft, have a robust compliance portfolio
    to assist their customers. They provide necessary tools such as Microsoft Purview
    and comprehensive documentation to aid customers on their compliance journey.
    For a full list, we recommend checking out the compliance offerings from Microsoft
    here: [https://learn.microsoft.com/en-us/compliance/regulatory/offering-home](https://learn.microsoft.com/en-us/compliance/regulatory/offering-home).'
  prefs: []
  type: TYPE_NORMAL
- en: Global and United States AI regulatory landscape
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The current global AI regulatory landscape is marked by diverse approaches and
    emerging trends. Accelerating capabilities in AI, including large language models,
    facial recognition, and advanced cognitive processing, have propelled AI regulation
    to prominence among policy-makers.
  prefs: []
  type: TYPE_NORMAL
- en: Europe has been the frontrunner in this journey towards AI regulation. The EU
    Act has made significant progress towards becoming law, with unanimous approval
    from EU member states as of February 2, 2024\. It sets a global standard for AI
    technology, emphasizing a balance between innovation and safety. The EU AI Act
    introduces a nuanced regulatory framework for artificial intelligence, categorizing
    AI systems based on their risk levels to ensure appropriate oversight. Systems
    posing an “**unacceptable risk**,” such as those capable of cognitive manipulation
    or implementing social scoring based on certain protected traits, biometric identification,
    and the categorization of people, are outright banned, with narrow exceptions
    for law enforcement under stringent conditions. “**High-risk”** AI systems, impacting
    safety or fundamental rights, are subject to strict assessment and registration
    requirements, covering a wide range of applications from critical infrastructure
    management, assistance in legal interpretation, and education to law enforcement.
    Meanwhile, “general purpose and generative AI,” such as ChatGPT, must adhere to
    transparency directives, including the disclosure of AI-generated content and
    measures against illegal and toxic content production and publishing summaries
    of copyrighted data used for training. Systems deemed “**limited risk”** should
    comply with minimal transparency requirements. This includes applications with
    image, audio, or video generation models, facilitating informed decisions by users.
    This stratified approach aims to balance the innovation potential of AI with necessary
    safeguards against its potential harms ([https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)).
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, India initially opted against AI regulation, focusing on policy
    and infrastructure to foster AI growth, but later considered a regulatory framework
    addressing algorithm biases and copyrights. The US hasn’t moved towards comprehensive
    federal AI legislation but has seen regulatory responses from agencies such as
    the National Institute of Standards and Technology (NIST), the Federal Trade Commission
    (FTC), and the Food and Drug Administration (FDA) regarding public concerns over
    AI technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory frameworks are developing globally to balance AI’s benefits against
    its risks. EY’s analysis of eight jurisdictions (Canada, China, EU, Japan, Korea,
    Singapore, UK, and the US) reflects a variety of regulatory approaches. The rules
    and policy initiatives were inspired by the OECD’s Organization for Economic Co-operation
    AI policy Observatory.
  prefs: []
  type: TYPE_NORMAL
- en: OECD is an international organization comprising 38 member countries, established
    to promote economic progress and world trade by offering a platform for democratic,
    market-economy nations to discuss policies, share experiences, and co-ordinate
    on global issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'As per this research from Ernst and Young, released in September 2023, five
    common regulatory trends have emerged globally:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Alignment with key AI principles**: The AI regulation and guidance being
    evaluated align with the key AI principles of human rights for respect, sustainability,
    transparency, and robust risk management, as established by the OECD and supported
    by the G20\. The Group of Twenty (G20) is an international forum of 19 countries
    and the European Union focused on addressing global economic issues and representing
    the world’s major economies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk-based approach**: These jurisdictions adopt a risk-based approach to
    AI regulation, meaning they customize their AI rules based on the perceived risks
    AI poses to fundamental values such as privacy, non-discrimination, transparency,
    and security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sector and sector-agnostic rules**: Due to the diverse applications of AI,
    certain jurisdictions are emphasizing the importance of sector-specific regulations
    alongside more general, sector-agnostic rules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Digital priority areas**: In the realm of other digital priority areas such
    as cybersecurity, data privacy, and intellectual property rights, jurisdictions
    are advancing in their creation of AI-specific regulations, with the European
    Union leading in adopting a comprehensive strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration with private sector and policy-makers**: Numerous jurisdictions
    employ regulatory sandboxes, allowing private sector collaboration with policy-makers
    to craft rules that both ensure safe, ethical AI and address the potential need
    for closer oversight in higher-risk AI innovations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biden Executive Order on AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On October 30, 2023, President Joe Biden issued an Executive Order, which we
    think is a major step towards regulating AI in the United States. The Executive
    Order is thoroughly comprehensive, simultaneously ensuring human safety and responsible
    AI use while fostering fair competition within the country and advancing leadership
    on the global stage. There are eight major topics that the EO covers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**New standards for AI safety and security**: The Executive Order requires
    developers of powerful AI systems to share safety test results with the US government.
    It establishes standards and tests to ensure AI systems are safe and secure before
    public release, addresses risks in using AI for biological materials, and combats
    AI-enabled fraud and deception. An advanced cybersecurity program will also be
    developed to leverage AI in securing software and networks. It directs the National
    Security Council and White House Chief of Staff to develop a National Security
    Memorandum, guiding further AI and security actions, ensuring the US military
    and intelligence community’s safe, ethical, and effective use of AI, and outlining
    measures to counter adversaries’ military AI applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Protecting Americans’ privacy**: The order emphasizes protecting privacy
    by accelerating the development and use of privacy-preserving techniques in AI.
    It includes funding research for privacy technologies and developing guidelines
    for federal agencies to evaluate the effectiveness of these techniques, especially
    in AI systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advancing equity and civil rights**: This addresses the responsible principles
    of fairness and inclusiveness. To combat discrimination and bias in AI, the order
    provides guidance to landlords and federal programs, addresses algorithmic discrimination
    through training and technical assistance, and aims to ensure fairness in the
    criminal justice system through the development of best practices in AI use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standing up for consumers, patients, and students**: This includes advancing
    responsible AI use in healthcare, such as developing affordable drugs and establishing
    a safety program for healthcare practices involving AI. It also involves creating
    resources to support educators using AI-enabled educational tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supporting workers**: The order directs the development of principles and
    best practices to maximize AI benefits for workers, addressing issues such as
    job displacement, labor standards, and workplace equity. It also includes producing
    a report on AI’s potential impact on the labor market.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Promoting innovation and competition**: Actions include catalyzing AI research
    nationwide, promoting a competitive AI ecosystem by providing resources to small
    developers, and expanding the ability of skilled immigrants to work in the US
    in AI-related fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advancing American leadership abroad**: The administration will work with
    other nations to support the global deployment and use of safe and trustworthy
    AI. This involves expanding engagements to collaborate on AI, developing AI standards
    with international partners, and promoting responsible AI development to address
    global challenges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensuring responsible and effective governmental use of AI**: The order aims
    to modernize federal AI infrastructure and ensure responsible AI deployment in
    government. This includes issuing guidance for AI use in agencies, accelerating
    the hiring of AI professionals, and providing AI training to government employees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, while compliance plays a pivotal role in fostering safer and more
    responsible AI systems, it can indeed be a double-edged sword. Excessive compliance
    requirements might stifle innovation, potentially hindering a country’s competitive
    edge on the global stage. Therefore, it’s imperative that regulators are well-informed
    and engage in thorough consultations with AI experts when crafting regulations
    and standards. This balanced approach ensures that AI develops in a safe and ethical
    manner while still allowing for the flexibility and creativity necessary for technological
    advancement and competitive success.
  prefs: []
  type: TYPE_NORMAL
- en: Startup ecosystem in RAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss a few notable startups emerging in the responsible
    AI space and building products that keep RAI at their core.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parity AI**: Founded by Rumman Chowdhury, Parity AI focuses on AI risk management
    and offers tools for auditing AI models for bias or legal compliance and provides
    recommendations for addressing these issues ([https://www.get-parity.com/](https://www.get-parity.com/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fiddler**: Founded by Krishna Gade, Fiddler focuses on explainability in
    AI, helping to make AI model decisions more transparent. It aids data science
    teams in monitoring their models’ performance and generating executive summaries
    from the outcomes. If a model’s accuracy declines or displays bias, Fiddler assists
    in identifying the reasons. Gade views model monitoring and enhancing clarity
    as key initial steps for more deliberate AI development and deployment ([https://www.fiddler.ai/ai-observability](https://www.fiddler.ai/ai-observability)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Arthur**: Founded in 2019, Arthur is a company specializing in AI performance,
    assisting enterprise clients in maximizing their AI’s potential through performance
    monitoring and optimization, providing explainability, and mitigating bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weights and Biases**: Founded in 2017, Weights and Biases focuses on the
    reproducibility aspect of machine learning model experiments. In my opinion, reproducibility
    is vital in AI because it forms the bedrock of scientific trust and validation.
    It allows for the independent verification of results, facilitating the correction
    of errors and building upon research findings. Crucially, in the context of AI’s
    rapid transition from research to real-world applications, reproducibility ensures
    that AI models are robust, unbiased, and safe. It also helps address the AI ‘black-box’
    problem by allowing a broader understanding of how models function. This is particularly
    important in high-stakes areas such as healthcare, law enforcement, and public
    interaction, where AI’s impact is direct and significant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datagen**: Datagen specializes in computer vision and facial data, ensuring
    their datasets are varied in terms of skin tones, hairstyles, genders, and angles
    to reduce bias in facial recognition technology ([https://datagen.tech/](https://datagen.tech/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Galileo and Snorkel AI**: Galileo and Snorkel AI focus on maintaining high
    data quality; Galileo does this by automatically adjusting biases in unstructured
    data, whereas Snorkel AI ensures equitable, automated labeling, along with data
    versioning and audit services ([https://www.rungalileo.io/](https://www.rungalileo.io/),[https://snorkel.ai/](https://snorkel.ai/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list is not exhaustive. This space is evolving, and there are
    numerous new start-ups making significant inroads in this field.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Start-up ecosystem in RAI](img/B21443_09_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Start-up ecosystem in RAI
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure, referenced from BGV ([https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/](https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/)),
    shows a few notable start-ups providing ethical AI services across five categories:
    data privacy, AI monitoring and observability, AI audits, governance, risk, compliance,
    targeted AI solutions and technologies, and open source solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize, the development of more sophisticated AI systems and the journey
    towards achieving **artificial general intelligence** (**AGI**) necessitates a
    steadfast commitment to RAI principles. Neglecting these principles could result
    in AI posing significant risks to humanity. In this chapter, we delved deeply
    into responsible AI principles, uncovering their theoretical and practical implications,
    especially within the realms of LLMs and Deepfake technology. We highlighted the
    importance of ethical vigilance and the role of architecture and leadership in
    guiding AI towards beneficial applications, alongside an analysis of the current
    regulatory landscape shaping AI’s evolution. Our exploration extended to responsible
    AI tools and the dynamic startup ecosystem, emphasizing how new companies are
    both influencing and adapting to these AI trends. These insights are crucial,
    as they equip us with the knowledge to harness AI’s power responsibly, ensuring
    its alignment with ethical standards and societal benefits. Looking ahead, in
    the final chapter, we will discuss the future of ChatGPT, where we’ll delve into
    emerging trends and potential advancements, highlighting innovative uses that
    are set to redefine our interaction with AI and society.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AI is sending people to jail—and getting it wrong: [https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thousands of Dollars for Something I Didn’t Do: [https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html?login=ml&auth=login-ml](https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html?login=ml&auth=login-ml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Can the criminal justice system’s AI be truly fair?: [https://tinyurl.com/bdejxubh](https://tinyurl.com/bdejxubh)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The journey to build an explainable AI-driven recommendation system to help
    scale sales efficiency across LinkedIn:[https://www.linkedin.com/blog/engineering/recommendations/the-journey-to-build-an-explainable-ai-driven-recommendation-sys](https://www.linkedin.com/blog/engineering/recommendations/the-journey-to-build-an-explainable-ai-driven-recommendation-sys)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Empowering the Future of Recruitment: 7 AI Hiring Tools Ushering in a Bright
    2023 - HyScaler: [https://hyscaler.com/insights/ai-hiring-tools-7-trends-2023/](https://hyscaler.com/insights/ai-hiring-tools-7-trends-2023/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Worried about your firm’s AI ethics? These startups are here to help. | MIT
    Technology Review: [https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/](https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The AI Ethics Boom: 150 Ethical AI Startups and Industry Trends - BGV: [https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/](https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Responsible AI toolkits: [https://odsc.medium.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today-fbc1c2ea2815](https://odsc.medium.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today-fbc1c2ea2815)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deepfakes, explained | MIT Sloan: [https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained](https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regulatory Landscape: [https://www.goodwinlaw.com/en/insights/%20publications/2023/04/04_12-us-artificial-intelligence-regulations](https://www.goodwinlaw.com/en/insights/%20publications/2023/04/04_12-us-artificial-intelligence-regulations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Artificial Intelligence regulation, global trends | EY - US: [https://www.ey.com/en_us/ai/how-to-navigate-global-trends-in-artificial-intelligence-regulation#:~:text=,rapidly%20evolving%20AI%20regulatory%20landscape](https://www.ey.com/en_us/ai/how-to-navigate-global-trends-in-artificial-intelligence-regulation#:~:text=,rapidly%20evolving%20AI%20regulatory%20landscape)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Infuse responsible AI tools and practices in your LLMOps | Microsoft Azure
    Blog: [https://azure.microsoft.com/en-us/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/](https://azure.microsoft.com/en-us/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 5: Generative AI – What’s Next?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concluding part delves into the future prospects of generative AI, particularly
    the advancements in multimodal AI, with a detailed look at GPT-4 Turbo with vision
    capabilities. It also examines the emergence of **Smaller Language Models** (**SLMs**)
    and their significant impact on edge computing, a trend that facilitates faster
    and more efficient AI processing closer to the data source. Additionally, we’ll
    explore other emerging trends, future predictions, and the integration of generative
    AI with robotics, highlighting the synergy between these technologies. The journey
    toward achieving **Artificial General Intelligence** (**AGI**) through the unparalleled
    computational power of quantum computing will also be discussed, mapping out the
    potential roadmap and the technological leaps required to realize AGI.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B21443_10.xhtml#_idTextAnchor218), *Future of Generative AI:
    Trends and Emerging Use Cases*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
