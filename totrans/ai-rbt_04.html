<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-63"><a id="_idTextAnchor126"/>4</h1>
<h1 id="_idParaDest-64"><a id="_idTextAnchor127"/>Recognizing Objects Using Neural Networks and Supervised Learning</h1>
<p>This is the chapter where we’ll start<a id="_idIndexMarker280"/> to combine <strong class="bold">robotics</strong> and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) to <a id="_idIndexMarker281"/>accomplish some of the tasks we laid out so carefully in previous chapters. The subject of this chapter is <strong class="bold">object recognition</strong> – we <a id="_idIndexMarker282"/>will be teaching the robot to recognize what a toy is so that it can then decide what to pick up and what to leave alone. We will be using <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) as machine learning tools for separating <a id="_idIndexMarker283"/>objects in images, recognizing them, and locating them in the camera frame so that the robot can then locate them. More specifically, we’ll be using images to recognize objects. We’ll be taking a picture and then looking to see whether the computer recognizes specific types of objects in those pictures. We won’t be recognizing objects themselves, but rather images or pictures of objects. We’ll also be putting bounding boxes around objects, separating them from other objects and background pixels.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>A brief overview of image processing</li>
<li>Understanding our object recognition task</li>
<li>Image manipulation</li>
<li>Using YOLOv8 – an object recognition model</li>
</ul>
<h1 id="_idParaDest-65"><a id="_idTextAnchor128"/>Technical requirements</h1>
<p>You will be able to accomplish all of this chapter’s tasks without a robot if yours cannot walk yet. We will, however, get better results if the camera is in the proper position on the robot. If you don’t have a robot, you can still do all of these tasks with a laptop and a USB camera. </p>
<p>Overall, here’s the hardware and software that you will need to complete the tasks in this chapter:</p>
<ul>
<li>Hardware:<ul><li>A laptop computer</li><li>Nvidia Jetson Nano</li><li>USB camera</li></ul></li>
<li>Software:<ul><li>Python 3</li><li>OpenCV2</li><li>TensorFlow</li><li>YOLOv8, which is available at <a href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</a></li></ul></li>
</ul>
<p>The source code for this chapter can be found at <a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e</a>.</p>
<p>In the next section, we will discuss what image processing is.</p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor129"/>A brief overview of image processing</h1>
<p>Most of you will <a id="_idIndexMarker284"/>be very familiar with computer images, formats, pixel depths, and maybe even convolutions. We will be discussing these concepts in the following sections; if you already know this, skip ahead. If this is new territory, read carefully, because everything we’ll do after is based on this information.</p>
<p>Images are stored in a computer as a two-dimensional<a id="_idIndexMarker285"/> array of <strong class="bold">pixels</strong> or picture elements. Each pixel is a tiny dot. Thousands or millions of tiny dots make up each image. Each pixel is a number or series of numbers that describe its color. If the image is only a grayscale or black-and-white image, then each pixel is represented by a single number that corresponds to how dark or light the tiny dot is. This is straightforward so far.</p>
<p>If the image is a color picture, then each dot has three numbers that are combined to make its color. Usually, these numbers are the intensity of <strong class="bold">Red, Green, and Blue</strong> (<strong class="bold">RGB</strong>) colors. The <a id="_idIndexMarker286"/>combination (0,0,0) represents black (or the absence of all colors), while (255,255,255) is white (the sum of all colors). This process is called the additive color model. If you work with watercolors instead of computer pixels, you’ll know that adding all the colors in your watercolor box makes black – that is a subtractive color model. Red, green, and blue are primary colors that can be used to make all of the other colors. Since an RGB pixel is represented by three colors, the actual image is a three-dimensional array rather than a two-dimensional one since each pixel has three numbers, making an array of (height, width, 3). So, a picture that is 800 x 600 pixels would be represented by an array of dimensions given by (800,600,3), or 1,440,000. That is a lot of numbers. We will be working very hard to minimize the number of pixels we are processing at any given time.</p>
<p>While RGB is one <a id="_idIndexMarker287"/>set of three numbers that can describe a pixel, there are other ways of describing <a id="_idIndexMarker288"/>the <strong class="bold">color formula</strong> that have various usages. We don’t have to use RGB – for instance, we can also use <strong class="bold">Cyan, Yellow, and Magenta</strong> (<strong class="bold">CYM</strong>), which <a id="_idIndexMarker289"/>are the complementary colors to RGB, as shown in <em class="italic">Figure 4</em><em class="italic">.2</em>. We can also break down colors using<a id="_idIndexMarker290"/> the <strong class="bold">Hue, Saturation, and Value</strong> (<strong class="bold">HSV</strong>) model, which classifies color by hue (shade of color), saturation (intensity of color), and value (brightness of color). HSV is a very useful color space for certain calculations, such as converting a color image into grayscale (black and white). To turn RGB into a grayscale pixel, you have to do a bit of math – you can’t just pull out one channel and keep it. The formula for RGB to grayscale, as defined by the <strong class="bold">National Television System Committee</strong> (<strong class="bold">NTSC</strong>), is<a id="_idIndexMarker291"/> as follows:</p>
<p><em class="italic">0.299*Red + 0.587*Green + </em><em class="italic">0.114*Blue</em></p>
<p>This is because the different wavelengths of light behave differently in our eyes, which are more sensitive to green. If you have color in the HSV color model, then creating a grayscale image involves considering <em class="italic">V</em> (value) and throwing the <em class="italic">H</em> and <em class="italic">S</em> values away. As you can imagine, this is a lot simpler. This is important to understand as we will be doing quite a bit of image manipulation throughout this chapter. But first, in the following section, we’ll discuss<a id="_idIndexMarker292"/> the image recognition task we will be performing in this chapter.</p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor130"/>Understanding our object recognition task</h1>
<p>Having a computer<a id="_idIndexMarker293"/> or robot recognize an image of a toy is not as simple as taking two pictures and then saying <code>if picture A = picture B, then toy</code>. We are going to have to do quite a bit of work to be able to recognize a variety of objects that are randomly rotated, strewn about, and at various distances. We could write a program to recognize simple shapes – hexagons, for instance, or simple color blobs – but nothing as complex as a toy stuffed dog. Writing a program that did some sort of analysis of an image and computed the pixels, colors, distributions, and ranges of every possible permutation would be extremely difficult, and the result would be very fragile – it would fail at the slightest change in lighting or color.</p>
<p>Speaking from experience, I had a recent misadventure with a large robot that used a traditional computer vision system to find its battery charger station. That robot mistook an old, faded soft drink machine for its charger – let’s just say that I had to go b<a id="_idTextAnchor131"/>uy more fuses.</p>
<p>What we will do instead is teach the robot to recognize a set of images corresponding to toys that we will take from various angles. We will do this by using a special type of <strong class="bold">artificial neural network</strong> (<strong class="bold">ANN</strong>) that performs convolution operations on images. It is classified as<a id="_idIndexMarker294"/> an AI technique because instead of programming our software to recognize objects by writing code, we will be training a neural network to correctly <em class="italic">segment</em> (separate from the rest of the image) and <em class="italic">label</em> (classify) groups of pixels in an image by how closely they resemble groups of labeled pixels that the network was trained on. Rather than the code determining the robot’s behavior, it is the data we train the network on that does the work. Since we (the humans) will be training the neural network by providing segmented and labeled images, this is <a id="_idIndexMarker295"/>called <strong class="bold">supervised learning</strong>. This involves telling the network what we want it to learn and reinforcing (rewarding) the network based on how well it performs. We’ll <a id="_idIndexMarker296"/>discuss <strong class="bold">unsupervised learning</strong> in <a href="B19846_08.xhtml#_idTextAnchor235"><em class="italic">Chapter 8</em></a>. This process entails us not telling the software exactly what to learn, which means it must determine that for itself.</p>
<p>To clarify, in this section, we will tell the ANN what we want it to learn, which in this case is to recognize a class of objects we will call <em class="italic">toys</em>, and to draw a bounding box around those objects. This bounding box will tell other parts of the robot that a toy is visible and where it is in the image.</p>
<p>I’ll be emphasizing the unique components we will use to accomplish our task of recognizing toys in an<a id="_idIndexMarker297"/> image. Do you remember what the storyboard from <a href="B19846_03.xhtml#_idTextAnchor043"><em class="italic">Chapter 3</em></a> told us to do?</p>
<div><div><img alt="Figure 4.1 – Use case for identifying objects as toys" src="img/B19846_04_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Use case for identifying objects as toys</p>
<p>Our image recognizer must figure out what objects are toys and then locate them in the image. This is illustrated <a id="_idIndexMarker298"/>in the preceding sketch; objects marked as toys have circles around them. The image recognizer must recognize not just <em class="italic">what</em> they are, but also <em class="italic">where</em> they are.</p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor132"/>Image manipulation</h1>
<p>So, now that we<a id="_idIndexMarker299"/> have an image, what can we do with it? You have probably played with Adobe Photoshop or some other image manipulation program such as GIMP, and you know that there are hundreds of operations, filters, changes, and tricks you can perform on images. For instance, can make an image brighter or darker by adjusting the brightness. We can increase the contrast between the white parts of the image and the dark parts. We can make an image blurry, usually by applying a Gaussian blur filter. We can also make an image sharper (somewhat) by using a filter such as an unsharp mask. You can also use an edge detector filter, such as the Canny filter, to isolate the edges of an image, where color or value changes. We will be using all of these techniques to help the computer identify images:</p>
<div><div><img alt="Figure 4.2 – Various convolutions applied to an image" src="img/B19846_04_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Various convolutions applied to an image</p>
<p>By performing<a id="_idIndexMarker300"/> these manipulations, we want the computer to not have the computer software be sensitive to the size of the image, which is called <strong class="bold">scale invariance</strong>, the angle at which the photograph was taken, or <strong class="bold">angle invariance</strong>, and the lighting available, which is known as <strong class="bold">illumination invariance</strong>. This is all very desirable in a computer vision system – we would not want an AI system that only recognizes our toys from the same angle and distance as the original image. Remember, we are going to train our vision system to recognize toys based on a labeled set of training images we take in advance, and the robot will have to recognize objects based on what it learned from the training set. Here, we are going to use features from the image that mostly don’t change based on size, angle, distance, or lighting. What sorts of features might these be?</p>
<p>If we look at a common household object, such as a chair, and inspect it from several angles, what about the chair does not change? The easy answer is the edges and corners. The chair has the same number of corners all the time, and we can see a consistent number of them from most angles. It also has a consistent number of edges.</p>
<p>Admittedly, that is a bit of an oversimplification of the approach. We will be training our ANN on a whole host of image features that may or may not be unique to this object and let it decide which work and which do not. We will accomplish this by using a generic approach<a id="_idIndexMarker301"/> to image manipulation called <strong class="bold">convolution</strong>.</p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor133"/>Convolution</h2>
<p>Every once in a while, you’ll <a id="_idIndexMarker302"/>come across some mathematical construction that turns a complex task into just a bunch of adding, subtracting, multiplying, and dividing. Vectors in geometry work like that, and, in image processing, we<a id="_idIndexMarker303"/> have the <strong class="bold">convolution kernel</strong>. It transpires that most of the common image processing techniques – edge detection, corner detection, blurring, sharpening, enhancing, and so on – can be accomplished with a simple array construct.</p>
<p>It is pretty easy to understand that in an image, the neighbors of a pixel are just as important to what a pixel is as the pixel itself. If you were going to try and find all the edge pixels of a box, you would look for a pixel that has one type of color on one side, and another type on the other. We need a function to find edges by comparing pixels on one side of a pixel to the other.</p>
<p>The convolution kernel is a matrix function that applies weights to the pixel neighbors – or pixels around the one pixel we are analyzing. The function is usually written like this, as a 3x3 matrix:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>-1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>-2</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>-1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – A sample convolution kernel</p>
<p><strong class="bold">Sobel edge detection</strong> is represented in the <em class="italic">Y</em> direction. This detects edges going up and down. Each <a id="_idIndexMarker304"/>block represents a pixel. The pixel being processed is in the center. The neighbors of the pixels on each side are the other blocks – top, bottom, left, and right. To compute the convolution, the corresponding weight is applied to the value of each pixel by multiplying the value (intensity) of that pixel, and then adding all of the results. If this image is in color – RGB – then we compute the convolution for each color separately and then<a id="_idIndexMarker305"/> combine the results. Here is an example of a convolution being applied to an image:</p>
<div><div><img alt="Figure 4.3 – Result of a Sobel edge detection convolutio﻿n" src="img/B19846_04_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Result of a Sobel edge detection convolutio<a id="_idTextAnchor134"/>n</p>
<p>The resulting image is the same size as the original. Note that we only get the edge as the result – if the colors are the same on either side of the center pixel, they cancel each other out and we get zero, or black. If they are different, we get 255, or white, as the answer. If we need a more complex result, we may also use a 5x5 convolution, which takes into account the two nearest pixels on each side, instead of just one.</p>
<p>The good news is that you don’t have to choose which convolution operation to apply to the input images – we will build a software frontend that will set up all of the convolutions. This <em class="italic">frontend</em> is just the part of the program that sets up the networks before we start training them. The neural network package we’ll be using will determine which convolutions provide the most data and support the training output we want.</p>
<p>"But wait," I hear you say. "What if the pixel is on the edge of the image and we don’t have neighbor pixels on one side?" In that case, we have to add padding to the image – which is a border of extra <a id="_idIndexMarker306"/>pixels that permits us to consider the edge pixels as well.</p>
<p>In the next section, we’ll get into the guts of a neural network.</p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor135"/>Artificial neurons</h2>
<p>What is a <strong class="bold">neuron</strong>? And <a id="_idIndexMarker307"/>how do we make a <a id="_idIndexMarker308"/>network <a id="_idIndexMarker309"/>out of them? If you can remember what you learned in biology, a biological or natural neuron has inputs, or dendrites, that connect it to other neurons or sensor inputs. All the inputs come to a central body and then leave via the axion, or connection, to other neurons via other dendrites. The connection between neurons is <a id="_idIndexMarker310"/>called a <strong class="bold">synapse</strong>, which is a tiny gap that the signal from the nerve must jump. A neuron takes inputs, processes them, and activates or sends an output after some threshold level is reached. An <strong class="bold">artificial neuron</strong> is a software construction that approximates the workings of the neurons in your brain and is a very simplified version of the natural neuron. It has several inputs, a set of weights, a bias, an activation function, and then some outputs to other neurons as a result of the network, as shown in the following figure:</p>
<div><div><img alt="Figure 4.4 – Diagram of an artificial neuron" src="img/B19846_04_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Diagram of an artificial neuron</p>
<p>Let’s<a id="_idIndexMarker311"/> describe <a id="_idIndexMarker312"/>each component in detail:</p>
<ul>
<li><strong class="bold">Input</strong>: This is a number or value that’s received from other neurons or as an input to the network. In our image processing example, these are pixels. This number can be a float or an integer – but it must be just one n<a id="_idTextAnchor136"/>umber.</li>
<li><strong class="bold">Weight</strong>: This is the adjustable value we change to <em class="italic">train</em> the neuron. Increasing the weight means that the input is more important to our answer, and likewise decreasing the weight means the input is used less. To determine the value of a neuron, we must combine the values of all the inputs. As the neural network is trained, the weights are adjusted on each input, which favors some inputs over others. We multiply the input by the weight and then sum all of the results together.</li>
<li><strong class="bold">Bias</strong>: This is a number that’s added to the sum of the weights. Bias prevents the neuron from getting stuck at zero and improves training. This is usually a small number. Imagine a scenario where all of the inputs to a neuron are zero; in this case, the weights would have no effect. Adding a small bias allows the neuron to still have an output, and the network can use that to affect learning. Without the bias, a neuron with zeros on its inputs can’t be trained (changing the weights has no effect) and is <em class="italic">stuck</em>.</li>
<li><strong class="bold">Activation function</strong>: This determines the output of the neuron based on the weighted sum of its inputs. The most common types are the <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>) – if the <a id="_idIndexMarker313"/>value of the neuron is less than zero, the output is zero; otherwise, the output is the input value – and the <strong class="bold">sigmoid</strong> (S-shaped) function, which<a id="_idIndexMarker314"/> is a log function. The activation function propagates information across the network and<a id="_idIndexMarker315"/> introduces non-linearity to the output of the neuron, which allows the neural<a id="_idIndexMarker316"/> network to approximate non-linear functions:</li>
</ul>
<div><div><img alt="Figure 4.5 – Common activation ﻿functions" src="img/B19846_04_5.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Common activation <a id="_idTextAnchor137"/>functions</p>
<ul>
<li><strong class="bold">Outputs</strong>: Each layer in the sequential neural network is connected to the next layer. Some layers are fully connected – with each neuron in the first layer connected to each neuron in the second layer. Others are sparsely connected. There is a common process in neural network training<a id="_idIndexMarker317"/> called <strong class="bold">dropout</strong>, where we randomly remove connections. This forces the network to have multiple paths for each bit of information it learns, which strengthens the network and makes it able to handle more diverse inputs.</li>
<li><strong class="bold">Max pooling of outputs</strong>: We use a special type of network layer (compared to a fully connected or sparse layer) called max pooling, where groups of neurons corresponding to regions in our image – say a 2x2 block of pixels – go to one neuron in the next level. The max pool neuron only takes the largest value from each of the four input neurons. This has the effect of downsampling the image (making it smaller). This allows the network to associate small features (such as the <a id="_idIndexMarker318"/>wheels<a id="_idIndexMarker319"/> in a Hot Wheels car) with larger features, such as the hood or windshield, to identify a toy car:</li>
</ul>
<div><div><img alt="Figure 4.6 – Max pooling operation" src="img/B19846_04_6.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – Max pooling operation</p>
<p>Now that you <a id="_idIndexMarker320"/>understand what a neural network is<a id="_idIndexMarker321"/> composed of, let’s explore how to train <a id="_idTextAnchor138"/><a id="_idTextAnchor139"/>and test one.</p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor140"/>Training a CNN</h2>
<p>I want to<a id="_idIndexMarker322"/> provide you with an end-to-end look at what we<a id="_idIndexMarker323"/> will be doing in the code for the rest of this chapter. Remember that we are building a CNN that examines pixels in a video frame and outputs if one or more pixel areas that resemble toys are in the image, and where they are. The following diagram shows the process that we will go through to train the neural network, step by step:</p>
<div><div><img alt="Figure 4.7 – CNN process" src="img/B19846_04_7.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – CNN process</p>
<p>For this task, I decided to use an already existing neural network rather than build one from scratch. There are a lot of good CNN object detectors available, and honestly, it’s hard to improve on an existing model structure. The one I’ve picked for this book is called <strong class="bold">YOLOv8</strong>, where <em class="italic">YOLO</em> stands for <em class="italic">You Only Look Once</em>. Let’s understand how we <a id="_idIndexMarker324"/>can use this model for our task.</p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor141"/>Using YOLOv8 – an object recognition model</h1>
<p>Before we dive into <a id="_idIndexMarker325"/>the details of the YOLOv8 model, let’s talk about why I selected it. First of all, the learning process is pretty much the same for any CNN we might use. YOLO is a strong open source object detection model with a lot of development behind it. It’s considered state of the art, and it already does what we need – it detects objects and shows us where they are in images by drawing bounding boxes around them. So, it tells us what objects are, and where they are located. As you will see, it is very easy to use and can be extended to detect other classes of objects other than what it was originally trained for. There are a lot of YOLO users out there who can provide a lot of support and a good basis for learning about AI object recognition for robotics.</p>
<p>As I mentioned at the beginning of this chapter, we have two tasks we need to accomplish to reach our goal of picking up toys with a robot. First, we must determine whether the robot can detect a toy with its camera (determine whether there is a toy in the camera image) and then figure out where it is in that image so that we can drive over to it and pick it up. In this chapter, we’ll learn how to detect toys, while in <a href="B19846_07.xhtml#_idTextAnchor221"><em class="italic">Chapter 7</em></a>, we’ll discuss how we determine distance and navigate to the toy.</p>
<p>YOLOv8 does both tasks in one pass, hence its name. Other kinds of object recognition models, such as the one I created in the first edition of this book, identified and located objects in images in two steps. First, it found that an object was present, and then it figured out where in the image it was located in a separate pass. This separate pass would use a sliding window approach, taking a segment of the image and using the detection part of the neural network to say <code>yes</code> or <code>no</code> if that segment contained an object it recognized. Then, it would slide the window it was considering across the image and test again. This was repeated until we had a bunch of image segments that contained the detected object. Then, a process called <em class="italic">minmax</em> would select the smallest box (min) that contained all the visible parts of the object (max).</p>
<p>YOLOv8 takes a different approach by combining two neural networks – one that detects objects it has been taught to recognize and another that is trained to draw bounding boxes based on the center of the object. The direct output of YOLOv8 includes both the detection and the bounding box for the object. YOLOv8 can also <em class="italic">segment</em> images by pixels, identifying not just a box with the object, but all the pixels that belong to that object. We’ll be using a bounding box to help us drive the robot to our toys.</p>
<p>YOLOv8 comes pretrained on a whole series of object classes (about 80), but we can check whether it already works with the toys we want to detect. Let’s test YOLOv8’s ability to detect our toys. We can install YOLOv8 using this simple command on our PC:</p>
<pre class="console">
pip install ultralytics</pre> <p>Now, to test our detection with a picture of toys in the playroom, we will use the smallest (in terms of model size) of the YOLOv8 detection models – the <code>yolov8n.pt</code>). This is the pretrained neural network that Ultralytics provides with YOLOv8:</p>
<pre class="console">
yolo task=detect mode=predict model=yolov8n.pt source="test.png"</pre> <p>As shown in the following<a id="_idIndexMarker326"/> figure, the only thing detected by the off-the-shelf YOLOv8 object model is an upside-down matchbook car, which it incorrectly labels as a skateboard:</p>
<div><div><img alt="Figure 4.8 – YOLOv8 output without specific training on our toys" src="img/B19846_04_8.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – YOLOv8 output without specific training on our toys</p>
<p>You have to admit, the little toy car does resemble a skateboard from this angle, but this is not the result we want. We need all the toys in the image to be detected, not just one. What can we do about this?</p>
<p>The answer is that we can add new training to the network, get all the advantages of YOLOv8, and have our custom objects detected as well. For this, we can use a process called <strong class="bold">tr<a id="_idTextAnchor142"/>ansfer learning</strong>.</p>
<p>Here is an overview of how we will train our toy detector, after which we will discuss these steps in greater detail:</p>
<ol>
<li>First, we will prepare a training set of images of the room with toys. This means we must take a lot of pictures of the toys from the viewpoint of the robot, using the same camera the robot will use. We want to take pictures from all the different angles and sides of the toys. I went around the room clockwise, then anti-clockwise, snapping pictures every few inches. I took 48 pictures in this step.</li>
<li>Next, we must use a data labeling program such as RoboFlow (<a href="https://roboflow.com">https://roboflow.com</a>) to annotate the images (you can refer to the relevant documentation for detailed instructions). The program lets us draw boxes around the objects we want to recognize (toys) and label them with a tag – we will use the <a id="_idIndexMarker327"/>name <code>toy</code>. We are separating the parts of the image that contain toys and telling the neural network what to call this type of object:</li>
</ol>
<div><div><img alt="Figure 4.9 – Annotating using RoboFlow, a free data labeling tool" src="img/B19846_04_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Annotating using RoboFlow, a free data labeling tool</p>
<ol>
<li value="3">Then, we must split the training set into three parts: a set we use to train the network, a set we use to validate the training, and a set we use to test the network. We will create a set of 87% of the images for training, 8% for validation, and 5% for testing. We’ll put the training data and test data in separate folders. RoboFlow has a procedure for this under the <strong class="bold">Generate</strong> tab, where there is a section labeled <strong class="bold">T<a id="_idTextAnchor143"/>rain/Test Split</strong>.</li>
<li>Now, we must take each image and multiply its training value by combining parts of different images in a mosaic. We’ll take parts of four random different pictures and combine them. This will increase our training set three-fold, a process called <strong class="bold">data augmentation</strong>. This is built into RoboFlow. I started with 36 pictures; after <a id="_idIndexMarker328"/>augmentation, I had 99:</li>
</ol>
<div><div><img alt="Figure 4.10 – Mosaic data augmentation creates more training data from our limited number of pictures" src="img/B19846_04_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Mosaic data augmentation creates more training data from our limited number of pictures</p>
<p>Why are we using this mosaic approach? We still want to have valid bounding boxes. The mosaic process resizes any partial bounding boxes that intersect the edges.</p>
<ol>
<li value="5">Next, we will be building two programs: the training program, which runs on our desktop computer and trains the network, and the working program, which uses the trained network to find toys. The training process may not run on our small computer onboard the robot or may take a long time to run, so we’ll use the desktop computer for this.</li>
<li>Now, we need to train the network. To achieve this, we must do the following:<ol><li>First, we must scale all our images down to reduce the processing time to a reasonable level.</li><li>Then, we must initialize the network with uniform random weights.</li><li>Next, we must encode a labeled image and input that to the network. The neural network only uses the image data to predict what class of object is in the picture, and what its bounding box should be. Since we pre-labeled the image with the correct answer and used the correct bounding box, we can judge whether the answer is right or wrong. If it is right, we can reinforce the weights on the<a id="_idIndexMarker329"/> inputs that contributed to this answer by incrementing them (the training value). If the answer is wrong, we can reduce the weights instead. In neural networks, the error between the desired result and the actual resul<a id="_idTextAnchor144"/>t is called <strong class="bold">loss</strong>. Repeat this process for each image.</li><li>Now, we must test the network by running the testing set of images – which are pictures of the same toys, but that were not in the training set. We must analyze what sort of output we get over this set (how many are wrong and how many are right). If this answer is above 90%, we stop. Otherwise, we go back and run all the training images again.</li><li>Once we are happy with the results – and we should need between 50 and 100 iterations to get there – we must stop and store the weights that we ended up with in the training network. This is our <strong class="bold">trained CNN</strong>.</li></ol></li>
<li>Our next task is to find the toys. To do this, we must <em class="italic">deploy</em> the trained network by loading it and using our video images from the live robot to look for toys. We’ll get a probability of an image containing a toy from 0% to 100%. We’ll scan the input video image in sections and find which sections contain toys. If we are not happy with this network, we can reload this network into the training program and<a id="_idIndexMarker330"/> train it some more.</li>
</ol>
<p>Now, let’s cover this in detail, step by step. We have a bit more theory to cover before we start writing the code.</p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor145"/>Understanding how to train our toy detector</h2>
<p>Our first task is to <a id="_idIndexMarker331"/>prepare a training set. We’ll put the camera <a id="_idIndexMarker332"/>on the robot and drive the robot around using the teleoperation interface (or just by pushing it around by hand), snapping still photos every foot or so. We just need pictures with toys in the image since we will be annotating the toys. We need about 200 pictures – the more, the better. We also need to have a set of pictures in the daytime with natural light, and at night, if your room changes lighting between day and night. This affords us several advantages: we are using the same room and the same camera to find the toys, all under the same lighting conditions.</p>
<p>Now, we need to label the images. We’ll load the images into RoboFlow to create a dataset called <code>toydetector</code>. Use the <strong class="bold">Upload</strong> tab and drag and drop the images or select the folder that c<a id="_idTextAnchor146"/>ontains the images.</p>
<p>The process for us is fairly straightforward. We look at each picture in turn and draw a box around any toy objects. We hit <em class="italic">Enter</em> or the <code>toy</code>. This is going to take a while.</p>
<p>Once we’ve labeled around 160 toys in our images, we can use the <strong class="bold">Generate</strong> button in RoboFlow to create our dataset. We must set up the preprocessing task to resize our images to 640x640 pixels. This makes the best use of our limited computer capacity on the robot. Then, we must augment the dataset to create additional images of our limited set, as mentioned previously. We’ll use the mosaic method to augment our dataset while preserving the bounding boxes. To do this, we must use the <strong class="bold">Generate</strong> tab in RoboFlow, then click <strong class="bold">Add Augmentation Step</strong> to select the type of operation that will affect our images. Then, we must add the mosiac augmentation to create more images out of our training set. Now, we can hit the <strong class="bold">Generate</strong> button to create our dataset.</p>
<p>We started with 48 images that I took (back in step 1); after augmentation, we have 114. We’ll set our test/train split so that it contains 99 images in the training set, nine images in the validation set, and six images in the test set (87% training, 8% validation, and 5% testing). This makes the best use of our limited dataset.</p>
<p>To download our datasets from RoboFlow, we must install RoboFlow’s interface on our computer. It’s a Python package:</p>
<pre class="console">
pip install roboflow</pre> <p>Then, we must create a short Python program called <code>downloadDataset.py</code>. When you build your dataset, RoboFlow will provide a unique <code>api_key</code> value; this will be the password for your<a id="_idIndexMarker333"/> account that authorizes access. This goes into<a id="_idIndexMarker334"/> the program as follows, where I put the asterisks:</p>
<pre class="source-code">
from roboflow import Roboflow
rf = Roboflow(api_key="*****************")
project = rf.workspace("toys").project("toydetector")
dataset = project.version(1).download("yolov8")</pre> <p>In the next section, we will retrain the network with this command:</p>
<pre class="console">
yolo task=detect mode=train model=yolov8n.pt data=datasets/data.yaml epochs=100 imgsz=640</pre> <p>Once we’ve done this, the program will produce a lot of output, as follows:</p>
<pre class="source-code">
(p310) E:\BOOK\YOLO&gt;yolo task=detect mode = val model=runs\detect\train3\weights\best.pt data=ToyDetector-1\data.yaml
Ultralytics YOLOv8.0.78 Python-3.10.10 torch-2.0.0 CUDA:0 (NVIDIA GeForce RTX 2070, 8192MiB)
Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs
..................
AMP: checks passed
optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias
train: Scanning E:\BOOK\YOLO\datasets\ToyDetector-1\train\labels.cache… 99 images, 0 backgrounds, 0 corrupt: 100%|███
val: Scanning E:\BOOK\YOLO\datasets\ToyDetector-1\valid\labels.cache… 9 images, 0 backgrounds, 0 corrupt: 100%|██████
Plotting labels to runs\detect\train5\labels.jpg…
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs\detect\train5
Starting training for 100 epochs…</pre> <p>One of the critical parts of training our model<a id="_idIndexMarker335"/> is the <strong class="bold">training optimizer</strong>. We will <a id="_idIndexMarker336"/>use <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) for this. SGD is<a id="_idIndexMarker337"/> another of those simple concepts with a fancy name. Stochastic just means <em class="italic">random</em>. What we want to do is tweak the weights of our neurons to give a better answer than we got the first time – this is what we are<a id="_idIndexMarker338"/> training, by adjusting the weights. We want to change the weights a small amount – but in which direction? We want to change the weights in the direction that improves the answer – it makes the prediction closer<a id="_idTextAnchor147"/> to what we want it to be.</p>
<p>To understand this better, let’s do a little thought experiment. We have a neuron that we know is producing the wrong answer and needs adjusting. We’ll add a small amount to the weight and see how the answer changes. It gets slightly worse – the number is further away from the correct answer. So, we must subtract a small amount instead – and, as you might think, the answer gets better. We have reduced the amount of error slightly. If we make a graph of the error produced by the neuron, we’ll see that we are moving toward an error of zero, or the graph is descending toward some minimum value. Another way of saying this is that the slope of the line is negative – going toward zero. The amount of the slope can be <a id="_idIndexMarker339"/>called a <strong class="bold">gradient</strong> – just as you would refer to the slope or steepness of a hill as the gradient. We can calculate the partial derivative (in other words, the slope of the error curve near this point), which tells us the slope of the line.</p>
<p>The way we go about adjusting the weights on the network as a whole to minimize the loss between the ground truth and the predicted value is<a id="_idIndexMarker340"/> called <code>Y1</code>, <code>Y2</code>, and <code>Y3</code>. We have three weights – <code>W1</code>, <code>W2</code>, and <code>W3</code>. We’ll have the bias, <code>B</code>, and our activation function, <code>D</code>, which is the ReLU rectifier. The values of our inputs are 0.2, 0.7, and 0.02. The weights are 0.3, 0.2, and 0.5. Our bias is 0.3, and the desired output is 1.0. We<a id="_idIndexMarker342"/> calculate <a id="_idIndexMarker343"/>the sum of the inputs and weights and we get a value of 0.21. After adding our bias, we get 0.51. The ReLU function passes any value greater than zero, so the activated output of this neuron is 0.51. Our desired value is 1.0, which comes from the truth (label) data. So, our error is 0.49. If we add the training rate value to each weight, what happens? Take a look at the following diagram:</p>
<div><div><img alt="Figure 4.11 – How backpropagation works to adjust weights" src="img/B19846_04_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – How backpropagation works to adjust weights</p>
<p>The output value <a id="_idIndexMarker344"/>now goes up to 0.5192. Our error<a id="_idIndexMarker345"/> goes down to 0.4808. We are on the right track! The gradient of our error slope is <em class="italic">(.4808-.49) / 1 = -0.97</em>. The <em class="italic">1</em> is because we just have one training sample so far. So, where does the stochastic part come from? Our recognition network may have 50 million neurons. We can’t be doing all of this math for each one. So, we must take a random sampling of inputs rather than all of them to determine whether our training is positive or negative.</p>
<p>In math terms, the slope of an equation is provided by the derivative of that equation. So, in practice, backpropagation takes the partial derivative of the error between training epochs to determine the slope of the error, and thus determine whether we are training our network correctly. As the slope gets smaller, we reduce our training rate to a smaller number to get closer and closer to the correct answer:</p>
<div><div><img alt="Figure 4.1﻿2 – The gradient descent process" src="img/B19846_04_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1<a id="_idTextAnchor148"/>2 – The gradient descent process</p>
<p>Now, we can to our <a id="_idIndexMarker346"/>next problem: how do we propagate our<a id="_idIndexMarker347"/> weight adjustments up the layers of the neural network? We can determine the error at the output neuron – just the label value minus the output of the network. How do we apply this information to the previous layer? Each neuron’s contribution to the error is proportional to its weight. We must divide the error by the weight of each input, and that value is now the applied error of the next neuron up the chain. Then, we can recompute their weights and so on. This is why neural networks take so much compute power:</p>
<div><div><img alt="Figure 4.13 – Backpropagation error" src="img/B19846_04_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Backpropagation error</p>
<p>We backpropagate the error back up the network from the end back to the beginning. Then, we start <a id="_idIndexMarker348"/>a<a id="_idTextAnchor149"/><a id="_idTextAnchor150"/><a id="_idTextAnchor151"/>ll <a id="_idIndexMarker349"/>over again with the next cycle.</p>
<p>At this point, we can test our toy detector. Let’s see how we can do this.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor152"/>Building the toy detector</h2>
<p>We can use <a id="_idIndexMarker350"/>the<a id="_idIndexMarker351"/> following command to test how we did:</p>
<pre class="console">
yolo task=detect mode=predict model=last.pt source=toy1.jpg imgsz=640</pre> <p>The program produces the following output. We can find our image with the labeled detections in the <code>./runs/detect/predict</code> directory with a number appended depending on how many times we run the detection:</p>
<pre class="console">
Speed: 4.0ms preprocess, 44.7ms inference, 82.6ms postprocess per image at shape (1, 3, 640, 640)
Results saved to runs\detect\predict4</pre> <p>The output of our prediction is shown in the following figure:</p>
<div><div><img alt="Figure 4.14 – The toy detector in action" src="img/B19846_04_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – The toy detector in action</p>
<p>With this, we have <a id="_idIndexMarker352"/>successfully created a toy detector using a <a id="_idIndexMarker353"/>neural network. The output of the detector, which we will use in <a href="B19846_05.xhtml#_idTextAnchor159"><em class="italic">Chapter 5</em></a> to direct the robot and the robot arm to drive to the toy <a id="_idTextAnchor153"/>and then pick it up, looks like this:</p>
<pre class="source-code">
"predictions": [
 {
 "x": 287.5,
 "y": 722.5,
 "width": 207,
 "height": 131,
 "confidence": 0.602,
 "class": "toy"
 },</pre> <p>For each detection, the neural network will provide several bits of information. We get the <code>x</code> and <code>y</code> locations of the center of the bounding box, and then the height and width of that box. Then, we get a confidence number of how certain the network is of the decision that this is a detection. Finally, we get the class of the object (what kind of object), which is, of course, a toy.</p>
<p>When we ran the training process for the neural network, if you look in the <code>training</code> folder found in <code>runs/detect/train</code>, there are a whole series of graphs. What do these graphs tell us?</p>
<p>The first one we need to look at is <code>F1_curve</code>. This is the product of precision and recall. <strong class="bold">Precision</strong> is the <a id="_idIndexMarker354"/>ratio of true positives (correctly classified objects) from all positives. <strong class="bold">Recall</strong> is the <a id="_idIndexMarker355"/>proportion of positive detections that were identified correctly. So, precision is<a id="_idIndexMarker356"/> defined <a id="_idIndexMarker357"/>as follows:</p>
<p>Precision =  TP _ TP + FP </p>
<p>Precision is the true positives divided by true positives and false positives (items that were identified as detections but were not).</p>
<p>Recall is defined slightly differently:</p>
<p>Recall =  TP _ TP + FN </p>
<p>Here, recall is the true positives divided by true positives plus false negatives. A false negative is a missed detection or an object that was not detected when it was, in fact, present.</p>
<p>To create the F1 curve, we must multiply precision and recall together and plot it against <em class="italic">confidence</em>. The graph shows the level of confidence in our detections that produces the best result of trading off precision and recall:</p>
<div><div><img alt="Figure 4.15 – The F1 confidence curve" src="img/B19846_04_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – The F1 confidence curve</p>
<p>In this case, a confidence level of 0.21 gives a detection rate of 0.87. This means that we get the best ratio of true detections to false detections. However, this best ratio – 87% – occurs at 0.21 confidence – a rather low number. Detections at this low confidence level are hard to distinguish <a id="_idIndexMarker358"/>and can be caused by noise in <a id="_idIndexMarker359"/>measurements. It might be more desirable to have our peak at a higher confidence level. I tried several approaches to address this. I ran 200 epochs rather than 100 and moved the peak F1 confidence level to 51%, but the detection level dropped a bit to 85%. Then, I changed the gradient descent technique from SDM<a id="_idIndexMarker360"/> to <strong class="bold">Adam</strong>, an adaptive gradient descent technique that reduces the learning rate as you get closer to our goal. This can be done using the following code:</p>
<pre class="console">
yolo task=detect mode=train model=yolov8n.pt data=datasets/data.yaml epochs=100 optimizer='adamW' imgsz=640</pre> <p>This produced a more satisfactory result of 88% true detections at 49% confidence, which I think will do a better job for our toy detector. In reviewing my detections, there were a few false positives (furniture and other objects being detected as toys), so I think that this version will be our toy detector neural network. Although I used a fairly small dataset, it would not hurt to have more <a id="_idTextAnchor154"/>pictures to work with from different angles. Before wrapping <a id="_idIndexMarker361"/>this chapter up, let’s briefly summarize<a id="_idIndexMarker362"/> what we’ve learned so far.</p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor155"/>Summary</h1>
<p>In this chapter, we dove head-first into the world of ANNs. An ANN can be thought of as a stepwise non-linear approximation function that slowly adjusts itself to fit a curve that matches the desired input to the desired output. The learning process consists of several steps, including preparing data, labeling data, creating the network, initializing the weights, creating the forward pass that provides the output, and calculating the loss (also called the error). We created a special type of ANN, a CNN, to examine images. The network was trained using images with toys, to which we added bounding boxes to tell the network what part of the image was a toy. We trained the network to get an accuracy better than 87% in classifying images with toys in them. Finally, we tested the network to verify its output and tuned our results using the Adam adaptive descent algorithm.</p>
<p>In the next chapter, we will look at machine learning for the robot arm in terms of reinforcement learning and genetic algorithms.</p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor156"/>Questions</h1>
<ol>
<li value="1">We went through a lot in this chapter. You can use the framework provided to investigate the properties of neural networks. Try several activation functions, or different settings for convolutions, to see what changes in the training process.</li>
<li>Draw a diagram of an artificial neuron and label the parts. Look up a natural, human biological neuron and compare them.</li>
<li>Which features of a real neuron and an artificial neuron are the same? Which ones are different?</li>
<li>What effect does the learning rate have on gradient descent? What if the learning rate is too large? Too small?</li>
<li>What relationship does the first layer of a neural network have with the input?</li>
<li>What relationship does the last layer of a neural network have with the output?</li>
<li>Look up three kinds of loss functions and describe how they work. Include mean square loss and the two kinds of cross-entropy loss.</li>
<li>What would you change if your network was trained and reached 40% accuracy of the classification and got s<a id="_idTextAnchor157"/>tuck, or was unable to learn anything further?</li>
</ol>
<h1 id="_idParaDest-77"><a id="_idTextAnchor158"/>Further reading</h1>
<p>For more information on the topics that were covered in this chapter, please refer to the following resources:</p>
<ul>
<li><em class="italic">Python Deep Learning Cookbook</em>, by Indra den Bakker, Packt Publishing, 2017</li>
<li><em class="italic">Artificial Intelligence with Python</em>, by Prateek Joshi, Packt Publishing, 2017</li>
<li><em class="italic">Python Deep Learning</em>, by Valentino Zocca, Gianmario Spacagna, Daniel Slater, and Peter Roelants, Packt Publishing, 2017</li>
<li><em class="italic">PyImageSearch Blog</em>, by Adrian Rosebrock, available at <a href="http://pyimagesearch.com">pyimagesearch.com</a>, 2018</li>
</ul>
</div>
<div><div></div>
</div>
</body></html>