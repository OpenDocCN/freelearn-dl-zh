<html><head></head><body>
<div id="_idContainer062">
<h1 class="chapter-number" id="_idParaDest-63"><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-64"><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.2.1">Recognizing Objects Using Neural Networks and Supervised Learning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This is the chapter where we’ll start</span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.4.1"> to combine </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">robotics</span></strong><span class="koboSpan" id="kobo.6.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.9.1">AI</span></strong><span class="koboSpan" id="kobo.10.1">) to </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.11.1">accomplish some of the tasks we laid out so carefully in previous chapters. </span><span class="koboSpan" id="kobo.11.2">The subject of this chapter is </span><strong class="bold"><span class="koboSpan" id="kobo.12.1">object recognition</span></strong><span class="koboSpan" id="kobo.13.1"> – we </span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.14.1">will be teaching the robot to recognize what a toy is so that it can then decide what to pick up and what to leave alone. </span><span class="koboSpan" id="kobo.14.2">We will be using </span><strong class="bold"><span class="koboSpan" id="kobo.15.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.17.1">CNNs</span></strong><span class="koboSpan" id="kobo.18.1">) as machine learning tools for separating </span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.19.1">objects in images, recognizing them, and locating them in the camera frame so that the robot can then locate them. </span><span class="koboSpan" id="kobo.19.2">More specifically, we’ll be using images to recognize objects. </span><span class="koboSpan" id="kobo.19.3">We’ll be taking a picture and then looking to see whether the computer recognizes specific types of objects in those pictures. </span><span class="koboSpan" id="kobo.19.4">We won’t be recognizing objects themselves, but rather images or pictures of objects. </span><span class="koboSpan" id="kobo.19.5">We’ll also be putting bounding boxes around objects, separating them from other objects and </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">background pixels.</span></span></p>
<p><span class="koboSpan" id="kobo.21.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.23.1">A brief overview of </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">image processing</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">Understanding our object </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">recognition task</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.27.1">Image manipulation</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Using YOLOv8 – an object </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">recognition model</span></span></li>
</ul>
<h1 id="_idParaDest-65"><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.30.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.31.1">You will be able to accomplish all of this chapter’s tasks without a robot if yours cannot walk yet. </span><span class="koboSpan" id="kobo.31.2">We will, however, get better results if the camera is in the proper position on the robot. </span><span class="koboSpan" id="kobo.31.3">If you don’t have a robot, you can still do all of these tasks with a laptop and a USB camera. </span></p>
<p><span class="koboSpan" id="kobo.32.1">Overall, here’s the hardware and software that you will need to complete the tasks in </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">this chapter:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.34.1">Hardware:</span></span><ul><li><span class="koboSpan" id="kobo.35.1">A </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">laptop computer</span></span></li><li><span class="koboSpan" id="kobo.37.1">Nvidia </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">Jetson Nano</span></span></li><li><span class="No-Break"><span class="koboSpan" id="kobo.39.1">USB camera</span></span></li></ul></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.40.1">Software:</span></span><ul><li><span class="No-Break"><span class="koboSpan" id="kobo.41.1">Python 3</span></span></li><li><span class="No-Break"><span class="koboSpan" id="kobo.42.1">OpenCV2</span></span></li><li><span class="No-Break"><span class="koboSpan" id="kobo.43.1">TensorFlow</span></span></li><li><span class="koboSpan" id="kobo.44.1">YOLOv8, which is available </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">at </span></span><a href="https://github.com/ultralytics/ultralytics"><span class="No-Break"><span class="koboSpan" id="kobo.46.1">https://github.com/ultralytics/ultralytics</span></span></a></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.47.1">The source code for this chapter can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">at </span></span><a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e"><span class="No-Break"><span class="koboSpan" id="kobo.49.1">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.50.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">In the next section, we will discuss what image </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">processing is.</span></span></p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.53.1">A brief overview of image processing</span></h1>
<p><span class="koboSpan" id="kobo.54.1">Most of you will </span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.55.1">be very familiar with computer images, formats, pixel depths, and maybe even convolutions. </span><span class="koboSpan" id="kobo.55.2">We will be discussing these concepts in the following sections; if you already know this, skip ahead. </span><span class="koboSpan" id="kobo.55.3">If this is new territory, read carefully, because everything we’ll do after is based on </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">this information.</span></span></p>
<p><span class="koboSpan" id="kobo.57.1">Images are stored in a computer as a two-dimensional</span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.58.1"> array of </span><strong class="bold"><span class="koboSpan" id="kobo.59.1">pixels</span></strong><span class="koboSpan" id="kobo.60.1"> or picture elements. </span><span class="koboSpan" id="kobo.60.2">Each pixel is a tiny dot. </span><span class="koboSpan" id="kobo.60.3">Thousands or millions of tiny dots make up each image. </span><span class="koboSpan" id="kobo.60.4">Each pixel is a number or series of numbers that describe its color. </span><span class="koboSpan" id="kobo.60.5">If the image is only a grayscale or black-and-white image, then each pixel is represented by a single number that corresponds to how dark or light the tiny dot is. </span><span class="koboSpan" id="kobo.60.6">This is straightforward </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">so far.</span></span></p>
<p><span class="koboSpan" id="kobo.62.1">If the image is a color picture, then each dot has three numbers that are combined to make its color. </span><span class="koboSpan" id="kobo.62.2">Usually, these numbers are the intensity of </span><strong class="bold"><span class="koboSpan" id="kobo.63.1">Red, Green, and Blue</span></strong><span class="koboSpan" id="kobo.64.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.65.1">RGB</span></strong><span class="koboSpan" id="kobo.66.1">) colors. </span><span class="koboSpan" id="kobo.66.2">The </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.67.1">combination (0,0,0) represents black (or the absence of all colors), while (255,255,255) is white (the sum of all colors). </span><span class="koboSpan" id="kobo.67.2">This process is called the additive color model. </span><span class="koboSpan" id="kobo.67.3">If you work with watercolors instead of computer pixels, you’ll know that adding all the colors in your watercolor box makes black – that is a subtractive color model. </span><span class="koboSpan" id="kobo.67.4">Red, green, and blue are primary colors that can be used to make all of the other colors. </span><span class="koboSpan" id="kobo.67.5">Since an RGB pixel is represented by three colors, the actual image is a three-dimensional array rather than a two-dimensional one since each pixel has three numbers, making an array of (height, width, 3). </span><span class="koboSpan" id="kobo.67.6">So, a picture that is 800 x 600 pixels would be represented by an array of dimensions given by (800,600,3), or 1,440,000. </span><span class="koboSpan" id="kobo.67.7">That is a lot of numbers. </span><span class="koboSpan" id="kobo.67.8">We will be working very hard to minimize the number of pixels we are processing at any </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">given time.</span></span></p>
<p><span class="koboSpan" id="kobo.69.1">While RGB is one </span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.70.1">set of three numbers that can describe a pixel, there are other ways of describing </span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.71.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.72.1">color formula</span></strong><span class="koboSpan" id="kobo.73.1"> that have various usages. </span><span class="koboSpan" id="kobo.73.2">We don’t have to use RGB – for instance, we can also use </span><strong class="bold"><span class="koboSpan" id="kobo.74.1">Cyan, Yellow, and Magenta</span></strong><span class="koboSpan" id="kobo.75.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.76.1">CYM</span></strong><span class="koboSpan" id="kobo.77.1">), which </span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.78.1">are the complementary colors to RGB, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.79.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.80.1">.2</span></em><span class="koboSpan" id="kobo.81.1">. </span><span class="koboSpan" id="kobo.81.2">We can also break down colors using</span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.82.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.83.1">Hue, Saturation, and Value</span></strong><span class="koboSpan" id="kobo.84.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.85.1">HSV</span></strong><span class="koboSpan" id="kobo.86.1">) model, which classifies color by hue (shade of color), saturation (intensity of color), and value (brightness of color). </span><span class="koboSpan" id="kobo.86.2">HSV is a very useful color space for certain calculations, such as converting a color image into grayscale (black and white). </span><span class="koboSpan" id="kobo.86.3">To turn RGB into a grayscale pixel, you have to do a bit of math – you can’t just pull out one channel and keep it. </span><span class="koboSpan" id="kobo.86.4">The formula for RGB to grayscale, as defined by the </span><strong class="bold"><span class="koboSpan" id="kobo.87.1">National Television System Committee</span></strong><span class="koboSpan" id="kobo.88.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.89.1">NTSC</span></strong><span class="koboSpan" id="kobo.90.1">), is</span><a id="_idIndexMarker291"/> <span class="No-Break"><span class="koboSpan" id="kobo.91.1">as follows:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.92.1">0.299*Red + 0.587*Green + </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.93.1">0.114*Blue</span></em></span></p>
<p><span class="koboSpan" id="kobo.94.1">This is because the different wavelengths of light behave differently in our eyes, which are more sensitive to green. </span><span class="koboSpan" id="kobo.94.2">If you have color in the HSV color model, then creating a grayscale image involves considering </span><em class="italic"><span class="koboSpan" id="kobo.95.1">V</span></em><span class="koboSpan" id="kobo.96.1"> (value) and throwing the </span><em class="italic"><span class="koboSpan" id="kobo.97.1">H</span></em><span class="koboSpan" id="kobo.98.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.99.1">S</span></em><span class="koboSpan" id="kobo.100.1"> values away. </span><span class="koboSpan" id="kobo.100.2">As you can imagine, this is a lot simpler. </span><span class="koboSpan" id="kobo.100.3">This is important to understand as we will be doing quite a bit of image manipulation throughout this chapter. </span><span class="koboSpan" id="kobo.100.4">But first, in the following section, we’ll discuss</span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.101.1"> the image recognition task we will be performing in </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">this chapter.</span></span></p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.103.1">Understanding our object recognition task</span></h1>
<p><span class="koboSpan" id="kobo.104.1">Having a computer</span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.105.1"> or robot recognize an image of a toy is not as simple as taking two pictures and then saying </span><strong class="source-inline"><span class="koboSpan" id="kobo.106.1">if picture A = picture B, then toy</span></strong><span class="koboSpan" id="kobo.107.1">. </span><span class="koboSpan" id="kobo.107.2">We are going to have to do quite a bit of work to be able to recognize a variety of objects that are randomly rotated, strewn about, and at various distances. </span><span class="koboSpan" id="kobo.107.3">We could write a program to recognize simple shapes – hexagons, for instance, or simple color blobs – but nothing as complex as a toy stuffed dog. </span><span class="koboSpan" id="kobo.107.4">Writing a program that did some sort of analysis of an image and computed the pixels, colors, distributions, and ranges of every possible permutation would be extremely difficult, and the result would be very fragile – it would fail at the slightest change in lighting </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">or color.</span></span></p>
<p><span class="koboSpan" id="kobo.109.1">Speaking from experience, I had a recent misadventure with a large robot that used a traditional computer vision system to find its battery charger station. </span><span class="koboSpan" id="kobo.109.2">That robot mistook an old, faded soft drink machine for its charger – let’s just say that I had to go b</span><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.110.1">uy </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">more fuses.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">What we will do instead is teach the robot to recognize a set of images corresponding to toys that we will take from various angles. </span><span class="koboSpan" id="kobo.112.2">We will do this by using a special type of </span><strong class="bold"><span class="koboSpan" id="kobo.113.1">artificial neural network</span></strong><span class="koboSpan" id="kobo.114.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.115.1">ANN</span></strong><span class="koboSpan" id="kobo.116.1">) that performs convolution operations on images. </span><span class="koboSpan" id="kobo.116.2">It is classified as</span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.117.1"> an AI technique because instead of programming our software to recognize objects by writing code, we will be training a neural network to correctly </span><em class="italic"><span class="koboSpan" id="kobo.118.1">segment</span></em><span class="koboSpan" id="kobo.119.1"> (separate from the rest of the image) and </span><em class="italic"><span class="koboSpan" id="kobo.120.1">label</span></em><span class="koboSpan" id="kobo.121.1"> (classify) groups of pixels in an image by how closely they resemble groups of labeled pixels that the network was trained on. </span><span class="koboSpan" id="kobo.121.2">Rather than the code determining the robot’s behavior, it is the data we train the network on that does the work. </span><span class="koboSpan" id="kobo.121.3">Since we (the humans) will be training the neural network by providing segmented and labeled images, this is </span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.122.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.123.1">supervised learning</span></strong><span class="koboSpan" id="kobo.124.1">. </span><span class="koboSpan" id="kobo.124.2">This involves telling the network what we want it to learn and reinforcing (rewarding) the network based on how well it performs. </span><span class="koboSpan" id="kobo.124.3">We’ll </span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.125.1">discuss </span><strong class="bold"><span class="koboSpan" id="kobo.126.1">unsupervised learning</span></strong><span class="koboSpan" id="kobo.127.1"> in </span><a href="B19846_08.xhtml#_idTextAnchor235"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.128.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.129.1">. </span><span class="koboSpan" id="kobo.129.2">This process entails us not telling the software exactly what to learn, which means it must determine that </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">for itself.</span></span></p>
<p><span class="koboSpan" id="kobo.131.1">To clarify, in this section, we will tell the ANN what we want it to learn, which in this case is to recognize a class of objects we will call </span><em class="italic"><span class="koboSpan" id="kobo.132.1">toys</span></em><span class="koboSpan" id="kobo.133.1">, and to draw a bounding box around those objects. </span><span class="koboSpan" id="kobo.133.2">This bounding box will tell other parts of the robot that a toy is visible and where it is in </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">the image.</span></span></p>
<p><span class="koboSpan" id="kobo.135.1">I’ll be emphasizing the unique components we will use to accomplish our task of recognizing toys in an</span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.136.1"> image. </span><span class="koboSpan" id="kobo.136.2">Do you remember what the storyboard from </span><a href="B19846_03.xhtml#_idTextAnchor043"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.137.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.138.1"> told us </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">to do?</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<span class="koboSpan" id="kobo.140.1"><img alt="Figure 4.1 – Use case for identifying objects as toys" src="image/B19846_04_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.141.1">Figure 4.1 – Use case for identifying objects as toys</span></p>
<p><span class="koboSpan" id="kobo.142.1">Our image recognizer must figure out what objects are toys and then locate them in the image. </span><span class="koboSpan" id="kobo.142.2">This is illustrated </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.143.1">in the preceding sketch; objects marked as toys have circles around them. </span><span class="koboSpan" id="kobo.143.2">The image recognizer must recognize not just </span><em class="italic"><span class="koboSpan" id="kobo.144.1">what</span></em><span class="koboSpan" id="kobo.145.1"> they are, but also </span><em class="italic"><span class="koboSpan" id="kobo.146.1">where</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.147.1">they are.</span></span></p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.148.1">Image manipulation</span></h1>
<p><span class="koboSpan" id="kobo.149.1">So, now that we</span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.150.1"> have an image, what can we do with it? </span><span class="koboSpan" id="kobo.150.2">You have probably played with Adobe Photoshop or some other image manipulation program such as GIMP, and you know that there are hundreds of operations, filters, changes, and tricks you can perform on images. </span><span class="koboSpan" id="kobo.150.3">For instance, can make an image brighter or darker by adjusting the brightness. </span><span class="koboSpan" id="kobo.150.4">We can increase the contrast between the white parts of the image and the dark parts. </span><span class="koboSpan" id="kobo.150.5">We can make an image blurry, usually by applying a Gaussian blur filter. </span><span class="koboSpan" id="kobo.150.6">We can also make an image sharper (somewhat) by using a filter such as an unsharp mask. </span><span class="koboSpan" id="kobo.150.7">You can also use an edge detector filter, such as the Canny filter, to isolate the edges of an image, where color or value changes. </span><span class="koboSpan" id="kobo.150.8">We will be using all of these techniques to help the computer </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">identify images:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.152.1"><img alt="Figure 4.2 – Various convolutions applied to an image" src="image/B19846_04_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.153.1">Figure 4.2 – Various convolutions applied to an image</span></p>
<p><span class="koboSpan" id="kobo.154.1">By performing</span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.155.1"> these manipulations, we want the computer to not have the computer software be sensitive to the size of the image, which is called </span><strong class="bold"><span class="koboSpan" id="kobo.156.1">scale invariance</span></strong><span class="koboSpan" id="kobo.157.1">, the angle at which the photograph was taken, or </span><strong class="bold"><span class="koboSpan" id="kobo.158.1">angle invariance</span></strong><span class="koboSpan" id="kobo.159.1">, and the lighting available, which is known as </span><strong class="bold"><span class="koboSpan" id="kobo.160.1">illumination invariance</span></strong><span class="koboSpan" id="kobo.161.1">. </span><span class="koboSpan" id="kobo.161.2">This is all very desirable in a computer vision system – we would not want an AI system that only recognizes our toys from the same angle and distance as the original image. </span><span class="koboSpan" id="kobo.161.3">Remember, we are going to train our vision system to recognize toys based on a labeled set of training images we take in advance, and the robot will have to recognize objects based on what it learned from the training set. </span><span class="koboSpan" id="kobo.161.4">Here, we are going to use features from the image that mostly don’t change based on size, angle, distance, or lighting. </span><span class="koboSpan" id="kobo.161.5">What sorts of features might </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">these be?</span></span></p>
<p><span class="koboSpan" id="kobo.163.1">If we look at a common household object, such as a chair, and inspect it from several angles, what about the chair does not change? </span><span class="koboSpan" id="kobo.163.2">The easy answer is the edges and corners. </span><span class="koboSpan" id="kobo.163.3">The chair has the same number of corners all the time, and we can see a consistent number of them from most angles. </span><span class="koboSpan" id="kobo.163.4">It also has a consistent number </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">of edges.</span></span></p>
<p><span class="koboSpan" id="kobo.165.1">Admittedly, that is a bit of an oversimplification of the approach. </span><span class="koboSpan" id="kobo.165.2">We will be training our ANN on a whole host of image features that may or may not be unique to this object and let it decide which work and which do not. </span><span class="koboSpan" id="kobo.165.3">We will accomplish this by using a generic approach</span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.166.1"> to image manipulation </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.168.1">convolution</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">.</span></span></p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.170.1">Convolution</span></h2>
<p><span class="koboSpan" id="kobo.171.1">Every once in a while, you’ll </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.172.1">come across some mathematical construction that turns a complex task into just a bunch of adding, subtracting, multiplying, and dividing. </span><span class="koboSpan" id="kobo.172.2">Vectors in geometry work like that, and, in image processing, we</span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.173.1"> have the </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">convolution kernel</span></strong><span class="koboSpan" id="kobo.175.1">. </span><span class="koboSpan" id="kobo.175.2">It transpires that most of the common image processing techniques – edge detection, corner detection, blurring, sharpening, enhancing, and so on – can be accomplished with a simple </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">array construct.</span></span></p>
<p><span class="koboSpan" id="kobo.177.1">It is pretty easy to understand that in an image, the neighbors of a pixel are just as important to what a pixel is as the pixel itself. </span><span class="koboSpan" id="kobo.177.2">If you were going to try and find all the edge pixels of a box, you would look for a pixel that has one type of color on one side, and another type on the other. </span><span class="koboSpan" id="kobo.177.3">We need a function to find edges by comparing pixels on one side of a pixel to </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">the other.</span></span></p>
<p><span class="koboSpan" id="kobo.179.1">The convolution kernel is a matrix function that applies weights to the pixel neighbors – or pixels around the one pixel we are analyzing. </span><span class="koboSpan" id="kobo.179.2">The function is usually written like this, as a </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">3x3 matrix:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.181.1">-1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.182.1">0</span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.183.1">1</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.184.1">-2</span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.185.1">0</span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.186.1">2</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.187.1">-1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.188.1">0</span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.189.1">1</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.190.1">Table 4.1 – A sample convolution kernel</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.191.1">Sobel edge detection</span></strong><span class="koboSpan" id="kobo.192.1"> is represented in the </span><em class="italic"><span class="koboSpan" id="kobo.193.1">Y</span></em><span class="koboSpan" id="kobo.194.1"> direction. </span><span class="koboSpan" id="kobo.194.2">This detects edges going up and down. </span><span class="koboSpan" id="kobo.194.3">Each </span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.195.1">block represents a pixel. </span><span class="koboSpan" id="kobo.195.2">The pixel being processed is in the center. </span><span class="koboSpan" id="kobo.195.3">The neighbors of the pixels on each side are the other blocks – top, bottom, left, and right. </span><span class="koboSpan" id="kobo.195.4">To compute the convolution, the corresponding weight is applied to the value of each pixel by multiplying the value (intensity) of that pixel, and then adding all of the results. </span><span class="koboSpan" id="kobo.195.5">If this image is in color – RGB – then we compute the convolution for each color separately and then</span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.196.1"> combine the results. </span><span class="koboSpan" id="kobo.196.2">Here is an example of a convolution being applied to </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">an image:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.198.1"><img alt="Figure 4.3 – Result of a Sobel edge detection convolutio﻿n" src="image/B19846_04_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.199.1">Figure 4.3 – Result of a Sobel edge detection convolutio</span><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.200.1">n</span></p>
<p><span class="koboSpan" id="kobo.201.1">The resulting image is the same size as the original. </span><span class="koboSpan" id="kobo.201.2">Note that we only get the edge as the result – if the colors are the same on either side of the center pixel, they cancel each other out and we get zero, or black. </span><span class="koboSpan" id="kobo.201.3">If they are different, we get 255, or white, as the answer. </span><span class="koboSpan" id="kobo.201.4">If we need a more complex result, we may also use a 5x5 convolution, which takes into account the two nearest pixels on each side, instead of </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">just one.</span></span></p>
<p><span class="koboSpan" id="kobo.203.1">The good news is that you don’t have to choose which convolution operation to apply to the input images – we will build a software frontend that will set up all of the convolutions. </span><span class="koboSpan" id="kobo.203.2">This </span><em class="italic"><span class="koboSpan" id="kobo.204.1">frontend</span></em><span class="koboSpan" id="kobo.205.1"> is just the part of the program that sets up the networks before we start training them. </span><span class="koboSpan" id="kobo.205.2">The neural network package we’ll be using will determine which convolutions provide the most data and support the training output </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">we want.</span></span></p>
<p><span class="koboSpan" id="kobo.207.1">"But wait," I hear you say. </span><span class="koboSpan" id="kobo.207.2">"What if the pixel is on the edge of the image and we don’t have neighbor pixels on one side?" </span><span class="koboSpan" id="kobo.207.3">In that case, we have to add padding to the image – which is a border of extra </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.208.1">pixels that permits us to consider the edge pixels </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.210.1">In the next section, we’ll get into the guts of a </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">neural network.</span></span></p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.212.1">Artificial neurons</span></h2>
<p><span class="koboSpan" id="kobo.213.1">What is a </span><strong class="bold"><span class="koboSpan" id="kobo.214.1">neuron</span></strong><span class="koboSpan" id="kobo.215.1">? </span><span class="koboSpan" id="kobo.215.2">And </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.216.1">how do we make a </span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.217.1">network </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.218.1">out of them? </span><span class="koboSpan" id="kobo.218.2">If you can remember what you learned in biology, a biological or natural neuron has inputs, or dendrites, that connect it to other neurons or sensor inputs. </span><span class="koboSpan" id="kobo.218.3">All the inputs come to a central body and then leave via the axion, or connection, to other neurons via other dendrites. </span><span class="koboSpan" id="kobo.218.4">The connection between neurons is </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.219.1">called a </span><strong class="bold"><span class="koboSpan" id="kobo.220.1">synapse</span></strong><span class="koboSpan" id="kobo.221.1">, which is a tiny gap that the signal from the nerve must jump. </span><span class="koboSpan" id="kobo.221.2">A neuron takes inputs, processes them, and activates or sends an output after some threshold level is reached. </span><span class="koboSpan" id="kobo.221.3">An </span><strong class="bold"><span class="koboSpan" id="kobo.222.1">artificial neuron</span></strong><span class="koboSpan" id="kobo.223.1"> is a software construction that approximates the workings of the neurons in your brain and is a very simplified version of the natural neuron. </span><span class="koboSpan" id="kobo.223.2">It has several inputs, a set of weights, a bias, an activation function, and then some outputs to other neurons as a result of the network, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.225.1"><img alt="Figure 4.4 – Diagram of an artificial neuron" src="image/B19846_04_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.226.1">Figure 4.4 – Diagram of an artificial neuron</span></p>
<p><span class="koboSpan" id="kobo.227.1">Let’s</span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.228.1"> describe </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.229.1">each component </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">in detail:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.231.1">Input</span></strong><span class="koboSpan" id="kobo.232.1">: This is a number or value that’s received from other neurons or as an input to the network. </span><span class="koboSpan" id="kobo.232.2">In our image processing example, these are pixels. </span><span class="koboSpan" id="kobo.232.3">This number can be a float or an integer – but it must be just </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">one n</span><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.234.1">umber.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.235.1">Weight</span></strong><span class="koboSpan" id="kobo.236.1">: This is the adjustable value we change to </span><em class="italic"><span class="koboSpan" id="kobo.237.1">train</span></em><span class="koboSpan" id="kobo.238.1"> the neuron. </span><span class="koboSpan" id="kobo.238.2">Increasing the weight means that the input is more important to our answer, and likewise decreasing the weight means the input is used less. </span><span class="koboSpan" id="kobo.238.3">To determine the value of a neuron, we must combine the values of all the inputs. </span><span class="koboSpan" id="kobo.238.4">As the neural network is trained, the weights are adjusted on each input, which favors some inputs over others. </span><span class="koboSpan" id="kobo.238.5">We multiply the input by the weight and then sum all of the </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">results together.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.240.1">Bias</span></strong><span class="koboSpan" id="kobo.241.1">: This is a number that’s added to the sum of the weights. </span><span class="koboSpan" id="kobo.241.2">Bias prevents the neuron from getting stuck at zero and improves training. </span><span class="koboSpan" id="kobo.241.3">This is usually a small number. </span><span class="koboSpan" id="kobo.241.4">Imagine a scenario where all of the inputs to a neuron are zero; in this case, the weights would have no effect. </span><span class="koboSpan" id="kobo.241.5">Adding a small bias allows the neuron to still have an output, and the network can use that to affect learning. </span><span class="koboSpan" id="kobo.241.6">Without the bias, a neuron with zeros on its inputs can’t be trained (changing the weights has no effect) and </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">is </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.243.1">stuck</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.245.1">Activation function</span></strong><span class="koboSpan" id="kobo.246.1">: This determines the output of the neuron based on the weighted sum of its inputs. </span><span class="koboSpan" id="kobo.246.2">The most common types are the </span><strong class="bold"><span class="koboSpan" id="kobo.247.1">Rectified Linear Unit</span></strong><span class="koboSpan" id="kobo.248.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.249.1">ReLU</span></strong><span class="koboSpan" id="kobo.250.1">) – if the </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.251.1">value of the neuron is less than zero, the output is zero; otherwise, the output is the input value – and the </span><strong class="bold"><span class="koboSpan" id="kobo.252.1">sigmoid</span></strong><span class="koboSpan" id="kobo.253.1"> (S-shaped) function, which</span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.254.1"> is a log function. </span><span class="koboSpan" id="kobo.254.2">The activation function propagates information across the network and</span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.255.1"> introduces non-linearity to the output of the neuron, which allows the neural</span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.256.1"> network to approximate </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">non-linear functions:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.258.1"><img alt="Figure 4.5 – Common activation ﻿functions" src="image/B19846_04_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.259.1">Figure 4.5 – Common activation </span><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.260.1">functions</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.261.1">Outputs</span></strong><span class="koboSpan" id="kobo.262.1">: Each layer in the sequential neural network is connected to the next layer. </span><span class="koboSpan" id="kobo.262.2">Some layers are fully connected – with each neuron in the first layer connected to each neuron in the second layer. </span><span class="koboSpan" id="kobo.262.3">Others are sparsely connected. </span><span class="koboSpan" id="kobo.262.4">There is a common process in neural network training</span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.263.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.264.1">dropout</span></strong><span class="koboSpan" id="kobo.265.1">, where we randomly remove connections. </span><span class="koboSpan" id="kobo.265.2">This forces the network to have multiple paths for each bit of information it learns, which strengthens the network and makes it able to handle more </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">diverse inputs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.267.1">Max pooling of outputs</span></strong><span class="koboSpan" id="kobo.268.1">: We use a special type of network layer (compared to a fully connected or sparse layer) called max pooling, where groups of neurons corresponding to regions in our image – say a 2x2 block of pixels – go to one neuron in the next level. </span><span class="koboSpan" id="kobo.268.2">The max pool neuron only takes the largest value from each of the four input neurons. </span><span class="koboSpan" id="kobo.268.3">This has the effect of downsampling the image (making it smaller). </span><span class="koboSpan" id="kobo.268.4">This allows the network to associate small features (such as the </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.269.1">wheels</span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.270.1"> in a Hot Wheels car) with larger features, such as the hood or windshield, to identify a </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">toy car:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.272.1"><img alt="Figure 4.6 – Max pooling operation" src="image/B19846_04_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.273.1">Figure 4.6 – Max pooling operation</span></p>
<p><span class="koboSpan" id="kobo.274.1">Now that you </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.275.1">understand what a neural network is</span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.276.1"> composed of, let’s explore how to train </span><a id="_idTextAnchor138"/><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.277.1">and </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">test one.</span></span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.279.1">Training a CNN</span></h2>
<p><span class="koboSpan" id="kobo.280.1">I want to</span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.281.1"> provide you with an end-to-end look at what we</span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.282.1"> will be doing in the code for the rest of this chapter. </span><span class="koboSpan" id="kobo.282.2">Remember that we are building a CNN that examines pixels in a video frame and outputs if one or more pixel areas that resemble toys are in the image, and where they are. </span><span class="koboSpan" id="kobo.282.3">The following diagram shows the process that we will go through to train the neural network, step </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">by step:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.284.1"><img alt="Figure 4.7 – CNN process" src="image/B19846_04_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.285.1">Figure 4.7 – CNN process</span></p>
<p><span class="koboSpan" id="kobo.286.1">For this task, I decided to use an already existing neural network rather than build one from scratch. </span><span class="koboSpan" id="kobo.286.2">There are a lot of good CNN object detectors available, and honestly, it’s hard to improve on an existing model structure. </span><span class="koboSpan" id="kobo.286.3">The one I’ve picked for this book is called </span><strong class="bold"><span class="koboSpan" id="kobo.287.1">YOLOv8</span></strong><span class="koboSpan" id="kobo.288.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.289.1">YOLO</span></em><span class="koboSpan" id="kobo.290.1"> stands for </span><em class="italic"><span class="koboSpan" id="kobo.291.1">You Only Look Once</span></em><span class="koboSpan" id="kobo.292.1">. </span><span class="koboSpan" id="kobo.292.2">Let’s understand how we </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.293.1">can use this model for </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">our task.</span></span></p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.295.1">Using YOLOv8 – an object recognition model</span></h1>
<p><span class="koboSpan" id="kobo.296.1">Before we dive into </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.297.1">the details of the YOLOv8 model, let’s talk about why I selected it. </span><span class="koboSpan" id="kobo.297.2">First of all, the learning process is pretty much the same for any CNN we might use. </span><span class="koboSpan" id="kobo.297.3">YOLO is a strong open source object detection model with a lot of development behind it. </span><span class="koboSpan" id="kobo.297.4">It’s considered state of the art, and it already does what we need – it detects objects and shows us where they are in images by drawing bounding boxes around them. </span><span class="koboSpan" id="kobo.297.5">So, it tells us what objects are, and where they are located. </span><span class="koboSpan" id="kobo.297.6">As you will see, it is very easy to use and can be extended to detect other classes of objects other than what it was originally trained for. </span><span class="koboSpan" id="kobo.297.7">There are a lot of YOLO users out there who can provide a lot of support and a good basis for learning about AI object recognition </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">for robotics.</span></span></p>
<p><span class="koboSpan" id="kobo.299.1">As I mentioned at the beginning of this chapter, we have two tasks we need to accomplish to reach our goal of picking up toys with a robot. </span><span class="koboSpan" id="kobo.299.2">First, we must determine whether the robot can detect a toy with its camera (determine whether there is a toy in the camera image) and then figure out where it is in that image so that we can drive over to it and pick it up. </span><span class="koboSpan" id="kobo.299.3">In this chapter, we’ll learn how to detect toys, while in </span><a href="B19846_07.xhtml#_idTextAnchor221"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.300.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.301.1">, we’ll discuss how we determine distance and navigate to </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">the toy.</span></span></p>
<p><span class="koboSpan" id="kobo.303.1">YOLOv8 does both tasks in one pass, hence its name. </span><span class="koboSpan" id="kobo.303.2">Other kinds of object recognition models, such as the one I created in the first edition of this book, identified and located objects in images in two steps. </span><span class="koboSpan" id="kobo.303.3">First, it found that an object was present, and then it figured out where in the image it was located in a separate pass. </span><span class="koboSpan" id="kobo.303.4">This separate pass would use a sliding window approach, taking a segment of the image and using the detection part of the neural network to say </span><strong class="source-inline"><span class="koboSpan" id="kobo.304.1">yes</span></strong><span class="koboSpan" id="kobo.305.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">no</span></strong><span class="koboSpan" id="kobo.307.1"> if that segment contained an object it recognized. </span><span class="koboSpan" id="kobo.307.2">Then, it would slide the window it was considering across the image and test again. </span><span class="koboSpan" id="kobo.307.3">This was repeated until we had a bunch of image segments that contained the detected object. </span><span class="koboSpan" id="kobo.307.4">Then, a process called </span><em class="italic"><span class="koboSpan" id="kobo.308.1">minmax</span></em><span class="koboSpan" id="kobo.309.1"> would select the smallest box (min) that contained all the visible parts of the </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">object (max).</span></span></p>
<p><span class="koboSpan" id="kobo.311.1">YOLOv8 takes a different approach by combining two neural networks – one that detects objects it has been taught to recognize and another that is trained to draw bounding boxes based on the center of the object. </span><span class="koboSpan" id="kobo.311.2">The direct output of YOLOv8 includes both the detection and the bounding box for the object. </span><span class="koboSpan" id="kobo.311.3">YOLOv8 can also </span><em class="italic"><span class="koboSpan" id="kobo.312.1">segment</span></em><span class="koboSpan" id="kobo.313.1"> images by pixels, identifying not just a box with the object, but all the pixels that belong to that object. </span><span class="koboSpan" id="kobo.313.2">We’ll be using a bounding box to help us drive the robot to </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">our toys.</span></span></p>
<p><span class="koboSpan" id="kobo.315.1">YOLOv8 comes pretrained on a whole series of object classes (about 80), but we can check whether it already works with the toys we want to detect. </span><span class="koboSpan" id="kobo.315.2">Let’s test YOLOv8’s ability to detect our toys. </span><span class="koboSpan" id="kobo.315.3">We can install YOLOv8 using this simple command on </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">our PC:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.317.1">
pip install ultralytics</span></pre> <p><span class="koboSpan" id="kobo.318.1">Now, to test our detection with a picture of toys in the playroom, we will use the smallest (in terms of model size) of the YOLOv8 detection models – the </span><strong class="bold"><span class="koboSpan" id="kobo.319.1">nano-sized model</span></strong><span class="koboSpan" id="kobo.320.1"> (which is called </span><strong class="source-inline"><span class="koboSpan" id="kobo.321.1">yolov8n.pt</span></strong><span class="koboSpan" id="kobo.322.1">). </span><span class="koboSpan" id="kobo.322.2">This is the pretrained neural network that Ultralytics provides </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">with YOLOv8:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.324.1">
yolo task=detect mode=predict model=yolov8n.pt source="test.png"</span></pre> <p><span class="koboSpan" id="kobo.325.1">As shown in the following</span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.326.1"> figure, the only thing detected by the off-the-shelf YOLOv8 object model is an upside-down matchbook car, which it incorrectly labels as </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">a skateboard:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.328.1"><img alt="Figure 4.8 – YOLOv8 output without specific training on our toys" src="image/B19846_04_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.329.1">Figure 4.8 – YOLOv8 output without specific training on our toys</span></p>
<p><span class="koboSpan" id="kobo.330.1">You have to admit, the little toy car does resemble a skateboard from this angle, but this is not the result we want. </span><span class="koboSpan" id="kobo.330.2">We need all the toys in the image to be detected, not just one. </span><span class="koboSpan" id="kobo.330.3">What can we do </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">about this?</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">The answer is that we can add new training to the network, get all the advantages of YOLOv8, and have our custom objects detected as well. </span><span class="koboSpan" id="kobo.332.2">For this, we can use a process called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.333.1">tr</span><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.334.1">ansfer learning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.336.1">Here is an overview of how we will train our toy detector, after which we will discuss these steps in </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">greater detail:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.338.1">First, we will prepare a training set of images of the room with toys. </span><span class="koboSpan" id="kobo.338.2">This means we must take a lot of pictures of the toys from the viewpoint of the robot, using the same camera the robot will use. </span><span class="koboSpan" id="kobo.338.3">We want to take pictures from all the different angles and sides of the toys. </span><span class="koboSpan" id="kobo.338.4">I went around the room clockwise, then anti-clockwise, snapping pictures every few inches. </span><span class="koboSpan" id="kobo.338.5">I took 48 pictures in </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">this step.</span></span></li>
<li><span class="koboSpan" id="kobo.340.1">Next, we must use a data labeling program such as RoboFlow (</span><a href="https://roboflow.com"><span class="koboSpan" id="kobo.341.1">https://roboflow.com</span></a><span class="koboSpan" id="kobo.342.1">) to annotate the images (you can refer to the relevant documentation for detailed instructions). </span><span class="koboSpan" id="kobo.342.2">The program lets us draw boxes around the objects we want to recognize (toys) and label them with a tag – we will use the </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.343.1">name </span><strong class="source-inline"><span class="koboSpan" id="kobo.344.1">toy</span></strong><span class="koboSpan" id="kobo.345.1">. </span><span class="koboSpan" id="kobo.345.2">We are separating the parts of the image that contain toys and telling the neural network what to call this type </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">of object:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.347.1"><img alt="Figure 4.9 – Annotating using RoboFlow, a free data labeling tool" src="image/B19846_04_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.348.1">Figure 4.9 – Annotating using RoboFlow, a free data labeling tool</span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.349.1">Then, we must split the training set into three parts: a set we use to train the network, a set we use to validate the training, and a set we use to test the network. </span><span class="koboSpan" id="kobo.349.2">We will create a set of 87% of the images for training, 8% for validation, and 5% for testing. </span><span class="koboSpan" id="kobo.349.3">We’ll put the training data and test data in separate folders. </span><span class="koboSpan" id="kobo.349.4">RoboFlow has a procedure for this under the </span><strong class="bold"><span class="koboSpan" id="kobo.350.1">Generate</span></strong><span class="koboSpan" id="kobo.351.1"> tab, where there is a section labeled </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.352.1">T</span><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.353.1">rain/Test Split</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.355.1">Now, we must take each image and multiply its training value by combining parts of different images in a mosaic. </span><span class="koboSpan" id="kobo.355.2">We’ll take parts of four random different pictures and combine them. </span><span class="koboSpan" id="kobo.355.3">This will increase our training set three-fold, a process called </span><strong class="bold"><span class="koboSpan" id="kobo.356.1">data augmentation</span></strong><span class="koboSpan" id="kobo.357.1">. </span><span class="koboSpan" id="kobo.357.2">This is built into RoboFlow. </span><span class="koboSpan" id="kobo.357.3">I started with 36 pictures; after </span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.358.1">augmentation, I </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">had 99:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.360.1"><img alt="Figure 4.10 – Mosaic data augmentation creates more training data from our limited number of pictures" src="image/B19846_04_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.361.1">Figure 4.10 – Mosaic data augmentation creates more training data from our limited number of pictures</span></p>
<p><span class="koboSpan" id="kobo.362.1">Why are we using this mosaic approach? </span><span class="koboSpan" id="kobo.362.2">We still want to have valid bounding boxes. </span><span class="koboSpan" id="kobo.362.3">The mosaic process resizes any partial bounding boxes that intersect </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">the edges.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.364.1">Next, we will be building two programs: the training program, which runs on our desktop computer and trains the network, and the working program, which uses the trained network to find toys. </span><span class="koboSpan" id="kobo.364.2">The training process may not run on our small computer onboard the robot or may take a long time to run, so we’ll use the desktop computer </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">for this.</span></span></li>
<li><span class="koboSpan" id="kobo.366.1">Now, we need to train the network. </span><span class="koboSpan" id="kobo.366.2">To achieve this, we must do </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">the following:</span></span><ol><li><span class="koboSpan" id="kobo.368.1">First, we must scale all our images down to reduce the processing time to a </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">reasonable level.</span></span></li><li><span class="koboSpan" id="kobo.370.1">Then, we must initialize the network with uniform </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">random weights.</span></span></li><li><span class="koboSpan" id="kobo.372.1">Next, we must encode a labeled image and input that to the network. </span><span class="koboSpan" id="kobo.372.2">The neural network only uses the image data to predict what class of object is in the picture, and what its bounding box should be. </span><span class="koboSpan" id="kobo.372.3">Since we pre-labeled the image with the correct answer and used the correct bounding box, we can judge whether the answer is right or wrong. </span><span class="koboSpan" id="kobo.372.4">If it is right, we can reinforce the weights on the</span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.373.1"> inputs that contributed to this answer by incrementing them (the training value). </span><span class="koboSpan" id="kobo.373.2">If the answer is wrong, we can reduce the weights instead. </span><span class="koboSpan" id="kobo.373.3">In neural networks, the error between the desired result and the actual resul</span><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.374.1">t is called </span><strong class="bold"><span class="koboSpan" id="kobo.375.1">loss</span></strong><span class="koboSpan" id="kobo.376.1">. </span><span class="koboSpan" id="kobo.376.2">Repeat this process for </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">each image.</span></span></li><li><span class="koboSpan" id="kobo.378.1">Now, we must test the network by running the testing set of images – which are pictures of the same toys, but that were not in the training set. </span><span class="koboSpan" id="kobo.378.2">We must analyze what sort of output we get over this set (how many are wrong and how many are right). </span><span class="koboSpan" id="kobo.378.3">If this answer is above 90%, we stop. </span><span class="koboSpan" id="kobo.378.4">Otherwise, we go back and run all the training </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">images again.</span></span></li><li><span class="koboSpan" id="kobo.380.1">Once we are happy with the results – and we should need between 50 and 100 iterations to get there – we must stop and store the weights that we ended up with in the training network. </span><span class="koboSpan" id="kobo.380.2">This is our </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.381.1">trained CNN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">.</span></span></li></ol></li>
<li><span class="koboSpan" id="kobo.383.1">Our next task is to find the toys. </span><span class="koboSpan" id="kobo.383.2">To do this, we must </span><em class="italic"><span class="koboSpan" id="kobo.384.1">deploy</span></em><span class="koboSpan" id="kobo.385.1"> the trained network by loading it and using our video images from the live robot to look for toys. </span><span class="koboSpan" id="kobo.385.2">We’ll get a probability of an image containing a toy from 0% to 100%. </span><span class="koboSpan" id="kobo.385.3">We’ll scan the input video image in sections and find which sections contain toys. </span><span class="koboSpan" id="kobo.385.4">If we are not happy with this network, we can reload this network into the training program and</span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.386.1"> train it </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">some more.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.388.1">Now, let’s cover this in detail, step by step. </span><span class="koboSpan" id="kobo.388.2">We have a bit more theory to cover before we start writing </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">the code.</span></span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.390.1">Understanding how to train our toy detector</span></h2>
<p><span class="koboSpan" id="kobo.391.1">Our first task is to </span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.392.1">prepare a training set. </span><span class="koboSpan" id="kobo.392.2">We’ll put the camera </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.393.1">on the robot and drive the robot around using the teleoperation interface (or just by pushing it around by hand), snapping still photos every foot or so. </span><span class="koboSpan" id="kobo.393.2">We just need pictures with toys in the image since we will be annotating the toys. </span><span class="koboSpan" id="kobo.393.3">We need about 200 pictures – the more, the better. </span><span class="koboSpan" id="kobo.393.4">We also need to have a set of pictures in the daytime with natural light, and at night, if your room changes lighting between day and night. </span><span class="koboSpan" id="kobo.393.5">This affords us several advantages: we are using the same room and the same camera to find the toys, all under the same </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">lighting conditions.</span></span></p>
<p><span class="koboSpan" id="kobo.395.1">Now, we need to label the images. </span><span class="koboSpan" id="kobo.395.2">We’ll load the images into RoboFlow to create a dataset called </span><strong class="source-inline"><span class="koboSpan" id="kobo.396.1">toydetector</span></strong><span class="koboSpan" id="kobo.397.1">. </span><span class="koboSpan" id="kobo.397.2">Use the </span><strong class="bold"><span class="koboSpan" id="kobo.398.1">Upload</span></strong><span class="koboSpan" id="kobo.399.1"> tab and drag and drop the images or select the folder that c</span><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.400.1">ontains </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">the images.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">The process for us is fairly straightforward. </span><span class="koboSpan" id="kobo.402.2">We look at each picture in turn and draw a box around any toy objects. </span><span class="koboSpan" id="kobo.402.3">We hit </span><em class="italic"><span class="koboSpan" id="kobo.403.1">Enter</span></em><span class="koboSpan" id="kobo.404.1"> or the </span><strong class="bold"><span class="koboSpan" id="kobo.405.1">Save</span></strong><span class="koboSpan" id="kobo.406.1"> button. </span><span class="koboSpan" id="kobo.406.2">The annotation dialog box will open. </span><span class="koboSpan" id="kobo.406.3">As you might have guessed, we’ll be labeling these </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">toy</span></strong><span class="koboSpan" id="kobo.408.1">. </span><span class="koboSpan" id="kobo.408.2">This is going to take </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">a while.</span></span></p>
<p><span class="koboSpan" id="kobo.410.1">Once we’ve labeled around 160 toys in our images, we can use the </span><strong class="bold"><span class="koboSpan" id="kobo.411.1">Generate</span></strong><span class="koboSpan" id="kobo.412.1"> button in RoboFlow to create our dataset. </span><span class="koboSpan" id="kobo.412.2">We must set up the preprocessing task to resize our images to 640x640 pixels. </span><span class="koboSpan" id="kobo.412.3">This makes the best use of our limited computer capacity on the robot. </span><span class="koboSpan" id="kobo.412.4">Then, we must augment the dataset to create additional images of our limited set, as mentioned previously. </span><span class="koboSpan" id="kobo.412.5">We’ll use the mosaic method to augment our dataset while preserving the bounding boxes. </span><span class="koboSpan" id="kobo.412.6">To do this, we must use the </span><strong class="bold"><span class="koboSpan" id="kobo.413.1">Generate</span></strong><span class="koboSpan" id="kobo.414.1"> tab in RoboFlow, then click </span><strong class="bold"><span class="koboSpan" id="kobo.415.1">Add Augmentation Step</span></strong><span class="koboSpan" id="kobo.416.1"> to select the type of operation that will affect our images. </span><span class="koboSpan" id="kobo.416.2">Then, we must add the mosiac augmentation to create more images out of our training set. </span><span class="koboSpan" id="kobo.416.3">Now, we can hit the </span><strong class="bold"><span class="koboSpan" id="kobo.417.1">Generate</span></strong><span class="koboSpan" id="kobo.418.1"> button to create </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">our dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.420.1">We started with 48 images that I took (back in step 1); after augmentation, we have 114. </span><span class="koboSpan" id="kobo.420.2">We’ll set our test/train split so that it contains 99 images in the training set, nine images in the validation set, and six images in the test set (87% training, 8% validation, and 5% testing). </span><span class="koboSpan" id="kobo.420.3">This makes the best use of our </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">limited dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.422.1">To download our datasets from RoboFlow, we must install RoboFlow’s interface on our computer. </span><span class="koboSpan" id="kobo.422.2">It’s a </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">Python package:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.424.1">
pip install roboflow</span></pre> <p><span class="koboSpan" id="kobo.425.1">Then, we must create a short Python program called </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">downloadDataset.py</span></strong><span class="koboSpan" id="kobo.427.1">. </span><span class="koboSpan" id="kobo.427.2">When you build your dataset, RoboFlow will provide a unique </span><strong class="source-inline"><span class="koboSpan" id="kobo.428.1">api_key</span></strong><span class="koboSpan" id="kobo.429.1"> value; this will be the password for your</span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.430.1"> account that authorizes access. </span><span class="koboSpan" id="kobo.430.2">This goes into</span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.431.1"> the program as follows, where I put </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">the asterisks:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.433.1">
from roboflow import Roboflow
rf = Roboflow(api_key="*****************")
project = rf.workspace("toys").project("toydetector")
dataset = project.version(1).download("yolov8")</span></pre> <p><span class="koboSpan" id="kobo.434.1">In the next section, we will retrain the network with </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">this command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.436.1">
yolo task=detect mode=train model=yolov8n.pt data=datasets/data.yaml epochs=100 imgsz=640</span></pre> <p><span class="koboSpan" id="kobo.437.1">Once we’ve done this, the program will produce a lot of output, </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.439.1">
(p310) E:\BOOK\YOLO&gt;yolo task=detect mode = val model=runs\detect\train3\weights\best.pt data=ToyDetector-1\data.yaml
Ultralytics YOLOv8.0.78 Python-3.10.10 torch-2.0.0 CUDA:0 (NVIDIA GeForce RTX 2070, 8192MiB)
Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs
..................
</span><span class="koboSpan" id="kobo.439.2">AMP: checks passed
optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias
train: Scanning E:\BOOK\YOLO\datasets\ToyDetector-1\train\labels.cache… 99 images, 0 backgrounds, 0 corrupt: 100%|███
val: Scanning E:\BOOK\YOLO\datasets\ToyDetector-1\valid\labels.cache… 9 images, 0 backgrounds, 0 corrupt: 100%|██████
Plotting labels to runs\detect\train5\labels.jpg…
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs\detect\train5
Starting training for 100 epochs…</span></pre> <p><span class="koboSpan" id="kobo.440.1">One of the critical parts of training our model</span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.441.1"> is the </span><strong class="bold"><span class="koboSpan" id="kobo.442.1">training optimizer</span></strong><span class="koboSpan" id="kobo.443.1">. </span><span class="koboSpan" id="kobo.443.2">We will </span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.444.1">use </span><strong class="bold"><span class="koboSpan" id="kobo.445.1">stochastic gradient descent</span></strong><span class="koboSpan" id="kobo.446.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.447.1">SGD</span></strong><span class="koboSpan" id="kobo.448.1">) for this. </span><span class="koboSpan" id="kobo.448.2">SGD is</span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.449.1"> another of those simple concepts with a fancy name. </span><span class="koboSpan" id="kobo.449.2">Stochastic just means </span><em class="italic"><span class="koboSpan" id="kobo.450.1">random</span></em><span class="koboSpan" id="kobo.451.1">. </span><span class="koboSpan" id="kobo.451.2">What we want to do is tweak the weights of our neurons to give a better answer than we got the first time – this is what we are</span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.452.1"> training, by adjusting the weights. </span><span class="koboSpan" id="kobo.452.2">We want to change the weights a small amount – but in which direction? </span><span class="koboSpan" id="kobo.452.3">We want to change the weights in the direction that improves the answer – it makes the prediction closer</span><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.453.1"> to what we want it </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">to be.</span></span></p>
<p><span class="koboSpan" id="kobo.455.1">To understand this better, let’s do a little thought experiment. </span><span class="koboSpan" id="kobo.455.2">We have a neuron that we know is producing the wrong answer and needs adjusting. </span><span class="koboSpan" id="kobo.455.3">We’ll add a small amount to the weight and see how the answer changes. </span><span class="koboSpan" id="kobo.455.4">It gets slightly worse – the number is further away from the correct answer. </span><span class="koboSpan" id="kobo.455.5">So, we must subtract a small amount instead – and, as you might think, the answer gets better. </span><span class="koboSpan" id="kobo.455.6">We have reduced the amount of error slightly. </span><span class="koboSpan" id="kobo.455.7">If we make a graph of the error produced by the neuron, we’ll see that we are moving toward an error of zero, or the graph is descending toward some minimum value. </span><span class="koboSpan" id="kobo.455.8">Another way of saying this is that the slope of the line is negative – going toward zero. </span><span class="koboSpan" id="kobo.455.9">The amount of the slope can be </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.456.1">called a </span><strong class="bold"><span class="koboSpan" id="kobo.457.1">gradient</span></strong><span class="koboSpan" id="kobo.458.1"> – just as you would refer to the slope or steepness of a hill as the gradient. </span><span class="koboSpan" id="kobo.458.2">We can calculate the partial derivative (in other words, the slope of the error curve near this point), which tells us the slope of </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">the line.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">The way we go about adjusting the weights on the network as a whole to minimize the loss between the ground truth and the predicted value is</span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.461.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.462.1">backpropagation</span></strong><span class="koboSpan" id="kobo.463.1">. </span><span class="koboSpan" id="kobo.463.2">This is because, as you might surmise, we have to start at the end of the network – where we know what the answer is supposed to be – and work our way toward the beginning. </span><span class="koboSpan" id="kobo.463.3">We have to calculate how each neuron contributes to the answer we want and adjust it slightly (this is known </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.464.1">as the </span><strong class="bold"><span class="koboSpan" id="kobo.465.1">learning rate</span></strong><span class="koboSpan" id="kobo.466.1">) in the right direction to move toward the correct answer every time. </span><span class="koboSpan" id="kobo.466.2">For this, we must go back to the idea of a neuron – we have inputs, weights for each input, a bias, and then an activation function to produce an output. </span><span class="koboSpan" id="kobo.466.3">If we know what the output is, we can work backward through the neuron to adjust the weights. </span><span class="koboSpan" id="kobo.466.4">Let’s take a simple example. </span><span class="koboSpan" id="kobo.466.5">We have a neuron with three inputs – </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">Y1</span></strong><span class="koboSpan" id="kobo.468.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">Y2</span></strong><span class="koboSpan" id="kobo.470.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">Y3</span></strong><span class="koboSpan" id="kobo.472.1">. </span><span class="koboSpan" id="kobo.472.2">We have three weights – </span><strong class="source-inline"><span class="koboSpan" id="kobo.473.1">W1</span></strong><span class="koboSpan" id="kobo.474.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">W2</span></strong><span class="koboSpan" id="kobo.476.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">W3</span></strong><span class="koboSpan" id="kobo.478.1">. </span><span class="koboSpan" id="kobo.478.2">We’ll have the bias, </span><strong class="source-inline"><span class="koboSpan" id="kobo.479.1">B</span></strong><span class="koboSpan" id="kobo.480.1">, and our activation function, </span><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">D</span></strong><span class="koboSpan" id="kobo.482.1">, which is the ReLU rectifier. </span><span class="koboSpan" id="kobo.482.2">The values of our inputs are 0.2, 0.7, and 0.02. </span><span class="koboSpan" id="kobo.482.3">The weights are 0.3, 0.2, and 0.5. </span><span class="koboSpan" id="kobo.482.4">Our bias is 0.3, and the desired output is 1.0. </span><span class="koboSpan" id="kobo.482.5">We</span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.483.1"> calculate </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.484.1">the sum of the inputs and weights and we get a value of 0.21. </span><span class="koboSpan" id="kobo.484.2">After adding our bias, we get 0.51. </span><span class="koboSpan" id="kobo.484.3">The ReLU function passes any value greater than zero, so the activated output of this neuron is 0.51. </span><span class="koboSpan" id="kobo.484.4">Our desired value is 1.0, which comes from the truth (label) data. </span><span class="koboSpan" id="kobo.484.5">So, our error is 0.49. </span><span class="koboSpan" id="kobo.484.6">If we add the training rate value to each weight, what happens? </span><span class="koboSpan" id="kobo.484.7">Take a look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.486.1"><img alt="Figure 4.11 – How backpropagation works to adjust weights" src="image/B19846_04_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.487.1">Figure 4.11 – How backpropagation works to adjust weights</span></p>
<p><span class="koboSpan" id="kobo.488.1">The output value </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.489.1">now goes up to 0.5192. </span><span class="koboSpan" id="kobo.489.2">Our error</span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.490.1"> goes down to 0.4808. </span><span class="koboSpan" id="kobo.490.2">We are on the right track! </span><span class="koboSpan" id="kobo.490.3">The gradient of our error slope is </span><em class="italic"><span class="koboSpan" id="kobo.491.1">(.4808-.49) / 1 = -0.97</span></em><span class="koboSpan" id="kobo.492.1">. </span><span class="koboSpan" id="kobo.492.2">The </span><em class="italic"><span class="koboSpan" id="kobo.493.1">1</span></em><span class="koboSpan" id="kobo.494.1"> is because we just have one training sample so far. </span><span class="koboSpan" id="kobo.494.2">So, where does the stochastic part come from? </span><span class="koboSpan" id="kobo.494.3">Our recognition network may have 50 million neurons. </span><span class="koboSpan" id="kobo.494.4">We can’t be doing all of this math for each one. </span><span class="koboSpan" id="kobo.494.5">So, we must take a random sampling of inputs rather than all of them to determine whether our training is positive </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">or negative.</span></span></p>
<p><span class="koboSpan" id="kobo.496.1">In math terms, the slope of an equation is provided by the derivative of that equation. </span><span class="koboSpan" id="kobo.496.2">So, in practice, backpropagation takes the partial derivative of the error between training epochs to determine the slope of the error, and thus determine whether we are training our network correctly. </span><span class="koboSpan" id="kobo.496.3">As the slope gets smaller, we reduce our training rate to a smaller number to get closer and closer to the </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">correct answer:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.498.1"><img alt="Figure 4.1﻿2 – The gradient descent process" src="image/B19846_04_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.499.1">Figure 4.1</span><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.500.1">2 – The gradient descent process</span></p>
<p><span class="koboSpan" id="kobo.501.1">Now, we can to our </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.502.1">next problem: how do we propagate our</span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.503.1"> weight adjustments up the layers of the neural network? </span><span class="koboSpan" id="kobo.503.2">We can determine the error at the output neuron – just the label value minus the output of the network. </span><span class="koboSpan" id="kobo.503.3">How do we apply this information to the previous layer? </span><span class="koboSpan" id="kobo.503.4">Each neuron’s contribution to the error is proportional to its weight. </span><span class="koboSpan" id="kobo.503.5">We must divide the error by the weight of each input, and that value is now the applied error of the next neuron up the chain. </span><span class="koboSpan" id="kobo.503.6">Then, we can recompute their weights and so on. </span><span class="koboSpan" id="kobo.503.7">This is why neural networks take so much </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">compute power:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.505.1"><img alt="Figure 4.13 – Backpropagation error" src="image/B19846_04_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.506.1">Figure 4.13 – Backpropagation error</span></p>
<p><span class="koboSpan" id="kobo.507.1">We backpropagate the error back up the network from the end back to the beginning. </span><span class="koboSpan" id="kobo.507.2">Then, we start </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.508.1">a</span><a id="_idTextAnchor149"/><a id="_idTextAnchor150"/><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.509.1">ll </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.510.1">over again with the </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">next cycle.</span></span></p>
<p><span class="koboSpan" id="kobo.512.1">At this point, we can test our toy detector. </span><span class="koboSpan" id="kobo.512.2">Let’s see how we can </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">do this.</span></span></p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.514.1">Building the toy detector</span></h2>
<p><span class="koboSpan" id="kobo.515.1">We can use </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.516.1">the</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.517.1"> following command to test how </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">we did:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.519.1">
yolo task=detect mode=predict model=last.pt source=toy1.jpg imgsz=640</span></pre> <p><span class="koboSpan" id="kobo.520.1">The program produces the following output. </span><span class="koboSpan" id="kobo.520.2">We can find our image with the labeled detections in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.521.1">./runs/detect/predict</span></strong><span class="koboSpan" id="kobo.522.1"> directory with a number appended depending on how many times we run </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">the detection:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.524.1">
Speed: 4.0ms preprocess, 44.7ms inference, 82.6ms postprocess per image at shape (1, 3, 640, 640)
Results saved to runs\detect\predict4</span></pre> <p><span class="koboSpan" id="kobo.525.1">The output of our prediction is shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.527.1"><img alt="Figure 4.14 – The toy detector in action" src="image/B19846_04_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.528.1">Figure 4.14 – The toy detector in action</span></p>
<p><span class="koboSpan" id="kobo.529.1">With this, we have </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.530.1">successfully created a toy detector using a </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.531.1">neural network. </span><span class="koboSpan" id="kobo.531.2">The output of the detector, which we will use in </span><a href="B19846_05.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.532.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.533.1"> to direct the robot and the robot arm to drive to the toy </span><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.534.1">and then pick it up, looks </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">like this:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.536.1">
"predictions": [
 {
 "x": 287.5,
 "y": 722.5,
 "width": 207,
 "height": 131,
 "confidence": 0.602,
 "class": "toy"
 },</span></pre> <p><span class="koboSpan" id="kobo.537.1">For each detection, the neural network will provide several bits of information. </span><span class="koboSpan" id="kobo.537.2">We get the </span><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">x</span></strong><span class="koboSpan" id="kobo.539.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.540.1">y</span></strong><span class="koboSpan" id="kobo.541.1"> locations of the center of the bounding box, and then the height and width of that box. </span><span class="koboSpan" id="kobo.541.2">Then, we get a confidence number of how certain the network is of the decision that this is a detection. </span><span class="koboSpan" id="kobo.541.3">Finally, we get the class of the object (what kind of object), which is, of course, </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">a toy.</span></span></p>
<p><span class="koboSpan" id="kobo.543.1">When we ran the training process for the neural network, if you look in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.544.1">training</span></strong><span class="koboSpan" id="kobo.545.1"> folder found in </span><strong class="source-inline"><span class="koboSpan" id="kobo.546.1">runs/detect/train</span></strong><span class="koboSpan" id="kobo.547.1">, there are a whole series of graphs. </span><span class="koboSpan" id="kobo.547.2">What do these graphs </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">tell us?</span></span></p>
<p><span class="koboSpan" id="kobo.549.1">The first one we need to look at is </span><strong class="source-inline"><span class="koboSpan" id="kobo.550.1">F1_curve</span></strong><span class="koboSpan" id="kobo.551.1">. </span><span class="koboSpan" id="kobo.551.2">This is the product of precision and recall. </span><strong class="bold"><span class="koboSpan" id="kobo.552.1">Precision</span></strong><span class="koboSpan" id="kobo.553.1"> is the </span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.554.1">ratio of true positives (correctly classified objects) from all positives. </span><strong class="bold"><span class="koboSpan" id="kobo.555.1">Recall</span></strong><span class="koboSpan" id="kobo.556.1"> is the </span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.557.1">proportion of positive detections that were identified correctly. </span><span class="koboSpan" id="kobo.557.2">So, precision is</span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.558.1"> defined </span><a id="_idIndexMarker357"/><span class="No-Break"><span class="koboSpan" id="kobo.559.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.560.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.561.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.562.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.563.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.564.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.565.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.566.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.567.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.568.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.569.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.570.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.571.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.572.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.573.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.574.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.575.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.576.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.577.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.578.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.579.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.580.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.581.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.582.1">Precision is the true positives divided by true positives and false positives (items that were identified as detections but </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">were not).</span></span></p>
<p><span class="koboSpan" id="kobo.584.1">Recall is defined </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">slightly differently:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.586.1">R</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.587.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.588.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.589.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.590.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.591.1">l</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.592.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.593.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.594.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.595.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.596.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.597.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.598.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.599.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.600.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.601.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.602.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.603.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.604.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.605.1">Here, recall is the true positives divided by true positives plus false negatives. </span><span class="koboSpan" id="kobo.605.2">A false negative is a missed detection or an object that was not detected when it was, in </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">fact, present.</span></span></p>
<p><span class="koboSpan" id="kobo.607.1">To create the F1 curve, we must multiply precision and recall together and plot it against </span><em class="italic"><span class="koboSpan" id="kobo.608.1">confidence</span></em><span class="koboSpan" id="kobo.609.1">. </span><span class="koboSpan" id="kobo.609.2">The graph shows the level of confidence in our detections that produces the best result of trading off precision </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">and recall:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.611.1"><img alt="Figure 4.15 – The F1 confidence curve" src="image/B19846_04_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.612.1">Figure 4.15 – The F1 confidence curve</span></p>
<p><span class="koboSpan" id="kobo.613.1">In this case, a confidence level of 0.21 gives a detection rate of 0.87. </span><span class="koboSpan" id="kobo.613.2">This means that we get the best ratio of true detections to false detections. </span><span class="koboSpan" id="kobo.613.3">However, this best ratio – 87% – occurs at 0.21 confidence – a rather low number. </span><span class="koboSpan" id="kobo.613.4">Detections at this low confidence level are hard to distinguish </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.614.1">and can be caused by noise in </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.615.1">measurements. </span><span class="koboSpan" id="kobo.615.2">It might be more desirable to have our peak at a higher confidence level. </span><span class="koboSpan" id="kobo.615.3">I tried several approaches to address this. </span><span class="koboSpan" id="kobo.615.4">I ran 200 epochs rather than 100 and moved the peak F1 confidence level to 51%, but the detection level dropped a bit to 85%. </span><span class="koboSpan" id="kobo.615.5">Then, I changed the gradient descent technique from SDM</span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.616.1"> to </span><strong class="bold"><span class="koboSpan" id="kobo.617.1">Adam</span></strong><span class="koboSpan" id="kobo.618.1">, an adaptive gradient descent technique that reduces the learning rate as you get closer to our goal. </span><span class="koboSpan" id="kobo.618.2">This can be done using the </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">following code:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.620.1">
yolo task=detect mode=train model=yolov8n.pt data=datasets/data.yaml epochs=100 optimizer='adamW' imgsz=640</span></pre> <p><span class="koboSpan" id="kobo.621.1">This produced a more satisfactory result of 88% true detections at 49% confidence, which I think will do a better job for our toy detector. </span><span class="koboSpan" id="kobo.621.2">In reviewing my detections, there were a few false positives (furniture and other objects being detected as toys), so I think that this version will be our toy detector neural network. </span><span class="koboSpan" id="kobo.621.3">Although I used a fairly small dataset, it would not hurt to have more </span><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.622.1">pictures to work with from different angles. </span><span class="koboSpan" id="kobo.622.2">Before wrapping </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.623.1">this chapter up, let’s briefly summarize</span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.624.1"> what we’ve learned </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">so far.</span></span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.626.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.627.1">In this chapter, we dove head-first into the world of ANNs. </span><span class="koboSpan" id="kobo.627.2">An ANN can be thought of as a stepwise non-linear approximation function that slowly adjusts itself to fit a curve that matches the desired input to the desired output. </span><span class="koboSpan" id="kobo.627.3">The learning process consists of several steps, including preparing data, labeling data, creating the network, initializing the weights, creating the forward pass that provides the output, and calculating the loss (also called the error). </span><span class="koboSpan" id="kobo.627.4">We created a special type of ANN, a CNN, to examine images. </span><span class="koboSpan" id="kobo.627.5">The network was trained using images with toys, to which we added bounding boxes to tell the network what part of the image was a toy. </span><span class="koboSpan" id="kobo.627.6">We trained the network to get an accuracy better than 87% in classifying images with toys in them. </span><span class="koboSpan" id="kobo.627.7">Finally, we tested the network to verify its output and tuned our results using the Adam adaptive </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">descent algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.629.1">In the next chapter, we will look at machine learning for the robot arm in terms of reinforcement learning and </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">genetic algorithms.</span></span></p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.631.1">Questions</span></h1>
<ol>
<li value="1"><span class="koboSpan" id="kobo.632.1">We went through a lot in this chapter. </span><span class="koboSpan" id="kobo.632.2">You can use the framework provided to investigate the properties of neural networks. </span><span class="koboSpan" id="kobo.632.3">Try several activation functions, or different settings for convolutions, to see what changes in the </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">training process.</span></span></li>
<li><span class="koboSpan" id="kobo.634.1">Draw a diagram of an artificial neuron and label the parts. </span><span class="koboSpan" id="kobo.634.2">Look up a natural, human biological neuron and </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">compare them.</span></span></li>
<li><span class="koboSpan" id="kobo.636.1">Which features of a real neuron and an artificial neuron are the same? </span><span class="koboSpan" id="kobo.636.2">Which ones </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">are different?</span></span></li>
<li><span class="koboSpan" id="kobo.638.1">What effect does the learning rate have on gradient descent? </span><span class="koboSpan" id="kobo.638.2">What if the learning rate is too large? </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">Too small?</span></span></li>
<li><span class="koboSpan" id="kobo.640.1">What relationship does the first layer of a neural network have with </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">the input?</span></span></li>
<li><span class="koboSpan" id="kobo.642.1">What relationship does the last layer of a neural network have with </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">the output?</span></span></li>
<li><span class="koboSpan" id="kobo.644.1">Look up three kinds of loss functions and describe how they work. </span><span class="koboSpan" id="kobo.644.2">Include mean square loss and the two kinds of </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">cross-entropy loss.</span></span></li>
<li><span class="koboSpan" id="kobo.646.1">What would you change if your network was trained and reached 40% accuracy of the classification and got s</span><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.647.1">tuck, or was unable to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">anything further?</span></span></li>
</ol>
<h1 id="_idParaDest-77"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.649.1">Further reading</span></h1>
<p><span class="koboSpan" id="kobo.650.1">For more information on the topics that were covered in this chapter, please refer to the </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">following resources:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.652.1">Python Deep Learning Cookbook</span></em><span class="koboSpan" id="kobo.653.1">, by Indra den Bakker, Packt </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">Publishing, 2017</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.655.1">Artificial Intelligence with Python</span></em><span class="koboSpan" id="kobo.656.1">, by Prateek Joshi, Packt </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">Publishing, 2017</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.658.1">Python Deep Learning</span></em><span class="koboSpan" id="kobo.659.1">, by Valentino Zocca, Gianmario Spacagna, Daniel Slater, and Peter Roelants, Packt </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">Publishing, 2017</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.661.1">PyImageSearch Blog</span></em><span class="koboSpan" id="kobo.662.1">, by Adrian Rosebrock, available at </span><a href="http://pyimagesearch.com"><span class="No-Break"><span class="koboSpan" id="kobo.663.1">pyimagesearch.com</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.664.1">, 2018</span></span></li>
</ul>
</div>
<div>
<div class="IMG---Figure" id="_idContainer063">
</div>
</div>
</body></html>