- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding Multimodal, Multifunctional Reasoning with Chain of Thought
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point in our journey, we’ve built the core framework of our GenAISys.
    We have a responsive, small-scale, ChatGPT-like interactive interface. We expanded
    beyond typical one-to-one copilot interactions, creating a collaborative multi-user
    environment where an AI agent actively participates in discussions. We further
    extended this human-centric design by integrating RAG, giving our AI agent access
    to a Pinecone index capable of managing both instruction scenarios and data. Finally,
    we built a flexible GenAISys that allows users to activate or deactivate the AI
    agent during collaborative meetings. In short, we have created a human-centric
    AI system that augments human teams rather than attempting to replace people with
    machine intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite its human-centric nature, the exponential growth of global
    transcontinental supply chains and the vast daily flow of goods, services, and
    digital content require significant levels of automation. For example, we cannot
    realistically expect social media platforms such as Meta, X, or LinkedIn to employ
    millions of people to moderate billions of messages—including images, audio, and
    video files—every day. Similarly, companies such as Amazon cannot manage millions
    of online transactions and physical deliveries exclusively through human efforts.
    Automation is essential to augment human decision-making and reasoning, particularly
    for critical tasks at scale. Therefore, in this chapter, we will extend the GenAISys
    framework by adding multimodal capabilities and reasoning functionalities. To
    address the challenges of cross-domain automation, we will implement image generation
    and analysis and begin integrating machine learning. Our objective is to build
    a new agentic AI layer into our GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by outlining features that we are integrating into our existing
    GenAISys framework. Given the broadening scope of our GenAISys, we will introduce
    **chain-of-thought** (**CoT**) reasoning processes to orchestrate and manage complex
    tasks effectively. We will then incorporate computer vision capabilities. This
    includes building an image generation function with DALL-E and an image analysis
    function using GPT-4o. Next, we will add audio functionality for those who prefer
    voice interactions—using **speech to text** (**STT**) for input prompts and **text
    to speech** (**TTS**) for responses. Lastly, we’ll introduce a decision tree classifier
    as a machine learning endpoint within the GenAISys, capable of predicting activities.
    By the end of this chapter, we will have successfully extended the GenAISys into
    a fully interactive, multimodal reasoning platform ready to tackle complex cross-domain
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In all, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the additional functions for our GenAISys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a widget image file processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a widget to enable voice dialogues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image generation with DALL-E
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image analysis with GPT-4o
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an endpoint for machine learning with a decision tree classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing CoT reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by designing an enhanced interface for our GenAISys with additional
    AI capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing the event-driven GenAISys interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, the GenAISys framework we’ve developed is event-driven, activated by
    user inputs (human- or system-generated) that trigger specific AI agent functions.
    In this chapter, we’ll expand the GenAISys by adding several new capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Voice interaction**, allowing users to manage the GenAISys through speech'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new **machine learning endpoint** using a decision tree classifier for predictive
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multimodal functionality**, including image generation with DALL-E and image
    analysis using GPT-4o'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **CoT** reasoning orchestrator to coordinate sophisticated, self-reflective
    instruction scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start by examining the expanded GenAISys architecture shown in *Figure
    5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: Architecture of the enhanced GenAISys interface](img/B32304_05_1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Architecture of the enhanced GenAISys interface'
  prefs: []
  type: TYPE_NORMAL
- en: 'This figure (which is an extended version of *Figure 4.1* from the previous
    chapter) highlights the new capabilities we’ll integrate into our GenAISys:'
  prefs: []
  type: TYPE_NORMAL
- en: '**I1** – **AI controller**: Enhanced with CoT reasoning, enabling automated
    sequences of tasks as needed and incorporating a widget to manage voice-based
    user interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I2** – **Multi-user chatbot**: Maintained exactly as designed in previous
    chapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1** – **Generative AI model**: Extended to handle multimodal tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F2** – **Memory retention**: Continues unchanged from earlier chapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F3** – **Modular RAG**: Continues unchanged from earlier chapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F4** – **Multifunctional capabilities**: New additions covering audio and
    image processing, including a decision tree classifier for making predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reminder**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The decision to present the main components of the GenAISys architecture without
    arrows is a deliberate choice designed to convey a core concept: modularity and
    architectural flexibility. The figure is not a rigid blueprint but rather a conceptual
    toolkit. It shows you the powerful components at your disposal—**I1\. AI controller**,
    **I2\. Multi-user chatbot**, **F1\. Generative AI model**, **F2\. Memory retention**,
    **F3\. Modular RAG**, and **F4\. Multifunctional capabilities**—as independent,
    interoperable blocks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are expanding the functionality of GenAISys as built in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110)
    by adding new layers rather than replacing existing components. Our emphasis here
    is on enhancement and seamless integration. The following figure provides a high-level
    flowchart demonstrating how the additional capabilities will integrate into our
    existing GenAISys architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: Flowchart of additional functions to the GenAISys](img/B32304_05_2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Flowchart of additional functions to the GenAISys'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following additional functions will be integrated into our existing GenAISys
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start**: Initializes two new widgets—one for TTS functionality and another
    to handle image files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User Input**: Now includes optional voice input, enabled if the user chooses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate Bot** and **Generate Bot Response**: These processes connect directly
    to the existing `VBox` interface, displaying reasoning steps clearly whenever
    the AI agent utilizes CoT logic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To achieve this expanded functionality, we will develop the following key features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**STT and TTS**: Integrated using **Google Text-to-Speech** (**gTTS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning endpoint**: Implementing a decision tree classifier for
    predictive capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image generation and analysis**: Powered by OpenAI’s DALL-E and GPT-4o models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoT reasoning**: Orchestrating tasks, functions, and extensions, thus providing
    GenAISys with explicit machine (not human) reasoning abilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we are adding several new functions, including reasoning functionality
    (CoT), we will introduce only a single new package installation, gTTS, to minimize
    complexity in this chapter. Our primary focus remains on building a reliable architecture
    with optimal dependency management. To begin, let’s explore the updated elements
    of the IPython interface and the enhancements to the AI agent.
  prefs: []
  type: TYPE_NORMAL
- en: IPython interface and AI agent enhancements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The GenAISys architecture we’ve developed can now be viewed as comprising three
    interconnected layers, as shown in *Figure 5.3*. These enhancements blur the lines
    between orchestration, control, and agent functionality, as these roles are now
    distributed across multiple layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layer 1 (IPython interface)** manages user and system inputs through event-driven
    widgets, orchestrating tasks based on user interactions (inputs and checkboxes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 2 (AI agent)** controls the generative AI models (in our case, OpenAI
    models) and can trigger a CoT reasoning sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 3 (functions and agents)** contains functions triggered by the AI agent.
    Notably, the CoT function itself acts as an agent, capable of orchestrating generative
    AI tasks, machine learning, and additional functions as needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.3: The three layers of the event-driven GenAISys](img/B32304_05_3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: The three layers of the event-driven GenAISys'
  prefs: []
  type: TYPE_NORMAL
- en: This high-level architecture integrates orchestrators, controllers, and agents,
    each broken down into specific Python functionalities. Let’s start by exploring
    **Layer 1**, the IPython interface, from a functional standpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer 1: IPython interface'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The IPython interface now incorporates three new features (highlighted in yellow
    in *Figure 5.4*): a voice widget, a file-handling widget, and a dedicated reasoning
    interface triggered by user inputs and AI agent activities. These enhancements
    bring the interface total to six interactive widgets and functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Voice, file, and reasoning features are added to the IPython
    interface](img/B32304_05_4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Voice, file, and reasoning features are added to the IPython interface'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through each widget and function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User selection** remains as designed in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110).
    It is central to the collaborative design of the GenAISys and remains unchanged.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**User input** is also retained from [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110)
    without modification; this widget remains central for capturing user prompts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **AI agent**, as described in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110),
    activates or deactivates the generative AI agent (`chat_with_gpt`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **voice widget** enables voice-based interactions through STT and TTS.
    We’re using cost-free, built-in functionality for STT:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Windows**: Press the Windows key + *H*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**macOS**: Enable **Dictation** under **Keyboard settings** and choose a custom
    shortcut'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For TTS, the gTTS service is utilized and controlled via a checkbox set to
    `False` by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If the AI agent’s checkbox is checked, then the TTS function is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting MP3 file (`response.mp3`) is automatically played in the `update_display()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**The files widget** is a new widget that activates file management. It will
    display images generated and saved by the generative AI model (DALL-E) triggered
    in the AI agent function, `chat_with_gpt`. It is controlled via another checkbox,
    initially set to `False`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If an image exists, it is displayed with the **Python Image Library** (**PIL**)
    in the `update_display()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Reasoning activated** is another new widget of the GenAISys. The user input
    will trigger an event in the AI agent, and that, in turn, will trigger a CoT reasoning
    process. The reasoning interface will display the thought process of the CoT in
    real time. The reasoning output widget is created at the start of a session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The widget will receive outputs from the CoT process and display them independently
    from `VBox` and persistently in the `update_display()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `VBox` interface now contains all interactive widgets, including the newly
    added TTS and files widgets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the length and complexity of responses from the AI agent (especially
    during CoT processes), we introduced an enhanced formatting feature using Markdown.
    The `update_display()` function now formats entries clearly, calling a dedicated
    formatting function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `format_entry(entry)` function formats the user’s (blue) and assistant’s
    (green) responses, ensuring readability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This design emphasizes that the IPython interface (**Layer 1**) is purely to
    orchestrate user interactions and trigger underlying layers of functions and agents.
    This architecture ensures that you have the flexibility you need if you want to
    call the functions and agents directly without a user interface.
  prefs: []
  type: TYPE_NORMAL
- en: With the IPython interface described, let’s explore the enhanced capabilities
    in **Layer 2**, the AI agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer 2: AI agent'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AI agent invoked by the IPython interface in **Layer 1** remains the `chat_with_gpt`
    function, reinforcing the conversational nature of GenAISys. With the introduction
    of reasoning capabilities, the conversation can now occur directly between AI
    agents as well.
  prefs: []
  type: TYPE_NORMAL
- en: The `chat_with_gpt` function has been expanded with several new features. If
    necessary, review the core functionalities described in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the new enhancements added to the AI agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '`continue_functions=True` has been introduced at the beginning of the function
    to ensure that only one requested task is executed at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`continue_functions` is set to `False` at the end of the Pinecone query process,
    triggered by the presence of the `Pinecone` keyword in the user message. This
    stops any additional unintended task executions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The new function, `reason.chain_of_thought_reasoning`, described later, in
    the *Reasoning with CoT* section, is called under specific conditions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `continue_functions==True` condition ensures the reasoning function is
    called with the initial user query. A sample customer activities file is also
    downloaded as part of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the example use case for this chapter, a team can automatically access and
    query a regularly updated customer activity data source. The sample file provided
    contains 10,000 records of historical customer activities, including customer
    IDs, locations, activity types, and activity ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5: The customer ratings of historical sites](img/B32304_05_5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: The customer ratings of historical sites'
  prefs: []
  type: TYPE_NORMAL
- en: 'A decision tree classifier later utilizes this dataset within the CoT reasoning
    function to predict the most popular customer activity. Once the response is generated,
    it is added to the output, and `continue` is set to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The new function, `reason.generate_image`, that we will implement in the *Image
    generation and analysis* section has also been integrated. It is called as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The generated image URL is returned, and the image itself is downloaded and
    saved locally for display or further processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A corresponding message is added to the output, and the `continue` flag is
    set to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The function previously known as `openai_api.make_openai_api_call` is now renamed
    `reason.make_openai_api_call`. It maintains the same functionality as in [*Chapter
    4*](Chapter_4.xhtml#_idTextAnchor110) but is now part of the GenAISys reasoning
    library. The memory management `if user_memory…else` condition, which takes the
    complete user history or just the present user message into account, has been
    updated with explicit conditions that check both the state of `user_memory` and
    the `continue_functions` flag:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The AI agent thus acts as an intermediate orchestrator, calling and managing
    the execution of lower-layer functions rather than executing them directly. The
    Pinecone interface remains the top layer that invokes the AI agent, which in turn
    interacts with the specific functions within **Layer 3**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer 3: Functions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this layer, our focus is on the new functionalities introduced to enable
    advanced reasoning through the CoT cognitive agent. Pinecone indexing and standard
    OpenAI calls remain as implemented in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110).
    The primary additions in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image generation and analysis** using DALL-E and GPT-4o, respectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoT reasoning**, which introduces a cognitive agent capable of orchestrating
    tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voice interaction capabilities** enabled through gTTS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A machine learning endpoint** leveraging a decision tree classifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will explore these functionalities in the upcoming sections of this chapter,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The environment setup and initialization for gTTS and machine learning are detailed
    in the *Setting up the environment* section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image functionalities are covered in the *Image generation and analysis* section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reasoning orchestration is built in the *Reasoning with CoT* section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, our enhanced three-layer GenAISys will have new,
    robust capabilities designed to expand even further in subsequent chapters. Let’s
    now dive deeper into these enhancements, beginning with the environment setup.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will enhance, expand, and rearrange the environment previously
    built to finalize the GenAISys framework. These changes are essential for the
    upcoming use cases in subsequent chapters. Open the `Multimodal_reasoning_with_Chain_of_Thought.ipynb`
    notebook within the Chapter05 directory on GitHub ([https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main](https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding package installations, the *Setting up the environment* section in
    the notebook remains largely unchanged from the previous chapter (`Event-driven_GenAISys_framework.ipynb`),
    with just one new addition: *Google Text-to-Speech (gTTS)*.'
  prefs: []
  type: TYPE_NORMAL
- en: However, several significant updates have been made to support the CoT generative
    AI reasoning features. Let’s examine each of these updates, starting with the
    *OpenAI* section.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first two files we download remain the same as in previous chapters. The
    third and fourth files, however, are new and have been added to support advanced
    functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`reason.py` now contains the generative AI library with the functions built
    in the previous chapters and the ones we are adding in this chapter. These functions
    in the generative AI library and their status are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`make_openai_api_call(input, mrole,mcontent,user_role)` is a general-purpose
    OpenAI API call described in the *Setting up the environment* section of [*Chapter
    1*](Chapter_1.xhtml#_idTextAnchor021). It is now imported as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`image_analysis` is the image analysis function that can describe an image
    or use the image as a starting point to generate content such as a story. This
    function is described in the *Image generation and analysis* section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_image` is a new function that generates images with DALL-E, detailed
    in the *Image generation and analysis* section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chain_of_thought_reasoning` is a new CoT logic function of the GenAISys we
    are building. We will implement it in the *Reasoning with CoT* section of this
    chapter. It can call functions from other libraries, such as `machine_learning`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`machine_learning.py` will now contain a decision tree classifier in a function
    named `ml_agent`. The function takes two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In our example use case, `feature1_value` will represent a customer location,
    and `feature2_column` will represent customer activities. The `ml_agent` classifier
    will predict the most popular customer activity for a specific location based
    on historical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We import `ml_agent` from `machine_learning.py` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The remaining OpenAI setup subsections, including package installation and API
    key initialization, remain identical to previous chapters. Let’s now initialize
    our new functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing gTTS, machine learning, and CoT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will initialize the following new functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**gTTS** is installed with `!pip install gTTS==2.5.4`, which is an open source,
    free TTS library that fits prototyping purposes: [https://pypi.org/project/gTTS/](https://pypi.org/project/gTTS/).
    `` `click` ``, a command-line library, is required for gTTS. The first cell checks
    if we wish to use gTTS by setting `use_gtts` to `True`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second cell of the notebook will check for and set up the correct `` `click`
    `` version if `use_gtts` is set to `True`. If an update is needed, it will then
    display a clear message in the notebook output prompting you to manually restart
    the runtime. After restarting, simply click `` `Run All` `` to continue. The code
    will display an HTML message to restart if the version is updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If `use_gtts` is set to `True`, we install gTTS and define a TTS conversion
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This function will be activated in the IPython interface when the AI agent
    returns a response, as explained earlier in the *Layer 1: IPython interface* section.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The ml_agent algorithm endpoint** is imported from `machine_learning.py`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This decision tree classifier function will predict popular customer activities
    based on historical data, enhancing our GenAISys’s predictive capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '**The CoT reasoning** framework is imported from `reason.py`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The Pinecone installation, initialization, and queries are then defined as explained
    in *Chapters 3* and *4*. Take some time to revisit those chapters if needed, as
    we will reuse the functions previously developed. We’re now prepared to build
    the image generation and analysis functions.
  prefs: []
  type: TYPE_NORMAL
- en: Image generation and analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will begin by creating a flexible image generation function
    using OpenAI’s DALL-E model. Following that, we’ll build a function for image
    analysis. The objective is to enhance GenAISys with computer vision capabilities
    while preserving its responsive, event-driven functionality, as illustrated in
    *Figure 5.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6: Generating images with flexible event-driven triggers](img/B32304_05_6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Generating images with flexible event-driven triggers'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure is an evolution of the architecture we first developed
    in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110). It has been augmented to include
    new capabilities: activation of speech (voice) features, management of image files,
    enhanced display functionality, and reasoning through CoT. In this section, our
    focus will specifically be on integrating and demonstrating computer vision capabilities
    alongside the enhanced display functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The image generation and analysis processes are designed to be flexible:'
  prefs: []
  type: TYPE_NORMAL
- en: No mandatory selection or explicit widget activation is required for image generation
    or analysis. We could easily add explicit widgets labeled **Image Generation**
    or **Image Analysis** if a use case demands it. However, the approach we’re adopting
    here is intentionally flexible, paving the way for integration within more complex,
    automated reasoning workflows such as CoT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Files** checkbox widget serves two distinct purposes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *unchecked*, an image is generated by DALL-E, saved to a file, but not displayed.
    This allows images to be generated quietly in the background for later use or
    storage.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If *checked*, the generated or analyzed image will be displayed in the user
    interface, as illustrated in *Figure 5.7*.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The AI conversational agent automatically activates image generation or analysis
    based on user prompts. These vision capabilities can also trigger automated reasoning
    processes, enabling the system to execute comprehensive CoT tasks seamlessly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that the display will only display image files if the **Files** widget
    is checked. Let’s now dive deeper into how these vision features are integrated
    within the GenAISys interface. Specifically, we’ll demonstrate the scenario where
    the **Files** checkbox is activated (checked), as depicted in *Figure 5.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7: The files checkbox is checked so that the image will be displayed](img/B32304_05_7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: The files checkbox is checked so that the image will be displayed'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the **Files** checkbox selected, the image generated by DALL-E in response
    to the user’s prompt will be immediately displayed, as shown in *Figure 5.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8: Entering a prompt and displaying the image generated](img/B32304_05_8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Entering a prompt and displaying the image generated'
  prefs: []
  type: TYPE_NORMAL
- en: If the **Files** option is not checked, the image will be generated and saved
    but not displayed. Similarly, image display functionality also applies to analyzing
    images downloaded from external sources. When the **Files** checkbox is unchecked,
    the analysis runs without visually displaying the image. We are now ready to examine
    the implementation details of the image generation function.
  prefs: []
  type: TYPE_NORMAL
- en: Image generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The function to generate an image is located in the custom generative AI library,
    `reason.py`, in the `commons` directory. A user prompt or a CoT framework can
    trigger this function. The name of the function is `generate_image`, and it takes
    five arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The five arguments are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt`: The query related to the image that is provided by the user or the
    system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`: The OpenAI model to use. In this case, the default value is `gpt-4o`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size`: The size of the image. The default size of the image is `1024x1024`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quality`: Defines the quality of the image. The default value is `standard`,
    which costs less than the higher-quality `hd` option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n`: Defines the number of images to generate. The default value is `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function returns the URL of the generated image. The code first initializes
    the OpenAI client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The DALL-E model is then called via the OpenAI API with the specified parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The parameters are described in detail in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021)
    in the *Setting up the environment* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the content, messages, and parameters are defined, the OpenAI API is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The URL of the image is extracted from `response` and returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Once an image has been generated or retrieved, we can choose to display or analyze
    it, depending on our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Image analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The function to analyze an image is also located in the custom generative AI
    library, `reason.py`, in the `commons` directory. This function, named `image_analysis`,
    is defined as follows, and takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The three arguments are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`image_path_or_url (str)`: The path to access a local image file or the URL
    of the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`query_text (str)`: The query related to the image that is provided by the
    user or the system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model (str)`: The OpenAI model to use. In this case, the default value is
    `gpt-4o`, which possesses vision capabilities(generation and analysis).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function initializes the content structure for the API call with the provided
    query text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The function then searches for the image in a URL or a local file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'If the image is in a URL, it is appended to the content. If the image is a
    local file, it is encoded in Base64 and formatted as a UTF-8 string. This format
    enables embedding the image data within text-based systems (such as JSON or HTML).
    A data URL is then created and appended to the content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The OpenAI message is created with the context that contains the query information
    and the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The API call includes a set of standard parameters, detailed in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021)
    (in the *Setting up the environment* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the content, messages, and parameters are defined, the OpenAI API is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'For further integration, particularly with RAG using Pinecone in [*Chapter
    6*](Chapter_6.xhtml#_idTextAnchor166), the response is saved as text in a file.
    This enables subsequent use and retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This `image_analysis` function will also be called by the CoT reasoning process
    built later in this chapter, where `query_text` will be dynamically created and
    passed into the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We now have fully functional computer vision components integrated into our
    GenAISys. With these capabilities, we are ready to build the CoT reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning with CoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The exponential acceleration of global markets has led to billions of micro-tasks
    being generated daily across platforms such as social media, e-marketing sites,
    production lines, and SaaS platforms. Without robust automation, keeping pace
    with these real-time demands is impossible. Speed and efficiency have become paramount,
    requiring tasks to be executed in real time or near-real time. Recent advances
    in AI have significantly helped us adapt to these market paradigms, where we must
    handle an increasing volume of tasks in increasingly shorter timeframes. However,
    as we increase the number and scope of AI functions to solve problems, it is becoming
    confusing for users to run complex scenarios with copilots. It is also quite challenging
    for a team of developers to create a GenAISys that contains the functions they
    need and includes a clear and intuitive sequence of operations for problem-solving.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we address these challenges by implementing CoT reasoning.
    CoT reasoning breaks complex tasks into smaller, more manageable steps where the
    output of one step becomes the input for the next. This process mimics (without
    replacing) human-like reasoning. It reduces cognitive overload for users, allowing
    them to focus primarily on decision-making. Additionally, CoT reasoning makes
    the AI agent’s internal thought process transparent, providing real-time explainability
    of each reasoning step.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this section is to build a CoT reasoning process using Python, leveraging
    the flexible and interactive GenAISys framework we’ve developed. Specifically,
    we will apply CoT to simulate customer-preference analysis for an online travel
    platform, generate creative suggestions for activities, produce images using DALL-E,
    and create storytelling narratives based on these images with GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, a CoT cognitive agent might seem similar to traditional sequences
    of functions found in classical software development. Hence, let’s first clarify
    the important distinctions between them before we dive into the code.
  prefs: []
  type: TYPE_NORMAL
- en: CoT in GenAISys versus traditional software sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Seasoned software developers are used to implementing complex sequences of
    functions. To bridge the conceptual gap between traditional software sequences
    and cognitive CoT reasoning (which mimics rather than replaces human cognition),
    let’s first distinguish their purposes clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: A **traditional sequence** of non-AI or AI functions consists of a series of
    steps executed independently, following a black-box model in which the output
    of one function serves as the static input of the next.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a **CoT reasoning process**, the steps mimic human-like reasoning. Each step
    goes beyond a simple function and follows a logical progression. Each new process
    builds on the output of the previous step, as we will see when we implement CoT.
    We will observe the GenAISys’s “thinking process” displayed in real time through
    our interactive interface. The process is transparent and explainable, as it is
    visualized in real time within the IPython interface. We can see what the system
    is doing and isolate any function to investigate the process if necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another critical aspect of CoT is its *intermediate reasoning*:'
  prefs: []
  type: TYPE_NORMAL
- en: Each step in a CoT process builds on the previous one, but not all steps are
    static. For instance, when DALL·E generates an image, it creates something entirely
    new—not retrieved from a database. This relies on a generative AI model, not pre-programmed
    content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step in the process isn’t pre-generated, like a fixed list of messages.
    For example, when DALL-E generates an image, we will ask GPT-4o to perform a storytelling
    task that it will invent *ex nihilo* based on the input it received. Alternatively,
    we could ask GPT-4o to simply describe the image—without needing to change or
    fine-tune the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoT reasoning offers *cognitive alignment* closer to human thinking patterns.
    We humans break monolithic problems into smaller parts, process each part, and
    then assemble the intermediate conclusions to reach a global solution. The human-like
    framework of the CoT process we are building in this chapter makes the GenAISys
    more intuitive and creative, mimicking (not replacing) human problem-solving methods.
    In the following chapters, notably in [*Chapter 6*](Chapter_6.xhtml#_idTextAnchor166),
    we’ll further expand and enhance the CoT reasoning capabilities. The takeaway
    here is that CoT involves sequences of tasks, but in a more flexible and creative
    way than classical non-AI or AI sequences. Let’s move on and define the cognitive
    flow of CoT reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Cognitive flow of CoT reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of the traditional term flowchart, we’ll use the term *cognitive flow*
    to describe the CoT process we are implementing. This term emphasizes the human-like
    reasoning and dynamic problem-solving capabilities of our AI agent, differentiating
    clearly from classical software flowcharts. A classic flowchart provides a visual
    representation of a sequence of functions. A reasoning CoT cognitive flow or cognitive
    workflow maps the logical progression of the AI agent’s thought process from one
    step to another. The cognitive flow shows how the AI agent mimics human reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first walk through the cognitive flow we will implement in Python, visualized
    in *Figure 5.9*. The Python functions we’ll use reside in `reason.py`, located
    in the `commons` directory, and are described in detail in the *OpenAI* subsection
    of this chapter’s *Setting up the environment* section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9: Cognitive flow of the CoT process](img/B32304_05_9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Cognitive flow of the CoT process'
  prefs: []
  type: TYPE_NORMAL
- en: The cognitive flow for our CoT reasoning process consists of five main phases,
    orchestrated by the `chain_of_thought_reasoning()` function. This sequence begins
    with **Start**.
  prefs: []
  type: TYPE_NORMAL
- en: Start
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The CoT reasoning process begins when it receives input text provided by the
    AI agent. The AI agent analyzes the user input and then triggers the CoT function,
    as described earlier in the *Layer 2: AI agent* section. At the start of the CoT
    function, two key initializations occur: the reasoning memory (`steps = []`) is
    initialized, and the reasoning display widget is activated within the IPython
    interactive interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`display(reasoning_output)` triggers the `display` widget, which enables real-time
    updates in the interactive IPython interface, ensuring the CoT process remains
    transparent and easily interpretable by users.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: ML-baseline'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step, **ML-baseline**, activates the machine learning endpoint (`machine_learning.ml_agent()`).
    It utilizes a decision tree classifier to analyze customer data dynamically and
    predict activities of interest. The function takes a location (for example, `"Rome"`)
    and `"ACTIVITY"` as the target column for the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This block of code is repeated for each reasoning step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each part of the thought process begins with a comment like so: `# Step 1:
    Analysis of the customer database and prediction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`steps.append("Process: Performing machine learning analysis of the customer
    database. \n")` appends a description of the step to the reasoning memory step
    list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with reasoning_output` initiates a code block for the display widget'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reasoning_output.clear_output(wait=True)` clears `reasoning_output t`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print(steps[-1]) # Print the current step` prints the most recent step added'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time.sleep(2) # processing time` introduces a two-second delay'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`result_ml =machine_learning.ml_agent("Rome", "ACTIVITY")` calls `ml_agent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`steps.append(f"Machine learning analysis result: {result_ml}")` appends the
    result returned by the machine learning function to the list of steps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from `machine_learning.ml_agent`, which predicts the top customer-preferred
    activity for the location `"Rome"`, becomes the input for the subsequent step,
    suggesting creative activities.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the next step, let’s briefly explore the underlying decision
    tree classifier inside `machine_learning.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree classifier
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A decision tree classifier is well suited for our task because it is a machine
    learning model that makes predictions by splitting data into a tree-like structure
    based on feature values. It works by recursively choosing the optimal feature
    at each split until it reaches a defined stopping condition, such as a maximum
    depth or a minimum sample size per leaf. At each step, the possibilities narrow
    down until a single prediction emerges.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run it, we first import the required libraries for handling data and building
    the decision tree. We also disable warnings to avoid cluttering the IPython output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define our classifier function, `ml_agent()`, with two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The two parameters are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature1_value`: The value of the location we want to predict activities for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature2_column`: The target column (`"ACTIVITY"`) we want to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function starts by loading the customer activities dataset into a pandas
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we encode the categorical variables (`LOCATION` and `ACTIVITY`) for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If no specific location (`feature1_value`) is provided, the function selects
    the most frequent location by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We then prepare the features (`X`) and the target variable (`y`) from our encoded
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'With our data prepared, we train the decision tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Setting `random_state=42` ensures consistent results each time we run the code.
    Now, we encode the provided (or default) location input to prepare it for prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The Python `.transform` method on the `le_location` object converts the categorical
    string into its unique integer code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function is now ready to predict the most probable activity and convert
    it back to its original label. We will use the Python `.predict` method of our
    trained model to see what it predicts for this new data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the function constructs a customer’s descriptive output message tailored
    to the predicted activity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This descriptive output is returned to the CoT function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'To invoke the classifier from the CoT function, we use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re letting the classifier find the location and activity. The expected output,
    in this case, will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now use the output of this step to suggest activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Suggest activities'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This step follows the same logic and structure as *Step 1*. The name of the
    process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from *Step 1* (`result_ml`) becomes part of the instruction sent
    to GPT-4o to augment the input context. The combined query (`umessage`) for GPT-4o
    becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'At this stage, the instructions are tailored specifically for our travel-focused
    domain. In [*Chapter 6*](Chapter_6.xhtml#_idTextAnchor166), we’ll evolve these
    instructions to become dynamic event-based variables. Here, we continue using
    the established GenAISys OpenAI API call we built in earlier chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The output received from GPT-4o (`task_response`) will serve as the input for
    the next step (*Step 3*). The method of appending and displaying the reasoning
    steps remains consistent with *Step 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Generate image'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This step begins by taking the detailed suggestion received from the previous
    step (`task_response`) and passing it directly as the prompt to DALL-E’s image
    generation function. The structure and logic here are consistent with the previous
    steps, now focused on generating images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Once generated, the image is downloaded and saved locally as `c_image.png`.
    This image file will then be displayed through the IPython interface if the **Files**
    widget is checked, as explained in the *Layer 1: IPython interface* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: With the image now generated and saved, the CoT process advances to analyzing
    this newly created image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Analyze image'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The input for this analysis step is the URL of the image generated in *Step
    3*, stored as `image_url`. As mentioned earlier, in this notebook, the query text
    is currently set as a generic, yet travel-specific, request to GPT-4o. In subsequent
    chapters, this query text will become event-driven and more dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our image analysis, we instruct the generative AI model to craft an engaging
    story based on the generated image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The code encapsulating the instructions is the same as in the previous steps.
    The CoT function now activates the `image_analysis` function as described previously
    in the *Image generation and analysis* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The output is returned to the `response` variable and saved in the `image_text.txt`
    file for further use. This marks the completion of the CoT reasoning steps.
  prefs: []
  type: TYPE_NORMAL
- en: End
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Upon completing all reasoning tasks, the CoT function signals the end of the
    process by clearing and updating the IPython display:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The IPython interface takes over from here. Let’s now run the CoT from a user
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Running CoT reasoning from a user perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll seamlessly run the complex GenAISys we’ve been building
    since the beginning of the book. A single prompt will trigger the entire CoT process.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll simulate a user activating the reasoning capabilities of the GenAISys
    to obtain comprehensive ideation for an online travel agency. Specifically, we
    aim to predict customer-preferred activities, generate engaging images, and create
    storytelling narratives to evoke customers’ episodic memories. These episodic
    memories might be real-world experiences or dreams of visiting a place and engaging
    in particular activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this scenario, make sure to check the **AI Agent** and **Files** checkboxes
    and enter the following prompt carefully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The `Use`, `reasoning`, `customer`, and `activities` keywords will be recognized
    by the AI agent and trigger the CoT process we built in this chapter. Alternatively,
    we could have implemented a drop-down menu or performed a similarity search in
    the Pinecone index to retrieve specific instruction scenarios. STT input is also
    possible. In this chapter, however, we’ll use typed prompts with keywords to clearly
    illustrate the CoT process.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](Chapter_7.xhtml#_idTextAnchor191), we’ll build a central keyword
    registry and an orchestrator to further optimize the AI agent’s decision-making
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Once the user presses *Enter*, all we have to do is sit back and watch just
    as we would with online ChatGPT-like copilots. The first process is to analyze
    the customer base to find the top-ranking activity based on daily data, as shown
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10: Searching for activities](img/B32304_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Searching for activities'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the whole process is complete, the decision tree classifier returns the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The next stage involves searching for suitable activities matching customer
    preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11: Searching for activities matching customer needs](img/B32304_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Searching for activities matching customer needs'
  prefs: []
  type: TYPE_NORMAL
- en: 'The creative output from GPT-4o provides structured steps to enhance the online
    offerings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the CoT instructs DALL-E to generate an engaging image based on these
    suggested activities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12: Image generation based on the output of the previous step](img/B32304_05_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Image generation based on the output of the previous step'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the **Files** checkbox is checked, the generated image is displayed.
    This image is a rather creative one and will vary with each run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13: A cultural and historical image](img/B32304_05_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: A cultural and historical image'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the image contains text such as `…understanding of history and
    its impact on modern life.`, which perfectly fits our request.
  prefs: []
  type: TYPE_NORMAL
- en: Note that each run might produce a different output due to context variations
    and the stochastic (probabilistic) nature of generative AI models such as GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next process involves asking GPT-4o to create a narrative for a storytelling
    promotion that leverages episodic memory of past real-life experiences or imagined
    trips:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14: Creating an engaging story based on the image generated](img/B32304_05_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Creating an engaging story based on the image generated'
  prefs: []
  type: TYPE_NORMAL
- en: 'The narrative output from GPT-4o, shown, is illustrative and will vary, as
    noted earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the CoT sequence concludes, the GenAISys maintains its reasoning state,
    waiting for new standalone prompts or further CoT runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15: Reasoning is persistently activated in the GenAISys](img/B32304_05_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: Reasoning is persistently activated in the GenAISys'
  prefs: []
  type: TYPE_NORMAL
- en: The *Load and display the conversation history* and *Load and summarize the
    conversation history* sections in the notebook utilize the same functions detailed
    in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110).
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now successfully built a small-scale ChatGPT-like GenAISys equipped with
    custom features, including multi-user support, domain-specific RAG, and tailored
    CoT capabilities. In the upcoming chapters, we’ll apply this GenAISys framework
    across several practical business domains.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have completed the basic framework of the GenAISys, consisting
    of three layers. The first layer is an IPython interactive interface that acts
    as an orchestrator. It now includes voice capability, file display, and CoT features,
    alongside user inputs, user selections, and the AI agent widget.
  prefs: []
  type: TYPE_NORMAL
- en: The second layer is the AI agent orchestrator, triggered by user prompts. This
    demonstrates that within the GenAISys, the boundaries between orchestration and
    control functions are somewhat blurred due to the interactive nature of these
    components. The AI agent distributes tasks between the Pinecone index for querying
    and the OpenAI API agent for generative tasks, such as content and image generation.
    The AI agent can also trigger the CoT process, and we will further enhance its
    capabilities in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The third and final layer contains the core functionality of the GenAISys, which
    involves AI workers powered by GPT-4o and DALL-E. In this chapter, we introduced
    DALL-E for image generation and utilized GPT-4o to provide insightful comments
    on these images. Additionally, we implemented a decision tree classifier to predict
    customer activities, incorporating machine learning capabilities into our GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the CoT feature marked our initial step toward creating seamless
    reasoning capabilities from an end user perspective. Complex tasks require sophisticated
    AI systems that can emulate human reasoning. Therefore, we will expand upon the
    reasoning abilities of the GenAISys, among other features, in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The seamless interface of an online generative AI system shows that the system
    is easy to build. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting a **large language model** (**LLM**) is sufficient to build a GenAISys.
    (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A generative AI application requires an event-driven interactive interface.
    (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An AI system can mimic human reasoning. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A **chain-of-thought** (**CoT**) process is just a sequence of classical functions.
    (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A CoT can process natural language but not computer vision. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A CoT is a cognitive agent that can include non-AI or AI functions. (True or
    False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reasoning GenAISys can group a set of tasks for an end user. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The continual acceleration of the economy requires more automation, including
    AI. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A human-centric reasoning GenAISys can boost the productivity of a team. (True
    or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Chan, Andy, Cassidy Ezell, Michael Kaufmann, Kevin Wei, Laurel Hammond, Hunter
    Bradley, Elliot Bluemke, Nandhini Rajkumar, David Krueger, Nikita Kolt, Lukas
    Heim, and Markus Anderljung. “Visibility into AI Agents.” In Proceedings of the
    2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘24),
    Rio de Janeiro, Brazil, June 3–6, 2024\. New York: ACM, 2024\. [https://arxiv.org/pdf/2401.13138](https://arxiv.org/pdf/2401.13138).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Putta, Praveen, Eric Mills, Naman Garg, Soham Motwani, Chelsea Finn, Divyansh
    Garg, and Rohan Rafailov. “Agent Q: Advanced Reasoning and Learning for Autonomous
    AI Agents.” Last modified 2024\. [https://arxiv.org/abs/2408.07199](https://arxiv.org/abs/2408.07199).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wiesinger, Jannis, Peter Marlow, and Vladimir Vuskovic. “Agents.” Kaggle Whitepaper.
    Accessed July 8, 2025\. [https://www.kaggle.com/whitepaper-agents](https://www.kaggle.com/whitepaper-agents).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI. OpenAI API Documentation. Accessed July 8, 2025\. [https://platform.openai.com/docs/api-reference/introduction](https://platform.openai.com/docs/api-reference/introduction).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Unlock this book’s exclusive benefits now
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scan this QR code or go to [packtpub.com/unlock](http://packtpub.com/unlock),
    then search for this book by name. | ![A qr code on a white background  AI-generated
    content may be incorrect.](img/Unlock.png) |
  prefs: []
  type: TYPE_NORMAL
- en: '| *Note: Keep your purchase invoice ready before you start.* |'
  prefs: []
  type: TYPE_TB
