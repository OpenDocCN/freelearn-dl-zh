- en: Deep Learning for NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在自然语言处理中的应用
- en: n the previous chapter, we used classic machine learning techniques to build
    our text classifiers. In this chapter, we will replace those with deep learning
    techniques via the use of **recurrent neural networks** (**RNN**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用经典的机器学习技术来构建我们的文本分类器。在本章中，我们将通过使用**循环神经网络**（**RNN**）来替换这些技术。
- en: In particular, we will use a relatively simple bidirectional LSTM model. If
    this is new to you, keep reading – if not, please feel free to skip ahead!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将使用一个相对简单的双向LSTM模型。如果你对此不熟悉，请继续阅读——如果你已经熟悉，请随意跳过！
- en: The dataset attribute of the batch variable should point to the `trn` variable
    of the `torchtext.data.TabularData` type. This is a useful checkpoint to understand
    how data flow differs in training deep learning models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 批变量数据集属性应指向`torchtext.data.TabularData`类型的`trn`变量。这是理解训练深度学习模型中数据流差异的有用检查点。
- en: Let's begin by touching upon the overhyped terms, that is, *deep* in deep learning
    and *neural* in deep neural networks. Before we do that, let's take a moment to
    explain why I use PyTorch and compare it to Tensorflow and Keras—the other popular
    deep learning frameworks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先谈谈被过度炒作的术语，即深度学习中的“深度”和深度神经网络中的“神经”。在我们这样做之前，让我们花点时间解释为什么我使用PyTorch，并将其与Tensorflow和Keras等其他流行的深度学习框架进行比较。
- en: I will be building the simplest possible architecture for demonstrative purposes
    here. Let's assume a general familiarity with RNNs and not introduce the same
    again.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我将构建尽可能简单的架构。让我们假设大家对循环神经网络（RNN）有一般的了解，不再重复介绍。
- en: 'In this chapter, we will answer the following questions:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回答以下问题：
- en: What is deep learning? How does it differ from what we have seen already?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是什么？它与我们所看到的不同在哪里？
- en: What are the key ideas in any deep learning model?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何深度学习模型中的关键思想是什么？
- en: Why PyTorch?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择PyTorch？
- en: How do we tokenize text and set up dataloaders with torchtext?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用torchtext对文本进行标记化并设置数据加载器？
- en: What are recurrent networks, and how can we use them for text classification?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是循环网络，我们如何使用它们进行文本分类？
- en: What is deep learning?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习是什么？
- en: 'Deep learning is a subset of machine learning: a new take on learning from
    data that puts an emphasis on learning successive layers of increasingly meaningful
    representations. But what does the *deep* in deep learning mean?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集：一种从数据中学习的新方法，它强调学习越来越有意义的表示的连续层。但深度学习中的“深度”究竟是什么意思呢？
- en: <q>"The deep in deep learning isn't a reference to any kind of deeper understanding
    achieved by the approach; rather, it stands for this idea of successive layers
    of representations."</q>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <q>"深度学习中的‘深度’并不是指通过这种方法获得的任何更深层次的理解；相反，它代表的是这种连续层表示的想法。”</q>
- en: – F. Chollet, Lead Developer of Keras
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: – Keras的主要开发者F. Chollet
- en: The depth of the model is indicative of how many layers of such representations
    we use. F Chollet suggested layered representations learning and hierarchical
    representations learning as better names for this. Another name could have been
    differentiable programming.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的深度表明我们使用了多少层这样的表示。F Chollet建议将分层表示学习和层次表示学习作为更好的名称。另一个可能的名称是可微分编程。
- en: The term *differentiable programming*, coined by Yann LeCun, stems from the
    fact that what our *deep learning methods* have in common is not more layers—it's
    the fact that all of these models learn via some form of differential calculus
    – most often stochastic gradient descent.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: “可微分编程”这个术语是由Yann LeCun提出的，源于我们的深度学习方法共有的不是更多的层——而是所有这些模型都通过某种形式微分计算来学习——通常是随机梯度下降。
- en: Differences between modern machine learning methods
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代机器学习方法的差异
- en: The modern machine learning methods that we have studied shot to being mainstream
    mainly in the 1990s. The binding factor among them was that they all use one layer
    of representations. For instance, decision trees just create one set of rules
    and apply them. Even if you add ensemble approaches, the *ensembling* is often
    shallow and only combines several ML models directly.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所研究的现代机器学习方法主要在20世纪90年代成为主流。它们之间的联系在于它们都使用一层表示。例如，决策树只创建一组规则并应用它们。即使你添加集成方法，集成通常也很浅，只是直接结合几个机器学习模型。
- en: 'Here is a better-worded interpretation of these differences:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对这些差异的更好表述：
- en: '"Modern deep learning often involves tens or even hundreds of successive layers
    of representations – and they’re all learned automatically from exposure to training
    data. Meanwhile, other approaches to machine learning tend to focus on learning
    only one or two layers of representations of the data; hence, they’re sometimes
    called shallow learning."'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: “现代深度学习通常涉及十几个甚至上百个连续的表示层——而且它们都是通过接触训练数据自动学习的。与此同时，其他机器学习的方法倾向于只学习数据的一层或两层表示；因此，它们有时被称为浅层学习。”
- en: – F Chollet
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: – F Chollet
- en: Let's look at the key terms behind deep learning, since this way we might come
    across some key ideas as well.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看深度学习背后的关键术语，因为这样我们可能会遇到一些关键思想。
- en: Understanding deep learning
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度学习
- en: In a loosely worded manner, machine learning is about mapping inputs (such as
    images, or *movie reviews*) to targets (such as the label cat or *positive*).
    The model does this by looking at (or training from) several pairs of input and
    targets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以宽松的方式来说，机器学习是关于将输入（如图像或*电影评论*）映射到目标（如标签猫或*正面*）。模型通过查看（或从多个输入和目标对进行训练）来完成这项工作。
- en: Deep neural networks do this input-to-target mapping using a long sequence of
    simple data transformations (layers). This sequence length is referred to as the
    depth of the network. The entire sequence from input-to-target is referred to
    as a model that learns about the data. These data transformations are learned
    by repeated observation of examples. Let's look at how this learning happens.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络通过一系列简单数据转换（层）来实现从输入到目标的映射。这个序列的长度被称为网络的深度。从输入到目标的整个序列被称为学习数据的模型。这些数据转换是通过重复观察示例来学习的。让我们看看这种学习是如何发生的。
- en: Puzzle pieces
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拼图碎片
- en: We are looking at a particular subclass of challenges where we want to learn
    an input-to-target mapping. This subclass is generally referred to as supervised
    machine learning. The word supervised denotes that we have target for each input.
    Unsupervised machine learning includes challenges such as trying to cluster text,
    where we do not have a target.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在研究一个特定的子类挑战，我们想要学习一个输入到目标的映射。这个子类通常被称为监督机器学习。这个词监督表示我们为每个输入都有一个目标。无监督机器学习包括尝试聚类文本等挑战，我们并没有目标。
- en: 'To do any supervised machine learning, we need the following in place:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行任何监督机器学习，我们需要以下条件：
- en: '**Input Data:** Anything ranging from past stock performance to your vacation
    pictures'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入数据**：从过去的股票表现到你的度假照片'
- en: '**Target:** Examples of the expected output'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：期望输出的示例'
- en: '**A way to measure whether the algorithm is doing a good job:** This is necessary
    to determine the distance between the algorithm''s current output and its expected
    output'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**衡量算法是否做得好的方法**：这是确定算法当前输出与其期望输出之间距离所必需的'
- en: 'The preceding components are universal to any supervised approach, be it machine
    learning or deep learning. Deep learning in particular has its own cast of puzzling
    factors:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述组件对于任何监督方法都是通用的，无论是机器学习还是深度学习。特别是深度学习有其自己的一套令人困惑的因素：
- en: The model itself
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型本身
- en: The loss function
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数
- en: The optimizer
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器
- en: Since these actors are new to the scene, let's take a minute in understanding
    what they do.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些演员是新来的，让我们花一分钟时间了解他们做什么。
- en: Model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: Each model is comprised of several layers. Each layer is a data transformation.
    This transformation is captured using a bunch of numbers, called layer weights.
    This is not a complete truth though, since most layers often have a mathematical
    operation associated with them, for example, convolution or an affine transform.
    A more precise perspective would be to say that a layer is **parameterized** by
    its weights. Hence, we use the terms *layer parameters* and *layer weights* interchangeably.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型由几个层组成。每个层是一个数据转换。这种转换通过一串数字来捕捉，称为层权重。但这并不是完全的真理，因为大多数层通常与一个数学运算相关联，例如卷积或仿射变换。一个更精确的观点是，层是通过其权重**参数化**的。因此，我们交替使用术语*层参数*和*层权重*。
- en: The state of all the layer weights together makes the model state captured in
    model weights. A model can have anywhere between a few thousand to a few million
    parameters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所有层权重共同的状态构成了模型状态，即模型权重。一个模型可能有几千到几百万个参数。
- en: 'Let''s try to understand the notion of model **learning** in this context:
    learning means finding values for the weights of all layers in a network, so that
    the network will correctly map example inputs to their associated targets.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解在这个背景下模型**学习**的概念：学习意味着找到网络中所有层的权重值，以便网络能够正确地将示例输入映射到其相关目标。
- en: Note that this value set is for *all layers* in one place. This nuance is important
    because changing the weights of one layer can change the behavior and predictions
    made by the entire model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个值集是针对一个地方的*所有层*。这个细微差别很重要，因为改变一个层的权重可能会改变整个模型的行为和预测。
- en: Loss function
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'One of the pieces that''s used to set up a machine learning task is to assess
    how a model is doing. The simplest answer would be to measure the notional accuracy
    of the model. Accuracy has few flaws, though:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置机器学习任务时使用的组件之一是评估模型的表现。最简单的答案就是衡量模型的概念性准确度。虽然准确度有几个缺点：
- en: Accuracy is a proxy metric tied to validation data and not training data.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确度是一个与验证数据相关联的代理指标，而不是训练数据。
- en: Accuracy measures how correct we are. During training, we want to measure how
    far our model predicts from the target.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确度衡量我们有多正确。在训练过程中，我们希望衡量我们的模型预测与目标有多远。
- en: These differences mean that we need a different function to meet our preceding
    criteria. This is fulfilled by the *loss function* in the context of deep learning.
    This is sometimes referred to as an *objective function* as well.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异意味着我们需要一个不同的函数来满足我们之前的标准。在深度学习的背景下，这由损失函数来实现。这有时也被称为目标函数。
- en: <q>"The loss function takes the predictions of the network and the true target
    (what you wanted the network to output) and computes a distance score, capturing
    how well the network has done on this specific example.</q>"
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <q>"损失函数将网络的预测和真实目标（你希望网络输出的内容）计算出一个距离分数，捕捉网络在这个特定例子上的表现如何。</q>"
- en: <q>- From Deep Learning in Python by F Chollet</q>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <q>- 来自F Chollet的《Python深度学习》</q>
- en: This distance measurement is called the loss score, or simply loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种距离测量被称为损失分数，或简单地称为损失。
- en: Optimizer
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器
- en: This loss is automatically used as a feedback signal to adjust the way the algorithm
    works. This adjustment step is what we call learning.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失值自动用作反馈信号来调整算法的工作方式。这个调整步骤就是我们所说的学习。
- en: This automatic adjustment in model weights is peculiar for deep learning. Each
    adjustment or *update* of weights is made in a direction that will lower the loss
    for the current training pair (input, target).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在模型权重上的自动调整是深度学习特有的。每次权重调整或*更新*都是朝着降低当前训练对（输入，目标）的损失的方向进行的。
- en: 'This adjustment is the job of the optimizer, which implements what''s called
    the backpropagation algorithm: the central algorithm in deep learning.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种调整是优化器的任务，它实现了所谓的反向传播算法：深度学习的核心算法。
- en: Optimizers and loss functions are common to all deep learning methods – even
    the cases where we don't have an input/target pair. All optimizers are based on
    differential calculus, such as **stochastic gradient descent** (**SGD**), Adam,
    and so on. Hence, the term *differentiable programming* is a more precise name
    for deep learning in my mind.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器和损失函数是所有深度学习方法的共同点——即使我们没有输入/目标对。所有优化器都基于微分计算，如**随机梯度下降**（**SGD**）、Adam等。因此，在我看来，*可微分编程*是深度学习的一个更精确的名称。
- en: Putting it all together – the training loop
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起——训练循环
- en: We now have a shared vocabulary. You have a notional understanding of what terms
    like layers, model weights, loss function, and optimizer mean. But how do they
    work together? How do we train them on arbitrary data? We can train them to give
    us the ability to recognize cat pictures or fraudulent reviews on Amazon.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个共享的词汇表。你对诸如层、模型权重、损失函数和优化器等术语有一个概念性的理解。但它们是如何协同工作的？我们如何对任意数据进行训练？我们可以训练它们，使它们能够识别猫的图片或亚马逊上的欺诈评论。
- en: 'Here is the rough outline of the steps that occur inside a training loop:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是训练循环内部发生步骤的大致轮廓：
- en: 'Initialize:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化：
- en: The network/model weights are assigned random values, usually in the form of
    (-1, 1) or (0, 1).
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络或模型权重被分配随机值，通常形式为(-1, 1)或(0, 1)。
- en: The model is very far from the target. This is because it is simply executing
    a series of random transformations.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型与目标相差甚远。这是因为它只是在执行一系列随机变换。
- en: The loss is very high.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失值非常高。
- en: 'With every example that the network processes, the following occurs:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网络处理每个示例时，都会发生以下情况：
- en: The weights are adjusted a little in the correct direction
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss score decreases
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the training loop, which is repeated several times. Each pass over the
    entire training set is often referred to as an **epoch**. Each training set suited
    for deep learning should typically have thousands of examples. The models are
    sometimes trained for thousands of epochs, or alternatively millions of **iterations**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: In a training setup (model, optimizer, loop), the preceding loop updates the
    weight values that minimize the loss function. A trained network is the one with
    the least possible loss score on the entire training and valid data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: It's a simple mechanism that, when repeated often, just works like magic.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle – text categorization challenge
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this particular section, we are going to visit the familiar task of text
    classification, but with a different dataset. We are going to try to solve the
    [Jigsaw Toxic Comment Classification Challenge.](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that you will need to accept the terms and conditions of the competition
    and data usage to get this dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: For a direct download, you can get the train and test data from the [data tab
    on the challenge website](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use the official Kaggle API ([github link](https://github.com/Kaggle/kaggle-api))
    to download the data via a Terminal or Python program as well.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: In the case of both direct download and Kaggle API, you have to split your train
    data into smaller train and validation splits for this notebook.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: You can create train and validation splits of the train data by using the
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn.model_selection.train_test_split` utility. Alternatively, you can
    download this directly from the accompanying code repository with this book.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In case you have any packages missing, you can install them from the notebook
    itself by using the following commands:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s get the imports out of our way:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, read the train file into a pandas DataFrame:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the following output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '|  | id | `comment_text` | `toxic` | `severe_toxic` | `obscene` | `threat`
    | `insult` | `identity_hate` |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0000997932d777bf | Explanation`\r\nWhy` the edits made under my use...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| 1 | 000103f0d9cfb60f | D''aww! He matches this background colour I''m s...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| 2 | 000113f07ec002fd | Hey man, I''m really not trying to edit war. It...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0001b41b1c6bb37e | `\r\nMore\r\n` I can''t make any real suggestions...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0001d958c54c6e35 | You, sir, are my hero. Any chance you remember...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: 'Let''s read the validation data and preview the same as well:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We get the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '|  | id | `comment_text` | `toxic` | `severe_toxic` | `obscene` | `threat`
    | `insult` | `identity_hate` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| 0 | 000eefc67a2c930f | Radial symmetry `\r\n\r\n` Several now extinct li...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: '| 1 | 000f35deef84dc4a | There''s no need to apologize. A Wikipedia arti...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
- en: '| 2 | 000ffab30195c5e1 | Yes, because the mother of the child in the ca...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0010307a3a50a353 | `\r\nOk`. But it will take a bit of work but I ...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0010833a96e1f886 | `== A barnstar` for you! `==\r\n\r\n` The Real L...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
- en: Multiple target dataset
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The interesting thing about this dataset is that each comment can have multiples
    labels. For instance, a comment could be insulting and toxic, or it could be obscene
    and have `identity_hate` elements in it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we are leveling up here by trying to predict not one label (such as positive
    or negative), but multiple labels in one go. For each label, we'll predict a value
    between 0 and 1 to indicate how likely it is to belong to that category.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: This is not a probability value in the Bayesian meaning of the word, but represents
    the same intent.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: I'd recommend trying out the models that we saw earlier with this dataset, and
    re-implementing this code for our favourite IMDb dataset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s preview the test dataset as well using the same idea:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We get the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `id` | `comment_text` |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| 0 | 00001cee341fdb12 | Yo bitch Ja Rule is more succesful then you''ll...
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0000247867823ef7 | `== From RfC == \r\n\r\n` The title is fine as i...
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| 2 | 00013b17ad220c46 | `\r\n\r\n == Sources == \r\n\r\n *` Zawe Ashto...
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| 3 | 00017563c3f7919a | If you have a look back at the source, the in... |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| 4 | 00017695ad8997eb | I don''t anonymously edit articles at all. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: This preview confirms that we have a text challenge. The focus here is on the
    semantic categorization of text. The test dataset does not have empty headers
    or columns for the target columns, but we can infer them from the train dataframe.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Why PyTorch?
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is a deep learning framework by Facebook, similar to TensorFlow by Google.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Being backed by Google, thousands of dollars have been spent on TensorFlow's
    marketing, development, and documentation. It also got to a stable 1.0 release
    almost a year ago, while PyTorch has only recently gotten to 0.4.1\. This means
    that it's usually easier to find a TensorFlow solution to your problem and that
    you can copy and paste code off the internet.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, PyTorch is programmer-friendly. It is semantically similar
    to NumPy and deep learning operations in one. This means that I can use the Python
    debugging tools that I am already familiar with.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '**Pythonic**: TensorFlow worked like a C program in the sense that the code
    was all written in one session, compiled, and then executed, thereby destroying
    its Python flavor altogether. This has been solved by TensorFlow''s Eager Execution
    feature release, which will soon be stable enough to use for most prototyping
    work.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Loop Visualization:** Up until a while ago, TensorFlow had a good
    visualization tool called TensorBoard for understanding training and validation
    performance (and other characteristics), which was absent in PyTorch. For a long
    while now, tensorboardX makes TensorBoard easy to use with PyTorch.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: In short, I recommend using PyTorch because it is easier to debug, more Pythonic,
    and more programmer-friendly.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch and torchtext
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can install the latest version of Pytorch ([website](https://pytorch.org/))
    via conda or pip for your target machine. I am running this code on a Windows
    laptop with a GPU.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: I have installed torch using `conda install pytorch cuda92 -c pytorch`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'For installing `torchtext`, I recommend using pip directly from their GitHub
    repository with the latest fixes instead of PyPi, which is not frequently updated.
    Uncomment the line when running this notebook for the first time:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s set up the imports for `torch`, `torch.nn` (which is used in modeling),
    and `torchtext`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you are running this code on a machine with a GPU, leave the `use_gpu` flag
    set to `True`; otherwise, set it to `False`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'If you set `use_gpu=True`, we will check whether the GPU is accessible to PyTorch
    or not using the `torch.cuda.is_available()` utility:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s see how many GPU devices are available to PyTorch on this machine:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Data loaders with torchtext
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing good data loaders is the most tedious part in most deep learning applications.
    This step often combines the preprocessing, text cleaning, and vectorization tasks
    that we saw earlier.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, it wraps our static data objects into iterators or generators.
    This is incredibly helpful in processing data sizes much larger than GPU memory—which
    is quite often the case. This is done by splitting the data so that you can make
    batches of batchsize samples that fit your GPU memory.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Batchsizes are often powers of 2, such as 32, 64, 512, and so on. This convention
    exists because it helps with vector operations on the instruction set level. Anecdotally,
    using a batchsize that's different from a power of 2 has not helped or hurt my
    processing speed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Conventions and style
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code, iterators, and wrappers that we will be using are from [Practical
    Torchtext](https://github.com/keitakurita/practical-torchtext/). This is a `torchtext`
    tutorial that was created by Keita Kurita—one of the top five contributors to
    `torchtext`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The naming conventions and style are loosely inspired from the preceding work
    and fastai—a deep learning framework based on PyTorch itself.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by setting up the required variable placeholders in place:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `Field` class determines how the data is preprocessed and converted into
    a numeric format. The `Field` class is a fundamental `torchtext` data structure
    and worth looking into. The `Field` class models common text processing and sets
    them up for numericalization (or vectorization):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`Field` 类确定数据如何进行预处理并转换为数值格式。`Field` 类是 `torchtext` 的基本数据结构，值得深入研究。`Field`
    类模拟常见的文本处理并将它们设置为数值化（或向量化）：'
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By default, all of the fields take in strings of words as input, and then the
    fields build a mapping from the words to integers later on. This mapping is called
    the vocab, and is effectively a one-hot encoding of the tokens.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有字段都接受单词字符串作为输入，然后字段在之后构建从单词到整数的映射。这个映射称为词汇表，实际上是标记的one-hot编码。
- en: 'We saw that each label in our case is already an integer marked as 0 or 1\.
    Therefore, we will not one-hot this – we will tell the `Field` class that this
    is already one-hot encoded and non-sequential by setting `use_vocab=False` and
    `sequential=False`, respectively:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到在我们的案例中，每个标签已经是一个标记为0或1的整数。因此，我们不会进行one-hot编码——我们将告诉 `Field` 类这已经是一组one-hot编码且非序列的，分别通过设置
    `use_vocab=False` 和 `sequential=False`：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A few things are happening here, so let''s unpack it a bit:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了一些事情，所以让我们稍微展开一下：
- en: '`lower=True`: All input is converted to lowercase.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lower=True`：所有输入都被转换为小写。'
- en: '`sequential=True`: If `False`, no tokenization is applied.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequential=True`：如果为 `False`，则不应用分词。'
- en: '`tokenizer`: We defined a custom tokenize function that simply splits the string
    on the space. You should replace this with the spaCy tokenizer (set `tokenize="spacy"`)
    and see if that changes the loss curve or final model''s performance.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`：我们定义了一个自定义的tokenize函数，它只是简单地根据空格分割字符串。你应该用spaCy分词器（设置 `tokenize="spacy"`)
    替换它，看看这会不会改变损失曲线或最终模型的表现。'
- en: Knowing the field
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解字段
- en: Along with the keyword arguments that we've already mentioned, the `Field` class
    will also allow the user to specify special tokens (`unk_token` for out-of-vocabulary
    unknown words, `pad_token` for padding, `eos_token` for the end of a sentence,
    and an optional `init_token` for the start of the sentence).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前提到的关键字参数之外，`Field` 类还将允许用户指定特殊标记（`unk_token` 用于词汇表外的未知单词，`pad_token` 用于填充，`eos_token`
    用于句子的结束，以及可选的 `init_token` 用于句子的开始）。
- en: The preprocessing and postprocessing parameters accept any `torchtext.data.Pipeline`
    that it receives. Preprocessing is applied after tokenizing but before numericalizing.
    Postprocessing is applied after numericalizing, but before converting them into
    a Tensor.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理和后处理参数接受它接收到的任何 `torchtext.data.Pipeline`。预处理在分词之后但在数值化之前应用。后处理在数值化之后但在将它们转换为Tensor之前应用。
- en: 'The docstrings for the `Field` class are relatively well written, so if you
    need some advanced preprocessing, you should probe them for more information:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`Field` 类的文档字符串写得相当好，所以如果你需要一些高级预处理，你应该查阅它们以获取更多信息：'
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`TabularDataset` is the class that we use to read `.csv`, `.tsv`, or `.json`
    files. You can specify the type of file that you are reading, that is, `.tsv`
    or `.json`, directly in the API, which is powerful and handy'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`TabularDataset` 是我们用来读取 `.csv`、`.tsv` 或 `.json` 文件的类。你可以在API中直接指定你正在读取的文件类型，即
    `.tsv` 或 `.json`，这既强大又方便。'
- en: At first glance, you might think that this class is a bit misplaced because
    a generic file I/O+processor API should be accessible directly in PyTorch and
    not in a package dedicated to text processing. Let's see why it is placed where
    it is.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，你可能觉得这个类有点放错位置，因为通用的文件I/O+处理器API应该直接在PyTorch中可用，而不是在专门用于文本处理的包中。让我们看看为什么它被放在那里。
- en: '`TabularData` has an interesting `fields` input parameter. For the CSV data
    format, `fields` is a list of tuples. Each tuple in turn is the column name and
    the `torchtext` variable we want to associate with it. The fields should be in
    the same order as the columns in the CSV or TSV file.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`TabularData` 有一个有趣的 `fields` 输入参数。对于CSV数据格式，`fields` 是一个元组的列表。每个元组反过来是列名和我们要与之关联的
    `torchtext` 变量。字段应该与CSV或TSV文件中的列顺序相同。'
- en: 'We have only two defined fields here: TEXT and LABEL. Therefore, each column
    is tagged as either one. We can simply mark the column as None if we want to ignore
    it completely. This is how we are tagging our columns as inputs (TEXT) and targets
    (LABEL) for the model to learn.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只定义了两个字段：TEXT和LABEL。因此，每一列都被标记为其中之一。如果我们想完全忽略某一列，我们可以简单地将其标记为None。这就是我们如何标记我们的列作为模型学习的输入（TEXT）和目标（LABEL）。
- en: 'This tight coupling of the fields parameter with `TabularData` is why this
    is part of `torchtext` and not PyTorch:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 字段参数与`TabularData`的这种紧密耦合是为什么它是`torchtext`的一部分而不是PyTorch的原因：
- en: '[PRE13]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This defines our list of inputs. I have done this manually here, but you could
    also do this with code by reading the column headers from `train_df` and assigning
    them TEXT or LABEL accordingly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了我们的输入列表。我在这里手动做了这件事，但您也可以通过代码读取`train_df`的列标题，并相应地分配TEXT或LABEL。
- en: As a reminder, we will have to define another fields list for our test data
    because it has a different header. It has no LABEL fields.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们将不得不为我们的测试数据定义另一个字段列表，因为它有不同的标题。它没有`LABEL`字段。
- en: '`TabularDataset` supports two APIs: `split` and `splits`. We will use the one
    with the extra s, `splits`. The splits API is simple:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`TabularDataset`支持两个API：`split`和`splits`。我们将使用带有额外`s`的`splits`。splits API很简单：'
- en: '`path`: This is the prefix of filenames'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path`：这是文件名的前缀'
- en: '`train`, `validation`: These are filenames of the corresponding dataset'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train`、`validation`：这是对应数据集的文件名'
- en: '`format`: Either `.csv`, `.tsv`, or `.json`, as stated earlier; this is set
    to `.csv` here'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format`：如前所述，可以是`.csv`、`.tsv`或`.json`；这里设置为`.csv`'
- en: '`skip_header`: This is set to `True` if your `.csv` file has column titles
    in it, as does ours'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_header`：如果您的`.csv`文件中有列标题，就像我们的一样，则设置为`True`'
- en: '`fields`: We pass the list of fields we just set up previously:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fields`：我们传递我们之前设置的字段列表：'
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s repeat the same for test data now. We drop the `id` column again and
    set `comment_text` to be our label:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们也重复同样的步骤来处理测试数据。我们再次删除`id`列，并将`comment_text`设置为我们的标签：
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We pass the entire relative file path directly into the path, instead of using
    the `path` and `test` variable combination here. We used the `path` and `train`
    combination when setting up the `trn` and `vld` variables.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接将整个相对文件路径传递到`path`中，而不是在这里使用`path`和`test`变量的组合。我们在设置`trn`和`vld`变量时使用了`path`和`train`的组合。
- en: 'As a note, these filenames are consistent with what Keita used in the `torchtext`
    tutorial:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 作为备注，这些文件名与Keita在`torchtext`教程中使用的一致：
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Exploring the dataset objects
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据集对象
- en: 'Let''s look at the dataset objects, that is, `trn`, `vld`, and `tst`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据集对象，即`trn`、`vld`和`tst`：
- en: '[PRE17]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'They are all objects from the same class. Our dataset objects can be indexed
    and iterated over like normal lists, so let''s see what the first element looks
    like:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都是同一类的对象。我们的数据集对象可以像正常列表一样索引和迭代，所以让我们看看第一个元素的样子：
- en: '[PRE18]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: All our elements are, in turn, objects of the `example.Example` class. Each
    example stores each column as an attribute. But where did our text and labels
    go?
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的元素都是`example.Example`类的对象。每个示例将每一列存储为一个属性。但我们的文本和标签去哪里了？
- en: '[PRE19]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `Example` object bundles the attributes of a single data point together.
    Our `comment_text` and the `labels` are now part of the dictionary that makes
    up each of these example objects. We found all of them by calling `__dict__.keys()`
    on an `example.Example` object:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`Example`对象将单个数据点的属性捆绑在一起。我们的`comment_text`和`labels`现在都是这些示例对象组成的字典的一部分。我们通过在`example.Example`对象上调用`__dict__.keys()`找到了所有这些对象：'
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The text has already been tokenized for us, but has not yet been vectorized
    or numericalized. We will use one-hot encoding for all the tokens that exist in
    our training corpus. This will convert our words into integers.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 文本已经被我们分词了，但还没有被向量化或数值化。我们将使用独热编码来处理训练语料库中存在的所有标记。这将把我们的单词转换成整数。
- en: 'We can do this by calling the `build_vocab` attribute of our `TEXT` field:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用我们的`TEXT`字段的`build_vocab`属性来完成这个操作：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This statement processes the entire train data – in particular, the `comment_text`
    field. The words are registered in the vocabulary.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语句处理整个训练数据——特别是`comment_text`字段。单词被注册到词汇表中。
- en: To handle the vocabulary, `torchtext` has its own class. The `Vocab` class can
    also take options such as `max_size` and `min_freq` that can let us know the number
    of words present in the vocabulary or how many times a word has to appear to be
    registered in the vocabulary.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理词汇表，`torchtext`有自己的类。`Vocab`类也可以接受如`max_size`和`min_freq`等选项，这些选项可以让我们知道词汇表中有多少单词，或者一个单词需要出现多少次才能被注册到词汇表中。
- en: 'Words that are not included in the vocabulary will be converted into `<unk>`,
    a token meaning for *unknown*. Words that occur that are too rare are also assigned
    the `<unk>` token for ease of processing. This can hurt or help the model''s performance,
    depending on which and how many words we lose to the `<unk>` token:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 不在词汇表中的单词将被转换为 `<unk>`，表示 *未知* 的标记。过于罕见的单词也会被分配 `<unk>` 标记以简化处理。这可能会损害或帮助模型的性能，具体取决于我们失去了多少单词给
    `<unk>` 标记：
- en: '[PRE22]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `TEXT` field now has a vocab attribute that is a specific instance of the
    `Vocab` class. We can use this in turn to look up the attributes of the vocab
    object. For instance, we can find the frequency of any word in the training corpus.
    The `TEXT.vocab.freqs` object is actually an object of `type collections.Counter`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`TEXT` 字段现在有一个词汇属性，它是 `Vocab` 类的特定实例。我们可以利用这一点来查找词汇对象的属性。例如，我们可以找到训练语料库中任何单词的频率。`TEXT.vocab.freqs`
    对象实际上是 `type collections.Counter` 的对象：'
- en: '[PRE23]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This means that it will support all functions, including the `most_common`
    API to sort the words by frequency and find the top k most frequently occurring
    words for us. Let''s take a look at them:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它将支持所有功能，包括按频率排序单词的 `most_common` API，并为我们找到出现频率最高的前 k 个单词。让我们来看看它们：
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `Vocab` class holds a mapping from word to `id` in its `stoi` attribute
    and a reverse mapping in its `itos` attribute. Let''s look at these attributes:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`Vocab` 类在其 `stoi` 属性中持有从单词到 `id` 的映射，并在其 `itos` 属性中持有反向映射。让我们看看这些属性：'
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**itos**, or integer to string mapping, is a list of words. The index of each
    word in the list is its integer mapping. For instance, the 7-indexed word would
    be *and* because its integer mapping is 7.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**itos**，或整数到字符串映射，是一个单词列表。列表中每个单词的索引是其整数映射。例如，索引为 7 的单词将是 *and*，因为它的整数映射是
    7。'
- en: '**stoi**, or string to integer mapping, is a dictionary of words. Each key
    is a word in the training corpus, with the value being an integer. For instance,
    the word "and" might have an integer mapping that can be looked up in this dictionary
    in O(1) time.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**stoi**，或字符串到整数映射，是一个单词的字典。每个键是训练语料库中的一个单词，其值是一个整数。例如，“and”这个单词可能有一个整数映射，可以在该字典中以
    O(1) 的时间复杂度查找。'
- en: 'Note that this convention automatically handles the off-by-one problem caused
    by zero indexing in Python:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，此约定自动处理由 Python 中的零索引引起的偏移量问题：
- en: '[PRE26]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Iterators
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Iterators
- en: '`torchtext` has renamed and extended the `DataLoader` objects from PyTorch
    and torchvision. In essence, it does the same three jobs:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext` 对 PyTorch 和 torchvision 中的 `DataLoader` 对象进行了重命名和扩展。本质上，它执行相同的三个任务：'
- en: Batching the data
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量加载数据
- en: Shuffling the data
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打乱数据
- en: Loading the data in parallel using `multiprocessing` workers
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `multiprocessing` 工作者并行加载数据
- en: This batch loading of data enables us to process a dataset that's much larger
    than the GPU RAM. `Iterators` extend and specialize the `DataLoader` for NLP/text
    processing applications.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种批量加载数据使我们能够处理比 GPU RAM 大得多的数据集。`Iterators` 扩展并专门化 `DataLoader` 以用于 NLP/文本处理应用。
- en: 'We will use both `Iterator` and its cousin, `BucketIterator`, here:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用 `Iterator` 和它的表亲 `BucketIterator`：
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: BucketIterator
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BucketIterator
- en: '`BucketIterator` automatically shuffles and buckets the input sequences into
    sequences of similar length.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`BucketIterator` 自动打乱并将输入序列桶化为相似长度的序列。'
- en: 'To enable batch processing, we need the input sequences in a batch that''s
    of identical length. This is done by padding the smaller input sequences to the
    length of the longest sequence in batch. Check out the following code:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用批量处理，我们需要具有相同长度的输入序列的批次。这是通过将较短的输入序列填充到批次中最长序列的长度来完成的。查看以下代码：
- en: '[PRE28]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will need to be padded to become the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要填充以成为以下内容：
- en: '[PRE29]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Additionally, the padding operation is most efficient when the sequences are
    of similar lengths. The `BucketIterator` does all of this behind the scenes. This
    is what makes it an extremely powerful abstraction for text processing.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当序列长度相似时，填充操作效率最高。`BucketIterator` 在幕后完成所有这些。这就是它成为文本处理中极其强大的抽象的原因。
- en: We want the bucket sorting to be based on the lengths of the `comment_text`
    field, so we pass that in as a keyword argument.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望桶排序基于 `comment_text` 字段的长度，因此我们将它作为关键字参数传递。
- en: 'Let''s go ahead and initialize the iterators for the train and validation data:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续初始化训练数据和验证数据的迭代器：
- en: '[PRE30]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s take a quick glance at the parameters we passed to this function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下传递给此函数的参数：
- en: '`batch_size`: We use a small batch size of 32 for both train and validation.
    This is because I am using a GTX 1060 with only 3 GB of memory.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`: 我们在训练和验证中都使用较小的批处理大小 32。这是因为我在使用只有 3 GB 内存 GTX 1060。'
- en: '`sort_key`: `BucketIterator` is told to use the number of tokens in the `comment_text`
    as the key to sort in any example.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sort_key`: `BucketIterator` 被告知使用 `comment_text` 中的标记数量作为排序的键在任何示例中。'
- en: '`sort_within_batch`: When set to `True`, this sorts the data within each minibatch
    in decreasing order, according to the `sort_key`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sort_within_batch`: 当设置为 `True` 时，它根据 `sort_key` 以降序对每个小批处理中的数据进行排序。'
- en: '`repeat`: When set to True, it allows us to loop over and see a previously
    seen sample again. We set it to `False` here because we are repeating using an
    abstraction that we will write in a minute.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repeat`: 当设置为 True 时，它允许我们循环遍历并再次看到之前看到的样本。我们在这里将其设置为 `False`，因为我们正在使用我们将在一分钟内编写的抽象进行重复。'
- en: 'In the meanwhile, let''s take a minute to explore the new variable that we
    just made:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，让我们花一分钟时间探索我们刚刚创建的新变量：
- en: '[PRE31]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, all that each batch has is torch tensors of exactly the same size (the
    size is the length of the vector of the vector of the vector of the vector of
    the vector of the vector of the vector here). These tensors have not been moved
    to GPU yet, but that's fine.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个批处理都只有大小完全相同的 torch 张量（这里的尺寸是向量的向量的向量的向量的向量的向量的向量的长度）。这些张量还没有被移动到 GPU 上，但这没关系。
- en: '`batch` is actually a wrapper over the already familiar example object that
    we have seen. It bundles all the attributes related to the batch in one variable
    dict:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch` 实际上是已经熟悉的示例对象的包装器，它将所有与批处理相关的属性捆绑在一个变量字典中：'
- en: '[PRE32]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If our preceding understanding is correct, and we know how Python''s object
    passing works, the dataset attribute of the batch variable should point to the
    `trn` variable of the `torchtext.data.TabularData` type. Let''s check for this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的先前的理解是正确的，并且我们知道 Python 的对象传递是如何工作的，那么批处理变量的数据集属性应该指向 `torchtext.data.TabularData`
    类型的 `trn` 变量。让我们检查这一点：
- en: '[PRE33]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Aha! We got this right.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 哈哈！我们做对了。
- en: 'For the test iterator, since we don''t need shuffling, we will use the plain
    `torchtext` `Iterator`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试迭代器，由于我们不需要洗牌，我们将使用普通的 `torchtext` `Iterator`：
- en: '[PRE34]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s take a look at this iterator, too:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个迭代器：
- en: '[PRE35]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The sequence length of `33` here is different from the input's `25`. That's
    fine. We can see that this is also a torch tensor now.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 `33` 的序列长度与输入的 `25` 不同。这没关系。我们可以看到这现在也是一个 torch 张量。
- en: Next, let's write a wrapper over the batch objects.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们为批处理对象编写一个包装器。
- en: BatchWrapper
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BatchWrapper
- en: Before we delve into `BatchWrapper`, let me tell you what the problem with batch
    objects is. Our batch iterator returns a custom datatype, `torchtext.data.Batch`.
    This has a similar to multiple `example.Example`. This returns with a batch of
    data from each field as attributes. This custom datatype makes code reuse difficult
    since, each time the column names change, we need to modify the code. This also
    makes `torchtext` hard to use with other libraries such as torchsample and fastai.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 `BatchWrapper` 之前，让我告诉你批处理对象的问题所在。我们的批处理迭代器返回一个自定义数据类型，`torchtext.data.Batch`。这类似于多个
    `example.Example`。它返回每个字段的批处理数据作为属性。这种自定义数据类型使得代码重用变得困难，因为每次列名更改时，我们需要修改代码。这也使得
    `torchtext` 难以与其他库如 torchsample 和 fastai 一起使用。
- en: So, how do we solve this?
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何解决这个问题呢？
- en: We will convert the batch into a tuple in the form (x, y). x is the input to
    the model and y is the target – or, more conventionally, x is the independent
    variable while y is the dependent variable. One way to think about this is that
    the model will learn the function mapping from x to y.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将批处理转换为形式为 (x, y) 的元组。x 是模型的输入，y 是目标 – 或者更传统地，x 是自变量，而 y 是因变量。一种思考方式是，模型将学习从
    x 到 y 的函数映射。
- en: 'BatchWrapper helps us reuse the modeling, training, and other code functions
    across datasets:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: BatchWrapper 帮助我们在不同数据集之间重用建模、训练和其他代码函数：
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `BatchWrapper` class accepts the iterator variable itself, the variable
    x name, and the variable y name during initialization. It yields tensor x and
    y. The x and y values are looked up from the `batch` in `self.dl` using `getattr`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`BatchWrapper` 类在初始化期间接受迭代器变量本身、变量 x 名称和变量 y 名称。它产生张量 x 和 y。x 和 y 的值通过 `getattr`
    在 `self.dl` 中的 `batch` 中查找。'
- en: If GPU is available, this class moves these tensors to the GPU as well with
    `x.cuda()` and `y.cuda()`, making it ready for consumption by the model.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 GPU 可用，这个类将使用 `x.cuda()` 和 `y.cuda()` 将这些张量移动到 GPU 上，使其准备好被模型消费。
- en: 'Let''s quickly wrap our `train`, `val`, and `test iter` objects using this
    new class:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速使用这个新类包装我们的`train`、`val`和`test iter`对象：
- en: '[PRE37]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This returns the simplest iterator, ready for model processing. Note that,
    in this particular case, the tensor has a "device" attribute set to `cuda:0`.
    Let''s preview this:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了最简单的迭代器，准备好进行模型处理。请注意，在这种情况下，张量有一个设置为`cuda:0`的"device"属性。让我们预览一下：
- en: '[PRE38]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Training a text classifier
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练文本分类器
- en: 'We are now ready for training our text classifier model. Let''s start with
    something simple: we are going to consider this model to be a black box for now.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练我们的文本分类器模型了。让我们从简单的事情开始：现在我们将这个模型视为一个黑盒。
- en: 'Model architecture is better explained by other sources, including several
    YouTube videos such as those by CS224n at Stanford ([http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)[)](http://web.stanford.edu/class/cs224n/).
    I suggest that you explore and connect it with the know-how that you already have:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构的更好解释来自其他来源，包括斯坦福大学的CS224n等YouTube视频（[http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)）。我建议您探索并将其与您已有的知识相结合：
- en: '[PRE39]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: All PyTorch models inherit from `torch.nn.Module`. They must all implement the
    `forward` function, which is executed when the model makes a prediction. The corresponding
    `backward` function for training is auto-computed.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 所有PyTorch模型都继承自`torch.nn.Module`。它们都必须实现`forward`函数，该函数在模型做出预测时执行。相应的用于训练的`backward`函数是自动计算的。
- en: Initializing the model
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化模型
- en: Any Pytorch model is instantiated like a Python object. Unlike TensorFlow, there
    is no strict notion of a session object inside which the code is compiled and
    then run. The model class is as we have written previously.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 任何Pytorch模型都是像Python对象一样实例化的。与TensorFlow不同，其中没有严格的会话对象概念，代码是在其中编译然后运行的。模型类就像我们之前写的那样。
- en: 'The `init` function of the preceding class accepts a few parameters:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个类的`init`函数接受一些参数：
- en: '`hidden_dim`: These are hidden layer dimensions, that is, the vector length
    of the hidden layers'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dim`：这些是隐藏层维度，即隐藏层的向量长度'
- en: '`emb_dim=300`: This is an embedding dimension, that is, the vector length of
    the first input *step* to the LSTM'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emb_dim=300`：这是一个嵌入维度，即LSTM的第一个输入*步*的向量长度'
- en: '`num_linear=2`: The other two dropout parameters:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_linear=2`：其他两个dropout参数：'
- en: '`spatial_dropout=0.05`'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spatial_dropout=0.05`'
- en: '`recurrent_dropout=0.1`'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recurrent_dropout=0.1`'
- en: Both dropout parameters act as regularizers. They help prevent the model from
    overfitting, that is, the state where the model ends up learning the samples in
    the training set instead of the more generic pattern that can be used to make
    predictions.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 两个dropout参数都充当正则化器。它们有助于防止模型过拟合，即模型最终学习的是训练集中的样本，而不是可以用于做出预测的更通用的模式。
- en: 'One way to think about the differences between the dropouts is that one of
    them acts on the input itself. The other acts during backpropagation or the weight
    update step, as mentioned earlier:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 关于dropout之间的差异的一种思考方式是，其中一个作用于输入本身。另一个在反向传播或权重更新步骤中起作用，如前所述：
- en: '[PRE40]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can print any PyTorch model to look at the architecture of the class. It
    is computed from the forward function implementation, which is exactly what we'd
    expect. This is really helpful when debugging the model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印任何PyTorch模型来查看类的架构。它是从`forward`函数实现中计算出来的，这正是我们所期望的。这在调试模型时非常有用。
- en: Let's write a small utility function to calculate the size of any PyTorch model.
    By size, we mean the number of model parameters that can be updated during training
    to learn the input-to-target mapping.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个小工具函数来计算任何PyTorch模型的大小。在这里，我们所说的“大小”是指可以在训练期间更新的模型参数数量，以学习输入到目标映射。
- en: 'While this function is implemented in Keras, it''s simple enough to write it
    again:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个函数是在Keras中实现的，但它足够简单，可以再次编写：
- en: '[PRE41]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We can see that even our simple baseline model has more than 4 million parameters.
    In comparison, a typical decision tree might only have a few hundred decision
    splits, maximum.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，即使是我们的简单基线模型也有超过400万个参数。相比之下，一个典型的决策树可能只有几百个决策分支，最多。
- en: 'Next, we will move the model weights to the GPU using the familiar `.cuda()`
    syntax:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用熟悉的`.cuda()`语法将模型权重移动到GPU上：
- en: '[PRE42]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Putting the pieces together again
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 再次将各个部分组合在一起
- en: 'These are the pieces which we looked at, let''s quickly summarize them:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们查看的部分，让我们快速总结一下：
- en: '**Loss function**: Binary cross entropy with Logit loss. It serves as the quality
    metric of how far the predictions are from the ground truth.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer**: We use the Adam optimizer with default parameters, set with
    a learning rate of 1e-2 or 0.01:'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is how we would see these 2 components in PyTorch:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We call set the number of epochs for which the model has to be trained here:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This is set to a very small value because this entire notebook, model, and training
    loop is just for demonstrative purposes.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Training loop
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training loop is logically split into two sections: `model.train()` and
    `model.eval()`. Note the placement of the following lines of code:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The first half is the actual learning loop. This is the sequence of steps inside
    the loop:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Set the optimizer's gradient to zero
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make model predictions on this training batch in `preds`
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the loss using `loss_func`
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the model weights using `loss.backward()`
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the optimizer state using `opt.step()`
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The entire back propagation hassle is handled in one line of code:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This level of abstraction that exposes the model's internals without worrying
    about the differential calculus aspects is why frameworks such as PyTorch are
    so convenient and useful.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The second loop is the evaluation loop. This is run on the validation split
    of the data. We set the model to *eval* mode, which locks the model weights. The
    weights will not be updated by accident as long as `model.eval()` is not set back
    to `model.train()`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'The only two things we do inside this second loop are simple:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Make predictions on the validation split
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the loss on this split
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aggregate loss from all validation batches is then printed at the end of
    every epoch, along with running training loss.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'One training loop will look something like the following:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We can see that the training loop ends with a low validation loss but a high
    training loss. This could be indicative of something wrong with either the model
    or the train and valid data splits. There is no easy way to debug this.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The good way forward is usually to train the model for a few more epochs until
    no further change in either loss is observed.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Prediction mode
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use the model we trained to make some predictions on the test data:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The entire loop is now in eval mode, which we use to lock the model weights.
    Alternatively, we could have set `model.train(False)` as well.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: We iteratively take batchsize samples from the test iterator, make predictions,
    and append them to a list. At the end, we stack them.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Converting predictions into a pandas DataFrame
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This helps us convert the predictions into a more interpretable format. Let''s
    read the test dataframe and insert the predictions in the correct columns:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, we can preview a few of the rows of the DataFrame:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We get the following output:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '|  | id | `comment_text` | `toxic` | `severe_toxic` | `obscene` | `threat`
    | `insult` | `identity_hate` |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| 0 | 00001cee341fdb12 | Yo bitch Ja Rule is more succesful then you''ll...
    | 0.629146 | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 00001cee341fdb12 | Yo bitch Ja Rule is more succesful then you''ll...
    | 0.629146 | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
- en: '| 1 | 0000247867823ef7 | `== From RfC == \r\n\r\n` The title is fine as i...
    | 0.629146 | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0000247867823ef7 | `== From RfC == \r\n\r\n` 标题是好的，正如我... | 0.629146
    | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
- en: '| 2 | 00013b17ad220c46 | "`\r\n\r\n == Sources == \r\n\r\n *`. Zawe Ashto...
    | 0.629146 | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 00013b17ad220c46 | "`\r\n\r\n == Sources == \r\n\r\n *`. Zawe Ashto...
    | 0.629146 | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
- en: Summary
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This was our first brush with deep learning for NLP. This was very a thorough
    introduction to `torchtext` and how we can leverage it with Pytorch. We also got
    a very broad view of deep learning as a puzzle of only two or three broad pieces:
    the model, the optimizer, and the loss functions. This is true irrespective of
    what framework or dataset you use.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第一次接触深度学习在NLP中的应用。这是一个对`torchtext`的全面介绍，以及我们如何利用Pytorch来利用它。我们还对深度学习作为一个只有两到三个广泛组成部分的谜团有了非常广泛的了解：模型、优化器和损失函数。这无论你使用什么框架或数据集都是正确的。
- en: We did skimp a bit on the model architecture explanation in the interest of
    keeping this short. We will avoid using concepts that have not been explained
    here in other sections.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简短，我们在模型架构解释上有所简化。我们将避免使用在其他部分未解释的概念。
- en: When we are working with modern ensembling methods, we don't always know how
    a particular prediction is being made. That's a black box to us, in the same sense
    that all deep learning model predictions are a black box.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用现代集成方法工作时，我们并不总是知道某个特定预测是如何被做出的。对我们来说，这是一个黑盒，就像所有深度学习模型的预测一样。
- en: In the next chapter, we will look at some tools and techniques that will help
    us look into these boxes – at least a little bit more.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一些工具和技术，这些工具和技术将帮助我们窥视这些盒子——至少是更多一点。
