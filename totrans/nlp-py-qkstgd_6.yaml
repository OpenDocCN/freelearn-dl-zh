- en: Deep Learning for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: n the previous chapter, we used classic machine learning techniques to build
    our text classifiers. In this chapter, we will replace those with deep learning
    techniques via the use of **recurrent neural networks** (**RNN**).
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we will use a relatively simple bidirectional LSTM model. If
    this is new to you, keep reading – if not, please feel free to skip ahead!
  prefs: []
  type: TYPE_NORMAL
- en: The dataset attribute of the batch variable should point to the `trn` variable
    of the `torchtext.data.TabularData` type. This is a useful checkpoint to understand
    how data flow differs in training deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by touching upon the overhyped terms, that is, *deep* in deep learning
    and *neural* in deep neural networks. Before we do that, let's take a moment to
    explain why I use PyTorch and compare it to Tensorflow and Keras—the other popular
    deep learning frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: I will be building the simplest possible architecture for demonstrative purposes
    here. Let's assume a general familiarity with RNNs and not introduce the same
    again.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning? How does it differ from what we have seen already?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the key ideas in any deep learning model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why PyTorch?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we tokenize text and set up dataloaders with torchtext?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are recurrent networks, and how can we use them for text classification?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning is a subset of machine learning: a new take on learning from
    data that puts an emphasis on learning successive layers of increasingly meaningful
    representations. But what does the *deep* in deep learning mean?'
  prefs: []
  type: TYPE_NORMAL
- en: <q>"The deep in deep learning isn't a reference to any kind of deeper understanding
    achieved by the approach; rather, it stands for this idea of successive layers
    of representations."</q>
  prefs: []
  type: TYPE_NORMAL
- en: – F. Chollet, Lead Developer of Keras
  prefs: []
  type: TYPE_NORMAL
- en: The depth of the model is indicative of how many layers of such representations
    we use. F Chollet suggested layered representations learning and hierarchical
    representations learning as better names for this. Another name could have been
    differentiable programming.
  prefs: []
  type: TYPE_NORMAL
- en: The term *differentiable programming*, coined by Yann LeCun, stems from the
    fact that what our *deep learning methods* have in common is not more layers—it's
    the fact that all of these models learn via some form of differential calculus
    – most often stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Differences between modern machine learning methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The modern machine learning methods that we have studied shot to being mainstream
    mainly in the 1990s. The binding factor among them was that they all use one layer
    of representations. For instance, decision trees just create one set of rules
    and apply them. Even if you add ensemble approaches, the *ensembling* is often
    shallow and only combines several ML models directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a better-worded interpretation of these differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Modern deep learning often involves tens or even hundreds of successive layers
    of representations – and they’re all learned automatically from exposure to training
    data. Meanwhile, other approaches to machine learning tend to focus on learning
    only one or two layers of representations of the data; hence, they’re sometimes
    called shallow learning."'
  prefs: []
  type: TYPE_NORMAL
- en: – F Chollet
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the key terms behind deep learning, since this way we might come
    across some key ideas as well.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a loosely worded manner, machine learning is about mapping inputs (such as
    images, or *movie reviews*) to targets (such as the label cat or *positive*).
    The model does this by looking at (or training from) several pairs of input and
    targets.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks do this input-to-target mapping using a long sequence of
    simple data transformations (layers). This sequence length is referred to as the
    depth of the network. The entire sequence from input-to-target is referred to
    as a model that learns about the data. These data transformations are learned
    by repeated observation of examples. Let's look at how this learning happens.
  prefs: []
  type: TYPE_NORMAL
- en: Puzzle pieces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are looking at a particular subclass of challenges where we want to learn
    an input-to-target mapping. This subclass is generally referred to as supervised
    machine learning. The word supervised denotes that we have target for each input.
    Unsupervised machine learning includes challenges such as trying to cluster text,
    where we do not have a target.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do any supervised machine learning, we need the following in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Data:** Anything ranging from past stock performance to your vacation
    pictures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target:** Examples of the expected output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A way to measure whether the algorithm is doing a good job:** This is necessary
    to determine the distance between the algorithm''s current output and its expected
    output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding components are universal to any supervised approach, be it machine
    learning or deep learning. Deep learning in particular has its own cast of puzzling
    factors:'
  prefs: []
  type: TYPE_NORMAL
- en: The model itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since these actors are new to the scene, let's take a minute in understanding
    what they do.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each model is comprised of several layers. Each layer is a data transformation.
    This transformation is captured using a bunch of numbers, called layer weights.
    This is not a complete truth though, since most layers often have a mathematical
    operation associated with them, for example, convolution or an affine transform.
    A more precise perspective would be to say that a layer is **parameterized** by
    its weights. Hence, we use the terms *layer parameters* and *layer weights* interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: The state of all the layer weights together makes the model state captured in
    model weights. A model can have anywhere between a few thousand to a few million
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to understand the notion of model **learning** in this context:
    learning means finding values for the weights of all layers in a network, so that
    the network will correctly map example inputs to their associated targets.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this value set is for *all layers* in one place. This nuance is important
    because changing the weights of one layer can change the behavior and predictions
    made by the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the pieces that''s used to set up a machine learning task is to assess
    how a model is doing. The simplest answer would be to measure the notional accuracy
    of the model. Accuracy has few flaws, though:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is a proxy metric tied to validation data and not training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy measures how correct we are. During training, we want to measure how
    far our model predicts from the target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These differences mean that we need a different function to meet our preceding
    criteria. This is fulfilled by the *loss function* in the context of deep learning.
    This is sometimes referred to as an *objective function* as well.
  prefs: []
  type: TYPE_NORMAL
- en: <q>"The loss function takes the predictions of the network and the true target
    (what you wanted the network to output) and computes a distance score, capturing
    how well the network has done on this specific example.</q>"
  prefs: []
  type: TYPE_NORMAL
- en: <q>- From Deep Learning in Python by F Chollet</q>
  prefs: []
  type: TYPE_NORMAL
- en: This distance measurement is called the loss score, or simply loss.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This loss is automatically used as a feedback signal to adjust the way the algorithm
    works. This adjustment step is what we call learning.
  prefs: []
  type: TYPE_NORMAL
- en: This automatic adjustment in model weights is peculiar for deep learning. Each
    adjustment or *update* of weights is made in a direction that will lower the loss
    for the current training pair (input, target).
  prefs: []
  type: TYPE_NORMAL
- en: 'This adjustment is the job of the optimizer, which implements what''s called
    the backpropagation algorithm: the central algorithm in deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers and loss functions are common to all deep learning methods – even
    the cases where we don't have an input/target pair. All optimizers are based on
    differential calculus, such as **stochastic gradient descent** (**SGD**), Adam,
    and so on. Hence, the term *differentiable programming* is a more precise name
    for deep learning in my mind.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together – the training loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have a shared vocabulary. You have a notional understanding of what terms
    like layers, model weights, loss function, and optimizer mean. But how do they
    work together? How do we train them on arbitrary data? We can train them to give
    us the ability to recognize cat pictures or fraudulent reviews on Amazon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the rough outline of the steps that occur inside a training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network/model weights are assigned random values, usually in the form of
    (-1, 1) or (0, 1).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is very far from the target. This is because it is simply executing
    a series of random transformations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss is very high.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With every example that the network processes, the following occurs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights are adjusted a little in the correct direction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss score decreases
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the training loop, which is repeated several times. Each pass over the
    entire training set is often referred to as an **epoch**. Each training set suited
    for deep learning should typically have thousands of examples. The models are
    sometimes trained for thousands of epochs, or alternatively millions of **iterations**.
  prefs: []
  type: TYPE_NORMAL
- en: In a training setup (model, optimizer, loop), the preceding loop updates the
    weight values that minimize the loss function. A trained network is the one with
    the least possible loss score on the entire training and valid data.
  prefs: []
  type: TYPE_NORMAL
- en: It's a simple mechanism that, when repeated often, just works like magic.
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle – text categorization challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this particular section, we are going to visit the familiar task of text
    classification, but with a different dataset. We are going to try to solve the
    [Jigsaw Toxic Comment Classification Challenge.](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that you will need to accept the terms and conditions of the competition
    and data usage to get this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For a direct download, you can get the train and test data from the [data tab
    on the challenge website](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use the official Kaggle API ([github link](https://github.com/Kaggle/kaggle-api))
    to download the data via a Terminal or Python program as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of both direct download and Kaggle API, you have to split your train
    data into smaller train and validation splits for this notebook.
  prefs: []
  type: TYPE_NORMAL
- en: You can create train and validation splits of the train data by using the
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn.model_selection.train_test_split` utility. Alternatively, you can
    download this directly from the accompanying code repository with this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In case you have any packages missing, you can install them from the notebook
    itself by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get the imports out of our way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, read the train file into a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | id | `comment_text` | `toxic` | `severe_toxic` | `obscene` | `threat`
    | `insult` | `identity_hate` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0000997932d777bf | Explanation`\r\nWhy` the edits made under my use...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 000103f0d9cfb60f | D''aww! He matches this background colour I''m s...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 000113f07ec002fd | Hey man, I''m really not trying to edit war. It...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0001b41b1c6bb37e | `\r\nMore\r\n` I can''t make any real suggestions...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0001d958c54c6e35 | You, sir, are my hero. Any chance you remember...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s read the validation data and preview the same as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | id | `comment_text` | `toxic` | `severe_toxic` | `obscene` | `threat`
    | `insult` | `identity_hate` |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 000eefc67a2c930f | Radial symmetry `\r\n\r\n` Several now extinct li...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 000f35deef84dc4a | There''s no need to apologize. A Wikipedia arti...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 000ffab30195c5e1 | Yes, because the mother of the child in the ca...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0010307a3a50a353 | `\r\nOk`. But it will take a bit of work but I ...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0010833a96e1f886 | `== A barnstar` for you! `==\r\n\r\n` The Real L...
    | 0 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Multiple target dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The interesting thing about this dataset is that each comment can have multiples
    labels. For instance, a comment could be insulting and toxic, or it could be obscene
    and have `identity_hate` elements in it.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we are leveling up here by trying to predict not one label (such as positive
    or negative), but multiple labels in one go. For each label, we'll predict a value
    between 0 and 1 to indicate how likely it is to belong to that category.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a probability value in the Bayesian meaning of the word, but represents
    the same intent.
  prefs: []
  type: TYPE_NORMAL
- en: I'd recommend trying out the models that we saw earlier with this dataset, and
    re-implementing this code for our favourite IMDb dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s preview the test dataset as well using the same idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `id` | `comment_text` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 00001cee341fdb12 | Yo bitch Ja Rule is more succesful then you''ll...
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0000247867823ef7 | `== From RfC == \r\n\r\n` The title is fine as i...
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 00013b17ad220c46 | `\r\n\r\n == Sources == \r\n\r\n *` Zawe Ashto...
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 00017563c3f7919a | If you have a look back at the source, the in... |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 00017695ad8997eb | I don''t anonymously edit articles at all. |'
  prefs: []
  type: TYPE_TB
- en: This preview confirms that we have a text challenge. The focus here is on the
    semantic categorization of text. The test dataset does not have empty headers
    or columns for the target columns, but we can infer them from the train dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Why PyTorch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is a deep learning framework by Facebook, similar to TensorFlow by Google.
  prefs: []
  type: TYPE_NORMAL
- en: Being backed by Google, thousands of dollars have been spent on TensorFlow's
    marketing, development, and documentation. It also got to a stable 1.0 release
    almost a year ago, while PyTorch has only recently gotten to 0.4.1\. This means
    that it's usually easier to find a TensorFlow solution to your problem and that
    you can copy and paste code off the internet.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, PyTorch is programmer-friendly. It is semantically similar
    to NumPy and deep learning operations in one. This means that I can use the Python
    debugging tools that I am already familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pythonic**: TensorFlow worked like a C program in the sense that the code
    was all written in one session, compiled, and then executed, thereby destroying
    its Python flavor altogether. This has been solved by TensorFlow''s Eager Execution
    feature release, which will soon be stable enough to use for most prototyping
    work.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Loop Visualization:** Up until a while ago, TensorFlow had a good
    visualization tool called TensorBoard for understanding training and validation
    performance (and other characteristics), which was absent in PyTorch. For a long
    while now, tensorboardX makes TensorBoard easy to use with PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, I recommend using PyTorch because it is easier to debug, more Pythonic,
    and more programmer-friendly.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch and torchtext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can install the latest version of Pytorch ([website](https://pytorch.org/))
    via conda or pip for your target machine. I am running this code on a Windows
    laptop with a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: I have installed torch using `conda install pytorch cuda92 -c pytorch`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For installing `torchtext`, I recommend using pip directly from their GitHub
    repository with the latest fixes instead of PyPi, which is not frequently updated.
    Uncomment the line when running this notebook for the first time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up the imports for `torch`, `torch.nn` (which is used in modeling),
    and `torchtext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you are running this code on a machine with a GPU, leave the `use_gpu` flag
    set to `True`; otherwise, set it to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you set `use_gpu=True`, we will check whether the GPU is accessible to PyTorch
    or not using the `torch.cuda.is_available()` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how many GPU devices are available to PyTorch on this machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Data loaders with torchtext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing good data loaders is the most tedious part in most deep learning applications.
    This step often combines the preprocessing, text cleaning, and vectorization tasks
    that we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, it wraps our static data objects into iterators or generators.
    This is incredibly helpful in processing data sizes much larger than GPU memory—which
    is quite often the case. This is done by splitting the data so that you can make
    batches of batchsize samples that fit your GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: Batchsizes are often powers of 2, such as 32, 64, 512, and so on. This convention
    exists because it helps with vector operations on the instruction set level. Anecdotally,
    using a batchsize that's different from a power of 2 has not helped or hurt my
    processing speed.
  prefs: []
  type: TYPE_NORMAL
- en: Conventions and style
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code, iterators, and wrappers that we will be using are from [Practical
    Torchtext](https://github.com/keitakurita/practical-torchtext/). This is a `torchtext`
    tutorial that was created by Keita Kurita—one of the top five contributors to
    `torchtext`.
  prefs: []
  type: TYPE_NORMAL
- en: The naming conventions and style are loosely inspired from the preceding work
    and fastai—a deep learning framework based on PyTorch itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by setting up the required variable placeholders in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Field` class determines how the data is preprocessed and converted into
    a numeric format. The `Field` class is a fundamental `torchtext` data structure
    and worth looking into. The `Field` class models common text processing and sets
    them up for numericalization (or vectorization):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: By default, all of the fields take in strings of words as input, and then the
    fields build a mapping from the words to integers later on. This mapping is called
    the vocab, and is effectively a one-hot encoding of the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw that each label in our case is already an integer marked as 0 or 1\.
    Therefore, we will not one-hot this – we will tell the `Field` class that this
    is already one-hot encoded and non-sequential by setting `use_vocab=False` and
    `sequential=False`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A few things are happening here, so let''s unpack it a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lower=True`: All input is converted to lowercase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequential=True`: If `False`, no tokenization is applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer`: We defined a custom tokenize function that simply splits the string
    on the space. You should replace this with the spaCy tokenizer (set `tokenize="spacy"`)
    and see if that changes the loss curve or final model''s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing the field
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Along with the keyword arguments that we've already mentioned, the `Field` class
    will also allow the user to specify special tokens (`unk_token` for out-of-vocabulary
    unknown words, `pad_token` for padding, `eos_token` for the end of a sentence,
    and an optional `init_token` for the start of the sentence).
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessing and postprocessing parameters accept any `torchtext.data.Pipeline`
    that it receives. Preprocessing is applied after tokenizing but before numericalizing.
    Postprocessing is applied after numericalizing, but before converting them into
    a Tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The docstrings for the `Field` class are relatively well written, so if you
    need some advanced preprocessing, you should probe them for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`TabularDataset` is the class that we use to read `.csv`, `.tsv`, or `.json`
    files. You can specify the type of file that you are reading, that is, `.tsv`
    or `.json`, directly in the API, which is powerful and handy'
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, you might think that this class is a bit misplaced because
    a generic file I/O+processor API should be accessible directly in PyTorch and
    not in a package dedicated to text processing. Let's see why it is placed where
    it is.
  prefs: []
  type: TYPE_NORMAL
- en: '`TabularData` has an interesting `fields` input parameter. For the CSV data
    format, `fields` is a list of tuples. Each tuple in turn is the column name and
    the `torchtext` variable we want to associate with it. The fields should be in
    the same order as the columns in the CSV or TSV file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have only two defined fields here: TEXT and LABEL. Therefore, each column
    is tagged as either one. We can simply mark the column as None if we want to ignore
    it completely. This is how we are tagging our columns as inputs (TEXT) and targets
    (LABEL) for the model to learn.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This tight coupling of the fields parameter with `TabularData` is why this
    is part of `torchtext` and not PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This defines our list of inputs. I have done this manually here, but you could
    also do this with code by reading the column headers from `train_df` and assigning
    them TEXT or LABEL accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, we will have to define another fields list for our test data
    because it has a different header. It has no LABEL fields.
  prefs: []
  type: TYPE_NORMAL
- en: '`TabularDataset` supports two APIs: `split` and `splits`. We will use the one
    with the extra s, `splits`. The splits API is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '`path`: This is the prefix of filenames'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train`, `validation`: These are filenames of the corresponding dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`format`: Either `.csv`, `.tsv`, or `.json`, as stated earlier; this is set
    to `.csv` here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_header`: This is set to `True` if your `.csv` file has column titles
    in it, as does ours'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fields`: We pass the list of fields we just set up previously:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s repeat the same for test data now. We drop the `id` column again and
    set `comment_text` to be our label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We pass the entire relative file path directly into the path, instead of using
    the `path` and `test` variable combination here. We used the `path` and `train`
    combination when setting up the `trn` and `vld` variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a note, these filenames are consistent with what Keita used in the `torchtext`
    tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the dataset objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the dataset objects, that is, `trn`, `vld`, and `tst`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'They are all objects from the same class. Our dataset objects can be indexed
    and iterated over like normal lists, so let''s see what the first element looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: All our elements are, in turn, objects of the `example.Example` class. Each
    example stores each column as an attribute. But where did our text and labels
    go?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Example` object bundles the attributes of a single data point together.
    Our `comment_text` and the `labels` are now part of the dictionary that makes
    up each of these example objects. We found all of them by calling `__dict__.keys()`
    on an `example.Example` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The text has already been tokenized for us, but has not yet been vectorized
    or numericalized. We will use one-hot encoding for all the tokens that exist in
    our training corpus. This will convert our words into integers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this by calling the `build_vocab` attribute of our `TEXT` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This statement processes the entire train data – in particular, the `comment_text`
    field. The words are registered in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: To handle the vocabulary, `torchtext` has its own class. The `Vocab` class can
    also take options such as `max_size` and `min_freq` that can let us know the number
    of words present in the vocabulary or how many times a word has to appear to be
    registered in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Words that are not included in the vocabulary will be converted into `<unk>`,
    a token meaning for *unknown*. Words that occur that are too rare are also assigned
    the `<unk>` token for ease of processing. This can hurt or help the model''s performance,
    depending on which and how many words we lose to the `<unk>` token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `TEXT` field now has a vocab attribute that is a specific instance of the
    `Vocab` class. We can use this in turn to look up the attributes of the vocab
    object. For instance, we can find the frequency of any word in the training corpus.
    The `TEXT.vocab.freqs` object is actually an object of `type collections.Counter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that it will support all functions, including the `most_common`
    API to sort the words by frequency and find the top k most frequently occurring
    words for us. Let''s take a look at them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Vocab` class holds a mapping from word to `id` in its `stoi` attribute
    and a reverse mapping in its `itos` attribute. Let''s look at these attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**itos**, or integer to string mapping, is a list of words. The index of each
    word in the list is its integer mapping. For instance, the 7-indexed word would
    be *and* because its integer mapping is 7.'
  prefs: []
  type: TYPE_NORMAL
- en: '**stoi**, or string to integer mapping, is a dictionary of words. Each key
    is a word in the training corpus, with the value being an integer. For instance,
    the word "and" might have an integer mapping that can be looked up in this dictionary
    in O(1) time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this convention automatically handles the off-by-one problem caused
    by zero indexing in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Iterators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`torchtext` has renamed and extended the `DataLoader` objects from PyTorch
    and torchvision. In essence, it does the same three jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: Batching the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuffling the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the data in parallel using `multiprocessing` workers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This batch loading of data enables us to process a dataset that's much larger
    than the GPU RAM. `Iterators` extend and specialize the `DataLoader` for NLP/text
    processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use both `Iterator` and its cousin, `BucketIterator`, here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: BucketIterator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`BucketIterator` automatically shuffles and buckets the input sequences into
    sequences of similar length.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable batch processing, we need the input sequences in a batch that''s
    of identical length. This is done by padding the smaller input sequences to the
    length of the longest sequence in batch. Check out the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will need to be padded to become the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, the padding operation is most efficient when the sequences are
    of similar lengths. The `BucketIterator` does all of this behind the scenes. This
    is what makes it an extremely powerful abstraction for text processing.
  prefs: []
  type: TYPE_NORMAL
- en: We want the bucket sorting to be based on the lengths of the `comment_text`
    field, so we pass that in as a keyword argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and initialize the iterators for the train and validation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a quick glance at the parameters we passed to this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size`: We use a small batch size of 32 for both train and validation.
    This is because I am using a GTX 1060 with only 3 GB of memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sort_key`: `BucketIterator` is told to use the number of tokens in the `comment_text`
    as the key to sort in any example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sort_within_batch`: When set to `True`, this sorts the data within each minibatch
    in decreasing order, according to the `sort_key`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repeat`: When set to True, it allows us to loop over and see a previously
    seen sample again. We set it to `False` here because we are repeating using an
    abstraction that we will write in a minute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the meanwhile, let''s take a minute to explore the new variable that we
    just made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, all that each batch has is torch tensors of exactly the same size (the
    size is the length of the vector of the vector of the vector of the vector of
    the vector of the vector of the vector here). These tensors have not been moved
    to GPU yet, but that's fine.
  prefs: []
  type: TYPE_NORMAL
- en: '`batch` is actually a wrapper over the already familiar example object that
    we have seen. It bundles all the attributes related to the batch in one variable
    dict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If our preceding understanding is correct, and we know how Python''s object
    passing works, the dataset attribute of the batch variable should point to the
    `trn` variable of the `torchtext.data.TabularData` type. Let''s check for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Aha! We got this right.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the test iterator, since we don''t need shuffling, we will use the plain
    `torchtext` `Iterator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at this iterator, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The sequence length of `33` here is different from the input's `25`. That's
    fine. We can see that this is also a torch tensor now.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's write a wrapper over the batch objects.
  prefs: []
  type: TYPE_NORMAL
- en: BatchWrapper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we delve into `BatchWrapper`, let me tell you what the problem with batch
    objects is. Our batch iterator returns a custom datatype, `torchtext.data.Batch`.
    This has a similar to multiple `example.Example`. This returns with a batch of
    data from each field as attributes. This custom datatype makes code reuse difficult
    since, each time the column names change, we need to modify the code. This also
    makes `torchtext` hard to use with other libraries such as torchsample and fastai.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we solve this?
  prefs: []
  type: TYPE_NORMAL
- en: We will convert the batch into a tuple in the form (x, y). x is the input to
    the model and y is the target – or, more conventionally, x is the independent
    variable while y is the dependent variable. One way to think about this is that
    the model will learn the function mapping from x to y.
  prefs: []
  type: TYPE_NORMAL
- en: 'BatchWrapper helps us reuse the modeling, training, and other code functions
    across datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `BatchWrapper` class accepts the iterator variable itself, the variable
    x name, and the variable y name during initialization. It yields tensor x and
    y. The x and y values are looked up from the `batch` in `self.dl` using `getattr`.
  prefs: []
  type: TYPE_NORMAL
- en: If GPU is available, this class moves these tensors to the GPU as well with
    `x.cuda()` and `y.cuda()`, making it ready for consumption by the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly wrap our `train`, `val`, and `test iter` objects using this
    new class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the simplest iterator, ready for model processing. Note that,
    in this particular case, the tensor has a "device" attribute set to `cuda:0`.
    Let''s preview this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Training a text classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready for training our text classifier model. Let''s start with
    something simple: we are going to consider this model to be a black box for now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model architecture is better explained by other sources, including several
    YouTube videos such as those by CS224n at Stanford ([http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)[)](http://web.stanford.edu/class/cs224n/).
    I suggest that you explore and connect it with the know-how that you already have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: All PyTorch models inherit from `torch.nn.Module`. They must all implement the
    `forward` function, which is executed when the model makes a prediction. The corresponding
    `backward` function for training is auto-computed.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any Pytorch model is instantiated like a Python object. Unlike TensorFlow, there
    is no strict notion of a session object inside which the code is compiled and
    then run. The model class is as we have written previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `init` function of the preceding class accepts a few parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_dim`: These are hidden layer dimensions, that is, the vector length
    of the hidden layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`emb_dim=300`: This is an embedding dimension, that is, the vector length of
    the first input *step* to the LSTM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_linear=2`: The other two dropout parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spatial_dropout=0.05`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recurrent_dropout=0.1`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both dropout parameters act as regularizers. They help prevent the model from
    overfitting, that is, the state where the model ends up learning the samples in
    the training set instead of the more generic pattern that can be used to make
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to think about the differences between the dropouts is that one of
    them acts on the input itself. The other acts during backpropagation or the weight
    update step, as mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We can print any PyTorch model to look at the architecture of the class. It
    is computed from the forward function implementation, which is exactly what we'd
    expect. This is really helpful when debugging the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's write a small utility function to calculate the size of any PyTorch model.
    By size, we mean the number of model parameters that can be updated during training
    to learn the input-to-target mapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this function is implemented in Keras, it''s simple enough to write it
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We can see that even our simple baseline model has more than 4 million parameters.
    In comparison, a typical decision tree might only have a few hundred decision
    splits, maximum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will move the model weights to the GPU using the familiar `.cuda()`
    syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Putting the pieces together again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are the pieces which we looked at, let''s quickly summarize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss function**: Binary cross entropy with Logit loss. It serves as the quality
    metric of how far the predictions are from the ground truth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer**: We use the Adam optimizer with default parameters, set with
    a learning rate of 1e-2 or 0.01:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is how we would see these 2 components in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We call set the number of epochs for which the model has to be trained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This is set to a very small value because this entire notebook, model, and training
    loop is just for demonstrative purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Training loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training loop is logically split into two sections: `model.train()` and
    `model.eval()`. Note the placement of the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The first half is the actual learning loop. This is the sequence of steps inside
    the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the optimizer's gradient to zero
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make model predictions on this training batch in `preds`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the loss using `loss_func`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the model weights using `loss.backward()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the optimizer state using `opt.step()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The entire back propagation hassle is handled in one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This level of abstraction that exposes the model's internals without worrying
    about the differential calculus aspects is why frameworks such as PyTorch are
    so convenient and useful.
  prefs: []
  type: TYPE_NORMAL
- en: The second loop is the evaluation loop. This is run on the validation split
    of the data. We set the model to *eval* mode, which locks the model weights. The
    weights will not be updated by accident as long as `model.eval()` is not set back
    to `model.train()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only two things we do inside this second loop are simple:'
  prefs: []
  type: TYPE_NORMAL
- en: Make predictions on the validation split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the loss on this split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aggregate loss from all validation batches is then printed at the end of
    every epoch, along with running training loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'One training loop will look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the training loop ends with a low validation loss but a high
    training loss. This could be indicative of something wrong with either the model
    or the train and valid data splits. There is no easy way to debug this.
  prefs: []
  type: TYPE_NORMAL
- en: The good way forward is usually to train the model for a few more epochs until
    no further change in either loss is observed.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use the model we trained to make some predictions on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The entire loop is now in eval mode, which we use to lock the model weights.
    Alternatively, we could have set `model.train(False)` as well.
  prefs: []
  type: TYPE_NORMAL
- en: We iteratively take batchsize samples from the test iterator, make predictions,
    and append them to a list. At the end, we stack them.
  prefs: []
  type: TYPE_NORMAL
- en: Converting predictions into a pandas DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This helps us convert the predictions into a more interpretable format. Let''s
    read the test dataframe and insert the predictions in the correct columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can preview a few of the rows of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | id | `comment_text` | `toxic` | `severe_toxic` | `obscene` | `threat`
    | `insult` | `identity_hate` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 00001cee341fdb12 | Yo bitch Ja Rule is more succesful then you''ll...
    | 0.629146 | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0000247867823ef7 | `== From RfC == \r\n\r\n` The title is fine as i...
    | 0.629146 | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 00013b17ad220c46 | "`\r\n\r\n == Sources == \r\n\r\n *`. Zawe Ashto...
    | 0.629146 | 0.116721 | 0.438606 | 0.156848 | 0.139696 | 0.388736 |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This was our first brush with deep learning for NLP. This was very a thorough
    introduction to `torchtext` and how we can leverage it with Pytorch. We also got
    a very broad view of deep learning as a puzzle of only two or three broad pieces:
    the model, the optimizer, and the loss functions. This is true irrespective of
    what framework or dataset you use.'
  prefs: []
  type: TYPE_NORMAL
- en: We did skimp a bit on the model architecture explanation in the interest of
    keeping this short. We will avoid using concepts that have not been explained
    here in other sections.
  prefs: []
  type: TYPE_NORMAL
- en: When we are working with modern ensembling methods, we don't always know how
    a particular prediction is being made. That's a black box to us, in the same sense
    that all deep learning model predictions are a black box.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at some tools and techniques that will help
    us look into these boxes – at least a little bit more.
  prefs: []
  type: TYPE_NORMAL
