- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mastering the Fundamentals of Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180), we briefly evaluated a fine-tuned
    **Large Language Model** (**LLM**) against a general-purpose model using in-context
    learning or the few-shot prompting approach. In this chapter, we will revisit
    and explore prompting techniques to examine how well we can adapt a general-purpose
    LLM without fine-tuning. We explore various prompting strategies that leverage
    the model’s inherent capabilities to produce targeted and contextually relevant
    outputs. We will start by examining the shift toward prompt-based language models.
    Then, we will revisit zero- and few-shot methods, explain prompt-chaining, and
    discuss various strategies, including more advanced techniques such as **Retrieval
    Augmented Generation** (**RAG**). At the end of the chapter, we will apply what
    we have learned and design a prompting strategy with the aim of consistently eliciting
    factual, accurate, and consistent responses that accomplish a specific business
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into specific prompt engineering techniques, we will review a
    few breakthroughs that pioneered **State-of-the-Art** (**SOTA**) prompt-based
    models. Research from early 2018 demonstrated how pretraining LLMs could enable
    few-shot generalization – accurate performance on new tasks given only a prompt
    statement and a few demonstrations. Follow-up work further tailored model architectures
    and training specifically for excelling at prompt-based inference across many
    text-specific tasks. More recent methods optimized model efficiency and stability,
    enabling accurate and reliable and efficient prompt completion. These innovations
    laid the groundwork for prompt engineering, demonstrating the remarkable versatility
    of prompt-based models with minimal input data. Now, prompt design is becoming
    its own subfield of research – unlocking SOTA performance for an ever-expanding
    range of tasks. Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: The shift to prompt-based approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in prior chapters, the development of the original GPT marked a
    significant advance in natural language generation, introducing the use of prompts
    to instruct the model. This method allowed models such as GPT to perform tasks
    such as translations – converting text such as “*Hello, how are you?*” to “*Bonjour,
    comment ça va?*” – without task-specific training, leveraging deeply contextualized
    semantic patterns learned during pretraining. This concept of interacting with
    language models via natural language prompts was significantly expanded with OpenAI’s
    GPT-3 in 2020\. Unlike its predecessors, GPT-3 showcased remarkable capabilities
    in understanding and responding to prompts in zero- and few-shot learning scenarios,
    a stark contrast to earlier models that weren’t as adept at such direct interactions.
    The methodologies, including the specific training strategies and datasets used
    for achieving GPT-3’s advanced performance, remain largely undisclosed. Nonetheless,
    it is inferred from OpenAI’s public research that the model learned to follow
    instructions based on its vast training corpus, and not explicit instruction-tuning.
    GPT-3’s success in performing tasks based on simple and direct prompting highlighted
    the potential for language models to understand and execute a wide range of tasks
    without requiring explicit task-specific training data for each new task. This
    led to a new paradigm in NLP research and applications, focusing on how effectively
    a model could be prompted with instructions to perform tasks such as summarization,
    translation, content generation, and more.
  prefs: []
  type: TYPE_NORMAL
- en: After the release of GPT-3, OpenAI was among the first to introduce specialized
    fine-tuning to respond more accurately to instructions in their release of InstructGPT
    (Ouyang et al., 2022). The researchers aimed to teach the model to closely follow
    instructions using two novel approaches. The first was **Supervised Fine-Tuning**
    (**SFT**), which involved fine-tuning using datasets carefully crafted from prompts
    and response pairs. These *demonstration* datasets were then used to perform SFT
    on top of the GPT-3 pretrained model, refining it to provide responses more closely
    aligned with human responses. *Figure 7**.1* provides an example of a prompt and
    response pair.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: InstructGPT SFT instruction and output pairs](img/B21773_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: InstructGPT SFT instruction and output pairs'
  prefs: []
  type: TYPE_NORMAL
- en: The second approach involved additional refinement using **Reinforcement Learning
    from Human Feedback** (**RLHF**). **Reinforcement Learning** (**RL**), established
    decades ago, aims to enhance autonomous agents’ decision-making capabilities.
    It does this by teaching them to optimize their actions based on the trade-off
    between risk and reward. The policy captures the guidelines for the agent’s behavior,
    dynamically updating as new insights and feedback are learned to refine decisions
    further. RL is the exact technology used in many robotic applications and is most
    famously applied to autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF is a variation of traditional RL, incorporating human feedback alongside
    the usual risk/reward signals to direct LLM behavior toward better alignment with
    human judgment. In practice, human labelers would provide preference ratings on
    model outputs from various prompts, and these ratings would be used to update
    the model policy, steering the LLM to generate responses that better conform to
    expected user intent across a range of tasks. In effect, this technique helped
    to reduce the model’s tendency to generate inappropriate, biased, harmful, or
    otherwise undesirable content. Although RLHF is not a perfect solution in this
    regard, it represents a significant step toward models that better understand
    and align with human values.
  prefs: []
  type: TYPE_NORMAL
- en: Later that year, following OpenAI’s introduction of InstructGPT, Google unveiled
    **Fine-tuned Language Net** or **FLAN** (Wei et al., 2021). FLAN represented another
    leap toward prompt-based LLMs, employing explicit instruction tuning. Google’s
    approach relied on formatting existing datasets into instructions, enabling the
    model to understand various tasks. Specifically, the authors of FLAN merged multiple
    NLP datasets across different categories, such as translation and question answering,
    creating distinct instruction templates for each dataset to frame them as instruction-following
    tasks. For example, the FLAN team leveraged ANLI challenges (Nie et al., 2020)
    to construct question-answer pairs explicitly designed to test the model’s understanding
    of complex textual relationships and reasoning. By framing these challenges as
    question-answer pairs, the FLAN team could directly measure a model’s proficiency
    in deducing these relationships under a unified instruction-following framework.
    Through this innovative approach, FLAN effectively broadened the scope of tasks
    a model can learn from, enhancing its overall performance and adaptability across
    a diverse set of NLU benchmarks. *Figure 7**.2* presents a theoretical example
    of question-answer pairs based on ANLI.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: Training templates based on the ANLI dataset](img/B21773_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Training templates based on the ANLI dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Again, the central idea behind FLAN was that each benchmark dataset (e.g., ANLI)
    could be translated into an intuitive instruction format, yielding a broad mixture
    of instructional data and natural language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: These advancements, among others, represent a significant evolution in the capabilities
    of LLMs, transitioning from models that required specific training for each task
    to those that can intuitively follow instructions and adapt to a multitude of
    tasks with a simple prompt. This shift has not only broadened the scope of tasks
    these models can perform but also demonstrated the potential for AI to process
    and generate human language in complex ways with unprecedented precision.
  prefs: []
  type: TYPE_NORMAL
- en: With this insight, we can shift our focus to prompt engineering. This discipline
    combines technical skill, creativity, and human psychology to maximize how models
    comprehend and respond, appropriately and accurately, to instructions. We will
    learn prompting techniques that increasingly influence the model’s behavior toward
    precision.
  prefs: []
  type: TYPE_NORMAL
- en: Basic prompting – guiding principles, types, and structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180), we introduced the concept
    of zero- and few-shot learning, providing the model either a direct instruction,
    or a direct instruction paired with examples specific to the task. In this section,
    we will focus on zero-shot learning, where prompting becomes a critical tool for
    guiding the model to perform specific tasks without prior explicit training on
    those tasks. This section explores elements of a prompt and how to structure it
    effectively for zero-shot learning. However, we will first establish some critical
    guiding principles to help us understand expected model behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Guiding principles for model interaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is absolutely critical to understand that LLMs, despite their unprecedented
    SOTA performance on natural language tasks, have significant inherent limitations,
    weaknesses, and susceptibilities. As described in [*Chapter 1*](B21773_01.xhtml#_idTextAnchor015),
    LLMs cannot establish rationale or perform logical operations natively. Our interactions
    with LLMs are typically supplemented by a highly sophisticated application layer
    that enables the raw model to carry on an extended exchange, integrate with systems
    that perform computations, and retrieve additional information and knowledge not
    intrinsic to the model itself. Independent of supplemental integrations, many
    LLMs are prone to erratic behavior. The most common of these is often referred
    to as **hallucination**, where the model generates a plausible output that is
    not entirely factual. As such, we should approach the general use of LLMs with
    the following guidelines in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apply domain knowledge and subject-matter expertise**: As SOTA LLMs are prone
    to generating inaccuracies that sound plausible, in use cases where factuality
    and precision are essential (e.g., code generation, technical writing, or academic
    research), users must have a firm grasp of the subject matter to detect potential
    inaccuracies. For example, suppose a user without medical expertise were to prompt
    a model for healthcare advice. In that case, the model may confuse, conflate,
    or simply invent information that could result in misleading or potentially dangerous
    advice. A mitigant for this behavior could be to provide the model with information
    from a reputable health journal and instruct it to generate its answers explicitly
    from the passages provided. This technique is often called grounding, and we will
    cover it in depth later. However, even when supplementing the model’s knowledge
    with verified information, the model can still misrepresent facts. Without expertise
    in the specific domain in question, we may never detect misinformation. Consequently,
    we should generally avoid using LLMs when we cannot verify the model output. Moreover,
    we should avoid using LLMs in high-stake scenarios where erroneous output could
    have profound implications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Acknowledge bias, underrepresentation, and toxicity**: We have described
    how LLMs are trained at an enormous scale and often on uncurated datasets. Inevitably,
    LLMs will learn, exhibit, and amplify societal biases. The model will propagate
    stereotypes, reflect biased assumptions, and generate toxic and harmful content.
    Moreover, LLMs can overrepresent certain populations and grossly underrepresent
    others, leading to a skewed or warped sociological perspective. These notions
    of bias can manifest in many ways. We will explore this topic, and other ethical
    implications of LLM use, in detail in [*Chapter 8*](B21773_08.xhtml#_idTextAnchor251).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoid ambiguity and lack of clarity**: Since LLMs were trained to synthesize
    information resembling human responses, they can often exhibit notions of creativity.
    In practice, if prompting is ambiguous or lacks clarity, the model will likely
    use its vast contextualized knowledge to “assume” or “infer” the meaning or objective
    of a given prompt or instruction. It may apply some context from its training
    instead of responding with a clarifying question. As we will describe in the next
    section, it is crucial to provide clarity by contextualizing input in most cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have established a few overarching principles to help navigate interactions
    and keep us within the boundaries of appropriate use, we can deconstruct the various
    elements of a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt elements and structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, a prompt acts as a guide, directing the model’s response toward the
    desired outcome. It typically comprises key elements that frame the task at hand,
    providing clarity and direction for the model’s generative capabilities. The following
    table presents the essential elements of a zero-shot prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Instruction** | A clear, concise statement describing what you want the
    model to do. This could be a direct command, a question, or a statement that implies
    a task. |'
  prefs: []
  type: TYPE_TB
- en: '| **Context** | Relevant information or background is needed to understand
    the instruction or the task. This could include definitions or clarifications.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Input** | Following the instructions, the model should work with specific
    data or content. This could be a piece of text, a question, or any information
    relevant to the task. |'
  prefs: []
  type: TYPE_TB
- en: '| **Output cue** | An indication of how the model’s response is to be structured.
    This can be part of the instruction or implied through the prompt’s formatting.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.1: Basic elements of a zero-shot prompt'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then structure these elements to maximize the zero-shot approach, whereby
    the model relies entirely on the prompt to understand and execute a task. In this
    context, we use the term *task* to describe a specific natural language task,
    such as summarization or translation. However, we will also encounter the term
    *task* applied more broadly to refer to the output the model should provide. Let’s
    explore a few concrete examples of various tasks. In this case, we will be referring
    to specific NLP tasks and applying a standard structure combining the key elements
    we’ve described:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Renewable energy sources like solar and wind power offer sustainable alternatives
    to fossil fuels, reducing greenhouse gas emissions and promoting` `environmental
    conservation...`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"Renewable energy sources, such as solar and wind, play a crucial role in
    reducing emissions and conserving` `the environment."`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"Hello, how` `are you?"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`This translates to "Hola, ¿``cómo estás?"`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The structured templates help us to efficiently and reliably prompt the model
    for a wide range of inputs, while maintaining a structure that the model has learned
    to recognize and respond to. In fact, we can take this a step further by asking
    the model to provide a specific format in its output. Using the output cue, we
    can instruct the model to provide a specified format such as Markdown.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"Please write a Python function to calculate the square of` `a number."`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Cue**: By using the Markdown format in the output cue, the model knows
    to provide this format and returns the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using LangChain to produce JSON-formatted output, we can leverage the same
    approach. Specifically, LangChain’s `PromptTemplate` provides a flexible way to
    dynamically define a structure for our prompts and insert elements:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Crafting effective prompts for zero-shot learning with LLMs requires a clear
    understanding of the task, thoughtful structuring of the prompt, and consideration
    of how the model interprets and responds to different elements within the prompt.
    By applying these principles, we can guide models to perform various tasks accurately
    and effectively. Subsequently, we will explore methods to guide models’ behavior
    through positive affirmations, emotional engagement, and other cognitive-behavioral
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Elevating prompts – iteration and influencing model behaviors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce techniques for enhancing AI model interactions
    inspired by cognitive-behavioral research. Behavioral prompting can guide models
    toward more accurate and nuanced responses. For example, LLM performance can be
    improved by providing the model with positive emotional stimuli, asking the model
    to assume a persona or character, or using situational prompting (i.e., role-play).
    However, it is crucial to recognize that these techniques can also be misused
    or used to inadvertently introduce stereotypes, as they rely on assumptions and
    generalizations that may not accurately reflect individual experiences or diverse
    perspectives. Without careful consideration and monitoring, there is a risk of
    reinforcing existing biases or creating new ones, potentially leading to skewed
    or harmful output. Given these challenges, we will explore a responsible approach
    to employing cognitive-behavioral techniques in AI interactions, aiming to harness
    their benefits while minimizing risks and ensuring inclusivity and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs respond to emotional cues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Research conducted by Microsoft in collaboration with various institutions,
    including the Beijing Normal University psychology department, suggests that LLMs
    can mimic and display some aspects of human emotional intelligence. This can lead
    to improved task performance when prompts are infused with emotional stimuli.
    In particular, the researchers hypothesize that emphasizing positive words can
    trigger more constructive and effective responses. The phenomenon is not well
    understood, but the effect is that positive emotional cues seem to improve model
    performance on various tasks consistently (Li et al., 2023). For example, the
    researchers input phrases encouraging confidence and positive outcomes, such as
    `This is very important to my career` or `Believe in your abilities and strive
    for excellence. Your hard work will yield remarkable results`. These types of
    psychological elements not only positively influenced the output quality but also
    made interactions more effective and nuanced.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of personas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Research has shown that the effect of **personas** enables LLMs to better simulate
    human-like interactions by adopting varied characteristics, perspectives, and
    response styles tailored to specific user profiles. This approach enhances the
    model’s capability for engaging in more personalized, contextually relevant conversations.
    Incorporating personas into LLM prompts effectively shifts the distribution of
    possible outputs, directing the model’s generation process toward responses that
    resonate with specific character traits or perspectives. Thanks to the model’s
    extensive training on diverse datasets, it can replicate patterns aligning with
    the described characteristics of personas, focusing its attention and selection
    process on a more defined set of outputs. This enhances relevance and specificity.
  prefs: []
  type: TYPE_NORMAL
- en: 'For clarity, let’s reconsider our hypothetical business StyleSprint, a fashion
    brand. If we prompt an LLM with “`As a creative and bold fashion brand, how would
    StyleSprint describe its upcoming summer collection?`”, the model, adopting the
    brand’s persona, might generate a response such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: “`This summer, StyleSprint dares you to stand out! Our collection is a vibrant
    explosion of colors and patterns, designed for the fearless fashionista. Think
    bold prints, daring cuts, and a statement that screams confidence. From sun-drenched
    beaches to chic city streets, our pieces promise to make every moment unforgettable.
    Embrace your boldness with StyleSprint` `this summer.`”.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we can see how personas guide the model’s outputs to reflect
    specific brand tones, such as creativity and boldness in this case, making the
    marketing material more engaging and aligned with the brand’s identity. Furthermore,
    using a few-shot approach, we could imbue model responses with a very specific
    branded tone. We will discuss few-shot in the sections that follow.
  prefs: []
  type: TYPE_NORMAL
- en: However, as discussed, personas should be used with caution. Personas can perpetuate
    stereotypes and biases, particularly against marginalized groups. A study conducted
    by researchers at Stanford University found that generating personas based on
    intersectional demographic groups often yields higher rates of racial stereotypes
    and patterns of **othering**, or portraying someone or a group as fundamentally
    different or alien, compared to human-written texts. In some cases, model outputs
    could amplify narratives and tropes (Cheng, Durmus, & Jurafsky, 2023).
  prefs: []
  type: TYPE_NORMAL
- en: Situational prompting or role-play
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Role-play in LLMs, in the same way as personas, involves adopting specific identities
    or characteristics. However, the two serve different purposes and are applied
    in distinct contexts. Personas are predefined sets of traits or characteristics
    that an LLM mimics to tailor its responses, focusing on consistency with those
    traits. As we have demonstrated with our StyleSprint example, this is useful for
    creating content with a specific tone or perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, role-play extends beyond adopting a set of traits to engage in a
    scenario or narrative dynamically. It involves the LLM taking on a character within
    a simulated environment or story, responding to inputs in a manner that aligns
    with both a persona and the evolving context of the role-play scenario. This can
    be especially useful in complex simulations where the LLM must navigate and contribute
    to ongoing narratives or dialogues that require understanding and adapting to
    new information or changing circumstances in real time.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.3\uFEFF: Persona versus role-play](img/B21773_07_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Persona versus role-play'
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting our real-world scenario, role-play could be particularly useful for
    creating interactive and engaging customer service experiences. For example, StyleSprint
    could design a role-play scenario where the LLM acts as a virtual personal stylist.
    In this role, the model would engage customers with prompts such as `I'm your
    personal stylist for today! What's the occasion you're dressing for?`. Based on
    the customer’s response, the LLM could ask follow-up questions to narrow down
    preferences, such as `Do you prefer bold colors or pastel shades?`. Finally, it
    could recommend outfits from StyleSprint’s collection that match the customer’s
    needs, saying something such as `For a summer wedding, I recommend our Floral
    Maxi Dress paired with the Vintage Sun Hat. It's elegant, yet perfect for an`
    `outdoor setting!`.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we leverage the LLM’s ability to dynamically adapt its dialogue
    based on customer inputs to create an advanced recommender system that facilitates
    a highly personalized shopping experience. It not only helps in providing tailored
    fashion advice but also engages customers in a novel way.
  prefs: []
  type: TYPE_NORMAL
- en: Having examined how behavior-inspired techniques, such as personas and role-play,
    influence model behavior through zero-shot learning, let’s now turn our attention
    to few-shot learning. This is also known as in-context learning, which we described
    in [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180). Recall that the few-shot approach
    can enhance the consistency, stability, and reliability of model responses. By
    providing the model with a few examples of the desired output within the prompt
    itself, few-shot learning effectively teaches the model the specific task at hand,
    leading to more predictable and accurate outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced prompting in action – few-shot learning and prompt chaining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In few-shot settings, the LLM is presented with a small number of examples of
    a task within the input prompt, guiding the model to generate responses that align
    with these examples. As discussed in the prior chapter, this method significantly
    reduces the need for fine-tuning on large, task-specific datasets. Instead, it
    leverages the model’s pre-existing knowledge and ability to infer context from
    the examples provided. In [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180), we
    saw how this approach was particularly useful for StyleSprint by enabling the
    model to answer specific questions after being provided with just a few examples,
    enhancing consistency and creativity in brand messaging.
  prefs: []
  type: TYPE_NORMAL
- en: This method typically involves using between 10 and 100 examples, depending
    on the model’s context window. Recall that the context window is the limit of
    tokens a language model can process in one turn. The primary benefit of the few-shot
    approach is that it minimizes the risk of the model learning a too-narrow distribution
    from a specific dataset through fine-tuning. Although the performance of few-shot
    may not always match its fine-tuned counterpart, few-shot learning often outperforms
    both one-shot and zero-shot learning, showing significant improvements in task
    adaptation and accuracy. This is especially true as more examples are added to
    the context window (Brown et al., 2020).
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications such as LangChain provide a simple and convenient pattern for
    few-shot implementation. Consider a scenario in which StyleSprint would like to
    generate taglines for its seasonal collections. In this case, we can provide the
    model with examples written by the content team to guide the model toward consistency
    with the brand tone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The LangChain API offers `FewShotPromptTemplate` to format the examples consistently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now apply the template to an LLM to generate a response that we can
    expect will closely align with the tone and style of our examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a consistent and programmatic method for providing the model
    with examples, we can iterate over the model responses using prompt chaining.
    A prompt chain generally refers to chaining together multiple prompts and LLM
    interactions to have a conversation with the model and iteratively build on the
    results. Remember, the model itself cannot store information and effectively has
    no memory or prior inputs and outputs. Instead, the application layer stores prior
    inputs and outputs, which are then provided to the model with each exchange. For
    example, you might start with an initial prompt such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The LLM might generate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You could then construct a follow-up prompt using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You could then keep iterating to improve the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chaining facilitates guiding and interactively refining the text generated
    rather than relying purely on the given examples. Notice that our prior few-shot
    code had already established a chain, which we can now use to iterate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The model is now working from both the examples we provided and any additional
    instructions we want to include as part of the chain. Prompt chaining, combined
    with few-shot learning, provides a powerful framework for iteratively guiding
    language model outputs. By leveraging application state to maintain conversation
    context, we can steer the model toward desired responses in line with our provided
    examples. This approach balances harnessing the model’s inferential capabilities
    and retaining control to align its creative outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into our practice project, which implements RAG. RAG augments
    model responses by retrieving and incorporating external data sources. This technique
    mitigates hallucination risks by grounding AI-generated text in real data. For
    example, StyleSprint may leverage past customer survey results or catalog data
    to enhance product descriptions. By combining retrieval with prompt chaining,
    RAG provides a scalable method for balancing creativity with accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Practice project: Implementing RAG with LlamaIndex using Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our practice project, we will shift from LangChain to exploring another
    library that facilitates the RAG approach. LlamaIndex is an open source library
    that is specifically designed for RAG-based applications. LlamaIndex simplifies
    ingestion and indexing across various data sources. However, before we dive into
    implementation, we will explain the underlying methods and approach behind RAG.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, the key premise of RAG is to enhance LLM outputs by supplying
    relevant context from external data sources. These sources should provide specific
    and verified information to ground model outputs. Moreover, RAG can optionally
    leverage the few-shot approach by retrieving few-shot examples at inference time
    to guide generation. This approach alleviates the need to store examples in the
    prompt chain and only retrieves relevant examples when needed. In essence, the
    RAG approach is a culmination of many of the prompt engineering techniques we
    have already discussed. It provides structure, chaining, few-shot learning, and
    grounding.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the RAG pipeline can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The RAG component ingests and indexes domain-specific data sources using vector
    embeddings to encode semantics. As we learned in [*Chapter 3*](B21773_03.xhtml#_idTextAnchor081),
    these embeddings are imbued with deeply contextualized, rich semantic information
    that the component uses later to perform a semantic search.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The component then uses the initial prompt as a search query. The query is input
    to retrieval systems, which find the most relevant snippets from the indexed data
    based on vector similarity. Similar to how we applied semantic similarity in prior
    chapters, RAG leverages a similarity metric to rank results by semantic relevance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, the original prompt is augmented with information from the retrieved
    contexts, and the augmented prompt is passed to the LLM to generate a response
    grounded in the external data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RAG introduces two major benefits. First, like the chaining approach, the indexed
    external data acts as a form of memory, overcoming the LLM’s statelessness. Second,
    this memory can rapidly scale beyond model context window limitations, since examples
    are curated and only provided at the time of the request as needed. Ultimately,
    RAG unlocks otherwise unattainable capabilities in reliable and factual text generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our practice project, we revisited the StyleSprint product descriptions.
    This time, we want to leverage RAG to retrieve detailed information about the
    product to produce very specific descriptions. For the purpose of keeping this
    project accessible, we will implement an in-memory vector store (Faiss) instead
    of an external database. We begin with installing the necessary libraries. We
    will leverage LlamaIndex’s integrated support for Faiss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then import the necessary libraries, load the data, and create the
    index. This vector store will rely on OpenAI’s embeddings, so we must also define
    `OPENAI_API_KEY` using a valid key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a vector store that the model can rely on to retrieve our very
    specific product data. This means we can query for very specific responses augmented
    by our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The result is a response that not only provides an accurate description of the
    summer dress but also includes specific details, such as the price. This level
    of detail enriches the customer’s shopping experience, providing relevant and
    real-time information for customers to consider when making a purchase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to evaluate our RAG implementation to ensure that the answer
    is relevant, faithful to the source text, reflective of contextual accuracy, and
    not in any way harmful or inappropriate. We can apply an open source evaluation
    framework (RAGAS), which provides implementation of the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faithfulness** assesses the degree to which the generated response is faithful
    or true to the original context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer relevance** evaluates how relevant the generated answer is to the
    given question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context precision** measures the precision of the context used to generate
    the answer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context recall** measures the recall of the context used to generate the
    answer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context relevancy** assesses the relevancy of the context used to generate
    the answer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Harmfulness** evaluates whether a submission (or answer) contains anything
    that could potentially cause harm to individuals, groups, or society at large'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This suite of metrics provides an objective measure of RAG application performance
    based on a comparison to ground truth. In our case, we can use responses generated
    from our product data, along with context and ground truth derived from the original
    dataset, to construct an evaluation dataset and perform a comprehensive evaluation
    using the metrics described.
  prefs: []
  type: TYPE_NORMAL
- en: The following is a simplified code snippet implementing the RAGAS evaluation
    for our generated product descriptions. A complete working implementation is available
    in the `Chapter 7` folder of the GitHub companion to this book ([https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Our evaluation program should produce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can observe that the system performs well in generating accurate and relevant
    answers, as evidenced by high faithfulness and answer relevancy scores. While
    context precision shows room for improvement, half of the relevant information
    is correctly identified. Context recall is effective, retrieving most of the relevant
    context. The absence of harmful content ensures safe interactions. Overall, the
    system displays robust performance in answering accurately and contextually, but
    could benefit from refinements in pinpointing the most pertinent context snippets.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in *Chapters 5* and *6*, the evaluation of LLMs often requires
    the additional operational burden of collecting ground-truth data. However, doing
    so makes it possible to perform a robust evaluation of model and application performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the intricacies of prompt engineering. We also
    explored advanced strategies to elicit precise and consistent responses from LLMs,
    offering a versatile alternative to fine-tuning. We traced the evolution of instruction-based
    models, highlighting how they’ve shifted the paradigm toward an intuitive understanding
    and adaptation to tasks through simple prompts. We expanded on the adaptability
    of LLMs with techniques such as few-shot learning and retrieval augmentation,
    which allow for dynamic model guidance across diverse tasks with minimal explicit
    training. The chapter further explored the structuring of effective prompts, and
    the use of personas and situational prompting to tailor model responses more closely
    to specific interaction contexts, enhancing the model’s applicability and interaction
    quality. We also addressed the nuanced aspects of prompt engineering, including
    the influence of emotional cues on model performance and the implementation of
    RLHF to refine model outputs. These discussions underscored the potential of LLMs
    to exhibit some level of emotional intelligence, leading to more effective and
    nuanced interactions. However, alongside these technological strides, we stressed
    the paramount importance of ethical considerations. We highlighted the need for
    responsible adoption and vigilance to mitigate potential harm and biases associated
    with these techniques, ensuring fairness, integrity, and the prevention of misuse.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we learned how to implement and evaluate the RAG approach to ground
    the LLM in contextual information from trusted sources and produce answers that
    are relevant and faithful to the source text. In the next chapter, we will look
    more closely at the role of individuals in advancing generative AI while emphasizing
    the dual responsibility of developers and researchers to navigate this rapidly
    evolving field with a conscientious approach, balancing innovation with ethical
    imperatives and societal impacts.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  prefs: []
  type: TYPE_NORMAL
- en: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P.,
    Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton,
    F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J.,
    & Lowe, R. (2022). *Training language models to follow instructions with human
    feedback*. In arXiv [cs.CL]. [http://arxiv.org/abs/2203.02155](http://arxiv.org/abs/2203.02155)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai,
    A. M., & Le, Q. V. (2021). *Finetuned language models are zero-shot learners*.
    In arXiv [cs.CL]. [http://arxiv.org/abs/2109.01652](http://arxiv.org/abs/2109.01652)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., & Kiela, D. (2020).
    *Adversarial NLI: A new benchmark for natural language* *understanding*. Arxiv.org.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q.,
    & Xie, X. (2023). *Large Language Models understand and can be enhanced by emotional
    stimuli*. In arXiv [cs.CL]. [http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng, M., Durmus, E., & Jurafsky, D. (2023). *Marked personas: Using natural
    language prompts to measure stereotypes in language models. Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics* (Volume
    1: Long Papers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan,
    A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,
    G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., …
    Amodei, D. (2020). *Language Models are Few-Shot Learners*. In arXiv [cs.CL].
    [http://arxiv.org/abs/2005.14165](http://arxiv.org/abs/2005.14165)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
