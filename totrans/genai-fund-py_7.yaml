- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Mastering the Fundamentals of Prompt Engineering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握提示工程的基本原理
- en: In [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180), we briefly evaluated a fine-tuned
    **Large Language Model** (**LLM**) against a general-purpose model using in-context
    learning or the few-shot prompting approach. In this chapter, we will revisit
    and explore prompting techniques to examine how well we can adapt a general-purpose
    LLM without fine-tuning. We explore various prompting strategies that leverage
    the model’s inherent capabilities to produce targeted and contextually relevant
    outputs. We will start by examining the shift toward prompt-based language models.
    Then, we will revisit zero- and few-shot methods, explain prompt-chaining, and
    discuss various strategies, including more advanced techniques such as **Retrieval
    Augmented Generation** (**RAG**). At the end of the chapter, we will apply what
    we have learned and design a prompting strategy with the aim of consistently eliciting
    factual, accurate, and consistent responses that accomplish a specific business
    task.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第五章*](B21773_05.xhtml#_idTextAnchor180)中，我们简要评估了使用情境学习或少量提示方法对经过微调的**大型语言模型**（**LLM**）与通用模型进行对比。在本章中，我们将重新审视并探索提示技术，以检验我们如何在不进行微调的情况下适应通用LLM。我们探讨了各种利用模型内在能力产生针对性和上下文相关输出的提示策略。我们将首先考察向基于提示的语言模型的转变。然后，我们将回顾零样本和少量样本方法，解释提示链，并讨论各种策略，包括更高级的技术，如**检索增强生成**（**RAG**）。在本章末尾，我们将应用所学知识，设计一个提示策略，旨在持续地引发事实性、准确性和一致性的响应，以完成特定的业务任务。
- en: Before diving into specific prompt engineering techniques, we will review a
    few breakthroughs that pioneered **State-of-the-Art** (**SOTA**) prompt-based
    models. Research from early 2018 demonstrated how pretraining LLMs could enable
    few-shot generalization – accurate performance on new tasks given only a prompt
    statement and a few demonstrations. Follow-up work further tailored model architectures
    and training specifically for excelling at prompt-based inference across many
    text-specific tasks. More recent methods optimized model efficiency and stability,
    enabling accurate and reliable and efficient prompt completion. These innovations
    laid the groundwork for prompt engineering, demonstrating the remarkable versatility
    of prompt-based models with minimal input data. Now, prompt design is becoming
    its own subfield of research – unlocking SOTA performance for an ever-expanding
    range of tasks. Let’s get started.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨具体的提示工程技术之前，我们将回顾一些开创性的突破，这些突破为**最先进**（**SOTA**）的基于提示的模型铺平了道路。2018年初的研究展示了如何通过预训练LLM实现少量样本泛化——在仅提供提示语句和少量演示的情况下，在新任务上实现准确的表现。后续工作进一步调整了模型架构和训练，以在许多特定文本任务上的基于提示的推理中表现出色。最近的方法优化了模型的效率和稳定性，实现了准确、可靠和高效的提示完成。这些创新为提示工程奠定了基础，展示了基于提示的模型在最小输入数据下的非凡灵活性。现在，提示设计正成为研究的一个子领域——解锁SOTA性能，以应对不断扩大的任务范围。让我们开始吧。
- en: The shift to prompt-based approaches
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于提示的方法的转变
- en: As discussed in prior chapters, the development of the original GPT marked a
    significant advance in natural language generation, introducing the use of prompts
    to instruct the model. This method allowed models such as GPT to perform tasks
    such as translations – converting text such as “*Hello, how are you?*” to “*Bonjour,
    comment ça va?*” – without task-specific training, leveraging deeply contextualized
    semantic patterns learned during pretraining. This concept of interacting with
    language models via natural language prompts was significantly expanded with OpenAI’s
    GPT-3 in 2020\. Unlike its predecessors, GPT-3 showcased remarkable capabilities
    in understanding and responding to prompts in zero- and few-shot learning scenarios,
    a stark contrast to earlier models that weren’t as adept at such direct interactions.
    The methodologies, including the specific training strategies and datasets used
    for achieving GPT-3’s advanced performance, remain largely undisclosed. Nonetheless,
    it is inferred from OpenAI’s public research that the model learned to follow
    instructions based on its vast training corpus, and not explicit instruction-tuning.
    GPT-3’s success in performing tasks based on simple and direct prompting highlighted
    the potential for language models to understand and execute a wide range of tasks
    without requiring explicit task-specific training data for each new task. This
    led to a new paradigm in NLP research and applications, focusing on how effectively
    a model could be prompted with instructions to perform tasks such as summarization,
    translation, content generation, and more.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，原始GPT的发展标志着自然语言生成的一个重大进步，引入了使用提示来指导模型的方法。这种方法使得像GPT这样的模型能够在没有特定任务训练的情况下执行翻译等任务——将“*你好，你好吗？*”之类的文本转换为“*Bonjour,
    comment ça va?*”。这是在预训练期间学习到的深度上下文语义模式的基础上实现的。这种通过自然语言提示与语言模型交互的概念，在2020年OpenAI的GPT-3中得到了显著扩展。与前辈们不同，GPT-3在零样本和少样本学习场景中理解并响应提示的能力非常突出，这与早期模型在直接交互方面的不足形成了鲜明对比。包括用于实现GPT-3高级性能的具体训练策略和数据集在内的方法仍然大部分未公开。然而，从OpenAI的公开研究中可以推断，该模型学会了根据其庞大的训练语料库遵循指令，而不是显式的指令调整。GPT-3在基于简单直接提示执行任务方面的成功突显了语言模型理解并执行广泛任务的能力，而无需为每个新任务提供显式的特定任务训练数据。这导致了NLP研究和应用领域的一个新范式，关注的是模型如何有效地通过指令来执行摘要、翻译、内容生成等任务。
- en: After the release of GPT-3, OpenAI was among the first to introduce specialized
    fine-tuning to respond more accurately to instructions in their release of InstructGPT
    (Ouyang et al., 2022). The researchers aimed to teach the model to closely follow
    instructions using two novel approaches. The first was **Supervised Fine-Tuning**
    (**SFT**), which involved fine-tuning using datasets carefully crafted from prompts
    and response pairs. These *demonstration* datasets were then used to perform SFT
    on top of the GPT-3 pretrained model, refining it to provide responses more closely
    aligned with human responses. *Figure 7**.1* provides an example of a prompt and
    response pair.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3发布后，OpenAI是首批将专用微调引入其InstructGPT（Ouyang等，2022年）发布中，以更准确地响应指令的公司之一。研究人员旨在通过两种新颖的方法教会模型紧密遵循指令。第一种是**监督式微调**（**SFT**），它涉及使用从提示和响应对精心制作的集合数据集进行微调。然后，这些*演示*数据集被用来在GPT-3预训练模型之上执行SFT，从而使其提供与人类响应更接近的响应。"图7.1"提供了一个提示和响应对的示例。
- en: '![Figure 7.1: InstructGPT SFT instruction and output pairs](img/B21773_07_01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1：InstructGPT SFT指令和输出对](img/B21773_07_01.jpg)'
- en: 'Figure 7.1: InstructGPT SFT instruction and output pairs'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：InstructGPT SFT指令和输出对
- en: The second approach involved additional refinement using **Reinforcement Learning
    from Human Feedback** (**RLHF**). **Reinforcement Learning** (**RL**), established
    decades ago, aims to enhance autonomous agents’ decision-making capabilities.
    It does this by teaching them to optimize their actions based on the trade-off
    between risk and reward. The policy captures the guidelines for the agent’s behavior,
    dynamically updating as new insights and feedback are learned to refine decisions
    further. RL is the exact technology used in many robotic applications and is most
    famously applied to autonomous driving.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法涉及使用**人类反馈强化学习**（**RLHF**）进行额外的细化。**强化学习**（**RL**）是几十年前建立的，旨在提高自主代理的决策能力。它通过教会他们根据风险与奖励之间的权衡来优化其行为来实现这一点。策略捕捉了代理行为的指南，随着新见解和反馈的学习而动态更新，以进一步细化决策。RL是许多机器人应用中使用的确切技术，最著名的是应用于自动驾驶。
- en: RLHF is a variation of traditional RL, incorporating human feedback alongside
    the usual risk/reward signals to direct LLM behavior toward better alignment with
    human judgment. In practice, human labelers would provide preference ratings on
    model outputs from various prompts, and these ratings would be used to update
    the model policy, steering the LLM to generate responses that better conform to
    expected user intent across a range of tasks. In effect, this technique helped
    to reduce the model’s tendency to generate inappropriate, biased, harmful, or
    otherwise undesirable content. Although RLHF is not a perfect solution in this
    regard, it represents a significant step toward models that better understand
    and align with human values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF是传统RL的一种变体，它结合了人类反馈以及通常的风险/奖励信号，以指导LLM的行为更好地与人类判断相一致。在实践中，人类标注员会对来自各种提示的模型输出提供偏好评分，这些评分将用于更新模型策略，引导LLM生成更符合预期用户意图的响应，涵盖各种任务。实际上，这项技术有助于减少模型生成不适当、有偏见、有害或其他不受欢迎内容的倾向。尽管RLHF在这方面不是完美的解决方案，但它代表了朝着更好地理解和与人类价值观相一致的方向迈出的重要一步。
- en: Later that year, following OpenAI’s introduction of InstructGPT, Google unveiled
    **Fine-tuned Language Net** or **FLAN** (Wei et al., 2021). FLAN represented another
    leap toward prompt-based LLMs, employing explicit instruction tuning. Google’s
    approach relied on formatting existing datasets into instructions, enabling the
    model to understand various tasks. Specifically, the authors of FLAN merged multiple
    NLP datasets across different categories, such as translation and question answering,
    creating distinct instruction templates for each dataset to frame them as instruction-following
    tasks. For example, the FLAN team leveraged ANLI challenges (Nie et al., 2020)
    to construct question-answer pairs explicitly designed to test the model’s understanding
    of complex textual relationships and reasoning. By framing these challenges as
    question-answer pairs, the FLAN team could directly measure a model’s proficiency
    in deducing these relationships under a unified instruction-following framework.
    Through this innovative approach, FLAN effectively broadened the scope of tasks
    a model can learn from, enhancing its overall performance and adaptability across
    a diverse set of NLU benchmarks. *Figure 7**.2* presents a theoretical example
    of question-answer pairs based on ANLI.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那年稍后，随着OpenAI推出InstructGPT，谷歌揭幕了**微调语言网络**或**FLAN**（Wei et al., 2021）。FLAN代表了向基于提示的LLM的又一跃进，采用了显式的指令微调。谷歌的方法依赖于将现有数据集格式化为指令，使模型能够理解各种任务。具体来说，FLAN的作者们将多个不同类别（如翻译和问答）的NLP数据集合并在一起，为每个数据集创建独特的指令模板，将它们作为遵循指令的任务来构建。例如，FLAN团队利用ANLI挑战（Nie
    et al., 2020）构建了旨在测试模型对复杂文本关系和推理理解的问答对。通过将这些挑战作为问答对来构建，FLAN团队可以直接衡量模型在统一遵循指令框架下推断这些关系的熟练程度。通过这种创新方法，FLAN有效地扩大了模型可以学习的任务范围，增强了其在各种NLU基准测试中的整体性能和适应性。*图7.2*展示了基于ANLI的问答对的理论示例。
- en: '![Figure 7.2: Training templates based on the ANLI dataset](img/B21773_07_02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2：基于ANLI数据集的训练模板](img/B21773_07_02.jpg)'
- en: 'Figure 7.2: Training templates based on the ANLI dataset'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：基于ANLI数据集的训练模板
- en: Again, the central idea behind FLAN was that each benchmark dataset (e.g., ANLI)
    could be translated into an intuitive instruction format, yielding a broad mixture
    of instructional data and natural language tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，FLAN背后的核心思想是，每个基准数据集（例如，ANLI）都可以被转换成直观的指令格式，从而产生广泛的混合指令数据和自然语言任务。
- en: These advancements, among others, represent a significant evolution in the capabilities
    of LLMs, transitioning from models that required specific training for each task
    to those that can intuitively follow instructions and adapt to a multitude of
    tasks with a simple prompt. This shift has not only broadened the scope of tasks
    these models can perform but also demonstrated the potential for AI to process
    and generate human language in complex ways with unprecedented precision.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些进步以及其他进步代表了LLMs能力的重要演变，从需要为每个任务进行特定训练的模型转变为可以直观地遵循指令并适应多种任务的简单提示的模型。这种转变不仅扩大了这些模型可以执行的任务范围，还展示了AI以前所未有的精确度处理和生成人类语言复杂方式的潜力。
- en: With this insight, we can shift our focus to prompt engineering. This discipline
    combines technical skill, creativity, and human psychology to maximize how models
    comprehend and respond, appropriately and accurately, to instructions. We will
    learn prompting techniques that increasingly influence the model’s behavior toward
    precision.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个洞察，我们可以将我们的关注点转移到提示工程。这门学科结合了技术技能、创造力和人类心理学，以最大化模型对指令的理解和响应的适当性和准确性。我们将学习影响模型行为趋向精确的提示技术。
- en: Basic prompting – guiding principles, types, and structures
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本提示 - 指导原则、类型和结构
- en: In [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180), we introduced the concept
    of zero- and few-shot learning, providing the model either a direct instruction,
    or a direct instruction paired with examples specific to the task. In this section,
    we will focus on zero-shot learning, where prompting becomes a critical tool for
    guiding the model to perform specific tasks without prior explicit training on
    those tasks. This section explores elements of a prompt and how to structure it
    effectively for zero-shot learning. However, we will first establish some critical
    guiding principles to help us understand expected model behavior.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第五章*](B21773_05.xhtml#_idTextAnchor180)中，我们介绍了零样本和少样本学习的概念，为模型提供直接指令，或与特定任务的示例相结合的直接指令。在本节中，我们将重点关注零样本学习，其中提示成为引导模型在没有先前明确训练的情况下执行特定任务的临界工具。本节探讨了提示的要素以及如何有效地构建它以进行零样本学习。然而，我们首先将建立一些关键的指导原则，以帮助我们理解预期的模型行为。
- en: Guiding principles for model interaction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型交互的指导原则
- en: 'It is absolutely critical to understand that LLMs, despite their unprecedented
    SOTA performance on natural language tasks, have significant inherent limitations,
    weaknesses, and susceptibilities. As described in [*Chapter 1*](B21773_01.xhtml#_idTextAnchor015),
    LLMs cannot establish rationale or perform logical operations natively. Our interactions
    with LLMs are typically supplemented by a highly sophisticated application layer
    that enables the raw model to carry on an extended exchange, integrate with systems
    that perform computations, and retrieve additional information and knowledge not
    intrinsic to the model itself. Independent of supplemental integrations, many
    LLMs are prone to erratic behavior. The most common of these is often referred
    to as **hallucination**, where the model generates a plausible output that is
    not entirely factual. As such, we should approach the general use of LLMs with
    the following guidelines in mind:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一点至关重要，即尽管LLMs在自然语言任务上取得了前所未有的SOTA性能，但它们具有显著的内生局限性、弱点和易受攻击性。如[*第一章*](B21773_01.xhtml#_idTextAnchor015)所述，LLMs无法建立推理或执行原生逻辑运算。我们与LLMs的互动通常由一个高度复杂的应用层补充，该应用层使原始模型能够进行扩展交流，与执行计算的系统集成，并检索模型本身非固有的额外信息和知识。独立于补充集成，许多LLMs容易产生不稳定的行为。其中最常见的是通常被称为**幻觉**的现象，即模型生成一个看似合理但并非完全真实的输出。因此，我们应该带着以下指南来考虑LLMs的通用使用：
- en: '**Apply domain knowledge and subject-matter expertise**: As SOTA LLMs are prone
    to generating inaccuracies that sound plausible, in use cases where factuality
    and precision are essential (e.g., code generation, technical writing, or academic
    research), users must have a firm grasp of the subject matter to detect potential
    inaccuracies. For example, suppose a user without medical expertise were to prompt
    a model for healthcare advice. In that case, the model may confuse, conflate,
    or simply invent information that could result in misleading or potentially dangerous
    advice. A mitigant for this behavior could be to provide the model with information
    from a reputable health journal and instruct it to generate its answers explicitly
    from the passages provided. This technique is often called grounding, and we will
    cover it in depth later. However, even when supplementing the model’s knowledge
    with verified information, the model can still misrepresent facts. Without expertise
    in the specific domain in question, we may never detect misinformation. Consequently,
    we should generally avoid using LLMs when we cannot verify the model output. Moreover,
    we should avoid using LLMs in high-stake scenarios where erroneous output could
    have profound implications.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用领域知识和专业知识**：由于最先进的LLMs容易生成听起来合理的错误信息，在事实性和精确性至关重要的用例中（例如，代码生成、技术写作或学术研究），用户必须对主题有牢固的掌握，以便检测潜在的不准确性。例如，如果一个没有医学专业知识的使用者要求模型提供医疗建议，模型可能会混淆、混淆或简单地发明可能导致误导或潜在危险建议的信息。对此行为的缓解措施之一是向模型提供来自权威健康杂志的信息，并指示它明确地从提供的段落中生成答案。这种技术通常被称为“扎根”，我们将在稍后深入探讨。然而，即使在补充模型知识以验证信息的情况下，模型仍然可能错误地表述事实。如果没有特定领域的专业知识，我们可能永远无法检测到错误信息。因此，我们应该一般避免在无法验证模型输出时使用LLMs。此外，我们应该避免在高风险场景中使用LLMs，因为错误的输出可能具有深远的影响。'
- en: '**Acknowledge bias, underrepresentation, and toxicity**: We have described
    how LLMs are trained at an enormous scale and often on uncurated datasets. Inevitably,
    LLMs will learn, exhibit, and amplify societal biases. The model will propagate
    stereotypes, reflect biased assumptions, and generate toxic and harmful content.
    Moreover, LLMs can overrepresent certain populations and grossly underrepresent
    others, leading to a skewed or warped sociological perspective. These notions
    of bias can manifest in many ways. We will explore this topic, and other ethical
    implications of LLM use, in detail in [*Chapter 8*](B21773_08.xhtml#_idTextAnchor251).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**承认偏见、代表性不足和毒性**：我们已经描述了LLMs是如何在巨大规模上以及通常在未经筛选的数据集上训练的。不可避免的是，LLMs将学习、展示并放大社会偏见。模型将传播刻板印象，反映有偏见的假设，并生成有毒和有害的内容。此外，LLMs可能会过度代表某些群体，而严重低估其他群体，导致社会视角扭曲或变形。这些偏见观念可以以多种方式表现出来。我们将详细探讨这一主题，以及其他LLM使用的伦理影响，在[*第8章*](B21773_08.xhtml#_idTextAnchor251)中。'
- en: '**Avoid ambiguity and lack of clarity**: Since LLMs were trained to synthesize
    information resembling human responses, they can often exhibit notions of creativity.
    In practice, if prompting is ambiguous or lacks clarity, the model will likely
    use its vast contextualized knowledge to “assume” or “infer” the meaning or objective
    of a given prompt or instruction. It may apply some context from its training
    instead of responding with a clarifying question. As we will describe in the next
    section, it is crucial to provide clarity by contextualizing input in most cases.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免歧义和缺乏清晰度**：由于大型语言模型（LLMs）被训练生成类似于人类响应的信息，它们往往表现出创造性。在实践中，如果提示不明确或缺乏清晰度，模型可能会利用其庞大的上下文知识来“假设”或“推断”给定提示或指令的意义或目标。它可能会应用其训练中的某些上下文信息，而不是以澄清问题的形式进行回应。正如我们将在下一节中描述的，在大多数情况下，通过上下文化输入来提供清晰度至关重要。'
- en: Now that we have established a few overarching principles to help navigate interactions
    and keep us within the boundaries of appropriate use, we can deconstruct the various
    elements of a prompt.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确立了一些指导原则，以帮助我们在互动中保持适当的使用范围，我们可以分解提示的各个要素。
- en: Prompt elements and structure
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示元素和结构
- en: Generally, a prompt acts as a guide, directing the model’s response toward the
    desired outcome. It typically comprises key elements that frame the task at hand,
    providing clarity and direction for the model’s generative capabilities. The following
    table presents the essential elements of a zero-shot prompt.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，提示充当指南，引导模型的响应趋向于期望的结果。它通常包含关键元素，这些元素构成了手头的任务，为模型的生成能力提供清晰和方向。以下表格展示了零样本提示的基本要素。
- en: '| **Instruction** | A clear, concise statement describing what you want the
    model to do. This could be a direct command, a question, or a statement that implies
    a task. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **指令** | 一个清晰、简洁的声明，描述你希望模型执行的操作。这可能是一个直接命令、一个问题，或者暗示一个任务的陈述。|'
- en: '| **Context** | Relevant information or background is needed to understand
    the instruction or the task. This could include definitions or clarifications.
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **上下文** | 需要的相关信息或背景知识，以理解指令或任务。这可能包括定义或说明。|'
- en: '| **Input** | Following the instructions, the model should work with specific
    data or content. This could be a piece of text, a question, or any information
    relevant to the task. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | 遵循指令，模型应使用特定的数据或内容。这可能是一段文本、一个问题或与任务相关的任何信息。|'
- en: '| **Output cue** | An indication of how the model’s response is to be structured.
    This can be part of the instruction or implied through the prompt’s formatting.
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **输出提示** | 指示模型响应的结构方式。这可以是指令的一部分，或者通过提示的格式暗示。|'
- en: 'Table 7.1: Basic elements of a zero-shot prompt'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1：零样本提示的基本元素
- en: 'We can then structure these elements to maximize the zero-shot approach, whereby
    the model relies entirely on the prompt to understand and execute a task. In this
    context, we use the term *task* to describe a specific natural language task,
    such as summarization or translation. However, we will also encounter the term
    *task* applied more broadly to refer to the output the model should provide. Let’s
    explore a few concrete examples of various tasks. In this case, we will be referring
    to specific NLP tasks and applying a standard structure combining the key elements
    we’ve described:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对这些元素进行结构化，以最大化零样本方法，其中模型完全依赖于提示来理解和执行任务。在这种情况下，我们使用术语*任务*来描述一个特定的自然语言任务，例如摘要或翻译。然而，我们也会遇到更广泛地使用*任务*这一术语，指的是模型应该提供的输出。让我们探讨一些各种任务的几个具体例子。在这种情况下，我们将参考特定的NLP任务，并应用一个标准结构，结合我们描述的关键元素：
- en: '`Renewable energy sources like solar and wind power offer sustainable alternatives
    to fossil fuels, reducing greenhouse gas emissions and promoting` `environmental
    conservation...`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`太阳能和风能等可再生能源为化石燃料提供了可持续的替代品，减少了温室气体排放，并促进了` `环境保护` `...`'
- en: '`"Renewable energy sources, such as solar and wind, play a crucial role in
    reducing emissions and conserving` `the environment."`'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`"可再生能源，如太阳能和风能，在减少排放和节约` `环境` `方面发挥着至关重要的作用。"`'
- en: '`"Hello, how` `are you?"`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"你好，你` `好吗？"`'
- en: '`This translates to "Hola, ¿``cómo estás?"`'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`这翻译成“Hola, ¿cómo estás?”`'
- en: The structured templates help us to efficiently and reliably prompt the model
    for a wide range of inputs, while maintaining a structure that the model has learned
    to recognize and respond to. In fact, we can take this a step further by asking
    the model to provide a specific format in its output. Using the output cue, we
    can instruct the model to provide a specified format such as Markdown.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结构化模板帮助我们高效且可靠地针对广泛的各种输入提示模型，同时保持模型已学会识别和响应的结构。实际上，我们可以更进一步，要求模型在其输出中提供特定的格式。使用输出提示，我们可以指示模型提供指定的格式，例如Markdown。
- en: '`"Please write a Python function to calculate the square of` `a number."`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"请编写一个Python函数来计算一个数字的平方。"`'
- en: '**Output Cue**: By using the Markdown format in the output cue, the model knows
    to provide this format and returns the following:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**输出提示**：通过在输出提示中使用Markdown格式，模型知道提供这种格式，并返回以下内容：'
- en: '[PRE0]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Using LangChain to produce JSON-formatted output, we can leverage the same
    approach. Specifically, LangChain’s `PromptTemplate` provides a flexible way to
    dynamically define a structure for our prompts and insert elements:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用LangChain生成JSON格式的输出，我们可以利用相同的方法。具体来说，LangChain的`PromptTemplate`提供了一个灵活的方式来动态定义我们提示的结构，并插入元素：
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This produces the following:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会产生以下内容：
- en: '[PRE30]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Crafting effective prompts for zero-shot learning with LLMs requires a clear
    understanding of the task, thoughtful structuring of the prompt, and consideration
    of how the model interprets and responds to different elements within the prompt.
    By applying these principles, we can guide models to perform various tasks accurately
    and effectively. Subsequently, we will explore methods to guide models’ behavior
    through positive affirmations, emotional engagement, and other cognitive-behavioral
    techniques.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLMs）进行零样本学习时，制定有效的提示需要清楚地理解任务，仔细构建提示结构，并考虑模型如何解释和响应提示中的不同元素。通过应用这些原则，我们可以引导模型准确有效地执行各种任务。随后，我们将探讨通过积极的肯定、情感参与和其他认知行为技术来引导模型行为的方法。
- en: Elevating prompts – iteration and influencing model behaviors
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升提示 – 迭代和影响模型行为
- en: In this section, we will introduce techniques for enhancing AI model interactions
    inspired by cognitive-behavioral research. Behavioral prompting can guide models
    toward more accurate and nuanced responses. For example, LLM performance can be
    improved by providing the model with positive emotional stimuli, asking the model
    to assume a persona or character, or using situational prompting (i.e., role-play).
    However, it is crucial to recognize that these techniques can also be misused
    or used to inadvertently introduce stereotypes, as they rely on assumptions and
    generalizations that may not accurately reflect individual experiences or diverse
    perspectives. Without careful consideration and monitoring, there is a risk of
    reinforcing existing biases or creating new ones, potentially leading to skewed
    or harmful output. Given these challenges, we will explore a responsible approach
    to employing cognitive-behavioral techniques in AI interactions, aiming to harness
    their benefits while minimizing risks and ensuring inclusivity and fairness.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍受认知行为研究启发的增强人工智能模型交互的技术。行为提示可以引导模型做出更准确和细腻的反应。例如，通过向模型提供积极的情感刺激、要求模型扮演一个角色或人物，或使用情境提示（即角色扮演），可以提高
    LLM 的性能。然而，认识到这些技术也可能被滥用或无意中引入刻板印象是至关重要的，因为它们依赖于可能不准确反映个人经验或多元视角的假设和概括。如果没有仔细考虑和监控，存在加强现有偏见或创造新偏见的风险，可能导致输出结果偏颇或有害。鉴于这些挑战，我们将探讨在人工智能交互中负责任地应用认知行为技术的途径，旨在利用其益处，同时最大限度地减少风险，确保包容性和公平性。
- en: LLMs respond to emotional cues
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs 对情感线索做出反应
- en: Research conducted by Microsoft in collaboration with various institutions,
    including the Beijing Normal University psychology department, suggests that LLMs
    can mimic and display some aspects of human emotional intelligence. This can lead
    to improved task performance when prompts are infused with emotional stimuli.
    In particular, the researchers hypothesize that emphasizing positive words can
    trigger more constructive and effective responses. The phenomenon is not well
    understood, but the effect is that positive emotional cues seem to improve model
    performance on various tasks consistently (Li et al., 2023). For example, the
    researchers input phrases encouraging confidence and positive outcomes, such as
    `This is very important to my career` or `Believe in your abilities and strive
    for excellence. Your hard work will yield remarkable results`. These types of
    psychological elements not only positively influenced the output quality but also
    made interactions more effective and nuanced.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 微软与包括北京师范大学心理学系在内的多个机构合作进行的研究表明，LLMs 可以模仿并表现出一些人类情感智能的方面。当提示中融入情感刺激时，这可以提高任务性能。特别是，研究人员假设强调积极词汇可以引发更建设性和有效的反应。这一现象尚未得到充分理解，但其效果是积极的情感线索似乎可以一致地提高模型在各种任务上的性能（Li
    等人，2023）。例如，研究人员输入了鼓励自信和积极结果的短语，如“这对我的职业生涯非常重要”或“相信你的能力，追求卓越。你的辛勤工作将产生显著的结果”。这类心理元素不仅对输出质量产生了积极影响，还使互动更加有效和细腻。
- en: Effect of personas
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 角色扮演的影响
- en: Research has shown that the effect of **personas** enables LLMs to better simulate
    human-like interactions by adopting varied characteristics, perspectives, and
    response styles tailored to specific user profiles. This approach enhances the
    model’s capability for engaging in more personalized, contextually relevant conversations.
    Incorporating personas into LLM prompts effectively shifts the distribution of
    possible outputs, directing the model’s generation process toward responses that
    resonate with specific character traits or perspectives. Thanks to the model’s
    extensive training on diverse datasets, it can replicate patterns aligning with
    the described characteristics of personas, focusing its attention and selection
    process on a more defined set of outputs. This enhances relevance and specificity.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，**角色**的效果使得大型语言模型能够通过采用针对特定用户配置文件定制的多样化特征、观点和响应风格来更好地模拟类似人类的交互。这种方法增强了模型进行更个性化、情境相关对话的能力。将角色纳入
    LLM 提示中有效地改变了可能输出的分布，将模型的生成过程引导到与特定性格特征或观点产生共鸣的响应。得益于模型在多样化数据集上的广泛训练，它可以复制与描述的角色特征相一致的模式，将注意力和选择过程集中在更明确的输出集上。这增强了相关性和特异性。
- en: 'For clarity, let’s reconsider our hypothetical business StyleSprint, a fashion
    brand. If we prompt an LLM with “`As a creative and bold fashion brand, how would
    StyleSprint describe its upcoming summer collection?`”, the model, adopting the
    brand’s persona, might generate a response such as the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，让我们重新考虑我们的假设商业案例 StyleSprint，一个时尚品牌。如果我们用一个 LLM 提示“`作为一个充满创意和大胆的时尚品牌，StyleSprint
    将如何描述其即将到来的夏季系列？`”，模型在采用品牌角色的情况下可能会生成以下类似的响应：
- en: “`This summer, StyleSprint dares you to stand out! Our collection is a vibrant
    explosion of colors and patterns, designed for the fearless fashionista. Think
    bold prints, daring cuts, and a statement that screams confidence. From sun-drenched
    beaches to chic city streets, our pieces promise to make every moment unforgettable.
    Embrace your boldness with StyleSprint` `this summer.`”.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: “`这个夏天，StyleSprint 挑战你脱颖而出！我们的系列设计充满活力，色彩斑斓，图案丰富，专为无畏的时尚达人打造。想象一下大胆的图案、大胆的剪裁，以及彰显自信的宣言。从阳光明媚的海滩到时尚的城市街道，我们的单品承诺让每一刻都难以忘怀。这个夏天，与
    StyleSprint 一起拥抱你的大胆吧。`”。
- en: In this example, we can see how personas guide the model’s outputs to reflect
    specific brand tones, such as creativity and boldness in this case, making the
    marketing material more engaging and aligned with the brand’s identity. Furthermore,
    using a few-shot approach, we could imbue model responses with a very specific
    branded tone. We will discuss few-shot in the sections that follow.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到角色如何引导模型的输出以反映特定的品牌调性，例如在本例中的创造力和大胆。这种方法使营销材料更具吸引力，并与品牌形象保持一致。此外，通过使用少量样本的方法，我们可以赋予模型响应以非常具体的品牌调性。我们将在接下来的章节中讨论少量样本。
- en: However, as discussed, personas should be used with caution. Personas can perpetuate
    stereotypes and biases, particularly against marginalized groups. A study conducted
    by researchers at Stanford University found that generating personas based on
    intersectional demographic groups often yields higher rates of racial stereotypes
    and patterns of **othering**, or portraying someone or a group as fundamentally
    different or alien, compared to human-written texts. In some cases, model outputs
    could amplify narratives and tropes (Cheng, Durmus, & Jurafsky, 2023).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如所讨论的，角色应谨慎使用。角色可能会延续刻板印象和偏见，尤其是针对边缘化群体。斯坦福大学的研究人员进行的一项研究发现，基于交叉人口统计群体的角色生成往往会产生比人类撰写的文本更高的种族刻板印象和其他化模式（或描绘某人或某个群体为本质上不同或外来的模式）。在某些情况下，模型输出可能会放大叙事和陈词滥调（Cheng,
    Durmus, & Jurafsky, 2023）。
- en: Situational prompting or role-play
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情境提示或角色扮演
- en: Role-play in LLMs, in the same way as personas, involves adopting specific identities
    or characteristics. However, the two serve different purposes and are applied
    in distinct contexts. Personas are predefined sets of traits or characteristics
    that an LLM mimics to tailor its responses, focusing on consistency with those
    traits. As we have demonstrated with our StyleSprint example, this is useful for
    creating content with a specific tone or perspective.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 中进行角色扮演，与角色类似，涉及采用特定的身份或特征。然而，这两个概念服务于不同的目的，并应用于不同的情境。角色是一组预定义的特征或特性，LLM
    通过模仿这些特征来定制其响应，专注于与这些特征的连贯性。正如我们在 StyleSprint 例子中所展示的，这对于创建具有特定语气或观点的内容是有用的。
- en: Conversely, role-play extends beyond adopting a set of traits to engage in a
    scenario or narrative dynamically. It involves the LLM taking on a character within
    a simulated environment or story, responding to inputs in a manner that aligns
    with both a persona and the evolving context of the role-play scenario. This can
    be especially useful in complex simulations where the LLM must navigate and contribute
    to ongoing narratives or dialogues that require understanding and adapting to
    new information or changing circumstances in real time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，角色扮演不仅涉及采取一系列特征来动态参与一个场景或叙事，它还涉及 LLM 在模拟环境或故事中扮演一个角色，以与角色和角色扮演场景的演变背景相一致的方式响应输入。这在复杂的模拟中特别有用，其中
    LLM 必须实时导航并参与需要理解和适应新信息或变化情况的持续叙事或对话。
- en: "![Figure 7.3\uFEFF: Persona versus role-play](img/B21773_07_03.jpg)"
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3：角色扮演与角色对比](img/B21773_07_03.jpg)'
- en: 'Figure 7.3: Persona versus role-play'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：角色扮演与角色对比
- en: Revisiting our real-world scenario, role-play could be particularly useful for
    creating interactive and engaging customer service experiences. For example, StyleSprint
    could design a role-play scenario where the LLM acts as a virtual personal stylist.
    In this role, the model would engage customers with prompts such as `I'm your
    personal stylist for today! What's the occasion you're dressing for?`. Based on
    the customer’s response, the LLM could ask follow-up questions to narrow down
    preferences, such as `Do you prefer bold colors or pastel shades?`. Finally, it
    could recommend outfits from StyleSprint’s collection that match the customer’s
    needs, saying something such as `For a summer wedding, I recommend our Floral
    Maxi Dress paired with the Vintage Sun Hat. It's elegant, yet perfect for an`
    `outdoor setting!`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的现实场景，角色扮演在创造互动和吸引人的客户服务体验方面可能特别有用。例如，StyleSprint 可以设计一个角色扮演场景，其中 LLM 扮演虚拟个人造型师。在这个角色中，模型会通过诸如“我是你今天的个人造型师！你为哪个场合打扮？”这样的提示与客户互动。根据客户的回答，LLM
    可以提出后续问题来缩小偏好范围，例如“你更喜欢鲜艳的颜色还是柔和的色调？”。最后，它可以推荐符合客户需求的 StyleSprint 收藏中的服装，比如说“对于夏日婚礼，我推荐我们的花卉长裙搭配复古太阳帽。它既优雅，又非常适合户外环境！”
- en: In this case, we leverage the LLM’s ability to dynamically adapt its dialogue
    based on customer inputs to create an advanced recommender system that facilitates
    a highly personalized shopping experience. It not only helps in providing tailored
    fashion advice but also engages customers in a novel way.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们利用 LLM 根据客户输入动态调整对话的能力，创建一个高级推荐系统，该系统有助于实现高度个性化的购物体验。它不仅有助于提供定制的时尚建议，而且以新颖的方式吸引客户。
- en: Having examined how behavior-inspired techniques, such as personas and role-play,
    influence model behavior through zero-shot learning, let’s now turn our attention
    to few-shot learning. This is also known as in-context learning, which we described
    in [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180). Recall that the few-shot approach
    can enhance the consistency, stability, and reliability of model responses. By
    providing the model with a few examples of the desired output within the prompt
    itself, few-shot learning effectively teaches the model the specific task at hand,
    leading to more predictable and accurate outputs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究了行为启发技术，如角色和角色扮演，如何通过零样本学习影响模型行为之后，我们现在将注意力转向少样本学习。这同样也被称为情境学习，我们在[*第 5 章*](B21773_05.xhtml#_idTextAnchor180)中进行了描述。回想一下，少样本方法可以增强模型响应的一致性、稳定性和可靠性。通过在提示中本身提供一些期望输出的示例，少样本学习有效地教会模型手头的特定任务，从而产生更可预测和准确的输出。
- en: Advanced prompting in action – few-shot learning and prompt chaining
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级提示的实际应用 - 少样本学习和提示链
- en: In few-shot settings, the LLM is presented with a small number of examples of
    a task within the input prompt, guiding the model to generate responses that align
    with these examples. As discussed in the prior chapter, this method significantly
    reduces the need for fine-tuning on large, task-specific datasets. Instead, it
    leverages the model’s pre-existing knowledge and ability to infer context from
    the examples provided. In [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180), we
    saw how this approach was particularly useful for StyleSprint by enabling the
    model to answer specific questions after being provided with just a few examples,
    enhancing consistency and creativity in brand messaging.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在少量示例设置中，LLM在输入提示中展示了少量任务示例，引导模型生成与这些示例一致的响应。正如前一章所讨论的，这种方法显著减少了在大规模、特定任务数据集上进行微调的需求。相反，它利用了模型预先存在的知识和从提供的示例中推断上下文的能力。在第[*5章*](B21773_05.xhtml#_idTextAnchor180)中，我们看到了这种方法如何特别有助于StyleSprint，通过使模型在仅提供几个示例后回答特定问题，增强了品牌信息的连贯性和创造性。
- en: This method typically involves using between 10 and 100 examples, depending
    on the model’s context window. Recall that the context window is the limit of
    tokens a language model can process in one turn. The primary benefit of the few-shot
    approach is that it minimizes the risk of the model learning a too-narrow distribution
    from a specific dataset through fine-tuning. Although the performance of few-shot
    may not always match its fine-tuned counterpart, few-shot learning often outperforms
    both one-shot and zero-shot learning, showing significant improvements in task
    adaptation and accuracy. This is especially true as more examples are added to
    the context window (Brown et al., 2020).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通常涉及使用10到100个示例，具体取决于模型的上下文窗口。请记住，上下文窗口是语言模型在一次操作中可以处理的标记数的限制。少量示例方法的主要优势是它最小化了模型通过微调从特定数据集中学习到一个过于狭窄分布的风险。尽管少量示例的性能可能并不总是与经过微调的模型相匹配，但少量示例学习通常优于单次和零次学习，显示出在任务适应性和准确性方面的显著改进。这一点在向上下文窗口添加更多示例时尤其正确（Brown
    et al., 2020）。
- en: 'Applications such as LangChain provide a simple and convenient pattern for
    few-shot implementation. Consider a scenario in which StyleSprint would like to
    generate taglines for its seasonal collections. In this case, we can provide the
    model with examples written by the content team to guide the model toward consistency
    with the brand tone:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain等应用提供了少量示例实现的简单方便的模式。考虑一个场景，StyleSprint希望为其季节性系列生成标语。在这种情况下，我们可以向模型提供内容团队编写的示例，以引导模型与品牌语气保持一致：
- en: '[PRE33]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The LangChain API offers `FewShotPromptTemplate` to format the examples consistently:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain API提供了`FewShotPromptTemplate`来格式化示例：
- en: '[PRE34]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can now apply the template to an LLM to generate a response that we can
    expect will closely align with the tone and style of our examples:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将模板应用于一个大型语言模型（LLM），生成一个我们预期将与我们示例的语气和风格紧密一致的响应：
- en: '[PRE35]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now that we have a consistent and programmatic method for providing the model
    with examples, we can iterate over the model responses using prompt chaining.
    A prompt chain generally refers to chaining together multiple prompts and LLM
    interactions to have a conversation with the model and iteratively build on the
    results. Remember, the model itself cannot store information and effectively has
    no memory or prior inputs and outputs. Instead, the application layer stores prior
    inputs and outputs, which are then provided to the model with each exchange. For
    example, you might start with an initial prompt such as the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了为模型提供示例的一致和程序化方法，我们可以通过提示链来迭代模型响应。提示链通常指的是将多个提示和LLM交互串联起来，与模型进行对话并迭代构建结果。记住，模型本身无法存储信息，实际上几乎没有记忆或先前的输入和输出。相反，应用层存储先前的输入和输出，这些输入和输出随后在每个交互中提供给模型。例如，您可能从一个初始提示开始，如下所示：
- en: '[PRE36]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The LLM might generate the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可能会生成以下内容：
- en: '[PRE37]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You could then construct a follow-up prompt using the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下内容构建后续提示：
- en: '[PRE38]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You could then keep iterating to improve the output.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以继续迭代以改进输出。
- en: 'Chaining facilitates guiding and interactively refining the text generated
    rather than relying purely on the given examples. Notice that our prior few-shot
    code had already established a chain, which we can now use to iterate as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 链接化有助于引导和交互式地完善生成的文本，而不是仅仅依赖于给定的示例。请注意，我们之前的少量示例代码已经建立了一个链，我们现在可以使用它来迭代如下：
- en: '[PRE39]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The model is now working from both the examples we provided and any additional
    instructions we want to include as part of the chain. Prompt chaining, combined
    with few-shot learning, provides a powerful framework for iteratively guiding
    language model outputs. By leveraging application state to maintain conversation
    context, we can steer the model toward desired responses in line with our provided
    examples. This approach balances harnessing the model’s inferential capabilities
    and retaining control to align its creative outputs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在正在从我们所提供的示例和我们想要作为链的一部分包含的任何附加指令中工作。提示链与少量示例学习相结合，提供了一个强大的框架，用于迭代引导语言模型输出。通过利用应用程序状态来维护对话上下文，我们可以引导模型朝着与提供的示例一致的期望响应。这种方法在利用模型的推理能力的同时，保持了对其创造性输出的控制。
- en: Next, we will dive into our practice project, which implements RAG. RAG augments
    model responses by retrieving and incorporating external data sources. This technique
    mitigates hallucination risks by grounding AI-generated text in real data. For
    example, StyleSprint may leverage past customer survey results or catalog data
    to enhance product descriptions. By combining retrieval with prompt chaining,
    RAG provides a scalable method for balancing creativity with accuracy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入我们的实践项目，该项目实现了 RAG。RAG 通过检索和整合外部数据源来增强模型响应。这种技术通过将 AI 生成的文本定位在真实数据中来减轻幻觉风险。例如，StyleSprint
    可以利用过去的客户调查结果或目录数据来增强产品描述。通过结合检索和提示链，RAG 提供了一种可扩展的方法，以平衡创造力和准确性。
- en: 'Practice project: Implementing RAG with LlamaIndex using Python'
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践项目：使用 Python 实现 RAG 与 LlamaIndex
- en: For our practice project, we will shift from LangChain to exploring another
    library that facilitates the RAG approach. LlamaIndex is an open source library
    that is specifically designed for RAG-based applications. LlamaIndex simplifies
    ingestion and indexing across various data sources. However, before we dive into
    implementation, we will explain the underlying methods and approach behind RAG.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实践项目中，我们将从 LangChain 转向探索另一个促进 RAG 方法实现的库。LlamaIndex 是一个开源库，专门为基于 RAG 的应用程序设计。LlamaIndex
    简化了跨各种数据源的摄取和索引。然而，在我们深入实施之前，我们将解释 RAG 背后的方法和途径。
- en: As discussed, the key premise of RAG is to enhance LLM outputs by supplying
    relevant context from external data sources. These sources should provide specific
    and verified information to ground model outputs. Moreover, RAG can optionally
    leverage the few-shot approach by retrieving few-shot examples at inference time
    to guide generation. This approach alleviates the need to store examples in the
    prompt chain and only retrieves relevant examples when needed. In essence, the
    RAG approach is a culmination of many of the prompt engineering techniques we
    have already discussed. It provides structure, chaining, few-shot learning, and
    grounding.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，RAG 的关键前提是通过提供来自外部数据源的相关上下文来增强 LLM 的输出。这些来源应提供具体且经过验证的信息，以使模型输出有据可依。此外，RAG
    可以选择性地利用少量示例方法，在推理时检索少量示例以指导生成。这种方法减轻了在提示链中存储示例的需要，并且仅在需要时检索相关示例。本质上，RAG 方法是我们已经讨论过的许多提示工程技术的综合。它提供了结构、链式、少量示例学习和定位。
- en: 'At a high level, the RAG pipeline can be described as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，RAG 管道可以描述如下：
- en: The RAG component ingests and indexes domain-specific data sources using vector
    embeddings to encode semantics. As we learned in [*Chapter 3*](B21773_03.xhtml#_idTextAnchor081),
    these embeddings are imbued with deeply contextualized, rich semantic information
    that the component uses later to perform a semantic search.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG 组件使用向量嵌入来编码语义，摄取和索引特定领域的数据源。正如我们在 [*第 3 章*](B21773_03.xhtml#_idTextAnchor081)
    中所学，这些嵌入包含着深入上下文、丰富的语义信息，该组件随后使用这些信息执行语义搜索。
- en: The component then uses the initial prompt as a search query. The query is input
    to retrieval systems, which find the most relevant snippets from the indexed data
    based on vector similarity. Similar to how we applied semantic similarity in prior
    chapters, RAG leverages a similarity metric to rank results by semantic relevance.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组件随后使用初始提示作为搜索查询。查询被输入到检索系统中，该系统根据向量相似性从索引数据中找到最相关的片段。类似于我们在先前章节中应用语义相似性的方式，RAG
    利用相似性度量来按语义相关性对结果进行排序。
- en: Lastly, the original prompt is augmented with information from the retrieved
    contexts, and the augmented prompt is passed to the LLM to generate a response
    grounded in the external data.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，原始提示通过检索到的上下文信息得到增强，增强后的提示被传递给LLM以生成基于外部数据的响应。
- en: RAG introduces two major benefits. First, like the chaining approach, the indexed
    external data acts as a form of memory, overcoming the LLM’s statelessness. Second,
    this memory can rapidly scale beyond model context window limitations, since examples
    are curated and only provided at the time of the request as needed. Ultimately,
    RAG unlocks otherwise unattainable capabilities in reliable and factual text generation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RAG引入了两大优势。首先，类似于链式方法，索引的外部数据充当一种记忆形式，克服了LLM的无状态性。其次，这种记忆可以快速超越模型上下文窗口的限制，因为示例是在请求时根据需要精心挑选并提供的。最终，RAG在可靠和事实性的文本生成中解锁了其他方法无法达到的能力。
- en: 'In our practice project, we revisited the StyleSprint product descriptions.
    This time, we want to leverage RAG to retrieve detailed information about the
    product to produce very specific descriptions. For the purpose of keeping this
    project accessible, we will implement an in-memory vector store (Faiss) instead
    of an external database. We begin with installing the necessary libraries. We
    will leverage LlamaIndex’s integrated support for Faiss:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实践项目中，我们重新审视了StyleSprint产品的描述。这次，我们希望利用RAG检索有关产品的详细信息以生成非常具体的描述。为了使这个项目易于访问，我们将实现一个内存中的向量存储（Faiss）而不是外部数据库。我们首先安装必要的库。我们将利用LlamaIndex对Faiss的集成支持：
- en: '[PRE40]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We will then import the necessary libraries, load the data, and create the
    index. This vector store will rely on OpenAI’s embeddings, so we must also define
    `OPENAI_API_KEY` using a valid key:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将导入必要的库，加载数据，并创建索引。这个向量存储将依赖于OpenAI的嵌入，因此我们必须使用有效的密钥定义`OPENAI_API_KEY`：
- en: '[PRE41]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We now have a vector store that the model can rely on to retrieve our very
    specific product data. This means we can query for very specific responses augmented
    by our data:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个向量存储，模型可以依赖它来检索我们非常具体的产品数据。这意味着我们可以查询非常具体的、由我们的数据增强的响应：
- en: '[PRE42]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The result is a response that not only provides an accurate description of the
    summer dress but also includes specific details, such as the price. This level
    of detail enriches the customer’s shopping experience, providing relevant and
    real-time information for customers to consider when making a purchase.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，不仅提供了夏季连衣裙的准确描述，还包括了价格等具体细节。这种详细程度丰富了客户的购物体验，为顾客在购买时考虑提供了相关和实时信息。
- en: 'The next step is to evaluate our RAG implementation to ensure that the answer
    is relevant, faithful to the source text, reflective of contextual accuracy, and
    not in any way harmful or inappropriate. We can apply an open source evaluation
    framework (RAGAS), which provides implementation of the following metrics:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是评估我们的RAG实现，以确保答案相关、忠实于源文本、反映上下文准确性，并且不会以任何方式有害或不适当。我们可以应用开源评估框架（RAGAS），该框架提供了以下指标的实现：
- en: '**Faithfulness** assesses the degree to which the generated response is faithful
    or true to the original context'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忠实度**评估生成的响应忠实或真实于原始上下文的程度'
- en: '**Answer relevance** evaluates how relevant the generated answer is to the
    given question'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案相关性**评估生成的答案与给定问题的相关性'
- en: '**Context precision** measures the precision of the context used to generate
    the answer'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文精确度**衡量用于生成答案的上下文的精确度'
- en: '**Context recall** measures the recall of the context used to generate the
    answer'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文回忆**衡量用于生成答案的上下文的回忆程度'
- en: '**Context relevancy** assesses the relevancy of the context used to generate
    the answer'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文相关性**评估用于生成答案的上下文的相关性'
- en: '**Harmfulness** evaluates whether a submission (or answer) contains anything
    that could potentially cause harm to individuals, groups, or society at large'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有害性**评估提交（或答案）中是否包含可能对个人、群体或整个社会造成伤害的内容'
- en: This suite of metrics provides an objective measure of RAG application performance
    based on a comparison to ground truth. In our case, we can use responses generated
    from our product data, along with context and ground truth derived from the original
    dataset, to construct an evaluation dataset and perform a comprehensive evaluation
    using the metrics described.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这套指标提供了一种基于与真实数据比较的客观度量RAG应用性能的方法。在我们的案例中，我们可以使用从我们的产品数据生成的响应，以及从原始数据集中提取的上下文和真实数据，来构建一个评估数据集，并使用所描述的指标进行全面的评估。
- en: The following is a simplified code snippet implementing the RAGAS evaluation
    for our generated product descriptions. A complete working implementation is available
    in the `Chapter 7` folder of the GitHub companion to this book ([https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个实现我们生成产品描述的RAGAS评估的简化代码片段。完整的可工作实现可在本书的GitHub配套文件夹的*第7章*中找到（[https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python))。
- en: '[PRE43]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Our evaluation program should produce the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估计划应产生以下结果：
- en: '[PRE44]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We can observe that the system performs well in generating accurate and relevant
    answers, as evidenced by high faithfulness and answer relevancy scores. While
    context precision shows room for improvement, half of the relevant information
    is correctly identified. Context recall is effective, retrieving most of the relevant
    context. The absence of harmful content ensures safe interactions. Overall, the
    system displays robust performance in answering accurately and contextually, but
    could benefit from refinements in pinpointing the most pertinent context snippets.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，系统在生成准确和相关的答案方面表现良好，这从高忠实度和答案相关性得分中可以看出。虽然上下文精确度还有改进的空间，但一半的相关信息被正确识别。上下文召回率有效，检索了大部分相关上下文。有害内容的缺失确保了安全交互。总体而言，系统在准确和上下文中回答方面表现出稳健的性能，但可以从改进最相关上下文片段的定位中受益。
- en: As discussed in *Chapters 5* and *6*, the evaluation of LLMs often requires
    the additional operational burden of collecting ground-truth data. However, doing
    so makes it possible to perform a robust evaluation of model and application performance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在*第5章*和*第6章*中讨论的那样，对LLMs的评估通常需要额外的操作负担，即收集真实数据。然而，这样做使得对模型和应用性能进行稳健评估成为可能。
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the intricacies of prompt engineering. We also
    explored advanced strategies to elicit precise and consistent responses from LLMs,
    offering a versatile alternative to fine-tuning. We traced the evolution of instruction-based
    models, highlighting how they’ve shifted the paradigm toward an intuitive understanding
    and adaptation to tasks through simple prompts. We expanded on the adaptability
    of LLMs with techniques such as few-shot learning and retrieval augmentation,
    which allow for dynamic model guidance across diverse tasks with minimal explicit
    training. The chapter further explored the structuring of effective prompts, and
    the use of personas and situational prompting to tailor model responses more closely
    to specific interaction contexts, enhancing the model’s applicability and interaction
    quality. We also addressed the nuanced aspects of prompt engineering, including
    the influence of emotional cues on model performance and the implementation of
    RLHF to refine model outputs. These discussions underscored the potential of LLMs
    to exhibit some level of emotional intelligence, leading to more effective and
    nuanced interactions. However, alongside these technological strides, we stressed
    the paramount importance of ethical considerations. We highlighted the need for
    responsible adoption and vigilance to mitigate potential harm and biases associated
    with these techniques, ensuring fairness, integrity, and the prevention of misuse.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了提示工程（prompt engineering）的复杂性。我们还探讨了从LLM中诱发出精确和一致响应的高级策略，提供了一种灵活的替代微调（fine-tuning）的方法。我们追溯了基于指令的模型（instruction-based
    models）的演变，强调了它们如何通过简单的提示将范式转向直观理解和适应任务。我们通过诸如少样本学习（few-shot learning）和检索增强（retrieval
    augmentation）等技术扩展了LLM的适应性，这些技术允许在最小显式训练的情况下，在多样化的任务中实现动态模型指导。本章进一步探讨了有效提示的结构化，以及使用角色扮演和情境提示来使模型响应更贴近特定的交互环境，从而提高模型的应用性和交互质量。我们还讨论了提示工程的细微之处，包括情感线索对模型性能的影响以及实施RLHF（Reinforcement
    Learning from Human Feedback）来优化模型输出。这些讨论强调了LLM展现出一定程度的情感智能的潜力，从而实现更有效和细致的交互。然而，随着这些技术进步，我们强调了道德考虑的首要重要性。我们强调了负责任地采用和警惕的必要性，以减轻这些技术可能带来的危害和偏见，确保公平性、完整性和防止滥用。
- en: Lastly, we learned how to implement and evaluate the RAG approach to ground
    the LLM in contextual information from trusted sources and produce answers that
    are relevant and faithful to the source text. In the next chapter, we will look
    more closely at the role of individuals in advancing generative AI while emphasizing
    the dual responsibility of developers and researchers to navigate this rapidly
    evolving field with a conscientious approach, balancing innovation with ethical
    imperatives and societal impacts.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何实现和评估RAG方法，以将LLM（大型语言模型）基于可信来源的上下文信息进行定位，并生成与源文本相关且忠实于源文本的答案。在下一章中，我们将更深入地探讨个人在推进生成式AI中的作用，同时强调开发者和研究人员在以负责任的方式导航这个快速发展的领域时，既要注重创新，也要平衡道德
    imperative和社会影响的双重责任。
- en: References
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本参考部分作为本书中引用的资源的存储库；您可以探索这些资源，以进一步加深对主题内容的理解和知识：
- en: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P.,
    Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton,
    F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J.,
    & Lowe, R. (2022). *Training language models to follow instructions with human
    feedback*. In arXiv [cs.CL]. [http://arxiv.org/abs/2203.02155](http://arxiv.org/abs/2203.02155)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P.,
    Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton,
    F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J.,
    & Lowe, R. (2022). *训练语言模型以遵循人类反馈的指令*. 在arXiv [cs.CL]. [http://arxiv.org/abs/2203.02155](http://arxiv.org/abs/2203.02155)
- en: Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai,
    A. M., & Le, Q. V. (2021). *Finetuned language models are zero-shot learners*.
    In arXiv [cs.CL]. [http://arxiv.org/abs/2109.01652](http://arxiv.org/abs/2109.01652)
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai,
    A. M., & Le, Q. V. (2021). *微调语言模型是零样本学习者*. 在arXiv [cs.CL]. [http://arxiv.org/abs/2109.01652](http://arxiv.org/abs/2109.01652)
- en: 'Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., & Kiela, D. (2020).
    *Adversarial NLI: A new benchmark for natural language* *understanding*. Arxiv.org.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., & Kiela, D. (2020).
    *对抗性NLI：自然语言理解的新基准*。Arxiv.org.
- en: Li, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q.,
    & Xie, X. (2023). *Large Language Models understand and can be enhanced by emotional
    stimuli*. In arXiv [cs.CL]. [http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q.,
    & Xie, X. (2023). *大型语言模型能够理解和通过情感刺激得到增强*。在arXiv [cs.CL]。[http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)
- en: 'Cheng, M., Durmus, E., & Jurafsky, D. (2023). *Marked personas: Using natural
    language prompts to measure stereotypes in language models. Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics* (Volume
    1: Long Papers).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng, M., Durmus, E., & Jurafsky, D. (2023). *标记人格：利用自然语言提示来衡量语言模型中的刻板印象。第61届计算语言学协会年度会议论文集*（第1卷：长篇论文）。
- en: Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan,
    A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,
    G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., …
    Amodei, D. (2020). *Language Models are Few-Shot Learners*. In arXiv [cs.CL].
    [http://arxiv.org/abs/2005.14165](http://arxiv.org/abs/2005.14165)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan,
    A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,
    G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., …
    Amodei, D. (2020). *语言模型是少样本学习者*。在arXiv [cs.CL]。[http://arxiv.org/abs/2005.14165](http://arxiv.org/abs/2005.14165)
