<html><head></head><body>
  <div><h1 class="chapterNumber">4</h1>
    <h1 id="_idParaDest-101" class="chapterTitle">Multimodal Modular RAG for Drone Technology</h1>
    <p class="normal">We will take generative AI to the next level with modular RAG in this chapter. We will build a system that uses different components or modules to handle different types of data and tasks. For example, one module processes textual information using LLMs, as we have done until the last chapter, while another module manages image data, identifying and labeling objects within images. Imagine using this technology in drones, which have become crucial across various industries, offering enhanced capabilities for aerial photography, efficient agricultural monitoring, and effective search and rescue operations. They even use advanced computer vision technology and algorithms to analyze images and identify objects like pedestrians, cars, trucks, and more. We can then activate an LLM agent to retrieve, augment, and respond to a user’s question.</p>
    <p class="normal">In this chapter, we will build a multimodal modular RAG program to generate responses to queries about drone technology using text and image data from multiple sources. We will first define the main aspects of modular RAG, multimodal data, multisource retrieval, modular generation, and augmented output. We will then build a multimodal modular RAG-driven generative AI system in Python applied to drone technology with LlamaIndex, Deep Lake, and OpenAI.</p>
    <p class="normal">Our system will use two datasets: the first one containing textual information about drones that we built in the previous chapter and the second one containing drone images and labels from Activeloop. We will use Deep Lake to work with multimodal data, LlamaIndex for indexing and retrieval, and generative queries with OpenAI LLMs. We will add multimodal augmented outputs with text and images. Finally, we will build performance metrics for the text responses and introduce an image recognition metric with GPT-4o, OpenAI’s powerful <strong class="keyWord">Multimodal LLM</strong> (<strong class="keyWord">MMLLM</strong>). By the end of the chapter, you will know how to build a multimodal modular RAG workflow leveraging innovative multimodal and multisource functionalities.</p>
    <p class="normal">This chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">Multimodal modular RAG</li>
      <li class="bulletList">Multisource retrieval</li>
      <li class="bulletList">OpenAI LLM-guided multimodal multisource retrieval</li>
      <li class="bulletList">Deep Lake multimodal datasets</li>
      <li class="bulletList">Image metadata-based retrieval</li>
      <li class="bulletList">Augmented multimodal output</li>
    </ul>
    <p class="normal">Let’s begin by defining multimodal modular RAG.</p>
    <h1 id="_idParaDest-102" class="heading-1">What is multimodal modular RAG?</h1>
    <p class="normal">Multimodal data combines different forms of information, such as text, images, audio, and video, to enrich data analysis <a id="_idIndexMarker241"/>and interpretation. Meanwhile, a system is a modular RAG system when it utilizes distinct modules for handling different data types and tasks. Each module is specialized; for example, one module will focus on text and another on images, demonstrating a sophisticated integration capability that enhances response generation with retrieved multimodal data.</p>
    <p class="normal">The program in this chapter will also be multisource through the two datasets we will use. We will use the LLM dataset on the drone technology built in the previous chapter. We will also use the Deep Lake multimodal VisDrone dataset, which contains thousands of labeled images captured by drones.</p>
    <p class="normal">We have selected drones for our example since drones have become crucial across various industries, offering enhanced capabilities for aerial photography, efficient agricultural monitoring, and effective search and rescue operations. They also facilitate wildlife tracking, streamline commercial deliveries, and enable safer infrastructure inspections. Additionally, drones support environmental research, traffic management, and firefighting. They can enhance surveillance for law enforcement, revolutionizing multiple fields by improving accessibility, safety, and cost-efficiency.</p>
    <p class="normal"><em class="italic">Figure 4.1</em> contains the workflow we will implement in this chapter. It is based on the generative RAG ecosystem illustrated in <em class="italic">Figure 1.3</em> from <em class="italic">Chapter 1</em>, <em class="italic">Why Retrieval-Augmented Generation?</em>. We added embedding and indexing functionality in the previous chapters, but this chapter will focus on retrieval and generation. The system we will build blurs the lines between retrieval and generation since the generator is intensively used for<a id="_idIndexMarker242"/> retrieving (seamless scoring and ranking) as well as generating in the chapter’s notebook.</p>
    <figure class="mediaobject"><img src="img/B31169_04_01.png" alt="A diagram of a multimodal modular rag system  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.1: A multimodal modular RAG system</p>
    <p class="normal">This chapter aims to build an educational modular RAG question-answering system focused on drone technology. You can rely on the functionality implemented in the notebooks of the preceding chapters, such as Deep Lake for vectors in <em class="italic">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>, and indices with LlamaIndex in <em class="italic">Chapter 3</em>, <em class="italic">Building</em> <em class="italic">Index-based RAG with LlamaIndex, Deep Lake, and OpenAI</em>. If necessary, take your time to go back to the previous chapters and have a look.</p>
    <p class="normal">Let’s go through the multimodal, multisource, modular RAG ecosystem in this chapter, represented in <em class="italic">Figure 4.1</em>. We will use the titles and subsections in this chapter represented in italics. Also, each <a id="_idIndexMarker243"/>phase is preceded by its location in <em class="italic">Figure 4.1</em>.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">(D4)</strong> <em class="italic">Loading the LLM dataset</em> created in <em class="italic">Chapter 3</em>, which contains textual data on drones.</li>
      <li class="bulletList"><strong class="keyWord">(D4)</strong> <em class="italic">Initializing the LLM query engine</em> with a LlamaIndex vector store index using <code class="inlineCode">VectorStoreIndex</code> and setting the created index for the query engine, which overlaps with <strong class="keyWord">(G4)</strong> as both a retriever and a generator with the OpenAI GPT model.</li>
      <li class="bulletList"><strong class="keyWord">(G1)</strong> Defining the <em class="italic">user input for multimodal modular RAG</em> for both the LLM query engine (for the textual dataset) and the multimodal query engine (for the <code class="inlineCode">VisDrone</code> dataset).</li>
    </ul>
    <p class="normal-one">Once the textual dataset has been loaded, the query engine has been created, and the user input has been defined as a baseline query for the textual dataset and the multimodal dataset, the process continues by generating a response for the textual dataset created in <em class="italic">Chapter 2</em>.</p>
    <ul>
      <li class="bulletList">While<em class="italic"> querying the textual dataset</em>, <strong class="keyWord">(G1)</strong>, <strong class="keyWord">(G2)</strong>, and <strong class="keyWord">(G4)</strong> overlap in the same seamless LlamaIndex process that retrieves data and generates content. The response is saved as <code class="inlineCode">llm_response</code> for the duration of the session.</li>
    </ul>
    <p class="normal">Now, the multimodal <code class="inlineCode">VisDrone</code> dataset <a id="_idIndexMarker244"/>will be loaded into memory and queried:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">(D4)</strong> The multimodal process begins by <em class="italic">loading and visualizing the multimodal dataset</em>. The program then continues by <em class="italic">navigating the multimodal dataset structure</em>, <em class="italic">selecting an image</em>, and <em class="italic">adding bounding boxes</em>.</li>
    </ul>
    <p class="normal">The same process as for the textual dataset is then applied to the <code class="inlineCode">VisDrone</code> multimodal dataset:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">(D4)</strong> <em class="italic">Building a multimodal query engine</em> with LlamaIndex by creating a vector store index based on <code class="inlineCode">VisDrone</code> data using <code class="inlineCode">VectorStoreIndex</code> and setting the created index for the query engine, which overlaps with <strong class="keyWord">(G4)</strong> as both a retriever and a generator with OpenAI GPT.</li>
      <li class="bulletList"><strong class="keyWord">(G1)</strong> The user input for the multimodal search engine is the same as the <em class="italic">user input for multimodal modular RAG</em> since it is used for both the LLM query engine (for the textual dataset) and the multimodal query engine (for the <code class="inlineCode">VisDrone</code> dataset).</li>
    </ul>
    <p class="normal">The multimodal <code class="inlineCode">VisDrone</code> dataset will now be loaded and indexed, and the query engine is ready. The purpose of <strong class="keyWord">(G1)</strong> user input is for the LlamaIndex query engine to retrieve relevant documents from VisDrone using an LLM—in this case, an OpenAI model. Then, the retrieval functions will trace the response back to its source in the multimodal dataset to find the image of the source nodes. We are, in fact, using the query engine to reach an image through its textual response:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">(G1)</strong>, <strong class="keyWord">(G2)</strong>, and <strong class="keyWord">(G4)</strong> overlap in a seamless LlamaIndex query when running a query on the <code class="inlineCode">VisDrone</code> multimodal dataset.</li>
      <li class="bulletList">Processing the response <strong class="keyWord">(G4)</strong> to find the source node and retrieve its image leads us back to <strong class="keyWord">(D4)</strong> for image retrieval. This leads to selecting and processing the image of the source node.</li>
    </ul>
    <p class="normal">At this point, we now have the textual and the image response. We can then build a summary and apply an accuracy performance metric after having visualized the time elapsed for each phase as we built the program:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">(G4)</strong> We present a<a id="_idIndexMarker245"/> merged output with the LLM response and the augmented output with the image of the multimodal response in a <em class="italic">multimodal modular summary</em>.</li>
      <li class="bulletList"><strong class="keyWord">(E)</strong> Finally, we create an <em class="italic">LLM performance metric</em> and a <em class="italic">multimodal performance metric</em>. We then sum them up as a <em class="italic">multimodal modular RAG performance metric</em>.</li>
    </ul>
    <p class="normal">We can draw two conclusions from this multimodal modular RAG system:</p>
    <ul>
      <li class="bulletList">The system we are building in this chapter is one of the many ways RAG-driven generative AI can be designed in real-life projects. Each project will have its specific needs and architecture.</li>
      <li class="bulletList">The rapid evolution from generative AI to the complexity of RAG-driven generative AI requires the corresponding development of seamlessly integrated cross-platform components such as LlamaIndex, Deep Lake, and OpenAI in this chapter. These platforms are also integrated with many other frameworks, such as Pinecone and LangChain, which we will discuss in <em class="italic">Chapter 6</em>, <em class="italic">Scaling RAG Bank Customer Data with Pinecone</em>.</li>
    </ul>
    <p class="normal">Now, let’s dive into Python and build the multimodal modular RAG program.</p>
    <h1 id="_idParaDest-103" class="heading-1">Building a multimodal modular RAG program for drone technology</h1>
    <p class="normal">In the following sections, we will build a<a id="_idIndexMarker246"/> multimodal modular RAG-driven generative system from scratch in Python, step by step. We will implement:</p>
    <ul>
      <li class="bulletList">LlamaIndex-managed <a id="_idIndexMarker247"/>OpenAI LLMs to process and understand text about drones</li>
      <li class="bulletList">Deep Lake multimodal datasets containing images and labels of drone images taken</li>
      <li class="bulletList">Functions to display images and identify objects within them using bounding boxes</li>
      <li class="bulletList">A system that can answer questions about drone technology using both text and images</li>
      <li class="bulletList">Performance metrics aimed at measuring the accuracy of the modular multimodal responses, including image analysis with GPT-4o</li>
    </ul>
    <p class="normal">Also, make sure you have created the LLM dataset in <em class="italic">Chapter 2</em> since we will be loading it in this section. However, you can read this chapter without running the notebook since it is self-contained with code and explanations. Now, let’s get to work!</p>
    <p class="normal">Open the <code class="inlineCode">Multimodal_Modular_RAG_Drones.ipynb</code> notebook in the GitHub repository for this chapter at <a href="https://github.com/Denis2054/RAG-Driven-Generative-AI/tree/main/Chapter04">https://github.com/Denis2054/RAG-Driven-Generative-AI/tree/main/Chapter04</a>. The packages installed are the same as those listed in the <em class="italic">Installing the environment</em> section of the previous chapter. Each of the following sections will guide you through building the multimodal modular notebook, starting with the LLM module. Let’s go through each section of the notebook step by step.</p>
    <h2 id="_idParaDest-104" class="heading-2">Loading the LLM dataset</h2>
    <p class="normal">We will load the <a id="_idIndexMarker248"/>drone dataset created in <em class="italic">Chapter 3</em>. Make sure to<a id="_idIndexMarker249"/> insert the path to your dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">import deeplake
dataset_path_llm = "hub://denis76/drone_v2"
ds_llm = deeplake.load(dataset_path_llm)
</code></pre>
    <p class="normal">The output will confirm that the dataset is loaded and will display the link to your dataset:</p>
    <pre class="programlisting con"><code class="hljs-con">This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/drone_v2
hub://denis76/drone_v2 loaded successfully.
</code></pre>
    <p class="normal">The program now creates <a id="_idIndexMarker250"/>a dictionary to hold the data to load it into a pandas DataFrame to visualize it:</p>
    <pre class="programlisting code"><code class="hljs-code">import json
import pandas as pd
import numpy as np
# Create a dictionary to hold the data
data_llm = {}
# Iterate through the tensors in the dataset
for tensor_name in ds_llm.tensors:
    tensor_data = ds_llm[tensor_name].numpy()
    # Check if the tensor is multi-dimensional
    if tensor_data.ndim &gt; 1:
        # Flatten multi-dimensional tensors
        data_llm[tensor_name] = [np.array(e).flatten().tolist() for e in tensor_data]
    else:
        # Convert 1D tensors directly to lists and decode text
        if tensor_name == "text":
            data_llm[tensor_name] = [t.tobytes().decode('utf-8') if t else "" for t in tensor_data]
        else:
            data_llm[tensor_name] = tensor_data.tolist()
# Create a Pandas DataFrame from the dictionary
df_llm = pd.DataFrame(data_llm)
df_llm
</code></pre>
    <p class="normal">The output shows the text dataset <a id="_idIndexMarker251"/>with its structure: <code class="inlineCode">embedding</code> (vectors), <code class="inlineCode">id</code> (unique string identifier), <code class="inlineCode">metadata</code> (in this case, the source of the data), and <code class="inlineCode">text</code>, which contains the content:</p>
    <figure class="mediaobject"><img src="img/B31169_04_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.2: Output of the text dataset structure and content</p>
    <p class="normal">We will <a id="_idIndexMarker252"/>now initialize <a id="_idIndexMarker253"/>the LLM query engine.</p>
    <h3 id="_idParaDest-105" class="heading-3">Initializing the LLM query engine</h3>
    <p class="normal">As in <em class="chapterRef">Chapter 3</em>,<em class="italic"> Building Indexed-Based RAG with LlamaIndex, Deep Lake, and OpenAI</em>, we will initialize a vector store index from the collection of drone documents (<code class="inlineCode">documents_llm</code>) of the dataset (<code class="inlineCode">ds</code>). The <code class="inlineCode">GPTVectorStoreIndex.from_documents()</code> method creates an index that increases the<a id="_idIndexMarker254"/> retrieval speed of documents based on vector similarity:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import VectorStoreIndex
vector_store_index_llm = VectorStoreIndex.from_documents(documents_llm)
</code></pre>
    <p class="normal">The <code class="inlineCode">as_query_engine()</code> method configures this index as a query engine with the specific parameters, as in <em class="chapterRef">Chapter 3</em>, for similarity and retrieval depth, allowing the system to answer queries by finding the most relevant documents:</p>
    <pre class="programlisting code"><code class="hljs-code">vector_query_engine_llm = vector_store_index_llm.as_query_engine(similarity_top_k=2, temperature=0.1, num_output=1024)
</code></pre>
    <p class="normal">Now, the program introduces the user input.</p>
    <h4 class="heading-4">User input for multimodal modular RAG</h4>
    <p class="normal">The goal of defining the user input in the context of the modular RAG system is to formulate a query that will <a id="_idIndexMarker255"/>effectively utilize both<a id="_idIndexMarker256"/> the text-based and image-based capabilities. This allows the system to generate a comprehensive and accurate response by leveraging multiple information sources:</p>
    <pre class="programlisting code"><code class="hljs-code">user_input="How do drones identify a truck?"
</code></pre>
    <p class="normal">In this context, the user input is the <em class="italic">baseline</em>, the starting point, or a standard query used to assess the system’s capabilities. It will establish the initial frame of reference for how well the system can handle and respond to queries utilizing its available resources (e.g., text and image data<a id="_idIndexMarker257"/> from various datasets). In this example, the baseline is empirical and will serve to evaluate the system from that <a id="_idIndexMarker258"/>reference point.</p>
    <h4 class="heading-4">Querying the textual dataset</h4>
    <p class="normal">We will run the vector query<a id="_idIndexMarker259"/> engine request as we did in <em class="chapterRef">Chapter 3</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
import textwrap
#start the timer
start_time = time.time()
llm_response = vector_query_engine_llm.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(textwrap.fill(str(llm_response), 100))
</code></pre>
    <p class="normal">The execution time is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">Query execution time: 1.5489 seconds
</code></pre>
    <p class="normal">The output content is also satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">Drones can identify a truck using visual detection and tracking methods, which may involve deep neural networks for performance benchmarking.
</code></pre>
    <p class="normal">The program now loads the multimodal drone dataset.</p>
    <h2 id="_idParaDest-106" class="heading-2">Loading and visualizing the multimodal dataset</h2>
    <p class="normal">We will use the existing<a id="_idIndexMarker260"/> pubic VisDrone dataset available on Deep Lake: <a href="https://datasets.activeloop.ai/docs/ml/datasets/visdrone-dataset/">https://datasets.activeloop.ai/docs/ml/datasets/visdrone-dataset/</a>. We will <em class="italic">not</em> create a vector store but <a id="_idIndexMarker261"/>simply load the existing dataset in memory:</p>
    <pre class="programlisting code"><code class="hljs-code">import deeplake
dataset_path = 'hub://activeloop/visdrone-det-train'
ds = deeplake.load(dataset_path) # Returns a Deep Lake Dataset but does not download data locally
</code></pre>
    <p class="normal">The output will display a link to the online dataset that you can explore with SQL, or natural language processing <a id="_idIndexMarker262"/>commands if you prefer, with the tools provided by Deep Lake:</p>
    <pre class="programlisting con"><code class="hljs-con">Opening dataset in read-only mode as you don't have write permissions.
This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/visdrone-det-train
hub://activeloop/visdrone-det-train loaded successfully.
</code></pre>
    <p class="normal">Let’s display the summary to explore the dataset in code:</p>
    <pre class="programlisting code"><code class="hljs-code">ds.summary()
</code></pre>
    <p class="normal">The output provides useful information on the structure of the dataset:</p>
    <pre class="programlisting con"><code class="hljs-con">Dataset(path='hub://activeloop/visdrone-det-train', read_only=True, tensors=['boxes', 'images', 'labels'])
tensor    htype            shape              dtype     compression
------    -----            -----              -----     -----------
boxes     bbox         (6471, 1:914, 4)       float32          None
images    image        (6471, 360:1500,                            
                        480:2000, 3)          uint8            jpeg
labels    class_label  (6471, 1:914)          uint32           None
</code></pre>
    <p class="normal">The structure contains<a id="_idIndexMarker263"/> images, boxes for the boundary boxes of the objects in the image, and labels describing the images and boundary boxes. Let’s visualize the dataset in code:</p>
    <pre class="programlisting code"><code class="hljs-code">ds.visualize()
</code></pre>
    <p class="normal">The output shows the images and their boundary boxes:</p>
    <figure class="mediaobject"><img src="img/B31169_04_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.3: Output showing boundary boxes</p>
    <p class="normal">Now, let’s go further <a id="_idIndexMarker264"/>and display the content of the dataset in a pandas DataFrame to see what<a id="_idIndexMarker265"/> the images look like:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
# Create an empty DataFrame with the defined structure
df = pd.DataFrame(columns=['image', 'boxes', 'labels'])
# Iterate through the samples using enumerate
for i, sample in enumerate(ds):
    # Image data (choose either path or compressed representation)
    # df.loc[i, 'image'] = sample.images.path  # Store image path
    df.loc[i, 'image'] = sample.images.tobytes()  # Store compressed image data
    # Bounding box data (as a list of lists)
    boxes_list = sample.boxes.numpy(aslist=True)
    df.loc[i, 'boxes'] = [box.tolist() for box in boxes_list]
    # Label data (as a list)
    label_data = sample.labels.data()
    df.loc[i, 'labels'] = label_data['text']
df
</code></pre>
    <p class="normal">The output in <em class="italic">Figure 4.4</em> shows the content of the dataset:</p>
    <figure class="mediaobject"><img src="img/B31169_04_04.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.4: Excerpt of the VisDrone dataset</p>
    <p class="normal">There are 6,471 rows of<a id="_idIndexMarker266"/> images in the dataset and 3 columns:</p>
    <ul>
      <li class="bulletList">The <code class="inlineCode">image</code> column contains<a id="_idIndexMarker267"/> the image. The format of the image in the dataset, as indicated by the byte sequence <code class="inlineCode">b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00...'</code>, is JPEG. The bytes <code class="inlineCode">b'\xff\xd8\xff\xe0'</code> specifically signify the start of a JPEG image file.</li>
      <li class="bulletList">The <code class="inlineCode">boxes</code> column contains the coordinates and dimensions of bounding boxes in the image, which are normally in the format <code class="inlineCode">[x, y, width, height]</code>.</li>
      <li class="bulletList">The <code class="inlineCode">labels</code> column contains the label of each bounding box in the <code class="inlineCode">boxes</code> column.</li>
    </ul>
    <p class="normal">We can display the list of labels for the images:</p>
    <pre class="programlisting code"><code class="hljs-code">labels_list = ds.labels.info['class_names']
labels_list
</code></pre>
    <p class="normal">The output provides the list of labels, which defines the scope of the dataset:</p>
    <pre class="programlisting con"><code class="hljs-con">['ignored regions',
 'pedestrian',
 'people',
 'bicycle',
 'car',
 'van',
 'truck',
 'tricycle',
 'awning-tricycle',
 'bus',
 'motor',
 'others']
</code></pre>
    <p class="normal">With that, we have <a id="_idIndexMarker268"/>successfully loaded the dataset and will now explore the multimodal dataset <a id="_idIndexMarker269"/>structure.</p>
    <h2 id="_idParaDest-107" class="heading-2">Navigating the multimodal dataset structure</h2>
    <p class="normal">In this section, we will select an<a id="_idIndexMarker270"/> image and display it using the dataset’s image column. To this image, we will then add the bounding boxes of a label that we will choose. The <a id="_idIndexMarker271"/>program first selects an image.</p>
    <h3 id="_idParaDest-108" class="heading-3">Selecting and displaying an image</h3>
    <p class="normal">We will select the<a id="_idIndexMarker272"/> first image in the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"># choose an image
ind=0
image = ds.images[ind].numpy() # Fetch the first image and return a numpy array
</code></pre>
    <p class="normal">Now, let’s display it with no <a id="_idIndexMarker273"/>bounding boxes:</p>
    <pre class="programlisting code"><code class="hljs-code">import deeplake
from IPython.display import display
from PIL import Image
import cv2  # Import OpenCV
image = ds.images[0].numpy()
# Convert from BGR to RGB (if necessary)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
# Create PIL Image and display
img = Image.fromarray(image_rgb)
display(img)
</code></pre>
    <p class="normal">The image displayed contains <a id="_idIndexMarker274"/>trucks, pedestrians, and other types of objects:</p>
    <figure class="mediaobject"><img src="img/B31169_04_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.5: Output displaying objects</p>
    <p class="normal">Now that the image is displayed, the program will add bounding boxes.</p>
    <h3 id="_idParaDest-109" class="heading-3">Adding bounding boxes and saving the image</h3>
    <p class="normal">We have displayed <a id="_idIndexMarker275"/>the first image. The program will then fetch all the<a id="_idIndexMarker276"/> labels for the selected image:</p>
    <pre class="programlisting code"><code class="hljs-code">labels = ds.labels[ind].data() # Fetch the labels in the selected image
print(labels)
</code></pre>
    <p class="normal">The output displays <code class="inlineCode">value</code>, which contains the numerical indices of a label, and <code class="inlineCode">text</code>, which contains the corresponding text labels of a label:</p>
    <pre class="programlisting con"><code class="hljs-con">{'value': array([1, 1, 7, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6,
       1, 1, 1, 1, 1, 1, 6, 6, 3, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 6, 6, 6], dtype=uint32), 'text': ['pedestrian', 'pedestrian', 'tricycle', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'pedestrian', 'truck', 'truck', 'truck', 'truck', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'truck', 'truck', 'bicycle', 'truck', 'truck', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'truck', 'truck', 'truck']}
</code></pre>
    <p class="normal">We can display the values and the corresponding text in two columns:</p>
    <pre class="programlisting code"><code class="hljs-code">values = labels['value']
text_labels = labels['text']
# Determine the maximum text label length for formatting
max_text_length = max(len(label) for label in text_labels)
# Print the header
print(f"{'Index':&lt;10}{'Label':&lt;{max_text_length + 2}}")
print("-" * (10 + max_text_length + 2))  # Add a separator line
# Print the indices and labels in two columns
for index, label in zip(values, text_labels):
    print(f"{index:&lt;10}{label:&lt;{max_text_length + 2}}")
</code></pre>
    <p class="normal">The output gives us a clear representation of the content of the labels of an image:</p>
    <pre class="programlisting con"><code class="hljs-con">Index     Label     
----------------------
1         pedestrian
1         pedestrian
7         tricycle  
1         pedestrian
1         pedestrian
1         pedestrian
1         pedestrian
6         truck     
6         truck    …
</code></pre>
    <p class="normal">We can group the<a id="_idIndexMarker277"/> class names (labels in plain text) of<a id="_idIndexMarker278"/> the images:</p>
    <pre class="programlisting code"><code class="hljs-code">ds.labels[ind].info['class_names'] # class names of the selected image
</code></pre>
    <p class="normal">We can now group and display all the labels that describe the image:</p>
    <pre class="programlisting code"><code class="hljs-code">ds.labels[ind].info['class_names'] #class names of the selected image
</code></pre>
    <p class="normal">We can see all the classes the image contains:</p>
    <pre class="programlisting con"><code class="hljs-con">['ignored regions',
 'pedestrian',
 'people',
 'bicycle',
 'car',
 'van',
 'truck',
 'tricycle',
 'awning-tricycle',
 'bus',
 'motor',
 'others']
</code></pre>
    <p class="normal">The number of label classes sometimes exceeds what a human eye can see in an image.</p>
    <p class="normal">Let’s now add bounding boxes. We first create a function to add the bounding boxes, display them, and save the image:</p>
    <pre class="programlisting code"><code class="hljs-code">def display_image_with_bboxes(image_data, bboxes, labels, label_name, ind=0):
    #Displays an image with bounding boxes for a specific label.
    image_bytes = io.BytesIO(image_data)
    img = Image.open(image_bytes)
    # Extract class names specifically for the selected image
    class_names = ds.labels[ind].info['class_names']
    # Filter for the specific label (or display all if class names are missing)
    if class_names is not None:
        try:
            label_index = class_names.index(label_name)
            relevant_indices = np.where(labels == label_index)[0]
        except ValueError:
            print(f"Warning: Label '{label_name}' not found. Displaying all boxes.")
            relevant_indices = range(len(labels))
    else:
        relevant_indices = []  # No labels found, so display no boxes
    # Draw bounding boxes
    draw = ImageDraw.Draw(img)
    for idx, box in enumerate(bboxes):  # Enumerate over bboxes
        if idx in relevant_indices:   # Check if this box is relevant
            x1, y1, w, h = box
            x2, y2 = x1 + w, y1 + h
            draw.rectangle([x1, y1, x2, y2], outline="red", width=2)
            draw.text((x1, y1), label_name, fill="red")
    # Save the image
    save_path="boxed_image.jpg"
    img.save(save_path)
    display(img)
</code></pre>
    <p class="normal">We can add the bounding <a id="_idIndexMarker279"/>boxes for a specific label. In this<a id="_idIndexMarker280"/> case, we selected the <code class="inlineCode">"truck"</code> label:</p>
    <pre class="programlisting code"><code class="hljs-code">import io
from PIL import ImageDraw
# Fetch labels and image data for the selected image
labels = ds.labels[ind].data()['value']
image_data = ds.images[ind].tobytes()
bboxes = ds.boxes[ind].numpy()
ibox="truck" # class in image
# Display the image with bounding boxes for the label chosen
display_image_with_bboxes(image_data, bboxes, labels, label_name=ibox)
</code></pre>
    <p class="normal">The image displayed<a id="_idIndexMarker281"/> now contains the bounding boxes for trucks:</p>
    <figure class="mediaobject"><img src="img/B31169_04_06.png" alt="A truck with several trailers  Description automatically generated with medium confidence"/></figure>
    <figure class="mediaobject">Figure 4.6: Output displaying bounding boxes</figure>
    <p class="normal">Let’s now activate a<a id="_idIndexMarker282"/> query engine to retrieve and obtain a response.</p>
    <h2 id="_idParaDest-110" class="heading-2">Building a multimodal query engine</h2>
    <p class="normal">In this section, we will query the VisDrone dataset and retrieve an image that fits the user input we entered in the <em class="italic">User input for multimodal modular RAG</em> section of this notebook. To achieve this <a id="_idIndexMarker283"/>goal, we will:</p>
    <ol>
      <li class="numberedList" value="1">Create a vector index for each row of the <code class="inlineCode">df</code> DataFrame containing the images, boxing data, and labels of the VisDrone dataset.</li>
      <li class="numberedList">Create a query engine that will <a id="_idIndexMarker284"/>query the text data of the dataset, retrieve relevant image information, and provide a text response.</li>
      <li class="numberedList">Parse the nodes of the response to find the keywords related to the user input.</li>
      <li class="numberedList">Parse the nodes of the response to find the source image.</li>
      <li class="numberedList">Add the bounding<a id="_idIndexMarker285"/> boxes of the source image to the image.</li>
      <li class="numberedList">Save the image.</li>
    </ol>
    <h3 id="_idParaDest-111" class="heading-3">Creating a vector index and query engine</h3>
    <p class="normal">The code first creates a<a id="_idIndexMarker286"/> document that will be processed to create a vector store index for the multimodal drone dataset. The <code class="inlineCode">df</code> DataFrame we created in the <em class="italic">Loading and visualizing the multimodal dataset</em> section of the <a id="_idIndexMarker287"/>notebook on GitHub does not have unique indices or embeddings. We will create them in memory with LlamaIndex.</p>
    <p class="normal">The program first assigns a unique ID to the DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code"># The DataFrame is named 'df'
df['doc_id'] = df.index.astype(str)  # Create unique IDs from the row indices
</code></pre>
    <p class="normal">This line adds a new column to the <code class="inlineCode">df</code> DataFrame called <code class="inlineCode">doc_id</code>. It assigns unique identifiers to each row by converting the DataFrame’s row indices to strings. An empty list named <code class="inlineCode">documents</code> is initialized, which we will use to create a vector index:</p>
    <pre class="programlisting code"><code class="hljs-code"># Create documents (extract relevant text for each image's labels)
documents = []
</code></pre>
    <p class="normal">Now, the <code class="inlineCode">iterrows()</code> method iterates through each row of the DataFrame, generating a sequence of index and row pairs:</p>
    <pre class="programlisting code"><code class="hljs-code">for _, row in df.iterrows():
    text_labels = row['labels'] # Each label is now a string
    text = " ".join(text_labels) # Join text labels into a single string
    document = Document(text=text, doc_id=row['doc_id'])
    documents.append(document)
</code></pre>
    <p class="normal"><code class="inlineCode">documents</code> is appended with all the records in the dataset, and a DataFrame is created:</p>
    <pre class="programlisting code"><code class="hljs-code"># The DataFrame is named 'df'
df['doc_id'] = df.index.astype(str)  # Create unique IDs from the row indices
# Create documents (extract relevant text for each image's labels)
documents = []
for _, row in df.iterrows():
    text_labels = row['labels'] # Each label is now a string
    text = " ".join(text_labels) # Join text labels into a single string
    document = Document(text=text, doc_id=row['doc_id'])
    documents.append(document)
</code></pre>
    <p class="normal">The documents are now ready<a id="_idIndexMarker288"/> to be indexed with <code class="inlineCode">GPTVectorStoreIndex</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import GPTVectorStoreIndex
vector_store_index = GPTVectorStoreIndex.from_documents(documents)
</code></pre>
    <p class="normal">The dataset is then<a id="_idIndexMarker289"/> seamlessly equipped with indices that we can visualize in the index dictionary:</p>
    <pre class="programlisting code"><code class="hljs-code">vector_store_index.index_struct
</code></pre>
    <p class="normal">The output shows that an index has now been added to the dataset:</p>
    <pre class="programlisting con"><code class="hljs-con">IndexDict(index_id='4ec313b4-9a1a-41df-a3d8-a4fe5ff6022c', summary=None, nodes_dict={'5e547c1d-0d65-4de6-b33e-a101665751e6': '5e547c1d-0d65-4de6-b33e-a101665751e6', '05f73182-37ed-4567-a855-4ff9e8ae5b8c': '05f73182-37ed-4567-a855-4ff9e8ae5b8c'
</code></pre>
    <p class="normal">We can now run a query on the multimodal dataset.</p>
    <h3 id="_idParaDest-112" class="heading-3">Running a query on the VisDrone multimodal dataset</h3>
    <p class="normal">We now set <code class="inlineCode">vector_store_index</code> as the<a id="_idIndexMarker290"/> query engine, as we did in the <em class="italic">Vector store index query engine</em> section in <em class="chapterRef">Chapter 3</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">vector_query_engine = vector_store_index.as_query_engine(similarity_top_k=1, temperature=0.1, num_output=1024)
</code></pre>
    <p class="normal">We can also run a query on the dataset of drone images, just as we did in <em class="chapterRef">Chapter 3</em> on an LLM dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
start_time = time.time()
response = vector_query_engine.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
</code></pre>
    <p class="normal">The execution time is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">Query execution time: 1.8461 seconds
</code></pre>
    <p class="normal">We will now examine the text response:</p>
    <pre class="programlisting code"><code class="hljs-code">print(textwrap.fill(str(response), 100))
</code></pre>
    <p class="normal">We can see that the output is logical and therefore satisfactory.</p>
    <p class="normal">Drones use various<a id="_idIndexMarker291"/> sensors such as cameras, LiDAR, and GPS to identify and track objects like trucks.</p>
    <h3 id="_idParaDest-113" class="heading-3">Processing the response</h3>
    <p class="normal">We will now parse the<a id="_idIndexMarker292"/> nodes in the response to find the unique words in the response and select one for this notebook:</p>
    <pre class="programlisting code"><code class="hljs-code">from itertools import groupby
def get_unique_words(text):
    text = text.lower().strip()
    words = text.split()
    unique_words = [word for word, _ in groupby(sorted(words))]
    return unique_words
for node in response.source_nodes:
    print(node.node_id)
    # Get unique words from the node text:
    node_text = node.get_text()
    unique_words = get_unique_words(node_text)
    print("Unique Words in Node Text:", unique_words)
</code></pre>
    <p class="normal">We found a unique word (<code class="inlineCode">'truck'</code>) and its unique index, which will lead us directly to the image of the source of the node that generated the response:</p>
    <pre class="programlisting con"><code class="hljs-con">1af106df-c5a6-4f48-ac17-f953dffd2402
Unique Words in Node Text: ['truck']
</code></pre>
    <p class="normal">We could select more words and design this function in many different ways depending on the specifications <a id="_idIndexMarker293"/>of each project.</p>
    <p class="normal">We will now search for the image by going through the source nodes, just as we did for an LLM dataset in the <em class="italic">Query response and source</em> section of the previous chapter. Multimodal vector stores and querying frameworks are flexible. Once we learn how to perform retrievals on an LLM and a multimodal dataset, we are ready for anything that comes up!</p>
    <p class="normal">Let’s select and process the information related to an image.</p>
    <h3 id="_idParaDest-114" class="heading-3">Selecting and processing the image of the source node</h3>
    <p class="normal">Before running<a id="_idIndexMarker294"/> the image retrieval and displaying function, let’s first delete the image we displayed in the <em class="italic">Adding bounding boxes and saving the image</em> section of this notebook to make sure we are working on a new image:</p>
    <pre class="programlisting code"><code class="hljs-code"># deleting any image previously saved
!rm /content/boxed_image.jpg
</code></pre>
    <p class="normal">We are now ready to search for the source image, call the bounding box, and display and save the function we defined earlier:</p>
    <pre class="programlisting code"><code class="hljs-code">display_image_with_bboxes(image_data, bboxes, labels, label_name=ibox)
</code></pre>
    <p class="normal">The program now goes through the source nodes with the keyword <code class="inlineCode">"truck"</code> search, applies the bounding boxes, and displays and saves the image:</p>
    <pre class="programlisting code"><code class="hljs-code">import io
from PIL import Image
def process_and_display(response, df, ds, unique_words):
    """Processes nodes, finds corresponding images in dataset, and displays them with bounding boxes.
    Args:
        response: The response object containing source nodes.
        df: The DataFrame with doc_id information.
        ds: The dataset containing images, labels, and boxes.
        unique_words: The list of unique words for filtering.
    """
…
            if i == row_index:
                image_bytes = io.BytesIO(sample.images.tobytes())
                img = Image.open(image_bytes)
                labels = ds.labels[i].data()['value']
                image_data = ds.images[i].tobytes()
                bboxes = ds.boxes[i].numpy()
                ibox = unique_words[0]  # class in image
                display_image_with_bboxes(image_data, bboxes, labels, label_name=ibox)
# Assuming you have your 'response', 'df', 'ds', and 'unique_words' objects prepared:
process_and_display(response, df, ds, unique_words)
</code></pre>
    <p class="normal">The output is<a id="_idIndexMarker295"/> satisfactory:</p>
    <figure class="mediaobject"><img src="img/B31169_04_07.png" alt="An aerial view of a factory  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.7: Displayed satisfactory output</p>
    <h2 id="_idParaDest-115" class="heading-2">Multimodal modular summary</h2>
    <p class="normal">We have built a multimodal modular program step by step that we can now assemble in a summary. We will create a <a id="_idIndexMarker296"/>function to display the source image of the response to the user input, then print the user input and the LLM output, and display the image.</p>
    <p class="normal">First, we create a function to display the source image saved by the multimodal retrieval engine:</p>
    <pre class="programlisting code"><code class="hljs-code"># 1.user input=user_input
print(user_input)
# 2.LLM response
print(textwrap.fill(str(llm_response), 100))
# 3.Multimodal response
image_path = "/content/boxed_image.jpg"
display_source_image(image_path)
</code></pre>
    <p class="normal">Then, we can display the user input, the LLM response, and the multimodal response. The output first displays<a id="_idIndexMarker297"/> the textual responses (user input and LLM response):</p>
    <pre class="programlisting con"><code class="hljs-con">How do drones identify a truck?
Drones can identify a truck using visual detection and tracking methods, which may involve deep neural networks for performance benchmarking.
</code></pre>
    <p class="normal">Then, the image is displayed with the bounding boxes for trucks in this case:</p>
    <figure class="mediaobject"><img src="img/B31169_04_08.png" alt="An aerial view of a factory  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.8: Output displaying boundary boxes</p>
    <p class="normal">By adding an image to a <a id="_idIndexMarker298"/>classical LLM response, we augmented the output. Multimodal RAG output augmentation will enrich generative AI by adding information to both the input and output. However, as for all AI programs, designing a performance metric requires efficient image recognition functionality.</p>
    <h2 id="_idParaDest-116" class="heading-2">Performance metric</h2>
    <p class="normal">Measuring the performance of a multimodal modular RAG requires two types of measurements: text and image. Measuring<a id="_idIndexMarker299"/> text is straightforward. However, measuring images is quite a challenge. Analyzing the image of <a id="_idIndexMarker300"/>a multimodal response is quite different. We extracted a keyword from the multimodal query <a id="_idIndexMarker301"/>engine. We then parsed the response for a source image to display. However, we will need to build an innovative approach to evaluate the source image of the response. Let’s begin with the LLM performance.</p>
    <h3 id="_idParaDest-117" class="heading-3">LLM performance metric</h3>
    <p class="normal">LlamaIndex seamlessly called an OpenAI model through its query engine, such as GPT-4, for example, and provided text content in its response. For text responses, we will use the same cosine similarity<a id="_idIndexMarker302"/> metric as in the <em class="italic">Evaluating the output with cosine similarity</em> section in <em class="chapterRef">Chapter 2</em>, and the <em class="italic">Vector store index query engine</em> section in <em class="chapterRef">Chapter 3</em>.</p>
    <p class="normal">The evaluation function uses <code class="inlineCode">sklearn</code> and <code class="inlineCode">sentence_transformers</code> to evaluate the similarity between two texts—in this case, an input and an output:</p>
    <pre class="programlisting code"><code class="hljs-code">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
def calculate_cosine_similarity_with_embeddings(text1, text2):
    embeddings1 = model.encode(text1)
    embeddings2 = model.encode(text2)
    similarity = cosine_similarity([embeddings1], [embeddings2])
    return similarity[0][0]
</code></pre>
    <p class="normal">We can now calculate the similarity between our baseline user input and the initial LLM response obtained:</p>
    <pre class="programlisting code"><code class="hljs-code">llm_similarity_score = calculate_cosine_similarity_with_embeddings(user_input, str(llm_response))
print(user_input)
print(llm_response)
print(f"Cosine Similarity Score: {llm_similarity_score:.3f}")
</code></pre>
    <p class="normal">The output displays the user input, the text response, and the cosine similarity between the two texts:</p>
    <pre class="programlisting con"><code class="hljs-con">How do drones identify a truck?
How do drones identify a truck?
Drones can identify a truck using visual detection and tracking methods, which may involve deep neural networks for performance benchmarking.
Cosine Similarity Score: 0.691
</code></pre>
    <p class="normal">The output is satisfactory. But we now need to design a way to measure the multimodal performance.</p>
    <h3 id="_idParaDest-118" class="heading-3">Multimodal performance metric</h3>
    <p class="normal">To evaluate the image returned, we cannot simply rely on the labels in the dataset. For small datasets, we can manually check the image, but when a system scales, automation is required. In this section, we will use the computer vision features of GPT-4o to analyze an image, parse it to find the<a id="_idIndexMarker303"/> objects we are looking for, and provide a description of that image. Then, we will apply cosine similarity to the description provided by GPT-4o and the label it is supposed to contain. GPT-4o is a multimodal generative AI model.</p>
    <p class="normal">Let’s first encode the image to simplify data transmission to GPT-4o. Base64 encoding converts binary data (like images) into ASCII characters, which are standard text characters. This transformation is crucial because it ensures that the image data can be transmitted over protocols (like HTTP) that are designed to handle text data smoothly. It also avoids issues related to binary data transmission, such as data corruption or interpretation errors.</p>
    <p class="normal">The program encodes the source image using Python’s <code class="inlineCode">base64</code> module:</p>
    <pre class="programlisting code"><code class="hljs-code">import base64
IMAGE_PATH = "/content/boxed_image.jpg"
# Open the image file and encode it as a base64 string
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")
base64_image = encode_image(IMAGE_PATH)
</code></pre>
    <p class="normal">We now create an OpenAI client and set the model to <code class="inlineCode">gpt-4o</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">from openai import OpenAI
#Set the API key for the client
client = OpenAI(api_key=openai.api_key)
MODEL="gpt-4o"
</code></pre>
    <p class="normal">The unique word will be the result of the LLM query to the multimodal dataset we obtained by parsing the response:</p>
    <pre class="programlisting code"><code class="hljs-code">u_word=unique_words[0]
print(u_word)
</code></pre>
    <p class="normal">We can now submit the image to OpenAI GPT-4o:</p>
    <pre class="programlisting code"><code class="hljs-code">response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content": f"You are a helpful assistant that analyzes images that contain {u_word}."},
        {"role": "user", "content": [
            {"type": "text", "text": f"Analyze the following image, tell me if there is one {u_word} or more in the bounding boxes and analyze them:"},
            {"type": "image_url", "image_url": {
                "url": f"data:image/png;base64,{base64_image}"}
            }
        ]}
    ],
    temperature=0.0,
)
response_image = response.choices[0].message.content
print(response_image)
</code></pre>
    <p class="normal">We instructed the <code class="inlineCode">system</code> and <code class="inlineCode">user</code> roles to analyze images looking for our target label, <code class="inlineCode">u_word</code>—in this case, <code class="inlineCode">truck</code>. We<a id="_idIndexMarker304"/> then submitted the source node image to the model. The output that describes the image is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">The image contains two trucks within the bounding boxes. Here is the analysis of each truck:
1. **First Truck (Top Bounding Box)**:
   - The truck appears to be a flatbed truck.
   - It is loaded with various materials, possibly construction or industrial supplies.
   - The truck is parked in an area with other construction materials and equipment.
2. **Second Truck (Bottom Bounding Box)**:
   - This truck also appears to be a flatbed truck.
   - It is carrying different types of materials, similar to the first truck.
   - The truck is situated in a similar environment, surrounded by construction materials and equipment.
Both trucks are in a construction or industrial area, likely used for transporting materials and equipment.
</code></pre>
    <p class="normal">We can now submit this response to the cosine similarity function by first adding an <code class="inlineCode">"s"</code> to align with multiple <a id="_idIndexMarker305"/>trucks in a response:</p>
    <pre class="programlisting code"><code class="hljs-code">resp=u_word+"s"
multimodal_similarity_score = calculate_cosine_similarity_with_embeddings(resp, str(response_image))
print(f"Cosine Similarity Score: {multimodal_similarity_score:.3f}")
</code></pre>
    <p class="normal">The output describes the image well but contains many other descriptions beyond the word “<code class="inlineCode">truck</code>,” which limits its similarity to the input requested:</p>
    <pre class="programlisting con"><code class="hljs-con">Cosine Similarity Score: 0.505
</code></pre>
    <p class="normal">A human observer might approve the image and the LLM response. However, even if the score was very high, the issue would be the same. Complex images are challenging to analyze in detail and with precision, although progress is continually made. Let’s now calculate the overall performance of the system.</p>
    <h3 id="_idParaDest-119" class="heading-3">Multimodal modular RAG performance metric</h3>
    <p class="normal">To obtain the overall performance<a id="_idIndexMarker306"/> of the system, we will divide the sum of the LLM response and the two multimodal response performances by <code class="inlineCode">2</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">score=(llm_similarity_score+multimodal_similarity_score)/2
print(f"Multimodal, Modular Score: {score:.3f}")
</code></pre>
    <p class="normal">The result shows that although a human who observes the results may be satisfied, it remains difficult to automatically assess the relevance of a complex image:</p>
    <pre class="programlisting con"><code class="hljs-con">Multimodal, Modular Score: 0.598
</code></pre>
    <p class="normal">The metric can be improved because a human observer sees that the image is relevant. This explains why the top AI agents, such as ChatGPT, Gemini, and Bing Copilot, always have a feedback process that includes thumbs up and thumbs down.</p>
    <p class="normal">Let’s now sum up the chapter and gear up to explore how RAG can be improved even further with human feedback.</p>
    <h1 id="_idParaDest-120" class="heading-1">Summary</h1>
    <p class="normal">This chapter introduced us to the world of multimodal modular RAG, which uses distinct modules for different data types (text and image) and tasks. We leveraged the functionality of LlamaIndex, Deep Lake, and OpenAI, which we explored in the previous chapters. The Deep Lake VisDrone dataset further introduced us to drone technology for analyzing images and identifying objects. The dataset contained images, labels, and bounding box information. Working on drone technology involves multimodal data, encouraging us to develop skills that we can use across many domains, such as wildlife tracking, streamlining commercial deliveries, and making safer infrastructure inspections.</p>
    <p class="normal">We built a multimodal modular RAG-driven generative AI system. The first step was to define a baseline user query for both LLM and multimodal queries. We began by querying the Deep Lake textual dataset that we implemented in <em class="italic">Chapter 3</em>. LlamaIndex seamlessly ran a query engine to retrieve, augment, and generate a response. Then, we loaded the Deep Lake VisDrone dataset and indexed it in memory with LlamaIndex to create an indexed vector search retrieval pipeline. We queried it through LlamaIndex, which used an OpenAI model such as GPT-4 and parsed the text generated for a keyword. Finally, we searched the source nodes of the response to find the source image, display it, and merge the LLM and image responses into an augmented output. We applied cosine similarity to the text response. Evaluating the image was challenging, so we first ran image recognition with GPT-4o on the image retrieved to obtain a text to which we applied cosine similarity.</p>
    <p class="normal">The journey into multimodal modular RAG-driven generative AI took us deep into the cutting edge of AI. Building a complex system was good preparation for real-life AI projects, which often require implementing multisource, multimodal, and unstructured data, leading to modular, complex systems. Thanks to transparent access to the source of a response, the complexity of RAG can be harnessed, controlled, and improved. We will see how we can leverage the transparency of the sources of a response to introduce human feedback to improve AI. The next chapter will take us further into transparency and precision in AI.</p>
    <h1 id="_idParaDest-121" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with <em class="italic">Yes</em> or <em class="italic">No</em>:</p>
    <ol>
      <li class="numberedList" value="1">Does multimodal modular RAG handle different types of data, such as text and images? </li>
      <li class="numberedList">Are drones used solely for agricultural monitoring and aerial photography? </li>
      <li class="numberedList">Is the Deep Lake VisDrone dataset used in this chapter for textual data only? </li>
      <li class="numberedList">Can bounding boxes be added to drone images to identify objects such as trucks and pedestrians? </li>
      <li class="numberedList">Does the modular system retrieve both text and image data for query responses? </li>
      <li class="numberedList">Is building a vector index necessary for querying the multimodal VisDrone dataset? </li>
      <li class="numberedList">Are the retrieved images processed without adding any labels or bounding boxes? </li>
      <li class="numberedList">Is the multimodal modular RAG performance metric based only on textual responses? </li>
      <li class="numberedList">Can a multimodal system such as the one described in this chapter handle only drone-related data? </li>
      <li class="numberedList">Is evaluating images as easy as evaluating text in multimodal RAG?</li>
    </ol>
    <h1 id="_idParaDest-122" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">LlamaIndex: <a href="https://docs.llamaindex.ai/en/stable/">https://docs.llamaindex.ai/en/stable/</a></li>
      <li class="bulletList">Activeloop Deep Lake: <a href="https://docs.activeloop.ai/">https://docs.activeloop.ai/</a></li>
      <li class="bulletList">OpenAI: <a href="https://platform.openai.com/docs/overview">https://platform.openai.com/docs/overview</a></li>
    </ul>
    <h1 id="_idParaDest-123" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Retrieval-Augmented Multimodal Language Modeling, Yasunaga et al. (2023), <a href="https://arxiv.org/pdf/2211.12561">https://arxiv.org/pdf/2211.12561</a></li>
    </ul>
    <h1 id="_idParaDest-124" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>