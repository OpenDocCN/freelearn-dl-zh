<html><head></head><body>
<div id="_idContainer045">
<h1 class="chapter-number" id="_idParaDest-144"><a id="_idTextAnchor251"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-145"><a id="_idTextAnchor252"/><span class="koboSpan" id="kobo.2.1">Addressing Ethical Considerations and Charting a Path Toward Trustworthy Generative AI</span></h1>
<p><span class="koboSpan" id="kobo.3.1">As generative AI advances, it will extend beyond basic language tasks, integrating into daily life and impacting almost every sector. </span><span class="koboSpan" id="kobo.3.2">The inevitability of its widespread adoption highlights the need to address its ethical implications. </span><span class="koboSpan" id="kobo.3.3">The promise of this technology to revolutionize industries, enhance creativity, and solve complex problems must be coupled with the responsibility to navigate its ethical landscape diligently. </span><span class="koboSpan" id="kobo.3.4">This chapter will explore these ethical considerations, dissect the intricacies of biases entangled in these models, and look at strategies for cultivating trust in general-purpose AI systems. </span><span class="koboSpan" id="kobo.3.5">Through thorough examination and reflection, we can begin to outline a path toward responsible use, helping to ensure that advancements in generative AI are leveraged for the greater good while </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">minimizing harm.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">To ground our discussion, we will first identify some ethical norms and universal values relevant to generative AI. </span><span class="koboSpan" id="kobo.5.2">While this chapter cannot be exhaustive, it aims to introduce key </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">ethical considerations.</span></span></p>
<h1 id="_idParaDest-146"><a id="_idTextAnchor253"/><span class="koboSpan" id="kobo.7.1">Ethical norms and values in the context of generative AI</span></h1>
<p><span class="koboSpan" id="kobo.8.1">The ethical norms and values guiding</span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.9.1"> the development and deployment of generative AI are rooted in transparency, equity, accountability, privacy, consent, security, and inclusivity. </span><span class="koboSpan" id="kobo.9.2">These principles can serve as a foundation for developing and adopting systems aligned with societal values and supporting</span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.10.1"> the greater good. </span><span class="koboSpan" id="kobo.10.2">Let’s explore these </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">in detail:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.12.1">Transparency</span></strong><span class="koboSpan" id="kobo.13.1"> involves clearly explaining</span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.14.1"> the methodologies, data sources, and processes behind large language model (LLM) construction. </span><span class="koboSpan" id="kobo.14.2">This practice builds trust by enabling stakeholders to understand the technology’s reliability and limits. </span><span class="koboSpan" id="kobo.14.3">For example, a company could publish a detailed report on the types of data trained on their LLM and the steps taken to ensure data privacy and </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">bias mitigation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.16.1">Equity</span></strong><span class="koboSpan" id="kobo.17.1"> in the context of LLMs ensures fair treatment and outcomes for all users by actively preventing biases in models. </span><span class="koboSpan" id="kobo.17.2">This requires thorough analysis and correction of training data and continuous monitoring of exchanges to reduce discrimination. </span><span class="koboSpan" id="kobo.17.3">One measure a firm might apply is a routine review of LLM performance across various demographic groups to identify and address </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">unintended biases.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.19.1">Accountability</span></strong><span class="koboSpan" id="kobo.20.1"> establishes that developers and users of LLMs are responsible for model outputs and impacts. </span><span class="koboSpan" id="kobo.20.2">It includes transparent and accessible mechanisms for reporting and addressing negative consequences or ethical violations. </span><span class="koboSpan" id="kobo.20.3">In practice, this could manifest as the establishment of an independent review board that oversees AI projects and intervenes in cases of </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">ethical misconduct.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.22.1">Privacy and consent</span></strong><span class="koboSpan" id="kobo.23.1">, in principle, involves ensuring that individual privacy and consent are respected and preserved during the use of personal data as input to LLMs. </span><span class="koboSpan" id="kobo.23.2">In practice, developers should avoid using personal data for training without explicit permission and implement strong data protection measures. </span><span class="koboSpan" id="kobo.23.3">For example, a developer might use data anonymization or privacy-preserving techniques to train models, ensuring that personal identifiers and sensitive information are removed before </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">data processing.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.25.1">Security</span></strong><span class="koboSpan" id="kobo.26.1"> involves protecting LLM-integrated systems and their data from unauthorized access and cyber threats. </span><span class="koboSpan" id="kobo.26.2">In practice, setting up LLM-specific red teams (or teams that test defenses by simulating attacks) can help safeguard AI systems against </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">potential breaches.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.28.1">Inclusivity</span></strong><span class="koboSpan" id="kobo.29.1"> involves the deliberate effort to include diverse voices and perspectives in the development process of LLMs, ensuring the technology is accessible and beneficial to a broad spectrum of users. </span><span class="koboSpan" id="kobo.29.2">In practice, it is vital to collaborate with socio-technical subject-matter experts who can guide appropriate actions</span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.30.1"> to promote and </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">preserve inclusion.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.32.1">This set of principles is not comprehensive but may help to form a conceptual foundation for ethical LLM development and adoption with the universal goal of advancing the technology in ways that </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">avoid harm.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">Additionally, various leading</span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.35.1"> authorities have published guidance regarding responsible AI, inclusive of ethical implications. </span><span class="koboSpan" id="kobo.35.2">These</span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.36.1"> include</span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.37.1"> the US </span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.38.1">Department of Commerce’s </span><strong class="bold"><span class="koboSpan" id="kobo.39.1">National Institute of Standards and Technology</span></strong><span class="koboSpan" id="kobo.40.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.41.1">NIST</span></strong><span class="koboSpan" id="kobo.42.1">), Stanford University’s </span><strong class="bold"><span class="koboSpan" id="kobo.43.1">Institute for Human-Centered Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.44.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.45.1">HAI</span></strong><span class="koboSpan" id="kobo.46.1">), and the </span><strong class="bold"><span class="koboSpan" id="kobo.47.1">Distributed AI Research Institute</span></strong><span class="koboSpan" id="kobo.48.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.49.1">DAIR</span></strong><span class="koboSpan" id="kobo.50.1">), to name</span><a id="_idTextAnchor254"/> <span class="No-Break"><span class="koboSpan" id="kobo.51.1">a few.</span></span></p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor255"/><span class="koboSpan" id="kobo.52.1">Investigating and minimizing bias in generative LLMs and generative image models</span></h1>
<p><span class="koboSpan" id="kobo.53.1">Bias in generative AI models, including</span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.54.1"> both LLMs and generative image</span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.55.1"> models, is a complex</span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.56.1"> issue that requires</span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.57.1"> careful investigation and mitigation strategies. </span><span class="koboSpan" id="kobo.57.2">Bias can manifest as unintended stereotypes, inaccuracies, and exclusions in the generated outputs, often stemming from biased datasets and model architectures. </span><span class="koboSpan" id="kobo.57.3">Recognizing and addressing these biases is crucial to creating equitable and trustworthy </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">AI systems.</span></span></p>
<p><span class="koboSpan" id="kobo.59.1">At its core, algorithmic or model bias refers to systematic errors</span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.60.1"> that lead to preferential treatment or unfair outcomes for certain groups. </span><span class="koboSpan" id="kobo.60.2">In generative AI, this can appear as gender, racial, or socioeconomic biases in outputs, often mirroring societal stereotypes. </span><span class="koboSpan" id="kobo.60.3">For example, an LLM may produce content that reinforces these biases, reflecting the historical and societal biases present in its </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">training data.</span></span></p>
<p><span class="koboSpan" id="kobo.62.1">Let us again revisit our hypothetical fashion retailer, StyleSprint. </span><span class="koboSpan" id="kobo.62.2">Consider a situation where StyleSprint experimented with using a multimodal generative LLM model to generate promotional images and captions for its latest sneaker line. </span><span class="koboSpan" id="kobo.62.3">It finds that the model predominantly generates sneakers in urban, graffiti-laden backgrounds, unintentionally drawing an association that relies on stereotypes. </span><span class="koboSpan" id="kobo.62.4">Moreover, the team begins noticing that the captions are also laden with language that perpetuates stereotypes. </span><span class="koboSpan" id="kobo.62.5">This realization prompts a reevaluation of the imagery and text, first with an investigation of how the </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">problem surfaced.</span></span></p>
<p><span class="koboSpan" id="kobo.64.1">Investigating bias involves various</span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.65.1"> techniques, from analyzing the diversity and representativeness of training</span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.66.1"> datasets to implementing testing</span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.67.1"> protocols that specifically</span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.68.1"> look for biased outputs across different demographics and scenarios. </span><span class="koboSpan" id="kobo.68.2">Statistical analysis can reveal disparities in model outcomes, while comparative studies and user feedback can help identify biases in the </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">generated content.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">In this case, let us assume that StyleSprint was using an LLM-provider without the ability to influence its training data or development process. </span><span class="koboSpan" id="kobo.70.2">To mitigate the risk of bias, the team might employ </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.72.1">Post-processing adjustments to diversify the imagery, ensuring a broader representation of backgrounds that resonate with its </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">customer base</span></span></li>
<li><span class="koboSpan" id="kobo.74.1">The institution of a manual review process, enlisting team members to scrutinize and curate AI-generated images and captions before publishing (i.e., “human-in-the-loop”), ensuring that every piece of content aligns with the brand’s commitment to diversity </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">and inclusion</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.76.1">As is true for other kinds of evaluation of generative AI, evaluating bias demands both quantitative and qualitative methods. </span><span class="koboSpan" id="kobo.76.2">Statistical analysis can uncover performance disparities across groups, and comparative studies can detect biases in outputs. </span><span class="koboSpan" id="kobo.76.3">Gathering feedback from diverse users</span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.77.1"> aids the understanding of real-world bias impacts, while independent audits</span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.78.1"> and research are essential for identifying</span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.79.1"> issues that internal evaluations</span><a id="_idIndexMarker597"/> <span class="No-Break"><span class="koboSpan" id="kobo.80.1">may miss.</span></span></p>
<p><span class="koboSpan" id="kobo.81.1">With a better understanding of how we might investigate and evaluate model outcomes for societal bias, we can explore technical methods for guiding model outcomes toward reliability, equity, and general trustworthiness to curb biased or inequitable outcome</span><a id="_idTextAnchor256"/><span class="koboSpan" id="kobo.82.1">s </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">during inference.</span></span></p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor257"/><span class="koboSpan" id="kobo.84.1">Constrained generation and eliciting trustworthy outcomes</span></h1>
<p><span class="koboSpan" id="kobo.85.1">In practice, it is possible to constrain model generation and guide outcomes toward factuality and equitable outcomes. </span><span class="koboSpan" id="kobo.85.2">As discussed, guiding models toward trustworthy outcomes can be done through continued training and fine-tuning, or during inference. </span><span class="koboSpan" id="kobo.85.3">For example, methodologies such as </span><strong class="bold"><span class="koboSpan" id="kobo.86.1">reinforcement learning from human feedback</span></strong><span class="koboSpan" id="kobo.87.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.88.1">RLHF</span></strong><span class="koboSpan" id="kobo.89.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.90.1">direct preference optimization</span></strong><span class="koboSpan" id="kobo.91.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.92.1">DPO</span></strong><span class="koboSpan" id="kobo.93.1">) increasingly refine model</span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.94.1"> outputs to align</span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.95.1"> model outcomes with human judgment. </span><span class="koboSpan" id="kobo.95.2">Additionally, as discussed in </span><a href="B21773_07.xhtml#_idTextAnchor225"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.96.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.97.1">, various grounding techniques help to ensure that model outputs reflect verified data, continuously guiding the model toward responsible and accurat</span><a id="_idTextAnchor258"/><span class="koboSpan" id="kobo.98.1">e </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">content generation.</span></span></p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor259"/><span class="koboSpan" id="kobo.100.1">Constrained generation with fine-tuning</span></h2>
<p><span class="koboSpan" id="kobo.101.1">Refinement strategies such as RLHF</span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.102.1"> integrate human judgments</span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.103.1"> into the model training process, steering the AI toward behavior that aligns with ethical and truthful standards. </span><span class="koboSpan" id="kobo.103.2">By incorporating human feedback loops, RLHF ensures that the AI’s outputs meet technical accuracy and </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">societal norms.</span></span></p>
<p><span class="koboSpan" id="kobo.105.1">Similarly, DPO refines model outputs based on explicit human preferences, providing precise control to ensure outcomes adhere to ethical standards and human values. </span><span class="koboSpan" id="kobo.105.2">This technique exemplifies the shift</span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.106.1"> toward more ethically aligned content</span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.107.1"> generation by directly incorporating human values into</span><a id="_idTextAnchor260"/><span class="koboSpan" id="kobo.108.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">optimization process.</span></span></p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor261"/><span class="koboSpan" id="kobo.110.1">Constrained generation through prompt engineering</span></h2>
<p><span class="koboSpan" id="kobo.111.1">As we discovered in </span><a href="B21773_07.xhtml#_idTextAnchor225"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.112.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.113.1">, we can guide model</span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.114.1"> responses by grounding</span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.115.1"> the LLM with factual information. </span><span class="koboSpan" id="kobo.115.2">This can be achieved directly using the context window or retrieval approach (e.g., Retrieval Augmented Generation (RAG)). </span><span class="koboSpan" id="kobo.115.3">Just as we can apply these methods to induce factual responses, we can apply the same technique to guide the model toward equitable and </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">inclusive outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.117.1">For example, consider an online news outlet looking to use an LLM to review article content for grammar and readability. </span><span class="koboSpan" id="kobo.117.2">The model does an excellent job of reviewing and revising its drafts. </span><span class="koboSpan" id="kobo.117.3">However, during peer review, it realizes some of the language is culturally insensitive or lacks inclusivity. </span><span class="koboSpan" id="kobo.117.4">As discussed, qualitative evaluation and human oversight are critical to ensuring that model output aligns with human judgment. </span><span class="koboSpan" id="kobo.117.5">Notwithstanding, the writing team can guide the model toward alignment with company values using a set of general guidelines for inclusive and debiased language. </span><span class="koboSpan" id="kobo.117.6">For example, it could ground the model with excerpts from its internal policy documents or content from its unconscious bias </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">training guides.</span></span></p>
<p><span class="koboSpan" id="kobo.119.1">Employing methodologies such as RLHF and DPO, alongside grounding techniques, ensures that LLMs generate content that is not only factual but also ethically aligned, demonstrating the potential of generative AI to adhere to high standards of truthfulness and inclusivity. </span><span class="koboSpan" id="kobo.119.2">Although we cannot underestimate or deemphasize the importance of human judgment in shaping model outputs, we can apply practical supplemental methods such as grounding to reduce the likelihood of harmful or biased </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">model outputs.</span></span></p>
<p><span class="koboSpan" id="kobo.121.1">In the next section, we’ll explore the risks and ethical dilemmas posed by attempts to circumvent the constraints we have just discussed, highlighting the ongoing challenge of balancing the rapid adoption of generative LLMs with appropriat</span><a id="_idTextAnchor262"/><span class="koboSpan" id="kobo.122.1">e safeguards </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">against misuse.</span></span></p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor263"/><span class="koboSpan" id="kobo.124.1">Understanding jailbreaking and harmful behaviors</span></h1>
<p><span class="koboSpan" id="kobo.125.1">In the context of generative</span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.126.1"> LLMs, the term </span><strong class="bold"><span class="koboSpan" id="kobo.127.1">jailbreaking</span></strong><span class="koboSpan" id="kobo.128.1"> describes techniques and strategies that intend to manipulate models to override any ethical safeguards or content restrictions, thereby enabling the generation of restricted or harmful content. </span><span class="koboSpan" id="kobo.128.2">Jailbreaking exploits models through sophisticated adversarial prompting that can induce unexpected or harmful responses. </span><span class="koboSpan" id="kobo.128.3">For example, an attacker might try to instruct an LLM to explain how to generate explicit content or express discriminatory views. </span><span class="koboSpan" id="kobo.128.4">Understanding this susceptibility is crucial for developers and stakeholders to safeguard applied generative AI against misuse and minimize </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">potential harm.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">These jailbreaking attacks</span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.131.1"> exploit the fact that LLMs are trained to interpret and respond to instructions. </span><span class="koboSpan" id="kobo.131.2">Despite sophisticated efforts to defend against misuse, attackers can take advantage of the complex and expansive knowledge embedded in LLMs to find gaps in their safety precautions. </span><span class="koboSpan" id="kobo.131.3">In particular, models that have been trained on uncurated datasets are the most susceptible, as the universe of possible outputs that the models sample from can include harmful and toxic content. </span><span class="koboSpan" id="kobo.131.4">Moreover, LLMs are multilingual and can accept various encodings as input. </span><span class="koboSpan" id="kobo.131.5">For example, an encoding such as </span><strong class="bold"><span class="koboSpan" id="kobo.132.1">base64</span></strong><span class="koboSpan" id="kobo.133.1">, which can be used to translate</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.134.1"> plain text into binary format, could be applied to obfuscate a harmful instruction. </span><span class="koboSpan" id="kobo.134.2">In this case, safety filters may perform inconsistently, failing to detect some languages or </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">alternative inputs.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">Despite this inherent weakness in LLMs, developers and practitioners can take several practical steps to mitigate jailbreaking risks. </span><span class="koboSpan" id="kobo.136.2">Remember, these cannot be exhaustive as new adversarial techniques are </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">often uncovered:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.138.1">Preprocessing and safety filtering</span></strong><span class="koboSpan" id="kobo.139.1">: Implement robust content</span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.140.1"> filtering to detect and block unsafe semantic patterns across languages and input types. </span><span class="koboSpan" id="kobo.140.2">For example, a firm might apply machine learning techniques to analyze prompts for adversarial patterns and block suspicious inputs before passing them to </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">the LLM.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Postprocessing and output screening</span></strong><span class="koboSpan" id="kobo.143.1">: Apply a specialized classifier or other sophisticated technique to screen LLM outputs for inappropriate content before </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">returning them.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.145.1">Safety-focused fine-tuning</span></strong><span class="koboSpan" id="kobo.146.1">: Provide additional safety-focused fine-tuning to the LLM to reinforce and expand its safety knowledge. </span><span class="koboSpan" id="kobo.146.2">Focus on known </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">jailbreaking tactics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.148.1">Monitoring and iterating</span></strong><span class="koboSpan" id="kobo.149.1">: Actively monitor for jailbreaking or policy violation attempts in production, analyze them to identify gaps, and continually update defense measures</span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.150.1"> to stay ahead of </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">creative attackers.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.152.1">While eliminating all possible jailbreaking attempts is infeasible, a multi-layered defense and operational best practices can significantly mitigate </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">the risk.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">In the next section, we will apply a real-time defense mechanism for jailbreaking, all while reducing the likelih</span><a id="_idTextAnchor264"/><span class="koboSpan" id="kobo.155.1">ood of biased and </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">harmful output.</span></span></p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor265"/><span class="koboSpan" id="kobo.157.1">Practice project: Minimizing harmful behaviors with filtering</span></h1>
<p><span class="koboSpan" id="kobo.158.1">For this project, we will use response</span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.159.1"> filtering to try to minimize misuse</span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.160.1"> and curb unwanted LLM output. </span><span class="koboSpan" id="kobo.160.2">Again, we’ll consider our hypothetical business, StyleSprint. </span><span class="koboSpan" id="kobo.160.3">After successfully using an LLM to generate product descriptions and fine-tuning it to answer FAQs, StyleSprint now wants to attempt to use a general-purpose LLM (without fine-tuning) to refine its website search. </span><span class="koboSpan" id="kobo.160.4">However, giving its customers direct access to the LLM poses the risk of misuse. </span><span class="koboSpan" id="kobo.160.5">Bad actors may attempt to use the LLM search to produce harmful content with the intention of harming StyleSprint’s reputation. </span><span class="koboSpan" id="kobo.160.6">To prevent this behavior, we can revisit our RAG implementation from </span><a href="B21773_07.xhtml#_idTextAnchor225"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.161.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.162.1">, applying a filter that evaluates whether queries deviate from the </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">appropriate use.</span></span></p>
<p><span class="koboSpan" id="kobo.164.1">Reusing our previous implementation from the last chapter (found in the GitHub repository: </span><a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python"><span class="koboSpan" id="kobo.165.1">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</span></a><span class="koboSpan" id="kobo.166.1">), which applied RAG to answer specific product-related questions, we can evaluate how the model would respond to questions outside the desired scope. </span><span class="koboSpan" id="kobo.166.2">Recall that RAG is simply a vector search engine combined with an LLM to produce coherent and more precise responses, contextualized by a specific data source. </span><span class="koboSpan" id="kobo.166.3">We will directly reuse that implementation and the same product data for simplicity, but this time, we’ll input a completely unrelated query instead of asking </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">about products:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.168.1">
# random query
response = query_engine.query("describe a giraffe")
print(response) 
=&gt; A giraffe is a tall mammal with a long neck, distinctive spotted coat, and long legs. </span><span class="koboSpan" id="kobo.168.2">They are known for their unique appearance and are the tallest land animals in the world.</span></pre>
<p><span class="koboSpan" id="kobo.169.1">As we can see, the model did not attempt to constrain its answer to the contents of the search index. </span><span class="koboSpan" id="kobo.169.2">It returned an answer based on its vast training. </span><span class="koboSpan" id="kobo.169.3">This is precisely the behavior we want to avoid. </span><span class="koboSpan" id="kobo.169.4">Imagine that a bad actor induced the model to produce explicit content or some other unwanted output. </span><span class="koboSpan" id="kobo.169.5">Moreover, consider a sophisticated attacker that could induce the model to leak training data or expose sensitive information accidentally memorized during training procedures (Carlini et al., 2018; Hu et al., 2022). </span><span class="koboSpan" id="kobo.169.6">In either case, StyleSprint could face material risk </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">and exposure.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">To prevent this, we can leverage</span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.172.1"> a filter to constrain the output</span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.173.1"> to provide answers relevant to a given question explicitly. </span><span class="koboSpan" id="kobo.173.2">The implementation is already built into the LlamaIndex RAG interface. </span><span class="koboSpan" id="kobo.173.3">It is a feature they call Structured </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">Answer Filtering:</span></span></p>
<p class="author-quote"><span class="koboSpan" id="kobo.175.1">With structured_answer_filtering set to True, our refine module is able to filter out any input nodes that are not relevant to the question being asked. </span><span class="koboSpan" id="kobo.175.2">This is particularly useful for RAG-based Q&amp;A systems that involve retrieving chunks of text from external vector store for a given user query. </span><span class="koboSpan" id="kobo.175.3">(LlamaIndex)</span></p>
<p><span class="koboSpan" id="kobo.176.1">In short, this functionality gives us fine-grained control to restrict the context we provide to the LLM for synthesis, ensuring that only the most relevant results are included. </span><span class="koboSpan" id="kobo.176.2">Filtering out irrelevant content before synthesizing responses ensures that only information related to the user’s question is used. </span><span class="koboSpan" id="kobo.176.3">This approach helps avoid answers that are off-topic or outside the intended subject matter. </span><span class="koboSpan" id="kobo.176.4">We can quickly reimplement our RAG approach, applying minor changes that enable </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">the feature.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.178.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.179.1">This functionality is most reliable when using an LLM that can support </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">function calling.</span></span></p>
<p><span class="koboSpan" id="kobo.181.1">Let’s see how this functionality can </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">be implemented.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.183.1">
from llama_index.core import get_response_synthesizer
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
# Configure retriever
retriever = VectorIndexRetriever(index=index,similarity_top_k=1)
# Configure response synthesizer
response_synthesizer = get_response_synthesizer(
    structured_answer_filtering=True,
    response_mode="refine"
)
# Assemble query engine
safe_query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer
)
# Execute query and evaluate response
print(safe_query_engine.query("describe a summer dress with price"))
# =&gt; A lightweight summer dress with a vibrant floral print, perfect for sunny days, priced at 59.99.
</span><span class="koboSpan" id="kobo.183.2">print(safe_query_engine.query("describe a horse"))
# =&gt; Empty Response</span></pre>
<p><span class="koboSpan" id="kobo.184.1">Using this approach, the model returns</span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.185.1"> a response to the standard question</span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.186.1"> but no response to the irrelevant question. </span><span class="koboSpan" id="kobo.186.2">In fact, we can take this further and compound this filtering with additional instructions in the prompt template. </span><span class="koboSpan" id="kobo.186.3">For example, if we revise </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">response_synthesizer</span></strong><span class="koboSpan" id="kobo.188.1">, we can promote a stricter response from </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">the LLM:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.190.1">
QA_PROMPT_TMPL = (
    "Context information is below.\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "Given only the context information and no prior knowledge, "
    "answer the query.\n"
    "Query: {query_str}\n"
    "Answer: "
    "Otherwise, state: I cannot answer."
</span><span class="koboSpan" id="kobo.190.2">)
STRICT_QA_PROMPT = PromptTemplate(
    QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER
)
# Configure response synthesizer
response_synthesizer = get_response_synthesizer(
    structured_answer_filtering=True,
    response_mode="refine",
    text_qa_template=STRICT_QA_PROMPT
)</span></pre>
<p><span class="koboSpan" id="kobo.191.1">This time, the model responded explicitly, </span><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">I cannot answer</span></strong><span class="koboSpan" id="kobo.193.1">. </span><span class="koboSpan" id="kobo.193.2">Using a prompt template, StyleSprint could return a message it deems appropriate in response to inputs unrelated to the search index and, as a side effect, ignore queries that do not adhere to its policies. </span><span class="koboSpan" id="kobo.193.3">Although not entirely a perfect solution, combining RAG with more strict answer filtering can help deter or defend against harmful instructions or adversarial prompting. </span><span class="koboSpan" id="kobo.193.4">Additionally, as explored in </span><a href="B21773_07.xhtml#_idTextAnchor225"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.194.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.195.1">, we can apply RAG-specific evaluation techniques such as RAGAS</span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.196.1"> to measure f</span><a id="_idTextAnchor266"/><span class="koboSpan" id="kobo.197.1">actual consistency</span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.198.1"> and </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">answer relevancy.</span></span></p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor267"/><span class="koboSpan" id="kobo.200.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.201.1">In this section, we recognized the increasing prominence of generative AI and explored the ethical considerations that should steer its progress. </span><span class="koboSpan" id="kobo.201.2">We outlined key concepts such as transparency, fairness, accountability, respect for privacy, informed consent, security, and inclusivity, which are essential to the responsible development and use of </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">these technologies.</span></span></p>
<p><span class="koboSpan" id="kobo.203.1">We reviewed strategies to attempt to counter these biases, including human-aligned training techniques and practical application-level measures against susceptibilities such as jailbreaking. </span><span class="koboSpan" id="kobo.203.2">In sum, we explored a multidimensional and human-centered approach to generative </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">AI adoption.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">Having completed our foundational exploration of generative AI, we can now reflect on our journey. </span><span class="koboSpan" id="kobo.205.2">We began by laying the groundwork, examining foundational generative architectures such as generative adversarial networks (GANs), diffusion models, </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">and transformers.</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.207.1">Chapters 2</span></em><span class="koboSpan" id="kobo.208.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.209.1">3</span></em><span class="koboSpan" id="kobo.210.1"> guided us through the evolution of language models, with a particular focus on autoregressive transformers. </span><span class="koboSpan" id="kobo.210.2">We explored how these models have significantly advanced the capabilities of generative AI, pushing the boundaries of machine understanding and the generation of </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">human-like language.</span></span></p>
<p><a href="B21773_04.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.212.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.213.1"> provided us with practical experience in production-ready environments. </span><span class="koboSpan" id="kobo.213.2">In </span><a href="B21773_05.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.214.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.215.1">, we explored the fine-tuning of LLMs for specific tasks, a technique that enhances their performance and adaptability to specific applications. </span><a href="B21773_06.xhtml#_idTextAnchor211"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.216.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.217.1"> focused on the concept of domain adaptation, demonstrating how tailoring AI models to understand domain-specific nuances can greatly improve their utility in specialized fields such as finance, law, </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">and healthcare.</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.219.1">Chapters 7</span></em><span class="koboSpan" id="kobo.220.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.221.1">8</span></em><span class="koboSpan" id="kobo.222.1"> centered on prompt engineering and constrained generation, addressing techniques to ensure that AI-generated content remains trustworthy and aligned with </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">ethical guidelines.</span></span></p>
<p><span class="koboSpan" id="kobo.224.1">This book has aimed to provide a solid foundation in generative AI, preparing professionals across disciplines and sectors with the necessary theoretical knowledge and practical skills to effectively engage with this transformative technology. </span><span class="koboSpan" id="kobo.224.2">The potential of generative AI is significant, and with our deeper understanding of its technologies, coupled with a thoughtful approach to ethical and societal considerations, we are read</span><a id="_idTextAnchor268"/><span class="koboSpan" id="kobo.225.1">y</span><a id="_idTextAnchor269"/><span class="koboSpan" id="kobo.226.1"> to responsibly leverage </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">its advantages.</span></span></p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor270"/><span class="koboSpan" id="kobo.228.1">References</span></h1>
<p><span class="koboSpan" id="kobo.229.1">This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">subject matter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.231.1">Sun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y., Lyu, W., Zhang, Y., Li, X., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Kailkhura, B., Xiong, C., Xiao, C., Li, C., Xing, E., . </span><span class="koboSpan" id="kobo.231.2">. </span><span class="koboSpan" id="kobo.231.3">. </span><span class="koboSpan" id="kobo.231.4">Zhao, Y. </span><span class="koboSpan" id="kobo.231.5">(2024). </span><em class="italic"><span class="koboSpan" id="kobo.232.1">TrustLLM: Trustworthiness in Large Language Models</span></em><span class="koboSpan" id="kobo.233.1">. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.234.1">ArXiv</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">. </span><span class="koboSpan" id="kobo.235.2">/abs/2401.05561</span></span></li>
<li><span class="koboSpan" id="kobo.236.1">Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., &amp; Song, D. </span><span class="koboSpan" id="kobo.236.2">(2018). </span><em class="italic"><span class="koboSpan" id="kobo.237.1">The secret sharer: Evaluating and testing unintended memorization in neural networks</span></em><span class="koboSpan" id="kobo.238.1">. </span><span class="koboSpan" id="kobo.238.2">In arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">cs.LG]. </span></span><a href="http://arxiv.org/abs/1802.08232"><span class="No-Break"><span class="koboSpan" id="kobo.240.1">http://arxiv.org/abs/1802.08232</span></span></a></li>
<li><span class="koboSpan" id="kobo.241.1">Hu, H., Salcic, Z., Sun, L., Dobbie, G., Yu, P. </span><span class="koboSpan" id="kobo.241.2">S., &amp; Zhang, X. </span><span class="koboSpan" id="kobo.241.3">(2022). </span><em class="italic"><span class="koboSpan" id="kobo.242.1">Membership inference attacks on machine learning: A survey. </span><span class="koboSpan" id="kobo.242.2">ACM Computing Surveys</span></em><span class="koboSpan" id="kobo.243.1">, 54(11s), </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">1–37. </span></span><a href="https://doi.org/10.1145/3523273"><span class="No-Break"><span class="koboSpan" id="kobo.245.1">https://doi.org/10.1145/3523273</span></span></a></li>
<li><span class="koboSpan" id="kobo.246.1">LlamaIndex. </span><span class="koboSpan" id="kobo.246.2">(n.d.). </span><em class="italic"><span class="koboSpan" id="kobo.247.1">Response synthesizers. </span><span class="koboSpan" id="kobo.247.2">In LlamaIndex Documentation (stable version)</span></em><span class="koboSpan" id="kobo.248.1">. </span><span class="koboSpan" id="kobo.248.2">Retrieved March 12, </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">2024. </span></span><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html"><span class="No-Break"><span class="koboSpan" id="kobo.250.1">https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html</span></span></a></li>
</ul>
</div>
</body></html>