<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-144"><a id="_idTextAnchor251"/>8</h1>
<h1 id="_idParaDest-145"><a id="_idTextAnchor252"/>Addressing Ethical Considerations and Charting a Path Toward Trustworthy Generative AI</h1>
<p>As generative AI advances, it will extend beyond basic language tasks, integrating into daily life and impacting almost every sector. The inevitability of its widespread adoption highlights the need to address its ethical implications. The promise of this technology to revolutionize industries, enhance creativity, and solve complex problems must be coupled with the responsibility to navigate its ethical landscape diligently. This chapter will explore these ethical considerations, dissect the intricacies of biases entangled in these models, and look at strategies for cultivating trust in general-purpose AI systems. Through thorough examination and reflection, we can begin to outline a path toward responsible use, helping to ensure that advancements in generative AI are leveraged for the greater good while minimizing harm.</p>
<p>To ground our discussion, we will first identify some ethical norms and universal values relevant to generative AI. While this chapter cannot be exhaustive, it aims to introduce key ethical considerations.</p>
<h1 id="_idParaDest-146"><a id="_idTextAnchor253"/>Ethical norms and values in the context of generative AI</h1>
<p>The ethical norms and values guiding<a id="_idIndexMarker577"/> the development and deployment of generative AI are rooted in transparency, equity, accountability, privacy, consent, security, and inclusivity. These principles can serve as a foundation for developing and adopting systems aligned with societal values and supporting<a id="_idIndexMarker578"/> the greater good. Let’s explore these in detail:</p>
<ul>
<li><strong class="bold">Transparency</strong> involves clearly explaining<a id="_idIndexMarker579"/> the methodologies, data sources, and processes behind large language model (LLM) construction. This practice builds trust by enabling stakeholders to understand the technology’s reliability and limits. For example, a company could publish a detailed report on the types of data trained on their LLM and the steps taken to ensure data privacy and bias mitigation.</li>
<li><strong class="bold">Equity</strong> in the context of LLMs ensures fair treatment and outcomes for all users by actively preventing biases in models. This requires thorough analysis and correction of training data and continuous monitoring of exchanges to reduce discrimination. One measure a firm might apply is a routine review of LLM performance across various demographic groups to identify and address unintended biases.</li>
<li><strong class="bold">Accountability</strong> establishes that developers and users of LLMs are responsible for model outputs and impacts. It includes transparent and accessible mechanisms for reporting and addressing negative consequences or ethical violations. In practice, this could manifest as the establishment of an independent review board that oversees AI projects and intervenes in cases of ethical misconduct.</li>
<li><strong class="bold">Privacy and consent</strong>, in principle, involves ensuring that individual privacy and consent are respected and preserved during the use of personal data as input to LLMs. In practice, developers should avoid using personal data for training without explicit permission and implement strong data protection measures. For example, a developer might use data anonymization or privacy-preserving techniques to train models, ensuring that personal identifiers and sensitive information are removed before data processing.</li>
<li><strong class="bold">Security</strong> involves protecting LLM-integrated systems and their data from unauthorized access and cyber threats. In practice, setting up LLM-specific red teams (or teams that test defenses by simulating attacks) can help safeguard AI systems against potential breaches.</li>
<li><strong class="bold">Inclusivity</strong> involves the deliberate effort to include diverse voices and perspectives in the development process of LLMs, ensuring the technology is accessible and beneficial to a broad spectrum of users. In practice, it is vital to collaborate with socio-technical subject-matter experts who can guide appropriate actions<a id="_idIndexMarker580"/> to promote and preserve inclusion.</li>
</ul>
<p>This set of principles is not comprehensive but may help to form a conceptual foundation for ethical LLM development and adoption with the universal goal of advancing the technology in ways that avoid harm.</p>
<p>Additionally, various leading<a id="_idIndexMarker581"/> authorities have published guidance regarding responsible AI, inclusive of ethical implications. These<a id="_idIndexMarker582"/> include<a id="_idIndexMarker583"/> the US <a id="_idIndexMarker584"/>Department of Commerce’s <strong class="bold">National Institute of Standards and Technology</strong> (<strong class="bold">NIST</strong>), Stanford University’s <strong class="bold">Institute for Human-Centered Artificial Intelligence</strong> (<strong class="bold">HAI</strong>), and the <strong class="bold">Distributed AI Research Institute</strong> (<strong class="bold">DAIR</strong>), to name<a id="_idTextAnchor254"/> a few.</p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor255"/>Investigating and minimizing bias in generative LLMs and generative image models</h1>
<p>Bias in generative AI models, including<a id="_idIndexMarker585"/> both LLMs and generative image<a id="_idIndexMarker586"/> models, is a complex<a id="_idIndexMarker587"/> issue that requires<a id="_idIndexMarker588"/> careful investigation and mitigation strategies. Bias can manifest as unintended stereotypes, inaccuracies, and exclusions in the generated outputs, often stemming from biased datasets and model architectures. Recognizing and addressing these biases is crucial to creating equitable and trustworthy AI systems.</p>
<p>At its core, algorithmic or model bias refers to systematic errors<a id="_idIndexMarker589"/> that lead to preferential treatment or unfair outcomes for certain groups. In generative AI, this can appear as gender, racial, or socioeconomic biases in outputs, often mirroring societal stereotypes. For example, an LLM may produce content that reinforces these biases, reflecting the historical and societal biases present in its training data.</p>
<p>Let us again revisit our hypothetical fashion retailer, StyleSprint. Consider a situation where StyleSprint experimented with using a multimodal generative LLM model to generate promotional images and captions for its latest sneaker line. It finds that the model predominantly generates sneakers in urban, graffiti-laden backgrounds, unintentionally drawing an association that relies on stereotypes. Moreover, the team begins noticing that the captions are also laden with language that perpetuates stereotypes. This realization prompts a reevaluation of the imagery and text, first with an investigation of how the problem surfaced.</p>
<p>Investigating bias involves various<a id="_idIndexMarker590"/> techniques, from analyzing the diversity and representativeness of training<a id="_idIndexMarker591"/> datasets to implementing testing<a id="_idIndexMarker592"/> protocols that specifically<a id="_idIndexMarker593"/> look for biased outputs across different demographics and scenarios. Statistical analysis can reveal disparities in model outcomes, while comparative studies and user feedback can help identify biases in the generated content.</p>
<p>In this case, let us assume that StyleSprint was using an LLM-provider without the ability to influence its training data or development process. To mitigate the risk of bias, the team might employ the following:</p>
<ul>
<li>Post-processing adjustments to diversify the imagery, ensuring a broader representation of backgrounds that resonate with its customer base</li>
<li>The institution of a manual review process, enlisting team members to scrutinize and curate AI-generated images and captions before publishing (i.e., “human-in-the-loop”), ensuring that every piece of content aligns with the brand’s commitment to diversity and inclusion</li>
</ul>
<p>As is true for other kinds of evaluation of generative AI, evaluating bias demands both quantitative and qualitative methods. Statistical analysis can uncover performance disparities across groups, and comparative studies can detect biases in outputs. Gathering feedback from diverse users<a id="_idIndexMarker594"/> aids the understanding of real-world bias impacts, while independent audits<a id="_idIndexMarker595"/> and research are essential for identifying<a id="_idIndexMarker596"/> issues that internal evaluations<a id="_idIndexMarker597"/> may miss.</p>
<p>With a better understanding of how we might investigate and evaluate model outcomes for societal bias, we can explore technical methods for guiding model outcomes toward reliability, equity, and general trustworthiness to curb biased or inequitable outcome<a id="_idTextAnchor256"/>s during inference.</p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor257"/>Constrained generation and eliciting trustworthy outcomes</h1>
<p>In practice, it is possible to constrain model generation and guide outcomes toward factuality and equitable outcomes. As discussed, guiding models toward trustworthy outcomes can be done through continued training and fine-tuning, or during inference. For example, methodologies such as <strong class="bold">reinforcement learning from human feedback</strong> (<strong class="bold">RLHF</strong>) and <strong class="bold">direct preference optimization</strong> (<strong class="bold">DPO</strong>) increasingly refine model<a id="_idIndexMarker598"/> outputs to align<a id="_idIndexMarker599"/> model outcomes with human judgment. Additionally, as discussed in <a href="B21773_07.xhtml#_idTextAnchor225"><em class="italic">Chapter 7</em></a>, various grounding techniques help to ensure that model outputs reflect verified data, continuously guiding the model toward responsible and accurat<a id="_idTextAnchor258"/>e content generation.</p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor259"/>Constrained generation with fine-tuning</h2>
<p>Refinement strategies such as RLHF<a id="_idIndexMarker600"/> integrate human judgments<a id="_idIndexMarker601"/> into the model training process, steering the AI toward behavior that aligns with ethical and truthful standards. By incorporating human feedback loops, RLHF ensures that the AI’s outputs meet technical accuracy and societal norms.</p>
<p>Similarly, DPO refines model outputs based on explicit human preferences, providing precise control to ensure outcomes adhere to ethical standards and human values. This technique exemplifies the shift<a id="_idIndexMarker602"/> toward more ethically aligned content<a id="_idIndexMarker603"/> generation by directly incorporating human values into<a id="_idTextAnchor260"/> the optimization process.</p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor261"/>Constrained generation through prompt engineering</h2>
<p>As we discovered in <a href="B21773_07.xhtml#_idTextAnchor225"><em class="italic">Chapter 7</em></a>, we can guide model<a id="_idIndexMarker604"/> responses by grounding<a id="_idIndexMarker605"/> the LLM with factual information. This can be achieved directly using the context window or retrieval approach (e.g., Retrieval Augmented Generation (RAG)). Just as we can apply these methods to induce factual responses, we can apply the same technique to guide the model toward equitable and inclusive outcomes.</p>
<p>For example, consider an online news outlet looking to use an LLM to review article content for grammar and readability. The model does an excellent job of reviewing and revising its drafts. However, during peer review, it realizes some of the language is culturally insensitive or lacks inclusivity. As discussed, qualitative evaluation and human oversight are critical to ensuring that model output aligns with human judgment. Notwithstanding, the writing team can guide the model toward alignment with company values using a set of general guidelines for inclusive and debiased language. For example, it could ground the model with excerpts from its internal policy documents or content from its unconscious bias training guides.</p>
<p>Employing methodologies such as RLHF and DPO, alongside grounding techniques, ensures that LLMs generate content that is not only factual but also ethically aligned, demonstrating the potential of generative AI to adhere to high standards of truthfulness and inclusivity. Although we cannot underestimate or deemphasize the importance of human judgment in shaping model outputs, we can apply practical supplemental methods such as grounding to reduce the likelihood of harmful or biased model outputs.</p>
<p>In the next section, we’ll explore the risks and ethical dilemmas posed by attempts to circumvent the constraints we have just discussed, highlighting the ongoing challenge of balancing the rapid adoption of generative LLMs with appropriat<a id="_idTextAnchor262"/>e safeguards against misuse.</p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor263"/>Understanding jailbreaking and harmful behaviors</h1>
<p>In the context of generative<a id="_idIndexMarker606"/> LLMs, the term <strong class="bold">jailbreaking</strong> describes techniques and strategies that intend to manipulate models to override any ethical safeguards or content restrictions, thereby enabling the generation of restricted or harmful content. Jailbreaking exploits models through sophisticated adversarial prompting that can induce unexpected or harmful responses. For example, an attacker might try to instruct an LLM to explain how to generate explicit content or express discriminatory views. Understanding this susceptibility is crucial for developers and stakeholders to safeguard applied generative AI against misuse and minimize potential harm.</p>
<p>These jailbreaking attacks<a id="_idIndexMarker607"/> exploit the fact that LLMs are trained to interpret and respond to instructions. Despite sophisticated efforts to defend against misuse, attackers can take advantage of the complex and expansive knowledge embedded in LLMs to find gaps in their safety precautions. In particular, models that have been trained on uncurated datasets are the most susceptible, as the universe of possible outputs that the models sample from can include harmful and toxic content. Moreover, LLMs are multilingual and can accept various encodings as input. For example, an encoding such as <strong class="bold">base64</strong>, which can be used to translate<a id="_idIndexMarker608"/> plain text into binary format, could be applied to obfuscate a harmful instruction. In this case, safety filters may perform inconsistently, failing to detect some languages or alternative inputs.</p>
<p>Despite this inherent weakness in LLMs, developers and practitioners can take several practical steps to mitigate jailbreaking risks. Remember, these cannot be exhaustive as new adversarial techniques are often uncovered:</p>
<ul>
<li><strong class="bold">Preprocessing and safety filtering</strong>: Implement robust content<a id="_idIndexMarker609"/> filtering to detect and block unsafe semantic patterns across languages and input types. For example, a firm might apply machine learning techniques to analyze prompts for adversarial patterns and block suspicious inputs before passing them to the LLM.</li>
<li><strong class="bold">Postprocessing and output screening</strong>: Apply a specialized classifier or other sophisticated technique to screen LLM outputs for inappropriate content before returning them.</li>
<li><strong class="bold">Safety-focused fine-tuning</strong>: Provide additional safety-focused fine-tuning to the LLM to reinforce and expand its safety knowledge. Focus on known jailbreaking tactics.</li>
<li><strong class="bold">Monitoring and iterating</strong>: Actively monitor for jailbreaking or policy violation attempts in production, analyze them to identify gaps, and continually update defense measures<a id="_idIndexMarker610"/> to stay ahead of creative attackers.</li>
</ul>
<p>While eliminating all possible jailbreaking attempts is infeasible, a multi-layered defense and operational best practices can significantly mitigate the risk.</p>
<p>In the next section, we will apply a real-time defense mechanism for jailbreaking, all while reducing the likelih<a id="_idTextAnchor264"/>ood of biased and harmful output.</p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor265"/>Practice project: Minimizing harmful behaviors with filtering</h1>
<p>For this project, we will use response<a id="_idIndexMarker611"/> filtering to try to minimize misuse<a id="_idIndexMarker612"/> and curb unwanted LLM output. Again, we’ll consider our hypothetical business, StyleSprint. After successfully using an LLM to generate product descriptions and fine-tuning it to answer FAQs, StyleSprint now wants to attempt to use a general-purpose LLM (without fine-tuning) to refine its website search. However, giving its customers direct access to the LLM poses the risk of misuse. Bad actors may attempt to use the LLM search to produce harmful content with the intention of harming StyleSprint’s reputation. To prevent this behavior, we can revisit our RAG implementation from <a href="B21773_07.xhtml#_idTextAnchor225"><em class="italic">Chapter 7</em></a>, applying a filter that evaluates whether queries deviate from the appropriate use.</p>
<p>Reusing our previous implementation from the last chapter (found in the GitHub repository: <a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</a>), which applied RAG to answer specific product-related questions, we can evaluate how the model would respond to questions outside the desired scope. Recall that RAG is simply a vector search engine combined with an LLM to produce coherent and more precise responses, contextualized by a specific data source. We will directly reuse that implementation and the same product data for simplicity, but this time, we’ll input a completely unrelated query instead of asking about products:</p>
<pre class="source-code">
# random query
response = query_engine.query("describe a giraffe")
print(response) 
=&gt; A giraffe is a tall mammal with a long neck, distinctive spotted coat, and long legs. They are known for their unique appearance and are the tallest land animals in the world.</pre>
<p>As we can see, the model did not attempt to constrain its answer to the contents of the search index. It returned an answer based on its vast training. This is precisely the behavior we want to avoid. Imagine that a bad actor induced the model to produce explicit content or some other unwanted output. Moreover, consider a sophisticated attacker that could induce the model to leak training data or expose sensitive information accidentally memorized during training procedures (Carlini et al., 2018; Hu et al., 2022). In either case, StyleSprint could face material risk and exposure.</p>
<p>To prevent this, we can leverage<a id="_idIndexMarker613"/> a filter to constrain the output<a id="_idIndexMarker614"/> to provide answers relevant to a given question explicitly. The implementation is already built into the LlamaIndex RAG interface. It is a feature they call Structured Answer Filtering:</p>
<p class="author-quote">With structured_answer_filtering set to True, our refine module is able to filter out any input nodes that are not relevant to the question being asked. This is particularly useful for RAG-based Q&amp;A systems that involve retrieving chunks of text from external vector store for a given user query. (LlamaIndex)</p>
<p>In short, this functionality gives us fine-grained control to restrict the context we provide to the LLM for synthesis, ensuring that only the most relevant results are included. Filtering out irrelevant content before synthesizing responses ensures that only information related to the user’s question is used. This approach helps avoid answers that are off-topic or outside the intended subject matter. We can quickly reimplement our RAG approach, applying minor changes that enable the feature.</p>
<p class="callout-heading">Note</p>
<p class="callout">This functionality is most reliable when using an LLM that can support function calling.</p>
<p>Let’s see how this functionality can be implemented.</p>
<pre class="source-code">
from llama_index.core import get_response_synthesizer
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
# Configure retriever
retriever = VectorIndexRetriever(index=index,similarity_top_k=1)
# Configure response synthesizer
response_synthesizer = get_response_synthesizer(
    structured_answer_filtering=True,
    response_mode="refine"
)
# Assemble query engine
safe_query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer
)
# Execute query and evaluate response
print(safe_query_engine.query("describe a summer dress with price"))
# =&gt; A lightweight summer dress with a vibrant floral print, perfect for sunny days, priced at 59.99.
print(safe_query_engine.query("describe a horse"))
# =&gt; Empty Response</pre>
<p>Using this approach, the model returns<a id="_idIndexMarker615"/> a response to the standard question<a id="_idIndexMarker616"/> but no response to the irrelevant question. In fact, we can take this further and compound this filtering with additional instructions in the prompt template. For example, if we revise <code>response_synthesizer</code>, we can promote a stricter response from the LLM:</p>
<pre class="source-code">
QA_PROMPT_TMPL = (
    "Context information is below.\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "Given only the context information and no prior knowledge, "
    "answer the query.\n"
    "Query: {query_str}\n"
    "Answer: "
    "Otherwise, state: I cannot answer."
)
STRICT_QA_PROMPT = PromptTemplate(
    QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER
)
# Configure response synthesizer
response_synthesizer = get_response_synthesizer(
    structured_answer_filtering=True,
    response_mode="refine",
    text_qa_template=STRICT_QA_PROMPT
)</pre>
<p>This time, the model responded explicitly, <code>I cannot answer</code>. Using a prompt template, StyleSprint could return a message it deems appropriate in response to inputs unrelated to the search index and, as a side effect, ignore queries that do not adhere to its policies. Although not entirely a perfect solution, combining RAG with more strict answer filtering can help deter or defend against harmful instructions or adversarial prompting. Additionally, as explored in <a href="B21773_07.xhtml#_idTextAnchor225"><em class="italic">Chapter 7</em></a>, we can apply RAG-specific evaluation techniques such as RAGAS<a id="_idIndexMarker617"/> to measure f<a id="_idTextAnchor266"/>actual consistency<a id="_idIndexMarker618"/> and answer relevancy.</p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor267"/>Summary</h1>
<p>In this section, we recognized the increasing prominence of generative AI and explored the ethical considerations that should steer its progress. We outlined key concepts such as transparency, fairness, accountability, respect for privacy, informed consent, security, and inclusivity, which are essential to the responsible development and use of these technologies.</p>
<p>We reviewed strategies to attempt to counter these biases, including human-aligned training techniques and practical application-level measures against susceptibilities such as jailbreaking. In sum, we explored a multidimensional and human-centered approach to generative AI adoption.</p>
<p>Having completed our foundational exploration of generative AI, we can now reflect on our journey. We began by laying the groundwork, examining foundational generative architectures such as generative adversarial networks (GANs), diffusion models, and transformers.</p>
<p><em class="italic">Chapters 2</em> and <em class="italic">3</em> guided us through the evolution of language models, with a particular focus on autoregressive transformers. We explored how these models have significantly advanced the capabilities of generative AI, pushing the boundaries of machine understanding and the generation of human-like language.</p>
<p><a href="B21773_04.xhtml#_idTextAnchor123"><em class="italic">Chapter 4</em></a> provided us with practical experience in production-ready environments. In <a href="B21773_05.xhtml#_idTextAnchor180"><em class="italic">Chapter 5</em></a>, we explored the fine-tuning of LLMs for specific tasks, a technique that enhances their performance and adaptability to specific applications. <a href="B21773_06.xhtml#_idTextAnchor211"><em class="italic">Chapter 6</em></a> focused on the concept of domain adaptation, demonstrating how tailoring AI models to understand domain-specific nuances can greatly improve their utility in specialized fields such as finance, law, and healthcare.</p>
<p><em class="italic">Chapters 7</em> and <em class="italic">8</em> centered on prompt engineering and constrained generation, addressing techniques to ensure that AI-generated content remains trustworthy and aligned with ethical guidelines.</p>
<p>This book has aimed to provide a solid foundation in generative AI, preparing professionals across disciplines and sectors with the necessary theoretical knowledge and practical skills to effectively engage with this transformative technology. The potential of generative AI is significant, and with our deeper understanding of its technologies, coupled with a thoughtful approach to ethical and societal considerations, we are read<a id="_idTextAnchor268"/>y<a id="_idTextAnchor269"/> to responsibly leverage its advantages.</p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor270"/>References</h1>
<p>This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the subject matter:</p>
<ul>
<li>Sun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y., Lyu, W., Zhang, Y., Li, X., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Kailkhura, B., Xiong, C., Xiao, C., Li, C., Xing, E., . . . Zhao, Y. (2024). <em class="italic">TrustLLM: Trustworthiness in Large Language Models</em>. <em class="italic">ArXiv</em>. /abs/2401.05561</li>
<li>Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., &amp; Song, D. (2018). <em class="italic">The secret sharer: Evaluating and testing unintended memorization in neural networks</em>. In arXiv [cs.LG]. <a href="http://arxiv.org/abs/1802.08232">http://arxiv.org/abs/1802.08232</a></li>
<li>Hu, H., Salcic, Z., Sun, L., Dobbie, G., Yu, P. S., &amp; Zhang, X. (2022). <em class="italic">Membership inference attacks on machine learning: A survey. ACM Computing Surveys</em>, 54(11s), 1–37. <a href="https://doi.org/10.1145/3523273">https://doi.org/10.1145/3523273</a></li>
<li>LlamaIndex. (n.d.). <em class="italic">Response synthesizers. In LlamaIndex Documentation (stable version)</em>. Retrieved March 12, 2024. <a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html">https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html</a></li>
</ul>
</div>
</body></html>