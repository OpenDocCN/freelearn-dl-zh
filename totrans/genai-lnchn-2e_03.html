<html><head></head><body>
<div><div><h1 class="chapterNumber"><a id="_idTextAnchor044"/>2</h1>
<h1 class="chapterTitle" id="_idParaDest-34"><a id="_idTextAnchor045"/>First Steps with LangChain</h1>
<p class="normal">In the previous chapter, we explored LLMs and introduced LangChain as a powerful framework for building LLM-powered applications. We discussed how LLMs have revolutionized natural language processing with their ability to understand context, generate human-like text, and perform complex reasoning. While these capabilities are impressive, we also examined their limitations—hallucinations, context constraints, and lack of up-to-date knowledge.</p>
<p class="normal">In this chapter, we’ll move from theory to practice by building our first LangChain application. We’ll start with the fundamentals: setting up a proper development environment, understanding LangChain’s core components, and creating simple chains. From there, we’ll explore more advanced capabilities, including running local models for privacy and cost efficiency and building multimodal applications that combine text with visual understanding. By the end of this chapter, you’ll have a solid foundation in LangChain’s building blocks and be ready to create increasingly sophisticated AI applications in subsequent chapters.</p>
<p class="normal">To sum up, this chapter will cover the following topics:</p>
<ul>
<li class="b lletList">Setting up dependencies</li>
<li class="b lletList">Exploring LangChain’s building blocks (model interfaces, prompts and templates, and LCEL)</li>
<li class="b lletList">Running local models</li>
<li class="b lletList">Multimodal AI applications</li>
</ul>
<div><div><div><p class="normal">Given the rapid evolution of both LangChain and the broader AI field, we maintain up-to-date code examples and resources in our GitHub repository: <a href="https://github.com/benman1/generative_ai_with_langchain">https://github.com/benman1/generative_ai_with_langchain</a>.</p>
<p class="normal">For questions or troubleshooting help, please create an issue on GitHub or join our Discord community: <a href="https://packt.link/lang">https://packt.link/lang</a>.</p>
</div>
</div>
<h1 class="heading-1" id="_idParaDest-35"><a id="_idTextAnchor046"/><a id="_idTextAnchor047"/>Setting up dependencies for this book</h1>
<p class="normal">This <a id="_idIndexMarker070"/>book provides multiple options for running the code examples, from zero-setup cloud notebooks to local development environments. Choose the approach that best fits your experience level and preferences. Even if you are familiar with dependency management, please read these instructions since all code in this book will depend on the correct installation of the environment as outlined here.</p>
<p class="normal">For the quickest start with no local setup required, we provide ready-to-use online notebooks for every chapter:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Google Colab</strong>: Run <a id="_idIndexMarker071"/>examples with free GPU access</li>
<li class="b lletList"><strong class="keyWord">Kaggle Notebooks</strong>: Experiment <a id="_idIndexMarker072"/>with integrated datasets</li>
<li class="b lletList"><strong class="keyWord">Gradient Notebooks</strong>: Access <a id="_idIndexMarker073"/>higher-performance compute options</li>
</ul>
<p class="normal">All code examples you find in this book are available as online notebooks on GitHub at <a href="https://github.com/benman1/generative_ai_with_langchain">https://github.com/benman1/generative_ai_with_langchain</a>.</p>
<p class="normal">These notebooks don’t have all dependencies pre-configured but, usually, a few install commands get you going. These tools allow you to start experimenting immediately without worrying about setup. If you prefer working locally, we recommend using conda for environment management:</p>
<ol>
<li class="numberedList" value="1">Install <a id="_idIndexMarker074"/>Miniconda if you don’t have it already.</li>
<li class="numberedList">Download it from <a href="https://docs.conda.io/en/latest/miniconda.html">https://docs.conda.io/en/latest/miniconda.html</a>.</li>
<li class="numberedList">Create a new environment with Python 3.11:<pre>conda create -n langchain-book python=3.11</pre></li>
<li class="numberedList">Activate the environment:<pre>conda activate langchain-book</pre></li>
<li class="numberedList">Install Jupyter and core dependencies:<pre>conda install jupyter
pip install langchain langchain-openai jupyter</pre></li>
<li class="numberedList">Launch Jupyter Notebook:<pre>jupyter notebook</pre></li>
</ol>
<div><p class="normal">This approach provides a clean, isolated environment for working with LangChain. For experienced developers with established workflows, we also support:</p>
<ul>
<li class="b lletList"><strong class="keyWord">pip with venv</strong>: Instructions in the GitHub repository</li>
<li class="b lletList"><strong class="keyWord">Docker containers</strong>: Dockerfiles provided in the GitHub repository</li>
<li class="b lletList"><strong class="keyWord">Poetry</strong>: Configuration files available in the GitHub repository</li>
</ul>
<p class="normal">Choose <a id="_idIndexMarker075"/>the method you’re most comfortable with but remember that all examples assume a Python 3.10+ environment with the dependencies listed in requirements.txt.</p>
<p class="normal">For developers, Docker, which <a id="_idIndexMarker076"/>provides isolation via containers, is a good option. The downside is that it uses a lot of disk space and is more complex than the other options. For data scientists, I’d recommend Conda or Poetry.</p>
<p class="normal">Conda<a id="_idIndexMarker077"/> handles intricate dependencies efficiently, although it can be excruciatingly slow in large environments. Poetry<a id="_idIndexMarker078"/> resolves dependencies well and manages environments; however, it doesn’t capture system dependencies.</p>
<p class="normal">All tools allow sharing and replicating dependencies from configuration files. You can find a set of instructions and the corresponding configuration files in the book’s repository at <a href="https://github.com/benman1/generative_ai_with_langchain">https://github.com/benman1/generative_ai_with_langchain</a>.</p>
<p class="normal">Once you are finished, please make sure you have LangChain version 0.3.17 installed. You can check this with the command <code class="inlineCode">pip show langchain</code>.</p>
<div><div><p class="normal">With the rapid pace of innovation in the LLM field, library updates are frequent. The code in this book is tested with LangChain 0.3.17, but newer versions may introduce changes. If you encounter any issues running the examples:</p>
<ul>
<li class="bulletList">Create an issue on our GitHub repository</li>
<li class="bulletList">Join the discussion on Discord at <a href="https://packt.link/lang ">https://packt.link/lang</a></li>
<li class="bulletList">Check the errata on the book’s Packt page</li>
</ul>
<p class="normal">This community support ensures you’ll be able to successfully implement all projects regardless of library <a id="_idTextAnchor048"/>updates.</p>
</div>
</div>
<div><h2 class="heading-2" id="_idParaDest-36"><a id="_idTextAnchor049"/>API key setup</h2>
<p class="normal">LangChain’s<a id="_idIndexMarker079"/> provider-agnostic approach supports a wide range of LLM providers, each with unique strengths and characteristics. Unless you use a local LLM, to use these services, you’ll need to obtain the appropriate authentication credentials.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Provider</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Environment Variable</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Setup URL</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Free Tier?</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">OpenAI</p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode">OPENAI_API_KEY</code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://platform.openai.com/">platform.openai.com</a></p>
</td>
<td class="No-Table-Style">
<p class="normal">No</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">HuggingFace</p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode">HUGGINGFACEHUB_API_TOKEN</code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://huggingface.co/settings/tokens">huggingface.co/settings/tokens</a></p>
</td>
<td class="No-Table-Style">
<p class="normal">Yes</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Anthropic</p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode">ANTHROPIC_API_KEY</code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://console.anthropic.com">console.anthropic.com</a></p>
</td>
<td class="No-Table-Style">
<p class="normal">No</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Google AI</p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode">GOOGLE_API_KEY</code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://ai.google.dev/gemini-api">ai.google.dev/gemini-api</a></p>
</td>
<td class="No-Table-Style">
<p class="normal">Yes</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Google VertexAI</p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode">Application Default Credentials</code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://cloud.google.com/vertex-ai">cloud.google.com/vertex-ai</a></p>
</td>
<td class="No-Table-Style">
<p class="normal">Yes (with limits)</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Replicate</p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode">REPLICATE_API_TOKEN</code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://replicate.com">replicate.com</a></p>
</td>
<td class="No-Table-Style">
<p class="normal">No</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 2.1: API keys reference table (overview)</p>
<p class="normal">Most providers <a id="_idIndexMarker080"/>require an API key, while cloud providers like AWS and Google Cloud also support alternative authentication methods like <strong class="keyWord">Application Default Credentials</strong> (<strong class="keyWord">ADC</strong>). Many<a id="_idIndexMarker081"/> providers offer free tiers without requiring credit card details, making it easy to get started.</p>
<p class="normal">To set an API key in an environment, in Python, we can execute the following lines:</p>
<pre>import os
os.environ["OPENAI_API_KEY"] = "&lt;your token&gt;"</pre>
<p class="normal">Here, <code class="inlineCode">OPENAI_API_KEY</code> is the <a id="_idIndexMarker082"/>environment key that is appropriate for OpenAI. Setting the keys in your environment has the advantage of not needing to include them as parameters in your code every time you use a model or service integration.</p>
<div><p class="normal">You can also expose these variables in your system environment from your terminal. In Linux and macOS, you can set a system environment variable from the terminal using the <code class="inlineCode">export </code>command:</p>
<pre>export OPENAI_API_KEY=&lt;your token&gt;</pre>
<p class="normal">To permanently set the environment variable in Linux or macOS, you would need to add the preceding line to the <code class="inlineCode">~/.bashrc</code> or <code class="inlineCode">~/.bash_profile</code> files, and then reload the shell using the command <code class="inlineCode">source ~/.bashrc</code> or <code class="inlineCode">source ~/.bash_profile</code>.</p>
<p class="normal">For Windows users, you can set the environment variable by searching for “Environment Variables” in the system settings, editing either “User variables” or “System variables,” and adding <code class="inlineCode">export</code> <code class="inlineCode">OPENAI_API_KEY=your_key_here</code>.</p>
<p class="normal">Our choice is to <a id="_idIndexMarker083"/>create a <code class="inlineCode">config.py</code> file where all API keys are stored. We then import a function from this module that loads these keys into the environment variables. This approach centralizes credential management and makes it easier to update keys when needed:</p>
<pre>import os
OPENAI_API_KEY =  "... "
# I'm omitting all other keys
def set_environment():
    variable_dict = globals().items()
 for key, value in variable_dict:
 if "API" in key or "ID" in key:
             os.environ[key] = value</pre>
<p class="normal">If you search for this file in the GitHub repository, you’ll notice it’s missing. This is intentional – I’ve excluded it from Git tracking using the <code class="inlineCode">.gitignore</code> file. The <code class="inlineCode">.gitignore</code> file tells Git which files to ignore when committing changes, which is essential for:</p>
<ol>
<li class="numberedList" value="1">Preventing sensitive credentials from being publicly exposed</li>
<li class="numberedList">Avoiding accidental commits of personal API keys</li>
<li class="numberedList">Protecting yourself from unauthorized usage charges</li>
</ol>
<p class="normal">To implement this yourself, simply add <code class="inlineCode">config.py</code> to your <code class="inlineCode">.gitignore</code> file:</p>
<pre># In .gitignore
config.py
.env
**/api_keys.txt
# Other sensitive files</pre>
<div><p class="normal">You can set all your keys in the <code class="inlineCode">config.py</code> file. This function, <code class="inlineCode">set_environment()</code>, loads all the keys into the environment as mentioned. Anytime you want to run an application, you import the function and run it like so:</p>
<pre>from config import set_environment
set_environment()</pre>
<p class="normal">For production environments, consider using dedicated secrets management services or environment variables injected at runtime. These approaches provide additional security while maintaining the separation between code and credentials.</p>
<p class="normal">While OpenAI’s models<a id="_idIndexMarker084"/> remain influential, the LLM ecosystem has rapidly diversified, offering developers multiple options for their applications. To maintain clarity, we’ll separate LLMs<a id="_idIndexMarker085"/> from the model gateways that provide access to them.</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Key LLM families</strong><ul><li class="bulletList level-2"><strong class="keyWord">Anthropic Claude</strong>: Excels in<a id="_idIndexMarker086"/> reasoning, long-form content processing, and vision analysis with up to 200K token context windows</li>
<li class="bulletList level-2"><strong class="keyWord">Mistral models</strong>: Powerful <a id="_idIndexMarker087"/>open-source models with strong multilingual capabilities and exceptional reasoning abilities</li>
<li class="bulletList level-2"><strong class="keyWord">Google Gemini</strong>: Advanced<a id="_idIndexMarker088"/> multimodal models with industry-leading 1M token context window and real-time information access</li>
<li class="bulletList level-2"><strong class="keyWord">OpenAI GPT-o</strong>: Leading<a id="_idIndexMarker089"/> omnimodal capabilities accepting text, audio, image, and video with enhanced reasoning</li>
<li class="bulletList level-2"><strong class="keyWord">DeepSeek models:</strong> Specialized<a id="_idIndexMarker090"/> in coding and technical reasoning with state-of-the-art performance on programming tasks</li>
<li class="bulletList level-2"><strong class="keyWord">AI21 Labs Jurassic:</strong> Strong in<a id="_idIndexMarker091"/> academic applications and long-form content generation</li>
<li class="bulletList level-2"><strong class="keyWord">Inflection Pi</strong>: Optimized <a id="_idIndexMarker092"/>for conversational AI with exceptional emotional intelligence</li>
<li class="bulletList level-2"><strong class="keyWord">Perplexity models</strong>: Focused<a id="_idIndexMarker093"/> on accurate, cited answers for research applications</li>
<li class="bulletList level-2"><strong class="keyWord">Cohere models</strong>: Specialized <a id="_idIndexMarker094"/>for enterprise applications with strong multilingual capabilities</li>
</ul></li>
<li class="b lletList"><strong class="keyWord">Cloud provider gateways</strong><ul><li class="bulletList level-2"><strong class="keyWord">Amazon Bedrock</strong>: Unified <a id="_idIndexMarker095"/>API access to models from Anthropic, AI21, Cohere, Mistral, and <a id="_idIndexMarker096"/>others with AWS integration</li>
<li class="bulletList level-2"><strong class="keyWord">Azure OpenAI Service</strong>: Enterprise-grade<a id="_idIndexMarker097"/> access to OpenAI and other models with robust security and Microsoft ecosystem integration</li>
<li class="bulletList level-2"><strong class="keyWord">Google Vertex AI</strong>: Access to <a id="_idIndexMarker098"/>Gemini and other models with seamless Google Cloud integration</li>
</ul></li>
<li class="b lletList"><strong class="keyWord">Independent platforms</strong><ul><li class="bulletList level-2"><strong class="keyWord">Together AI</strong>: Hosts 200+ open-source <a id="_idIndexMarker099"/>models with both serverless and dedicated GPU options</li>
<li class="bulletList level-2"><strong class="keyWord">Replicate</strong>: Specializes<a id="_idIndexMarker100"/> in deploying multimodal open-source models with pay-as-you-go pricing</li>
<li class="bulletList level-2"><strong class="keyWord">HuggingFace Inference Endpoints</strong>: Production deployment of thousands of open-source models <a id="_idIndexMarker101"/>with fine-tuning capabilities</li>
</ul></li>
</ul>
<p class="normal">Throughout this book, we’ll work with various models accessed through different providers, giving you the<a id="_idIndexMarker102"/> flexibility to choose the best option for your specific needs and infrastructure requirements.</p>
<p class="normal">We will use OpenAI for many applications but will also try LLMs from other organizations. Refer to the <em class="italic">Appendix</em> at the end of the book to learn how to get API keys for OpenAI, Hugging Face, Google, and other providers.</p>
<div><div><p class="normal">There are two main integration packages:</p>
<ul>
<li class="bulletList"><code class="inlineCode">langchain-google-vertexai</code></li>
<li class="bulletList"><code class="inlineCode">langchain-google-genai</code></li>
</ul>
<p class="normal">We’ll be using <code class="inlineCode">langchain-google-genai</code>, the package recommended by LangChain for individual developers. The setup is a lot simpler, only requiring a Google account and API key. It is recommended to move to <code class="inlineCode">langchain-google-vertexai</code> for larger projects. This integration offers enterprise features such as customer encryption keys, virtual private cloud integration, and more, requiring a Google Cloud account with billing.</p>
<p class="normal">If you’ve followed the instructions on GitHub, as indicated in the previous section, you should already have the <code class="inlineCode">langchain-google-ge<a id="_idTextAnchor050"/><a id="_idTextAnchor051"/><a id="_idTextAnchor052"/><a id="_idTextAnchor053"/><a id="_idTextAnchor054"/>nai</code> package installed.</p>
</div>
</div>
<div><h1 class="heading-1" id="_idParaDest-37"><a id="_idTextAnchor055"/>Exploring LangChain’s building blocks</h1>
<p class="normal">To build <a id="_idIndexMarker103"/>practical applications, we need to know how to work with different model providers. Let’s explore the various options available, from cloud services to local deployments. We’ll start with fundamental concepts like LLMs and chat models, then dive into prompts, chai<a id="_idTextAnchor056"/>ns, and memory systems.</p>
<h2 class="heading-2" id="_idParaDest-38"><a id="_idTextAnchor057"/>Model interfaces</h2>
<p class="normal">LangChain provides a unified interface for working with various LLM providers. This abstraction makes it easy to switch between different models while maintaining a consistent code structure. The<a id="_idIndexMarker104"/> following examples demonstrate how to implement<a id="_idIndexMarker105"/> LangChain’s core components in practical scenarios.</p>
<div><div><p class="normal">Please note that users should almost exclusively be using the newer chat models as most model providers have adopted a chat-like interface for interacting with language models. We still provide the LLM interface, because it’s very easy to use <a id="_idTextAnchor058"/>as string-in, string-out.</p>
</div>
</div>
<h3 class="heading-3" id="_idParaDest-39"><a id="_idTextAnchor059"/>LLM interaction patterns</h3>
<p class="normal">The <a id="_idIndexMarker106"/>LLM interface represents traditional text completion models that take a string input and return a string output. More and more use cases in LangChain use only the ChatModel interface, mainly because it’s better suited for building complex workflows and developing agents. The LangChain documentation is now deprecating the LLM interface and recommending the use of chat-based interfaces. While this chapter demonstrates both interfaces, we recommend using chat models as they represent the current standard to be up to date with LangChain.</p>
<p class="normal">Let’s see the LLM interface in action:</p>
<pre>from langchain_openai import OpenAI
from langchain_google_genai import GoogleGenerativeAI
# Initialize OpenAI model
openai_llm = OpenAI()
# Initialize a Gemini model
gemini_pro = GoogleGenerativeAI(model="gemini-1.5-pro")</pre>
<div><pre># Either one or both can be used with the same interface
response = openai_llm.invoke("Tell me a joke about light bulbs!")
print(response)</pre>
<p class="normal">Please note that you must set your environment variables to the provider keys when you run this. For example, when running this I’d start the file by calling <code class="inlineCode">set_environment() from config</code>:</p>
<pre>from config import set_environment
set_environment()</pre>
<p class="normal">We get this output:</p>
<pre>Why did the light bulb go to therapy?
Because it was feeling a little dim!</pre>
<p class="normal">For the Gemini model, we can run:</p>
<pre>response = gemini_pro.invoke("Tell me a joke about light bulbs!")</pre>
<p class="normal">For me, Gemini<a id="_idIndexMarker107"/> comes up with this joke:</p>
<pre>Why did the light bulb get a speeding ticket?
Because it was caught going over the watt limit!</pre>
<p class="normal">Notice how we use the same <code class="inlineCode">invoke()</code> method regardless of the provider. This consistency makes it easy to experiment with different models or swit<a id="_idTextAnchor060"/>ch providers in production.</p>
<h3 class="heading-3" id="_idParaDest-40"><a id="_idTextAnchor061"/>Development testing</h3>
<p class="normal">During development, you<a id="_idIndexMarker108"/> might want to test your application without making actual API calls. LangChain provides <code class="inlineCode">F<a id="_idTextAnchor062"/>akeListLLM</code> for this purpose:</p>
<pre>from langchain_community.llms import FakeListLLM
# Create a fake LLM that always returns the same response
fake_llm = FakeListLLM(responses=["Hello"])
result = fake_llm.invoke("Any input will return Hello")
print(result)  # Output: Hello</pre>
<div><h3 class="heading-3" id="_idParaDest-41"><a id="_idTextAnchor063"/>Working with chat models</h3>
<p class="normal">Chat models <a id="_idIndexMarker109"/>are LLMs that are fine-tuned for multi-turn interaction between a model and a human. These days most LLMs are fine-tuned for multi-turned conversations. Instead of providing input to the model, such as:</p>
<pre>human: turn1
ai: answer1
human: turn2
ai: answer2</pre>
<p class="normal">where we expect it to generate an output by continuing the conversation, these days model providers typically expose an API that expects each turn as a separate well-formatted part of the payload. Model providers typically don’t store the chat history server-side, they get the full history sent each time from the client and only format the final prompt server-side.</p>
<p class="normal">LangChain follows the same pattern with ChatModels, processing conversations through structured messages with roles and content. Each message contains:</p>
<ul>
<li class="b lletList">Role (who’s speaking), which is defined by the message class (all messages inherit from BaseMessage)</li>
<li class="b lletList">Content (what’s being said)</li>
</ul>
<p class="normal">Message <a id="_idIndexMarker110"/>types include:</p>
<ul>
<li class="b lletList"><code class="inlineCode">SystemMessage</code>: Sets behavior and context for the model. Example:<pre><code class="inlineCode">SystemMessage(content=</code>"You're a helpful programming assistant"<code class="inlineCode">)</code></pre></li>
<li class="b lletList"><code class="inlineCode">HumanMessage</code>: Represents user input like questions, commands, and data. Example:<pre>HumanMessage(content="Write a Python function to calculate factorial")</pre></li>
<li class="b lletList"><code class="inlineCode">AIMessage</code>: Contains model responses</li>
</ul>
<p class="normal">Let’s see this in action:</p>
<pre>from langchain_anthropic import ChatAnthropic
from langchain_core.messages import SystemMessage, HumanMessage
chat = ChatAnthropic(model="claude-3-opus-20240229")
messages = [</pre>
<div><pre>    SystemMessage(content="You're a helpful programming assistant"),
    HumanMessage(content="Write a Python function to calculate factorial")
]
response = chat.invoke(messages)
print(response)</pre>
<p class="normal">Claude comes up with a function, an explanation, and examples for calling the function.</p>
<p class="normal">Here’s a Python function that calculates the factorial of a given number:</p>
<pre>```python
def factorial(n):
 if n &lt; 0:
 raise ValueError("Factorial is not defined for negative numbers.")
 elif n == 0:
 return 1
 else:
        result = 1
 for i in range(1, n + 1):
            result *= i
 return result
```
Let's break that down. The <code class="inlineCode">factorial</code> function is designed to take an integer <code class="inlineCode">n</code> as input and calculate its factorial. It starts by checking if <code class="inlineCode">n</code> is negative, and if so, it raises a <code class="inlineCode">ValueError</code> since factorials aren't defined for negative numbers. If <code class="inlineCode">n</code> is zero, the function returns <code class="inlineCode">1</code>, which makes sense because, by definition, the factorial of 0 is 1.
When dealing with positive numbers, the function kicks things off by setting a variable <code class="inlineCode">result</code> to 1. From there, it enters a loop that runs from 1 to <code class="inlineCode">n</code>, inclusive, thanks to the <code class="inlineCode">range</code> function. During each step of the loop, it multiplies the result by the current number, gradually building up the factorial. Once<a id="_idIndexMarker111"/> the loop completes, the function returns the final calculated value. You can call this function by providing a non-negative integer as an argument. Here are a few examples:
```python
print(factorial(0))  # Output: 1
print(factorial(5))  # Output: 120
print(factorial(10))  # Output: 3628800
print(factorial(-5))  # Raises ValueError: Factorial is not defined for negative numbers.
```</pre>
<div><pre>Note that the factorial function grows very quickly, so calculating the factorial of large numbers may exceed the maximum representable value in Python. In such cases, you might need to use a different approach or a library that supports arbitrary-precision arithmetic.</pre>
<p class="normal">Similarly, we could have asked an OpenAI<a id="_idTextAnchor064"/> model such as GPT-4 or GPT-4o:</p>
<pre>from langchain_openai.chat_models import ChatOpenAI
chat = ChatOpenAI(model_name='gpt-4o')</pre>
<h3 class="heading-3" id="_idParaDest-42"><a id="_idTextAnchor065"/>Reasoning models</h3>
<p class="normal">Anthropic’s Claude 3.7 Sonnet<a id="_idIndexMarker112"/> introduces a powerful capability called <em class="italic">extended thinking</em> that allows the model to show its reasoning process before delivering a final answer. This feature represents a significant advancement in how developers can leverage LLMs for complex reasoning tasks.</p>
<p class="normal">Here’s how to configure extended thinking through the ChatAnthropic class:</p>
<pre>from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
# Create a template
template = ChatPromptTemplate.from_messages([
    ("system", "You are an experienced programmer and mathematical analyst."),
    ("user", "{problem}")
])
# Initialize Claude with extended thinking enabled
chat = ChatAnthropic(
    model_name="claude-3-7-sonnet-20240326",  # Use latest model version
    max_tokens=64_000,                        # Total response length limit
    thinking={"type": "enabled", "budget_tokens": 15000},  # Allocate tokens for thinking
)
# Create and run a chain
chain = template | chat
# Complex algorithmic problem
problem = """</pre>
<div><pre>Design an algorithm to find the kth largest element in an unsorted array
with the optimal time complexity. Analyze the time and space complexity
of your solution and explain why it's optimal.
"""
# Get response with thinking included
response = chat.invoke([HumanMessage(content=problem)])
print(response.content)</pre>
<p class="normal">The<a id="_idIndexMarker113"/> response will include Claude’s step-by-step reasoning about algorithm selection, complexity analysis, and optimization considerations before presenting its final solution. In the preceding example:</p>
<ul>
<li class="b lletList">Out of the 64,000-token maximum response length, up to 15,000 tokens can be used for Claude’s thinking process.</li>
<li class="b lletList">The remaining ~49,000 tokens are available for the final response.</li>
<li class="b lletList">Claude doesn’t always use the entire thinking budget—it uses what it needs for the specific task. If Claude runs out of thinking tokens, it will transition to its final answer.</li>
</ul>
<p class="normal">While Claude offers explicit thinking configuration, you can achieve similar (though not identical) results with other providers through different techniques:</p>
<pre>from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
template = ChatPromptTemplate.from_messages([
    ("system", "You are a problem-solving assistant."),
    ("user", "{problem}")
])
# Initialize with reasoning_effort parameter
chat = ChatOpenAI(
    model="o3-mini","
    reasoning_effort="high"  # Options: "low", "medium", "high"
)
chain = template | chat
response = chain.invoke({"problem": "Calculate the optimal strategy for..."})</pre>
<div><pre>chat = ChatOpenAI(model="gpt-4o")
chain = template | chat
response = chain.invoke({"problem": "Calculate the optimal strategy for..."})</pre>
<p class="normal">The <code class="inlineCode">reasoning_effort</code> parameter streamlines your workflow by eliminating the need for complex<a id="_idIndexMarker114"/> reasoning prompts, allows you to adjust performance by reducing effort when speed matters more than detailed analysis, and helps manage token consumption by controlling how much processing power goes toward reasoning processes.</p>
<p class="normal">DeepSeek models also offer explicit thinking configuration <a id="_idTextAnchor066"/>through the LangChain integration.</p>
<h3 class="heading-3" id="_idParaDest-43"><a id="_idTextAnchor067"/>Controlling model behavior</h3>
<p class="normal">Understanding <a id="_idIndexMarker115"/>how to control an LLM’s behavior is crucial for tailoring its output to specific needs. Without careful parameter adjustments, the model might produce overly creative, inconsistent, or verbose responses that are unsuitable for practical applications. For instance, in customer service, you’d want consistent, factual answers, while in content generation, you might aim for more creative and promotional outputs.</p>
<p class="normal">LLMs offer several parameters that allow fine-grained control over generation behavior, though exact implementation may vary between providers. Let’s explore the most important ones:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Parameter</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Typical Range</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Best For</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Temperature</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Controls randomness in text generation</p>
</td>
<td class="No-Table-Style">
<p class="normal">0.0-1.0 (OpenAI, Anthropic)</p>
<p class="normal">0.0-2.0 (Gemini)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Lower (0.0-0.3): Factual tasks, Q&amp;A</p>
<p class="normal">Higher (0.7+): Creative writing, brainstorming</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Top-k</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Limits token selection to k most probable tokens</p>
</td>
<td class="No-Table-Style">
<p class="normal">1-100</p>
</td>
<td class="No-Table-Style">
<p class="normal">Lower values (1-10): More focused outputs</p>
<p class="normal">Higher values: More diverse completions</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Top-p (Nucleus Sampling)</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Considers tokens until cumulative probability reaches threshold</p>
</td>
<td class="No-Table-Style">
<p class="normal">0.0-1.0</p>
</td>
<td class="No-Table-Style">
<p class="normal">Lower values (0.5): More focused outputs</p>
<p class="normal">Higher values (0.9): More exploratory responses</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<div><p class="normal"><strong class="keyWord">Max tokens</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Limits maximum response length</p>
</td>
<td class="No-Table-Style">
<p class="normal">Model-specific</p>
</td>
<td class="No-Table-Style">
<p class="normal">Controlling costs and preventing verbose outputs</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Presence/frequency penalties</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Discourages repetition by penalizing tokens that have appeared</p>
</td>
<td class="No-Table-Style">
<p class="normal">-2.0 to 2.0</p>
</td>
<td class="No-Table-Style">
<p class="normal">Longer content generation where repetition is undesirable</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Stop sequences</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Tells model when to stop generating</p>
</td>
<td class="No-Table-Style">
<p class="normal">Custom strings</p>
</td>
<td class="No-Table-Style">
<p class="normal">Controlling exact ending points of generation</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 2.2: Parameters offered by LLMs</p>
<p class="normal">These<a id="_idIndexMarker116"/> parameters work together to shape model output:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Temperature + Top-k/Top-p</strong>: First, Top-k/Top-p filter the token distribution, and then temperature affects randomness within that filtered set</li>
<li class="b lletList"><strong class="keyWord">Penalties + Temperature</strong>: Higher temperatures with low penalties can produce creative but potentially repetitive text</li>
</ul>
<p class="normal">LangChain provides a consistent interface for setting these parameters across different LLM providers:</p>
<pre>from langchain_openai import OpenAI
# For factual, consistent responses
factual_llm = OpenAI(temperature=0.1, max_tokens=256)
# For creative brainstorming
creative_llm = OpenAI(temperature=0.8, top_p=0.95, max_tokens=512)</pre>
<p class="normal">A few provider-specific considerations to keep in mind are:</p>
<ul>
<li class="b lletList"><strong class="keyWord">OpenAI</strong>: Known for consistent behavior with temperature in the 0.0-1.0 range</li>
<li class="b lletList"><strong class="keyWord">Anthropic</strong>: May need lower temperature settings to achieve similar creativity levels to other providers</li>
<li class="b lletList"><strong class="keyWord">Gemini</strong>: Supports temperature up to 2.0, allowing for more extreme creativity at higher settings</li>
<li class="b lletList"><strong class="keyWord">Open-source models</strong>: Often require different parameter combinations than commercial APIs</li>
</ul>
<div><h3 class="heading-3" id="_idParaDest-44"><a id="_idTextAnchor068"/>Choosing parameters for applications</h3>
<p class="normal">For enterprise<a id="_idIndexMarker117"/> applications requiring consistency and accuracy, lower temperatures (0.0-0.3) combined with moderate top-p values (0.5-0.7) are typically preferred. For creative assistants or brainstorming tools, higher temperatures produce more diverse outputs, especially when paired with higher top-p values.</p>
<p class="normal">Remember that parameter tuning is often empirical – start with provider recommendations, then adjust based on your specific application needs and observed outputs.</p>
<h2 class="heading-2" id="_idParaDest-45"><a id="_idTextAnchor069"/>Prompts and templates</h2>
<p class="normal">Prompt engineering<a id="_idIndexMarker118"/> is a crucial skill for LLM application development, particularly in production environments. LangChain provides a robust system for managing <a id="_idIndexMarker119"/>prompts with features that address common development challenges:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Template systems</strong> for dynamic prompt generation</li>
<li class="b lletList"><strong class="keyWord">Prompt management and versioning </strong>for tracking changes</li>
<li class="b lletList"><strong class="keyWord">Few-shot example management</strong> for improved model performance</li>
<li class="b lletList"><strong class="keyWord">Output parsing and validation</strong> for reliable results</li>
</ul>
<p class="normal">LangChain’s <a id="_idIndexMarker120"/>prompt templates transform static text into dynamic prompts with variable substitution – compare these two approaches to see the key differences:</p>
<ol>
<li class="numberedList" value="1">Static use – problematic at scale:<pre> def generate_prompt(question, context=None):
 if context:
 return f"Context information: {context}\n\nAnswer this question concisely: {question}"
 return f"Answer this question concisely: {question}"
 # example use:
      prompt_text = generate_prompt("What is the capital of France?")</pre></li>
<li class="numberedList">PromptTemplate – production-ready:<pre>from langchain_core.prompts import PromptTemplate
# Define once, reuse everywhere
question_template = PromptTemplate.from_template( "Answer this question concisely: {question}" )
question_with_context_template = PromptTemplate.from_template( "Context information: {context}\n\nAnswer this question concisely: {question}" )
# Generate prompts by filling in variables
prompt_text = question_template.format(question="What is the capital of France?")</pre></li>
</ol>
<div><p class="normal">Templates<a id="_idIndexMarker121"/> matter – here’s why:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Consistency</strong>: They standardize prompts across your application.</li>
<li class="b lletList"><strong class="keyWord">Maintainability</strong>: They allow you to change the prompt structure in one place instead of throughout your codebase.</li>
<li class="b lletList"><strong class="keyWord">Readability</strong>: They clearly separate template logic from business logic.</li>
<li class="b lletList"><strong class="keyWord">Testability</strong>: It is easier to unit test prompt generation separately from LLM calls.</li>
</ul>
<p class="normal">In production applications, you’ll often need to manage dozens or hundreds of prompts. Templates provide a scalable way to organize this comple<a id="_idTextAnchor070"/>xity.</p>
<h3 class="heading-3" id="_idParaDest-46"><a id="_idTextAnchor071"/>Chat prompt templates</h3>
<p class="normal">For chat models, we <a id="_idIndexMarker122"/>can create more structured prompts that incorporate different roles:</p>
<pre>from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
template = ChatPromptTemplate.from_messages([
    ("system", "You are an English to French translator."),
    ("user", "Translate this to French: {text}")
])
chat = ChatOpenAI()
formatted_messages = template.format_messages(text="Hello, how are you?")
response = chat.invoke(formatted_messages)
print(response)</pre>
<p class="normal">Let’s start by looking at <strong class="keyWord">LangChain Expression Language</strong> (<strong class="keyWord">LCEL</strong>), which provides a clean, intuitive way to build LLM applica<a id="_idTextAnchor072"/>tions.</p>
<div><h2 class="heading-2" id="_idParaDest-47"><a id="_idTextAnchor073"/>LangChain Expression Language (LCEL)</h2>
<p class="normal">LCEL <a id="_idIndexMarker123"/>represents a significant <a id="_idIndexMarker124"/>evolution in how we build LLM-powered applications with LangChain. Introduced in August 2023, LCEL is a declarative approach to constructing complex LLM workflows. Rather than focusing on <em class="italic">how</em> to execute each step, LCEL lets you define <em class="italic">what</em> you want to accomplish, allowing LangChain to handle the execution details behind the scenes.</p>
<p class="normal">At its core, LCEL serves as a minimalist code layer that makes it remarkably easy to connect different LangChain<a id="_idIndexMarker125"/> components. If you’re familiar with Unix pipes or data processing libraries like pandas, you’ll recognize the intuitive syntax: components are connected using the pipe operator (|) to create processing pipelines.</p>
<p class="normal">As we briefly introduced in <a href="E_Chapter_1.xhtml#_idTextAnchor001"><em class="italic">Chapter 1</em></a>, LangChain has always used the concept of a “chain” as its fundamental pattern for connecting components. Chains represent sequences of operations that <a id="_idIndexMarker126"/>transform inputs into outputs.</p>
<p class="normal">Originally, LangChain implemented this pattern through specific <code class="inlineCode">Chain</code> classes like <code class="inlineCode">LLMChain</code> and <code class="inlineCode">ConversationChain</code>. While these legacy classes still exist, they’ve been deprecated in favor of the more flexible and powerful LCEL approach, which is built upon the Runnable interface.</p>
<p class="normal">The Runnable interface is the cornerstone of modern LangChain. A Runnable is any component that can process inputs and produce outputs in a standardized way. Every component built with LCEL adheres to this interface, which provides consistent methods including:</p>
<ul>
<li class="b lletList"><code class="inlineCode">invoke()</code>: Processes a single input synchronously and returns an output</li>
<li class="b lletList"><code class="inlineCode">stream()</code>: Streams output as it’s being generated</li>
<li class="b lletList"><code class="inlineCode">batch()</code>: Efficiently processes multiple inputs in parallel</li>
<li class="b lletList"><code class="inlineCode">ainvoke()</code>, <code class="inlineCode">abatch()</code>, <code class="inlineCode">astream()</code>: Asynchronous versions of the above methods</li>
</ul>
<p class="normal">This standardization means any Runnable component—whether it’s an LLM, a prompt template, a document retriever, or a custom function—can be connected to any other Runnable, creating a powerful composability system.</p>
<p class="normal">Every Runnable implements a consistent set of methods including:</p>
<ul>
<li class="b lletList"><code class="inlineCode">invoke()</code>: Processes a single input synchronously and returns an output</li>
<li class="b lletList"><code class="inlineCode">stream()</code>: Streams output as it’s being generated</li>
</ul>
<p class="normal">This standardization is powerful because it means any Runnable component—whether it’s an LLM, a prompt template, a document retriever, or a custom function—can be connected to any other Runnable. The consistency of this interface enables complex applications to be built from simpler building blocks.</p>
<div><div><div><p class="normal">LCEL offers several advantages that make it the preferred approach for building LangChain applications:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Rapid development</strong>: The declarative syntax enables faster prototyping and iteration of complex chains.</li>
<li class="bulletList"><strong class="keyWord">Production-ready features</strong>: LCEL provides built-in support for streaming, asynchronous execution, and parallel processing.</li>
<li class="bulletList"><strong class="keyWord">Improved readability</strong>: The pipe syntax makes it easy to visualize data flow through your application.</li>
<li class="bulletList"><strong class="keyWord">Seamless ecosystem integration</strong>: Applications built with LCEL automatically work with LangSmith for observability and LangServe for deployment.</li>
<li class="bulletList"><strong class="keyWord">Customizability</strong>: Easily incorporate custom Python functions into your chains with RunnableLambda.</li>
<li class="bulletList"><strong class="keyWord">Runtime optimization</strong>: LangChain can automatically optimize the execution of LCEL-defined chains.</li>
</ul>
</div>
</div>
<p class="normal">LCEL truly shines when you need to build complex applications that combine multiple components in <a id="_idIndexMarker127"/>sophisticated workflows. In the next sections, we’ll <a id="_idIndexMarker128"/>explore how to use LCEL to build real-world applications, starting with the basic building blocks and gradually incorporating more advanced patterns.</p>
<p class="normal">The pipe operator (|) serves as the cornerstone of LCEL, allowing you to chain components sequentially:</p>
<pre># 1. Basic sequential chain: Just prompt to LLM
basic_chain = prompt | llm | StrOutputParser()</pre>
<p class="normal">Here, <code class="inlineCode">StrOutputParser()</code> is a simple output parser that extracts the string response from an LLM. It takes the structured output from an LLM and converts it to a plain string, making it easier to work with. This parser is especially useful when you need just the text content without metadata.</p>
<p class="normal">Under the hood, LCEL uses Python’s operator overloading to transform this expression into a RunnableSequence where each component’s output flows into the next component’s input. The pipe (|) is syntactic sugar that overrides the <code class="inlineCode">__or__</code> hidden method, in other words, <code class="inlineCode">A | B</code> is equivalent to <code class="inlineCode">B.__or__(A)</code>.</p>
<div><p class="normal">The pipe syntax is equivalent to creating a <code class="inlineCode">RunnableSequence</code> programmatically:</p>
<pre>chain = RunnableSequence(first= prompt, middle=[llm], last= output_parser)
LCEL also supports adding transformations and custom functions:
with_transformation = prompt | llm | (lambda x: x.upper()) | StrOutputParser()</pre>
<p class="normal">For more complex workflows, you can incorporate branching logic:</p>
<pre>decision_chain = prompt | llm | (lambda x: route_based_on_content(x)) | {
 "summarize": summarize_chain,
 "analyze": analyze_chain
}</pre>
<p class="normal">Non-Runnable elements like functions and dictionaries are automatically converted to appropriate Runnable types:</p>
<pre># Function to Runnable
length_func = lambda x: len(x)
chain = prompt | length_func | output_parser
# Is converted to:
chain = prompt | RunnableLambda(length_func) | output_parser</pre>
<p class="normal">The flexible, composable<a id="_idIndexMarker129"/> nature of LCEL will allow us to <a id="_idIndexMarker130"/>tackle real-world LLM application challenges with elegant, main<a id="_idTextAnchor074"/>tainable code.</p>
<h3 class="heading-3" id="_idParaDest-48"><a id="_idTextAnchor075"/>Simple workflows with LCEL</h3>
<p class="normal">As we’ve seen, LCEL <a id="_idIndexMarker131"/>provides a declarative syntax for composing LLM application components using the pipe operator. This approach dramatically simplifies workflow construction compared to traditional imperative code. Let’s build a simple joke generator to see LCEL in action:</p>
<pre>from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
# Create components
prompt = PromptTemplate.from_template("Tell me a joke about {topic}")
llm = ChatOpenAI()
output_parser = StrOutputParser()</pre>
<div><pre># Chain them together using LCEL
chain = prompt | llm | output_parser
#  Execute the workflow with a single call
result = chain.invoke({"topic": "programming"})
print(result)</pre>
<p class="normal">This produces a programming joke:</p>
<pre>Why don't programmers like nature?
It has too many bugs!</pre>
<p class="normal">Without LCEL, the same workflow is equivalent to separate function calls with manual data passing:</p>
<pre>formatted_prompt = prompt.invoke({"topic": "programming"})
llm_output = llm.invoke(formatted_prompt)
result = output_parser.invoke(llm_output)</pre>
<p class="normal">As you can see, we <a id="_idIndexMarker132"/>have detached chain construction fro<a id="_idTextAnchor076"/>m its execution.</p>
<p class="normal">In production applications, this pattern becomes even more valuable when handling complex workflows with branching logic, error handling, or parallel processing – topics we’ll explore in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>.</p>
<h3 class="heading-3" id="_idParaDest-49"><a id="_idTextAnchor077"/>Complex chain example</h3>
<p class="normal">While the <a id="_idIndexMarker133"/>simple joke generator demonstrated basic LCEL usage, real-world applications typically require more sophisticated data handling. Let’s explore advanced patterns using a story generation and analysis example.</p>
<p class="normal">In this example, we’ll build a multi-stage workflow that demonstrates how to:</p>
<ol>
<li class="numberedList" value="1">Generate content with one LLM call</li>
<li class="numberedList">Feed that content into a second LLM call</li>
<li class="numberedList">Preserve and transform data throughout the chain</li>
</ol>
<pre>from langchain_core.prompts import PromptTemplate
from langchain_google_genai import GoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
# Initialize the model
llm = GoogleGenerativeAI(model="gemini-1.5-pro")</pre>
<div><pre># First chain generates a story
story_prompt = PromptTemplate.from_template("Write a short story about {topic}")
story_chain = story_prompt | llm | StrOutputParser()
# Second chain analyzes the story
analysis_prompt = PromptTemplate.from_template(
 "Analyze the following story's mood:\n{story}"
)
analysis_chain = analysis_prompt | llm | StrOutputParser()</pre>
<p class="normal">We can compose these two chains together. Our first simple approach pipes the story directly into the analysis chain:</p>
<pre># Combine chains
story_with_analysis = story_chain | analysis_chain
# Run the combined chain
story_analysis = story_with_analysis.invoke({"topic": "a rainy day"})
print("\nAnalysis:", story_analysis)</pre>
<p class="normal">I get a long analysis. Here’s how it starts:</p>
<pre>Analysis: The mood of the story is predominantly **calm, peaceful, and subtly romantic.** There's a sense of gentle melancholy brought on by the rain and the quiet emptiness of the bookshop, but this is balanced by a feeling of warmth and hope.</pre>
<p class="normal">While this <a id="_idIndexMarker134"/>works, we’ve lost the original story in our result – we only get the analysis! In production applications, we typically want to preserve context throughout the chain:</p>
<pre>from langchain_core.runnables import RunnablePassthrough
# Using RunnablePassthrough.assign to preserve data
enhanced_chain = RunnablePassthrough.assign(
    story=story_chain  # Add 'story' key with generated content
).assign(
    analysis=analysis_chain  # Add 'analysis' key with analysis of the story
)
# Execute the chain</pre>
<div><pre>result = enhanced_chain.invoke({"topic": "a rainy day"})
print(result.keys())  # Output: dict_keys(['topic', 'story', 'analysis'])  # dict_keys(['topic', 'story', 'analysis'])</pre>
<p class="normal">For more control over the output structure, we could also construct dictionaries manually:</p>
<pre>from operator import itemgetter
# Alternative approach using dictionary construction
manual_chain = (
    RunnablePassthrough() |  # Pass through input
    {
 "story": story_chain,  # Add story result
 "topic": itemgetter("topic")  # Preserve original topic
    } |
    RunnablePassthrough().assign(  # Add analysis based on story
        analysis=analysis_chain
    )
)
result = manual_chain.invoke({"topic": "a rainy day"})
print(result.keys())  # Output: dict_keys(['story', 'topic', 'analysis'])</pre>
<p class="normal">We can simplify this with dictionary conversion using a LCEL shorthand:</p>
<pre># Simplified dictionary construction
simple_dict_chain = story_chain | {"analysis": analysis_chain}
result = simple_dict_chain.invoke({"topic": "a rainy day"}) print(result.keys()) # Output: dict_keys(['analysis', 'output'])</pre>
<p class="normal">What makes<a id="_idIndexMarker135"/> these examples more complex than our simple joke generator?</p>
<ul>
<li class="b lletList"><strong class="keyWord">M</strong><strong class="keyWord">ultiple LLM calls</strong>: Rather than a single prompt <img alt="" src="img/Icon.png"/> LLM <img alt="" src="img/Icon.png"/> parser flow, we’re chaining multiple LLM interactions</li>
<li class="b lletList"><strong class="keyWord">Data transformation</strong>: Using tools like <code class="inlineCode">RunnablePassthrough</code> and <code class="inlineCode">itemgetter</code> to manage and transform data</li>
<li class="b lletList"><strong class="keyWord">Dictionary preservation</strong>: Maintaining context throughout the chain rather than just passing single values</li>
<li class="b lletList"><strong class="keyWord">Structured outputs</strong>: Creating structured output dictionaries rather than simple strings</li>
</ul>
<div><p class="normal">These patterns are essential for production applications where you need to:</p>
<ul>
<li class="b lletList">Track the provenance of generated content</li>
<li class="b lletList">Combine results from multiple operations</li>
<li class="b lletList">Structure data for downstream processing or display</li>
<li class="b lletList">Implement more sophisticated error handling</li>
</ul>
<div><div><p class="normal">While LCEL handles many complex workflows elegantly, for state management and advanced branching logic, you’ll want to explore LangGraph, which we’ll <a id="_idTextAnchor078"/>cover in <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>.</p>
</div>
</div>
<p class="normal">While our previous examples used cloud-based models like OpenAI and Google’s Gemini, LangChain’s LCEL and other functionality work seamlessly with local models as well. This flexibility allows you to choose the right deployment approach for your specific needs.</p>
<h1 class="heading-1" id="_idParaDest-50"><a id="_idTextAnchor079"/>Running local models</h1>
<p class="normal">When building LLM applications<a id="_idIndexMarker136"/> with LangChain, you need to decide where your models will run.</p>
<ul>
<li class="b lletList">Advantages of local models:<ul><li class="bulletList level-2">Complete data control and privacy</li>
<li class="bulletList level-2">No API costs or usage limits</li>
<li class="bulletList level-2">No internet dependency</li>
<li class="bulletList level-2">Control over model parameters and fine-tuning</li>
</ul></li>
<li class="b lletList">Advantages of cloud models:<ul><li class="bulletList level-2">No hardware requirements or setup complexity</li>
<li class="bulletList level-2">Access to the most powerful, state-of-the-art models</li>
<li class="bulletList level-2">Elastic scaling without infrastructure management</li>
<li class="bulletList level-2">Continuous model improvements without manual updates</li>
</ul></li>
<li class="b lletList">When to choose <a id="_idIndexMarker137"/>local models:<ul><li class="bulletList level-2">Applications with strict data privacy requirements</li>
<li class="bulletList level-2">Development and testing environments</li>
<li class="bulletList level-2">Edge or offline deployment scenarios</li>
<li class="bulletList level-2">Cost-sensitive applications with predictable, high-volume usage</li>
</ul></li>
</ul>
<div><p class="normal">Let’s start with one of the most developer-friendly options for <a id="_idTextAnchor080"/>running local models.</p>
<h2 class="heading-2" id="_idParaDest-51"><a id="_idTextAnchor081"/>Getting started with Ollama</h2>
<p class="normal">Ollama <a id="_idIndexMarker138"/>provides<a id="_idIndexMarker139"/> a developer-friendly way to run powerful open-source models locally. It provides a simple interface for downloading and running various open-source models. The <code class="inlineCode">langchain-ollama</code> dependency should already be installed if you’ve followed the instructions in this chapter; however, let’s go through them briefly anyway:</p>
<ol>
<li class="numberedList" value="1">Install the LangChain Ollama integration:<pre>pip install langchain-ollama</pre></li>
<li class="numberedList">Then pull a model. From the command line, a terminal such as bash or the WindowsPowerShell, run:<pre>ollama pull deepseek-r1:1.5b</pre></li>
<li class="numberedList">Start the Ollama server:<pre>ollama serve</pre></li>
</ol>
<p class="normal">Here’s how to integrate Ollama with the LCEL patterns we’ve explored:</p>
<pre>from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
# Initialize Ollama with your chosen model
local_llm = ChatOllama(
    model="deepseek-r1:1.5b",
    temperature=0,
)
# Create an LCEL chain using the local model
prompt = PromptTemplate.from_template("Explain {concept} in simple terms")
local_chain = prompt | local_llm | StrOutputParser()
# Use the chain with your local model
result = local_chain.invoke({"concept": "quantum computing"})
print(result)</pre>
<p class="normal">This LCEL <a id="_idIndexMarker140"/>chain <a id="_idIndexMarker141"/>functions identically to our cloud-based examples, demonstrating LangChain’s model-agnostic design.</p>
<div><p class="normal">Please note that since you are running a local model, you don’t need to set up any keys. The answer is very long – although quite reasonable. You can run this yourself and see what answers you get.</p>
<p class="normal">Now that we’ve seen basic text generation, let’s look at another integration. Hugging Face offers an approachable way to run models locally, with access to a vast ecosyst<a id="_idTextAnchor082"/>em of pre-trained models.</p>
<h2 class="heading-2" id="_idParaDest-52"><a id="_idTextAnchor083"/>Working with Hugging Face models locally</h2>
<p class="normal">With<a id="_idIndexMarker142"/> Hugging Face, you can either run a model locally (HuggingFacePipeline) or on <a id="_idIndexMarker143"/>the Hugging Face Hub (HuggingFaceEndpoint). Here, we are talking about local runs, so we’ll focus on <code class="inlineCode">HuggingFacePipeline</code>. Here we go:</p>
<pre>from langchain_core.messages import SystemMessage, HumanMessage
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
# Create a pipeline with a small model:
llm = HuggingFacePipeline.from_model_id(
    model_id="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    task="text-generation",
    pipeline_kwargs=dict(
        max_new_tokens=512,
        do_sample=False,
        repetition_penalty=1.03,
    ),
)
chat_model = ChatHuggingFace(llm=llm)
# Use it like any other LangChain LLM
messages = [
    SystemMessage(content="You're a helpful assistant"),
    HumanMessage(
        content="Explain the concept of machine learning in simple terms"
    ),
]
ai_msg = chat_model.invoke(messages)
print(ai_msg.content)</pre>
<p class="normal">This can take quite a while, especially the first time, since the model has to be downloaded first. We’ve omitted<a id="_idIndexMarker144"/> the model response for the sake of brevity.</p>
<div><p class="normal">LangChain <a id="_idIndexMarker145"/>supports running models locally through other integrations as well, for example:</p>
<ul>
<li class="b lletList"><strong class="keyWord">llama.cpp:</strong> This<a id="_idIndexMarker146"/> high-performance C++ implementation allows running LLaMA-based models efficiently on consumer hardware. While we won’t cover the setup process in detail, LangChain provides straightforward integration with llama.cpp for both inference and fine-tuning.</li>
<li class="b lletList"><strong class="keyWord">GPT4All</strong>: GPT4All<a id="_idIndexMarker147"/> offers lightweight models that can run on consumer hardware. LangChain’s integration makes it easy to use these models as drop-in replacements for cloud-based LLMs in many applications.</li>
</ul>
<p class="normal">As you begin working with local models, you’ll want to optimize their performance and handle common challenges. Here are some essential tips and patterns that will help you get the most out of your lo<a id="_idTextAnchor084"/>cal deployments with LangChain.</p>
<h2 class="heading-2" id="_idParaDest-53"><a id="_idTextAnchor085"/>Tips for local models</h2>
<p class="normal">When <a id="_idIndexMarker148"/>working with local models, keep these points in mind:</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Resource management</strong>: Local models require careful configuration to balance performance and resource usage. The following example demonstrates how to configure an Ollama model for efficient operation:<pre>#  Configure model with optimized memory and processing settings
from langchain_ollama import ChatOllama
llm = ChatOllama(
  model="mistral:q4_K_M", # 4-bit quantized model (smaller memory footprint)
  num_gpu=1, # Number of GPUs to utilize (adjust based on hardware)
 num_thread=4 # Number of CPU threads for parallel processing
)</pre></li>
</ol>
<p class="normal-one">Let’s look at what each parameter does:</p>
<div><ul>
<li class="bulletList level-2"><strong class="keyWord">model=”mistral:q4_K_M”</strong>: Specifies a 4-bit quantized version of the Mistral model. Quantization reduces the model size by representing weights with fewer bits, trading minimal precision for significant memory savings. For example:<ul><li class="bulletList level-3">Full precision model: ~8GB RAM required</li>
<li class="bulletList level-3">4-bit quantized model: ~2GB RAM required</li>
</ul></li>
<li class="bulletList level-2"><strong class="keyWord">num_gpu=1</strong>: Allocates GPU resources. Options include:<ul><li class="bulletList level-3"><a id="_idTextAnchor086"/>0: CPU-only mode (slower but works without a GPU)</li>
<li class="bulletList level-3"><a id="_idTextAnchor087"/>1: Uses a single GPU (appropriate for most desktop setups)</li>
<li class="bulletList level-3">Higher values: For multi-GPU systems only</li>
</ul></li>
<li class="bulletList level-2"><strong class="keyWord">num_thread=4</strong>: Controls CPU parallelization:<ul><li class="bulletList level-3">Lower values (2-4): Good for running alongside other applications</li>
<li class="bulletList level-3">Higher values (8-16): Maximizes performance on dedicated servers</li>
<li class="bulletList level-3">Optimal setting: Usually matches your CPU’s physical core count</li>
</ul></li>
</ul>
<ol>
<li class="numberedList" value="2"><strong class="keyWord">Error handling</strong>: Local <a id="_idIndexMarker149"/>models can encounter various errors, from out-of-memory conditions to unexpected terminations. A robust error-handling strategy is essential:</li>
</ol>
<pre>def safe_model_call(llm, prompt, max_retries=2):
 """Safely call a local model with retry logic and graceful
    failure"""
    retries = 0
 while retries &lt;= max_retries:
 try:
 return llm.invoke(prompt)
 except RuntimeError as e:
 # Common error with local models when running out of VRAM
 if "CUDA out of memory" in str(e):
 print(f"GPU memory error, waiting and retrying ({retries+1}/{max_retries+1})")
                time.sleep(2)  # Give system time to free resources
                retries += 1
 else:
 print(f"Runtime error: {e}")
 return "An error occurred while processing your request."
 except Exception as e:
 print(f"Unexpected error calling model: {e}")
 return "An error occurred while processing your request."
 # If we exhausted retries
 return "Model is currently experiencing high load. Please try again later."
# Use the safety wrapper in your LCEL chain
from langchain_core.prompts import PromptTemplate</pre>
<div><pre>from langchain_core.runnables import RunnableLambda
prompt = PromptTemplate.from_template("Explain {concept} in simple terms")
safe_llm = RunnableLambda(lambda x: safe_model_call(llm, x))
safe_chain = prompt | safe_llm
response = safe_chain.invoke({"concept": "quantum computing"})</pre>
<p class="normal">Common local model<a id="_idIndexMarker150"/> errors you might run into are as follows:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Out of memory</strong>: Occurs when the model requires more VRAM than available</li>
<li class="b lletList"><strong class="keyWord">Model loading failure</strong>: When model files are corrupt or incompatible</li>
<li class="b lletList"><strong class="keyWord">Timeout issues</strong>: When inference takes too long on resource-constrained systems</li>
<li class="b lletList"><strong class="keyWord">Context length errors</strong>: When input exceeds the model’s maximum token limit</li>
</ul>
<p class="normal">By implementing these optimizations and error-handling strategies, you can create robust LangChain applications that leverage local models effectively while maintaining a good user experience even when issues arise.</p>
<figure class="mediaobject"><img alt="Figure 2.1: Decision chart for choosing between local and cloud-based models" src="img/B32363_02_01.png"/></figure>
<p class="packt_figref">Figure 2.1: Decision chart for choosing between local and cloud-based models</p>
<div><p class="normal">Having explored how to build text-based applications with LangChain, we’ll now extend our understanding to multimodal capabilities. As AI systems increasingly work with multiple forms of data, LangChain<a id="_idIndexMarker151"/> provides interfaces for both generating images from text and understanding visual content – capabilities that complement the text processing we’ve already covered and open new possibilitie<a id="_idTextAnchor088"/>s for more immersive applic<a id="_idTextAnchor089"/>ations.</p>
<h1 class="heading-1" id="_idParaDest-54"><a id="_idTextAnchor090"/>Multimodal AI applications</h1>
<p class="normal">AI systems <a id="_idIndexMarker152"/>have evolved beyond text-only processing to work with diverse data types. In the current landscape, we can distinguish between two key capabilities that are often confused but represent different technological approaches.</p>
<p class="normal">Multimodal understanding represents the ability of models to process multiple types of inputs simultaneously to perform reasoning and generate responses. These advanced systems can understand the relationships between different modalities, accepting inputs like text, images, PDFs, audio, video, and structured data. Their processing capabilities include cross-modal reasoning, context awareness, and sophisticated information extraction. Models like Gemini 2.5, GPT-4V, Sonnet 3.7, and Llama 4 exemplify this capability. For instance, a multimodal model can analyze a chart image along with a text question to provide insights about the data trend, combining visual and textual understanding in a single processing flow.</p>
<p class="normal">Content generation capabilities, by contrast, focus on creating specific types of media, often with extraordinary quality but more specialized functionality. Text-to-image models create visual content from descriptions, text-to-video systems generate video clips from prompts, text-to-audio tools produce music or speech, and image-to-image models transform existing visuals. Examples include Midjourney, DALL-E, and Stable Diffusion for images; Sora and Pika for video; and Suno and ElevenLabs for audio. Unlike true multimodal models, many generation systems are specialized for their specific output modality, even if they can accept multiple input types. They excel at creation rather than understanding.</p>
<p class="normal">As LLMs evolve beyond text, LangChain is expanding to support both multimodal understanding and content generation workflows. The framework provides developers with tools to incorporate these advanced capabilities into their applications without needing to implement complex integrations from scratch. Let’s start with generating images from text descriptions. LangChain provides several approaches to incorporate image generation through external integrations and wrappers. We’ll explore multiple implementation patterns, starting with the simplest and progressing to more sophisticated techniques that can be incorporated into your applications.</p>
<div><h2 class="heading-2" id="_idParaDest-55"><a id="_idTextAnchor091"/>Text-to-image</h2>
<p class="normal">LangChain <a id="_idIndexMarker153"/>integrates with various image generation models <a id="_idIndexMarker154"/>and services, allowing you to:</p>
<ul>
<li class="b lletList">Generate images from text descriptions</li>
<li class="b lletList">Edit existing images based on text prompts</li>
<li class="b lletList">Control image generation parameters</li>
<li class="b lletList">Handle image variations and styles</li>
</ul>
<p class="normal">LangChain includes <a id="_idIndexMarker155"/>wrappers and models for popular image generation services. First, let’s see how to generate ima<a id="_idTextAnchor092"/>ges with OpenAI’s DALL-E model series.</p>
<h3 class="heading-3" id="_idParaDest-56"><a id="_idTextAnchor093"/>Using DALL-E through OpenAI</h3>
<p class="normal">LangChain’s <a id="_idIndexMarker156"/>wrapper for DALL-E simplifies<a id="_idIndexMarker157"/> the process of generating images from text prompts. The implementation uses OpenAI’s API under the hood but provides a standardized interface consistent with other LangChain components.</p>
<pre>from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper
dalle = DallEAPIWrapper(
   model_name="dall-e-3",  # Options: "dall-e-2" (default) or "dall-e-3"
   size="1024x1024",       # Image dimensions
    quality="standard",     # "standard" or "hd" for DALL-E 3
    n=1 # Number of images to generate (only for DALL-E 2)
)
# Generate an image
image_url = dalle.run("A detailed technical diagram of a quantum computer")
# Display the image in a notebook
from IPython.display import Image, display
display(Image(url=image_url))
# Or save it locally
import requests
response = requests.get(image_url)</pre>
<div><pre>with open("generated_library.png", "wb") as f:
    f.write(response.content)</pre>
<p class="normal">Here’s the image we got:</p>
<figure class="mediaobject"><img alt="Figure 2.2: An image generated by OpenAI’s DALL-E Image Generator" src="img/B32363_02_02.png"/></figure>
<p class="packt_figref">Figure 2.2: An image generated by OpenAI’s DALL-E Image Generator</p>
<p class="normal">You might <a id="_idIndexMarker158"/>notice that text generation within <a id="_idIndexMarker159"/>these images is not one of the strong suites of these models. You can find a lot of models for image generation on Replicate, including the latest Stable Diffusi<a id="_idTextAnchor094"/>on models, so this is what we’ll use now.</p>
<div><h3 class="heading-3" id="_idParaDest-57"><a id="_idTextAnchor095"/>Using Stable Diffusion</h3>
<p class="normal">Stable Diffusion <a id="_idIndexMarker160"/>3.5 Large is Stability AI’s latest<a id="_idIndexMarker161"/> text-to-image model, released in March 2024. It’s a <strong class="keyWord">Multimodal Diffusion Transformer</strong> (<strong class="keyWord">MMDiT</strong>) that <a id="_idIndexMarker162"/>generates high-resolution images with remarkable detail and quality.</p>
<p class="normal">This model uses three fixed, pre-trained text encoders and implements Query-Key Normalization for improved training stability. It’s capable of producing diverse outputs from the same prompt and supports various artistic styles.</p>
<pre>from langchain_community.llms import Replicate
# Initialize the text-to-image model with Stable Diffusion 3.5 Large
text2image = Replicate(
    model="stability-ai/stable-diffusion-3.5-large",
    model_kwargs={
 "prompt_strength": 0.85,
 "cfg": 4.5,
 "steps": 40,
 "aspect_ratio": "1:1",
 "output_format": "webp",
 "output_quality": 90
    }
)
# Generate an image
image_url = text2image.invoke(
 "A detailed technical diagram of an AI agent"
)</pre>
<p class="normal">The<a id="_idIndexMarker163"/> recommended parameters for the new <a id="_idIndexMarker164"/>model include:</p>
<ul>
<li class="b lletList"><strong class="keyWord">prompt_strength</strong>: Controls how closely the image follows the prompt (0.85)</li>
<li class="b lletList"><strong class="keyWord">cfg</strong>: Controls how strictly the model follows the prompt (4.5)</li>
<li class="b lletList"><strong class="keyWord">steps</strong>: More steps result in higher-quality images (40)</li>
<li class="b lletList"><strong class="keyWord">aspect_ratio</strong>: Set to 1:1 for square images</li>
<li class="b lletList"><strong class="keyWord">output_format</strong>: Using WebP for a better quality-to-size ratio</li>
<li class="b lletList"><strong class="keyWord">output_quality</strong>: Set to 90 for high-quality output</li>
</ul>
<div><p class="normal">Here’s the image we got:</p>
<figure class="mediaobject"><img alt="Figure 2.3: An image generated by Stable Diffusion" src="img/B32363_02_03.png"/></figure>
<p class="packt_figref">Figure 2.3: An image generated by Stable Diffusion</p>
<p class="normal">Now let’s explore how to analyze a<a id="_idTextAnchor096"/>nd understand images using multimodal models.</p>
<h2 class="heading-2" id="_idParaDest-58"><a id="_idTextAnchor097"/>Image understanding</h2>
<p class="normal">Image <a id="_idIndexMarker165"/>understanding refers to an AI system’s ability <a id="_idIndexMarker166"/>to interpret and analyze visual information in ways similar to human visual perception. Unlike traditional computer vision (which focuses on specific tasks like object detection or facial recognition), modern multimodal models can perform general reasoning about images, understanding context, relationships, and even implicit meaning within visual content.</p>
<p class="normal">Gemini 2.5 Pro and GPT-4 Vision, among other models, can analyze images and provide detail<a id="_idTextAnchor098"/>ed descriptions or answer questions about them.</p>
<h3 class="heading-3" id="_idParaDest-59"><a id="_idTextAnchor099"/>Using Gemini 1.5 Pro</h3>
<p class="normal">LangChain <a id="_idIndexMarker167"/>handles multimodal input through the <a id="_idIndexMarker168"/>same <code class="inlineCode">ChatModel</code> interface. It accepts <code class="inlineCode">Messages</code> as an input, and a <code class="inlineCode">Message</code> object has a <code class="inlineCode">content</code> field. IA <code class="inlineCode">content</code> can consist of multiple parts, and each part can represent a different modality (that allows you to mix different modalities in your prompt).</p>
<div><p class="normal">You can send<a id="_idIndexMarker169"/> multimodal input by value or by reference. To send it by value, you should encode bytes as <a id="_idIndexMarker170"/>a string and construct an <code class="inlineCode">image_url</code> variable formatted as in the example below using the image we generated using Stable Diffusion:</p>
<pre>import base64
from langchain_google_genai.chat_models import ChatGoogleGenerativeAI
from langchain_core.messages.human import HumanMessage
with open("stable-diffusion.png", 'rb') as image_file:
    image_bytes = image_file.read()
    base64_bytes = base64.b64encode(image_bytes).decode("utf-8")
prompt = [
   {"type": "text", "text": "Describe the image: "},
   {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_bytes}"}},
]
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    temperature=0,
)
response = llm.invoke([HumanMessage(content=prompt)])
print(response.content)
The image presents a futuristic, stylized depiction of a humanoid robot's upper body against a backdrop of glowing blue digital displays. The robot's head is rounded and predominantly white, with sections of dark, possibly metallic, material around the face and ears.  The face itself features glowing orange eyes and a smooth, minimalist design, lacking a nose or mouth in the traditional human sense.  Small, bright dots, possibly LEDs or sensors, are scattered across the head and body, suggesting advanced technology and intricate construction.
The robot's neck and shoulders are visible, revealing a complex internal structure of dark, interconnected parts, possibly wires or cables, which contrast with the white exterior. The shoulders and upper chest are also white, with similar glowing dots and hints of the internal mechanisms showing through. The overall impression is of a sleek, sophisticated machine.</pre>
<div><pre>The background is a grid of various digital interfaces, displaying graphs, charts, and other abstract data visualizations. These elements are all in shades of blue, creating a cool, technological ambiance that complements the robot's appearance. The displays vary in size and complexity, adding to the sense of a sophisticated control panel or monitoring system. The combination of the robot and the background suggests a theme of advanced robotics, artificial intelligence, or data analysis.</pre>
<p class="normal">As multimodal inputs <a id="_idIndexMarker171"/>typically have a large size, sending raw bytes as part of your request might not be the best idea. You can send it by reference by<a id="_idIndexMarker172"/> pointing to the blob storage, but the specific type of storage depends on the model’s provider. For example, Gemini accepts multimedia input as a reference to Google Cloud Storage – a blob storage service provided by Google Cloud.</p>
<pre>prompt = [
   {"type": "text", "text": "Describe the video in a few sentences."},
   {"type": "media", "file_uri": video_uri, "mime_type": "video/mp4"},
]
response = llm.invoke([HumanMessage(content=prompt)])
print(response.content)</pre>
<p class="normal">Exact details on how to construct a multimodal input might depend on the provider of the LLM (and a corresponding LangChain integration handles a dictionary corresponding to a part of a <code class="inlineCode">content</code> field accordingly). For example, Gemini accepts an additional <code class="inlineCode">"video_metadata"</code> key that can point to the start and/or end offset of a video piece to be analyzed:</p>
<pre>offset_hint = {
 "start_offset": {"seconds": 10},
 "end_offset": {"seconds": 20},
       }
prompt = [
   {"type": "text", "text": "Describe the video in a few sentences."},
   {"type": "media", "file_uri": video_uri, "mime_type": "video/mp4", "video_metadata": offset_hint},
]
response = llm.invoke([HumanMessage(content=prompt)])
print(response.content)</pre>
<div><p class="normal">And, of course, such multimodal parts can also be templated. Let’s demonstrate it with a simple template that expects an <a id="_idTextAnchor100"/><code class="inlineCode">image_bytes_str</code> argument that contains encoded bytes:</p>
<pre>prompt = ChatPromptTemplate.from_messages(
   [("user",
    [{"type": "image_url",
 "image_url": {"url": "data:image/jpeg;base64,{image_bytes_str}"},
      }])]
)
prompt.invoke({"image_bytes_str": "test-url"})</pre>
<h3 class="heading-3" id="_idParaDest-60"><a id="_idTextAnchor101"/>Using GPT-4 Vision</h3>
<p class="normal">After <a id="_idIndexMarker173"/>having explored image generation, let’s examine how<a id="_idIndexMarker174"/> LangChain handles image understanding using multimodal models. GPT-4 Vision capabilities (available in models like GPT-4o and GPT-4o-mini) allow us to analyze images alongside text, enabling applications that can “see” and reason about visual content.</p>
<p class="normal">LangChain simplifies working with these models by providing a consistent interface for multimodal inputs. Let’s implement a flexible image analyzer:</p>
<pre>from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
def analyze_image(image_url: str, question: str) -&gt; str:
    chat = ChatOpenAI(model="gpt-4o-mini", max_tokens=256)
 
    message = HumanMessage(
        content=[
            {
 "type": "text",
 "text": question
            },
            {
 "type": "image_url",
 "image_url": {
 "url": image_url,
 "detail": "auto"
                }
            }</pre>
<div><pre>        ]
    )
 
    response = chat.invoke([message])
 return response.content
# Example usage
image_url = "https://replicate.delivery/yhqm/pMrKGpyPDip0LRciwSzrSOKb5ukcyXCyft0IBElxsT7fMrLUA/out-0.png"
questions = [
 "What objects do you see in this image?",
 "What is the overall mood or atmosphere?",
 "Are there any people in the image?"
]
for question in questions:
 print(f"\nQ: {question}")
 print(f"A: {analyze_image(image_url, question)}")</pre>
<p class="normal">The model <a id="_idIndexMarker175"/>provides a rich, detailed analysis of our generated cityscape:</p>
<pre>Q: What objects do you see in this image?
A: The image features a futuristic cityscape with tall, sleek skyscrapers. The buildings appear to have a glowing or neon effect, suggesting a high-tech environment. There is a large, bright sun or light source in the sky, adding to the vibrant atmosphere. A road or pathway is visible in the foreground, leading toward the city, possibly with light streaks indicating motion or speed. Overall, the scene conveys a dynamic, otherworldly urban landscape.
Q: What is the overall mood or atmosphere?
A: The overall mood or atmosphere of the scene is futuristic and vibrant. The glowing outlines of the skyscrapers and the bright sunset create a sense of energy and possibility. The combination of deep colors and light adds a dramatic yet hopeful tone, suggesting a dynamic and evolving urban environment.
Q: Are there any people in the image?
A: There are no people in the image. It appears to be a futuristic cityscape with tall buildings and a sunset.</pre>
<div><p class="normal">This capability <a id="_idIndexMarker176"/>opens numerous possibilities for LangChain applications. By combining image analysis with the text processing patterns we explored earlier in this chapter, you can build sophisticated applications that reason across modalities. In the next chapter, we’ll build on these conce<a id="_idTextAnchor102"/>pts to create more sophisticated multimodal applications.</p>
<h1 class="heading-1" id="_idParaDest-61"><a id="_idTextAnchor103"/>Summary</h1>
<p class="normal">After setting up our development environment and configuring necessary API keys, we’ve explored the foundations of LangChain development, from basic chains to multimodal capabilities. We’ve seen how LCEL simplifies complex workflows and how LangChain integrates with both text and image processing. These building blocks prepare us for more advanced applications in the coming chapters.</p>
<p class="normal">In the next chapter, we’ll expand on these concepts to create more sophisticated multimodal applications with enhanced control flow, structured outputs, and advanced prompt techniques. You’ll learn how to combine multiple modalities in complex chains, incorporate more sophisticated error handling, and build appl<a id="_idTextAnchor104"/>ications that leverage the full potential of modern LLMs.</p>
<h1 class="heading-1" id="_idParaDest-62"><a id="_idTextAnchor105"/>Review questions</h1>
<ol>
<li class="numberedList" value="1">What are the three main limitations of raw LLMs that LangChain addresses?<ul><li class="bulletList level-2">Memory limitations</li>
<li class="bulletList level-2">Tool integration</li>
<li class="bulletList level-2">Context constraints</li>
<li class="bulletList level-2">Processing speed</li>
<li class="bulletList level-2">Cost optimization</li>
</ul></li>
<li class="numberedList">Which of the following best describes the purpose of LCEL (LangChain Expression Language)?<ul><li class="bulletList level-2">A programming language for LLMs</li>
<li class="bulletList level-2">A unified interface for composing LangChain components</li>
<li class="bulletList level-2">A template system for prompts</li>
<li class="bulletList level-2">A testing framework for LLMs</li>
</ul></li>
<li class="numberedList">Name three types of memory systems available in LangChain</li>
<li class="numberedList">Compare and contrast LLMs and chat models in LangChain. How do their interfaces and use cases differ?</li>
<li class="numberedList">What role do Runnables play in LangChain? How do they contribute to building modular LLM applications?</li>
<li class="numberedList">When running models locally, which factors affect model performance? (Select all that apply)<ul><li class="bulletList level-2">Available RAM</li>
<li class="bulletList level-2">CPU/GPU capabilities</li>
<li class="bulletList level-2">Internet connection speed</li>
<li class="bulletList level-2">Model quantization level</li>
<li class="bulletList level-2">Operating system type</li>
</ul></li>
<li class="numberedList">Compare the following model deployment options and identify scenarios where each would be most appropriate:<ul><li class="bulletList level-2">Cloud-based models (e.g., OpenAI)</li>
<li class="bulletList level-2">Local models with llama.cpp</li>
<li class="bulletList level-2">GPT4All integration</li>
</ul></li>
<li class="numberedList">Design a basic chain using LCEL that would:<ul><li class="bulletList level-2">Take a user question about a product</li>
<li class="bulletList level-2">Query a database for product information</li>
<li class="bulletList level-2">Generate a response using an LLM</li>
</ul></li>
<li class="numberedList">Provide a sketch outlining the components and how they connect.</li>
<li class="numberedList">Compare the following approaches for image analysis and mention the trade-offs between them:<ul><li class="bulletList level-2">Approach A<pre>from langchain_openai import ChatOpenAI
chat = ChatOpenAI(model="gpt-4-vision-preview")</pre></li>
<li class="bulletList level-2">Approach B<pre>from langchain_community.llms import Ollama
local_model = Ollama(model="llava")</pre></li>
</ul></li>
</ol>
<div><div><h1 class="heading-1" id="_idParaDest-63"><a id="_idTextAnchor106"/>Subscribe to our weekly newsletter</h1>
<p class="normal">Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers, and innovators, at <a href="https://packt.link/Q5UyU">https://packt.link/Q5UyU</a>.</p>
<p class="normal"><img alt="" src="img/Newsletter_QRcode1.jpg"/></p>
</div>
</body></html>