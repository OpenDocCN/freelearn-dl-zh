<html><head></head><body>
<div aria-label="25" epub:type="pagebreak" id="page1-4" role="doc-pagebreak"/>
<div id="_idContainer034">
<h1 class="chapterNumber"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 class="chapterTitle" id="_idParaDest-34"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.2.1">First Steps with LangChain</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">In the previous chapter, we explored LLMs and introduced LangChain as a powerful framework for building LLM-powered applications. </span><span class="koboSpan" id="kobo.3.2">We discussed how LLMs have revolutionized natural language processing with their ability to understand context, generate human-like text, and perform complex reasoning. </span><span class="koboSpan" id="kobo.3.3">While these capabilities are impressive, we also examined their limitations—hallucinations, context constraints, and lack of up-to-date knowledge.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.4.1">In this chapter, we’ll move from theory to practice by building our first LangChain application. </span><span class="koboSpan" id="kobo.4.2">We’ll start with the fundamentals: setting up a proper development environment, understanding LangChain’s core components, and creating simple chains. </span><span class="koboSpan" id="kobo.4.3">From there, we’ll explore more advanced capabilities, including running local models for privacy and cost efficiency and building multimodal applications that combine text with visual understanding. </span><span class="koboSpan" id="kobo.4.4">By the end of this chapter, you’ll have a solid foundation in LangChain’s building blocks and be ready to create increasingly sophisticated AI applications in subsequent chapters.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.5.1">To sum up, this chapter will cover the following topics:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.6.1">Setting up dependencies</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.7.1">Exploring LangChain’s building blocks (model interfaces, prompts and templates, and LCEL)</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.8.1">Running local models</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.9.1">Multimodal AI applications</span></li>
</ul>
<div>
<div class="note" id="_idContainer022">
<div aria-label="26" epub:type="pagebreak" id="page2-4" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.10.1">Given the rapid evolution of both LangChain and the broader AI field, we maintain up-to-date code examples and resources in our GitHub repository: </span><a href="https://github.com/benman1/generative_ai_with_langchain"><span class="url"><span class="koboSpan" id="kobo.11.1">https://github.com/benman1/generative_ai_with_langchain</span></span></a><span class="koboSpan" id="kobo.12.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.13.1">For questions or troubleshooting help, please create an issue on GitHub or join our Discord community: </span><a href="https://packt.link/lang"><span class="url"><span class="koboSpan" id="kobo.14.1">https://packt.link/lang</span></span></a><span class="koboSpan" id="kobo.15.1">.</span></p>
</div>
</div>
<h1 class="heading-1" id="_idParaDest-35"><a id="_idTextAnchor046"/><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.16.1">Setting up dependencies for this book</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.17.1">This </span><a id="_idIndexMarker070"/><span class="koboSpan" id="kobo.18.1">book provides multiple options for running the code examples, from zero-setup cloud notebooks to local development environments. </span><span class="koboSpan" id="kobo.18.2">Choose the approach that best fits your experience level and preferences. </span><span class="koboSpan" id="kobo.18.3">Even if you are familiar with dependency management, please read these instructions since all code in this book will depend on the correct installation of the environment as outlined here.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.19.1">For the quickest start with no local setup required, we provide ready-to-use online notebooks for every chapter:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.20.1">Google Colab</span></strong><span class="koboSpan" id="kobo.21.1">: Run </span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.22.1">examples with free GPU access</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.23.1">Kaggle Notebooks</span></strong><span class="koboSpan" id="kobo.24.1">: Experiment </span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.25.1">with integrated datasets</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.26.1">Gradient Notebooks</span></strong><span class="koboSpan" id="kobo.27.1">: Access </span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.28.1">higher-performance compute options</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.29.1">All code examples you find in this book are available as online notebooks on GitHub at </span><a href="https://github.com/benman1/generative_ai_with_langchain"><span class="url"><span class="koboSpan" id="kobo.30.1">https://github.com/benman1/generative_ai_with_langchain</span></span></a><span class="koboSpan" id="kobo.31.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.32.1">These notebooks don’t have all dependencies pre-configured but, usually, a few install commands get you going. </span><span class="koboSpan" id="kobo.32.2">These tools allow you to start experimenting immediately without worrying about setup. </span><span class="koboSpan" id="kobo.32.3">If you prefer working locally, we recommend using conda for environment management:</span></p>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.33.1">Install </span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.34.1">Miniconda if you don’t have it already.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.35.1">Download it from </span><a href="https://docs.conda.io/en/latest/miniconda.html"><span class="url"><span class="koboSpan" id="kobo.36.1">https://docs.conda.io/en/latest/miniconda.html</span></span></a><span class="koboSpan" id="kobo.37.1">.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.38.1">Create a new environment with Python 3.11:</span><p class="snippet-con-one"><span class="koboSpan" id="kobo.39.1">conda create -n langchain-book python=3.11</span></p></li>
<li class="numberedList"><span class="koboSpan" id="kobo.40.1">Activate the environment:</span><p class="snippet-con-one"><span class="koboSpan" id="kobo.41.1">conda activate langchain-book</span></p></li>
<li class="numberedList"><span class="koboSpan" id="kobo.42.1">Install Jupyter and core dependencies:</span><p class="snippet-con-one"><span class="koboSpan" id="kobo.43.1">conda install jupyter</span></p><p class="snippet-con-one"><span class="koboSpan" id="kobo.44.1">pip install langchain langchain-openai jupyter</span></p></li>
<li class="numberedList"><span class="koboSpan" id="kobo.45.1">Launch Jupyter Notebook:</span><p class="snippet-con-one"><span class="koboSpan" id="kobo.46.1">jupyter notebook</span></p></li>
</ol>
<div aria-label="27" epub:type="pagebreak" id="page3-4" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.47.1">This approach provides a clean, isolated environment for working with LangChain. </span><span class="koboSpan" id="kobo.47.2">For experienced developers with established workflows, we also support:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.48.1">pip with venv</span></strong><span class="koboSpan" id="kobo.49.1">: Instructions in the GitHub repository</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.50.1">Docker containers</span></strong><span class="koboSpan" id="kobo.51.1">: Dockerfiles provided in the GitHub repository</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.52.1">Poetry</span></strong><span class="koboSpan" id="kobo.53.1">: Configuration files available in the GitHub repository</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.54.1">Choose </span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.55.1">the method you’re most comfortable with but remember that all examples assume a Python 3.10+ environment with the dependencies listed in requirements.txt.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.56.1">For developers, Docker, which </span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.57.1">provides isolation via containers, is a good option. </span><span class="koboSpan" id="kobo.57.2">The downside is that it uses a lot of disk space and is more complex than the other options. </span><span class="koboSpan" id="kobo.57.3">For data scientists, I’d recommend Conda or Poetry.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.58.1">Conda</span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.59.1"> handles intricate dependencies efficiently, although it can be excruciatingly slow in large environments. </span><span class="koboSpan" id="kobo.59.2">Poetry</span><a id="_idIndexMarker078"/><span class="koboSpan" id="kobo.60.1"> resolves dependencies well and manages environments; however, it doesn’t capture system dependencies.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.61.1">All tools allow sharing and replicating dependencies from configuration files. </span><span class="koboSpan" id="kobo.61.2">You can find a set of instructions and the corresponding configuration files in the book’s repository at </span><a href="https://github.com/benman1/generative_ai_with_langchain"><span class="url"><span class="koboSpan" id="kobo.62.1">https://github.com/benman1/generative_ai_with_langchain</span></span></a><span class="koboSpan" id="kobo.63.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.64.1">Once you are finished, please make sure you have LangChain version 0.3.17 installed. </span><span class="koboSpan" id="kobo.64.2">You can check this with the command </span><code class="inlineCode"><span class="koboSpan" id="kobo.65.1">pip show langchain</span></code><span class="koboSpan" id="kobo.66.1">.</span></p>
<div>
<div class="note" id="_idContainer023">
<p class="normal"><span class="koboSpan" id="kobo.67.1">With the rapid pace of innovation in the LLM field, library updates are frequent. </span><span class="koboSpan" id="kobo.67.2">The code in this book is tested with LangChain 0.3.17, but newer versions may introduce changes. </span><span class="koboSpan" id="kobo.67.3">If you encounter any issues running the examples:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.68.1">Create an issue on our GitHub repository</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.69.1">Join the discussion on Discord at </span><a href="https://packt.link/lang "><span class="url"><span class="koboSpan" id="kobo.70.1">https://packt.link/lang</span></span></a></li>
<li class="bulletList"><span class="koboSpan" id="kobo.71.1">Check the errata on the book’s Packt page</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.72.1">This community support ensures you’ll be able to successfully implement all projects regardless of library </span><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.73.1">updates.</span></p>
</div>
</div>
<div aria-label="28" epub:type="pagebreak" id="page4-4" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-36"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.74.1">API key setup</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.75.1">LangChain’s</span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.76.1"> provider-agnostic approach supports a wide range of LLM providers, each with unique strengths and characteristics. </span><span class="koboSpan" id="kobo.76.2">Unless you use a local LLM, to use these services, you’ll need to obtain the appropriate authentication credentials.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.77.1">Provider</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.78.1">Environment Variable</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.79.1">Setup URL</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.80.1">Free Tier?</span></strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.81.1">OpenAI</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.82.1">OPENAI_API_KEY</span></code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://platform.openai.com/"><span class="url"><span class="koboSpan" id="kobo.83.1">platform.openai.com</span></span></a></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.84.1">No</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.85.1">HuggingFace</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.86.1">HUGGINGFACEHUB_API_TOKEN</span></code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://huggingface.co/settings/tokens"><span class="url"><span class="koboSpan" id="kobo.87.1">huggingface.co/settings/tokens</span></span></a></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.88.1">Yes</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.89.1">Anthropic</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.90.1">ANTHROPIC_API_KEY</span></code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://console.anthropic.com"><span class="url"><span class="koboSpan" id="kobo.91.1">console.anthropic.com</span></span></a></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.92.1">No</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.93.1">Google AI</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.94.1">GOOGLE_API_KEY</span></code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://ai.google.dev/gemini-api"><span class="url"><span class="koboSpan" id="kobo.95.1">ai.google.dev/gemini-api</span></span></a></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.96.1">Yes</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.97.1">Google VertexAI</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.98.1">Application Default Credentials</span></code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://cloud.google.com/vertex-ai"><span class="url"><span class="koboSpan" id="kobo.99.1">cloud.google.com/vertex-ai</span></span></a></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.100.1">Yes (with limits)</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.101.1">Replicate</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.102.1">REPLICATE_API_TOKEN</span></code></p>
</td>
<td class="No-Table-Style">
<p class="normal"><a href="https://replicate.com"><span class="url"><span class="koboSpan" id="kobo.103.1">replicate.com</span></span></a></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.104.1">No</span></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref"><span class="koboSpan" id="kobo.105.1">Table 2.1: API keys reference table (overview)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.106.1">Most providers </span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.107.1">require an API key, while cloud providers like AWS and Google Cloud also support alternative authentication methods like </span><strong class="keyWord"><span class="koboSpan" id="kobo.108.1">Application Default Credentials</span></strong><span class="koboSpan" id="kobo.109.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.110.1">ADC</span></strong><span class="koboSpan" id="kobo.111.1">). </span><span class="koboSpan" id="kobo.111.2">Many</span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.112.1"> providers offer free tiers without requiring credit card details, making it easy to get started.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.113.1">To set an API key in an environment, in Python, we can execute the following lines:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.114.1">import</span></span><span class="koboSpan" id="kobo.115.1"> os</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.116.1">os.environ[</span><span class="hljs-string"><span class="koboSpan" id="kobo.117.1">"OPENAI_API_KEY"</span></span><span class="koboSpan" id="kobo.118.1">] = </span><span class="hljs-string"><span class="koboSpan" id="kobo.119.1">"&lt;your token&gt;"</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.120.1">Here, </span><code class="inlineCode"><span class="koboSpan" id="kobo.121.1">OPENAI_API_KEY</span></code><span class="koboSpan" id="kobo.122.1"> is the </span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.123.1">environment key that is appropriate for OpenAI. </span><span class="koboSpan" id="kobo.123.2">Setting the keys in your environment has the advantage of not needing to include them as parameters in your code every time you use a model or service integration.</span></p>
<div aria-label="29" epub:type="pagebreak" id="page5-4" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.124.1">You can also expose these variables in your system environment from your terminal. </span><span class="koboSpan" id="kobo.124.2">In Linux and macOS, you can set a system environment variable from the terminal using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.125.1">export </span></code><span class="koboSpan" id="kobo.126.1">command:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.127.1">export OPENAI_API_KEY=&lt;your token&gt;</span></p>
<p class="normal"><span class="koboSpan" id="kobo.128.1">To permanently set the environment variable in Linux or macOS, you would need to add the preceding line to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.129.1">~/.bashrc</span></code><span class="koboSpan" id="kobo.130.1"> or </span><code class="inlineCode"><span class="koboSpan" id="kobo.131.1">~/.bash_profile</span></code><span class="koboSpan" id="kobo.132.1"> files, and then reload the shell using the command </span><code class="inlineCode"><span class="koboSpan" id="kobo.133.1">source ~/.bashrc</span></code><span class="koboSpan" id="kobo.134.1"> or </span><code class="inlineCode"><span class="koboSpan" id="kobo.135.1">source ~/.bash_profile</span></code><span class="koboSpan" id="kobo.136.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.137.1">For Windows users, you can set the environment variable by searching for “Environment Variables” in the system settings, editing either “User variables” or “System variables,” and adding </span><code class="inlineCode"><span class="koboSpan" id="kobo.138.1">export</span></code> <code class="inlineCode"><span class="koboSpan" id="kobo.139.1">OPENAI_API_KEY=your_key_here</span></code><span class="koboSpan" id="kobo.140.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.141.1">Our choice is to </span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.142.1">create a </span><code class="inlineCode"><span class="koboSpan" id="kobo.143.1">config.py</span></code><span class="koboSpan" id="kobo.144.1"> file where all API keys are stored. </span><span class="koboSpan" id="kobo.144.2">We then import a function from this module that loads these keys into the environment variables. </span><span class="koboSpan" id="kobo.144.3">This approach centralizes credential management and makes it easier to update keys when needed:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.145.1">import</span></span><span class="koboSpan" id="kobo.146.1"> os</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.147.1">OPENAI_API_KEY =  </span><span class="hljs-string"><span class="koboSpan" id="kobo.148.1">"... </span><span class="koboSpan" id="kobo.148.2">"</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.149.1"># I'm omitting all other keys</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.150.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.151.1">set_environment</span></span><span class="koboSpan" id="kobo.152.1">():</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.153.1">    variable_dict = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.154.1">globals</span></span><span class="koboSpan" id="kobo.155.1">().items()</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.156.1">for</span></span><span class="koboSpan" id="kobo.157.1"> key, value </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.158.1">in</span></span><span class="koboSpan" id="kobo.159.1"> variable_dict:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.160.1">if</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.161.1">"API"</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.162.1">in</span></span><span class="koboSpan" id="kobo.163.1"> key </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.164.1">or</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.165.1">"ID"</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.166.1">in</span></span><span class="koboSpan" id="kobo.167.1"> key:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.168.1">             os.environ[key] = value</span></p>
<p class="normal"><span class="koboSpan" id="kobo.169.1">If you search for this file in the GitHub repository, you’ll notice it’s missing. </span><span class="koboSpan" id="kobo.169.2">This is intentional – I’ve excluded it from Git tracking using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.170.1">.gitignore</span></code><span class="koboSpan" id="kobo.171.1"> file. </span><span class="koboSpan" id="kobo.171.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.172.1">.gitignore</span></code><span class="koboSpan" id="kobo.173.1"> file tells Git which files to ignore when committing changes, which is essential for:</span></p>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.174.1">Preventing sensitive credentials from being publicly exposed</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.175.1">Avoiding accidental commits of personal API keys</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.176.1">Protecting yourself from unauthorized usage charges</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.177.1">To implement this yourself, simply add </span><code class="inlineCode"><span class="koboSpan" id="kobo.178.1">config.py</span></code><span class="koboSpan" id="kobo.179.1"> to your </span><code class="inlineCode"><span class="koboSpan" id="kobo.180.1">.gitignore</span></code><span class="koboSpan" id="kobo.181.1"> file:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.182.1"># In .gitignore</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.183.1">config.py</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.184.1">.env</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.185.1">**/api_keys.txt</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.186.1"># Other sensitive files</span></span></p>
<div aria-label="30" epub:type="pagebreak" id="page6-4" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.187.1">You can set all your keys in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.188.1">config.py</span></code><span class="koboSpan" id="kobo.189.1"> file. </span><span class="koboSpan" id="kobo.189.2">This function, </span><code class="inlineCode"><span class="koboSpan" id="kobo.190.1">set_environment()</span></code><span class="koboSpan" id="kobo.191.1">, loads all the keys into the environment as mentioned. </span><span class="koboSpan" id="kobo.191.2">Anytime you want to run an application, you import the function and run it like so:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.192.1">from</span></span><span class="koboSpan" id="kobo.193.1"> config </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.194.1">import</span></span><span class="koboSpan" id="kobo.195.1"> set_environment</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.196.1">set_environment()</span></p>
<p class="normal"><span class="koboSpan" id="kobo.197.1">For production environments, consider using dedicated secrets management services or environment variables injected at runtime. </span><span class="koboSpan" id="kobo.197.2">These approaches provide additional security while maintaining the separation between code and credentials.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.198.1">While OpenAI’s models</span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.199.1"> remain influential, the LLM ecosystem has rapidly diversified, offering developers multiple options for their applications. </span><span class="koboSpan" id="kobo.199.2">To maintain clarity, we’ll separate LLMs</span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.200.1"> from the model gateways that provide access to them.</span></p>
<div aria-label="31" epub:type="pagebreak" id="page7-3" role="doc-pagebreak"/>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.201.1">Key LLM families</span></strong><ul><li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.202.1">Anthropic Claude</span></strong><span class="koboSpan" id="kobo.203.1">: Excels in</span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.204.1"> reasoning, long-form content processing, and vision analysis with up to 200K token context windows</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.205.1">Mistral models</span></strong><span class="koboSpan" id="kobo.206.1">: Powerful </span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.207.1">open-source models with strong multilingual capabilities and exceptional reasoning abilities</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.208.1">Google Gemini</span></strong><span class="koboSpan" id="kobo.209.1">: Advanced</span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.210.1"> multimodal models with industry-leading 1M token context window and real-time information access</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.211.1">OpenAI GPT-o</span></strong><span class="koboSpan" id="kobo.212.1">: Leading</span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.213.1"> omnimodal capabilities accepting text, audio, image, and video with enhanced reasoning</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.214.1">DeepSeek models:</span></strong><span class="koboSpan" id="kobo.215.1"> Specialized</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.216.1"> in coding and technical reasoning with state-of-the-art performance on programming tasks</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.217.1">AI21 Labs Jurassic:</span></strong><span class="koboSpan" id="kobo.218.1"> Strong in</span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.219.1"> academic applications and long-form content generation</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.220.1">Inflection Pi</span></strong><span class="koboSpan" id="kobo.221.1">: Optimized </span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.222.1">for conversational AI with exceptional emotional intelligence</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.223.1">Perplexity models</span></strong><span class="koboSpan" id="kobo.224.1">: Focused</span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.225.1"> on accurate, cited answers for research applications</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.226.1">Cohere models</span></strong><span class="koboSpan" id="kobo.227.1">: Specialized </span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.228.1">for enterprise applications with strong multilingual capabilities</span></li>
</ul></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.229.1">Cloud provider gateways</span></strong><ul><li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.230.1">Amazon Bedrock</span></strong><span class="koboSpan" id="kobo.231.1">: Unified </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.232.1">API access to models from Anthropic, AI21, Cohere, Mistral, and </span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.233.1">others with AWS integration</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.234.1">Azure OpenAI Service</span></strong><span class="koboSpan" id="kobo.235.1">: Enterprise-grade</span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.236.1"> access to OpenAI and other models with robust security and Microsoft ecosystem integration</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.237.1">Google Vertex AI</span></strong><span class="koboSpan" id="kobo.238.1">: Access to </span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.239.1">Gemini and other models with seamless Google Cloud integration</span></li>
</ul></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.240.1">Independent platforms</span></strong><ul><li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.241.1">Together AI</span></strong><span class="koboSpan" id="kobo.242.1">: Hosts 200+ open-source </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.243.1">models with both serverless and dedicated GPU options</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.244.1">Replicate</span></strong><span class="koboSpan" id="kobo.245.1">: Specializes</span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.246.1"> in deploying multimodal open-source models with pay-as-you-go pricing</span></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.247.1">HuggingFace Inference Endpoints</span></strong><span class="koboSpan" id="kobo.248.1">: Production deployment of thousands of open-source models </span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.249.1">with fine-tuning capabilities</span></li>
</ul></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.250.1">Throughout this book, we’ll work with various models accessed through different providers, giving you the</span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.251.1"> flexibility to choose the best option for your specific needs and infrastructure requirements.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.252.1">We will use OpenAI for many applications but will also try LLMs from other organizations. </span><span class="koboSpan" id="kobo.252.2">Refer to the </span><em class="italic"><span class="koboSpan" id="kobo.253.1">Appendix</span></em><span class="koboSpan" id="kobo.254.1"> at the end of the book to learn how to get API keys for OpenAI, Hugging Face, Google, and other providers.</span></p>
<div>
<div class="note" id="_idContainer024">
<p class="normal"><span class="koboSpan" id="kobo.255.1">There are two main integration packages:</span></p>
<ul>
<li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.256.1">langchain-google-vertexai</span></code></li>
<li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.257.1">langchain-google-genai</span></code></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.258.1">We’ll be using </span><code class="inlineCode"><span class="koboSpan" id="kobo.259.1">langchain-google-genai</span></code><span class="koboSpan" id="kobo.260.1">, the package recommended by LangChain for individual developers. </span><span class="koboSpan" id="kobo.260.2">The setup is a lot simpler, only requiring a Google account and API key. </span><span class="koboSpan" id="kobo.260.3">It is recommended to move to </span><code class="inlineCode"><span class="koboSpan" id="kobo.261.1">langchain-google-vertexai</span></code><span class="koboSpan" id="kobo.262.1"> for larger projects. </span><span class="koboSpan" id="kobo.262.2">This integration offers enterprise features such as customer encryption keys, virtual private cloud integration, and more, requiring a Google Cloud account with billing.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.263.1">If you’ve followed the instructions on GitHub, as indicated in the previous section, you should already have the </span><code class="inlineCode"><span class="koboSpan" id="kobo.264.1">langchain-google-ge</span><a id="_idTextAnchor050"/><a id="_idTextAnchor051"/><a id="_idTextAnchor052"/><a id="_idTextAnchor053"/><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.265.1">nai</span></code><span class="koboSpan" id="kobo.266.1"> package installed.</span></p>
</div>
</div>
<div aria-label="32" epub:type="pagebreak" id="page8-3" role="doc-pagebreak"/>
<h1 class="heading-1" id="_idParaDest-37"><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.267.1">Exploring LangChain’s building blocks</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.268.1">To build </span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.269.1">practical applications, we need to know how to work with different model providers. </span><span class="koboSpan" id="kobo.269.2">Let’s explore the various options available, from cloud services to local deployments. </span><span class="koboSpan" id="kobo.269.3">We’ll start with fundamental concepts like LLMs and chat models, then dive into prompts, chai</span><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.270.1">ns, and memory systems.</span></p>
<h2 class="heading-2" id="_idParaDest-38"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.271.1">Model interfaces</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.272.1">LangChain provides a unified interface for working with various LLM providers. </span><span class="koboSpan" id="kobo.272.2">This abstraction makes it easy to switch between different models while maintaining a consistent code structure. </span><span class="koboSpan" id="kobo.272.3">The</span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.273.1"> following examples demonstrate how to implement</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.274.1"> LangChain’s core components in practical scenarios.</span></p>
<div>
<div class="note" id="_idContainer025">
<p class="normal"><span class="koboSpan" id="kobo.275.1">Please note that users should almost exclusively be using the newer chat models as most model providers have adopted a chat-like interface for interacting with language models. </span><span class="koboSpan" id="kobo.275.2">We still provide the LLM interface, because it’s very easy to use </span><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.276.1">as string-in, string-out.</span></p>
</div>
</div>
<h3 class="heading-3" id="_idParaDest-39"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.277.1">LLM interaction patterns</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.278.1">The </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.279.1">LLM interface represents traditional text completion models that take a string input and return a string output. </span><span class="koboSpan" id="kobo.279.2">More and more use cases in LangChain use only the ChatModel interface, mainly because it’s better suited for building complex workflows and developing agents. </span><span class="koboSpan" id="kobo.279.3">The LangChain documentation is now deprecating the LLM interface and recommending the use of chat-based interfaces. </span><span class="koboSpan" id="kobo.279.4">While this chapter demonstrates both interfaces, we recommend using chat models as they represent the current standard to be up to date with LangChain.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.280.1">Let’s see the LLM interface in action:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.281.1">from</span></span><span class="koboSpan" id="kobo.282.1"> langchain_openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.283.1">import</span></span><span class="koboSpan" id="kobo.284.1"> OpenAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.285.1">from</span></span><span class="koboSpan" id="kobo.286.1"> langchain_google_genai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.287.1">import</span></span><span class="koboSpan" id="kobo.288.1"> GoogleGenerativeAI</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.289.1"># Initialize OpenAI model</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.290.1">openai_llm = OpenAI()</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.291.1"># Initialize a Gemini model</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.292.1">gemini_pro = GoogleGenerativeAI(model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.293.1">"gemini-1.5-pro"</span></span><span class="koboSpan" id="kobo.294.1">)</span></p>
<div aria-label="33" epub:type="pagebreak" id="page9-2" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.295.1"># Either one or both can be used with the same interface</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.296.1">response = openai_llm.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.297.1">"Tell me a joke about light bulbs!"</span></span><span class="koboSpan" id="kobo.298.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.299.1">print</span></span><span class="koboSpan" id="kobo.300.1">(response)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.301.1">Please note that you must set your environment variables to the provider keys when you run this. </span><span class="koboSpan" id="kobo.301.2">For example, when running this I’d start the file by calling </span><code class="inlineCode"><span class="koboSpan" id="kobo.302.1">set_environment() from config</span></code><span class="koboSpan" id="kobo.303.1">:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.304.1">from</span></span><span class="koboSpan" id="kobo.305.1"> config </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.306.1">import</span></span><span class="koboSpan" id="kobo.307.1"> set_environment</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.308.1">set_environment()</span></p>
<p class="normal"><span class="koboSpan" id="kobo.309.1">We get this output:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.310.1">Why did the light bulb go to therapy?</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.311.1">Because it was feeling a little dim!</span></p>
<p class="normal"><span class="koboSpan" id="kobo.312.1">For the Gemini model, we can run:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.313.1">response = gemini_pro.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.314.1">"Tell me a joke about light bulbs!"</span></span><span class="koboSpan" id="kobo.315.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.316.1">For me, Gemini</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.317.1"> comes up with this joke:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.318.1">Why did the light bulb get a speeding ticket?</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.319.1">Because it was caught going over the watt limit!</span></p>
<p class="normal"><span class="koboSpan" id="kobo.320.1">Notice how we use the same </span><code class="inlineCode"><span class="koboSpan" id="kobo.321.1">invoke()</span></code><span class="koboSpan" id="kobo.322.1"> method regardless of the provider. </span><span class="koboSpan" id="kobo.322.2">This consistency makes it easy to experiment with different models or swit</span><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.323.1">ch providers in production.</span></p>
<h3 class="heading-3" id="_idParaDest-40"><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.324.1">Development testing</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.325.1">During development, you</span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.326.1"> might want to test your application without making actual API calls. </span><span class="koboSpan" id="kobo.326.2">LangChain provides </span><code class="inlineCode"><span class="koboSpan" id="kobo.327.1">F</span><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.328.1">akeListLLM</span></code><span class="koboSpan" id="kobo.329.1"> for this purpose:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.330.1">from</span></span><span class="koboSpan" id="kobo.331.1"> langchain_community.llms </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.332.1">import</span></span><span class="koboSpan" id="kobo.333.1"> FakeListLLM</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.334.1"># Create a fake LLM that always returns the same response</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.335.1">fake_llm = FakeListLLM(responses=[</span><span class="hljs-string"><span class="koboSpan" id="kobo.336.1">"Hello"</span></span><span class="koboSpan" id="kobo.337.1">])</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.338.1">result = fake_llm.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.339.1">"Any input will return Hello"</span></span><span class="koboSpan" id="kobo.340.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.341.1">print</span></span><span class="koboSpan" id="kobo.342.1">(result)  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.343.1"># Output: Hello</span></span></p>
<div aria-label="34" epub:type="pagebreak" id="page10-2" role="doc-pagebreak"/>
<h3 class="heading-3" id="_idParaDest-41"><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.344.1">Working with chat models</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.345.1">Chat models </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.346.1">are LLMs that are fine-tuned for multi-turn interaction between a model and a human. </span><span class="koboSpan" id="kobo.346.2">These days most LLMs are fine-tuned for multi-turned conversations. </span><span class="koboSpan" id="kobo.346.3">Instead of providing input to the model, such as:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.347.1">human: turn1</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.348.1">ai: answer1</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.349.1">human: turn2</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.350.1">ai: answer2</span></p>
<p class="normal"><span class="koboSpan" id="kobo.351.1">where we expect it to generate an output by continuing the conversation, these days model providers typically expose an API that expects each turn as a separate well-formatted part of the payload. </span><span class="koboSpan" id="kobo.351.2">Model providers typically don’t store the chat history server-side, they get the full history sent each time from the client and only format the final prompt server-side.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.352.1">LangChain follows the same pattern with ChatModels, processing conversations through structured messages with roles and content. </span><span class="koboSpan" id="kobo.352.2">Each message contains:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.353.1">Role (who’s speaking), which is defined by the message class (all messages inherit from BaseMessage)</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.354.1">Content (what’s being said)</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.355.1">Message </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.356.1">types include:</span></p>
<ul>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.357.1">SystemMessage</span></code><span class="koboSpan" id="kobo.358.1">: Sets behavior and context for the model. </span><span class="koboSpan" id="kobo.358.2">Example:</span><p class="snippet-code-one"><code class="inlineCode"><span class="koboSpan" id="kobo.359.1">SystemMessage(content=</span></code><span class="hljs-string"><span class="koboSpan" id="kobo.360.1">"You're a helpful programming assistant"</span></span><code class="inlineCode"><span class="koboSpan" id="kobo.361.1">)</span></code></p></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.362.1">HumanMessage</span></code><span class="koboSpan" id="kobo.363.1">: Represents user input like questions, commands, and data. </span><span class="koboSpan" id="kobo.363.2">Example:</span><p class="snippet-code-one"><span class="koboSpan" id="kobo.364.1">HumanMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.365.1">"Write a Python function to calculate factorial"</span></span><span class="koboSpan" id="kobo.366.1">)</span></p></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.367.1">AIMessage</span></code><span class="koboSpan" id="kobo.368.1">: Contains model responses</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.369.1">Let’s see this in action:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.370.1">from</span></span><span class="koboSpan" id="kobo.371.1"> langchain_anthropic </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.372.1">import</span></span><span class="koboSpan" id="kobo.373.1"> ChatAnthropic</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.374.1">from</span></span><span class="koboSpan" id="kobo.375.1"> langchain_core.messages </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.376.1">import</span></span><span class="koboSpan" id="kobo.377.1"> SystemMessage, HumanMessage</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.378.1">chat = ChatAnthropic(model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.379.1">"claude-3-opus-20240229"</span></span><span class="koboSpan" id="kobo.380.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.381.1">messages = [</span></p>
<div aria-label="35" epub:type="pagebreak" id="page11-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.382.1">    SystemMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.383.1">"You're a helpful programming assistant"</span></span><span class="koboSpan" id="kobo.384.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.385.1">    HumanMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.386.1">"Write a Python function to calculate factorial"</span></span><span class="koboSpan" id="kobo.387.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.388.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.389.1">response = chat.invoke(messages)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.390.1">print</span></span><span class="koboSpan" id="kobo.391.1">(response)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.392.1">Claude comes up with a function, an explanation, and examples for calling the function.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.393.1">Here’s a Python function that calculates the factorial of a given number:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.394.1">```python</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.395.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.396.1">factorial</span></span><span class="koboSpan" id="kobo.397.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.398.1">n</span></span><span class="koboSpan" id="kobo.399.1">):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.400.1">if</span></span><span class="koboSpan" id="kobo.401.1"> n &lt; </span><span class="hljs-number"><span class="koboSpan" id="kobo.402.1">0</span></span><span class="koboSpan" id="kobo.403.1">:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.404.1">raise</span></span><span class="koboSpan" id="kobo.405.1"> ValueError(</span><span class="hljs-string"><span class="koboSpan" id="kobo.406.1">"Factorial is not defined for negative numbers."</span></span><span class="koboSpan" id="kobo.407.1">)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.408.1">elif</span></span><span class="koboSpan" id="kobo.409.1"> n == </span><span class="hljs-number"><span class="koboSpan" id="kobo.410.1">0</span></span><span class="koboSpan" id="kobo.411.1">:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.412.1">return</span></span> <span class="hljs-number"><span class="koboSpan" id="kobo.413.1">1</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.414.1">else</span></span><span class="koboSpan" id="kobo.415.1">:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.416.1">        result = </span><span class="hljs-number"><span class="koboSpan" id="kobo.417.1">1</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.418.1">for</span></span><span class="koboSpan" id="kobo.419.1"> i </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.420.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.421.1">range</span></span><span class="koboSpan" id="kobo.422.1">(</span><span class="hljs-number"><span class="koboSpan" id="kobo.423.1">1</span></span><span class="koboSpan" id="kobo.424.1">, n + </span><span class="hljs-number"><span class="koboSpan" id="kobo.425.1">1</span></span><span class="koboSpan" id="kobo.426.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.427.1">            result *= i</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.428.1">return</span></span><span class="koboSpan" id="kobo.429.1"> result</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.430.1">```</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.431.1">Let's break that down. </span><span class="koboSpan" id="kobo.431.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.432.1">factorial</span></code><span class="koboSpan" id="kobo.433.1"> function is designed to take an integer </span><code class="inlineCode"><span class="koboSpan" id="kobo.434.1">n</span></code><span class="koboSpan" id="kobo.435.1"> as input and calculate its factorial. </span><span class="koboSpan" id="kobo.435.2">It starts by checking if </span><code class="inlineCode"><span class="koboSpan" id="kobo.436.1">n</span></code><span class="koboSpan" id="kobo.437.1"> is negative, and if so, it raises a </span><code class="inlineCode"><span class="koboSpan" id="kobo.438.1">ValueError</span></code><span class="koboSpan" id="kobo.439.1"> since factorials aren't defined for negative numbers. </span><span class="koboSpan" id="kobo.439.2">If </span><code class="inlineCode"><span class="koboSpan" id="kobo.440.1">n</span></code><span class="koboSpan" id="kobo.441.1"> is zero, the function returns </span><code class="inlineCode"><span class="koboSpan" id="kobo.442.1">1</span></code><span class="koboSpan" id="kobo.443.1">, which makes sense because, by definition, the factorial of 0 is 1.</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.444.1">When dealing with positive numbers, the function kicks things off by setting a variable </span><code class="inlineCode"><span class="koboSpan" id="kobo.445.1">result</span></code><span class="koboSpan" id="kobo.446.1"> to 1. </span><span class="koboSpan" id="kobo.446.2">From there, it enters a loop that runs from 1 to </span><code class="inlineCode"><span class="koboSpan" id="kobo.447.1">n</span></code><span class="koboSpan" id="kobo.448.1">, inclusive, thanks to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.449.1">range</span></code><span class="koboSpan" id="kobo.450.1"> function. </span><span class="koboSpan" id="kobo.450.2">During each step of the loop, it multiplies the result by the current number, gradually building up the factorial. </span><span class="koboSpan" id="kobo.450.3">Once</span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.451.1"> the loop completes, the function returns the final calculated value. </span><span class="koboSpan" id="kobo.451.2">You can call this function by providing a non-negative integer as an argument. </span><span class="koboSpan" id="kobo.451.3">Here are a few examples:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.452.1">```python</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.453.1">print</span></span><span class="koboSpan" id="kobo.454.1">(factorial(</span><span class="hljs-number"><span class="koboSpan" id="kobo.455.1">0</span></span><span class="koboSpan" id="kobo.456.1">))  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.457.1"># Output: 1</span></span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.458.1">print</span></span><span class="koboSpan" id="kobo.459.1">(factorial(</span><span class="hljs-number"><span class="koboSpan" id="kobo.460.1">5</span></span><span class="koboSpan" id="kobo.461.1">))  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.462.1"># Output: 120</span></span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.463.1">print</span></span><span class="koboSpan" id="kobo.464.1">(factorial(</span><span class="hljs-number"><span class="koboSpan" id="kobo.465.1">10</span></span><span class="koboSpan" id="kobo.466.1">))  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.467.1"># Output: 3628800</span></span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.468.1">print</span></span><span class="koboSpan" id="kobo.469.1">(factorial(-</span><span class="hljs-number"><span class="koboSpan" id="kobo.470.1">5</span></span><span class="koboSpan" id="kobo.471.1">))  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.472.1"># Raises ValueError: Factorial is not defined for negative numbers.</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.473.1">```</span></p>
<div aria-label="36" epub:type="pagebreak" id="page12-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.474.1">Note that the factorial function grows very quickly, so calculating the factorial of large numbers may exceed the maximum representable value in Python. </span><span class="koboSpan" id="kobo.474.2">In such cases, you might need to use a different approach or a library that supports arbitrary-precision arithmetic.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.475.1">Similarly, we could have asked an OpenAI</span><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.476.1"> model such as GPT-4 or GPT-4o:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.477.1">from</span></span><span class="koboSpan" id="kobo.478.1"> langchain_openai.chat_models </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.479.1">import</span></span><span class="koboSpan" id="kobo.480.1"> ChatOpenAI</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.481.1">chat = ChatOpenAI(model_name=</span><span class="hljs-string"><span class="koboSpan" id="kobo.482.1">'gpt-4o'</span></span><span class="koboSpan" id="kobo.483.1">)</span></p>
<h3 class="heading-3" id="_idParaDest-42"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.484.1">Reasoning models</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.485.1">Anthropic’s Claude 3.7 Sonnet</span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.486.1"> introduces a powerful capability called </span><em class="italic"><span class="koboSpan" id="kobo.487.1">extended thinking</span></em><span class="koboSpan" id="kobo.488.1"> that allows the model to show its reasoning process before delivering a final answer. </span><span class="koboSpan" id="kobo.488.2">This feature represents a significant advancement in how developers can leverage LLMs for complex reasoning tasks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.489.1">Here’s how to configure extended thinking through the ChatAnthropic class:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.490.1">from</span></span><span class="koboSpan" id="kobo.491.1"> langchain_anthropic </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.492.1">import</span></span><span class="koboSpan" id="kobo.493.1"> ChatAnthropic</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.494.1">from</span></span><span class="koboSpan" id="kobo.495.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.496.1">import</span></span><span class="koboSpan" id="kobo.497.1"> ChatPromptTemplate</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.498.1"># Create a template</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.499.1">template = ChatPromptTemplate.from_messages([</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.500.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.501.1">"system"</span></span><span class="koboSpan" id="kobo.502.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.503.1">"You are an experienced programmer and mathematical analyst."</span></span><span class="koboSpan" id="kobo.504.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.505.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.506.1">"user"</span></span><span class="koboSpan" id="kobo.507.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.508.1">"{problem}"</span></span><span class="koboSpan" id="kobo.509.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.510.1">])</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.511.1"># Initialize Claude with extended thinking enabled</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.512.1">chat = ChatAnthropic(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.513.1">    model_name=</span><span class="hljs-string"><span class="koboSpan" id="kobo.514.1">"claude-3-7-sonnet-20240326"</span></span><span class="koboSpan" id="kobo.515.1">,  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.516.1"># Use latest model version</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.517.1">    max_tokens=</span><span class="hljs-number"><span class="koboSpan" id="kobo.518.1">64_000</span></span><span class="koboSpan" id="kobo.519.1">,                        </span><span class="hljs-comment"><span class="koboSpan" id="kobo.520.1"># Total response length limit</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.521.1">    thinking={</span><span class="hljs-string"><span class="koboSpan" id="kobo.522.1">"type"</span></span><span class="koboSpan" id="kobo.523.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.524.1">"enabled"</span></span><span class="koboSpan" id="kobo.525.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.526.1">"budget_tokens"</span></span><span class="koboSpan" id="kobo.527.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.528.1">15000</span></span><span class="koboSpan" id="kobo.529.1">},  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.530.1"># Allocate tokens for thinking</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.531.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.532.1"># Create and run a chain</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.533.1">chain = template | chat</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.534.1"># Complex algorithmic problem</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.535.1">problem = </span><span class="hljs-string"><span class="koboSpan" id="kobo.536.1">"""</span></span></p>
<div aria-label="37" epub:type="pagebreak" id="page13-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.537.1">Design an algorithm to find the kth largest element in an unsorted array</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.538.1">with the optimal time complexity. </span><span class="koboSpan" id="kobo.538.2">Analyze the time and space complexity</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.539.1">of your solution and explain why it's optimal.</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.540.1">"""</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.541.1"># Get response with thinking included</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.542.1">response = chat.invoke([HumanMessage(content=problem)])</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.543.1">print</span></span><span class="koboSpan" id="kobo.544.1">(response.content)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.545.1">The</span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.546.1"> response will include Claude’s step-by-step reasoning about algorithm selection, complexity analysis, and optimization considerations before presenting its final solution. </span><span class="koboSpan" id="kobo.546.2">In the preceding example:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.547.1">Out of the 64,000-token maximum response length, up to 15,000 tokens can be used for Claude’s thinking process.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.548.1">The remaining ~49,000 tokens are available for the final response.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.549.1">Claude doesn’t always use the entire thinking budget—it uses what it needs for the specific task. </span><span class="koboSpan" id="kobo.549.2">If Claude runs out of thinking tokens, it will transition to its final answer.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.550.1">While Claude offers explicit thinking configuration, you can achieve similar (though not identical) results with other providers through different techniques:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.551.1">from</span></span><span class="koboSpan" id="kobo.552.1"> langchain_openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.553.1">import</span></span><span class="koboSpan" id="kobo.554.1"> ChatOpenAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.555.1">from</span></span><span class="koboSpan" id="kobo.556.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.557.1">import</span></span><span class="koboSpan" id="kobo.558.1"> ChatPromptTemplate</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.559.1">template = ChatPromptTemplate.from_messages([</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.560.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.561.1">"system"</span></span><span class="koboSpan" id="kobo.562.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.563.1">"You are a problem-solving assistant."</span></span><span class="koboSpan" id="kobo.564.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.565.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.566.1">"user"</span></span><span class="koboSpan" id="kobo.567.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.568.1">"{problem}"</span></span><span class="koboSpan" id="kobo.569.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.570.1">])</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.571.1"># Initialize with reasoning_effort parameter</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.572.1">chat = ChatOpenAI(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.573.1">    model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.574.1">"o3-mini"</span></span><span class="koboSpan" id="kobo.575.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.576.1">"</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.577.1">    reasoning_effort="</span></span><span class="koboSpan" id="kobo.578.1">high</span><span class="hljs-string"><span class="koboSpan" id="kobo.579.1">"  # Options: "</span></span><span class="koboSpan" id="kobo.580.1">low</span><span class="hljs-string"><span class="koboSpan" id="kobo.581.1">", "</span></span><span class="koboSpan" id="kobo.582.1">medium</span><span class="hljs-string"><span class="koboSpan" id="kobo.583.1">", "</span></span><span class="koboSpan" id="kobo.584.1">high</span><span class="hljs-string"><span class="koboSpan" id="kobo.585.1">"</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.586.1">)</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.587.1">chain = template | chat</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.588.1">response = chain.invoke({"</span></span><span class="koboSpan" id="kobo.589.1">problem</span><span class="hljs-string"><span class="koboSpan" id="kobo.590.1">": "</span></span><span class="koboSpan" id="kobo.591.1">Calculate the optimal strategy </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.592.1">for</span></span><span class="koboSpan" id="kobo.593.1">...</span><span class="hljs-string"><span class="koboSpan" id="kobo.594.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.595.1">})</span></span></p>
<div aria-label="38" epub:type="pagebreak" id="page14-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.596.1">chat = ChatOpenAI(model="</span></span><span class="koboSpan" id="kobo.597.1">gpt-4o</span><span class="hljs-string"><span class="koboSpan" id="kobo.598.1">")</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.599.1">chain = template | chat</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.600.1">response = chain.invoke({"</span></span><span class="koboSpan" id="kobo.601.1">problem</span><span class="hljs-string"><span class="koboSpan" id="kobo.602.1">": "</span></span><span class="koboSpan" id="kobo.603.1">Calculate the optimal strategy </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.604.1">for</span></span><span class="koboSpan" id="kobo.605.1">...</span><span class="hljs-string"><span class="koboSpan" id="kobo.606.1">"})</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.607.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.608.1">reasoning_effort</span></code><span class="koboSpan" id="kobo.609.1"> parameter streamlines your workflow by eliminating the need for complex</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.610.1"> reasoning prompts, allows you to adjust performance by reducing effort when speed matters more than detailed analysis, and helps manage token consumption by controlling how much processing power goes toward reasoning processes.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.611.1">DeepSeek models also offer explicit thinking configuration </span><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.612.1">through the LangChain integration.</span></p>
<h3 class="heading-3" id="_idParaDest-43"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.613.1">Controlling model behavior</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.614.1">Understanding </span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.615.1">how to control an LLM’s behavior is crucial for tailoring its output to specific needs. </span><span class="koboSpan" id="kobo.615.2">Without careful parameter adjustments, the model might produce overly creative, inconsistent, or verbose responses that are unsuitable for practical applications. </span><span class="koboSpan" id="kobo.615.3">For instance, in customer service, you’d want consistent, factual answers, while in content generation, you might aim for more creative and promotional outputs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.616.1">LLMs offer several parameters that allow fine-grained control over generation behavior, though exact implementation may vary between providers. </span><span class="koboSpan" id="kobo.616.2">Let’s explore the most important ones:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.617.1">Parameter</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.618.1">Description</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.619.1">Typical Range</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.620.1">Best For</span></strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.621.1">Temperature</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.622.1">Controls randomness in text generation</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.623.1">0.0-1.0 (OpenAI, Anthropic)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.624.1">0.0-2.0 (Gemini)</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.625.1">Lower (0.0-0.3): Factual tasks, Q&amp;A</span></p>
<p class="normal"><span class="koboSpan" id="kobo.626.1">Higher (0.7+): Creative writing, brainstorming</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.627.1">Top-k</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.628.1">Limits token selection to k most probable tokens</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.629.1">1-100</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.630.1">Lower values (1-10): More focused outputs</span></p>
<p class="normal"><span class="koboSpan" id="kobo.631.1">Higher values: More diverse completions</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.632.1">Top-p (Nucleus Sampling)</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.633.1">Considers tokens until cumulative probability reaches threshold</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.634.1">0.0-1.0</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.635.1">Lower values (0.5): More focused outputs</span></p>
<p class="normal"><span class="koboSpan" id="kobo.636.1">Higher values (0.9): More exploratory responses</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<div aria-label="39" epub:type="pagebreak" id="page15-1" role="doc-pagebreak"/>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.637.1">Max tokens</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.638.1">Limits maximum response length</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.639.1">Model-specific</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.640.1">Controlling costs and preventing verbose outputs</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.641.1">Presence/frequency penalties</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.642.1">Discourages repetition by penalizing tokens that have appeared</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.643.1">-2.0 to 2.0</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.644.1">Longer content generation where repetition is undesirable</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.645.1">Stop sequences</span></strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.646.1">Tells model when to stop generating</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.647.1">Custom strings</span></p>
</td>
<td class="No-Table-Style">
<p class="normal"><span class="koboSpan" id="kobo.648.1">Controlling exact ending points of generation</span></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref"><span class="koboSpan" id="kobo.649.1">Table 2.2: Parameters offered by LLMs</span></p>
<p class="normal"><span class="koboSpan" id="kobo.650.1">These</span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.651.1"> parameters work together to shape model output:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.652.1">Temperature + Top-k/Top-p</span></strong><span class="koboSpan" id="kobo.653.1">: First, Top-k/Top-p filter the token distribution, and then temperature affects randomness within that filtered set</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.654.1">Penalties + Temperature</span></strong><span class="koboSpan" id="kobo.655.1">: Higher temperatures with low penalties can produce creative but potentially repetitive text</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.656.1">LangChain provides a consistent interface for setting these parameters across different LLM providers:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.657.1">from</span></span><span class="koboSpan" id="kobo.658.1"> langchain_openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.659.1">import</span></span><span class="koboSpan" id="kobo.660.1"> OpenAI</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.661.1"># For factual, consistent responses</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.662.1">factual_llm = OpenAI(temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.663.1">0.1</span></span><span class="koboSpan" id="kobo.664.1">, max_tokens=</span><span class="hljs-number"><span class="koboSpan" id="kobo.665.1">256</span></span><span class="koboSpan" id="kobo.666.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.667.1"># For creative brainstorming</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.668.1">creative_llm = OpenAI(temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.669.1">0.8</span></span><span class="koboSpan" id="kobo.670.1">, top_p=</span><span class="hljs-number"><span class="koboSpan" id="kobo.671.1">0.95</span></span><span class="koboSpan" id="kobo.672.1">, max_tokens=</span><span class="hljs-number"><span class="koboSpan" id="kobo.673.1">512</span></span><span class="koboSpan" id="kobo.674.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.675.1">A few provider-specific considerations to keep in mind are:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.676.1">OpenAI</span></strong><span class="koboSpan" id="kobo.677.1">: Known for consistent behavior with temperature in the 0.0-1.0 range</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.678.1">Anthropic</span></strong><span class="koboSpan" id="kobo.679.1">: May need lower temperature settings to achieve similar creativity levels to other providers</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.680.1">Gemini</span></strong><span class="koboSpan" id="kobo.681.1">: Supports temperature up to 2.0, allowing for more extreme creativity at higher settings</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.682.1">Open-source models</span></strong><span class="koboSpan" id="kobo.683.1">: Often require different parameter combinations than commercial APIs</span></li>
</ul>
<div aria-label="40" epub:type="pagebreak" id="page16-1" role="doc-pagebreak"/>
<h3 class="heading-3" id="_idParaDest-44"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.684.1">Choosing parameters for applications</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.685.1">For enterprise</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.686.1"> applications requiring consistency and accuracy, lower temperatures (0.0-0.3) combined with moderate top-p values (0.5-0.7) are typically preferred. </span><span class="koboSpan" id="kobo.686.2">For creative assistants or brainstorming tools, higher temperatures produce more diverse outputs, especially when paired with higher top-p values.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.687.1">Remember that parameter tuning is often empirical – start with provider recommendations, then adjust based on your specific application needs and observed outputs.</span></p>
<h2 class="heading-2" id="_idParaDest-45"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.688.1">Prompts and templates</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.689.1">Prompt engineering</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.690.1"> is a crucial skill for LLM application development, particularly in production environments. </span><span class="koboSpan" id="kobo.690.2">LangChain provides a robust system for managing </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.691.1">prompts with features that address common development challenges:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.692.1">Template systems</span></strong><span class="koboSpan" id="kobo.693.1"> for dynamic prompt generation</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.694.1">Prompt management and versioning </span></strong><span class="koboSpan" id="kobo.695.1">for tracking changes</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.696.1">Few-shot example management</span></strong><span class="koboSpan" id="kobo.697.1"> for improved model performance</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.698.1">Output parsing and validation</span></strong><span class="koboSpan" id="kobo.699.1"> for reliable results</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.700.1">LangChain’s </span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.701.1">prompt templates transform static text into dynamic prompts with variable substitution – compare these two approaches to see the key differences:</span></p>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.702.1">Static use – problematic at scale:</span><p class="snippet-code-one"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.703.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.704.1">generate_prompt</span></span><span class="koboSpan" id="kobo.705.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.706.1">question, context=</span></span><span class="hljs-literal"><span class="koboSpan" id="kobo.707.1">None</span></span><span class="koboSpan" id="kobo.708.1">):</span></p><p class="snippet-code-one"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.709.1">if</span></span><span class="koboSpan" id="kobo.710.1"> context:</span></p><p class="snippet-code-one"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.711.1">return</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.712.1">f"Context information: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.713.1">{context}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.714.1">\n\nAnswer this question concisely: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.715.1">{question}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.716.1">"</span></span></p><p class="snippet-code-one"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.717.1">return</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.718.1">f"Answer this question concisely: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.719.1">{question}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.720.1">"</span></span></p><p class="snippet-code-one"> <span class="hljs-comment"><span class="koboSpan" id="kobo.721.1"># example use:</span></span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.722.1">      prompt_text = generate_prompt(</span><span class="hljs-string"><span class="koboSpan" id="kobo.723.1">"What is the capital of France?"</span></span><span class="koboSpan" id="kobo.724.1">)</span></p></li>
<li class="numberedList"><span class="koboSpan" id="kobo.725.1">PromptTemplate – production-ready:</span><p class="snippet-code-one"><span class="hljs-keyword"><span class="koboSpan" id="kobo.726.1">from</span></span><span class="koboSpan" id="kobo.727.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.728.1">import</span></span><span class="koboSpan" id="kobo.729.1"> PromptTemplate</span></p><p class="snippet-code-one"><span class="hljs-comment"><span class="koboSpan" id="kobo.730.1"># Define once, reuse everywhere</span></span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.731.1">question_template = PromptTemplate.from_template( </span><span class="hljs-string"><span class="koboSpan" id="kobo.732.1">"Answer this question concisely: {question}"</span></span><span class="koboSpan" id="kobo.733.1"> )</span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.734.1">question_with_context_template = PromptTemplate.from_template( </span><span class="hljs-string"><span class="koboSpan" id="kobo.735.1">"Context information: {context}\n\nAnswer this question concisely: {question}"</span></span><span class="koboSpan" id="kobo.736.1"> )</span></p><p class="snippet-code-one"><span class="hljs-comment"><span class="koboSpan" id="kobo.737.1"># Generate prompts by filling in variables</span></span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.738.1">prompt_text = question_template.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.739.1">format</span></span><span class="koboSpan" id="kobo.740.1">(question=</span><span class="hljs-string"><span class="koboSpan" id="kobo.741.1">"What is the capital of France?"</span></span><span class="koboSpan" id="kobo.742.1">)</span></p></li>
</ol>
<div aria-label="41" epub:type="pagebreak" id="page17-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.743.1">Templates</span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.744.1"> matter – here’s why:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.745.1">Consistency</span></strong><span class="koboSpan" id="kobo.746.1">: They standardize prompts across your application.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.747.1">Maintainability</span></strong><span class="koboSpan" id="kobo.748.1">: They allow you to change the prompt structure in one place instead of throughout your codebase.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.749.1">Readability</span></strong><span class="koboSpan" id="kobo.750.1">: They clearly separate template logic from business logic.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.751.1">Testability</span></strong><span class="koboSpan" id="kobo.752.1">: It is easier to unit test prompt generation separately from LLM calls.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.753.1">In production applications, you’ll often need to manage dozens or hundreds of prompts. </span><span class="koboSpan" id="kobo.753.2">Templates provide a scalable way to organize this comple</span><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.754.1">xity.</span></p>
<h3 class="heading-3" id="_idParaDest-46"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.755.1">Chat prompt templates</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.756.1">For chat models, we </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.757.1">can create more structured prompts that incorporate different roles:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.758.1">from</span></span><span class="koboSpan" id="kobo.759.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.760.1">import</span></span><span class="koboSpan" id="kobo.761.1"> ChatPromptTemplate</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.762.1">from</span></span><span class="koboSpan" id="kobo.763.1"> langchain_openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.764.1">import</span></span><span class="koboSpan" id="kobo.765.1"> ChatOpenAI</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.766.1">template = ChatPromptTemplate.from_messages([</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.767.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.768.1">"system"</span></span><span class="koboSpan" id="kobo.769.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.770.1">"You are an English to French translator."</span></span><span class="koboSpan" id="kobo.771.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.772.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.773.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.774.1">user"</span></span><span class="koboSpan" id="kobo.775.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.776.1">"Translate this to French: {text}"</span></span><span class="koboSpan" id="kobo.777.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.778.1">])</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.779.1">chat = ChatOpenAI()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.780.1">formatted_messages = template.format_messages(text=</span><span class="hljs-string"><span class="koboSpan" id="kobo.781.1">"Hello, how are you?"</span></span><span class="koboSpan" id="kobo.782.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.783.1">response = chat.invoke(formatted_messages)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.784.1">print</span></span><span class="koboSpan" id="kobo.785.1">(response)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.786.1">Let’s start by looking at </span><strong class="keyWord"><span class="koboSpan" id="kobo.787.1">LangChain Expression Language</span></strong><span class="koboSpan" id="kobo.788.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.789.1">LCEL</span></strong><span class="koboSpan" id="kobo.790.1">), which provides a clean, intuitive way to build LLM applica</span><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.791.1">tions.</span></p>
<div aria-label="42" epub:type="pagebreak" id="page18-1" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-47"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.792.1">LangChain Expression Language (LCEL)</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.793.1">LCEL </span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.794.1">represents a significant </span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.795.1">evolution in how we build LLM-powered applications with LangChain. </span><span class="koboSpan" id="kobo.795.2">Introduced in August 2023, LCEL is a declarative approach to constructing complex LLM workflows. </span><span class="koboSpan" id="kobo.795.3">Rather than focusing on </span><em class="italic"><span class="koboSpan" id="kobo.796.1">how</span></em><span class="koboSpan" id="kobo.797.1"> to execute each step, LCEL lets you define </span><em class="italic"><span class="koboSpan" id="kobo.798.1">what</span></em><span class="koboSpan" id="kobo.799.1"> you want to accomplish, allowing LangChain to handle the execution details behind the scenes.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.800.1">At its core, LCEL serves as a minimalist code layer that makes it remarkably easy to connect different LangChain</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.801.1"> components. </span><span class="koboSpan" id="kobo.801.2">If you’re familiar with Unix pipes or data processing libraries like pandas, you’ll recognize the intuitive syntax: components are connected using the pipe operator (|) to create processing pipelines.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.802.1">As we briefly introduced in </span><a href="E_Chapter_1.xhtml#_idTextAnchor001"><em class="italic"><span class="koboSpan" id="kobo.803.1">Chapter 1</span></em></a><span class="koboSpan" id="kobo.804.1">, LangChain has always used the concept of a “chain” as its fundamental pattern for connecting components. </span><span class="koboSpan" id="kobo.804.2">Chains represent sequences of operations that </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.805.1">transform inputs into outputs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.806.1">Originally, LangChain implemented this pattern through specific </span><code class="inlineCode"><span class="koboSpan" id="kobo.807.1">Chain</span></code><span class="koboSpan" id="kobo.808.1"> classes like </span><code class="inlineCode"><span class="koboSpan" id="kobo.809.1">LLMChain</span></code><span class="koboSpan" id="kobo.810.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.811.1">ConversationChain</span></code><span class="koboSpan" id="kobo.812.1">. </span><span class="koboSpan" id="kobo.812.2">While these legacy classes still exist, they’ve been deprecated in favor of the more flexible and powerful LCEL approach, which is built upon the Runnable interface.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.813.1">The Runnable interface is the cornerstone of modern LangChain. </span><span class="koboSpan" id="kobo.813.2">A Runnable is any component that can process inputs and produce outputs in a standardized way. </span><span class="koboSpan" id="kobo.813.3">Every component built with LCEL adheres to this interface, which provides consistent methods including:</span></p>
<ul>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.814.1">invoke()</span></code><span class="koboSpan" id="kobo.815.1">: Processes a single input synchronously and returns an output</span></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.816.1">stream()</span></code><span class="koboSpan" id="kobo.817.1">: Streams output as it’s being generated</span></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.818.1">batch()</span></code><span class="koboSpan" id="kobo.819.1">: Efficiently processes multiple inputs in parallel</span></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.820.1">ainvoke()</span></code><span class="koboSpan" id="kobo.821.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.822.1">abatch()</span></code><span class="koboSpan" id="kobo.823.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.824.1">astream()</span></code><span class="koboSpan" id="kobo.825.1">: Asynchronous versions of the above methods</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.826.1">This standardization means any Runnable component—whether it’s an LLM, a prompt template, a document retriever, or a custom function—can be connected to any other Runnable, creating a powerful composability system.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.827.1">Every Runnable implements a consistent set of methods including:</span></p>
<ul>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.828.1">invoke()</span></code><span class="koboSpan" id="kobo.829.1">: Processes a single input synchronously and returns an output</span></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.830.1">stream()</span></code><span class="koboSpan" id="kobo.831.1">: Streams output as it’s being generated</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.832.1">This standardization is powerful because it means any Runnable component—whether it’s an LLM, a prompt template, a document retriever, or a custom function—can be connected to any other Runnable. </span><span class="koboSpan" id="kobo.832.2">The consistency of this interface enables complex applications to be built from simpler building blocks.</span></p>
<div>
<div class="note" id="_idContainer026">
<div aria-label="43" epub:type="pagebreak" id="page19-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.833.1">LCEL offers several advantages that make it the preferred approach for building LangChain applications:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.834.1">Rapid development</span></strong><span class="koboSpan" id="kobo.835.1">: The declarative syntax enables faster prototyping and iteration of complex chains.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.836.1">Production-ready features</span></strong><span class="koboSpan" id="kobo.837.1">: LCEL provides built-in support for streaming, asynchronous execution, and parallel processing.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.838.1">Improved readability</span></strong><span class="koboSpan" id="kobo.839.1">: The pipe syntax makes it easy to visualize data flow through your application.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.840.1">Seamless ecosystem integration</span></strong><span class="koboSpan" id="kobo.841.1">: Applications built with LCEL automatically work with LangSmith for observability and LangServe for deployment.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.842.1">Customizability</span></strong><span class="koboSpan" id="kobo.843.1">: Easily incorporate custom Python functions into your chains with RunnableLambda.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.844.1">Runtime optimization</span></strong><span class="koboSpan" id="kobo.845.1">: LangChain can automatically optimize the execution of LCEL-defined chains.</span></li>
</ul>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.846.1">LCEL truly shines when you need to build complex applications that combine multiple components in </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.847.1">sophisticated workflows. </span><span class="koboSpan" id="kobo.847.2">In the next sections, we’ll </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.848.1">explore how to use LCEL to build real-world applications, starting with the basic building blocks and gradually incorporating more advanced patterns.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.849.1">The pipe operator (|) serves as the cornerstone of LCEL, allowing you to chain components sequentially:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.850.1"># 1. </span><span class="koboSpan" id="kobo.850.2">Basic sequential chain: Just prompt to LLM</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.851.1">basic_chain = prompt | llm | StrOutputParser()</span></p>
<p class="normal"><span class="koboSpan" id="kobo.852.1">Here, </span><code class="inlineCode"><span class="koboSpan" id="kobo.853.1">StrOutputParser()</span></code><span class="koboSpan" id="kobo.854.1"> is a simple output parser that extracts the string response from an LLM. </span><span class="koboSpan" id="kobo.854.2">It takes the structured output from an LLM and converts it to a plain string, making it easier to work with. </span><span class="koboSpan" id="kobo.854.3">This parser is especially useful when you need just the text content without metadata.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.855.1">Under the hood, LCEL uses Python’s operator overloading to transform this expression into a RunnableSequence where each component’s output flows into the next component’s input. </span><span class="koboSpan" id="kobo.855.2">The pipe (|) is syntactic sugar that overrides the </span><code class="inlineCode"><span class="koboSpan" id="kobo.856.1">__or__</span></code><span class="koboSpan" id="kobo.857.1"> hidden method, in other words, </span><code class="inlineCode"><span class="koboSpan" id="kobo.858.1">A | B</span></code><span class="koboSpan" id="kobo.859.1"> is equivalent to </span><code class="inlineCode"><span class="koboSpan" id="kobo.860.1">B.__or__(A)</span></code><span class="koboSpan" id="kobo.861.1">.</span></p>
<div aria-label="44" epub:type="pagebreak" id="page20-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.862.1">The pipe syntax is equivalent to creating a </span><code class="inlineCode"><span class="koboSpan" id="kobo.863.1">RunnableSequence</span></code><span class="koboSpan" id="kobo.864.1"> programmatically:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.865.1">chain = RunnableSequence(first= prompt, middle=[llm], last= output_parser)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.866.1">LCEL also supports adding transformations </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.867.1">and</span></span><span class="koboSpan" id="kobo.868.1"> custom functions:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.869.1">with_transformation = prompt | llm | (</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.870.1">lambda</span></span><span class="koboSpan" id="kobo.871.1"> x: x.upper()) | StrOutputParser()</span></p>
<p class="normal"><span class="koboSpan" id="kobo.872.1">For more complex workflows, you can incorporate branching logic:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.873.1">decision_chain = prompt | llm | (</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.874.1">lambda</span></span><span class="koboSpan" id="kobo.875.1"> x: route_based_on_content(x)) | {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.876.1">"summarize"</span></span><span class="koboSpan" id="kobo.877.1">: summarize_chain,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.878.1">"analyze"</span></span><span class="koboSpan" id="kobo.879.1">: analyze_chain</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.880.1">}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.881.1">Non-Runnable elements like functions and dictionaries are automatically converted to appropriate Runnable types:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.882.1"># Function to Runnable</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.883.1">length_func = </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.884.1">lambda</span></span><span class="koboSpan" id="kobo.885.1"> x: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.886.1">len</span></span><span class="koboSpan" id="kobo.887.1">(x)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.888.1">chain = prompt | length_func | output_parser</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.889.1"># Is converted to:</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.890.1">chain = prompt | RunnableLambda(length_func) | output_parser</span></p>
<p class="normal"><span class="koboSpan" id="kobo.891.1">The flexible, composable</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.892.1"> nature of LCEL will allow us to </span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.893.1">tackle real-world LLM application challenges with elegant, main</span><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.894.1">tainable code.</span></p>
<h3 class="heading-3" id="_idParaDest-48"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.895.1">Simple workflows with LCEL</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.896.1">As we’ve seen, LCEL </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.897.1">provides a declarative syntax for composing LLM application components using the pipe operator. </span><span class="koboSpan" id="kobo.897.2">This approach dramatically simplifies workflow construction compared to traditional imperative code. </span><span class="koboSpan" id="kobo.897.3">Let’s build a simple joke generator to see LCEL in action:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.898.1">from</span></span><span class="koboSpan" id="kobo.899.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.900.1">import</span></span><span class="koboSpan" id="kobo.901.1"> PromptTemplate</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.902.1">from</span></span><span class="koboSpan" id="kobo.903.1"> langchain_core.output_parsers </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.904.1">import</span></span><span class="koboSpan" id="kobo.905.1"> StrOutputParser</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.906.1">from</span></span><span class="koboSpan" id="kobo.907.1"> langchain_openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.908.1">import</span></span><span class="koboSpan" id="kobo.909.1"> ChatOpenAI</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.910.1"># Create components</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.911.1">prompt = PromptTemplate.from_template(</span><span class="hljs-string"><span class="koboSpan" id="kobo.912.1">"Tell me a joke about {topic}"</span></span><span class="koboSpan" id="kobo.913.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.914.1">llm = ChatOpenAI()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.915.1">output_parser = StrOutputParser()</span></p>
<div aria-label="45" epub:type="pagebreak" id="page21-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.916.1"># Chain them together using LCEL</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.917.1">chain = prompt | llm | output_parser</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.918.1">#  Execute the workflow with a single call</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.919.1">result = chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.920.1">"topic"</span></span><span class="koboSpan" id="kobo.921.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.922.1">"programming"</span></span><span class="koboSpan" id="kobo.923.1">})</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.924.1">print</span></span><span class="koboSpan" id="kobo.925.1">(result)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.926.1">This produces a programming joke:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.927.1">Why don't programmers like nature?</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.928.1">It has too many bugs!</span></p>
<p class="normal"><span class="koboSpan" id="kobo.929.1">Without LCEL, the same workflow is equivalent to separate function calls with manual data passing:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.930.1">formatted_prompt = prompt.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.931.1">"topic"</span></span><span class="koboSpan" id="kobo.932.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.933.1">"programming"</span></span><span class="koboSpan" id="kobo.934.1">})</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.935.1">llm_output = llm.invoke(formatted_prompt)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.936.1">result = output_parser.invoke(llm_output)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.937.1">As you can see, we </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.938.1">have detached chain construction fro</span><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.939.1">m its execution.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.940.1">In production applications, this pattern becomes even more valuable when handling complex workflows with branching logic, error handling, or parallel processing – topics we’ll explore in </span><a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic"><span class="koboSpan" id="kobo.941.1">Chapter 3</span></em></a><span class="koboSpan" id="kobo.942.1">.</span></p>
<h3 class="heading-3" id="_idParaDest-49"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.943.1">Complex chain example</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.944.1">While the </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.945.1">simple joke generator demonstrated basic LCEL usage, real-world applications typically require more sophisticated data handling. </span><span class="koboSpan" id="kobo.945.2">Let’s explore advanced patterns using a story generation and analysis example.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.946.1">In this example, we’ll build a multi-stage workflow that demonstrates how to:</span></p>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.947.1">Generate content with one LLM call</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.948.1">Feed that content into a second LLM call</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.949.1">Preserve and transform data throughout the chain</span></li>
</ol>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.950.1">from</span></span><span class="koboSpan" id="kobo.951.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.952.1">import</span></span><span class="koboSpan" id="kobo.953.1"> PromptTemplate</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.954.1">from</span></span><span class="koboSpan" id="kobo.955.1"> langchain_google_genai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.956.1">import</span></span><span class="koboSpan" id="kobo.957.1"> GoogleGenerativeAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.958.1">from</span></span><span class="koboSpan" id="kobo.959.1"> langchain_core.output_parsers </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.960.1">import</span></span><span class="koboSpan" id="kobo.961.1"> StrOutputParser</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.962.1"># Initialize the model</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.963.1">llm = GoogleGenerativeAI(model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.964.1">"gemini-1.5-pro"</span></span><span class="koboSpan" id="kobo.965.1">)</span></p>
<div aria-label="46" epub:type="pagebreak" id="page22-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.966.1"># First chain generates a story</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.967.1">story_prompt = PromptTemplate.from_template(</span><span class="hljs-string"><span class="koboSpan" id="kobo.968.1">"Write a short story about {topic}"</span></span><span class="koboSpan" id="kobo.969.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.970.1">story_chain = story_prompt | llm | StrOutputParser()</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.971.1"># Second chain analyzes the story</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.972.1">analysis_prompt = PromptTemplate.from_template(</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.973.1">"Analyze the following story's mood:\n{story}"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.974.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.975.1">analysis_chain = analysis_prompt | llm | StrOutputParser()</span></p>
<p class="normal"><span class="koboSpan" id="kobo.976.1">We can compose these two chains together. </span><span class="koboSpan" id="kobo.976.2">Our first simple approach pipes the story directly into the analysis chain:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.977.1"># Combine chains</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.978.1">story_with_analysis = story_chain | analysis_chain</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.979.1"># Run the combined chain</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.980.1">story_analysis = story_with_analysis.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.981.1">"topic"</span></span><span class="koboSpan" id="kobo.982.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.983.1">"a rainy day"</span></span><span class="koboSpan" id="kobo.984.1">})</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.985.1">print</span></span><span class="koboSpan" id="kobo.986.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.987.1">"\nAnalysis:"</span></span><span class="koboSpan" id="kobo.988.1">, story_analysis)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.989.1">I get a long analysis. </span><span class="koboSpan" id="kobo.989.2">Here’s how it starts:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.990.1">Analysis: The mood of the story is predominantly **calm, peaceful, and subtly romantic.** There's a sense of gentle melancholy brought on by the rain and the quiet emptiness of the bookshop, but this is balanced by a feeling of warmth and hope.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.991.1">While this </span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.992.1">works, we’ve lost the original story in our result – we only get the analysis! </span><span class="koboSpan" id="kobo.992.2">In production applications, we typically want to preserve context throughout the chain:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.993.1">from</span></span><span class="koboSpan" id="kobo.994.1"> langchain_core.runnables </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.995.1">import</span></span><span class="koboSpan" id="kobo.996.1"> RunnablePassthrough</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.997.1"># Using RunnablePassthrough.assign to preserve data</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.998.1">enhanced_chain = RunnablePassthrough.assign(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.999.1">    story=story_chain  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1000.1"># Add 'story' key with generated content</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1001.1">).assign(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1002.1">    analysis=analysis_chain  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1003.1"># Add 'analysis' key with analysis of the story</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1004.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1005.1"># Execute the chain</span></span></p>
<div aria-label="47" epub:type="pagebreak" id="page23-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.1006.1">result = enhanced_chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1007.1">"topic"</span></span><span class="koboSpan" id="kobo.1008.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1009.1">"a rainy day"</span></span><span class="koboSpan" id="kobo.1010.1">})</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1011.1">print</span></span><span class="koboSpan" id="kobo.1012.1">(result.keys())  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1013.1"># Output: dict_keys(['topic', 'story', 'analysis'])  # dict_keys(['topic', 'story', 'analysis'])</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.1014.1">For more control over the output structure, we could also construct dictionaries manually:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1015.1">from</span></span><span class="koboSpan" id="kobo.1016.1"> operator </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1017.1">import</span></span><span class="koboSpan" id="kobo.1018.1"> itemgetter</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1019.1"># Alternative approach using dictionary construction</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1020.1">manual_chain = (</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1021.1">    RunnablePassthrough() |  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1022.1"># Pass through input</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1023.1">    {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1024.1">"story"</span></span><span class="koboSpan" id="kobo.1025.1">: story_chain,  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1026.1"># Add story result</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1027.1">"topic"</span></span><span class="koboSpan" id="kobo.1028.1">: itemgetter(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1029.1">"topic"</span></span><span class="koboSpan" id="kobo.1030.1">)  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1031.1"># Preserve original topic</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1032.1">    } |</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1033.1">    RunnablePassthrough().assign(  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1034.1"># Add analysis based on story</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1035.1">        analysis=analysis_chain</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1036.1">    )</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1037.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1038.1">result = manual_chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1039.1">"topic"</span></span><span class="koboSpan" id="kobo.1040.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1041.1">"a rainy day"</span></span><span class="koboSpan" id="kobo.1042.1">})</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1043.1">print</span></span><span class="koboSpan" id="kobo.1044.1">(result.keys())  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1045.1"># Output: dict_keys(['story', 'topic', 'analysis'])</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.1046.1">We can simplify this with dictionary conversion using a LCEL shorthand:</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1047.1"># Simplified dictionary construction</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1048.1">simple_dict_chain = story_chain | {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1049.1">"analysis"</span></span><span class="koboSpan" id="kobo.1050.1">: analysis_chain}</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1051.1">result = simple_dict_chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1052.1">"topic"</span></span><span class="koboSpan" id="kobo.1053.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1054.1">"a rainy day"</span></span><span class="koboSpan" id="kobo.1055.1">}) </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1056.1">print</span></span><span class="koboSpan" id="kobo.1057.1">(result.keys()) </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1058.1"># Output: dict_keys(['analysis', 'output'])</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.1059.1">What makes</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.1060.1"> these examples more complex than our simple joke generator?</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1061.1">M</span></strong><strong class="keyWord"><span class="koboSpan" id="kobo.1062.1">ultiple LLM calls</span></strong><span class="koboSpan" id="kobo.1063.1">: Rather than a single prompt </span><span class="koboSpan" id="kobo.1064.1"><img alt="" src="../Images/Icon.png"/></span><span class="koboSpan" id="kobo.1065.1"> LLM </span><span class="koboSpan" id="kobo.1066.1"><img alt="" src="../Images/Icon.png"/></span><span class="koboSpan" id="kobo.1067.1"> parser flow, we’re chaining multiple LLM interactions</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1068.1">Data transformation</span></strong><span class="koboSpan" id="kobo.1069.1">: Using tools like </span><code class="inlineCode"><span class="koboSpan" id="kobo.1070.1">RunnablePassthrough</span></code><span class="koboSpan" id="kobo.1071.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1072.1">itemgetter</span></code><span class="koboSpan" id="kobo.1073.1"> to manage and transform data</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1074.1">Dictionary preservation</span></strong><span class="koboSpan" id="kobo.1075.1">: Maintaining context throughout the chain rather than just passing single values</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1076.1">Structured outputs</span></strong><span class="koboSpan" id="kobo.1077.1">: Creating structured output dictionaries rather than simple strings</span></li>
</ul>
<div aria-label="48" epub:type="pagebreak" id="page24-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1078.1">These patterns are essential for production applications where you need to:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.1079.1">Track the provenance of generated content</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1080.1">Combine results from multiple operations</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1081.1">Structure data for downstream processing or display</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1082.1">Implement more sophisticated error handling</span></li>
</ul>
<div>
<div class="note" id="_idContainer029">
<p class="normal"><span class="koboSpan" id="kobo.1083.1">While LCEL handles many complex workflows elegantly, for state management and advanced branching logic, you’ll want to explore LangGraph, which we’ll </span><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.1084.1">cover in </span><a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic"><span class="koboSpan" id="kobo.1085.1">Chapter 3</span></em></a><span class="koboSpan" id="kobo.1086.1">.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.1087.1">While our previous examples used cloud-based models like OpenAI and Google’s Gemini, LangChain’s LCEL and other functionality work seamlessly with local models as well. </span><span class="koboSpan" id="kobo.1087.2">This flexibility allows you to choose the right deployment approach for your specific needs.</span></p>
<h1 class="heading-1" id="_idParaDest-50"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.1088.1">Running local models</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1089.1">When building LLM applications</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.1090.1"> with LangChain, you need to decide where your models will run.</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.1091.1">Advantages of local models:</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1092.1">Complete data control and privacy</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1093.1">No API costs or usage limits</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1094.1">No internet dependency</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1095.1">Control over model parameters and fine-tuning</span></li>
</ul></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1096.1">Advantages of cloud models:</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1097.1">No hardware requirements or setup complexity</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1098.1">Access to the most powerful, state-of-the-art models</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1099.1">Elastic scaling without infrastructure management</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1100.1">Continuous model improvements without manual updates</span></li>
</ul></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1101.1">When to choose </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.1102.1">local models:</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1103.1">Applications with strict data privacy requirements</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1104.1">Development and testing environments</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1105.1">Edge or offline deployment scenarios</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1106.1">Cost-sensitive applications with predictable, high-volume usage</span></li>
</ul></li>
</ul>
<div aria-label="49" epub:type="pagebreak" id="page25" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1107.1">Let’s start with one of the most developer-friendly options for </span><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.1108.1">running local models.</span></p>
<h2 class="heading-2" id="_idParaDest-51"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.1109.1">Getting started with Ollama</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1110.1">Ollama </span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.1111.1">provides</span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.1112.1"> a developer-friendly way to run powerful open-source models locally. </span><span class="koboSpan" id="kobo.1112.2">It provides a simple interface for downloading and running various open-source models. </span><span class="koboSpan" id="kobo.1112.3">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.1113.1">langchain-ollama</span></code><span class="koboSpan" id="kobo.1114.1"> dependency should already be installed if you’ve followed the instructions in this chapter; however, let’s go through them briefly anyway:</span></p>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.1115.1">Install the LangChain Ollama integration:</span><p class="snippet-con-one"><span class="koboSpan" id="kobo.1116.1">pip install langchain-ollama</span></p></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1117.1">Then pull a model. </span><span class="koboSpan" id="kobo.1117.2">From the command line, a terminal such as bash or the WindowsPowerShell, run:</span><p class="snippet-con-one"><span class="koboSpan" id="kobo.1118.1">ollama pull deepseek-r1:1.5b</span></p></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1119.1">Start the Ollama server:</span><p class="snippet-con-one"><span class="koboSpan" id="kobo.1120.1">ollama serve</span></p></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.1121.1">Here’s how to integrate Ollama with the LCEL patterns we’ve explored:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1122.1">from</span></span><span class="koboSpan" id="kobo.1123.1"> langchain_ollama </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1124.1">import</span></span><span class="koboSpan" id="kobo.1125.1"> ChatOllama</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1126.1">from</span></span><span class="koboSpan" id="kobo.1127.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1128.1">import</span></span><span class="koboSpan" id="kobo.1129.1"> PromptTemplate</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1130.1">from</span></span><span class="koboSpan" id="kobo.1131.1"> langchain_core.output_parsers </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1132.1">import</span></span><span class="koboSpan" id="kobo.1133.1"> StrOutputParser</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1134.1"># Initialize Ollama with your chosen model</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1135.1">local_llm = ChatOllama(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1136.1">    model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1137.1">"deepseek-r1:1.5b"</span></span><span class="koboSpan" id="kobo.1138.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1139.1">    temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1140.1">0</span></span><span class="koboSpan" id="kobo.1141.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1142.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1143.1"># Create an LCEL chain using the local model</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1144.1">prompt = PromptTemplate.from_template(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1145.1">"Explain {concept} in simple terms"</span></span><span class="koboSpan" id="kobo.1146.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1147.1">local_chain = prompt | local_llm | StrOutputParser()</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1148.1"># Use the chain with your local model</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1149.1">result = local_chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1150.1">"concept"</span></span><span class="koboSpan" id="kobo.1151.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1152.1">"quantum computing"</span></span><span class="koboSpan" id="kobo.1153.1">})</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1154.1">print</span></span><span class="koboSpan" id="kobo.1155.1">(result)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1156.1">This LCEL </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.1157.1">chain </span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.1158.1">functions identically to our cloud-based examples, demonstrating LangChain’s model-agnostic design.</span></p>
<div aria-label="50" epub:type="pagebreak" id="page26" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1159.1">Please note that since you are running a local model, you don’t need to set up any keys. </span><span class="koboSpan" id="kobo.1159.2">The answer is very long – although quite reasonable. </span><span class="koboSpan" id="kobo.1159.3">You can run this yourself and see what answers you get.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1160.1">Now that we’ve seen basic text generation, let’s look at another integration. </span><span class="koboSpan" id="kobo.1160.2">Hugging Face offers an approachable way to run models locally, with access to a vast ecosyst</span><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.1161.1">em of pre-trained models.</span></p>
<h2 class="heading-2" id="_idParaDest-52"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.1162.1">Working with Hugging Face models locally</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1163.1">With</span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.1164.1"> Hugging Face, you can either run a model locally (HuggingFacePipeline) or on </span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.1165.1">the Hugging Face Hub (HuggingFaceEndpoint). </span><span class="koboSpan" id="kobo.1165.2">Here, we are talking about local runs, so we’ll focus on </span><code class="inlineCode"><span class="koboSpan" id="kobo.1166.1">HuggingFacePipeline</span></code><span class="koboSpan" id="kobo.1167.1">. </span><span class="koboSpan" id="kobo.1167.2">Here we go:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1168.1">from</span></span><span class="koboSpan" id="kobo.1169.1"> langchain_core.messages </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1170.1">import</span></span><span class="koboSpan" id="kobo.1171.1"> SystemMessage, HumanMessage</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1172.1">from</span></span><span class="koboSpan" id="kobo.1173.1"> langchain_huggingface </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1174.1">import</span></span><span class="koboSpan" id="kobo.1175.1"> ChatHuggingFace, HuggingFacePipeline</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1176.1"># Create a pipeline with a small model:</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1177.1">llm = HuggingFacePipeline.from_model_id(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1178.1">    model_id=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1179.1">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span></span><span class="koboSpan" id="kobo.1180.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1181.1">    task=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1182.1">"text-generation"</span></span><span class="koboSpan" id="kobo.1183.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1184.1">    pipeline_kwargs=</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1185.1">dict</span></span><span class="koboSpan" id="kobo.1186.1">(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1187.1">        max_new_tokens=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1188.1">512</span></span><span class="koboSpan" id="kobo.1189.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1190.1">        do_sample=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1191.1">False</span></span><span class="koboSpan" id="kobo.1192.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1193.1">        repetition_penalty=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1194.1">1.03</span></span><span class="koboSpan" id="kobo.1195.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1196.1">    ),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1197.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1198.1">chat_model = ChatHuggingFace(llm=llm)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1199.1"># Use it like any other LangChain LLM</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1200.1">messages = [</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1201.1">    SystemMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1202.1">"You're a helpful assistant"</span></span><span class="koboSpan" id="kobo.1203.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1204.1">    HumanMessage(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1205.1">        content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1206.1">"Explain the concept of machine learning in simple terms"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1207.1">    ),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1208.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1209.1">ai_msg = chat_model.invoke(messages)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1210.1">print</span></span><span class="koboSpan" id="kobo.1211.1">(ai_msg.content)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1212.1">This can take quite a while, especially the first time, since the model has to be downloaded first. </span><span class="koboSpan" id="kobo.1212.2">We’ve omitted</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.1213.1"> the model response for the sake of brevity.</span></p>
<div aria-label="51" epub:type="pagebreak" id="page27" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1214.1">LangChain </span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.1215.1">supports running models locally through other integrations as well, for example:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1216.1">llama.cpp:</span></strong><span class="koboSpan" id="kobo.1217.1"> This</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.1218.1"> high-performance C++ implementation allows running LLaMA-based models efficiently on consumer hardware. </span><span class="koboSpan" id="kobo.1218.2">While we won’t cover the setup process in detail, LangChain provides straightforward integration with llama.cpp for both inference and fine-tuning.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1219.1">GPT4All</span></strong><span class="koboSpan" id="kobo.1220.1">: GPT4All</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.1221.1"> offers lightweight models that can run on consumer hardware. </span><span class="koboSpan" id="kobo.1221.2">LangChain’s integration makes it easy to use these models as drop-in replacements for cloud-based LLMs in many applications.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.1222.1">As you begin working with local models, you’ll want to optimize their performance and handle common challenges. </span><span class="koboSpan" id="kobo.1222.2">Here are some essential tips and patterns that will help you get the most out of your lo</span><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.1223.1">cal deployments with LangChain.</span></p>
<h2 class="heading-2" id="_idParaDest-53"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.1224.1">Tips for local models</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1225.1">When </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.1226.1">working with local models, keep these points in mind:</span></p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.1227.1">Resource management</span></strong><span class="koboSpan" id="kobo.1228.1">: Local models require careful configuration to balance performance and resource usage. </span><span class="koboSpan" id="kobo.1228.2">The following example demonstrates how to configure an Ollama model for efficient operation:</span><p class="snippet-code-one"><span class="hljs-comment"><span class="koboSpan" id="kobo.1229.1">#  Configure model with optimized memory and processing settings</span></span></p><p class="snippet-code-one"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1230.1">from</span></span><span class="koboSpan" id="kobo.1231.1"> langchain_ollama </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1232.1">import</span></span><span class="koboSpan" id="kobo.1233.1"> ChatOllama</span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.1234.1">llm = ChatOllama(</span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.1235.1">  model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1236.1">"mistral:q4_K_M"</span></span><span class="koboSpan" id="kobo.1237.1">, </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1238.1"># 4-bit quantized model (smaller memory footprint)</span></span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.1239.1">  num_gpu=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1240.1">1</span></span><span class="koboSpan" id="kobo.1241.1">, </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1242.1"># Number of GPUs to utilize (adjust based on hardware)</span></span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.1243.1"> num_thread=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1244.1">4</span></span> <span class="hljs-comment"><span class="koboSpan" id="kobo.1245.1"># Number of CPU threads for parallel processing</span></span></p><p class="snippet-code-one"><span class="koboSpan" id="kobo.1246.1">)</span></p></li>
</ol>
<p class="normal-one"><span class="koboSpan" id="kobo.1247.1">Let’s look at what each parameter does:</span></p>
<div aria-label="52" epub:type="pagebreak" id="page28" role="doc-pagebreak"/>
<ul>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.1248.1">model=”mistral:q4_K_M”</span></strong><span class="koboSpan" id="kobo.1249.1">: Specifies a 4-bit quantized version of the Mistral model. </span><span class="koboSpan" id="kobo.1249.2">Quantization reduces the model size by representing weights with fewer bits, trading minimal precision for significant memory savings. </span><span class="koboSpan" id="kobo.1249.3">For example:</span><ul><li class="bulletList level-3"><span class="koboSpan" id="kobo.1250.1">Full precision model: ~8GB RAM required</span></li>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.1251.1">4-bit quantized model: ~2GB RAM required</span></li>
</ul></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.1252.1">num_gpu=1</span></strong><span class="koboSpan" id="kobo.1253.1">: Allocates GPU resources. </span><span class="koboSpan" id="kobo.1253.2">Options include:</span><ul><li class="bulletList level-3"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.1254.1">0: CPU-only mode (slower but works without a GPU)</span></li>
<li class="bulletList level-3"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.1255.1">1: Uses a single GPU (appropriate for most desktop setups)</span></li>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.1256.1">Higher values: For multi-GPU systems only</span></li>
</ul></li>
<li class="bulletList level-2"><strong class="keyWord"><span class="koboSpan" id="kobo.1257.1">num_thread=4</span></strong><span class="koboSpan" id="kobo.1258.1">: Controls CPU parallelization:</span><ul><li class="bulletList level-3"><span class="koboSpan" id="kobo.1259.1">Lower values (2-4): Good for running alongside other applications</span></li>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.1260.1">Higher values (8-16): Maximizes performance on dedicated servers</span></li>
<li class="bulletList level-3"><span class="koboSpan" id="kobo.1261.1">Optimal setting: Usually matches your CPU’s physical core count</span></li>
</ul></li>
</ul>
<ol>
<li class="numberedList" value="2"><strong class="keyWord"><span class="koboSpan" id="kobo.1262.1">Error handling</span></strong><span class="koboSpan" id="kobo.1263.1">: Local </span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.1264.1">models can encounter various errors, from out-of-memory conditions to unexpected terminations. </span><span class="koboSpan" id="kobo.1264.2">A robust error-handling strategy is essential:</span></li>
</ol>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1265.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1266.1">safe_model_call</span></span><span class="koboSpan" id="kobo.1267.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1268.1">llm, prompt, max_retries=</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1269.1">2</span></span><span class="koboSpan" id="kobo.1270.1">):</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1271.1">"""Safely call a local model with retry logic and graceful</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.1272.1">    failure"""</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1273.1">    retries = </span><span class="hljs-number"><span class="koboSpan" id="kobo.1274.1">0</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1275.1">while</span></span><span class="koboSpan" id="kobo.1276.1"> retries &lt;= max_retries:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1277.1">try</span></span><span class="koboSpan" id="kobo.1278.1">:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1279.1">return</span></span><span class="koboSpan" id="kobo.1280.1"> llm.invoke(prompt)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1281.1">except</span></span><span class="koboSpan" id="kobo.1282.1"> RuntimeError </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1283.1">as</span></span><span class="koboSpan" id="kobo.1284.1"> e:</span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.1285.1"># Common error with local models when running out of VRAM</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1286.1">if</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.1287.1">"CUDA out of memory"</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1288.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1289.1">str</span></span><span class="koboSpan" id="kobo.1290.1">(e):</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1291.1">print</span></span><span class="koboSpan" id="kobo.1292.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1293.1">f"GPU memory error, waiting and retrying (</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1294.1">{retries+</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1295.1">1</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1296.1">}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1297.1">/</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1298.1">{max_retries+</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1299.1">1</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1300.1">}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1301.1">)"</span></span><span class="koboSpan" id="kobo.1302.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1303.1">                time.sleep(</span><span class="hljs-number"><span class="koboSpan" id="kobo.1304.1">2</span></span><span class="koboSpan" id="kobo.1305.1">)  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1306.1"># Give system time to free resources</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1307.1">                retries += </span><span class="hljs-number"><span class="koboSpan" id="kobo.1308.1">1</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1309.1">else</span></span><span class="koboSpan" id="kobo.1310.1">:</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1311.1">print</span></span><span class="koboSpan" id="kobo.1312.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1313.1">f"Runtime error: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1314.1">{e}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1315.1">"</span></span><span class="koboSpan" id="kobo.1316.1">)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1317.1">return</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.1318.1">"An error occurred while processing your request."</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1319.1">except</span></span><span class="koboSpan" id="kobo.1320.1"> Exception </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1321.1">as</span></span><span class="koboSpan" id="kobo.1322.1"> e:</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1323.1">print</span></span><span class="koboSpan" id="kobo.1324.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1325.1">f"Unexpected error calling model: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1326.1">{e}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1327.1">"</span></span><span class="koboSpan" id="kobo.1328.1">)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1329.1">return</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.1330.1">"An error occurred while processing your request."</span></span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.1331.1"># If we exhausted retries</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1332.1">return</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.1333.1">"Model is currently experiencing high load. </span><span class="koboSpan" id="kobo.1333.2">Please try again later."</span></span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1334.1"># Use the safety wrapper in your LCEL chain</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1335.1">from</span></span><span class="koboSpan" id="kobo.1336.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1337.1">import</span></span><span class="koboSpan" id="kobo.1338.1"> PromptTemplate</span></p>
<div aria-label="53" epub:type="pagebreak" id="page29" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1339.1">from</span></span><span class="koboSpan" id="kobo.1340.1"> langchain_core.runnables </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1341.1">import</span></span><span class="koboSpan" id="kobo.1342.1"> RunnableLambda</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1343.1">prompt = PromptTemplate.from_template(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1344.1">"Explain {concept} in simple terms"</span></span><span class="koboSpan" id="kobo.1345.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1346.1">safe_llm = RunnableLambda(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1347.1">lambda</span></span><span class="koboSpan" id="kobo.1348.1"> x: safe_model_call(llm, x))</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1349.1">safe_chain = prompt | safe_llm</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1350.1">response = safe_chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1351.1">"concept"</span></span><span class="koboSpan" id="kobo.1352.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1353.1">"quantum computing"</span></span><span class="koboSpan" id="kobo.1354.1">})</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1355.1">Common local model</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.1356.1"> errors you might run into are as follows:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1357.1">Out of memory</span></strong><span class="koboSpan" id="kobo.1358.1">: Occurs when the model requires more VRAM than available</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1359.1">Model loading failure</span></strong><span class="koboSpan" id="kobo.1360.1">: When model files are corrupt or incompatible</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1361.1">Timeout issues</span></strong><span class="koboSpan" id="kobo.1362.1">: When inference takes too long on resource-constrained systems</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1363.1">Context length errors</span></strong><span class="koboSpan" id="kobo.1364.1">: When input exceeds the model’s maximum token limit</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.1365.1">By implementing these optimizations and error-handling strategies, you can create robust LangChain applications that leverage local models effectively while maintaining a good user experience even when issues arise.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1366.1"><img alt="Figure 2.1: Decision chart for choosing between local and cloud-based models" src="../Images/B32363_02_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1367.1">Figure 2.1: Decision chart for choosing between local and cloud-based models</span></p>
<div aria-label="54" epub:type="pagebreak" id="page30" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1368.1">Having explored how to build text-based applications with LangChain, we’ll now extend our understanding to multimodal capabilities. </span><span class="koboSpan" id="kobo.1368.2">As AI systems increasingly work with multiple forms of data, LangChain</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.1369.1"> provides interfaces for both generating images from text and understanding visual content – capabilities that complement the text processing we’ve already covered and open new possibilitie</span><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.1370.1">s for more immersive applic</span><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.1371.1">ations.</span></p>
<h1 class="heading-1" id="_idParaDest-54"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.1372.1">Multimodal AI applications</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1373.1">AI systems </span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.1374.1">have evolved beyond text-only processing to work with diverse data types. </span><span class="koboSpan" id="kobo.1374.2">In the current landscape, we can distinguish between two key capabilities that are often confused but represent different technological approaches.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1375.1">Multimodal understanding represents the ability of models to process multiple types of inputs simultaneously to perform reasoning and generate responses. </span><span class="koboSpan" id="kobo.1375.2">These advanced systems can understand the relationships between different modalities, accepting inputs like text, images, PDFs, audio, video, and structured data. </span><span class="koboSpan" id="kobo.1375.3">Their processing capabilities include cross-modal reasoning, context awareness, and sophisticated information extraction. </span><span class="koboSpan" id="kobo.1375.4">Models like Gemini 2.5, GPT-4V, Sonnet 3.7, and Llama 4 exemplify this capability. </span><span class="koboSpan" id="kobo.1375.5">For instance, a multimodal model can analyze a chart image along with a text question to provide insights about the data trend, combining visual and textual understanding in a single processing flow.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1376.1">Content generation capabilities, by contrast, focus on creating specific types of media, often with extraordinary quality but more specialized functionality. </span><span class="koboSpan" id="kobo.1376.2">Text-to-image models create visual content from descriptions, text-to-video systems generate video clips from prompts, text-to-audio tools produce music or speech, and image-to-image models transform existing visuals. </span><span class="koboSpan" id="kobo.1376.3">Examples include Midjourney, DALL-E, and Stable Diffusion for images; Sora and Pika for video; and Suno and ElevenLabs for audio. </span><span class="koboSpan" id="kobo.1376.4">Unlike true multimodal models, many generation systems are specialized for their specific output modality, even if they can accept multiple input types. </span><span class="koboSpan" id="kobo.1376.5">They excel at creation rather than understanding.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1377.1">As LLMs evolve beyond text, LangChain is expanding to support both multimodal understanding and content generation workflows. </span><span class="koboSpan" id="kobo.1377.2">The framework provides developers with tools to incorporate these advanced capabilities into their applications without needing to implement complex integrations from scratch. </span><span class="koboSpan" id="kobo.1377.3">Let’s start with generating images from text descriptions. </span><span class="koboSpan" id="kobo.1377.4">LangChain provides several approaches to incorporate image generation through external integrations and wrappers. </span><span class="koboSpan" id="kobo.1377.5">We’ll explore multiple implementation patterns, starting with the simplest and progressing to more sophisticated techniques that can be incorporated into your applications.</span></p>
<div aria-label="55" epub:type="pagebreak" id="page31" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-55"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.1378.1">Text-to-image</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1379.1">LangChain </span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.1380.1">integrates with various image generation models </span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.1381.1">and services, allowing you to:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.1382.1">Generate images from text descriptions</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1383.1">Edit existing images based on text prompts</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1384.1">Control image generation parameters</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1385.1">Handle image variations and styles</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.1386.1">LangChain includes </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.1387.1">wrappers and models for popular image generation services. </span><span class="koboSpan" id="kobo.1387.2">First, let’s see how to generate ima</span><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.1388.1">ges with OpenAI’s DALL-E model series.</span></p>
<h3 class="heading-3" id="_idParaDest-56"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.1389.1">Using DALL-E through OpenAI</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1390.1">LangChain’s </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.1391.1">wrapper for DALL-E simplifies</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.1392.1"> the process of generating images from text prompts. </span><span class="koboSpan" id="kobo.1392.2">The implementation uses OpenAI’s API under the hood but provides a standardized interface consistent with other LangChain components.</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1393.1">from</span></span><span class="koboSpan" id="kobo.1394.1"> langchain_community.utilities.dalle_image_generator </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1395.1">import</span></span><span class="koboSpan" id="kobo.1396.1"> DallEAPIWrapper</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1397.1">dalle = DallEAPIWrapper(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1398.1">   model_name=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1399.1">"dall-e-3"</span></span><span class="koboSpan" id="kobo.1400.1">,  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1401.1"># Options: "dall-e-2" (default) or "dall-e-3"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1402.1">   size=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1403.1">"1024x1024"</span></span><span class="koboSpan" id="kobo.1404.1">,       </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1405.1"># Image dimensions</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1406.1">    quality=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1407.1">"standard"</span></span><span class="koboSpan" id="kobo.1408.1">,     </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1409.1"># "standard" or "hd" for DALL-E 3</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1410.1">    n=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1411.1">1</span></span> <span class="hljs-comment"><span class="koboSpan" id="kobo.1412.1"># Number of images to generate (only for DALL-E 2)</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1413.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1414.1"># Generate an image</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1415.1">image_url = dalle.run(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1416.1">"A detailed technical diagram of a quantum computer"</span></span><span class="koboSpan" id="kobo.1417.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1418.1"># Display the image in a notebook</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1419.1">from</span></span><span class="koboSpan" id="kobo.1420.1"> IPython.display </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1421.1">import</span></span><span class="koboSpan" id="kobo.1422.1"> Image, display</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1423.1">display(Image(url=image_url))</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1424.1"># Or save it locally</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1425.1">import</span></span><span class="koboSpan" id="kobo.1426.1"> requests</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1427.1">response = requests.get(image_url)</span></p>
<div aria-label="56" epub:type="pagebreak" id="page32" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1428.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1429.1">open</span></span><span class="koboSpan" id="kobo.1430.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1431.1">"generated_library.png"</span></span><span class="koboSpan" id="kobo.1432.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1433.1">"wb"</span></span><span class="koboSpan" id="kobo.1434.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1435.1">as</span></span><span class="koboSpan" id="kobo.1436.1"> f:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1437.1">    f.write(response.content)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1438.1">Here’s the image we got:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1439.1"><img alt="Figure 2.2: An image generated by OpenAI’s DALL-E Image Generator" src="../Images/B32363_02_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1440.1">Figure 2.2: An image generated by OpenAI’s DALL-E Image Generator</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1441.1">You might </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.1442.1">notice that text generation within </span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.1443.1">these images is not one of the strong suites of these models. </span><span class="koboSpan" id="kobo.1443.2">You can find a lot of models for image generation on Replicate, including the latest Stable Diffusi</span><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.1444.1">on models, so this is what we’ll use now.</span></p>
<div aria-label="57" epub:type="pagebreak" id="page33" role="doc-pagebreak"/>
<h3 class="heading-3" id="_idParaDest-57"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.1445.1">Using Stable Diffusion</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1446.1">Stable Diffusion </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.1447.1">3.5 Large is Stability AI’s latest</span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.1448.1"> text-to-image model, released in March 2024. </span><span class="koboSpan" id="kobo.1448.2">It’s a </span><strong class="keyWord"><span class="koboSpan" id="kobo.1449.1">Multimodal Diffusion Transformer</span></strong><span class="koboSpan" id="kobo.1450.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.1451.1">MMDiT</span></strong><span class="koboSpan" id="kobo.1452.1">) that </span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.1453.1">generates high-resolution images with remarkable detail and quality.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1454.1">This model uses three fixed, pre-trained text encoders and implements Query-Key Normalization for improved training stability. </span><span class="koboSpan" id="kobo.1454.2">It’s capable of producing diverse outputs from the same prompt and supports various artistic styles.</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1455.1">from</span></span><span class="koboSpan" id="kobo.1456.1"> langchain_community.llms </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1457.1">import</span></span><span class="koboSpan" id="kobo.1458.1"> Replicate</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1459.1"># Initialize the text-to-image model with Stable Diffusion 3.5 Large</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1460.1">text2image = Replicate(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1461.1">    model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1462.1">"stability-ai/stable-diffusion-3.5-large"</span></span><span class="koboSpan" id="kobo.1463.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1464.1">    model_kwargs={</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1465.1">"prompt_strength"</span></span><span class="koboSpan" id="kobo.1466.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1467.1">0.85</span></span><span class="koboSpan" id="kobo.1468.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1469.1">"cfg"</span></span><span class="koboSpan" id="kobo.1470.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1471.1">4.5</span></span><span class="koboSpan" id="kobo.1472.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1473.1">"steps"</span></span><span class="koboSpan" id="kobo.1474.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1475.1">40</span></span><span class="koboSpan" id="kobo.1476.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1477.1">"aspect_ratio"</span></span><span class="koboSpan" id="kobo.1478.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1479.1">"1:1"</span></span><span class="koboSpan" id="kobo.1480.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1481.1">"output_format"</span></span><span class="koboSpan" id="kobo.1482.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1483.1">"webp"</span></span><span class="koboSpan" id="kobo.1484.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1485.1">"output_quality"</span></span><span class="koboSpan" id="kobo.1486.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1487.1">90</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1488.1">    }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1489.1">)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1490.1"># Generate an image</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1491.1">image_url = text2image.invoke(</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1492.1">"A detailed technical diagram of an AI agent"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1493.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1494.1">The</span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.1495.1"> recommended parameters for the new </span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.1496.1">model include:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1497.1">prompt_strength</span></strong><span class="koboSpan" id="kobo.1498.1">: Controls how closely the image follows the prompt (0.85)</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1499.1">cfg</span></strong><span class="koboSpan" id="kobo.1500.1">: Controls how strictly the model follows the prompt (4.5)</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1501.1">steps</span></strong><span class="koboSpan" id="kobo.1502.1">: More steps result in higher-quality images (40)</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1503.1">aspect_ratio</span></strong><span class="koboSpan" id="kobo.1504.1">: Set to 1:1 for square images</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1505.1">output_format</span></strong><span class="koboSpan" id="kobo.1506.1">: Using WebP for a better quality-to-size ratio</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1507.1">output_quality</span></strong><span class="koboSpan" id="kobo.1508.1">: Set to 90 for high-quality output</span></li>
</ul>
<div aria-label="58" epub:type="pagebreak" id="page34" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1509.1">Here’s the image we got:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1510.1"><img alt="Figure 2.3: An image generated by Stable Diffusion" src="../Images/B32363_02_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1511.1">Figure 2.3: An image generated by Stable Diffusion</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1512.1">Now let’s explore how to analyze a</span><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.1513.1">nd understand images using multimodal models.</span></p>
<h2 class="heading-2" id="_idParaDest-58"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.1514.1">Image understanding</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1515.1">Image </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.1516.1">understanding refers to an AI system’s ability </span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.1517.1">to interpret and analyze visual information in ways similar to human visual perception. </span><span class="koboSpan" id="kobo.1517.2">Unlike traditional computer vision (which focuses on specific tasks like object detection or facial recognition), modern multimodal models can perform general reasoning about images, understanding context, relationships, and even implicit meaning within visual content.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1518.1">Gemini 2.5 Pro and GPT-4 Vision, among other models, can analyze images and provide detail</span><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.1519.1">ed descriptions or answer questions about them.</span></p>
<h3 class="heading-3" id="_idParaDest-59"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.1520.1">Using Gemini 1.5 Pro</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1521.1">LangChain </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.1522.1">handles multimodal input through the </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.1523.1">same </span><code class="inlineCode"><span class="koboSpan" id="kobo.1524.1">ChatModel</span></code><span class="koboSpan" id="kobo.1525.1"> interface. </span><span class="koboSpan" id="kobo.1525.2">It accepts </span><code class="inlineCode"><span class="koboSpan" id="kobo.1526.1">Messages</span></code><span class="koboSpan" id="kobo.1527.1"> as an input, and a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1528.1">Message</span></code><span class="koboSpan" id="kobo.1529.1"> object has a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1530.1">content</span></code><span class="koboSpan" id="kobo.1531.1"> field. </span><span class="koboSpan" id="kobo.1531.2">IA </span><code class="inlineCode"><span class="koboSpan" id="kobo.1532.1">content</span></code><span class="koboSpan" id="kobo.1533.1"> can consist of multiple parts, and each part can represent a different modality (that allows you to mix different modalities in your prompt).</span></p>
<div aria-label="59" epub:type="pagebreak" id="page35" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1534.1">You can send</span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.1535.1"> multimodal input by value or by reference. </span><span class="koboSpan" id="kobo.1535.2">To send it by value, you should encode bytes as </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.1536.1">a string and construct an </span><code class="inlineCode"><span class="koboSpan" id="kobo.1537.1">image_url</span></code><span class="koboSpan" id="kobo.1538.1"> variable formatted as in the example below using the image we generated using Stable Diffusion:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1539.1">import</span></span><span class="koboSpan" id="kobo.1540.1"> base64</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1541.1">from</span></span><span class="koboSpan" id="kobo.1542.1"> langchain_google_genai.chat_models </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1543.1">import</span></span><span class="koboSpan" id="kobo.1544.1"> ChatGoogleGenerativeAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1545.1">from</span></span><span class="koboSpan" id="kobo.1546.1"> langchain_core.messages.human </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1547.1">import</span></span><span class="koboSpan" id="kobo.1548.1"> HumanMessage</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1549.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1550.1">open</span></span><span class="koboSpan" id="kobo.1551.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1552.1">"stable-diffusion.png"</span></span><span class="koboSpan" id="kobo.1553.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1554.1">'rb'</span></span><span class="koboSpan" id="kobo.1555.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1556.1">as</span></span><span class="koboSpan" id="kobo.1557.1"> image_file:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1558.1">    image_bytes = image_file.read()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1559.1">    base64_bytes = base64.b64encode(image_bytes).decode(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1560.1">"utf-8"</span></span><span class="koboSpan" id="kobo.1561.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1562.1">prompt = [</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1563.1">   {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1564.1">"type"</span></span><span class="koboSpan" id="kobo.1565.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1566.1">"text"</span></span><span class="koboSpan" id="kobo.1567.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1568.1">"text"</span></span><span class="koboSpan" id="kobo.1569.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1570.1">"Describe the image: "</span></span><span class="koboSpan" id="kobo.1571.1">},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1572.1">   {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1573.1">"type"</span></span><span class="koboSpan" id="kobo.1574.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1575.1">"image_url"</span></span><span class="koboSpan" id="kobo.1576.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1577.1">"image_url"</span></span><span class="koboSpan" id="kobo.1578.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1579.1">"url"</span></span><span class="koboSpan" id="kobo.1580.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1581.1">f"data:image/jpeg;base64,</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1582.1">{base64_bytes}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1583.1">"</span></span><span class="koboSpan" id="kobo.1584.1">}},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1585.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1586.1">llm = ChatGoogleGenerativeAI(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1587.1">    model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1588.1">"gemini-1.5-pro"</span></span><span class="koboSpan" id="kobo.1589.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1590.1">    temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1591.1">0</span></span><span class="koboSpan" id="kobo.1592.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1593.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1594.1">response = llm.invoke([HumanMessage(content=prompt)])</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1595.1">print</span></span><span class="koboSpan" id="kobo.1596.1">(response.content)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1597.1">The image presents a futuristic, stylized depiction of a humanoid robot's upper body against a backdrop of glowing blue digital displays. </span><span class="koboSpan" id="kobo.1597.2">The robot's head is rounded and predominantly white, with sections of dark, possibly metallic, material around the face and ears.  </span><span class="koboSpan" id="kobo.1597.3">The face itself features glowing orange eyes and a smooth, minimalist design, lacking a nose or mouth in the traditional human sense.  </span><span class="koboSpan" id="kobo.1597.4">Small, bright dots, possibly LEDs or sensors, are scattered across the head and body, suggesting advanced technology and intricate construction.</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1598.1">The robot's neck and shoulders are visible, revealing a complex internal structure of dark, interconnected parts, possibly wires or cables, which contrast with the white exterior. </span><span class="koboSpan" id="kobo.1598.2">The shoulders and upper chest are also white, with similar glowing dots and hints of the internal mechanisms showing through. </span><span class="koboSpan" id="kobo.1598.3">The overall impression is of a sleek, sophisticated machine.</span></p>
<div aria-label="60" epub:type="pagebreak" id="page36" role="doc-pagebreak"/>
<p class="snippet-con"><span class="koboSpan" id="kobo.1599.1">The background is a grid of various digital interfaces, displaying graphs, charts, and other abstract data visualizations. </span><span class="koboSpan" id="kobo.1599.2">These elements are all in shades of blue, creating a cool, technological ambiance that complements the robot's appearance. </span><span class="koboSpan" id="kobo.1599.3">The displays vary in size and complexity, adding to the sense of a sophisticated control panel or monitoring system. </span><span class="koboSpan" id="kobo.1599.4">The combination of the robot and the background suggests a theme of advanced robotics, artificial intelligence, or data analysis.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1600.1">As multimodal inputs </span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.1601.1">typically have a large size, sending raw bytes as part of your request might not be the best idea. </span><span class="koboSpan" id="kobo.1601.2">You can send it by reference by</span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.1602.1"> pointing to the blob storage, but the specific type of storage depends on the model’s provider. </span><span class="koboSpan" id="kobo.1602.2">For example, Gemini accepts multimedia input as a reference to Google Cloud Storage – a blob storage service provided by Google Cloud.</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1603.1">prompt = [</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1604.1">   {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1605.1">"type"</span></span><span class="koboSpan" id="kobo.1606.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1607.1">"text"</span></span><span class="koboSpan" id="kobo.1608.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1609.1">"text"</span></span><span class="koboSpan" id="kobo.1610.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1611.1">"Describe the video in a few sentences."</span></span><span class="koboSpan" id="kobo.1612.1">},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1613.1">   {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1614.1">"type"</span></span><span class="koboSpan" id="kobo.1615.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1616.1">"media"</span></span><span class="koboSpan" id="kobo.1617.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1618.1">"file_uri"</span></span><span class="koboSpan" id="kobo.1619.1">: video_uri, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1620.1">"mime_type"</span></span><span class="koboSpan" id="kobo.1621.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1622.1">"video/mp4"</span></span><span class="koboSpan" id="kobo.1623.1">},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1624.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1625.1">response = llm.invoke([HumanMessage(content=prompt)])</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1626.1">print</span></span><span class="koboSpan" id="kobo.1627.1">(response.content)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1628.1">Exact details on how to construct a multimodal input might depend on the provider of the LLM (and a corresponding LangChain integration handles a dictionary corresponding to a part of a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1629.1">content</span></code><span class="koboSpan" id="kobo.1630.1"> field accordingly). </span><span class="koboSpan" id="kobo.1630.2">For example, Gemini accepts an additional </span><code class="inlineCode"><span class="koboSpan" id="kobo.1631.1">"video_metadata"</span></code><span class="koboSpan" id="kobo.1632.1"> key that can point to the start and/or end offset of a video piece to be analyzed:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1633.1">offset_hint = {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1634.1">"start_offset"</span></span><span class="koboSpan" id="kobo.1635.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1636.1">"seconds"</span></span><span class="koboSpan" id="kobo.1637.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1638.1">10</span></span><span class="koboSpan" id="kobo.1639.1">},</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1640.1">"end_offset"</span></span><span class="koboSpan" id="kobo.1641.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1642.1">"seconds"</span></span><span class="koboSpan" id="kobo.1643.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1644.1">20</span></span><span class="koboSpan" id="kobo.1645.1">},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1646.1">       }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1647.1">prompt = [</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1648.1">   {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1649.1">"type"</span></span><span class="koboSpan" id="kobo.1650.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1651.1">"text"</span></span><span class="koboSpan" id="kobo.1652.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1653.1">"text"</span></span><span class="koboSpan" id="kobo.1654.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1655.1">"Describe the video in a few sentences."</span></span><span class="koboSpan" id="kobo.1656.1">},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1657.1">   {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1658.1">"type"</span></span><span class="koboSpan" id="kobo.1659.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1660.1">"media"</span></span><span class="koboSpan" id="kobo.1661.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1662.1">"file_uri"</span></span><span class="koboSpan" id="kobo.1663.1">: video_uri, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1664.1">"mime_type"</span></span><span class="koboSpan" id="kobo.1665.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1666.1">"video/mp4"</span></span><span class="koboSpan" id="kobo.1667.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1668.1">"video_metadata"</span></span><span class="koboSpan" id="kobo.1669.1">: offset_hint},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1670.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1671.1">response = llm.invoke([HumanMessage(content=prompt)])</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1672.1">print</span></span><span class="koboSpan" id="kobo.1673.1">(response.content)</span></p>
<div aria-label="61" epub:type="pagebreak" id="page37" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1674.1">And, of course, such multimodal parts can also be templated. </span><span class="koboSpan" id="kobo.1674.2">Let’s demonstrate it with a simple template that expects an </span><a id="_idTextAnchor100"/><code class="inlineCode"><span class="koboSpan" id="kobo.1675.1">image_bytes_str</span></code><span class="koboSpan" id="kobo.1676.1"> argument that contains encoded bytes:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1677.1">prompt = ChatPromptTemplate.from_messages(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1678.1">   [(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1679.1">"user"</span></span><span class="koboSpan" id="kobo.1680.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1681.1">    [{</span><span class="hljs-string"><span class="koboSpan" id="kobo.1682.1">"type"</span></span><span class="koboSpan" id="kobo.1683.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1684.1">"image_url"</span></span><span class="koboSpan" id="kobo.1685.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1686.1">"image_url"</span></span><span class="koboSpan" id="kobo.1687.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1688.1">"url"</span></span><span class="koboSpan" id="kobo.1689.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1690.1">"data:image/jpeg;base64,{image_bytes_str}"</span></span><span class="koboSpan" id="kobo.1691.1">},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1692.1">      }])]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1693.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1694.1">prompt.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1695.1">"image_bytes_str"</span></span><span class="koboSpan" id="kobo.1696.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1697.1">"test-url"</span></span><span class="koboSpan" id="kobo.1698.1">})</span></p>
<h3 class="heading-3" id="_idParaDest-60"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.1699.1">Using GPT-4 Vision</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1700.1">After </span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.1701.1">having explored image generation, let’s examine how</span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.1702.1"> LangChain handles image understanding using multimodal models. </span><span class="koboSpan" id="kobo.1702.2">GPT-4 Vision capabilities (available in models like GPT-4o and GPT-4o-mini) allow us to analyze images alongside text, enabling applications that can “see” and reason about visual content.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1703.1">LangChain simplifies working with these models by providing a consistent interface for multimodal inputs. </span><span class="koboSpan" id="kobo.1703.2">Let’s implement a flexible image analyzer:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1704.1">from</span></span><span class="koboSpan" id="kobo.1705.1"> langchain_core.messages </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1706.1">import</span></span><span class="koboSpan" id="kobo.1707.1"> HumanMessage</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1708.1">from</span></span><span class="koboSpan" id="kobo.1709.1"> langchain_openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1710.1">import</span></span><span class="koboSpan" id="kobo.1711.1"> ChatOpenAI</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1712.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1713.1">analyze_image</span></span><span class="koboSpan" id="kobo.1714.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1715.1">image_url: </span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1716.1">str</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.1717.1">, question: </span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1718.1">str</span></span><span class="koboSpan" id="kobo.1719.1">) -&gt; </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1720.1">str</span></span><span class="koboSpan" id="kobo.1721.1">:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1722.1">    chat = ChatOpenAI(model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1723.1">"gpt-4o-mini"</span></span><span class="koboSpan" id="kobo.1724.1">, max_tokens=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1725.1">256</span></span><span class="koboSpan" id="kobo.1726.1">)</span></p>
<p class="snippet-code"> </p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1727.1">    message = HumanMessage(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1728.1">        content=[</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1729.1">            {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1730.1">"type"</span></span><span class="koboSpan" id="kobo.1731.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1732.1">"text"</span></span><span class="koboSpan" id="kobo.1733.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1734.1">"text"</span></span><span class="koboSpan" id="kobo.1735.1">: question</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1736.1">            },</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1737.1">            {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1738.1">"type"</span></span><span class="koboSpan" id="kobo.1739.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1740.1">"image_url"</span></span><span class="koboSpan" id="kobo.1741.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1742.1">"image_url"</span></span><span class="koboSpan" id="kobo.1743.1">: {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1744.1">"url"</span></span><span class="koboSpan" id="kobo.1745.1">: image_url,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1746.1">"detail"</span></span><span class="koboSpan" id="kobo.1747.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1748.1">"auto"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1749.1">                }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1750.1">            }</span></p>
<div aria-label="62" epub:type="pagebreak" id="page38" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.1751.1">        ]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1752.1">    )</span></p>
<p class="snippet-code"> </p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1753.1">    response = chat.invoke([message])</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1754.1">return</span></span><span class="koboSpan" id="kobo.1755.1"> response.content</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1756.1"># Example usage</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1757.1">image_url = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1758.1">"https://replicate.delivery/yhqm/pMrKGpyPDip0LRciwSzrSOKb5ukcyXCyft0IBElxsT7fMrLUA/out-0.png"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1759.1">questions = [</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1760.1">"What objects do you see in this image?"</span></span><span class="koboSpan" id="kobo.1761.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1762.1">"What is the overall mood or atmosphere?"</span></span><span class="koboSpan" id="kobo.1763.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1764.1">"Are there any people in the image?"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1765.1">]</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1766.1">for</span></span><span class="koboSpan" id="kobo.1767.1"> question </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1768.1">in</span></span><span class="koboSpan" id="kobo.1769.1"> questions:</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1770.1">print</span></span><span class="koboSpan" id="kobo.1771.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1772.1">f"\nQ: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1773.1">{question}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1774.1">"</span></span><span class="koboSpan" id="kobo.1775.1">)</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1776.1">print</span></span><span class="koboSpan" id="kobo.1777.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1778.1">f"A: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1779.1">{analyze_image(image_url, question)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1780.1">"</span></span><span class="koboSpan" id="kobo.1781.1">)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1782.1">The model </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.1783.1">provides a rich, detailed analysis of our generated cityscape:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1784.1">Q: What objects do you see in this image?</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1785.1">A: The image features a futuristic cityscape with tall, sleek skyscrapers. </span><span class="koboSpan" id="kobo.1785.2">The buildings appear to have a glowing or neon effect, suggesting a high-tech environment. </span><span class="koboSpan" id="kobo.1785.3">There is a large, bright sun or light source in the sky, adding to the vibrant atmosphere. </span><span class="koboSpan" id="kobo.1785.4">A road or pathway is visible in the foreground, leading toward the city, possibly with light streaks indicating motion or speed. </span><span class="koboSpan" id="kobo.1785.5">Overall, the scene conveys a dynamic, otherworldly urban landscape.</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1786.1">Q: What is the overall mood or atmosphere?</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1787.1">A: The overall mood or atmosphere of the scene is futuristic and vibrant. </span><span class="koboSpan" id="kobo.1787.2">The glowing outlines of the skyscrapers and the bright sunset create a sense of energy and possibility. </span><span class="koboSpan" id="kobo.1787.3">The combination of deep colors and light adds a dramatic yet hopeful tone, suggesting a dynamic and evolving urban environment.</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1788.1">Q: Are there any people in the image?</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1789.1">A: There are no people in the image. </span><span class="koboSpan" id="kobo.1789.2">It appears to be a futuristic cityscape with tall buildings and a sunset.</span></p>
<div aria-label="63" epub:type="pagebreak" id="page39" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1790.1">This capability </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.1791.1">opens numerous possibilities for LangChain applications. </span><span class="koboSpan" id="kobo.1791.2">By combining image analysis with the text processing patterns we explored earlier in this chapter, you can build sophisticated applications that reason across modalities. </span><span class="koboSpan" id="kobo.1791.3">In the next chapter, we’ll build on these conce</span><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.1792.1">pts to create more sophisticated multimodal applications.</span></p>
<h1 class="heading-1" id="_idParaDest-61"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.1793.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1794.1">After setting up our development environment and configuring necessary API keys, we’ve explored the foundations of LangChain development, from basic chains to multimodal capabilities. </span><span class="koboSpan" id="kobo.1794.2">We’ve seen how LCEL simplifies complex workflows and how LangChain integrates with both text and image processing. </span><span class="koboSpan" id="kobo.1794.3">These building blocks prepare us for more advanced applications in the coming chapters.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1795.1">In the next chapter, we’ll expand on these concepts to create more sophisticated multimodal applications with enhanced control flow, structured outputs, and advanced prompt techniques. </span><span class="koboSpan" id="kobo.1795.2">You’ll learn how to combine multiple modalities in complex chains, incorporate more sophisticated error handling, and build appl</span><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.1796.1">ications that leverage the full potential of modern LLMs.</span></p>
<h1 class="heading-1" id="_idParaDest-62"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.1797.1">Review questions</span></h1>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.1798.1">What are the three main limitations of raw LLMs that LangChain addresses?</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1799.1">Memory limitations</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1800.1">Tool integration</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1801.1">Context constraints</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1802.1">Processing speed</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1803.1">Cost optimization</span></li>
</ul></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1804.1">Which of the following best describes the purpose of LCEL (LangChain Expression Language)?</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1805.1">A programming language for LLMs</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1806.1">A unified interface for composing LangChain components</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1807.1">A template system for prompts</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1808.1">A testing framework for LLMs</span></li>
</ul></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1809.1">Name three types of memory systems available in LangChain</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1810.1">Compare and contrast LLMs and chat models in LangChain. </span><span class="koboSpan" id="kobo.1810.2">How do their interfaces and use cases differ?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1811.1">What role do Runnables play in LangChain? </span><span class="koboSpan" id="kobo.1811.2">How do they contribute to building modular LLM applications?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1812.1">When running models locally, which factors affect model performance? </span><span class="koboSpan" id="kobo.1812.2">(Select all that apply)</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1813.1">Available RAM</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1814.1">CPU/GPU capabilities</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1815.1">Internet connection speed</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1816.1">Model quantization level</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1817.1">Operating system type</span></li>
</ul></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1818.1">Compare the following model deployment options and identify scenarios where each would be most appropriate:</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1819.1">Cloud-based models (e.g., OpenAI)</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1820.1">Local models with llama.cpp</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1821.1">GPT4All integration</span></li>
</ul></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1822.1">Design a basic chain using LCEL that would:</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1823.1">Take a user question about a product</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1824.1">Query a database for product information</span></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1825.1">Generate a response using an LLM</span></li>
</ul></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1826.1">Provide a sketch outlining the components and how they connect.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1827.1">Compare the following approaches for image analysis and mention the trade-offs between them:</span><ul><li class="bulletList level-2"><span class="koboSpan" id="kobo.1828.1">Approach A</span><p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1829.1">from</span></span><span class="koboSpan" id="kobo.1830.1"> langchain_openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1831.1">import</span></span><span class="koboSpan" id="kobo.1832.1"> ChatOpenAI</span></p><p class="snippet-code"><span class="koboSpan" id="kobo.1833.1">chat = ChatOpenAI(model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1834.1">"gpt-4-vision-preview"</span></span><span class="koboSpan" id="kobo.1835.1">)</span></p></li>
<li class="bulletList level-2"><span class="koboSpan" id="kobo.1836.1">Approach B</span><p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1837.1">from</span></span><span class="koboSpan" id="kobo.1838.1"> langchain_community.llms </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1839.1">import</span></span><span class="koboSpan" id="kobo.1840.1"> Ollama</span></p><p class="snippet-code"><span class="koboSpan" id="kobo.1841.1">local_model = Ollama(model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1842.1">"llava"</span></span><span class="koboSpan" id="kobo.1843.1">)</span></p></li>
</ul></li>
</ol>
<div aria-label="64" epub:type="pagebreak" id="page40" role="doc-pagebreak"/>
<div aria-label="65" epub:type="pagebreak" id="page41" role="doc-pagebreak"/>
<h1 class="heading-1" id="_idParaDest-63"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.1844.1">Subscribe to our weekly newsletter</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1845.1">Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers, and innovators, at </span><a href="https://packt.link/Q5UyU"><span class="url"><span class="koboSpan" id="kobo.1846.1">https://packt.link/Q5UyU</span></span></a><span class="koboSpan" id="kobo.1847.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1848.1"><img alt="" src="../Images/Newsletter_QRcode1.jpg"/></span></p>
</div>
</body></html>