<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer053">
			<h1 id="_idParaDest-296" class="chapter-number"><a id="_idTextAnchor366"/>26</h1>
			<h1 id="_idParaDest-297"><a id="_idTextAnchor367"/>Retrieval-Augmented Generation</h1>
			<p><strong class="bold">Retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) is a<a id="_idIndexMarker1130"/> technique that enhances the performance of AI models, particularly in tasks that require knowledge or data not contained within the model’s pre-trained parameters. It combines the strengths of both retrieval-based models and generative models. The retrieval component fetches relevant information from external sources, such as databases, documents, or web content, and the generative component uses this information to produce more accurate, contextually <span class="No-Break">enriched responses.</span></p>
			<p>RAG is implemented by integrating a retrieval mechanism with a language model. The process begins by querying a knowledge base or external resource for relevant documents or snippets. These retrieved pieces of information are then fed into the language model, which generates a response by incorporating both the prompt and the retrieved data. This approach improves the model’s ability to answer questions or solve problems with up-to-date or domain-specific information that it would <span class="No-Break">otherwise lack.</span></p>
			<p>In this chapter, we’ll introduce you to RAG. You’ll learn how to implement a simple RAG system that can enhance LLM outputs with relevant <span class="No-Break">external information.</span></p>
			<p>The key benefits of RAG include enhanced factual accuracy, access to current information, improved domain-specific knowledge, and reduced hallucination in <span class="No-Break">LLM outputs.</span></p>
			<p>In this chapter, we’ll cover embedding and indexing techniques of vector databases for efficient retrieval, query formulation strategies, and methods for integrating retrieved information with LLM generation. By the end of this chapter, you’ll be able to implement basic RAG systems to augment your LLMs with <span class="No-Break">external knowledge.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Building a simple RAG system <span class="No-Break">for LLMs</span></li>
				<li>Embedding and indexing techniques for <span class="No-Break">LLM retrieval</span></li>
				<li>Query formulation strategies in <span class="No-Break">LLM-based RAG</span></li>
				<li>Integrating retrieved information with <span class="No-Break">LLM generation</span></li>
				<li>Challenges and opportunities in RAG <span class="No-Break">for LLMs</span></li>
			</ul>
			<h1 id="_idParaDest-298"><a id="_idTextAnchor368"/>Building a simple RAG system for LLMs</h1>
			<p>This section<a id="_idIndexMarker1131"/> provides a practical illustration of a simple RAG system, leveraging <a id="_idIndexMarker1132"/>the robust search capabilities <a id="_idIndexMarker1133"/>of <strong class="bold">SerpApi</strong>, the semantic understanding of sentence embeddings, and the generative prowess of OpenAI’s GPT-4o model. SerpApi is a web scraping API that provides real-time access to search engine results, offering structured data for Google, Bing, and other platforms without the need for <span class="No-Break">manual scraping.</span></p>
			<p>Through this example, we will explore the fundamental components of a RAG system, including query-based web searching, snippet extraction and ranking, and, ultimately, the generation of a comprehensive answer using a state-of-the-art LLM, highlighting the interplay between these elements in a <span class="No-Break">step-by-step manner.</span></p>
			<p>The code for the simple RAG system we’ll be building contains <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">SerpApi</strong>: To find<a id="_idIndexMarker1134"/> relevant web pages based on the <span class="No-Break">user’s query.</span></li>
				<li><strong class="bold">Sentence embeddings</strong>: To extract the most relevant snippets from the search results using sentence embeddings and cosine similarity. Sentence embeddings are dense numerical representations of text that capture semantic meaning by mapping words, phrases, or entire sentences into high-dimensional vector space, where similar meanings are positioned closer together. Cosine similarity measures the angle between these embedding vectors (ranging from -1 to 1), rather than their magnitude, making it an effective way to evaluate semantic similarity regardless of text length; when two embeddings have a cosine similarity close to 1, they’re highly similar in meaning, while values closer to 0 indicate unrelated content and negative values suggest opposing meanings. This combination of techniques powers many modern <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) applications, from <a id="_idIndexMarker1135"/>search engines and recommendation systems to language translation and <span class="No-Break">content clustering.</span></li>
				<li><strong class="bold">OpenAI’s GPT-4o</strong>: To generate a comprehensive and coherent answer based on the retrieved<a id="_idIndexMarker1136"/> snippets (context) and the <span class="No-Break">original query.</span></li>
			</ul>
			<p>First, let’s install the <span class="No-Break">following dependencies:</span></p>
			<pre class="console">
pip install google-search-results sentence-transformers openai</pre>			<p>In the preceding command, we install <strong class="source-inline">serpapi</strong> for searching, <strong class="source-inline">sentence_transformers</strong> for embedding, and <strong class="source-inline">openai</strong> for <span class="No-Break">accessing GPT-4o.</span></p>
			<p>Next, let us see how a complete RAG system is implemented using search APIs, embeddings, and <span class="No-Break">an LLM:</span></p>
			<ol>
				<li>We first import the installed libraries along with <strong class="source-inline">torch</strong> for tensor operations. The code snippet also sets up API keys for SerpApi and OpenAI. Remember to replace <a id="_idIndexMarker1137"/>the<a id="_idIndexMarker1138"/> placeholders with your actual <span class="No-Break">API keys:</span><pre class="source-code">
from serpapi import GoogleSearch
from sentence_transformers import SentenceTransformer, util
import torch
import openai
SERPAPI_KEY = "YOUR_SERPAPI_KEY"  # Replace with your SerpAPI key
OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"  # Replace with your OpenAI key
openai.api_key = OPENAI_API_KEY</pre></li>				<li>We then initialize the search engine and Sentence Transformer. The following code defines the search function to perform a Google search using SerpApi and initializes the Sentence Transformer model (<strong class="source-inline">all-mpnet-base-v2</strong>) for creating <span class="No-Break">sentence embeddings:</span><pre class="source-code">
    def search(query):
    params = {
        "q": query,
        "hl": "en",
        "gl": "us",
        "google_domain": "google.com",
        "api_key": SERPAPI_KEY,
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    return results
model = SentenceTransformer('all-mpnet-base-v2')</pre></li>				<li>Next, we retrieve relevant snippets. We define the <strong class="source-inline">retrieve_snippets</strong> function, which takes the search results, extracts snippets, computes their embeddings, and <a id="_idIndexMarker1139"/>calculates the cosine similarity<a id="_idIndexMarker1140"/> between the query embedding and each snippet embedding. It then returns the top <em class="italic">k</em> snippets that are most similar to <span class="No-Break">the query:</span><pre class="source-code">
   def retrieve_snippets(query, results, top_k=3):
    snippets = [
        result.get("snippet", "")
        for result in results.get("organic_results", [])
    ]
    if not snippets:
        return []
    query_embedding = model.encode(query,
        convert_to_tensor=True)
    snippet_embeddings = model.encode(snippets,
        convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(
        query_embedding, snippet_embeddings
    )[0]
    top_results = torch.topk(cosine_scores, k=top_k)
    return [snippets[i] for i in top_results.indices]</pre></li>				<li>We then define the <strong class="source-inline">generate_answer</strong> function to generate an answer using GPT-4o. This is<a id="_idIndexMarker1141"/> the<a id="_idIndexMarker1142"/> core of the generation part of our <span class="No-Break">RAG system:</span><pre class="source-code">
    def generate_answer(query, context):
    messages = [
        {
            "role": "system",
            "content": "You are a knowledgeable expert. Answer the user's query based only on the information provided in the context. "
                       "If the answer is not in the context, say 'I couldn't find an answer to your question in the provided context.'",
        },
        {
            "role": "user",
            "content": f"Context: {context}\n\nQuery: {query}",
        },
    ]
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.7,
        max_tokens=256
    )
    return response.choices[0].message.content</pre><p class="list-inset">This function constructs a structured prompt for an LLM to generate an answer constrained strictly to a given context. It formats the conversation as a system-user message pair, instructing the model to act as a subject matter expert and restrict its answer to the supplied information, explicitly avoiding speculation. If the information isn’t present, the system is directed to return a fallback message<a id="_idIndexMarker1143"/> indicating<a id="_idIndexMarker1144"/> that the answer couldn’t be found. The query and context are embedded directly into the user message, and the LLM (in this case, <strong class="source-inline">gpt-4o</strong>) is queried with a moderate creativity level via <strong class="source-inline">temperature=0.7</strong> and a response length cap of <strong class="source-inline">256</strong> tokens. This design makes the function reliable for context-grounded Q&amp;A tasks, particularly in RAG pipelines or constrained-answering settings such as document QA or <span class="No-Break">compliance tools.</span></p></li>				<li>Here’s the main RAG function and <span class="No-Break">example usage:</span><pre class="source-code">
   def rag_system(query):
    search_results = search(query)
    relevant_snippets = retrieve_snippets(query, search_results)
    if not relevant_snippets:
        return "Could not find any information related to your query"
    context = " ".join(relevant_snippets)
    answer = generate_answer(query, context)
    return answer
# Example usage
query = "What are the latest advancements in quantum computing?"
answer = rag_system(query)
print(answer)</pre><p class="list-inset">This code defines the <strong class="source-inline">rag_system</strong> function, which orchestrates the entire process: searching, retrieving snippets, and generating an answer. It then demonstrates how to use <strong class="source-inline">rag_system</strong> with an example query, printing the generated answer to <span class="No-Break">the console</span></p><p class="list-inset">The <strong class="source-inline">rag_system</strong> function answers a query by first searching for relevant information using <strong class="source-inline">search(query)</strong> and then extracting relevant snippets through the API called <strong class="source-inline">retrieve_snippets(query, search_results)</strong>. If no snippets are found, it returns a message indicating no information was found. If snippets are available, they are combined into a single context string and used to generate an answer through <strong class="source-inline">generate_answer(query, context)</strong>. Finally, the function returns the generated answer based on the context. In the example usage, the function is called with the query <strong class="source-inline">"What are the latest advancements in quantum computing?"</strong> and will return a generated response based on the relevant search results. In real production systems, we should implement retries and error <a id="_idIndexMarker1145"/>handling <a id="_idIndexMarker1146"/>around <strong class="source-inline">retrieve_snippets</strong> <span class="No-Break">API calls.</span></p></li>			</ol>
			<p>Before we move on to the next section, here are some things <span class="No-Break">to remember:</span></p>
			<ul>
				<li><strong class="bold">API keys</strong>: Make sure you have valid API keys for both SerpApi and OpenAI and have replaced the placeholders in <span class="No-Break">the code.</span></li>
				<li><strong class="bold">OpenAI costs</strong>: Be mindful of OpenAI API usage costs. GPT-4o can be more expensive than <span class="No-Break">other models.</span></li>
				<li><strong class="bold">Prompt engineering</strong>: The quality of the generated answer heavily depends on the prompt you provide to GPT-4o. You might need to experiment with different prompts to get the best results. Consider adding instructions about the desired answer format, length, <span class="No-Break">or style.</span></li>
				<li><strong class="bold">Error handling</strong>: For a production-ready system, add error handling (e.g., <strong class="source-inline">try-except</strong> blocks) to handle potential issues such as network problems, API errors, or <span class="No-Break">invalid inputs.</span></li>
				<li><strong class="bold">Advanced techniques</strong>: This is a basic RAG system. You can improve it further by doing <span class="No-Break">the following:</span><ul><li><strong class="bold">Better snippet selection</strong>: Consider factors such as source diversity, factuality, and <span class="No-Break">snippet length</span></li><li><strong class="bold">Iterative retrieval</strong>: Retrieve more context if the initial answer is <span class="No-Break">not satisfactory</span></li><li><strong class="bold">Fine-tuning</strong>: Fine-tune a smaller, more specialized language model on your specific domain for potentially better performance and <span class="No-Break">lower costs</span></li></ul></li>
			</ul>
			<p>We’ve successfully built a simple RAG system, covering the core components of retrieval and generation. Now that we have a functional RAG system, let’s dive deeper into the crucial techniques <a id="_idIndexMarker1147"/>that<a id="_idIndexMarker1148"/> enable efficient retrieval from large datasets: embedding and indexing. We’ll explore different methods for representing text semantically and organizing these representations for fast <span class="No-Break">similarity search.</span></p>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor369"/>Embeddings and indexing for retrieval in LLM applications</h1>
			<p>Embedding and indexing techniques<a id="_idIndexMarker1149"/> provide <a id="_idIndexMarker1150"/>efficient and effective retrieval in RAG-based LLM applications. They allow LLMs to quickly find and utilize relevant information from vast amounts of data. The following subsections provide a breakdown of <span class="No-Break">common techniques.</span></p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor370"/>Embeddings</h2>
			<p>Embeddings are <a id="_idIndexMarker1151"/>numerical vector representations of data, such as text, images, or <a id="_idIndexMarker1152"/>audio, that map complex, high-dimensional data into a continuous vector space where similar items are positioned close to each other. These vectors capture the underlying patterns, relationships, and semantic properties of the data, making it easier for machine learning models to understand and process. For text, for example, word embeddings transform words or phrases into dense vectors that represent their meaning in a way that reflects semantic relationships, such as synonyms being closer together in the vector space. Embeddings are typically learned from large datasets through techniques such as neural networks, and they serve as a foundation for tasks such as information retrieval, classification, clustering, and recommendation systems. By reducing the dimensionality of data while preserving important features, embeddings enable models to generalize better and make sense of varied input <span class="No-Break">data efficiently.</span></p>
			<p>For LLMs, text embeddings are most relevant. They are generated by passing text through a neural network (like the Sentence Transformer models we used in the <span class="No-Break">previous section).</span></p>
			<h3>Why do we need embeddings?</h3>
			<p>Embeddings are <a id="_idIndexMarker1153"/>important for<a id="_idIndexMarker1154"/> RAG applications for the <span class="No-Break">following reasons:</span></p>
			<ul>
				<li><strong class="bold">Semantic search</strong>: Embeddings enable semantic search, where you find information based on meaning rather than just <span class="No-Break">keyword matching</span></li>
				<li><strong class="bold">Contextual understanding</strong>: LLMs can use embeddings to understand the relationships between different pieces of information, improving their ability to reason and generate <span class="No-Break">relevant responses</span></li>
				<li><strong class="bold">Efficient retrieval</strong>: When combined with appropriate indexing, embeddings allow for the fast <a id="_idIndexMarker1155"/>retrieval<a id="_idIndexMarker1156"/> of relevant information from <span class="No-Break">large datasets</span></li>
			</ul>
			<h3>Common embedding technologies</h3>
			<p>Several embedding<a id="_idIndexMarker1157"/> technologies are<a id="_idIndexMarker1158"/> commonly used in RAG systems, and these vary in their underlying models, methods, and suitability for different applications. Here are some prominent embedding technologies <span class="No-Break">for RAG:</span></p>
			<ul>
				<li><strong class="bold">Pre-trained transformer-based embeddings (e.g., BERT, RoBERTa, and T5)</strong>: Transformer models such as <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>) and its variants, such as RoBERTa and T5, have<a id="_idIndexMarker1159"/> been widely used to generate dense, contextual embeddings for text. These models are fine-tuned on large corpora and capture a rich understanding of language semantics. In a RAG system, these embeddings can be used to retrieve relevant passages from a document store based on semantic similarity. The embeddings are typically high-dimensional and are generated by feeding text through the transformer model to produce a <span class="No-Break">fixed-size vector.</span></li>
				<li><strong class="bold">Sentence-BERT (SBERT)</strong>: A variation<a id="_idIndexMarker1160"/> of BERT designed for sentence-level embeddings, SBERT focuses on optimizing the model for tasks such as semantic textual similarity and clustering. It uses a Siamese network architecture to map sentences into a dense vector space where semantically similar sentences are closer together. This makes it particularly effective for tasks such as information retrieval in RAG, where retrieving semantically<a id="_idIndexMarker1161"/> relevant passages from a large corpus <span class="No-Break">is essential.</span></li>
				<li><strong class="bold">Facebook AI Similarity Search (Faiss)</strong>: Faiss is a <a id="_idIndexMarker1162"/>library developed by Facebook AI Research that provides efficient similarity search through <strong class="bold">approximate nearest neighbor</strong> (<strong class="bold">ANN</strong>) search. Faiss is not <a id="_idIndexMarker1163"/>an embedding technology by itself but works in conjunction with various embedding models to index and search over large collections of vectors. When used in RAG, Faiss enables the fast retrieval of relevant documents or passages by comparing the similarity of their embeddings against a <span class="No-Break">query embedding.</span></li>
				<li><strong class="bold">Dense retriever models (e.g., DPR and ColBERT)</strong>: <strong class="bold">Dense Passage Retrieval</strong> (<strong class="bold">DPR</strong>) is an approach to information retrieval that uses two separate encoders (usually BERT-based models) to encode both queries and passages into dense<a id="_idIndexMarker1164"/> vectors. DPR outperforms traditional sparse retrieval methods by leveraging the contextual knowledge encoded in dense embeddings. ColBERT, on the other hand, is another dense retrieval model that balances the efficiency of dense retrieval and the effectiveness of traditional methods. These models are especially useful for RAG when retrieving high-quality passages that are semantically related to <span class="No-Break">a query.</span></li>
				<li><strong class="bold">Contrastive Language-Image Pre-Training (CLIP)</strong>: While originally designed for<a id="_idIndexMarker1165"/> multimodal applications (text and image), CLIP has been adapted for text-only tasks as well. It learns embeddings by aligning text and image data in a shared vector space. Although CLIP is primarily used for multimodal tasks, its ability to represent language in a common space with images provides a flexible <a id="_idIndexMarker1166"/>embedding framework that can<a id="_idIndexMarker1167"/> be used in RAG, especially when working with <span class="No-Break">multimodal data.</span></li>
				<li><strong class="bold">Deep semantic similarity models (e.g., USE and InferSent)</strong>: Models such <a id="_idIndexMarker1168"/>as the <strong class="bold">Universal Sentence Encoder</strong> (<strong class="bold">USE</strong>) and InferSent generate sentence embeddings by capturing deeper semantic meaning, which can be used for various NLP tasks, including document retrieval. These models produce fixed-size vector representations that can be compared for similarity, making them useful for RAG when paired with <span class="No-Break">retrieval systems.</span></li>
				<li><strong class="bold">Doc2Vec</strong>: An extension of Word2Vec, Doc2Vec generates embeddings for entire documents rather than individual words. It maps variable-length text into a fixed-size vector, which can be used to retrieve semantically similar documents or passages. Though not as powerful as transformer-based models in terms of semantic <a id="_idIndexMarker1169"/>richness, Doc2Vec is still an effective tool for more lightweight retrieval tasks in <span class="No-Break">RAG applications.</span></li>
				<li><strong class="bold">Embedding-based search engines (e.g., Elasticsearch with dense vectors)</strong>: Some modern search engines, such as Elasticsearch, have integrated support for dense vectors alongside traditional keyword-based indexing. Elasticsearch can store and retrieve text embeddings, allowing for more flexible and semantically aware searches. When used with RAG, these embeddings can be used to rank documents by their relevance to a query, improving <span class="No-Break">retrieval performance.</span></li>
				<li><strong class="bold">OpenAI embeddings (e.g., GPT-based models)</strong>: OpenAI’s embeddings, derived from models such as GPT-3, are also used for RAG tasks. These embeddings are based on the language model’s ability to generate high-quality text representations, which can be indexed and searched over large corpora. While they are not as specifically tuned for retrieval as some other models (such as DPR), they are highly flexible and can be used in general-purpose <span class="No-Break">RAG applications.</span></li>
			</ul>
			<p>These embedding technologies provide various advantages depending on the specific requirements of a RAG system, such as retrieval speed, model accuracy, and the scale of the data being <a id="_idIndexMarker1170"/>processed. Each of them can be<a id="_idIndexMarker1171"/> fine-tuned and optimized for specific use cases, and the choice of embedding technology will depend on factors such as the nature of the documents being retrieved, computational resources, and <span class="No-Break">latency requirements.</span></p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor371"/>Indexing</h2>
			<p>Indexing is the process of<a id="_idIndexMarker1172"/> organizing embeddings in a data structure that <a id="_idIndexMarker1173"/>allows for fast similarity search. Think of it like the index of a book, but for vectors instead <span class="No-Break">of words.</span></p>
			<p>Using a more detailed description using LLM terminology, vector indexing technologies optimize embedding storage and retrieval by creating specialized data structures that organize high-dimensional vectors according to their similarity relationships, rather than sequential order. These structures—whether graph-based (connecting similar vectors through navigable pathways), tree-based (recursively partitioning the vector space), or quantization-based (compressing vectors while preserving similarity)—all serve the fundamental purpose of transforming an otherwise prohibitively expensive exhaustive search into a manageable process by strategically limiting the search space, enabling vector databases to handle billions of embeddings with sub-second query times while maintaining an acceptable<a id="_idIndexMarker1174"/> trade-off between speed, memory efficiency, and <span class="No-Break">result accuracy.</span></p>
			<h3>Why is indexing important?</h3>
			<p>Indexing is <a id="_idIndexMarker1175"/>important for<a id="_idIndexMarker1176"/> LLMs for the <span class="No-Break">following reasons:</span></p>
			<ul>
				<li><strong class="bold">Speed</strong>: Without indexing, you would have to compare a query embedding to every single embedding in your dataset, which is computationally expensive <span class="No-Break">and slow</span></li>
				<li><strong class="bold">Scalability</strong>: Indexing allows LLM applications to scale to handle massive datasets containing millions or even billions of <span class="No-Break">data points</span></li>
			</ul>
			<h3>Common indexing techniques</h3>
			<p>Let’s look at <a id="_idIndexMarker1177"/>some of the common indexing techniques<a id="_idIndexMarker1178"/> <span class="No-Break">for LLMs.</span></p>
			<p>For a visual diagram of these index techniques, I recommend you check out the following <span class="No-Break">website: </span><a href="https://kdb.ai/learning-hub/articles/indexing-basics/"><span class="No-Break">https://kdb.ai/learning-hub/articles/indexing-basics/</span></a></p>
			<ul>
				<li><strong class="bold">Flat index (</strong><span class="No-Break"><strong class="bold">brute force)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">How it works</strong>: Stores all<a id="_idIndexMarker1179"/> embeddings in a simple list or array. During a search, it calculates the distance (e.g., cosine similarity) between the query embedding and every embedding in <span class="No-Break">the index.</span></li><li><strong class="bold">Pros</strong>: Simple to implement and perfect accuracy (finds the true <span class="No-Break">nearest neighbors).</span></li><li><strong class="bold">Cons</strong>: Slow and computationally expensive for large datasets, as it requires an <span class="No-Break">exhaustive search.</span></li><li><strong class="bold">Suitable for</strong>: Very small datasets or when perfect accuracy is an <span class="No-Break">absolute requirement.</span></li></ul></li>
				<li><strong class="bold">Inverted file </strong><span class="No-Break"><strong class="bold">index (IVF)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">How </strong><span class="No-Break"><strong class="bold">it works</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Clustering</strong>: Divides <a id="_idIndexMarker1180"/>the embedding space into clusters using algorithms such <span class="No-Break">as </span><span class="No-Break"><em class="italic">k</em></span><span class="No-Break">-means</span></li><li><strong class="bold">Inverted index</strong>: Creates an inverted index that maps each cluster centroid to a list of the embeddings belonging to <span class="No-Break">that cluster</span></li><li><span class="No-Break"><strong class="bold">Search</strong></span><span class="No-Break">:</span><ol><li class="lower-roman">Finds the nearest cluster centroid(s) to the <span class="No-Break">query embedding</span></li><li class="lower-roman">Only searches within those clusters, significantly reducing the <span class="No-Break">search space</span></li></ol></li></ul></li><li><strong class="bold">Pros</strong>: Faster than<a id="_idIndexMarker1181"/> a flat index; relatively simple <a id="_idIndexMarker1182"/><span class="No-Break">to implement</span></li><li><strong class="bold">Cons</strong>: Approximate (might not always find the true nearest neighbors); accuracy depends on the number <span class="No-Break">of clusters</span></li><li><strong class="bold">Suitable for</strong>: Medium-sized datasets where a good balance between speed and accuracy <span class="No-Break">is needed</span></li></ul></li>
				<li><strong class="bold">Hierarchical navigable small </strong><span class="No-Break"><strong class="bold">world (HNSW)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">How </strong><span class="No-Break"><strong class="bold">it works</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Graph-based</strong>: Constructs <a id="_idIndexMarker1183"/>a hierarchical graph where each node represents <span class="No-Break">an embedding.</span></li><li><strong class="bold">Layers</strong>: The graph has multiple layers, with the top layer having long-range connections (for faster traversal) and the bottom layer having short-range connections (for <span class="No-Break">accurate search).</span></li><li><strong class="bold">Search</strong>: Starts at a random node in the top layer and greedily moves towards the query embedding by exploring connections. The search progresses down the layers, refining <span class="No-Break">the results.</span></li></ul></li><li><strong class="bold">Pros</strong>: Very fast and accurate; often considered the state of the art for <span class="No-Break">ANN search</span></li><li><strong class="bold">Cons</strong>: More complex to implement than IVF, and higher memory overhead due to the <span class="No-Break">graph structure</span></li><li><strong class="bold">Suitable for</strong>: Large datasets where both speed and accuracy <span class="No-Break">are crucial</span></li></ul></li>
				<li><strong class="bold">Product </strong><span class="No-Break"><strong class="bold">quantization (PQ)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">How </strong><span class="No-Break"><strong class="bold">it works</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Subvectors</strong>: Divides <a id="_idIndexMarker1184"/>each embedding into <span class="No-Break">multiple subvectors.</span></li><li><strong class="bold">Codebooks</strong>: Creates separate codebooks for each subvector using clustering. Each codebook contains a set of representative <span class="No-Break">subvectors (centroids).</span></li><li><strong class="bold">Encoding</strong>: Encodes each embedding by replacing its subvectors with the closest<a id="_idIndexMarker1185"/> centroids<a id="_idIndexMarker1186"/> from the corresponding codebooks. This creates a compressed representation of <span class="No-Break">the embedding.</span></li><li><strong class="bold">Search</strong>: Calculates the approximate distance between the query and the encoded embeddings using pre-computed distances between the query’s subvectors and the <span class="No-Break">codebook centroids.</span></li></ul></li><li><strong class="bold">Pros</strong>: Significantly reduces memory usage by compressing embeddings; <span class="No-Break">fast search.</span></li><li><strong class="bold">Cons</strong>: Approximate, and accuracy depends on the number of subvectors and the size of <span class="No-Break">the codebooks.</span></li><li><strong class="bold">Suitable for</strong>: Very<a id="_idIndexMarker1187"/> large datasets where memory efficiency is a <span class="No-Break">primary concern.</span></li></ul></li>
				<li><strong class="bold">Locality sensitive </strong><span class="No-Break"><strong class="bold">hashing (LSH)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">How it works</strong>: Uses hash<a id="_idIndexMarker1188"/> functions to map similar embeddings to the same “bucket” with <span class="No-Break">high probability</span></li><li><strong class="bold">Pros</strong>: Relatively simple; can be distributed across <span class="No-Break">multiple machines</span></li><li><strong class="bold">Cons</strong>: Approximate, and performance depends on the choice of hash functions and the number <span class="No-Break">of buckets</span></li><li><strong class="bold">Suitable for</strong>: Very large, <span class="No-Break">high-dimensional datasets</span></li></ul></li>
			</ul>
			<p>Now that we’ve covered different indexing methods, let’s introduce some popular libraries and tools that implement these indexing techniques, making them easier to use in practice. This<a id="_idIndexMarker1189"/> will provide a practical perspective on how to leverage these technologies in<a id="_idIndexMarker1190"/> your <span class="No-Break">RAG applications.</span></p>
			<p>The following are some libraries and tools for <span class="No-Break">implementing indexing:</span></p>
			<ul>
				<li><strong class="bold">Faiss</strong>: A highly <a id="_idIndexMarker1191"/>optimized library developed by Facebook AI for efficient similarity search and clustering of dense vectors. It implements many of the indexing techniques mentioned previously (flat, IVF, HNSW, <span class="No-Break">and PQ).</span></li>
				<li><strong class="bold">Approximate Nearest Neighbors Oh Yeah (Annoy)</strong>: Another popular library for ANN search, known for its ease of use and good performance. It uses a <span class="No-Break">tree-based approach.</span></li>
				<li><strong class="bold">Scalable Nearest Neighbors (ScaNN)</strong>: A library developed by Google, designed for large-scale, <span class="No-Break">high-dimensional datasets.</span></li>
				<li><strong class="bold">Vespa.ai</strong>: Provide tools to query, organize, and make inferences in vectors, tensors, text, and structured data. It is used <span class="No-Break">by </span><a href="https://www.perplexity.ai/"><span class="No-Break">https://www.perplexity.ai/</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Pinecone, Weaviate, Milvus, Qdrant</strong>: Vector databases designed specifically for storing and searching embeddings. They handle indexing, scaling, and other <span class="No-Break">infrastructure concerns.</span></li>
			</ul>
			<p>The best embedding <a id="_idIndexMarker1192"/>and indexing techniques for your LLM application will depend on <span class="No-Break">several factors:</span></p>
			<ul>
				<li><strong class="bold">Dataset size</strong>: For small datasets, a flat index might be sufficient. For large datasets, consider HNSW, IVF, <span class="No-Break">or PQ.</span></li>
				<li><strong class="bold">Speed requirements</strong>: If low latency is critical, HNSW is generally the <span class="No-Break">fastest option.</span></li>
				<li><strong class="bold">Accuracy requirements</strong>: If perfect accuracy is required, a flat index is the only choice, but it’s not scalable. HNSW often provides the best accuracy among <span class="No-Break">approximate methods.</span></li>
				<li><strong class="bold">Memory constraints</strong>: If memory is limited, PQ can significantly reduce <span class="No-Break">storage requirements.</span></li>
				<li><strong class="bold">Development effort</strong>: Faiss and Annoy offer a good balance between performance and ease of implementation. Vector databases simplify <span class="No-Break">infrastructure management.</span></li>
			</ul>
			<p>By carefully considering these factors and understanding the strengths and weaknesses of each technique and library, you can choose the most appropriate embedding and indexing methods to build efficient and effective <span class="No-Break">LLM applications.</span></p>
			<p>We’ll now demonstrate an example involving embedding, indexing, and searching using Faiss, a powerful library for efficient similarity search. I’ll use the <strong class="source-inline">all-mpnet-base-v2</strong> Sentence Transformer<a id="_idIndexMarker1193"/> model to generate embeddings. Since<a id="_idIndexMarker1194"/> the code will be more than 20 lines, I’ll break it down into blocks with explanations preceding <span class="No-Break">each block.</span></p>
			<h2 id="_idParaDest-302"><a id="_idTextAnchor372"/>Example code demonstrating embedding, indexing, and searching</h2>
			<p>In this section, we’ll be <a id="_idIndexMarker1195"/>showing the code <a id="_idIndexMarker1196"/>for a typical workflow <a id="_idIndexMarker1197"/>for using embeddings and indexing to enable fast similarity search within a collection of text documents: Here’s what <span class="No-Break">it does:</span></p>
			<ol>
				<li><strong class="bold">Loads a Sentence Transformer model</strong>: Initializes a pre-trained model for generating <span class="No-Break">sentence embeddings.</span></li>
				<li><strong class="bold">Creates sample data</strong>: Defines a list of example sentences (you would replace this with your <span class="No-Break">actual data).</span></li>
				<li><strong class="bold">Generates embeddings</strong>: Uses <strong class="source-inline">SentenceTransformer</strong> to create embeddings for <span class="No-Break">each sentence.</span></li>
				<li><strong class="bold">Creates an index</strong>: Builds a Faiss index (using <strong class="source-inline">IndexFlatL2</strong> for a flat L2 distance index in this example) to store <span class="No-Break">the embeddings.</span></li>
				<li><strong class="bold">Adds embeddings to the index</strong>: Adds the generated embeddings to the <span class="No-Break">Faiss index.</span></li>
				<li><strong class="bold">Defines a search query</strong>: Sets a sample query for which we want to find <span class="No-Break">similar sentences.</span></li>
				<li><strong class="bold">Encodes the query</strong>: Creates an embedding for the search query using the same Sentence <span class="No-Break">Transformer model.</span></li>
				<li><strong class="bold">Performs a search</strong>: Uses the Faiss index to search for the <em class="italic">k</em> most similar embeddings to the <span class="No-Break">query embedding.</span></li>
				<li><strong class="bold">Prints the results</strong>: Displays the indices and distances of the <em class="italic">k</em> nearest neighbors found in <span class="No-Break">the index.</span></li>
			</ol>
			<p>Before we check out the code, let us install the <span class="No-Break">following dependencies:</span></p>
			<pre class="console">
pip install faiss-cpu sentence-transformers
# Use faiss-gpu if you have a compatible GPU</pre>			<p>Let us now see the <span class="No-Break">code example:</span></p>
			<ol>
				<li>First, we import the necessary libraries—<strong class="source-inline">sentence_transformers</strong> for creating embeddings and <strong class="source-inline">faiss</strong> for indexing and searching—and load the <strong class="source-inline">all-mpnet-base-v2</strong> Sentence <span class="No-Break">Transformer model:</span><pre class="source-code">
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
# Load the SentenceTransformer model
model = SentenceTransformer('all-mpnet-base-v2')</pre></li>				<li>We then <a id="_idIndexMarker1198"/>prepare<a id="_idIndexMarker1199"/> the data by defining <a id="_idIndexMarker1200"/>some sample sentences (you can replace these with your actual data) and then use the Sentence Transformer model to generate embeddings for each sentence (the embeddings are converted to float32, which is required <span class="No-Break">by Faiss):</span><pre class="source-code">
# Sample sentences
text_data = [
    "A man is walking his dog in the park.",
    "Children are playing with toys indoors.",
    "An artist is painting a landscape on canvas.",
    "The sun sets behind the mountain ridge.",
    "Birds are singing outside the window."
]
# Generate vector representations using a SentenceTransformer model
import numpy as np
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')  # Replace with your model if different
vectors = model.encode(text_data, convert_to_tensor=True)
# Ensure compatibility with Faiss by converting to 32-bit floating point and moving to CPU
vectors = vectors.detach().cpu().numpy().astype(np.float32)</pre></li>				<li>We then <a id="_idIndexMarker1201"/>create <a id="_idIndexMarker1202"/>a Faiss <a id="_idIndexMarker1203"/>index and add the embeddings <span class="No-Break">to it:</span><pre class="source-code">
# Get the dimensionality of the embeddings
dimension = embeddings.shape[1]
# Create a Faiss index (flat L2 distance)
index = faiss.IndexFlatL2(dimension)
# Add the embeddings to the index
index.add(embeddings)</pre><p class="list-inset">Here, we’re using IndexFlatL2, which is a flat index that uses L2 distance (Euclidean distance) for similarity comparisons. This type of index provides accurate results but can be slow for very large datasets. The index is created with the correct dimensionality (<strong class="source-inline">768</strong> for this Sentence <span class="No-Break">Transformer model).</span></p></li>				<li>Next, we define<a id="_idIndexMarker1204"/> a <a id="_idIndexMarker1205"/>sample search query and encode<a id="_idIndexMarker1206"/> it into an embedding using the same Sentence Transformer model. The query embedding is also converted <span class="No-Break">to float32:</span><pre class="source-code">
# Define a search query
query = "What is the dog doing?"
# Encode the query
query_embedding = model.encode(query, convert_to_tensor=True)
query_embedding = \
    query_embedding.cpu().numpy().astype('float32')</pre></li>				<li>Finally, we perform similarity search using the <strong class="source-inline">index.search()</strong> method. We search for the two most similar sentences (<em class="italic">k</em>=2). The method returns the distances and the indices of the nearest neighbors. We then print the indices and distances of the nearest <span class="No-Break">neighbors found:</span><pre class="source-code">
# Search for the k nearest neighbors
k = 2
distances, indices = index.search(query_embedding, k)
# Print the results
print("Nearest neighbors:")
for i, idx in enumerate(indices[0]):
    print(f"  Index: {idx}, Distance: {distances[0][i]},
    Sentence: {sentences[idx]}")</pre></li>			</ol>
			<p>The following is sample output you might get from running the preceding <span class="No-Break">code blocks:</span></p>
			<pre class="console">
Nearest neighbors:
  Index: 0, Distance: 0.634912312, Sentence: A man is walking his dog in the park.
  Index: 1, Distance: 1.237844944, Sentence: Children are playing with toys indoors.</pre>			<p>This demonstrates how semantic similarity search works using Sentence Transformers and Faiss. Note that actual numbers will vary depending on the hardware, model versions, and <span class="No-Break">runtime conditions.</span></p>
			<p>Here’s <span class="No-Break">what’s happening.</span></p>
			<p>The query <strong class="source-inline">"What is the dog doing?"</strong> is embedded and compared against all embedded sentences in the list. Faiss retrieves the two most semantically similar sentences based on Euclidean (L2) distance in the embedding space. The smallest distance indicates the highest similarity. In this example, the sentence about the man walking his dog is closest to the query, which makes <span class="No-Break">sense semantically.</span></p>
			<p>If you’re running this<a id="_idIndexMarker1207"/> on<a id="_idIndexMarker1208"/> your <a id="_idIndexMarker1209"/>machine, your values may look different due to the non-determinism in model initialization and floating-point precision, but the closest sentence should consistently be the one most semantically related to <span class="No-Break">the query.</span></p>
			<p class="callout-heading">Important</p>
			<p class="callout"><strong class="bold">Index type</strong>: For very large<a id="_idIndexMarker1210"/> datasets, you would likely want to use a more advanced index type, such as <strong class="source-inline">IndexIVFFlat</strong> or <strong class="source-inline">IndexHNSWFlat</strong> from Faiss, to improve <span class="No-Break">search speed.</span></p>
			<p class="callout"><strong class="bold">GPU acceleration</strong>: If you<a id="_idIndexMarker1211"/> have a compatible GPU, you can install <strong class="source-inline">faiss-gpu</strong> to significantly speed up indexing <span class="No-Break">and searching.</span></p>
			<p class="callout"><strong class="bold">Data preprocessing</strong>: For<a id="_idIndexMarker1212"/> real-world applications, you might need to perform additional data preprocessing steps, such as lowercasing, removing punctuation, or stemming/lemmatization, depending on your specific needs and the nature of <span class="No-Break">your data.</span></p>
			<p class="callout"><strong class="bold">Distance metrics</strong>: Faiss supports <a id="_idIndexMarker1213"/>different distance metrics. We used L2 distance here, but you could also use the inner product (IndexFlatIP) or other metrics depending on how your embeddings are generated and what kind of similarity you want <span class="No-Break">to measure.</span></p>
			<p class="callout"><strong class="bold">Vector databases</strong>: For<a id="_idIndexMarker1214"/> production-level systems, consider using a dedicated vector database such as Pinecone, Weaviate, or Milvus to manage your embeddings and indexes more efficiently. They often provide features such as automatic indexing, scaling, and data management, which simplify the deployment of similarity <span class="No-Break">search applications.</span></p>
			<p>We’ve covered the fundamentals of embeddings, indexing, and searching with Faiss, along with important considerations for real-world implementation. Now, let’s turn our attention to <a id="_idIndexMarker1215"/>another <a id="_idIndexMarker1216"/>crucial aspect of RAG: query formulation. We’ll <a id="_idIndexMarker1217"/>explore various strategies to refine and expand user queries, ultimately leading to more effective information retrieval from the <span class="No-Break">knowledge base.</span></p>
			<h1 id="_idParaDest-303"><a id="_idTextAnchor373"/>Query formulation strategies in LLM-based RAG</h1>
			<p>Query formulation <a id="_idIndexMarker1218"/>strategies in <a id="_idIndexMarker1219"/>LLM-based RAG systems aim to enhance retrieval by improving the expressiveness and coverage of user queries. Common expansion strategies include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Synonym and paraphrase expansion</strong>: This involves generating semantically equivalent alternatives using LLMs or lexical resources. For example, expanding “climate change impact” to include “effects of global warming” or “environmental consequences of climate change” can help match a broader range <span class="No-Break">of documents.</span></li>
				<li><strong class="bold">Contextual reformulation</strong>: LLMs can reinterpret queries by inferring their intent based on conversational or document context. This helps in tailoring the query to better align with how the information might be expressed in the <span class="No-Break">knowledge base.</span></li>
				<li><strong class="bold">Pseudo-relevance feedback</strong>: Also known as blind relevance feedback, this strategy involves running an initial query, analyzing the top-ranked documents for salient terms, and using these terms to expand the query. While effective, it requires safeguards against <span class="No-Break">topic drift.</span></li>
				<li><strong class="bold">Template-based augmentation</strong>: Useful in structured domains, this method uses domain-specific templates or patterns to systematically generate variants. For example, a medical query about “treatment for hypertension” might also include “hypertension therapy” or “managing high <span class="No-Break">blood pressure.”</span></li>
				<li><strong class="bold">Entity and concept linking</strong>: Named entities and domain concepts in the query are identified and replaced or augmented with their aliases, definitions, or hierarchical relations. This is often guided by ontologies or <span class="No-Break">knowledge graphs.</span></li>
				<li><strong class="bold">Prompt-based query rewriting</strong>: With LLMs, prompts can be crafted to explicitly instruct the model to generate reformulated queries. This is particularly useful in multilingual or multi-domain RAG systems, where queries need to be adapted to match the style and vocabulary of the <span class="No-Break">target corpus.</span></li>
			</ul>
			<p>Each strategy <a id="_idIndexMarker1220"/>contributes<a id="_idIndexMarker1221"/> differently to recall and precision. Choosing or combining them depends on the structure and variability of the underlying <span class="No-Break">knowledge base.</span></p>
			<p>In the following code, the <strong class="source-inline">QueryExpansionRAG</strong> implementation uses a prompt-based query rewriting strategy powered by a pre-trained sequence-to-sequence language model (specifically, T5-small). This approach instructs the model to generate alternative phrasings of the input query by prefixing the prompt with <strong class="source-inline">"expand query:"</strong>. The generated expansions reflect paraphrastic reformulation, where the model synthesizes <a id="_idIndexMarker1222"/>semantically <a id="_idIndexMarker1223"/>related variations to increase <span class="No-Break">retrieval coverage:</span></p>
			<pre class="source-code">
from transformers import pipeline
class QueryExpansionRAG(AdvancedRAG):
    def __init__(
        self, model_name, knowledge_base,
        query_expansion_model="t5-small"
    ):
        super().__init__(model_name, knowledge_base)
        self.query_expander = pipeline(
            "text2text-generation", model=query_expansion_model
        )
    def expand_query(self, query):
        expanded = self.query_expander(
            f"expand query: {query}", max_length=50,
            num_return_sequences=3
        )
        return [query] + [e['generated_text'] for e in expanded]
    def retrieve(self, query, k=5):
        expanded_queries = self.expand_query(query)
        all_retrieved = []
        for q in expanded_queries:
            all_retrieved.extend(super().retrieve(q, k))
        # Remove duplicates and return top k
        unique_retrieved = list(dict.fromkeys(all_retrieved))
        return unique_retrieved[:k]
# Example usage
rag_system = QueryExpansionRAG(model_name, knowledge_base)
retrieved_docs = rag_system.retrieve(query)
print("Retrieved documents:", retrieved_docs)</pre>			<p>This code defines a <strong class="source-inline">QueryExpansionRAG</strong> class that extends a RAG framework by incorporating query expansion using a pre-trained T5 model. When a user submits a query, the <strong class="source-inline">expand_query</strong> method uses the T5 model through a text-to-text generation pipeline to produce multiple alternative phrasings of the query, which are then combined with the original query. The <strong class="source-inline">retrieve</strong> method iterates over these expanded queries, retrieving documents for each one and aggregating the results while removing duplicates. This approach increases the chances of retrieving relevant content by broadening the lexical and semantic scope of the original query, making it especially effective when<a id="_idIndexMarker1224"/> the<a id="_idIndexMarker1225"/> knowledge base expresses information in <span class="No-Break">varied ways.</span></p>
			<p>Keep in mind that poorly expanded queries can introduce noise and reduce retrieval precision. In this implementation, expansions generated by the T5 model are combined with the original query, increasing coverage. However, to maintain a balance, consider reranking results using similarity scores or assigning lower weights to generated expansions during retrieval. This helps ensure that expansions improve recall without compromising the alignment with the <span class="No-Break">original intent.</span></p>
			<p>We’ve seen how query expansion can enhance retrieval in RAG systems, but it’s essential to manage the trade-off between recall and precision. Now, let’s shift our focus to the other side of the RAG pipeline: integrating the retrieved information with the LLM to generate the final answer. We’ll explore how to craft prompts that effectively leverage<a id="_idIndexMarker1226"/> the <span class="No-Break">retrieved</span><span class="No-Break"><a id="_idIndexMarker1227"/></span><span class="No-Break"> context.</span></p>
			<h1 id="_idParaDest-304"><a id="_idTextAnchor374"/>Integrating retrieved information with LLM generation</h1>
			<p>To integrate<a id="_idIndexMarker1228"/> retrieved information <a id="_idIndexMarker1229"/>with LLM generation, we can create a prompt that incorporates the <span class="No-Break">retrieved documents:</span></p>
			<pre class="source-code">
from transformers import AutoModelForCausalLM
class GenerativeRAG(QueryExpansionRAG):
    def __init__(
        self, retriever_model, generator_model, knowledge_base
    ):
        super().__init__(retriever_model, knowledge_base)
        self.generator = \
            AutoModelForCausalLM.from_pretrained(generator_model)
        self.generator_tokenizer = \
            AutoTokenizer.from_pretrained(generator_model)
    def generate_response(self, query, max_length=100):
        retrieved_docs = self.retrieve(query)
        context = "\n".join(retrieved_docs)
        prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
        inputs = self.generator_tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs,
            max_length=max_length)
        return self.generator_tokenizer.decode(
            outputs[0], skip_special_tokens=True)
# Example usage
retriever_model = "all-MiniLM-L6-v2"
generator_model = "gpt2-medium"
rag_system = GenerativeRAG(
    retriever_model, generator_model, knowledge_base
)
response = rag_system.generate_response(query)
print("Generated response:", response)</pre>			<p>In the preceding code snippet, the <strong class="source-inline">GenerativeRAG</strong> class extends a RAG pipeline by integrating a causal language model for answer generation. It inherits from <strong class="source-inline">QueryExpansionRAG</strong>, which already provides retrieval functionality, and adds a generator component using Hugging Face’s <strong class="source-inline">AutoModelForCausalLM</strong>. In the constructor, it initializes the generator model and tokenizer based on the given model name. The <strong class="source-inline">generate_response</strong> method first retrieves relevant documents for a given query, concatenates them into a single context string, and constructs a prompt that combines this context with the question. This prompt is then tokenized and passed into the language model, which generates a text continuation as the answer. The final output is obtained by decoding the generated tokens into a string. This modular structure separates the retrieval and generation steps, making it easy to scale or replace individual components depending on the task or model <span class="No-Break">performance requirements.</span></p>
			<p>Having covered the basics of RAG systems, we will now focus on real-world challenges, such as scalability, dynamic<a id="_idIndexMarker1230"/> updates, and<a id="_idIndexMarker1231"/> multilingual retrieval. Specifically, we will discuss how a sharded indexing architecture can improve retrieval efficiency at scale, highlighting its impact on performance in <span class="No-Break">data-heavy environments.</span></p>
			<h1 id="_idParaDest-305"><a id="_idTextAnchor375"/>Challenges and opportunities in RAG for LLMs</h1>
			<p>Some key challenges<a id="_idIndexMarker1232"/> and opportunities in RAG include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Scalability</strong>: Efficiently handling very large <span class="No-Break">knowledge bases.</span></li>
				<li><strong class="bold">Dynamic knowledge updating</strong>: Keeping the knowledge <span class="No-Break">base current.</span></li>
				<li><strong class="bold">Cross-lingual RAG</strong>: Retrieving and generating in <span class="No-Break">multiple languages.</span></li>
				<li><strong class="bold">Multi-modal RAG</strong>: Incorporating non-text information in retrieval <span class="No-Break">and generation.</span><p class="list-inset">Keep in mind that cross-lingual and multi-modal RAG will need specialized retrieval pipelines or adapters because standard retrieval approaches often struggle with semantic matching across languages or modalities, requiring dedicated components that can properly encode, align, and retrieve relevant information regardless of the source language or format while maintaining contextual understanding <span class="No-Break">and relevance.</span></p></li>
				<li><strong class="bold">Explainable RAG</strong>: Providing transparency in the retrieval and <span class="No-Break">generation process.</span></li>
			</ul>
			<p>To keep this chapter from becoming too long, in this section, we will only show an example on how to address the scalability challenge by implementing a sharded index. A sharded index refers to a distributed data structure that partitions the index into multiple smaller, manageable segments called shards, each stored and maintained independently across different nodes or storage units. This approach enables parallel processing, reduces lookup time, and mitigates bottlenecks associated with centralized indexing, making it suitable for handling large-scale datasets or high query volumes commonly encountered in <span class="No-Break">AI applications:</span></p>
			<pre class="source-code">
class ShardedRAG(GenerativeRAG):
    def __init__(
        self, retriever_model, generator_model,
        knowledge_base, num_shards=5
    ):
        super().__init__(retriever_model, generator_model,
            knowledge_base)
        self.num_shards = num_shards
        self.sharded_indexes = self.build_sharded_index()
    def build_sharded_index(self):
        embeddings = self.get_embeddings(self.knowledge_base)
        sharded_indexes = []
        shard_size = len(embeddings) // self.num_shards
        for i in range(self.num_shards):
            start = i * shard_size
            end = start + shard_size if i &lt; self.num_shards - 1
                else len(embeddings)
            shard_index = faiss.IndexFlatL2(embeddings.shape[1])
            shard_index.add(embeddings[start:end])
            sharded_indexes.append(shard_index)
        return sharded_indexes
    def retrieve(self, query, k=5):
        query_embedding = self.get_embeddings([query])[0]
        all_retrieved = []
        for shard_index in self.sharded_indexes:
            _, indices = shard_index.search(
                np.array([query_embedding]), k)
            all_retrieved.extend([self.knowledge_base[i]
                for i in indices[0]])
        # Remove duplicates and return top k
        unique_retrieved = list(dict.fromkeys(all_retrieved))
        return unique_retrieved[:k]
# Example usage
sharded_rag = ShardedRAG(retriever_model, generator_model,
    knowledge_base)
response = sharded_rag.generate_response(query)
print("Generated response:", response)</pre>			<p>In the preceding code, the scalability is handled by dividing the knowledge base into multiple smaller indexes, or shards, each containing a portion of the overall data. This approach reduces the computational and memory burden on any single index and allows retrieval operations to remain efficient even as the dataset grows. During a query, the system embeds the query once, searches across all shards independently, and then merges the results. This design avoids bottlenecks that would arise from searching a single<a id="_idIndexMarker1233"/> large index and makes it feasible to scale to much larger knowledge bases. It also lays the groundwork for fur<a id="_idTextAnchor376"/>ther optimizations, such as parallelizing shard queries or distributing them across <span class="No-Break">multiple machines.</span></p>
			<h1 id="_idParaDest-306"><a id="_idTextAnchor377"/>Summary</h1>
			<p>RAG is a powerful technique for enhancing LLMs with external knowledge. By implementing the strategies and techniques discussed in this chapter, you can create more informed and accurate language models capable of accessing and utilizing vast amounts <span class="No-Break">of information.</span></p>
			<p>As we move forward, the next chapter will explore graph-based RAG for LLMs, which extends the RAG concept to leverage structured knowledge representations. This will further enhance the ability of LLMs to reason over complex relationships and generate more contextually <span class="No-Break">appropriate responses.</span></p>
		</div>
	</div></div></body></html>