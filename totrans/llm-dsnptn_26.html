<html><head></head><body><div><div><div><h1 id="_idParaDest-296" class="chapter-number"><a id="_idTextAnchor366"/>26</h1>
			<h1 id="_idParaDest-297"><a id="_idTextAnchor367"/>Retrieval-Augmented Generation</h1>
			<p><strong class="bold">Retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) is a<a id="_idIndexMarker1130"/> technique that enhances the performance of AI models, particularly in tasks that require knowledge or data not contained within the model’s pre-trained parameters. It combines the strengths of both retrieval-based models and generative models. The retrieval component fetches relevant information from external sources, such as databases, documents, or web content, and the generative component uses this information to produce more accurate, contextually enriched responses.</p>
			<p>RAG is implemented by integrating a retrieval mechanism with a language model. The process begins by querying a knowledge base or external resource for relevant documents or snippets. These retrieved pieces of information are then fed into the language model, which generates a response by incorporating both the prompt and the retrieved data. This approach improves the model’s ability to answer questions or solve problems with up-to-date or domain-specific information that it would otherwise lack.</p>
			<p>In this chapter, we’ll introduce you to RAG. You’ll learn how to implement a simple RAG system that can enhance LLM outputs with relevant external information.</p>
			<p>The key benefits of RAG include enhanced factual accuracy, access to current information, improved domain-specific knowledge, and reduced hallucination in LLM outputs.</p>
			<p>In this chapter, we’ll cover embedding and indexing techniques of vector databases for efficient retrieval, query formulation strategies, and methods for integrating retrieved information with LLM generation. By the end of this chapter, you’ll be able to implement basic RAG systems to augment your LLMs with external knowledge.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Building a simple RAG system for LLMs</li>
				<li>Embedding and indexing techniques for LLM retrieval</li>
				<li>Query formulation strategies in LLM-based RAG</li>
				<li>Integrating retrieved information with LLM generation</li>
				<li>Challenges and opportunities in RAG for LLMs</li>
			</ul>
			<h1 id="_idParaDest-298"><a id="_idTextAnchor368"/>Building a simple RAG system for LLMs</h1>
			<p>This section<a id="_idIndexMarker1131"/> provides a practical illustration of a simple RAG system, leveraging <a id="_idIndexMarker1132"/>the robust search capabilities <a id="_idIndexMarker1133"/>of <strong class="bold">SerpApi</strong>, the semantic understanding of sentence embeddings, and the generative prowess of OpenAI’s GPT-4o model. SerpApi is a web scraping API that provides real-time access to search engine results, offering structured data for Google, Bing, and other platforms without the need for manual scraping.</p>
			<p>Through this example, we will explore the fundamental components of a RAG system, including query-based web searching, snippet extraction and ranking, and, ultimately, the generation of a comprehensive answer using a state-of-the-art LLM, highlighting the interplay between these elements in a step-by-step manner.</p>
			<p>The code for the simple RAG system we’ll be building contains the following:</p>
			<ul>
				<li><strong class="bold">SerpApi</strong>: To find<a id="_idIndexMarker1134"/> relevant web pages based on the user’s query.</li>
				<li><strong class="bold">Sentence embeddings</strong>: To extract the most relevant snippets from the search results using sentence embeddings and cosine similarity. Sentence embeddings are dense numerical representations of text that capture semantic meaning by mapping words, phrases, or entire sentences into high-dimensional vector space, where similar meanings are positioned closer together. Cosine similarity measures the angle between these embedding vectors (ranging from -1 to 1), rather than their magnitude, making it an effective way to evaluate semantic similarity regardless of text length; when two embeddings have a cosine similarity close to 1, they’re highly similar in meaning, while values closer to 0 indicate unrelated content and negative values suggest opposing meanings. This combination of techniques powers many modern <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) applications, from <a id="_idIndexMarker1135"/>search engines and recommendation systems to language translation and content clustering.</li>
				<li><strong class="bold">OpenAI’s GPT-4o</strong>: To generate a comprehensive and coherent answer based on the retrieved<a id="_idIndexMarker1136"/> snippets (context) and the original query.</li>
			</ul>
			<p>First, let’s install the following dependencies:</p>
			<pre class="console">
pip install google-search-results sentence-transformers openai</pre>			<p>In the preceding command, we install <code>serpapi</code> for searching, <code>sentence_transformers</code> for embedding, and <code>openai</code> for accessing GPT-4o.</p>
			<p>Next, let us see how a complete RAG system is implemented using search APIs, embeddings, and an LLM:</p>
			<ol>
				<li>We first import the installed libraries along with <code>torch</code> for tensor operations. The code snippet also sets up API keys for SerpApi and OpenAI. Remember to replace <a id="_idIndexMarker1137"/>the<a id="_idIndexMarker1138"/> placeholders with your actual API keys:<pre class="source-code">
from serpapi import GoogleSearch
from sentence_transformers import SentenceTransformer, util
import torch
import openai
SERPAPI_KEY = "YOUR_SERPAPI_KEY"  # Replace with your SerpAPI key
OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"  # Replace with your OpenAI key
openai.api_key = OPENAI_API_KEY</pre></li>				<li>We then initialize the search engine and Sentence Transformer. The following code defines the search function to perform a Google search using SerpApi and initializes the Sentence Transformer model (<code>all-mpnet-base-v2</code>) for creating sentence embeddings:<pre class="source-code">
    def search(query):
    params = {
        "q": query,
        "hl": "en",
        "gl": "us",
        "google_domain": "google.com",
        "api_key": SERPAPI_KEY,
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    return results
model = SentenceTransformer('all-mpnet-base-v2')</pre></li>				<li>Next, we retrieve relevant snippets. We define the <code>retrieve_snippets</code> function, which takes the search results, extracts snippets, computes their embeddings, and <a id="_idIndexMarker1139"/>calculates the cosine similarity<a id="_idIndexMarker1140"/> between the query embedding and each snippet embedding. It then returns the top <em class="italic">k</em> snippets that are most similar to the query:<pre class="source-code">
   def retrieve_snippets(query, results, top_k=3):
    snippets = [
        result.get("snippet", "")
        for result in results.get("organic_results", [])
    ]
    if not snippets:
        return []
    query_embedding = model.encode(query,
        convert_to_tensor=True)
    snippet_embeddings = model.encode(snippets,
        convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(
        query_embedding, snippet_embeddings
    )[0]
    top_results = torch.topk(cosine_scores, k=top_k)
    return [snippets[i] for i in top_results.indices]</pre></li>				<li>We then define the <code>generate_answer</code> function to generate an answer using GPT-4o. This is<a id="_idIndexMarker1141"/> the<a id="_idIndexMarker1142"/> core of the generation part of our RAG system:<pre class="source-code">
    def generate_answer(query, context):
    messages = [
        {
            "role": "system",
            "content": "You are a knowledgeable expert. Answer the user's query based only on the information provided in the context. "
                       "If the answer is not in the context, say 'I couldn't find an answer to your question in the provided context.'",
        },
        {
            "role": "user",
            "content": f"Context: {context}\n\nQuery: {query}",
        },
    ]
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.7,
        max_tokens=256
    )
    return response.choices[0].message.content</pre><p class="list-inset">This function constructs a structured prompt for an LLM to generate an answer constrained strictly to a given context. It formats the conversation as a system-user message pair, instructing the model to act as a subject matter expert and restrict its answer to the supplied information, explicitly avoiding speculation. If the information isn’t present, the system is directed to return a fallback message<a id="_idIndexMarker1143"/> indicating<a id="_idIndexMarker1144"/> that the answer couldn’t be found. The query and context are embedded directly into the user message, and the LLM (in this case, <code>gpt-4o</code>) is queried with a moderate creativity level via <code>temperature=0.7</code> and a response length cap of <code>256</code> tokens. This design makes the function reliable for context-grounded Q&amp;A tasks, particularly in RAG pipelines or constrained-answering settings such as document QA or compliance tools.</p></li>				<li>Here’s the main RAG function and example usage:<pre class="source-code">
   def rag_system(query):
    search_results = search(query)
    relevant_snippets = retrieve_snippets(query, search_results)
    if not relevant_snippets:
        return "Could not find any information related to your query"
    context = " ".join(relevant_snippets)
    answer = generate_answer(query, context)
    return answer
# Example usage
query = "What are the latest advancements in quantum computing?"
answer = rag_system(query)
print(answer)</pre><p class="list-inset">This code defines the <code>rag_system</code> function, which orchestrates the entire process: searching, retrieving snippets, and generating an answer. It then demonstrates how to use <code>rag_system</code> with an example query, printing the generated answer to the console</p><p class="list-inset">The <code>rag_system</code> function answers a query by first searching for relevant information using <code>search(query)</code> and then extracting relevant snippets through the API called <code>retrieve_snippets(query, search_results)</code>. If no snippets are found, it returns a message indicating no information was found. If snippets are available, they are combined into a single context string and used to generate an answer through <code>generate_answer(query, context)</code>. Finally, the function returns the generated answer based on the context. In the example usage, the function is called with the query <code>"What are the latest advancements in quantum computing?"</code> and will return a generated response based on the relevant search results. In real production systems, we should implement retries and error <a id="_idIndexMarker1145"/>handling <a id="_idIndexMarker1146"/>around <code>retrieve_snippets</code> API calls.</p></li>			</ol>
			<p>Before we move on to the next section, here are some things to remember:</p>
			<ul>
				<li><strong class="bold">API keys</strong>: Make sure you have valid API keys for both SerpApi and OpenAI and have replaced the placeholders in the code.</li>
				<li><strong class="bold">OpenAI costs</strong>: Be mindful of OpenAI API usage costs. GPT-4o can be more expensive than other models.</li>
				<li><strong class="bold">Prompt engineering</strong>: The quality of the generated answer heavily depends on the prompt you provide to GPT-4o. You might need to experiment with different prompts to get the best results. Consider adding instructions about the desired answer format, length, or style.</li>
				<li><code>try-except</code> blocks) to handle potential issues such as network problems, API errors, or invalid inputs.</li>
				<li><strong class="bold">Advanced techniques</strong>: This is a basic RAG system. You can improve it further by doing the following:<ul><li><strong class="bold">Better snippet selection</strong>: Consider factors such as source diversity, factuality, and snippet length</li><li><strong class="bold">Iterative retrieval</strong>: Retrieve more context if the initial answer is not satisfactory</li><li><strong class="bold">Fine-tuning</strong>: Fine-tune a smaller, more specialized language model on your specific domain for potentially better performance and lower costs</li></ul></li>
			</ul>
			<p>We’ve successfully built a simple RAG system, covering the core components of retrieval and generation. Now that we have a functional RAG system, let’s dive deeper into the crucial techniques <a id="_idIndexMarker1147"/>that<a id="_idIndexMarker1148"/> enable efficient retrieval from large datasets: embedding and indexing. We’ll explore different methods for representing text semantically and organizing these representations for fast similarity search.</p>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor369"/>Embeddings and indexing for retrieval in LLM applications</h1>
			<p>Embedding and indexing techniques<a id="_idIndexMarker1149"/> provide <a id="_idIndexMarker1150"/>efficient and effective retrieval in RAG-based LLM applications. They allow LLMs to quickly find and utilize relevant information from vast amounts of data. The following subsections provide a breakdown of common techniques.</p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor370"/>Embeddings</h2>
			<p>Embeddings are <a id="_idIndexMarker1151"/>numerical vector representations of data, such as text, images, or <a id="_idIndexMarker1152"/>audio, that map complex, high-dimensional data into a continuous vector space where similar items are positioned close to each other. These vectors capture the underlying patterns, relationships, and semantic properties of the data, making it easier for machine learning models to understand and process. For text, for example, word embeddings transform words or phrases into dense vectors that represent their meaning in a way that reflects semantic relationships, such as synonyms being closer together in the vector space. Embeddings are typically learned from large datasets through techniques such as neural networks, and they serve as a foundation for tasks such as information retrieval, classification, clustering, and recommendation systems. By reducing the dimensionality of data while preserving important features, embeddings enable models to generalize better and make sense of varied input data efficiently.</p>
			<p>For LLMs, text embeddings are most relevant. They are generated by passing text through a neural network (like the Sentence Transformer models we used in the previous section).</p>
			<h3>Why do we need embeddings?</h3>
			<p>Embeddings are <a id="_idIndexMarker1153"/>important for<a id="_idIndexMarker1154"/> RAG applications for the following reasons:</p>
			<ul>
				<li><strong class="bold">Semantic search</strong>: Embeddings enable semantic search, where you find information based on meaning rather than just keyword matching</li>
				<li><strong class="bold">Contextual understanding</strong>: LLMs can use embeddings to understand the relationships between different pieces of information, improving their ability to reason and generate relevant responses</li>
				<li><strong class="bold">Efficient retrieval</strong>: When combined with appropriate indexing, embeddings allow for the fast <a id="_idIndexMarker1155"/>retrieval<a id="_idIndexMarker1156"/> of relevant information from large datasets</li>
			</ul>
			<h3>Common embedding technologies</h3>
			<p>Several embedding<a id="_idIndexMarker1157"/> technologies are<a id="_idIndexMarker1158"/> commonly used in RAG systems, and these vary in their underlying models, methods, and suitability for different applications. Here are some prominent embedding technologies for RAG:</p>
			<ul>
				<li><strong class="bold">Pre-trained transformer-based embeddings (e.g., BERT, RoBERTa, and T5)</strong>: Transformer models such as <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>) and its variants, such as RoBERTa and T5, have<a id="_idIndexMarker1159"/> been widely used to generate dense, contextual embeddings for text. These models are fine-tuned on large corpora and capture a rich understanding of language semantics. In a RAG system, these embeddings can be used to retrieve relevant passages from a document store based on semantic similarity. The embeddings are typically high-dimensional and are generated by feeding text through the transformer model to produce a fixed-size vector.</li>
				<li><strong class="bold">Sentence-BERT (SBERT)</strong>: A variation<a id="_idIndexMarker1160"/> of BERT designed for sentence-level embeddings, SBERT focuses on optimizing the model for tasks such as semantic textual similarity and clustering. It uses a Siamese network architecture to map sentences into a dense vector space where semantically similar sentences are closer together. This makes it particularly effective for tasks such as information retrieval in RAG, where retrieving semantically<a id="_idIndexMarker1161"/> relevant passages from a large corpus is essential.</li>
				<li><strong class="bold">Facebook AI Similarity Search (Faiss)</strong>: Faiss is a <a id="_idIndexMarker1162"/>library developed by Facebook AI Research that provides efficient similarity search through <strong class="bold">approximate nearest neighbor</strong> (<strong class="bold">ANN</strong>) search. Faiss is not <a id="_idIndexMarker1163"/>an embedding technology by itself but works in conjunction with various embedding models to index and search over large collections of vectors. When used in RAG, Faiss enables the fast retrieval of relevant documents or passages by comparing the similarity of their embeddings against a query embedding.</li>
				<li><strong class="bold">Dense retriever models (e.g., DPR and ColBERT)</strong>: <strong class="bold">Dense Passage Retrieval</strong> (<strong class="bold">DPR</strong>) is an approach to information retrieval that uses two separate encoders (usually BERT-based models) to encode both queries and passages into dense<a id="_idIndexMarker1164"/> vectors. DPR outperforms traditional sparse retrieval methods by leveraging the contextual knowledge encoded in dense embeddings. ColBERT, on the other hand, is another dense retrieval model that balances the efficiency of dense retrieval and the effectiveness of traditional methods. These models are especially useful for RAG when retrieving high-quality passages that are semantically related to a query.</li>
				<li><strong class="bold">Contrastive Language-Image Pre-Training (CLIP)</strong>: While originally designed for<a id="_idIndexMarker1165"/> multimodal applications (text and image), CLIP has been adapted for text-only tasks as well. It learns embeddings by aligning text and image data in a shared vector space. Although CLIP is primarily used for multimodal tasks, its ability to represent language in a common space with images provides a flexible <a id="_idIndexMarker1166"/>embedding framework that can<a id="_idIndexMarker1167"/> be used in RAG, especially when working with multimodal data.</li>
				<li><strong class="bold">Deep semantic similarity models (e.g., USE and InferSent)</strong>: Models such <a id="_idIndexMarker1168"/>as the <strong class="bold">Universal Sentence Encoder</strong> (<strong class="bold">USE</strong>) and InferSent generate sentence embeddings by capturing deeper semantic meaning, which can be used for various NLP tasks, including document retrieval. These models produce fixed-size vector representations that can be compared for similarity, making them useful for RAG when paired with retrieval systems.</li>
				<li><strong class="bold">Doc2Vec</strong>: An extension of Word2Vec, Doc2Vec generates embeddings for entire documents rather than individual words. It maps variable-length text into a fixed-size vector, which can be used to retrieve semantically similar documents or passages. Though not as powerful as transformer-based models in terms of semantic <a id="_idIndexMarker1169"/>richness, Doc2Vec is still an effective tool for more lightweight retrieval tasks in RAG applications.</li>
				<li><strong class="bold">Embedding-based search engines (e.g., Elasticsearch with dense vectors)</strong>: Some modern search engines, such as Elasticsearch, have integrated support for dense vectors alongside traditional keyword-based indexing. Elasticsearch can store and retrieve text embeddings, allowing for more flexible and semantically aware searches. When used with RAG, these embeddings can be used to rank documents by their relevance to a query, improving retrieval performance.</li>
				<li><strong class="bold">OpenAI embeddings (e.g., GPT-based models)</strong>: OpenAI’s embeddings, derived from models such as GPT-3, are also used for RAG tasks. These embeddings are based on the language model’s ability to generate high-quality text representations, which can be indexed and searched over large corpora. While they are not as specifically tuned for retrieval as some other models (such as DPR), they are highly flexible and can be used in general-purpose RAG applications.</li>
			</ul>
			<p>These embedding technologies provide various advantages depending on the specific requirements of a RAG system, such as retrieval speed, model accuracy, and the scale of the data being <a id="_idIndexMarker1170"/>processed. Each of them can be<a id="_idIndexMarker1171"/> fine-tuned and optimized for specific use cases, and the choice of embedding technology will depend on factors such as the nature of the documents being retrieved, computational resources, and latency requirements.</p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor371"/>Indexing</h2>
			<p>Indexing is the process of<a id="_idIndexMarker1172"/> organizing embeddings in a data structure that <a id="_idIndexMarker1173"/>allows for fast similarity search. Think of it like the index of a book, but for vectors instead of words.</p>
			<p>Using a more detailed description using LLM terminology, vector indexing technologies optimize embedding storage and retrieval by creating specialized data structures that organize high-dimensional vectors according to their similarity relationships, rather than sequential order. These structures—whether graph-based (connecting similar vectors through navigable pathways), tree-based (recursively partitioning the vector space), or quantization-based (compressing vectors while preserving similarity)—all serve the fundamental purpose of transforming an otherwise prohibitively expensive exhaustive search into a manageable process by strategically limiting the search space, enabling vector databases to handle billions of embeddings with sub-second query times while maintaining an acceptable<a id="_idIndexMarker1174"/> trade-off between speed, memory efficiency, and result accuracy.</p>
			<h3>Why is indexing important?</h3>
			<p>Indexing is <a id="_idIndexMarker1175"/>important for<a id="_idIndexMarker1176"/> LLMs for the following reasons:</p>
			<ul>
				<li><strong class="bold">Speed</strong>: Without indexing, you would have to compare a query embedding to every single embedding in your dataset, which is computationally expensive and slow</li>
				<li><strong class="bold">Scalability</strong>: Indexing allows LLM applications to scale to handle massive datasets containing millions or even billions of data points</li>
			</ul>
			<h3>Common indexing techniques</h3>
			<p>Let’s look at <a id="_idIndexMarker1177"/>some of the common indexing techniques<a id="_idIndexMarker1178"/> for LLMs.</p>
			<p>For a visual diagram of these index techniques, I recommend you check out the following website: <a href="https://kdb.ai/learning-hub/articles/indexing-basics/">https://kdb.ai/learning-hub/articles/indexing-basics/</a></p>
			<ul>
				<li><strong class="bold">Flat index (</strong><strong class="bold">brute force)</strong>:<ul><li><strong class="bold">How it works</strong>: Stores all<a id="_idIndexMarker1179"/> embeddings in a simple list or array. During a search, it calculates the distance (e.g., cosine similarity) between the query embedding and every embedding in the index.</li><li><strong class="bold">Pros</strong>: Simple to implement and perfect accuracy (finds the true nearest neighbors).</li><li><strong class="bold">Cons</strong>: Slow and computationally expensive for large datasets, as it requires an exhaustive search.</li><li><strong class="bold">Suitable for</strong>: Very small datasets or when perfect accuracy is an absolute requirement.</li></ul></li>
				<li><strong class="bold">Inverted file </strong><strong class="bold">index (IVF)</strong>:<ul><li><strong class="bold">How </strong><strong class="bold">it works</strong>:<ul><li><strong class="bold">Clustering</strong>: Divides <a id="_idIndexMarker1180"/>the embedding space into clusters using algorithms such as <em class="italic">k</em>-means</li><li><strong class="bold">Inverted index</strong>: Creates an inverted index that maps each cluster centroid to a list of the embeddings belonging to that cluster</li><li><strong class="bold">Search</strong>:<ol><li class="lower-roman">Finds the nearest cluster centroid(s) to the query embedding</li><li class="lower-roman">Only searches within those clusters, significantly reducing the search space</li></ol></li></ul></li><li><strong class="bold">Pros</strong>: Faster than<a id="_idIndexMarker1181"/> a flat index; relatively simple <a id="_idIndexMarker1182"/>to implement</li><li><strong class="bold">Cons</strong>: Approximate (might not always find the true nearest neighbors); accuracy depends on the number of clusters</li><li><strong class="bold">Suitable for</strong>: Medium-sized datasets where a good balance between speed and accuracy is needed</li></ul></li>
				<li><strong class="bold">Hierarchical navigable small </strong><strong class="bold">world (HNSW)</strong>:<ul><li><strong class="bold">How </strong><strong class="bold">it works</strong>:<ul><li><strong class="bold">Graph-based</strong>: Constructs <a id="_idIndexMarker1183"/>a hierarchical graph where each node represents an embedding.</li><li><strong class="bold">Layers</strong>: The graph has multiple layers, with the top layer having long-range connections (for faster traversal) and the bottom layer having short-range connections (for accurate search).</li><li><strong class="bold">Search</strong>: Starts at a random node in the top layer and greedily moves towards the query embedding by exploring connections. The search progresses down the layers, refining the results.</li></ul></li><li><strong class="bold">Pros</strong>: Very fast and accurate; often considered the state of the art for ANN search</li><li><strong class="bold">Cons</strong>: More complex to implement than IVF, and higher memory overhead due to the graph structure</li><li><strong class="bold">Suitable for</strong>: Large datasets where both speed and accuracy are crucial</li></ul></li>
				<li><strong class="bold">Product </strong><strong class="bold">quantization (PQ)</strong>:<ul><li><strong class="bold">How </strong><strong class="bold">it works</strong>:<ul><li><strong class="bold">Subvectors</strong>: Divides <a id="_idIndexMarker1184"/>each embedding into multiple subvectors.</li><li><strong class="bold">Codebooks</strong>: Creates separate codebooks for each subvector using clustering. Each codebook contains a set of representative subvectors (centroids).</li><li><strong class="bold">Encoding</strong>: Encodes each embedding by replacing its subvectors with the closest<a id="_idIndexMarker1185"/> centroids<a id="_idIndexMarker1186"/> from the corresponding codebooks. This creates a compressed representation of the embedding.</li><li><strong class="bold">Search</strong>: Calculates the approximate distance between the query and the encoded embeddings using pre-computed distances between the query’s subvectors and the codebook centroids.</li></ul></li><li><strong class="bold">Pros</strong>: Significantly reduces memory usage by compressing embeddings; fast search.</li><li><strong class="bold">Cons</strong>: Approximate, and accuracy depends on the number of subvectors and the size of the codebooks.</li><li><strong class="bold">Suitable for</strong>: Very<a id="_idIndexMarker1187"/> large datasets where memory efficiency is a primary concern.</li></ul></li>
				<li><strong class="bold">Locality sensitive </strong><strong class="bold">hashing (LSH)</strong>:<ul><li><strong class="bold">How it works</strong>: Uses hash<a id="_idIndexMarker1188"/> functions to map similar embeddings to the same “bucket” with high probability</li><li><strong class="bold">Pros</strong>: Relatively simple; can be distributed across multiple machines</li><li><strong class="bold">Cons</strong>: Approximate, and performance depends on the choice of hash functions and the number of buckets</li><li><strong class="bold">Suitable for</strong>: Very large, high-dimensional datasets</li></ul></li>
			</ul>
			<p>Now that we’ve covered different indexing methods, let’s introduce some popular libraries and tools that implement these indexing techniques, making them easier to use in practice. This<a id="_idIndexMarker1189"/> will provide a practical perspective on how to leverage these technologies in<a id="_idIndexMarker1190"/> your RAG applications.</p>
			<p>The following are some libraries and tools for implementing indexing:</p>
			<ul>
				<li><strong class="bold">Faiss</strong>: A highly <a id="_idIndexMarker1191"/>optimized library developed by Facebook AI for efficient similarity search and clustering of dense vectors. It implements many of the indexing techniques mentioned previously (flat, IVF, HNSW, and PQ).</li>
				<li><strong class="bold">Approximate Nearest Neighbors Oh Yeah (Annoy)</strong>: Another popular library for ANN search, known for its ease of use and good performance. It uses a tree-based approach.</li>
				<li><strong class="bold">Scalable Nearest Neighbors (ScaNN)</strong>: A library developed by Google, designed for large-scale, high-dimensional datasets.</li>
				<li><strong class="bold">Vespa.ai</strong>: Provide tools to query, organize, and make inferences in vectors, tensors, text, and structured data. It is used by <a href="https://www.perplexity.ai/">https://www.perplexity.ai/</a>.</li>
				<li><strong class="bold">Pinecone, Weaviate, Milvus, Qdrant</strong>: Vector databases designed specifically for storing and searching embeddings. They handle indexing, scaling, and other infrastructure concerns.</li>
			</ul>
			<p>The best embedding <a id="_idIndexMarker1192"/>and indexing techniques for your LLM application will depend on several factors:</p>
			<ul>
				<li><strong class="bold">Dataset size</strong>: For small datasets, a flat index might be sufficient. For large datasets, consider HNSW, IVF, or PQ.</li>
				<li><strong class="bold">Speed requirements</strong>: If low latency is critical, HNSW is generally the fastest option.</li>
				<li><strong class="bold">Accuracy requirements</strong>: If perfect accuracy is required, a flat index is the only choice, but it’s not scalable. HNSW often provides the best accuracy among approximate methods.</li>
				<li><strong class="bold">Memory constraints</strong>: If memory is limited, PQ can significantly reduce storage requirements.</li>
				<li><strong class="bold">Development effort</strong>: Faiss and Annoy offer a good balance between performance and ease of implementation. Vector databases simplify infrastructure management.</li>
			</ul>
			<p>By carefully considering these factors and understanding the strengths and weaknesses of each technique and library, you can choose the most appropriate embedding and indexing methods to build efficient and effective LLM applications.</p>
			<p>We’ll now demonstrate an example involving embedding, indexing, and searching using Faiss, a powerful library for efficient similarity search. I’ll use the <code>all-mpnet-base-v2</code> Sentence Transformer<a id="_idIndexMarker1193"/> model to generate embeddings. Since<a id="_idIndexMarker1194"/> the code will be more than 20 lines, I’ll break it down into blocks with explanations preceding each block.</p>
			<h2 id="_idParaDest-302"><a id="_idTextAnchor372"/>Example code demonstrating embedding, indexing, and searching</h2>
			<p>In this section, we’ll be <a id="_idIndexMarker1195"/>showing the code <a id="_idIndexMarker1196"/>for a typical workflow <a id="_idIndexMarker1197"/>for using embeddings and indexing to enable fast similarity search within a collection of text documents: Here’s what it does:</p>
			<ol>
				<li><strong class="bold">Loads a Sentence Transformer model</strong>: Initializes a pre-trained model for generating sentence embeddings.</li>
				<li><strong class="bold">Creates sample data</strong>: Defines a list of example sentences (you would replace this with your actual data).</li>
				<li><code>SentenceTransformer</code> to create embeddings for each sentence.</li>
				<li><code>IndexFlatL2</code> for a flat L2 distance index in this example) to store the embeddings.</li>
				<li><strong class="bold">Adds embeddings to the index</strong>: Adds the generated embeddings to the Faiss index.</li>
				<li><strong class="bold">Defines a search query</strong>: Sets a sample query for which we want to find similar sentences.</li>
				<li><strong class="bold">Encodes the query</strong>: Creates an embedding for the search query using the same Sentence Transformer model.</li>
				<li><strong class="bold">Performs a search</strong>: Uses the Faiss index to search for the <em class="italic">k</em> most similar embeddings to the query embedding.</li>
				<li><strong class="bold">Prints the results</strong>: Displays the indices and distances of the <em class="italic">k</em> nearest neighbors found in the index.</li>
			</ol>
			<p>Before we check out the code, let us install the following dependencies:</p>
			<pre class="console">
pip install faiss-cpu sentence-transformers
# Use faiss-gpu if you have a compatible GPU</pre>			<p>Let us now see the code example:</p>
			<ol>
				<li>First, we import the necessary libraries—<code>sentence_transformers</code> for creating embeddings and <code>faiss</code> for indexing and searching—and load the <code>all-mpnet-base-v2</code> Sentence Transformer model:<pre class="source-code">
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
# Load the SentenceTransformer model
model = SentenceTransformer('all-mpnet-base-v2')</pre></li>				<li>We then <a id="_idIndexMarker1198"/>prepare<a id="_idIndexMarker1199"/> the data by defining <a id="_idIndexMarker1200"/>some sample sentences (you can replace these with your actual data) and then use the Sentence Transformer model to generate embeddings for each sentence (the embeddings are converted to float32, which is required by Faiss):<pre class="source-code">
# Sample sentences
text_data = [
    "A man is walking his dog in the park.",
    "Children are playing with toys indoors.",
    "An artist is painting a landscape on canvas.",
    "The sun sets behind the mountain ridge.",
    "Birds are singing outside the window."
]
# Generate vector representations using a SentenceTransformer model
import numpy as np
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')  # Replace with your model if different
vectors = model.encode(text_data, convert_to_tensor=True)
# Ensure compatibility with Faiss by converting to 32-bit floating point and moving to CPU
vectors = vectors.detach().cpu().numpy().astype(np.float32)</pre></li>				<li>We then <a id="_idIndexMarker1201"/>create <a id="_idIndexMarker1202"/>a Faiss <a id="_idIndexMarker1203"/>index and add the embeddings to it:<pre class="source-code">
# Get the dimensionality of the embeddings
dimension = embeddings.shape[1]
# Create a Faiss index (flat L2 distance)
index = faiss.IndexFlatL2(dimension)
# Add the embeddings to the index
index.add(embeddings)</pre><p class="list-inset">Here, we’re using IndexFlatL2, which is a flat index that uses L2 distance (Euclidean distance) for similarity comparisons. This type of index provides accurate results but can be slow for very large datasets. The index is created with the correct dimensionality (<code>768</code> for this Sentence Transformer model).</p></li>				<li>Next, we define<a id="_idIndexMarker1204"/> a <a id="_idIndexMarker1205"/>sample search query and encode<a id="_idIndexMarker1206"/> it into an embedding using the same Sentence Transformer model. The query embedding is also converted to float32:<pre class="source-code">
# Define a search query
query = "What is the dog doing?"
# Encode the query
query_embedding = model.encode(query, convert_to_tensor=True)
query_embedding = \
    query_embedding.cpu().numpy().astype('float32')</pre></li>				<li>Finally, we perform similarity search using the <code>index.search()</code> method. We search for the two most similar sentences (<em class="italic">k</em>=2). The method returns the distances and the indices of the nearest neighbors. We then print the indices and distances of the nearest neighbors found:<pre class="source-code">
# Search for the k nearest neighbors
k = 2
distances, indices = index.search(query_embedding, k)
# Print the results
print("Nearest neighbors:")
for i, idx in enumerate(indices[0]):
    print(f"  Index: {idx}, Distance: {distances[0][i]},
    Sentence: {sentences[idx]}")</pre></li>			</ol>
			<p>The following is sample output you might get from running the preceding code blocks:</p>
			<pre class="console">
Nearest neighbors:
  Index: 0, Distance: 0.634912312, Sentence: A man is walking his dog in the park.
  Index: 1, Distance: 1.237844944, Sentence: Children are playing with toys indoors.</pre>			<p>This demonstrates how semantic similarity search works using Sentence Transformers and Faiss. Note that actual numbers will vary depending on the hardware, model versions, and runtime conditions.</p>
			<p>Here’s what’s happening.</p>
			<p>The query <code>"What is the dog doing?"</code> is embedded and compared against all embedded sentences in the list. Faiss retrieves the two most semantically similar sentences based on Euclidean (L2) distance in the embedding space. The smallest distance indicates the highest similarity. In this example, the sentence about the man walking his dog is closest to the query, which makes sense semantically.</p>
			<p>If you’re running this<a id="_idIndexMarker1207"/> on<a id="_idIndexMarker1208"/> your <a id="_idIndexMarker1209"/>machine, your values may look different due to the non-determinism in model initialization and floating-point precision, but the closest sentence should consistently be the one most semantically related to the query.</p>
			<p class="callout-heading">Important</p>
			<p class="callout"><code>IndexIVFFlat</code> or <code>IndexHNSWFlat</code> from Faiss, to improve search speed.</p>
			<p class="callout"><code>faiss-gpu</code> to significantly speed up indexing and searching.</p>
			<p class="callout"><strong class="bold">Data preprocessing</strong>: For<a id="_idIndexMarker1212"/> real-world applications, you might need to perform additional data preprocessing steps, such as lowercasing, removing punctuation, or stemming/lemmatization, depending on your specific needs and the nature of your data.</p>
			<p class="callout"><strong class="bold">Distance metrics</strong>: Faiss supports <a id="_idIndexMarker1213"/>different distance metrics. We used L2 distance here, but you could also use the inner product (IndexFlatIP) or other metrics depending on how your embeddings are generated and what kind of similarity you want to measure.</p>
			<p class="callout"><strong class="bold">Vector databases</strong>: For<a id="_idIndexMarker1214"/> production-level systems, consider using a dedicated vector database such as Pinecone, Weaviate, or Milvus to manage your embeddings and indexes more efficiently. They often provide features such as automatic indexing, scaling, and data management, which simplify the deployment of similarity search applications.</p>
			<p>We’ve covered the fundamentals of embeddings, indexing, and searching with Faiss, along with important considerations for real-world implementation. Now, let’s turn our attention to <a id="_idIndexMarker1215"/>another <a id="_idIndexMarker1216"/>crucial aspect of RAG: query formulation. We’ll <a id="_idIndexMarker1217"/>explore various strategies to refine and expand user queries, ultimately leading to more effective information retrieval from the knowledge base.</p>
			<h1 id="_idParaDest-303"><a id="_idTextAnchor373"/>Query formulation strategies in LLM-based RAG</h1>
			<p>Query formulation <a id="_idIndexMarker1218"/>strategies in <a id="_idIndexMarker1219"/>LLM-based RAG systems aim to enhance retrieval by improving the expressiveness and coverage of user queries. Common expansion strategies include the following:</p>
			<ul>
				<li><strong class="bold">Synonym and paraphrase expansion</strong>: This involves generating semantically equivalent alternatives using LLMs or lexical resources. For example, expanding “climate change impact” to include “effects of global warming” or “environmental consequences of climate change” can help match a broader range of documents.</li>
				<li><strong class="bold">Contextual reformulation</strong>: LLMs can reinterpret queries by inferring their intent based on conversational or document context. This helps in tailoring the query to better align with how the information might be expressed in the knowledge base.</li>
				<li><strong class="bold">Pseudo-relevance feedback</strong>: Also known as blind relevance feedback, this strategy involves running an initial query, analyzing the top-ranked documents for salient terms, and using these terms to expand the query. While effective, it requires safeguards against topic drift.</li>
				<li><strong class="bold">Template-based augmentation</strong>: Useful in structured domains, this method uses domain-specific templates or patterns to systematically generate variants. For example, a medical query about “treatment for hypertension” might also include “hypertension therapy” or “managing high blood pressure.”</li>
				<li><strong class="bold">Entity and concept linking</strong>: Named entities and domain concepts in the query are identified and replaced or augmented with their aliases, definitions, or hierarchical relations. This is often guided by ontologies or knowledge graphs.</li>
				<li><strong class="bold">Prompt-based query rewriting</strong>: With LLMs, prompts can be crafted to explicitly instruct the model to generate reformulated queries. This is particularly useful in multilingual or multi-domain RAG systems, where queries need to be adapted to match the style and vocabulary of the target corpus.</li>
			</ul>
			<p>Each strategy <a id="_idIndexMarker1220"/>contributes<a id="_idIndexMarker1221"/> differently to recall and precision. Choosing or combining them depends on the structure and variability of the underlying knowledge base.</p>
			<p>In the following code, the <code>QueryExpansionRAG</code> implementation uses a prompt-based query rewriting strategy powered by a pre-trained sequence-to-sequence language model (specifically, T5-small). This approach instructs the model to generate alternative phrasings of the input query by prefixing the prompt with <code>"expand query:"</code>. The generated expansions reflect paraphrastic reformulation, where the model synthesizes <a id="_idIndexMarker1222"/>semantically <a id="_idIndexMarker1223"/>related variations to increase retrieval coverage:</p>
			<pre class="source-code">
from transformers import pipeline
class QueryExpansionRAG(AdvancedRAG):
    def __init__(
        self, model_name, knowledge_base,
        query_expansion_model="t5-small"
    ):
        super().__init__(model_name, knowledge_base)
        self.query_expander = pipeline(
            "text2text-generation", model=query_expansion_model
        )
    def expand_query(self, query):
        expanded = self.query_expander(
            f"expand query: {query}", max_length=50,
            num_return_sequences=3
        )
        return [query] + [e['generated_text'] for e in expanded]
    def retrieve(self, query, k=5):
        expanded_queries = self.expand_query(query)
        all_retrieved = []
        for q in expanded_queries:
            all_retrieved.extend(super().retrieve(q, k))
        # Remove duplicates and return top k
        unique_retrieved = list(dict.fromkeys(all_retrieved))
        return unique_retrieved[:k]
# Example usage
rag_system = QueryExpansionRAG(model_name, knowledge_base)
retrieved_docs = rag_system.retrieve(query)
print("Retrieved documents:", retrieved_docs)</pre>			<p>This code defines a <code>QueryExpansionRAG</code> class that extends a RAG framework by incorporating query expansion using a pre-trained T5 model. When a user submits a query, the <code>expand_query</code> method uses the T5 model through a text-to-text generation pipeline to produce multiple alternative phrasings of the query, which are then combined with the original query. The <code>retrieve</code> method iterates over these expanded queries, retrieving documents for each one and aggregating the results while removing duplicates. This approach increases the chances of retrieving relevant content by broadening the lexical and semantic scope of the original query, making it especially effective when<a id="_idIndexMarker1224"/> the<a id="_idIndexMarker1225"/> knowledge base expresses information in varied ways.</p>
			<p>Keep in mind that poorly expanded queries can introduce noise and reduce retrieval precision. In this implementation, expansions generated by the T5 model are combined with the original query, increasing coverage. However, to maintain a balance, consider reranking results using similarity scores or assigning lower weights to generated expansions during retrieval. This helps ensure that expansions improve recall without compromising the alignment with the original intent.</p>
			<p>We’ve seen how query expansion can enhance retrieval in RAG systems, but it’s essential to manage the trade-off between recall and precision. Now, let’s shift our focus to the other side of the RAG pipeline: integrating the retrieved information with the LLM to generate the final answer. We’ll explore how to craft prompts that effectively leverage<a id="_idIndexMarker1226"/> the retrieved<a id="_idIndexMarker1227"/> context.</p>
			<h1 id="_idParaDest-304"><a id="_idTextAnchor374"/>Integrating retrieved information with LLM generation</h1>
			<p>To integrate<a id="_idIndexMarker1228"/> retrieved information <a id="_idIndexMarker1229"/>with LLM generation, we can create a prompt that incorporates the retrieved documents:</p>
			<pre class="source-code">
from transformers import AutoModelForCausalLM
class GenerativeRAG(QueryExpansionRAG):
    def __init__(
        self, retriever_model, generator_model, knowledge_base
    ):
        super().__init__(retriever_model, knowledge_base)
        self.generator = \
            AutoModelForCausalLM.from_pretrained(generator_model)
        self.generator_tokenizer = \
            AutoTokenizer.from_pretrained(generator_model)
    def generate_response(self, query, max_length=100):
        retrieved_docs = self.retrieve(query)
        context = "\n".join(retrieved_docs)
        prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
        inputs = self.generator_tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs,
            max_length=max_length)
        return self.generator_tokenizer.decode(
            outputs[0], skip_special_tokens=True)
# Example usage
retriever_model = "all-MiniLM-L6-v2"
generator_model = "gpt2-medium"
rag_system = GenerativeRAG(
    retriever_model, generator_model, knowledge_base
)
response = rag_system.generate_response(query)
print("Generated response:", response)</pre>			<p>In the preceding code snippet, the <code>GenerativeRAG</code> class extends a RAG pipeline by integrating a causal language model for answer generation. It inherits from <code>QueryExpansionRAG</code>, which already provides retrieval functionality, and adds a generator component using Hugging Face’s <code>AutoModelForCausalLM</code>. In the constructor, it initializes the generator model and tokenizer based on the given model name. The <code>generate_response</code> method first retrieves relevant documents for a given query, concatenates them into a single context string, and constructs a prompt that combines this context with the question. This prompt is then tokenized and passed into the language model, which generates a text continuation as the answer. The final output is obtained by decoding the generated tokens into a string. This modular structure separates the retrieval and generation steps, making it easy to scale or replace individual components depending on the task or model performance requirements.</p>
			<p>Having covered the basics of RAG systems, we will now focus on real-world challenges, such as scalability, dynamic<a id="_idIndexMarker1230"/> updates, and<a id="_idIndexMarker1231"/> multilingual retrieval. Specifically, we will discuss how a sharded indexing architecture can improve retrieval efficiency at scale, highlighting its impact on performance in data-heavy environments.</p>
			<h1 id="_idParaDest-305"><a id="_idTextAnchor375"/>Challenges and opportunities in RAG for LLMs</h1>
			<p>Some key challenges<a id="_idIndexMarker1232"/> and opportunities in RAG include the following:</p>
			<ul>
				<li><strong class="bold">Scalability</strong>: Efficiently handling very large knowledge bases.</li>
				<li><strong class="bold">Dynamic knowledge updating</strong>: Keeping the knowledge base current.</li>
				<li><strong class="bold">Cross-lingual RAG</strong>: Retrieving and generating in multiple languages.</li>
				<li><strong class="bold">Multi-modal RAG</strong>: Incorporating non-text information in retrieval and generation.<p class="list-inset">Keep in mind that cross-lingual and multi-modal RAG will need specialized retrieval pipelines or adapters because standard retrieval approaches often struggle with semantic matching across languages or modalities, requiring dedicated components that can properly encode, align, and retrieve relevant information regardless of the source language or format while maintaining contextual understanding and relevance.</p></li>
				<li><strong class="bold">Explainable RAG</strong>: Providing transparency in the retrieval and generation process.</li>
			</ul>
			<p>To keep this chapter from becoming too long, in this section, we will only show an example on how to address the scalability challenge by implementing a sharded index. A sharded index refers to a distributed data structure that partitions the index into multiple smaller, manageable segments called shards, each stored and maintained independently across different nodes or storage units. This approach enables parallel processing, reduces lookup time, and mitigates bottlenecks associated with centralized indexing, making it suitable for handling large-scale datasets or high query volumes commonly encountered in AI applications:</p>
			<pre class="source-code">
class ShardedRAG(GenerativeRAG):
    def __init__(
        self, retriever_model, generator_model,
        knowledge_base, num_shards=5
    ):
        super().__init__(retriever_model, generator_model,
            knowledge_base)
        self.num_shards = num_shards
        self.sharded_indexes = self.build_sharded_index()
    def build_sharded_index(self):
        embeddings = self.get_embeddings(self.knowledge_base)
        sharded_indexes = []
        shard_size = len(embeddings) // self.num_shards
        for i in range(self.num_shards):
            start = i * shard_size
            end = start + shard_size if i &lt; self.num_shards - 1
                else len(embeddings)
            shard_index = faiss.IndexFlatL2(embeddings.shape[1])
            shard_index.add(embeddings[start:end])
            sharded_indexes.append(shard_index)
        return sharded_indexes
    def retrieve(self, query, k=5):
        query_embedding = self.get_embeddings([query])[0]
        all_retrieved = []
        for shard_index in self.sharded_indexes:
            _, indices = shard_index.search(
                np.array([query_embedding]), k)
            all_retrieved.extend([self.knowledge_base[i]
                for i in indices[0]])
        # Remove duplicates and return top k
        unique_retrieved = list(dict.fromkeys(all_retrieved))
        return unique_retrieved[:k]
# Example usage
sharded_rag = ShardedRAG(retriever_model, generator_model,
    knowledge_base)
response = sharded_rag.generate_response(query)
print("Generated response:", response)</pre>			<p>In the preceding code, the scalability is handled by dividing the knowledge base into multiple smaller indexes, or shards, each containing a portion of the overall data. This approach reduces the computational and memory burden on any single index and allows retrieval operations to remain efficient even as the dataset grows. During a query, the system embeds the query once, searches across all shards independently, and then merges the results. This design avoids bottlenecks that would arise from searching a single<a id="_idIndexMarker1233"/> large index and makes it feasible to scale to much larger knowledge bases. It also lays the groundwork for fur<a id="_idTextAnchor376"/>ther optimizations, such as parallelizing shard queries or distributing them across multiple machines.</p>
			<h1 id="_idParaDest-306"><a id="_idTextAnchor377"/>Summary</h1>
			<p>RAG is a powerful technique for enhancing LLMs with external knowledge. By implementing the strategies and techniques discussed in this chapter, you can create more informed and accurate language models capable of accessing and utilizing vast amounts of information.</p>
			<p>As we move forward, the next chapter will explore graph-based RAG for LLMs, which extends the RAG concept to leverage structured knowledge representations. This will further enhance the ability of LLMs to reason over complex relationships and generate more contextually appropriate responses.</p>
		</div>
	</div></div></body></html>