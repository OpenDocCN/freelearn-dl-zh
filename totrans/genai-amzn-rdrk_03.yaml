- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Engineering Prompts for Effective Model Usage
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计有效模型使用的提示
- en: This chapter begins with an overview of prompt engineering and its importance.
    We will walk through various prompt engineering techniques and the ability to
    incorporate them while prompting any model on Amazon Bedrock, primarily focusing
    on designing and analyzing effective prompt techniques to get the desired outcome
    from the Bedrock models. This chapter also entails some of the best practices
    associated with prompt engineering.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从对提示工程及其重要性的概述开始。我们将探讨各种提示工程技巧，以及如何在Amazon Bedrock上对任何模型进行提示时应用这些技巧，主要关注设计和分析有效的提示技巧，以从Bedrock模型中获得期望的结果。本章还涉及与提示工程相关的一些最佳实践。
- en: By the end of this chapter, you will have developed a clear understanding of
    the practical aspects of prompt engineering and be able to craft effective prompts
    while following the best practices to get the desired outcome from the models
    available on Amazon Bedrock.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将清楚地理解提示工程的实践方面，并能够在遵循最佳实践的同时，制作出有效的提示，从而从Amazon Bedrock上可用的模型中获得期望的结果。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: What is prompt engineering?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是提示工程？
- en: Unlocking prompt engineering techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解锁提示工程技巧
- en: Designing prompts for Amazon Bedrock models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计用于Amazon Bedrock模型的提示
- en: Understanding best practices in prompt engineering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解提示工程的最佳实践
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete this chapter, you need to have access to the *AWS* console so that
    you can navigate to Amazon Bedrock Playground to execute prompt engineering techniques.
    Here’s the web page to access the console: [https://console.aws.amazon.com/](https://console.aws.amazon.com/).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章，您需要能够访问*AWS*控制台，以便您可以在Amazon Bedrock游乐场中导航以执行提示工程技巧。访问控制台的网页如下：[https://console.aws.amazon.com/](https://console.aws.amazon.com/).
- en: Secondly, you need to have the right permissions to invoke Amazon Bedrock models
    from your local machine using *Amazon Bedrock APIs* or *Bedrock Python SDK* so
    that you can execute the prompts. To learn more, go to [https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html](https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，您需要拥有正确的权限，以便使用*Amazon Bedrock APIs*或*Bedrock Python SDK*从您的本地机器调用Amazon
    Bedrock模型，以便执行提示。了解更多信息，请访问[https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html](https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html).
- en: What is prompt engineering?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是提示工程？
- en: Since we have been discussing Amazon Bedrock models and how to invoke them,
    we need to dive into prompt engineering. Essentially, in a way that a particular
    child asks their parents questions about anything and everything, we can also
    ask an LLM anything under the Sun! However, to get the best and most precise outputs
    possible, we must train ourselves to ask the model the right questions in the
    right manner.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们一直在讨论Amazon Bedrock模型及其调用方法，因此我们需要深入了解提示工程。本质上，就像某个孩子可以向他们的父母询问任何和所有问题一样，我们也可以向LLM提出任何问题！然而，为了获得最佳和最精确的输出，我们必须训练自己以正确的方式向模型提出正确的问题。
- en: With the increasing popularity of LLMs, users are actively striving to refine
    their way of asking the model different kinds of questions to attain a desired
    response. For instance, we can simply ask an LLM questions such as `Who was the
    first person to land on the Moon?` or `How many moons does Jupiter have?`. Based
    on these questions, the language model can respond to the user’s queries either
    factually or provide an inadequate/incorrect response based on the LLM’s knowledge,
    which is the data it has been trained on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的日益流行，用户正积极努力改进他们向模型提问的方式，以期获得期望的回应。例如，我们可以简单地询问一个LLM诸如“谁是第一个登上月球的人？”或“木星有多少颗卫星？”等问题。基于这些问题，语言模型可以基于其训练数据（即LLM的知识）对用户的查询给出事实性的回答，或者提供不充分/错误的回答。
- en: Incorrect responses that the users get without fact-checking are what we refer
    to as **hallucinations**. It is often seen that if the user asks an ambiguous
    question or a particularly complex math problem that the model hasn’t been trained
    to answer, it will determine a probabilistic answer that may or may not be factually
    accurate. This can also occur with large vision models such as text-to-image models,
    where the model ends up providing an undesirable image as the prompted response.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用户在没有事实核查的情况下得到的错误回答，我们称之为**幻觉**。通常情况下，如果用户提出一个模糊的问题或一个模型尚未训练过的特别复杂的数学问题，它将确定一个可能或不可能符合事实的概率性答案。这种情况也可能出现在大型视觉模型中，例如文本到图像模型，其中模型最终提供了一个不希望得到的图像作为提示响应。
- en: Hence, how we ask questions to the model and how effectively we can provide
    a description regarding our question becomes a crucial factor for the model to
    generate a desirable output.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们如何向模型提问以及我们如何有效地描述我们的问题，成为模型生成期望输出的关键因素。
- en: The method of prompting the model in the right manner while avoiding any ambiguity
    in your prompts becomes the essence of effective **prompt engineering**. This
    is not just applicable to the technical community anymore!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在正确地提示模型的同时避免提示语中的任何歧义，这是有效**提示工程**的精髓。这不再仅适用于技术社区！
- en: Even people with varying technical backgrounds can use LLMs for a range of tasks.
    Based on the user prompts, the models can offer basic tips on entrepreneurship
    or provide fundamental insights into website creation through detailed, informative
    conversations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是技术背景各异的人也可以使用大型语言模型（LLMs）来完成各种任务。根据用户提示，模型可以提供关于创业的基本建议，或者通过详细、信息丰富的对话，提供关于网站创建的基本见解。
- en: Effective prompt engineering techniques pave the way for users to get the desired
    responses. Furthermore, some companies have been offering high-paying jobs for
    researchers and personas who can write or adopt effective prompt engineering to
    get their model to perform responsible actions, thereby enhancing the company’s
    productivity to execute their functions/tasks at an accelerated pace.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的提示工程技巧为用户获得期望的响应铺平了道路。此外，一些公司已经开始为研究人员和能够编写或采用有效提示工程以使模型执行负责任行为的人提供高薪工作，从而提高公司执行其职能/任务的速度。
- en: This chapter will explain how effective prompt engineering techniques can be
    applied to LLMs. But first, let’s dive into the structure of a prompt and some
    key ideas that focus on effective prompt techniques.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将解释如何将有效的提示工程技巧应用于大型语言模型（LLMs）。但首先，让我们深入了解提示语的结构和一些关注有效提示技巧的关键思想。
- en: Components of prompts
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示语的组成部分
- en: 'How you write a prompt plays a crucial role in guiding the behavior of the
    model. Prompts contain a few key elements. Let’s understand those elements through
    an example (*Figure 3**.1*):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何编写提示语在引导模型行为中起着至关重要的作用。提示语包含几个关键元素。让我们通过一个例子来理解这些元素（*图3**.1*）：
- en: '![Figure 3.1 – Components of a prompt](img/B22045_03_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 提示语的组成部分](img/B22045_03_01.jpg)'
- en: Figure 3.1 – Components of a prompt
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 提示语的组成部分
- en: 'Let’s take a closer look at the terms highlighted in the preceding figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看前图中突出显示的术语：
- en: '**Instruction**: With an instruction, you provide a clear and concise description
    or instruction to the model on how it should perform the task, whether it be summarizing
    text, translating languages, composing music, or any number of other things. In
    the preceding figure, you can see that we have asked the model to act as a specialist
    in quantum computing and answer the user’s question in detail and in layman’s
    terms, along with examples.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指令**：通过指令，你向模型提供一个清晰、简洁的描述或指令，说明它应该如何执行任务，无论是总结文本、翻译语言、创作音乐还是其他任何事情。在前面的图中，你可以看到我们要求模型扮演量子计算专家的角色，并详细、通俗易懂地回答用户的问题，并提供例子。'
- en: '**Context**: Context refers to the relevant background information that you
    provide to the model to enhance its performance. This can include any relevant
    data, past experiences, or domain-specific knowledge. In the preceding figure,
    in terms of context, we stated that the model has recently completed a PhD and
    has been asked to be part of an interview on a talk show that explains complex
    topics in layman’s terms. This primes the model with pertinent knowledge.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文**：上下文是指你提供给模型以增强其性能的相关背景信息。这可以包括任何相关数据、过去的经验或特定领域的知识。在前面的图中，在上下文方面，我们说明了模型最近完成了博士学位，并被邀请参加一个访谈节目，该节目用通俗易懂的方式解释复杂话题。这为模型提供了相关的知识。'
- en: '`What is Quantum Computing?`. As depicted in the preceding figure, the input
    question that goes to the model is `Can you provide me your thoughts on Quantum`
    `Machine Learning?`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`什么是量子计算？`。如前图所示，输入到模型的问题为`你能提供你对量子机器学习的看法吗？`。'
- en: '`Quantum Ninja`, as depicted in the preceding figure, so that the model understands
    that its output should be in this layout, or it could be in a specific format,
    such as text, JSON, an audio clip, and so on. Special syntax such as *<|endoftext|>*
    signals the end of the input and the beginning of the model’s output. This special
    syntax can vary on a per-model basis.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Quantum Ninja`，如前图所示，以便模型理解其输出应采用此布局，或者它可以是特定格式，如文本、JSON、音频剪辑等。特殊的语法如 */endoftext/>*
    表示输入的结束和模型输出的开始。这种特殊语法可能因模型而异。'
- en: 'Although prompts need not have all four elements, their form depends on the
    task. Let’s examine a few sample prompts:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然提示不需要包含所有四个元素，但它们的格式取决于任务。让我们考察几个示例提示：
- en: '**Example 1:** **SQL query**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 1：** **SQL查询**'
- en: '![Figure 3.2 – SQL query prompt](img/B22045_03_02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – SQL查询提示](img/B22045_03_02.jpg)'
- en: Figure 3.2 – SQL query prompt
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – SQL查询提示
- en: 'As shown in *Figure 3**.2*, we are specifying the following prompt elements
    to the Titan Text G1 – Premier model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 3**.2*所示，我们向Titan Text G1 – Premier模型指定了以下提示元素：
- en: '`You are querying a database with the following schema: Customers(id, name,
    age) and Orders(id, cust_id,` `product, amount).`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`您正在查询以下模式的数据库：Customers(id, name, age) 和 Orders(id, cust_id, product, amount)。`'
- en: '`List all customers who have placed more than 1 order, along with their total`
    `order amounts.`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`列出所有下过超过1个订单的客户及其总订单金额。`'
- en: '`SQL query:`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SQL查询：`'
- en: This prompt provides clear instructions, relevant schema context, a sample input,
    and output indicators to produce a suitable SQL query.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示提供了清晰的指示、相关的模式上下文、一个示例输入和输出指示符，以生成合适的SQL查询。
- en: '**Example 2:** **Recipe generation**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 2：** **食谱生成**'
- en: 'Here is another example (*Figure 3**.3*):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个例子（*图 3**.3*）：
- en: '![Figure 3.3 – Recipe prompt](img/B22045_03_03.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 食谱提示](img/B22045_03_03.jpg)'
- en: Figure 3.3 – Recipe prompt
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 食谱提示
- en: 'Let’s take a closer look:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看：
- en: '`Baked salmon is a healthy weeknight dinner option perfect with roasted potatoes
    or rice. The fresh dill adds an` `aromatic flavor.`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`烤三文鱼是一种健康的周日晚餐选择，与烤土豆或米饭搭配完美。新鲜的莳萝增添了香料的味道。`'
- en: '`Salmon fillet, dill, lemon, salt, pepper,` `olive oil`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`三文鱼鱼片、莳萝、柠檬、盐、胡椒、` `橄榄油`。'
- en: '`<|endoftext|>`.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<|endoftext|>`。'
- en: The preceding prompt provides the model with the recipe’s title, introductory
    context, ingredients as input data, and `<|endoftext|>` as an output indicator,
    which signals where the recipe steps should begin.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的提示为模型提供了食谱的标题、介绍性背景、作为输入数据的成分，以及`<|endoftext|>`作为输出指示符，表示食谱步骤应该开始的地方。
- en: Prompt engineering applications
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程应用
- en: Now that we understand how we can communicate with the model, let’s learn about
    some prompt engineering techniques that can aid us in getting better responses
    from the model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何与模型沟通，让我们学习一些提示工程技巧，这些技巧可以帮助我们从模型中获得更好的响应。
- en: However, primarily, we need to understand that the optimal prompt engineering
    approach for any given use case is heavily reliant on the task at hand, as well
    as the data on which it has been trained.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，首先，我们需要理解，对于任何特定的用例，最佳的提示工程方法高度依赖于手头的任务以及它所训练的数据。
- en: 'Some of the tasks that the models on Bedrock excel at are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Bedrock上的模型擅长以下任务：
- en: '**Classification**: LLMs exhibit prowess in text classification, a supervised
    learning technique for assigning categories to text. For instance, sentiment analysis
    involves discerning whether an input passage conveys positive or negative emotion.
    Some LLMs available via Amazon Bedrock, such as the Amazon Titan models, can also
    identify toxic, harmless, or fact-based content. Their deep contextual understanding
    aids the judgment of subtle linguistic cues.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：LLMs在文本分类方面表现出色，这是一种监督学习技术，用于将文本分配到类别中。例如，情感分析涉及判断输入段落是否传达积极或消极情绪。一些通过Amazon
    Bedrock提供的LLMs，如Amazon Titan模型，还可以识别有毒、无害或基于事实的内容。它们对上下文的深入理解有助于判断细微的语言线索。'
- en: '**Question-answering**: The models can answer questions accurately without
    external context due to their vast parameters gained from ingesting hundreds of
    billions of words during pre-training. When provided with relevant documents,
    their performance further improves by reasoning over the additional context.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答**：由于在预训练期间摄入了数百亿个单词，这些模型拥有庞大的参数，因此它们可以在没有外部上下文的情况下准确回答问题。当提供相关文档时，它们通过推理额外的上下文进一步提高了性能。'
- en: '**Summarization**: The models condense lengthy texts into concise summaries
    that preserve key details, learning to differentiate salient points. Adding such
    a prompt facilitates rapid analysis of documents.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要**：这些模型将长文本压缩成简洁的摘要，保留关键细节，并学习区分显著点。添加此类提示有助于快速分析文档。'
- en: '**Text generation**: The model can generate original coherent text given a
    short prompt. Their fluency and semantic consistency allow realistic synthesis
    of stories, poems, scripts, and more.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：给定简短的提示，模型可以生成原创的连贯文本。它们的流畅性和语义一致性允许合成故事、诗歌、剧本等。'
- en: '**Code generation**: For a textual description of a programming need, the models
    can generate executable code in languages such as SQL and Python. For example,
    a prompt could request text-to-SQL or Python code generation, thereby accomplishing
    the outlined computational goal.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码生成**：对于编程需求的文本描述，模型可以在SQL和Python等语言中生成可执行代码。例如，提示可以要求生成文本到SQL或Python代码，从而实现概述的计算目标。'
- en: '**Mathematical reasoning**: The models exhibit an aptitude for mathematical
    problems provided in text form. This includes numerical calculations, logical
    deduction, and geometric reasoning. They can further justify solutions with step-by-step
    explanations.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数学推理**：模型在处理以文本形式提供的数学问题时表现出才能。这包括数值计算、逻辑推理和几何推理。它们还可以通过逐步解释来进一步证明解决方案的正确性。'
- en: The breadth of natural language tasks that can be mastered by LLMs on Amazon
    Bedrock exemplifies their versatility. Their adaptive capacity promises to expand
    application domains even further.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon Bedrock上，LLMs能够掌握的自然语言任务范围展示了它们的通用性。它们的适应性保证了应用领域的进一步扩展。
- en: Now that we’ve gained insights into applications of prompt engineering in the
    real world, let’s try to unlock some of the most common prompt engineering techniques.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了提示工程在现实世界中的应用，让我们尝试解锁一些最常见的提示工程技巧。
- en: Unlocking prompt engineering techniques
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解锁提示工程技巧
- en: The field of prompt engineering is an active area of research and innovation
    with new techniques and patterns emerging frequently, driven by the pursuit to
    improve the performance of the models and generate more natural human-like responses.
    In this section, we are going to look at some of the most common patterns.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程领域是一个活跃的研究和创新领域，新技术和模式频繁出现，这得益于对提高模型性能和生成更自然的人类似响应的追求。在本节中，我们将探讨一些最常见的模式。
- en: Zero-shot prompting
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本提示
- en: '**Zero-shot** refers to the ability of LLMs to generate reasonable responses
    to prompts that it has not been explicitly trained on. It relies solely on a descriptive
    prompt to specify the desired output, as depicted in *Figure 3**.4*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**零样本**指的是LLMs（大型语言模型）对未经过显式训练的提示生成合理响应的能力。它完全依赖于描述性提示来指定所需的输出，如图*图3.4*所示：'
- en: '![Figure 3.4 – Zero-shot prompting](img/B22045_03_04.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – 零样本提示](img/B22045_03_04.jpg)'
- en: Figure 3.4 – Zero-shot prompting
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 零样本提示
- en: For instance, a zero-shot prompt to get a poem could be `Write a rhyming poem
    with 4 stanzas about` `seasons changing`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个用于获取诗歌的零样本提示可以是`写一首关于` `季节变化` `的押韵诗，共4节`。
- en: The main advantage of this method is that it’s easier; prompt crafting can be
    done without providing examples in the input. However, output quality can vary
    without concrete examples to base on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要优势在于它更容易操作；在输入中无需提供示例即可进行即时创作。然而，如果没有具体的示例作为依据，输出质量可能会有所不同。
- en: Few-shot prompting
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 少样本提示
- en: '**Few-shot prompting** or **few-shot learning** builds on zero-shot’s capabilities.
    As depicted in *Figure 3**.5*, on top of instructions/questions, you can provide
    a few examples that establish a concept or scenario, at which point models can
    start to generate reasonable continuations:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**少样本提示**或**少样本学习**建立在零样本能力的基础上。如图 3**.5** 所示，在指令/问题之上，你可以提供一些示例来建立概念或场景，此时模型可以开始生成合理的后续内容：'
- en: '![Figure 3.5 – Few-shot prompting](img/B22045_03_05.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 少样本提示](img/B22045_03_05.jpg)'
- en: Figure 3.5 – Few-shot prompting
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 少样本提示
- en: For instance, after showing two or three examples of short conversations about
    booking a doctor’s appointment, LLMs can produce an appointment booking dialog
    without needing thousands of examples. The key benefit over zero-shot is that
    few-shot examples help narrow down the context and constrain the generation process,
    making outputs more precise.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在展示两三个关于预约医生简短对话的示例之后，LLMs可以生成一个预约对话，而无需数千个示例。与零样本相比，关键优势在于少样本示例有助于缩小上下文范围并约束生成过程，使输出更加精确。
- en: Let’s look at some of the examples of few-shot prompting. The following are
    two inputs that we can use in the poem writing task from the previous sub-section
    that we can give to the model as examples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些少样本提示的示例。以下是我们可以在上一小节中用于诗歌创作任务的两个输入，我们可以将这些输入作为示例提供给模型。
- en: '`Roses are red, violets are blue, spring brings` `life anew.`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`玫瑰是红的，紫罗兰是蓝的，春天带来` `新的生命。`'
- en: '`Summer sun shining bright, long days full` `of light.`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`夏日阳光明媚，漫长的白天充满` `光明。`'
- en: '`Now you write a rhyming poem about autumn` `changing leaves.`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`现在请你写一首关于秋天落叶的押韵诗` `叶子变化。`'
- en: By analyzing these examples, LLMs can learn the pattern of rhyming four-line
    stanzas about seasons. It can then follow the template to generate an autumn poem.
    Balancing creativity and guidance is key in few-shot prompts.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析这些示例，LLMs可以学习关于季节的四行押韵诗的规律。然后，它可以遵循模板生成一首秋天的诗。在少样本提示中平衡创造性和指导性是关键。
- en: 'Additional examples of few-shot prompting are available here: [https://www.promptingguide.ai/techniques/fewshot](https://www.promptingguide.ai/techniques/fewshot).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些少样本提示的附加示例：[https://www.promptingguide.ai/techniques/fewshot](https://www.promptingguide.ai/techniques/fewshot)。
- en: Chain-of-thought prompting
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链提示
- en: '**Chain of thought** (**CoT**) **prompting** aims to elicit reasoning chains
    from language models. It involves providing the LLM with a prompt that lays out
    a reasoning chain or train of thought for the model to follow (*Figure 3**.6*).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**思维链**（**CoT**）**提示**旨在从语言模型中激发推理链。它涉及向LLM提供一个提示，其中包含一个推理链或思维流程，供模型遵循（*图 3**.6*）。'
- en: '![Figure 3.6 – Chain-of-thought prompting](img/B22045_03_06.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 思维链提示](img/B22045_03_06.jpg)'
- en: Figure 3.6 – Chain-of-thought prompting
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 思维链提示
- en: 'A prompt may start with background context, state a hypothesis or problem,
    provide reasoning steps, and end with a conclusion to be expanded on. The model
    then tries to continue the chain of reasoning coherently in its generated text:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个提示可能从背景信息开始，陈述一个假设或问题，提供推理步骤，并以一个需要扩展的结论结束。然后，模型试图在其生成的文本中连贯地继续推理链：
- en: 'Here’s an example of a CoT prompt:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个CoT提示的示例：
- en: '`Recycling is beneficial for the environment because it reduces waste sent
    to landfills. Landfills produce methane, a potent greenhouse gas. They also take
    up large amounts of space. Recycling reduces landfill contributions by reusing
    materials. In conclusion, recycling helps fight climate change by reducing landfill
    methane and space requirements. The main environmental benefits of` `recycling
    are...`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`回收利用对环境有益，因为它减少了送往垃圾填埋场的废物。垃圾填埋场产生甲烷，这是一种强效的温室气体。它们还占据了大量的空间。通过回收利用材料，回收减少了垃圾填埋场的贡献。总之，回收通过减少垃圾填埋场的甲烷和空间需求，有助于对抗气候变化。回收的主要环境效益是...`'
- en: This prompting style guides the LLM to follow the provided reasoning chain and
    elaborate further on the conclusion statement. The generated text will likely
    discuss reduced methane emissions and land use from increased recycling in more
    detail. Hence, chaining further encourages step-by-step logical thinking that
    focuses on the end goal over open-ended, meandering text.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种提示风格引导 LLM 跟随提供的推理链，并进一步阐述结论陈述。生成的文本可能会更详细地讨论由于回收增加而减少的甲烷排放和土地利用。因此，进一步链接鼓励逐步的逻辑思考，重点关注最终目标，而不是开放式、蜿蜒曲折的文本。
- en: 'Let’s look at some examples of CoT prompting and the responses:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些 CoT 提示的示例及其响应：
- en: '**Example 1**:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 1**:'
- en: '`Write a story about a professor exploring a` `mysterious artifact`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`写一个关于教授探索神秘文物的故事`'
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`As she opens the box, a glowing light emerges, illuminating symbols on` `the
    walls`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`当她打开盒子时，一束光芒出现，照亮了墙上的符号`'
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Example 2**:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例 2**:'
- en: '`What are the pros and cons of renewable` `energy sources?`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`可再生能源的优缺点是什么？`'
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`You missed one major renewable source. Please include geothermal energy when
    comparing the pros` `and cons.`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`您遗漏了一个主要的可再生能源来源。在比较优缺点时，请包括地热能。`'
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this approach, the human can guide and shape the overall narrative or analysis
    by inserting additional prompts whenever they want to redirect, refine, or expand
    the model’s response. It allows for a more conversational flow based on an initial
    theme or direction provided by the human.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，人类可以通过在需要引导、细化或扩展模型响应时插入额外的提示来引导和塑造整体叙事或分析。它允许基于人类提供的初始主题或方向进行更对话式的流程。
- en: Additional examples of CoT prompting are available at [https://www.promptingguide.ai/techniques/cot](https://www.promptingguide.ai/techniques/cot).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 提示的更多示例可在 [https://www.promptingguide.ai/techniques/cot](https://www.promptingguide.ai/techniques/cot)
    找到。
- en: ReAct prompting
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReAct 提示
- en: '**Reasoning and Acting** (**ReAct**) prompting is derived from the ReAct framework,
    as introduced in the paper *ReAct: Synergizing Reasoning and Acting in Language*
    *Models* ([https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理和行动**（**ReAct**）提示是从在论文 *ReAct: Synergizing Reasoning and Acting in Language
    Models* 中介绍的 ReAct 框架中派生出来的 ([https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629))：'
- en: '![Figure 3.7 – ReAct prompting](img/B22045_03_07.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – ReAct 提示](img/B22045_03_07.jpg)'
- en: Figure 3.7 – ReAct prompting
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – ReAct 提示
- en: This framework generates various reasoning traces. Then, based on those traces,
    it performs relevant actions. Through this process of reasoning and acting, you
    can also allow the application to have access to the organization’s internal data
    stores and APIs, or external sources from various tools, websites, web searches,
    and more, as shown in *Figure 3**.7*. This process of generating reasoning traces
    and performing actions is iterative and occurs until the LLM finds the answer
    to the user’s prompt.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此框架生成各种推理轨迹。然后，基于这些轨迹，它执行相关操作。通过推理和行动的过程，您还可以允许应用程序访问组织的内部数据存储和 API，或来自各种工具、网站、网络搜索等外部来源，如图
    *图 3**.7* 所示。这个过程生成推理轨迹并执行操作是迭代的，直到 LLM 找到用户提示的答案。
- en: We will discuss ReAct prompting in more detail in [*Chapter 10*](B22045_10.xhtml#_idTextAnchor192).
    For now, let’s create a ReAct-SQL tool for deriving a comprehensive response from
    the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [*第 10 章*](B22045_10.xhtml#_idTextAnchor192) 中更详细地讨论 ReAct 提示。现在，让我们为从模型中得出全面响应创建一个
    ReAct-SQL 工具。
- en: '`Provide an updated financial summary report` `for AnyCompany`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`为 AnyCompany 提供一份更新的财务总结报告`'
- en: '**Assistant response:**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**助手回复：**'
- en: '*Thought*: The user is asking for financial information about AnyCompany. To
    answer this, I need to do the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*思考*: 用户正在请求 AnyCompany 的财务信息。为了回答这个问题，我需要做以下事情：'
- en: Query the database to find financial records for AnyCompany
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询数据库以找到 AnyCompany 的财务记录
- en: Write a summary of the financial information obtained
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 撰写获取的财务信息的摘要
- en: So, I will need to use the `sql_db_query` tool.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我需要使用 `sql_db_query` 工具。
- en: '*Action*: `sql_db_query`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*动作*: `sql_db_query`'
- en: '*Action input*: `SELECT * FROM financial_db WHERE company = ''``AnyCompany'';`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*动作输入*: `SELECT * FROM financial_db WHERE company = ''``AnyCompany'';`'
- en: '`[(260B, 55B,` `321B, 2.2T)]`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`[(260B, 55B,` `321B, 2.2T)]`'
- en: '*Answer*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*答案*:'
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Similar to the preceding form of prompting patterns, new prompt engineering
    techniques and patterns are burgeoning these days. More details on prompt engineering
    can be found in the prompt engineering guide at [https://www.promptingguide.ai/techniques](https://www.promptingguide.ai/techniques).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的提示模式形式类似，新的提示工程技术和模式正在兴起。更多关于提示工程的信息可以在[https://www.promptingguide.ai/techniques](https://www.promptingguide.ai/techniques)的提示工程指南中找到。
- en: In this section, we uncovered several prompt engineering techniques that are
    being leveraged in the realm of building Generative AI applications for various
    use cases. Next, we will work on designing prompts for Amazon Bedrock FMs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们揭示了在构建适用于各种用例的生成式AI应用领域中被利用的几个提示工程技术。接下来，我们将致力于设计Amazon Bedrock FM的提示。
- en: Designing prompts for Amazon Bedrock models
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计Amazon Bedrock模型的提示
- en: In this section, we’re going to cover prompt guidance for some of the models
    available via Amazon Bedrock. We will kick things off with the Anthropic Claude
    model and provide details around prompt guidance for this. The majority of the
    learning for prompt guidance can be inherited from Claude models. Furthermore,
    for the sake of striking a balance between brevity and detail, we will shine a
    light on the models from Amazon Titan, AI21 Labs, and Stability AI Stable Diffusion.
    This will sum up our prompt guidance and associated prompt recommendations for
    invoking Amazon Bedrock models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍通过Amazon Bedrock提供的某些模型的提示指导。我们将从Anthropic Claude模型开始，并详细介绍该模型的提示指导。大部分关于提示指导的学习都可以从Claude模型中继承。此外，为了在简洁和详细之间取得平衡，我们将重点关注Amazon
    Titan、AI21 Labs和Stability AI Stable Diffusion的模型。这将总结我们的提示指导和与调用Amazon Bedrock模型相关的提示建议。
- en: Prompting Anthropic Claude 3
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活Anthropic Claude 3
- en: 'Here are some things to keep in mind while prompting the Anthropic Claude 3
    model:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示Anthropic Claude 3模型时，以下是一些需要注意的事项：
- en: '`You are a seasoned Children''s Book Author` or `You are a` `Business Expert`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`你是一位资深的儿童图书作家`或`你是一位商业专家`。'
- en: '`Imagine you are a kindergarten teacher and have to provide an explanation
    for rainbows in the sky to` `the children.`:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`想象你是一位幼儿园老师，需要向孩子们解释天空中的彩虹。`：'
- en: '![Figure 3.8 – Anthropic Claude 3 Haiku – simple prompting](img/B22045_03_08.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – Anthropic Claude 3 Haiku – 简单提示](img/B22045_03_08.jpg)'
- en: Figure 3.8 – Anthropic Claude 3 Haiku – simple prompting
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – Anthropic Claude 3 Haiku – 简单提示
- en: Without assigning a role/persona, the answer may be complex to understand, as
    shown in *Figure 3**.8*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 没有分配角色/人物时，答案可能难以理解，如图3.8所示。
- en: 'After adding the role/persona, you can see that the output response aligns
    more with a child’s complexity level in *Figure 3**.9*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加角色/人物之后，你可以看到输出响应与图3.10中孩子的复杂程度更相符。9：
- en: '![Figure 3.9 – Anthropic Claude 3 Haiku – assigning role personas](img/B22045_03_09.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – Anthropic Claude 3 Haiku – 分配角色人物](img/B22045_03_09.jpg)'
- en: Figure 3.9 – Anthropic Claude 3 Haiku – assigning role personas
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – Anthropic Claude 3 Haiku – 分配角色人物
- en: '`Temperature` parameter can be set to a higher value, the output that’s generated
    might be different, as shown here. However, the clear instructions in the latter
    provide a more direct output without any additional context, as desired by the
    user:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`温度`参数可以设置得更高，生成的输出可能会有所不同，如这里所示。然而，后面的清晰指令提供了更直接的输出，没有任何额外的上下文，这正是用户所期望的：'
- en: '![Figure 3.10 – Providing clear and direct instructions](img/B22045_03_10.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10 – 提供清晰直接的指令](img/B22045_03_10.jpg)'
- en: Figure 3.10 – Providing clear and direct instructions
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 提供清晰直接的指令
- en: '**Few-shot prompt examples**: Providing examples (as covered in the *Few-shot
    prompting* section) of some common scenarios aids in the overall performance gain
    of the model and generating succinct responses with proper formatting.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少样本提示示例**：提供一些常见场景的示例（如“少样本提示”部分所述），有助于提高模型的整体性能，并生成格式正确的简洁响应。'
- en: '`<tag>content</tag>`, can assist in providing a definitive structure in the
    prompt and the output response. We can provide additional context and clarification
    to Claude stating a piece of information can be found within the tags to be leveraged
    for generating the output. In such a way, Claude understands how to frame the
    output response, by extracting the key relevant information from the tags.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<tag>内容</tag>`，可以在提示和输出响应中提供明确的结构。我们可以提供额外的背景和澄清，向Claude说明可以找到一些信息在标签中，以便用于生成输出。这样，Claude就能理解如何构建输出响应，通过从标签中提取关键相关信息。'
- en: It’s also recommended to separate the input data from the instructions to generate
    a more structured prompt for easier and more performant processing by the model.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还建议将输入数据与指令分开，以生成更结构化的提示，以便模型更容易、更高效地处理。
- en: 'Here’s an example of tags:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是一个标签的示例：
- en: '![Figure 3.11 – XML tags](img/B22045_03_11.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – XML标签](img/B22045_03_11.jpg)'
- en: Figure 3.11 – XML tags
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – XML标签
- en: As shown in *Figure 3**.11*, we provided a `<email>` tag, which generated a
    more structured output response from the model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.11*所示，我们提供了一个`<email>`标签，这使得模型生成了更结构化的输出响应。
- en: '**Response limiters and defining the output format**: Anthropic Claude models
    (especially the 100K and 200K token length models) are capable of providing comprehensive
    and verbose responses. The user can limit the response length by explicitly stating
    the word limit, or character count, as part of the prompt to provide a more succinct
    and relevant output. Let’s look at an example:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应限制器和定义输出格式**：Anthropic Claude模型（尤其是100K和200K令牌长度的模型）能够提供全面和详尽的响应。用户可以通过在提示中明确声明单词限制或字符计数来限制响应长度，从而提供更简洁、更相关的输出。让我们看一个例子：'
- en: '![Figure 3.12 – Response limiters](img/B22045_03_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图3.12 – 响应限制器](img/B22045_03_12.jpg)'
- en: Figure 3.12 – Response limiters
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 响应限制器
- en: In *Figure 3**.12*, we have set the response limiter to 100 words as part of
    the prompt. Furthermore, specifying the desired output format – be it a list,
    JSON, paragraph, Markdown, and so on – can lead to more performant and precise
    output, as desired by the user, which aids in eliminating any irrelevant verbiage
    from chatty models.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.12*中，我们将响应限制器设置为100个单词作为提示的一部分。此外，指定所需的输出格式——无论是列表、JSON、段落、Markdown等——可以导致更高效、更精确的输出，正如用户所期望的，这有助于从健谈的模型中消除任何无关的冗词。
- en: '`If you don''t know the answer, respond with the following format – My sincere
    apologies, I''m not aware of the answer` can aid in avoiding any form of hallucination
    from chatty Claude models. Guardrails for the topics can also be added such that
    Claude doesn’t respond to unwanted inputs. This concept will also be discussed
    in [*Chapter 12*](B22045_12.xhtml#_idTextAnchor226).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`如果您不知道答案，请以以下格式回答——我真诚地道歉，我不知道答案`可以帮助避免健谈的Claude模型产生任何形式的幻觉。还可以添加主题的护栏，以便Claude不会对不想要的输入做出响应。这一概念也将在[*第12章*](B22045_12.xhtml#_idTextAnchor226)中讨论。'
- en: Prompting Mistral models
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指导Mistral模型
- en: 'Similar to other models, when working with Mistral models, crafting well-designed
    prompts is crucial to obtaining high-quality and relevant outputs. Here are some
    key points to keep in mind while designing the prompts for Mistral models:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他模型类似，当与Mistral模型一起工作时，精心设计提示对于获得高质量和相关的输出至关重要。在设计Mistral模型的提示时，以下是一些需要记住的关键点：
- en: Clearly define the task or objective you want the model to accomplish through
    the prompt. Are you looking for text classification, summarization, personalization,
    or something else?
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确定义您希望通过提示让模型完成的任务或目标。您是在寻找文本分类、摘要、个性化还是其他内容？
- en: Provide relevant context, examples, or background information to ground the
    model’s understanding before stating the core prompt. Context helps the model
    better comprehend the prompt.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在陈述核心提示之前，提供相关的背景、示例或背景信息，以使模型更好地理解提示。背景有助于模型更好地理解提示。
- en: Use clear formatting and delimiters such as `#`, `###`, or `<<< >>>` to separate
    different sections of the prompt, such as instructions, examples, and the main
    query. This enhances the prompt’s structure.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用清晰的格式和分隔符，如`#`、`###`或`<<< >>>`来分隔提示的不同部分，例如指令、示例和主要查询。这增强了提示的结构。
- en: When possible, demonstrate the desired output through examples in a *few-shot*
    learning style. Showing examples guides the model toward the expected format.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当可能时，通过在*少量样本*学习风格中提供示例来展示期望的输出。展示示例有助于引导模型朝向预期的格式。
- en: Specify the role the model should take on, such as a customer service agent
    or technical writer. Defining a persona makes responses more tailored.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定模型应扮演的角色，例如客户服务代表或技术作家。定义一个角色可以使回答更加定制化。
- en: For an open-ended generation, provide clear instructions on the desired output
    length and structure through numeric targets such as word counts or the number
    of sentences/paragraphs.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于开放式生成，通过数字目标（如单词计数或句子/段落数量）提供关于所需输出长度和结构的明确指令。
- en: Ask the model to include confidence scores or assessments when generating outputs
    to gauge its certainty levels.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成输出时，请要求模型包含置信度分数或评估，以衡量其确定性水平。
- en: Consider chaining multiple Mistral models in a sequence, where the output from
    one model feeds into the next for enhanced capabilities.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑将多个Mistral模型按顺序串联，其中一个模型的输出作为下一个模型的输入，以增强功能。
- en: Test and iterate on prompt designs through evaluations to find optimal prompting
    strategies.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过评估测试和迭代提示设计，以找到最佳的提示策略。
- en: '*Figure 3**.13* shows the Mixtral 8x7B Instruct model being invoked in Amazon
    Bedrock Playground.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.13*显示了在亚马逊Bedrock游乐场中调用Mixtral 8x7B Instruct模型。'
- en: 'Please note that the `<s>` and `</s>` tokens are utilized to denote the `[INST]`
    and `[/INST]` strings tell the model that the content enclosed between them constitutes
    instructions that the model should adhere to:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`<s>` 和 `</s>` 标记用于表示 `[INST]` 和 `[/INST]` 字符串，告诉模型它们之间的内容构成模型应遵守的指令：
- en: '![Figure 3.13 – Prompting the Mixtral 8x7B Instruct model](img/B22045_03_13.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13 – 指示Mixtral 8x7B Instruct模型](img/B22045_03_13.jpg)'
- en: Figure 3.13 – Prompting the Mixtral 8x7B Instruct model
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 – 指示Mixtral 8x7B Instruct模型
- en: The key is to carefully structure prompts with clear context, examples, instructions,
    and formatting to steer Mistral models to generate high-quality, tailored outputs
    matching your needs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于仔细构建带有清晰上下文、示例、指令和格式的提示，以引导Mistral模型生成符合您需求的高质量、定制化输出。
- en: Prompt guidance for Amazon Titan text models
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊泰坦文本模型的提示指导
- en: 'As we learned in [*Chapter 1*](B22045_01.xhtml#_idTextAnchor014), Amazon Titan
    text models are well suited for a plethora of use cases:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[*第一章*](B22045_01.xhtml#_idTextAnchor014)中学习到的，亚马逊泰坦文本模型非常适合多种用例：
- en: Dialog and roleplay systems
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话和角色扮演系统
- en: Text summarization and Q&A
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本摘要和问答
- en: Machine translation
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Metadata extraction and analysis
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据提取和分析
- en: RAG (This will be covered in detail in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090))
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG（将在[*第五章*](B22045_05.xhtml#_idTextAnchor090)中详细说明）
- en: Code generation approaches
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成方法
- en: Text and content generation
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本和内容生成
- en: 'When generating text outputs with the model, it is often recommended to provide
    clear instructions on the desired output length and structure to get optimal results.
    Here are some additional tips for Titan text models:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用模型生成文本输出时，通常建议提供关于所需输出长度和结构的明确指令以获得最佳结果。以下是针对泰坦文本模型的额外提示：
- en: Focus prompts on concise, directed questions to get targeted answers by default.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将提示集中在简洁、有针对性的问题上，以默认方式获得针对性的答案。
- en: Systems perform best on single sentences or short paragraphs.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统在处理单个句子或简短段落时表现最佳。
- en: For longer inputs, place instructions at the end to guide high-quality responses.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较长的输入，将指令放在末尾以引导高质量的响应。
- en: Adding explicit instructions in the prompt produces more tailored results.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中添加明确的指令可以产生更定制的成果。
- en: Specify an exact number of words, sentences, bullet points, or paragraphs you
    want the AI to generate in the prompt. Providing a numerical range (for example,
    100-200 words) can also work well. This gives the model a clear target to aim
    for.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中指定AI应生成的确切单词数、句子数、项目符号或段落数。提供数值范围（例如，100-200个单词）也可以很好地工作。这给模型一个明确的目标。
- en: Avoid vague instructions such as `keep it short` or `summarize briefly`. These
    are open to interpretation by the AI. Precise numbers remove ambiguity.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免使用模糊的指令，如“保持简短”或“简要总结”。这些指令容易被AI解释。精确的数字可以消除歧义。
- en: Word count alone may not sufficiently guide output length as sentence lengths
    can vary. Specifying the number of sentences/paragraphs provides more robust control.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词计数本身可能不足以充分指导输出长度，因为句子长度可能会有所不同。指定句子/段落数量可以提供更稳健的控制。
- en: 'If the model seems unable to generate a quality response for a prompt, program
    it to default to a message such as `Unsure about answer` rather than attempt to
    force a poor response. Here’s an example of such a prompt: `Tell me about Quantum
    Computing. Respond with Unsure about the answer or I don''t know in case you are
    not sure about the question` `being asked`.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型似乎无法为提示生成高质量的响应，请编程使其默认显示消息，例如“不确定答案”，而不是尝试强制生成较差的响应。以下是一个此类提示的示例：“告诉我关于量子计算的信息。如果你对问题不确定，请回答‘不确定答案’或‘我不知道’。”
- en: When relevant, provide context paragraphs for the AI to reference before asking
    a question. This provides knowledge for an informed response.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当相关时，在提问之前为AI提供上下文段落以供参考。这提供了知识，以便做出有根据的响应。
- en: Test different output length instructions to find the right balance between
    conciseness and adequate detail for your use case. Err on the side of more specificity
    with numbers.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试不同的输出长度指令，以找到适合您用例的简洁性和充分细节之间的平衡。在数字上偏向更多具体性。
- en: You can go back to the *Amazon Titan FMs* section in [*Chapter 1*](B22045_01.xhtml#_idTextAnchor014)
    if you wish to look at example prompts and responses in Titan models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想查看Titan模型中的示例提示和响应，可以回到[*第一章*](B22045_01.xhtml#_idTextAnchor014)中的*Amazon
    Titan FMs*部分。
- en: AI21 Labs – instruct models
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI21 Labs – 指示模型
- en: 'AI21 Labs models work very well with languages other than English, such as
    Spanish, French, German, Portuguese, Italian, and Dutch. The model is proficient
    in text summarization, text generation, and Q&A tasks. In this section, we will
    walk through a few key concepts to be inculcated with AI21 models available via
    Amazon Bedrock:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: AI21 Labs的模型在英语以外的语言中表现良好，例如西班牙语、法语、德语、葡萄牙语、意大利语和荷兰语。该模型在文本摘要、文本生成和问答任务中都很熟练。在本节中，我们将介绍一些与通过Amazon
    Bedrock提供的AI21模型相关的关键概念：
- en: '**Output length**: To generate desirable responses from AI12 models, it is
    advisable to specify the output length – that is, the number of paragraphs, items,
    and so on or an approximation of the same – instead of using words/characters.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出长度**：为了从AI12模型生成期望的响应，建议指定输出长度——即段落、条目等的数量或其近似值——而不是使用单词/字符。'
- en: '**Provide short yet detailed task descriptions**: Craft clear, detailed task
    descriptions to minimize ambiguity. AI21 models excel at following precise instructions
    for even complex jobs.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供简短而详细的任务描述**：制定清晰、详细的任务描述以减少歧义。AI21模型擅长遵循精确的指令，即使是复杂的任务。'
- en: '`no more than x statements`. It is always recommended to state requirements
    directly and affirmatively.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`不超过x个陈述`。始终建议直接和肯定地陈述要求。'
- en: '`Instruction:` header to clarify the prompt. Separate prompt sections with
    newlines to highlight distinct pieces and for readability purposes.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`指令:`标题以澄清提示。使用换行符分隔提示部分以突出不同的部分并提高可读性。'
- en: '**Evaluate multiple prompting patterns**: Try both zero-shot and few-shot learning.
    Pick an ideal approach for your use case. For instance, as depicted in the zero-shot
    and few-shot examples shown in this chapter, depending on the use case under consideration,
    you may initiate by providing zero examples and determining the response, and
    simultaneously compare the response generated from the model after providing certain
    examples to guide the output. In some cases, there may not be a need to provide
    a ton of examples if the model can generate a desirable response.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估多种提示模式**：尝试零样本学习和少量样本学习。为您的用例选择一个理想的方法。例如，如本章中展示的零样本和少量样本示例所示，根据考虑的用例，您可能从提供零个示例并确定响应开始，同时比较在提供一定示例以引导输出后模型生成的响应。在某些情况下，如果模型可以生成期望的响应，可能不需要提供大量示例。'
- en: '*Figure 3**.14* depicts a product description summarization example from AI21
    Jurassic-2 Ultra within Amazon Bedrock:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3**.14*展示了来自Amazon Bedrock中AI21 Jurassic-2 Ultra的产品描述摘要示例：'
- en: '![Figure 3.14 – Prompting the AI21 Jurassic-2 Ultra model](img/B22045_03_14.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14 – 提示AI21 Jurassic-2 Ultra模型](img/B22045_03_14.jpg)'
- en: Figure 3.14 – Prompting the AI21 Jurassic-2 Ultra model
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – 提示AI21 Jurassic-2 Ultra模型
- en: Further details on prompt engineering and design with AI21 models, along with
    examples, can be found at [https://docs.ai21.com/docs/prompt-engineering](https://docs.ai21.com/docs/prompt-engineering).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于使用AI21模型进行提示工程和设计的详细信息，以及示例，可以在[https://docs.ai21.com/docs/prompt-engineering](https://docs.ai21.com/docs/prompt-engineering)找到。
- en: Prompting Meta Llama models
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示Meta Llama模型
- en: 'Similar to any other LLM, effective prompting is essential for getting the
    most out of Llama models. Since the following is standard prompt guidance for
    any LLM, we will cover some of the best practices here:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他大型语言模型 (LLM) 一样，有效的提示对于充分利用 Llama 模型至关重要。由于以下内容是任何 LLM 的标准提示指南，我们将在此处介绍一些最佳实践：
- en: '**Clarity** **and specificity**:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清晰性和具体性**：'
- en: Ensure that your prompts are clear, concise, and unambiguous
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您的提示清晰、简洁且无歧义。
- en: Provide sufficient context and details to guide the model toward the desired
    output
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供足够的信息和细节，以引导模型生成所需输出。
- en: Use precise language and avoid vague or open-ended statements
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用精确的语言，避免含糊或开放式陈述。
- en: '**Structure** **and formatting**:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构和格式**：'
- en: Organize your prompts logically and structure them in a way that aligns with
    the desired output format
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合理组织您的提示，并以与所需输出格式一致的方式构建它们。
- en: Utilize formatting elements such as bullet points, numbered lists, or headings
    to enhance readability and comprehension
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用格式化元素，如项目符号、编号列表或标题，以增强可读性和理解性。
- en: Consider providing examples or templates to illustrate the expected format of
    the output
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑提供示例或模板来说明预期输出格式。
- en: '**Task framing**:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务构建**：'
- en: Frame your prompts as specific tasks or instructions for the model to follow
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将提示构建为模型需要遵循的具体任务或指令。
- en: Clearly specify the desired action, such as summarizing, generating, or analyzing
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确指定所需操作，例如总结、生成或分析。
- en: Provide context about the intended use case or audience for the output
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供有关预期用例或受众的背景信息。
- en: '**Iterative refinement**:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代优化**：'
- en: Prompting is an iterative process, and you may need to refine your prompts based
    on the model’s responses
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示是一个迭代过程，您可能需要根据模型的响应来优化您的提示。
- en: Analyze the output and identify areas for improvement or clarification
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析输出并确定改进或澄清的领域。
- en: Incorporate feedback and adjust the prompt accordingly to steer the model toward
    better results
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合反馈并根据需要调整提示，以引导模型获得更好的结果。
- en: '**Fine-tuning** **and customization**:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调和定制**：'
- en: Explore the possibility of fine-tuning the Llama model on domain-specific data
    or examples
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索在特定领域数据或示例上微调 Llama 模型的可能性。
- en: Customize the model’s behavior and outputs by incorporating specific instructions
    or constraints in the prompt
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在提示中包含特定指令或约束来定制模型的行为和输出。
- en: Leverage techniques such as prompting with few-shot examples or demonstrations
    to improve performance
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用诸如使用少量示例或演示进行提示等技巧来提高性能。
- en: '**Ethical and** **safety considerations**:'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理和安全考虑**：'
- en: Be mindful of potential biases or harmful outputs that the model could generate
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意模型可能产生的潜在偏见或有害输出。
- en: Incorporate explicit instructions or filters to mitigate risks and ensure the
    model’s responses align with ethical and safety guidelines
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含明确的指令或过滤器以减轻风险并确保模型的响应符合伦理和安全指南。
- en: Monitor and evaluate the model’s outputs for any concerning or inappropriate
    content
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和评估模型的输出，以查找任何令人担忧或不适当的内容。
- en: 'Llama models also consider special kinds of tokens. For Llama 3, the following
    tokens are used:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 模型也考虑特殊类型的标记。对于 Llama 3，以下标记被使用：
- en: The `<|begin_of_text|>` token represents the BOS token.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<|begin_of_text|>` 标记代表 BOS 标记。'
- en: The `<|eot_id|>` token indicates the end of the current turn or message.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<|eot_id|>` 标记表示当前回合或消息的结束。'
- en: The `<|start_header_id|>{role}<|end_header_id|>` token encloses the role for
    a particular message, which can be either **system**, **user**, or **assistant**.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<|start_header_id|>{role}<|end_header_id|>` 标记包围特定消息的角色，可以是 **system**、**user**
    或 **assistant**。'
- en: The `<|end_of_text|>` token is equivalent to the EOS token. Upon generating
    this token, Llama 3 will stop producing any further tokens.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<|end_of_text|>` 标记等同于 EOS 标记。生成此标记后，Llama 3 将停止生成任何进一步的标记。'
- en: 'For more details on prompt formats, go to [https://llama.meta.com/docs/model-cards-and-prompt-formats](https://llama.meta.com/docs/model-cards-and-prompt-formats):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有关提示格式的更多详细信息，请访问 [https://llama.meta.com/docs/model-cards-and-prompt-formats](https://llama.meta.com/docs/model-cards-and-prompt-formats)：
- en: '![Figure 3.15 – Prompting the Llama2 Chat 70B model](img/B22045_03_15.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.15 – 激活 Llama2 Chat 70B 模型](img/B22045_03_15.jpg)'
- en: Figure 3.15 – Prompting the Llama2 Chat 70B model
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – 激活 Llama2 Chat 70B 模型
- en: As shown in *Figure 3**.15*, the Llama 2 Chat 70B model is being invoked in
    Amazon Bedrock Playground.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3*.15 所示，Llama 2 Chat 70B 模型正在 Amazon Bedrock Playground 中被调用。
- en: The `[INST]` and `[/INST]` strings tell the model that the content enclosed
    between them constitutes instructions that the model should adhere to.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`[INST]` 和 `[/INST]` 字符串告诉模型，它们之间的内容构成了模型应遵守的指令。'
- en: If you’d like to learn about the different examples and templates for invoking
    various models, including any new models being added to Amazon Bedrock, go to
    [https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解调用各种模型的不同示例和模板，包括任何添加到亚马逊Bedrock的新模型，请访问[https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html)。
- en: Prompt guidance for Stability AI – Stable Diffusion
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Stability AI – 稳定扩散的提示指南
- en: 'Stable Diffusion models (along with Amazon Titan image models) have been gaining
    popularity in image generation use cases. Here are some key tips for crafting
    effective prompts when using Stability AI’s Stable Diffusion for image generation:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散模型（包括亚马逊泰坦图像模型）在图像生成用例中越来越受欢迎。以下是在使用Stability AI的稳定扩散进行图像生成时制定有效提示的关键提示：
- en: '`A photo of a cat` or `An illustration of a robot`. Being more specific usually
    produces better results.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “一张猫的相片”或“一个机器人的插图”。更具体通常会产生更好的结果。
- en: '`in impressionist style` or `a cartoon drawing of`. Styles help steer the output.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in impressionist style` 或 `a cartoon drawing of`。风格有助于引导输出。'
- en: '`cat:1.5, sitting:1.2, couch:1`. Higher weights make elements more prominent.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cat:1.5, sitting:1.2, couch:1`. 较高的权重使元素更加突出。'
- en: '`-`, improves quality by excluding unwanted elements. For example, if you provide
    `Cars racing on racetrack` as a prompt and give `red car` as a negative prompt,
    it will exclude red cars from the image’s output, as shown in *Figure 3**.16*:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-` 通过排除不需要的元素来提高质量。例如，如果您提供“赛车在赛道上”作为提示，并给出“红色汽车”作为负面提示，它将排除图像输出中的红色汽车，如图*图3.16*所示：'
- en: '![Figure 3.16 – Negative prompt example](img/B22045_03_16.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – 负面提示示例](img/B22045_03_16.jpg)'
- en: Figure 3.16 – Negative prompt example
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 负面提示示例
- en: '**Be detailed and specific**: Using more descriptive and distinctive words,
    rather than general terms, produces more tailored results.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**详细具体**：使用更多描述性和独特的词语，而不是通用术语，可以产生更定制的结果。'
- en: '`--ar`, `--v`, and `--n` control the aspect ratio, vividness, and level of
    detail, respectively. Tweak them to refine the output.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--ar`、`--v` 和 `--n` 分别控制长宽比、生动性和细节级别。调整它们以细化输出。'
- en: 'Providing additional descriptive details always aids the model in being more
    performant. The following are examples of such aspects:'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供额外的描述性细节始终有助于模型表现得更好。以下是一些此类方面的示例：
- en: Specify the medium (painting, drawing, CGI, and so on)
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定介质（绘画、绘图、CGI等）
- en: Define colors used
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义使用的颜色。
- en: Describe lighting and shadows
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述光照和阴影
- en: Include the artist’s name if mimicking a style
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模仿某种风格，请包含艺术家姓名。
- en: Mention the website if you’re reproducing a specific image
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在复制特定图像，请提及网站。
- en: Add any other descriptive remarks or adjectives
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加任何其他描述性评论或形容词。
- en: Specify the desired resolution if needed for print or digital use
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如需打印或数字使用，请指定所需的分辨率。
- en: Let’s look at an example.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。
- en: '`Portrait photo of an Indian old warrior chief, tribal panther make up, front
    profile, looking straight into the camera, serious eyes, 50mm portrait photography,
    hard rim lighting photography–beta –ar` `2:3 –beta`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: “印度老战士首领的肖像照片，部落豹纹妆容，正面轮廓，直视镜头，严肃的眼神，50mm肖像摄影，硬边照明摄影–beta –ar` `2:3 –beta`
- en: '**Output response from** **SDXL 1.0**:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**SDXL 1.0的输出响应**：'
- en: '![Figure 3.17 – Image generation output from SDXL 1.0](img/B22045_03_17.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图3.17 – SDXL 1.0的图像生成输出](img/B22045_03_17.jpg)'
- en: Figure 3.17 – Image generation output from SDXL 1.0
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – SDXL 1.0的图像生成输出
- en: 'When prompted with the same input, the output response from the Titan Image
    Generator G1 model is as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用相同的输入提示时，Titan Image Generator G1模型的输出响应如下：
- en: '![Figure 3.18 – Image generation output from Titan Image Generator](img/B22045_03_18.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图3.18 – Titan Image Generator的图像生成输出](img/B22045_03_18.jpg)'
- en: Figure 3.18 – Image generation output from Titan Image Generator
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – Titan Image Generator的图像生成输出
- en: Providing this level of detail and context will help produce a more accurate
    image that matches your vision. Adjust prompts iteratively to refine results.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 提供此级别的详细信息和上下文将有助于生成更准确、符合您愿景的图像。通过迭代调整提示以细化结果。
- en: 'You might be wondering why the same prompt generated different outputs from
    two models. The reason for this is that SDXL is trained on a different set of
    data than Titan Image Generator, so the output you will see from these models
    will differ. Think of it this way: SDXL and Titan are two people, who learn from
    two different books for an exam. During the exam, when they were asked the same
    question, they would have two different viewpoints, and their answer would be
    based on the books they read.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么相同的提示在两个模型中产生了不同的输出。原因在于SDXL是在与Titan Image Generator不同的数据集上训练的，因此这些模型产生的输出会有所不同。可以这样想：SDXL和Titan是两个人，他们为了考试从不同的书中学习。在考试中，当被问到相同的问题时，他们会有不同的观点，他们的答案将基于他们所读的书籍。
- en: If you are trying these prompts in your environment, you might also notice another
    thing. The output image that you are seeing in your environment might differ from
    the one shown here, even if you provide the same prompts. The reason for this
    is to do with the added degree of randomness. These models will generate output
    based on the inference parameters, such as *Prompt Strength* and *Seed*. We will
    cover these parameters in detail in [*Chapter 9*](B22045_09.xhtml#_idTextAnchor171).
    However, in short, prompt strength controls how you want the model output to be
    influenced by the prompt, and the seed is a way to randomize an output image.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在你的环境中尝试这些提示，你也可能注意到另一件事。你环境中看到的输出图像可能与这里展示的不同，即使你提供了相同的提示。这是因为增加了随机性程度。这些模型将根据推理参数（如*提示强度*和*种子*）生成输出。我们将在[*第9章*](B22045_09.xhtml#_idTextAnchor171)中详细讨论这些参数。然而，简而言之，提示强度控制着模型输出受提示影响的程度，而种子是一种随机化输出图像的方法。
- en: Now that you have a good understanding of prompt guidance for FMs provided by
    Amazon Bedrock, in the next section, we will try to sum up some of the key principles
    and techniques you must follow when it comes to prompt engineering in various
    use cases.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对Amazon Bedrock提供的FMs提示指导有了很好的理解，在下一节中，我们将尝试总结一些关键原则和技术，这些原则和技术在处理各种用例时的提示工程中必须遵循。
- en: Understanding best practices in prompt engineering
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解提示工程的最佳实践
- en: 'To summarize, when crafting prompts, you must adhere to the following key principles:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在构建提示时，你必须遵守以下关键原则：
- en: '`What are the use cases of renewable resources? List 5` `key points`.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`可再生能源有哪些应用场景？列出5个` `关键点`。'
- en: '*Language emphasis*: Using simple flowing language with coherent sentences
    assists in crafting a better prompt and avoiding isolated phrases.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言强调*：使用简单流畅的语言和连贯的句子有助于构建更好的提示，避免使用孤立的短语。'
- en: '*Get into the model’s head*: Craft prompts to nudge it toward helpful behaviors.
    Think of it as someone who has all the right answers but only for correctly articulated
    questions.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*进入模型的思维模式*：构建提示以引导其朝向有益的行为。把它想象成一个人，他拥有所有正确的答案，但只对正确表述的问题有答案。'
- en: '`Summarize the chapter in` `500 words`.*   *Provide example responses*: Adding
    some example responses within the prompt with the expected output can refine the
    responses more desirably – for example, `Summarize this chapter in one paragraph
    (1000 characters): [New study shows decreasing activity in region X leads to impairment.]`.
    Surrounding the example response in brackets indicates the model adheres to the
    guidelines set by the user while responding in the desired format.*   *Add constraints*:
    Constraining prompt responses by format, additional information inclusion, length,
    and more can lead to more controlled output.*   **Strike the right detail balance**:
    Too little detail fails to guide the model adequately while excessive verbosity
    limits creative flourishes. Distill prompts down to concise essence:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`用500字` `总结本章内容`.*   *提供示例响应*：在提示中添加一些示例响应，以期望的输出来细化响应——例如，`用一段话（1000字符）总结本章：[新研究表明区域X的活动减少导致功能障碍。]`。用括号包围示例响应表示模型在以用户设定的指南和期望的格式进行响应时遵守了规定。*   *添加约束*：通过格式、附加信息包含、长度等对提示响应进行约束，可以导致更可控的输出。*   **平衡正确细节的多少**：细节太少无法充分引导模型，而过于冗长则限制了创造性发挥。将提示提炼为简洁的精华：'
- en: '*Complex* *task handling*:'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*复杂任务处理*：'
- en: FMs can hallucinate when dealing with complex tasks. It is advisable to break
    down the complex task into subtasks or even consider splitting complex tasks into
    multiple prompts.
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理复杂任务时，FMs可能会产生幻觉。建议将复杂任务分解为子任务，甚至考虑将复杂任务拆分为多个提示。
- en: Provide emphasis by using keywords to ask the model to think step-by-step or
    provide logical reasoning as it is crafting the output. Provide some key examples
    in the input for complex tasks.
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用关键词来要求模型逐步思考或提供逻辑推理，以构建输出，从而提供强调。在复杂任务中，为输入提供一些关键示例。
- en: '*Rinse, lather, repeat*: Iteratively break down and try different prompts to
    optimize model responses for your goals. Continue adjusting while testing and
    experimenting to achieve the desired results.'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重复清洗、泡沫、再重复*：迭代地分解并尝试不同的提示，以优化模型响应以符合你的目标。在测试和实验的同时继续调整，以达到预期的结果。'
- en: '*Continuous evaluation*: Iteratively reviewing the model’s responses to provide
    the desired quality is a must when it comes to handling different use cases and
    complex scenarios.'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*持续评估*：在处理不同的用例和复杂场景时，迭代地审查模型的响应以提供所需的质量是必不可少的。'
- en: Prompts unlock Generative AI’s capabilities but require thoughtfulness to construct
    properly. Learn your target model’s strengths and limitations, iterate carefully
    on prompt phrasings, and appreciate these systems’ ever-evolving nature. Wield
    prompts judiciously and enjoy the fruits of AI’s burgeoning creativity!
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 提示解锁了生成AI的能力，但需要深思熟虑才能正确构建。了解你的目标模型的优势和局限性，仔细迭代提示措辞，并欣赏这些系统不断演变的本质。明智地使用提示，享受AI蓬勃发展的创造力的果实！
- en: Complexity arises in structuring the right prompts to handle intricate goals.
    But when done well, prompts unlock AI like a skeleton key, opening doors to breathtaking
    new generative capabilities. The prompt contains the potential; our role is to
    shape and guide it with thoughtful prompts.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建处理复杂目标的正确提示时，会出现复杂性。但做得好的话，提示就像万能钥匙一样解锁AI，打开通往令人叹为观止的新生成能力的大门。提示包含了潜力；我们的角色是通过深思熟虑的提示来塑造和引导它。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about several prompt engineering techniques to gain
    a deeper understanding of prompting patterns and uncovered insights while considering
    examples of said prompting patterns. Then, we dived into prompt guidance with
    Amazon Bedrock models for Anthropic Claude, AI21 Labs, Amazon Titan, and Stability
    AI’s Stable Diffusion.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了几个提示工程技巧，以更深入地理解提示模式，并在考虑这些提示模式的例子时发现了见解。然后，我们深入探讨了Amazon Bedrock模型为Anthropic
    Claude、AI21 Labs、Amazon Titan和Stability AI的Stable Diffusion提供的提示指导。
- en: Lastly, we summarized the practical approach to prompt guidance while looking
    at Amazon Bedrock models that can be applied to various use cases. Through various
    examples, we learned how to craft the most effective prompts.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在查看适用于各种用例的Amazon Bedrock模型的同时，总结了提示指导的实用方法。通过各种示例，我们学习了如何构建最有效的提示。
- en: At this point, you should have a good understanding of the importance of prompt
    engineering. Furthermore, you should be able to analyze various prompt techniques
    and best practices involved in prompt engineering in the context of building Generative
    AI applications.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对提示工程的重要性有了很好的理解。此外，你应该能够分析在构建生成AI应用的情况下，涉及提示工程的多种提示技术和最佳实践。
- en: In the next chapter, we’ll learn how to customize a model using fine-tuning
    and continued pretraining techniques. We will delve into how fine-tuning works,
    look at various APIs, analyze the results, and perform inference on our fine-tuned
    model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用微调和持续预训练技术来定制模型。我们将深入了解微调的工作原理，查看各种API，分析结果，并在我们的微调模型上进行推理。
