- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Engineering Prompts for Effective Model Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter begins with an overview of prompt engineering and its importance.
    We will walk through various prompt engineering techniques and the ability to
    incorporate them while prompting any model on Amazon Bedrock, primarily focusing
    on designing and analyzing effective prompt techniques to get the desired outcome
    from the Bedrock models. This chapter also entails some of the best practices
    associated with prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have developed a clear understanding of
    the practical aspects of prompt engineering and be able to craft effective prompts
    while following the best practices to get the desired outcome from the models
    available on Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlocking prompt engineering techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing prompts for Amazon Bedrock models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding best practices in prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete this chapter, you need to have access to the *AWS* console so that
    you can navigate to Amazon Bedrock Playground to execute prompt engineering techniques.
    Here’s the web page to access the console: [https://console.aws.amazon.com/](https://console.aws.amazon.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, you need to have the right permissions to invoke Amazon Bedrock models
    from your local machine using *Amazon Bedrock APIs* or *Bedrock Python SDK* so
    that you can execute the prompts. To learn more, go to [https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html](https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html).
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we have been discussing Amazon Bedrock models and how to invoke them,
    we need to dive into prompt engineering. Essentially, in a way that a particular
    child asks their parents questions about anything and everything, we can also
    ask an LLM anything under the Sun! However, to get the best and most precise outputs
    possible, we must train ourselves to ask the model the right questions in the
    right manner.
  prefs: []
  type: TYPE_NORMAL
- en: With the increasing popularity of LLMs, users are actively striving to refine
    their way of asking the model different kinds of questions to attain a desired
    response. For instance, we can simply ask an LLM questions such as `Who was the
    first person to land on the Moon?` or `How many moons does Jupiter have?`. Based
    on these questions, the language model can respond to the user’s queries either
    factually or provide an inadequate/incorrect response based on the LLM’s knowledge,
    which is the data it has been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect responses that the users get without fact-checking are what we refer
    to as **hallucinations**. It is often seen that if the user asks an ambiguous
    question or a particularly complex math problem that the model hasn’t been trained
    to answer, it will determine a probabilistic answer that may or may not be factually
    accurate. This can also occur with large vision models such as text-to-image models,
    where the model ends up providing an undesirable image as the prompted response.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, how we ask questions to the model and how effectively we can provide
    a description regarding our question becomes a crucial factor for the model to
    generate a desirable output.
  prefs: []
  type: TYPE_NORMAL
- en: The method of prompting the model in the right manner while avoiding any ambiguity
    in your prompts becomes the essence of effective **prompt engineering**. This
    is not just applicable to the technical community anymore!
  prefs: []
  type: TYPE_NORMAL
- en: Even people with varying technical backgrounds can use LLMs for a range of tasks.
    Based on the user prompts, the models can offer basic tips on entrepreneurship
    or provide fundamental insights into website creation through detailed, informative
    conversations.
  prefs: []
  type: TYPE_NORMAL
- en: Effective prompt engineering techniques pave the way for users to get the desired
    responses. Furthermore, some companies have been offering high-paying jobs for
    researchers and personas who can write or adopt effective prompt engineering to
    get their model to perform responsible actions, thereby enhancing the company’s
    productivity to execute their functions/tasks at an accelerated pace.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explain how effective prompt engineering techniques can be
    applied to LLMs. But first, let’s dive into the structure of a prompt and some
    key ideas that focus on effective prompt techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Components of prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How you write a prompt plays a crucial role in guiding the behavior of the
    model. Prompts contain a few key elements. Let’s understand those elements through
    an example (*Figure 3**.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Components of a prompt](img/B22045_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Components of a prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at the terms highlighted in the preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instruction**: With an instruction, you provide a clear and concise description
    or instruction to the model on how it should perform the task, whether it be summarizing
    text, translating languages, composing music, or any number of other things. In
    the preceding figure, you can see that we have asked the model to act as a specialist
    in quantum computing and answer the user’s question in detail and in layman’s
    terms, along with examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context**: Context refers to the relevant background information that you
    provide to the model to enhance its performance. This can include any relevant
    data, past experiences, or domain-specific knowledge. In the preceding figure,
    in terms of context, we stated that the model has recently completed a PhD and
    has been asked to be part of an interview on a talk show that explains complex
    topics in layman’s terms. This primes the model with pertinent knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`What is Quantum Computing?`. As depicted in the preceding figure, the input
    question that goes to the model is `Can you provide me your thoughts on Quantum`
    `Machine Learning?`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Quantum Ninja`, as depicted in the preceding figure, so that the model understands
    that its output should be in this layout, or it could be in a specific format,
    such as text, JSON, an audio clip, and so on. Special syntax such as *<|endoftext|>*
    signals the end of the input and the beginning of the model’s output. This special
    syntax can vary on a per-model basis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although prompts need not have all four elements, their form depends on the
    task. Let’s examine a few sample prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1:** **SQL query**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – SQL query prompt](img/B22045_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – SQL query prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 3**.2*, we are specifying the following prompt elements
    to the Titan Text G1 – Premier model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`You are querying a database with the following schema: Customers(id, name,
    age) and Orders(id, cust_id,` `product, amount).`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`List all customers who have placed more than 1 order, along with their total`
    `order amounts.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SQL query:`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This prompt provides clear instructions, relevant schema context, a sample input,
    and output indicators to produce a suitable SQL query.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 2:** **Recipe generation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another example (*Figure 3**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Recipe prompt](img/B22045_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Recipe prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Baked salmon is a healthy weeknight dinner option perfect with roasted potatoes
    or rice. The fresh dill adds an` `aromatic flavor.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Salmon fillet, dill, lemon, salt, pepper,` `olive oil`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<|endoftext|>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding prompt provides the model with the recipe’s title, introductory
    context, ingredients as input data, and `<|endoftext|>` as an output indicator,
    which signals where the recipe steps should begin.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we understand how we can communicate with the model, let’s learn about
    some prompt engineering techniques that can aid us in getting better responses
    from the model.
  prefs: []
  type: TYPE_NORMAL
- en: However, primarily, we need to understand that the optimal prompt engineering
    approach for any given use case is heavily reliant on the task at hand, as well
    as the data on which it has been trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the tasks that the models on Bedrock excel at are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: LLMs exhibit prowess in text classification, a supervised
    learning technique for assigning categories to text. For instance, sentiment analysis
    involves discerning whether an input passage conveys positive or negative emotion.
    Some LLMs available via Amazon Bedrock, such as the Amazon Titan models, can also
    identify toxic, harmless, or fact-based content. Their deep contextual understanding
    aids the judgment of subtle linguistic cues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question-answering**: The models can answer questions accurately without
    external context due to their vast parameters gained from ingesting hundreds of
    billions of words during pre-training. When provided with relevant documents,
    their performance further improves by reasoning over the additional context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarization**: The models condense lengthy texts into concise summaries
    that preserve key details, learning to differentiate salient points. Adding such
    a prompt facilitates rapid analysis of documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generation**: The model can generate original coherent text given a
    short prompt. Their fluency and semantic consistency allow realistic synthesis
    of stories, poems, scripts, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: For a textual description of a programming need, the models
    can generate executable code in languages such as SQL and Python. For example,
    a prompt could request text-to-SQL or Python code generation, thereby accomplishing
    the outlined computational goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mathematical reasoning**: The models exhibit an aptitude for mathematical
    problems provided in text form. This includes numerical calculations, logical
    deduction, and geometric reasoning. They can further justify solutions with step-by-step
    explanations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The breadth of natural language tasks that can be mastered by LLMs on Amazon
    Bedrock exemplifies their versatility. Their adaptive capacity promises to expand
    application domains even further.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve gained insights into applications of prompt engineering in the
    real world, let’s try to unlock some of the most common prompt engineering techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Unlocking prompt engineering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of prompt engineering is an active area of research and innovation
    with new techniques and patterns emerging frequently, driven by the pursuit to
    improve the performance of the models and generate more natural human-like responses.
    In this section, we are going to look at some of the most common patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Zero-shot** refers to the ability of LLMs to generate reasonable responses
    to prompts that it has not been explicitly trained on. It relies solely on a descriptive
    prompt to specify the desired output, as depicted in *Figure 3**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Zero-shot prompting](img/B22045_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Zero-shot prompting
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a zero-shot prompt to get a poem could be `Write a rhyming poem
    with 4 stanzas about` `seasons changing`.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this method is that it’s easier; prompt crafting can be
    done without providing examples in the input. However, output quality can vary
    without concrete examples to base on.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Few-shot prompting** or **few-shot learning** builds on zero-shot’s capabilities.
    As depicted in *Figure 3**.5*, on top of instructions/questions, you can provide
    a few examples that establish a concept or scenario, at which point models can
    start to generate reasonable continuations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Few-shot prompting](img/B22045_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Few-shot prompting
  prefs: []
  type: TYPE_NORMAL
- en: For instance, after showing two or three examples of short conversations about
    booking a doctor’s appointment, LLMs can produce an appointment booking dialog
    without needing thousands of examples. The key benefit over zero-shot is that
    few-shot examples help narrow down the context and constrain the generation process,
    making outputs more precise.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of the examples of few-shot prompting. The following are
    two inputs that we can use in the poem writing task from the previous sub-section
    that we can give to the model as examples.
  prefs: []
  type: TYPE_NORMAL
- en: '`Roses are red, violets are blue, spring brings` `life anew.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Summer sun shining bright, long days full` `of light.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Now you write a rhyming poem about autumn` `changing leaves.`'
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing these examples, LLMs can learn the pattern of rhyming four-line
    stanzas about seasons. It can then follow the template to generate an autumn poem.
    Balancing creativity and guidance is key in few-shot prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional examples of few-shot prompting are available here: [https://www.promptingguide.ai/techniques/fewshot](https://www.promptingguide.ai/techniques/fewshot).'
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-thought prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Chain of thought** (**CoT**) **prompting** aims to elicit reasoning chains
    from language models. It involves providing the LLM with a prompt that lays out
    a reasoning chain or train of thought for the model to follow (*Figure 3**.6*).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Chain-of-thought prompting](img/B22045_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Chain-of-thought prompting
  prefs: []
  type: TYPE_NORMAL
- en: 'A prompt may start with background context, state a hypothesis or problem,
    provide reasoning steps, and end with a conclusion to be expanded on. The model
    then tries to continue the chain of reasoning coherently in its generated text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of a CoT prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Recycling is beneficial for the environment because it reduces waste sent
    to landfills. Landfills produce methane, a potent greenhouse gas. They also take
    up large amounts of space. Recycling reduces landfill contributions by reusing
    materials. In conclusion, recycling helps fight climate change by reducing landfill
    methane and space requirements. The main environmental benefits of` `recycling
    are...`'
  prefs: []
  type: TYPE_NORMAL
- en: This prompting style guides the LLM to follow the provided reasoning chain and
    elaborate further on the conclusion statement. The generated text will likely
    discuss reduced methane emissions and land use from increased recycling in more
    detail. Hence, chaining further encourages step-by-step logical thinking that
    focuses on the end goal over open-ended, meandering text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some examples of CoT prompting and the responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Write a story about a professor exploring a` `mysterious artifact`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`As she opens the box, a glowing light emerges, illuminating symbols on` `the
    walls`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Example 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`What are the pros and cons of renewable` `energy sources?`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`You missed one major renewable source. Please include geothermal energy when
    comparing the pros` `and cons.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this approach, the human can guide and shape the overall narrative or analysis
    by inserting additional prompts whenever they want to redirect, refine, or expand
    the model’s response. It allows for a more conversational flow based on an initial
    theme or direction provided by the human.
  prefs: []
  type: TYPE_NORMAL
- en: Additional examples of CoT prompting are available at [https://www.promptingguide.ai/techniques/cot](https://www.promptingguide.ai/techniques/cot).
  prefs: []
  type: TYPE_NORMAL
- en: ReAct prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reasoning and Acting** (**ReAct**) prompting is derived from the ReAct framework,
    as introduced in the paper *ReAct: Synergizing Reasoning and Acting in Language*
    *Models* ([https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – ReAct prompting](img/B22045_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – ReAct prompting
  prefs: []
  type: TYPE_NORMAL
- en: This framework generates various reasoning traces. Then, based on those traces,
    it performs relevant actions. Through this process of reasoning and acting, you
    can also allow the application to have access to the organization’s internal data
    stores and APIs, or external sources from various tools, websites, web searches,
    and more, as shown in *Figure 3**.7*. This process of generating reasoning traces
    and performing actions is iterative and occurs until the LLM finds the answer
    to the user’s prompt.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss ReAct prompting in more detail in [*Chapter 10*](B22045_10.xhtml#_idTextAnchor192).
    For now, let’s create a ReAct-SQL tool for deriving a comprehensive response from
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '`Provide an updated financial summary report` `for AnyCompany`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assistant response:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Thought*: The user is asking for financial information about AnyCompany. To
    answer this, I need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Query the database to find financial records for AnyCompany
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a summary of the financial information obtained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, I will need to use the `sql_db_query` tool.
  prefs: []
  type: TYPE_NORMAL
- en: '*Action*: `sql_db_query`'
  prefs: []
  type: TYPE_NORMAL
- en: '*Action input*: `SELECT * FROM financial_db WHERE company = ''``AnyCompany'';`'
  prefs: []
  type: TYPE_NORMAL
- en: '`[(260B, 55B,` `321B, 2.2T)]`'
  prefs: []
  type: TYPE_NORMAL
- en: '*Answer*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the preceding form of prompting patterns, new prompt engineering
    techniques and patterns are burgeoning these days. More details on prompt engineering
    can be found in the prompt engineering guide at [https://www.promptingguide.ai/techniques](https://www.promptingguide.ai/techniques).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we uncovered several prompt engineering techniques that are
    being leveraged in the realm of building Generative AI applications for various
    use cases. Next, we will work on designing prompts for Amazon Bedrock FMs.
  prefs: []
  type: TYPE_NORMAL
- en: Designing prompts for Amazon Bedrock models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’re going to cover prompt guidance for some of the models
    available via Amazon Bedrock. We will kick things off with the Anthropic Claude
    model and provide details around prompt guidance for this. The majority of the
    learning for prompt guidance can be inherited from Claude models. Furthermore,
    for the sake of striking a balance between brevity and detail, we will shine a
    light on the models from Amazon Titan, AI21 Labs, and Stability AI Stable Diffusion.
    This will sum up our prompt guidance and associated prompt recommendations for
    invoking Amazon Bedrock models.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting Anthropic Claude 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some things to keep in mind while prompting the Anthropic Claude 3
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`You are a seasoned Children''s Book Author` or `You are a` `Business Expert`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Imagine you are a kindergarten teacher and have to provide an explanation
    for rainbows in the sky to` `the children.`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Anthropic Claude 3 Haiku – simple prompting](img/B22045_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Anthropic Claude 3 Haiku – simple prompting
  prefs: []
  type: TYPE_NORMAL
- en: Without assigning a role/persona, the answer may be complex to understand, as
    shown in *Figure 3**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding the role/persona, you can see that the output response aligns
    more with a child’s complexity level in *Figure 3**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Anthropic Claude 3 Haiku – assigning role personas](img/B22045_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Anthropic Claude 3 Haiku – assigning role personas
  prefs: []
  type: TYPE_NORMAL
- en: '`Temperature` parameter can be set to a higher value, the output that’s generated
    might be different, as shown here. However, the clear instructions in the latter
    provide a more direct output without any additional context, as desired by the
    user:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Providing clear and direct instructions](img/B22045_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Providing clear and direct instructions
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-shot prompt examples**: Providing examples (as covered in the *Few-shot
    prompting* section) of some common scenarios aids in the overall performance gain
    of the model and generating succinct responses with proper formatting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<tag>content</tag>`, can assist in providing a definitive structure in the
    prompt and the output response. We can provide additional context and clarification
    to Claude stating a piece of information can be found within the tags to be leveraged
    for generating the output. In such a way, Claude understands how to frame the
    output response, by extracting the key relevant information from the tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s also recommended to separate the input data from the instructions to generate
    a more structured prompt for easier and more performant processing by the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here’s an example of tags:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11 – XML tags](img/B22045_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – XML tags
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 3**.11*, we provided a `<email>` tag, which generated a
    more structured output response from the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Response limiters and defining the output format**: Anthropic Claude models
    (especially the 100K and 200K token length models) are capable of providing comprehensive
    and verbose responses. The user can limit the response length by explicitly stating
    the word limit, or character count, as part of the prompt to provide a more succinct
    and relevant output. Let’s look at an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Response limiters](img/B22045_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Response limiters
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.12*, we have set the response limiter to 100 words as part of
    the prompt. Furthermore, specifying the desired output format – be it a list,
    JSON, paragraph, Markdown, and so on – can lead to more performant and precise
    output, as desired by the user, which aids in eliminating any irrelevant verbiage
    from chatty models.
  prefs: []
  type: TYPE_NORMAL
- en: '`If you don''t know the answer, respond with the following format – My sincere
    apologies, I''m not aware of the answer` can aid in avoiding any form of hallucination
    from chatty Claude models. Guardrails for the topics can also be added such that
    Claude doesn’t respond to unwanted inputs. This concept will also be discussed
    in [*Chapter 12*](B22045_12.xhtml#_idTextAnchor226).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting Mistral models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to other models, when working with Mistral models, crafting well-designed
    prompts is crucial to obtaining high-quality and relevant outputs. Here are some
    key points to keep in mind while designing the prompts for Mistral models:'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly define the task or objective you want the model to accomplish through
    the prompt. Are you looking for text classification, summarization, personalization,
    or something else?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide relevant context, examples, or background information to ground the
    model’s understanding before stating the core prompt. Context helps the model
    better comprehend the prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use clear formatting and delimiters such as `#`, `###`, or `<<< >>>` to separate
    different sections of the prompt, such as instructions, examples, and the main
    query. This enhances the prompt’s structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When possible, demonstrate the desired output through examples in a *few-shot*
    learning style. Showing examples guides the model toward the expected format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the role the model should take on, such as a customer service agent
    or technical writer. Defining a persona makes responses more tailored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For an open-ended generation, provide clear instructions on the desired output
    length and structure through numeric targets such as word counts or the number
    of sentences/paragraphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask the model to include confidence scores or assessments when generating outputs
    to gauge its certainty levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider chaining multiple Mistral models in a sequence, where the output from
    one model feeds into the next for enhanced capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test and iterate on prompt designs through evaluations to find optimal prompting
    strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 3**.13* shows the Mixtral 8x7B Instruct model being invoked in Amazon
    Bedrock Playground.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that the `<s>` and `</s>` tokens are utilized to denote the `[INST]`
    and `[/INST]` strings tell the model that the content enclosed between them constitutes
    instructions that the model should adhere to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Prompting the Mixtral 8x7B Instruct model](img/B22045_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Prompting the Mixtral 8x7B Instruct model
  prefs: []
  type: TYPE_NORMAL
- en: The key is to carefully structure prompts with clear context, examples, instructions,
    and formatting to steer Mistral models to generate high-quality, tailored outputs
    matching your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt guidance for Amazon Titan text models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we learned in [*Chapter 1*](B22045_01.xhtml#_idTextAnchor014), Amazon Titan
    text models are well suited for a plethora of use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Dialog and roleplay systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization and Q&A
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata extraction and analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG (This will be covered in detail in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code generation approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text and content generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When generating text outputs with the model, it is often recommended to provide
    clear instructions on the desired output length and structure to get optimal results.
    Here are some additional tips for Titan text models:'
  prefs: []
  type: TYPE_NORMAL
- en: Focus prompts on concise, directed questions to get targeted answers by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systems perform best on single sentences or short paragraphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For longer inputs, place instructions at the end to guide high-quality responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding explicit instructions in the prompt produces more tailored results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify an exact number of words, sentences, bullet points, or paragraphs you
    want the AI to generate in the prompt. Providing a numerical range (for example,
    100-200 words) can also work well. This gives the model a clear target to aim
    for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid vague instructions such as `keep it short` or `summarize briefly`. These
    are open to interpretation by the AI. Precise numbers remove ambiguity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word count alone may not sufficiently guide output length as sentence lengths
    can vary. Specifying the number of sentences/paragraphs provides more robust control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model seems unable to generate a quality response for a prompt, program
    it to default to a message such as `Unsure about answer` rather than attempt to
    force a poor response. Here’s an example of such a prompt: `Tell me about Quantum
    Computing. Respond with Unsure about the answer or I don''t know in case you are
    not sure about the question` `being asked`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When relevant, provide context paragraphs for the AI to reference before asking
    a question. This provides knowledge for an informed response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test different output length instructions to find the right balance between
    conciseness and adequate detail for your use case. Err on the side of more specificity
    with numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can go back to the *Amazon Titan FMs* section in [*Chapter 1*](B22045_01.xhtml#_idTextAnchor014)
    if you wish to look at example prompts and responses in Titan models.
  prefs: []
  type: TYPE_NORMAL
- en: AI21 Labs – instruct models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI21 Labs models work very well with languages other than English, such as
    Spanish, French, German, Portuguese, Italian, and Dutch. The model is proficient
    in text summarization, text generation, and Q&A tasks. In this section, we will
    walk through a few key concepts to be inculcated with AI21 models available via
    Amazon Bedrock:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output length**: To generate desirable responses from AI12 models, it is
    advisable to specify the output length – that is, the number of paragraphs, items,
    and so on or an approximation of the same – instead of using words/characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provide short yet detailed task descriptions**: Craft clear, detailed task
    descriptions to minimize ambiguity. AI21 models excel at following precise instructions
    for even complex jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no more than x statements`. It is always recommended to state requirements
    directly and affirmatively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Instruction:` header to clarify the prompt. Separate prompt sections with
    newlines to highlight distinct pieces and for readability purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate multiple prompting patterns**: Try both zero-shot and few-shot learning.
    Pick an ideal approach for your use case. For instance, as depicted in the zero-shot
    and few-shot examples shown in this chapter, depending on the use case under consideration,
    you may initiate by providing zero examples and determining the response, and
    simultaneously compare the response generated from the model after providing certain
    examples to guide the output. In some cases, there may not be a need to provide
    a ton of examples if the model can generate a desirable response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 3**.14* depicts a product description summarization example from AI21
    Jurassic-2 Ultra within Amazon Bedrock:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Prompting the AI21 Jurassic-2 Ultra model](img/B22045_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Prompting the AI21 Jurassic-2 Ultra model
  prefs: []
  type: TYPE_NORMAL
- en: Further details on prompt engineering and design with AI21 models, along with
    examples, can be found at [https://docs.ai21.com/docs/prompt-engineering](https://docs.ai21.com/docs/prompt-engineering).
  prefs: []
  type: TYPE_NORMAL
- en: Prompting Meta Llama models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to any other LLM, effective prompting is essential for getting the
    most out of Llama models. Since the following is standard prompt guidance for
    any LLM, we will cover some of the best practices here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clarity** **and specificity**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that your prompts are clear, concise, and unambiguous
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide sufficient context and details to guide the model toward the desired
    output
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use precise language and avoid vague or open-ended statements
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structure** **and formatting**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organize your prompts logically and structure them in a way that aligns with
    the desired output format
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize formatting elements such as bullet points, numbered lists, or headings
    to enhance readability and comprehension
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider providing examples or templates to illustrate the expected format of
    the output
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task framing**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame your prompts as specific tasks or instructions for the model to follow
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly specify the desired action, such as summarizing, generating, or analyzing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide context about the intended use case or audience for the output
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative refinement**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting is an iterative process, and you may need to refine your prompts based
    on the model’s responses
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the output and identify areas for improvement or clarification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporate feedback and adjust the prompt accordingly to steer the model toward
    better results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning** **and customization**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the possibility of fine-tuning the Llama model on domain-specific data
    or examples
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Customize the model’s behavior and outputs by incorporating specific instructions
    or constraints in the prompt
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage techniques such as prompting with few-shot examples or demonstrations
    to improve performance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical and** **safety considerations**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be mindful of potential biases or harmful outputs that the model could generate
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporate explicit instructions or filters to mitigate risks and ensure the
    model’s responses align with ethical and safety guidelines
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor and evaluate the model’s outputs for any concerning or inappropriate
    content
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Llama models also consider special kinds of tokens. For Llama 3, the following
    tokens are used:'
  prefs: []
  type: TYPE_NORMAL
- en: The `<|begin_of_text|>` token represents the BOS token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<|eot_id|>` token indicates the end of the current turn or message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<|start_header_id|>{role}<|end_header_id|>` token encloses the role for
    a particular message, which can be either **system**, **user**, or **assistant**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<|end_of_text|>` token is equivalent to the EOS token. Upon generating
    this token, Llama 3 will stop producing any further tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more details on prompt formats, go to [https://llama.meta.com/docs/model-cards-and-prompt-formats](https://llama.meta.com/docs/model-cards-and-prompt-formats):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Prompting the Llama2 Chat 70B model](img/B22045_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Prompting the Llama2 Chat 70B model
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 3**.15*, the Llama 2 Chat 70B model is being invoked in
    Amazon Bedrock Playground.
  prefs: []
  type: TYPE_NORMAL
- en: The `[INST]` and `[/INST]` strings tell the model that the content enclosed
    between them constitutes instructions that the model should adhere to.
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to learn about the different examples and templates for invoking
    various models, including any new models being added to Amazon Bedrock, go to
    [https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html).
  prefs: []
  type: TYPE_NORMAL
- en: Prompt guidance for Stability AI – Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Stable Diffusion models (along with Amazon Titan image models) have been gaining
    popularity in image generation use cases. Here are some key tips for crafting
    effective prompts when using Stability AI’s Stable Diffusion for image generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`A photo of a cat` or `An illustration of a robot`. Being more specific usually
    produces better results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in impressionist style` or `a cartoon drawing of`. Styles help steer the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cat:1.5, sitting:1.2, couch:1`. Higher weights make elements more prominent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-`, improves quality by excluding unwanted elements. For example, if you provide
    `Cars racing on racetrack` as a prompt and give `red car` as a negative prompt,
    it will exclude red cars from the image’s output, as shown in *Figure 3**.16*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Negative prompt example](img/B22045_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Negative prompt example
  prefs: []
  type: TYPE_NORMAL
- en: '**Be detailed and specific**: Using more descriptive and distinctive words,
    rather than general terms, produces more tailored results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--ar`, `--v`, and `--n` control the aspect ratio, vividness, and level of
    detail, respectively. Tweak them to refine the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Providing additional descriptive details always aids the model in being more
    performant. The following are examples of such aspects:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the medium (painting, drawing, CGI, and so on)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Define colors used
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe lighting and shadows
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include the artist’s name if mimicking a style
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mention the website if you’re reproducing a specific image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add any other descriptive remarks or adjectives
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the desired resolution if needed for print or digital use
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: '`Portrait photo of an Indian old warrior chief, tribal panther make up, front
    profile, looking straight into the camera, serious eyes, 50mm portrait photography,
    hard rim lighting photography–beta –ar` `2:3 –beta`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output response from** **SDXL 1.0**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Image generation output from SDXL 1.0](img/B22045_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Image generation output from SDXL 1.0
  prefs: []
  type: TYPE_NORMAL
- en: 'When prompted with the same input, the output response from the Titan Image
    Generator G1 model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Image generation output from Titan Image Generator](img/B22045_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – Image generation output from Titan Image Generator
  prefs: []
  type: TYPE_NORMAL
- en: Providing this level of detail and context will help produce a more accurate
    image that matches your vision. Adjust prompts iteratively to refine results.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be wondering why the same prompt generated different outputs from
    two models. The reason for this is that SDXL is trained on a different set of
    data than Titan Image Generator, so the output you will see from these models
    will differ. Think of it this way: SDXL and Titan are two people, who learn from
    two different books for an exam. During the exam, when they were asked the same
    question, they would have two different viewpoints, and their answer would be
    based on the books they read.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are trying these prompts in your environment, you might also notice another
    thing. The output image that you are seeing in your environment might differ from
    the one shown here, even if you provide the same prompts. The reason for this
    is to do with the added degree of randomness. These models will generate output
    based on the inference parameters, such as *Prompt Strength* and *Seed*. We will
    cover these parameters in detail in [*Chapter 9*](B22045_09.xhtml#_idTextAnchor171).
    However, in short, prompt strength controls how you want the model output to be
    influenced by the prompt, and the seed is a way to randomize an output image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a good understanding of prompt guidance for FMs provided by
    Amazon Bedrock, in the next section, we will try to sum up some of the key principles
    and techniques you must follow when it comes to prompt engineering in various
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding best practices in prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To summarize, when crafting prompts, you must adhere to the following key principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '`What are the use cases of renewable resources? List 5` `key points`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Language emphasis*: Using simple flowing language with coherent sentences
    assists in crafting a better prompt and avoiding isolated phrases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Get into the model’s head*: Craft prompts to nudge it toward helpful behaviors.
    Think of it as someone who has all the right answers but only for correctly articulated
    questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Summarize the chapter in` `500 words`.*   *Provide example responses*: Adding
    some example responses within the prompt with the expected output can refine the
    responses more desirably – for example, `Summarize this chapter in one paragraph
    (1000 characters): [New study shows decreasing activity in region X leads to impairment.]`.
    Surrounding the example response in brackets indicates the model adheres to the
    guidelines set by the user while responding in the desired format.*   *Add constraints*:
    Constraining prompt responses by format, additional information inclusion, length,
    and more can lead to more controlled output.*   **Strike the right detail balance**:
    Too little detail fails to guide the model adequately while excessive verbosity
    limits creative flourishes. Distill prompts down to concise essence:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Complex* *task handling*:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: FMs can hallucinate when dealing with complex tasks. It is advisable to break
    down the complex task into subtasks or even consider splitting complex tasks into
    multiple prompts.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide emphasis by using keywords to ask the model to think step-by-step or
    provide logical reasoning as it is crafting the output. Provide some key examples
    in the input for complex tasks.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rinse, lather, repeat*: Iteratively break down and try different prompts to
    optimize model responses for your goals. Continue adjusting while testing and
    experimenting to achieve the desired results.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Continuous evaluation*: Iteratively reviewing the model’s responses to provide
    the desired quality is a must when it comes to handling different use cases and
    complex scenarios.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts unlock Generative AI’s capabilities but require thoughtfulness to construct
    properly. Learn your target model’s strengths and limitations, iterate carefully
    on prompt phrasings, and appreciate these systems’ ever-evolving nature. Wield
    prompts judiciously and enjoy the fruits of AI’s burgeoning creativity!
  prefs: []
  type: TYPE_NORMAL
- en: Complexity arises in structuring the right prompts to handle intricate goals.
    But when done well, prompts unlock AI like a skeleton key, opening doors to breathtaking
    new generative capabilities. The prompt contains the potential; our role is to
    shape and guide it with thoughtful prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about several prompt engineering techniques to gain
    a deeper understanding of prompting patterns and uncovered insights while considering
    examples of said prompting patterns. Then, we dived into prompt guidance with
    Amazon Bedrock models for Anthropic Claude, AI21 Labs, Amazon Titan, and Stability
    AI’s Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we summarized the practical approach to prompt guidance while looking
    at Amazon Bedrock models that can be applied to various use cases. Through various
    examples, we learned how to craft the most effective prompts.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a good understanding of the importance of prompt
    engineering. Furthermore, you should be able to analyze various prompt techniques
    and best practices involved in prompt engineering in the context of building Generative
    AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn how to customize a model using fine-tuning
    and continued pretraining techniques. We will delve into how fine-tuning works,
    look at various APIs, analyze the results, and perform inference on our fine-tuned
    model.
  prefs: []
  type: TYPE_NORMAL
