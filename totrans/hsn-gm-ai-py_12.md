# 全部关于 Rainbow DQN

在本书中，我们已经学习了各种 **强化学习**（**RL**）的线索是如何结合形成现代强化学习，然后通过加入 **深度学习**（**DL**）而发展到 **深度强化学习**（**DRL**）。像大多数其他从这种融合中产生的专业领域一样，我们现在看到的是回到针对特定环境类别的专门方法。我们在涵盖 **策略梯度**（**PG**）方法和它专门化的连续控制环境章节中开始看到这一点。这一面的反面是更典型的周期性游戏环境，它具有某种形式的离散控制机制。这些环境通常使用 DQN 表现更好，但问题变成了关于 DQN 的。好吧，在这一章中，我们将探讨聪明人如何通过引入 Rainbow DQN 来解决这个问题。

在这一章中，我们将介绍 Rainbow DQN，并理解它试图解决的问题以及它提供的解决方案。由于我们已经在其他章节中涵盖了大多数这些解决方案，我们将通过首先查看用于更好地理解采样和探索的噪声或模糊网络来介绍使 Rainbow 不同的少数几个解决方案。然后，我们将探讨分布式强化学习以及它如何通过预测分布来提高价值估计，这与我们的策略网络从 PG 类似，将这些改进和其他改进结合到 Rainbow 中，并观察其表现如何。最后，我们将探讨分层 DQN 以理解任务和子任务，为可能的训练环境提供计划，并计划在以后使用这种高级 DQN 在更复杂的环境中。

本章我们将涵盖的主要主题包括：

+   Rainbow – 结合深度强化学习中的改进

+   使用 TensorBoard

+   介绍分布式强化学习

+   理解噪声网络

+   揭示 Rainbow DQN

一定要复习你对概率、统计学、随机过程以及/或贝叶斯推理和变分推理方法的理解。这应该是你在前面的章节中就已经在做的事情，但随着我们转向 DRL 以及 DL 的更高级内容，这些知识现在将变得至关重要。

在下一节中，我们将探讨理解 Rainbow DQN 是什么以及为什么需要它。

# Rainbow – 结合深度强化学习中的改进

2017年10月，DeepMind发表的介绍Rainbow DQN的论文，*Rainbow：结合深度强化学习的改进*，旨在解决DQN的几个不足。DQN是由DeepMind的同一组人引入的，由David Silver领导，旨在在Atari游戏中打败人类。然而，正如我们在几个章节中学到的，虽然该算法具有开创性，但它确实存在一些不足。其中一些我们已经通过DDQN和经验回放等进步解决了。为了理解Rainbow所包含的所有内容，让我们看看它对RL/DRL的主要贡献：

+   **DQN**：当然，这是核心算法，我们到现在应该已经很好地理解了。我们已经在[第6章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)，“深入探索DQN”中介绍了DQN。

+   **双DQN**：这不要与DDQN或对抗DQN混淆。再次强调，我们已经在[第7章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)，“深入探索DDQN”中介绍过这一点。

+   **优先经验回放**：这是我们已经在[第6章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)，“深入探索DQN”中介绍过的另一个改进。

+   **对抗网络架构**（**DDQN**）：这是我们之前已经介绍过的元素，并在之前提到过。

+   **多步回报**：这是我们计算TD lambda和期望或优势估计的方法。

+   **分布式强化学习**：这试图理解价值分布，与我们的策略模型在演员-评论家中的作用类似，但在这个情况下，我们使用的是价值。这种增强将在本章的单独部分进行介绍。

+   **噪声网络**：噪声或模糊网络是深度学习网络，它们学习平衡权重参数的分布，而不是网络权重的实际判别值。这些高级深度学习网络已被用于更好地理解数据分布及其数据，通过将使用的权重建模为分布来实现。我们将在本章的后续部分介绍这些高级深度学习网络。

我们在之前的章节中已经介绍了许多这些改进，除了分布式强化学习和噪声网络。我们将在本章中介绍这两个改进，从未来的部分开始介绍分布式强化学习。但在我们这样做之前，让我们退一步，在下一节中通过TensorBoard提高我们的日志输出能力。

# 使用TensorBoard

在本书的这个阶段，我们需要超越构建玩具示例，转向构建你可以用来在未来训练自己的代理的模块或框架。实际上，我们将使用本章中的代码来训练代理解决我们在后续章节中提出的其他挑战环境。这意味着我们需要一种更通用的方式来记录我们的进度，最好是记录到我们以后可以查看的日志文件中。由于构建这样的框架对于整个机器学习来说是一个如此常见的任务，因此Google开发了一个非常有用的日志框架，称为TensorBoard。TensorBoard最初是作为我们之前提到的其他深度学习框架的一个子集开发的。幸运的是，对于我们来说，PyTorch包含了一个支持将日志记录到TensorBoard的扩展。因此，在本节中，我们将设置和安装TensorBoard，用作日志记录和绘图平台。

在下一个练习中，我们将为与PyTorch一起使用安装TensorBoard。如果你只使用过PyTorch，你可能需要遵循这些说明。对于那些之前已经安装过TensorFlow的你们，你们已经可以开始了，可以跳过这个练习：

1.  打开一个Anaconda或Python shell并切换到你的虚拟环境。你很可能已经有一个打开的环境了。你可能想为TensorBoard创建一个完全独立的干净虚拟环境。这可以最小化在该环境中可能或将会破坏的代码量。TensorBoard是一个服务器应用程序，最好将其视为服务器应用程序，这意味着它运行的应该是一个纯净的环境。

1.  然后，我们需要使用以下命令从你的Anaconda或Python shell中安装TensorBoard：

[PRE0]

这将在你的虚拟环境中安装TensorBoard。

1.  接下来，为了避免可能出现的依赖性问题，我们需要运行以下命令：

[PRE1]

1.  安装完成后，我们可以使用以下命令运行TensorBoard：

[PRE2]

1.  这将在默认的`6006`端口启动一个服务器应用程序，并从名为`runs`的文件夹中拉取日志，这是PyTorch默认使用的。如果你需要自定义端口或输入日志文件夹，可以使用以下命令选项：

[PRE3]

1.  TensorBoard是一个具有网络界面的服务器应用程序。这对于我们总是希望运行并从某些输出日志或其他数据处理文件夹中提取数据的应用程序来说是很常见的。以下图显示了在shell中运行的TensorBoard：

![图片](img/73b2b78f-0a04-403d-a8b0-af0d601d02fa.png)

TensorBoard启动中

1.  当TB首次运行时，它将输出你可以在浏览器中使用的地址以查看界面。按照图示复制URL并粘贴到你喜欢的网页浏览器中。请注意，如果你访问页面有困难，可能是一个绑定问题，意味着你的电脑可能阻止了这种访问。你可以尝试使用`localhost:6006`或`127.0.0.1:6006`地址，或者为TB使用绑定所有选项。

1.  当你的浏览器打开，并且假设你之前没有运行过TensorBoard或者没有在数据文件夹中放置输出，你将看到如下内容：

![图片](img/75b8dfd3-8c5d-4cf0-ba31-7cd6e6ebf197.png)

空的TensorBoard运行

当第一次运行TB时，需要注意的一个重要点是它正在使用正确的数据文件夹。你应该会看到`数据位置：runs`标签，指明将包含日志输出的文件夹。

在设置好TB之后，我们现在可以继续探索使Rainbow DQN比vanilla DQN好得多的创新之处。虽然我们已经在使用和探索了一些这些创新，但现在我们可以继续理解Rainbow中包含的剩余两个创新。我们将在下一节中从分布式RL开始。

# 分布式强化学习介绍

分布式强化学习这个名字可能有点误导，可能会让人联想到多层分布式网络中的DQN一起工作。好吧，这确实可能是分布式RL的描述，但分布RL是我们试图找到DQN预测的价值分布，也就是说，不仅仅是找到最大值或平均值，而是理解生成它的数据分布。这与PG方法的直觉和目的非常相似。我们通过将已知的或先前预测的分布投影到未来的或未来预测的分布来实现这一点。

这确实需要我们回顾一个代码示例，所以打开`Chapter_10_QRDQN.py`并跟随下一个练习：

1.  整个代码列表太长了，无法在这里全部展示，所以我们将查看重要的部分。我们将从**QRDQN**或**Quantile Regressive DQN**开始。分位数回归是一种从观察中预测分布的技术。QRDQN的列表如下：

[PRE4]

1.  这段代码的大部分看起来和之前一样，但需要注意的一点是`qvalues`表示一个Q值（状态-动作），而不是像PG方法中我们看到的那样表示Q策略值。

1.  接下来，我们将滚动到`projection_distribution`函数，如下所示：

[PRE5]

1.  这段代码相当数学化，并且超出了本书的范围。它本质上只是提取它认为的Q值的分布。

1.  之后，我们可以看到我们两个模型的构建，这表明我们在这里正在使用以下代码构建一个DDQN模型：

[PRE6]

1.  之后，我们得到使用`computer_td_loss`函数计算TD损失，如下所示：

[PRE7]

1.  这个损失计算函数与我们之前看到的其他DQN实现类似，尽管这个实现确实暴露了一些曲折。大多数曲折都是通过使用**分位数回归**（**QR**）引入的。QR本质上是通过使用分位数或分位数来预测分布，即概率的切片，以迭代地确定预测分布。然后使用这个预测分布来确定网络损失，并通过深度学习网络进行训练。如果你向上滚动，你可以注意到引入了三个新的超参数，允许我们调整搜索。这里显示的新值允许我们定义迭代次数`num_quants`和搜索范围`Vmin`和`Vmax`：

[PRE8]

1.  最后，我们可以通过滚动到代码的底部并在这里查看，来了解训练代码是如何运行的：

[PRE9]

1.  我们在第六章和第七章中已经看到过非常类似的代码，当时我们之前查看DQN时，所以这里不会进行回顾。相反，如果你需要的话，再次熟悉一下DQN模型。注意它与PG方法之间的区别。当你准备好时，像平常一样运行代码。运行样本的输出如下所示：

![图片](img/973d2e7f-a4f7-427c-a407-f0b192ec7e6a.png)

Chapter_10_QRDQN.py的示例输出

从这个样本生成的输出只是一个提醒，我们还有更多信息被输出到日志文件夹中。要查看该日志文件夹，我们需要再次运行TensorBoard，我们将在下一节中这样做。

# 回到TensorBoard

在上一个练习的样本仍在运行的情况下，我们想回到TensorBoard，并现在查看样本运行的输出。为此，打开一个新的Python/Anaconda命令行窗口，并按照下一个练习进行操作：

1.  打开与你在运行之前的练习代码示例相同的文件夹的shell。切换到你的虚拟环境或专门用于TB的特殊环境，然后运行以下命令以启动过程：

[PRE10]

1.  这将在当前文件夹中启动TB，使用该`runs`文件夹作为数据转储目录。样本运行一段时间后，当你现在访问TB网络界面时，你可能看到以下类似的内容：

![图片](img/14068085-91c6-41a9-bd47-290e50f4f0c6.png)

TensorBoard输出来自Chapter_10_QRDQN.py

1.  将截图所示的**平滑**控制调高，以查看数据的可视化趋势。看到数据的一般趋势允许你推断你的智能体何时可能完全训练。

1.  现在，我们需要回到`Chapter_10_QRDQN.py`示例代码，看看我们是如何生成这些输出数据的。首先，注意新的`import`和声明一个新变量`writer`，它是从`torch.utils.tensorboard`导入的`SummaryWriter`类，如下所示：

[PRE11]

1.  `writer` 对象用于输出到在 `run` 文件夹中构建的日志文件。现在每次我们运行这个示例代码块时，这个 writer 都会输出到 `run` 文件夹。你可以通过将目录输入到 `SummaryWriter` 构造函数中来改变这种行为。

1.  接下来，向下滚动到修订的 `plot` 函数。这个函数，如这里所示，现在生成我们可以用 TB 可视化的日志输出：

[PRE12]

1.  这个更新的代码块现在使用 TB `writer` 而不是我们之前使用的 `matplotlib plot` 输出结果。每次调用 `writer.add_scalar` 都会将一个值添加到我们之前可视化的数据图中。有许多其他你可以调用的函数来添加许多不同类型的输出。考虑到我们生成令人印象深刻输出的容易程度，你可能永远不会再次需要使用 `matplotlib`。

1.  返回你的 TB 网页界面，并观察持续的训练输出。

这个代码示例可能需要一些调整才能将智能体调整到成功的策略。然而，你现在有更多更强大的工具 TensorBoard 来帮助你做到这一点。在下一节中，我们将探讨 Rainbow 引入的最后一个改进：噪声网络。

# 理解噪声网络

噪声网络并不是指那些需要知道一切的网络——那些会是好奇的网络。相反，噪声网络引入了噪声的概念，用于通过网络预测输出时的权重。因此，我们不再只有一个标量值来表示感知器中的权重，我们现在认为权重是从某种形式的分布中抽取的。显然，我们在这里有一个共同的主题，那就是从处理单个标量值作为数字到更好的描述为数据分布。如果你研究过贝叶斯或变分推理的主题，你可能会具体理解这个概念。

对于没有那个背景的人来说，让我们看看以下图中分布可能是什么：

![图片](img/08f99ae7-4012-4212-8ed5-be3f45e89823.png)

不同数据分布的示例

前面图表的来源是Akshay Sharma的一篇博客文章（[https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec](https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec)）。图表中显示的是各种知名数据分布的采样模式。基本统计学假设所有数据总是均匀或正态分布的。在统计学中，你将了解到其他分布，你将使用它们来定义各种方差测试或拟合测试，如卡方或学生t分布。如果你在计算机程序中随机采样过数字，你很可能非常熟悉均匀分布。大多数计算机程序总是假设均匀分布，这在某种程度上是机器学习中的问题，因此转向更好地理解真实数据或动作/事件是如何分布的。

变分推理或定量风险分析是一种技术，我们使用工程、经济学或其他学科的典型方程，并假设它们的输入是分布而不是判别值。因此，使用数据的分布作为输入，我们假设我们的输出也是某种形式的分布。这个分布可以用来评估风险或奖励的术语。

是时候进行另一个练习了，所以打开`Chapter_10_NDQN.py`并遵循下一个练习：

1.  这是一个很大的例子，所以我们只会关注重要的部分。让我们先向下滚动，看看这里的`NoisyDQN`类：

[PRE13]

1.  这与我们的之前的DQN示例非常相似，但有一个关键的区别：添加了一个新的专业深度学习网络层类型，称为`NoisyLinear`**。**

1.  继续向下滚动，我们可以看到`td_compute_loss`函数已更新以处理噪声或模糊层：

[PRE14]

1.  这个函数与我们之前的vanilla DQN示例非常相似，这是因为所有的工作/差异都在新的噪声层中，我们将在稍后讨论。

1.  滚动回`NoisyLinear`类的定义，如下所示：

[PRE15]

1.  `NoisyLinear`类是一个使用正态分布来定义层中每个权重的层。这个分布被假定为正态分布，这意味着它由均值、mu和标准差、sigma定义。因此，如果我们之前在一个层中假设有100个权重，我们现在将有两个值（mu和sigma）来定义权重的采样方式。反过来，mu和sigma的值也成为了我们在网络上训练的值。

在其他框架中，将应用变分权重到层中的能力构建进去相当困难，通常需要更多的代码。幸运的是，这是PyTorch的一个优势，它增强了一个内置的概率框架，旨在预测和处理分布数据。

1.  不同的分布可能使用不同的描述性值来定义它们。正态分布或高斯分布由mu和sigma定义，而均匀分布通常只由最小/最大值定义，而三角形分布则是由最小/最大值和峰值定义的。我们几乎总是更喜欢使用正态分布来描述大多数自然事件。

1.  滚动到训练代码，你会看到几乎与上一个练习相同的代码，只有一个关键的区别。在探索中，我们不是使用epsilon，而是引入了一个称为beta的术语。beta成为我们的实际探索术语，并取代了epsilon。

1.  按照正常方式运行应用程序，并在TensorBoard中观察训练，如图中所示截图：

![图片](img/79eca78e-3c2f-490c-aa79-4dede0817696.png)

从样本`Chapter_10_NDQN.py`生成的TensorBoard输出

箭头及其指向的方向表示我们希望我们的代理/算法移动的方向。我们通过将**平滑**参数增加到最大值，即.99，来获得这些趋势图。通过这样做，更容易看到一般或中值趋势。

在我们继续到Rainbow之前，我们需要重新审视一下在使用噪声网络时如何管理探索。这也有助于解释新beta参数对于*z*的使用。

# 用于探索和重要性采样的噪声网络

使用噪声网络也会在我们的动作预测中引入模糊性。也就是说，由于网络的权重现在是从一个分布中抽取的，这也意味着它们正在变得分布化。我们也可以说它们是随机的，而随机性是由一个分布定义的，基本上意味着相同的输入可能会产生两个完全不同的结果，这意味着我们不能再仅仅取最大或最佳动作，因为现在这只是一个模糊的概念。相反，我们需要一种方法来减小我们用于权重的采样分布的大小，因此减少我们在代理选择的动作中的不确定性。

减小分布的大小基本上等同于减少该数据的不确定性。这是数据科学和机器学习的一个基石。

我们通过引入一个称为beta的因子来减少这种不确定性，这个因子会随着时间的推移而增加。这种增加与epsilon不同，只是方向相反。让我们通过重新打开`Chapter_10_NDQN.py`文件并跟随这里的练习来看看这看起来像什么代码：

1.  我们可以通过查看主要代码来了解beta是如何定义的：

[PRE16]

1.  这个设置和方程再次与之前定义的epsilon类似。这里的区别在于beta是逐渐增加的。

1.  Beta用于纠正正在训练的权重，从而引入了重要性采样的概念。重要性采样是关于我们在纠正/采样权重之前对它们的重视程度。Beta因此成为重要性采样因子，其中1.0的值表示100%重要，而0表示没有重要性。

1.  打开同一项目中 `common` 文件夹中的 `replay_buffer.py` 文件。向下滚动到 `sample` 函数，并注意代码，如图所示：

[PRE17]

1.  `sample` 函数是我们使用的 `PrioritizedExperienceReplay` 类的一部分，用于存储经验。除了意识到它按优先级排序经验之外，我们不需要审查这个类的全部内容。

1.  最后，回到示例代码并回顾一下绘图函数。现在TensorBoard中生成我们的beta绘图的那一行看起来是这样的：

[PRE18]

1.  到目前为止，你可以回顾更多的代码，或者尝试调整新的超参数，然后再继续。

这样，我们就完成了对噪声和非噪声网络探索的考察。我们看到了如何引入分布作为我们深度学习网络权重的使用。然后，我们看到为了补偿这一点，我们需要引入一个新的训练参数，beta。在下一节中，我们将看到所有这些部分如何在Rainbow DQN中结合在一起。

# 揭示Rainbow DQN

《Rainbow：结合深度强化学习中的改进》一书的作者Matteo Hessel ([https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+M](https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+M))，与其他一些最先进的DRL模型进行了多次比较，其中许多我们已经看过。他们使用标准的2D经典Atari游戏进行了这些比较，并取得了令人印象深刻的结果。Rainbow DQN优于所有当前最先进的算法。在论文中，他们使用了熟悉的经典Atari环境。这是可以的，因为DeepMind为该环境有大量数据，可以与比较的模型相关联。然而，许多人观察到，论文缺乏PG方法，如PPO的比较。当然，PPO是OpenAI的进步，它可能被Google DeepMind视为侵权，或者只是想通过完全不进行比较来避免认可。不幸的是，这也表明，即使是像DRL这样高度智力追求的东西也无法摆脱政治。

PPO等方法已被用于克服DRL当前的一些最大挑战。实际上，PPO在Unity Obstacle Tower Challenge中赢得了10万美元的大奖。因此，你很快就不应该低估PG方法。

根据前面的绘图，我们应该对Rainbow抱有很高的期望。所以，让我们打开 `Chapter_10_Rainbow.py` 文件，并跟随下一个练习：

1.  到现在为止，这个例子应该已经很熟悉了，我们将限制自己只查看差异，从下面这里 `RainbowDQN` 类的主要实现开始：

[PRE19]

1.  上述代码定义了Rainbow DQN的网络结构。这个网络有点复杂，所以我们已经将主要元素放在这里的图中：

![图片](img/5542d332-f67b-4fe3-9c44-113580d656d9.png)

Rainbow网络架构

1.  如果你查看`init`和`forward`函数，你应该能够看到这个图是如何构建的。

1.  我们还不能离开前面的代码，我们需要再次审查act函数，如下所示：

[PRE20]

1.  `act`函数展示了智能体如何选择动作。我们在这里已经细化了动作选择策略，现在使用`Vmin`、`Vmax`和`num_atoms`的值。我们将这些值作为输入传递给`torch.linspace`，以此创建一个从`Vmin`到`Vmax`的离散分布，步长由`num_atoms`定义。这会输出最小/最大范围内的缩放值，然后这些值乘以`forward`函数输出的原始分布`dist`。将`forward`函数返回的分布与由`torch.linspace`生成的分布相乘，这是一种缩放类型。

你可能已经注意到，超参数`num_atoms`、`Vmin`和`Vmax`现在在调整模型参数时具有双重作用。这通常是一件坏事。也就是说，你总是希望定义的超参数具有单一目的。

1.  接下来，我们将向下滚动并查看`projection_distribution`函数中的差异。记住这个函数是执行寻找分布的分布部分，而不是离散值：

[PRE21]

1.  这段代码与我们之前查看的量分回归代码有很大不同。这里的主要区别是使用了PyTorch库，而之前代码更底层。使用库会使代码更加冗长，但希望你现在可以欣赏到代码的说明性比之前的示例更清晰。

1.  这里需要注意的是，我们继续使用`epsilon`进行探索，如下面的代码所示：

[PRE22]

1.  按照常规方式运行示例，并观察输出。

请记住，由于这个示例缺少优先级回放缓冲区，它无法成为完整的RainbowDQN实现。然而，它确实涵盖了80/20规则，实现优先级回放缓冲区被留作读者的练习。在我们跳转到观察训练的下一节时，让样本继续运行。

# 训练何时会失败？

对于任何新进入深度学习领域的人来说，尤其是对于深度强化学习，一个常见的问题就是何时知道你的模型是失败的，只是有点固执，或者永远都不会工作。这是一个在人工智能领域引起挫败感和焦虑的问题，常常让你想知道：“如果我让那个智能体再训练一天会怎样？”不幸的是，如果你和专家交谈，他们通常会说你只需要耐心，继续训练，但这可能是在那些挫败感的基础上。毕竟，如果你构建的东西永远没有希望做任何事情，你是在浪费时间精力去维持它吗？

许多人都面临的一个问题是，算法/模型越复杂，训练所需的时间就越长，除非你之前训练过它或者阅读过一篇使用完全相同模型的非常优秀的论文。即使使用完全相同的模型，环境也可能有所不同，可能更加复杂。考虑到所有这些因素，以及调整超参数的痛苦，真让人惊讶，为什么任何理智的人会想要在强化学习（DRL）领域工作。

作者主持了一个针对强化学习（RL）和深度强化学习（DRL）的深度学习Meetup支持小组。这个小组中经常讨论的一个话题是，深度学习研究人员如何保持他们的理智和/或降低他们的压力水平。如果你从事人工智能工作，你就会理解不断满足世界期望的必要性，这种期望是好事，但涉及投资者或缺乏耐心的老板时，也可能是一件坏事。

幸运的是，有了像TensorBoard这样的高级工具，我们可以深入了解代理是如何训练或希望如何训练的。

打开TensorBoard并遵循下一个练习，了解如何有效地诊断训练问题：

1.  以下截图显示了TB的输出：

![图片](img/eea7ff49-3611-4ef4-a3a5-4503e588c03a.png)

Rainbow DQN的TensorBoard输出

1.  从前面的截图可以看出，**平滑**已经提高到.99，我们可以看到训练失败了。记住，截图中的图表都有注释，以显示首选的方向。对于所有这些图表，情况并非如此。然而，不要假设如果图表的方向相反，它就一定是坏的——它不是。相反，任何移动都是一些训练活动的更好指标。这也是我们为什么要高度平滑这些图表的原因。

1.  常常决定未来训练性能的一个关键图是**损失**图。当损失减少时，代理在学习；当损失增加时，代理可能会忘记/困惑。如果损失保持不变，那么代理可能停滞不前，可能会困惑或陷入困境。这里展示的截图是对这一点的有益总结：

![图片](img/bc442dbe-4558-4574-a52b-73049901ac6a.png)

损失图总结

1.  上述截图显示了在超过50万次回合的理想训练过程中，对于这个环境，你可以预期训练的回合数是原来的两倍或三倍。一般来说，如果训练时间超过10%，无论是正向还是负向的移动，都应被视为失败。例如，如果你正在训练一个代理进行100万次迭代，那么你的10%窗口大约是10万次迭代。如果你的代理在某个图上持续训练或处于平坦状态，除了优势之外，在等于或大于10%窗口大小的期间，可能最好调整超参数并重新开始。

1.  再次强调，特别关注损失图，因为它提供了训练问题的最强指标。

1.  您可以通过重复运行样本相同的迭代次数来并排查看多个训练尝试的结果，如图中所示：

![](img/0cbc852e-2d96-4c24-ae03-0a8cd81e8af4.png)

同一图表上的多个训练输出示例

1.  停止当前样本，更改一些超参数，然后再次运行以查看前面的示例。

这些简单的规则可能会帮助您在构建/训练新或不同环境中的模型时避免挫败感。幸运的是，我们还有更多章节要学习，其中包括大量类似下一节中展示的练习。

# 练习

当涉及到在现实世界中工作时，您从这些练习中获得的经验可能意味着得到那份工作与保住那份工作的区别。作为一名程序员，您不仅需要理解某物是如何工作的；您是一个需要亲自动手并实际工作的机械师/工程师：

1.  调整`Chapter_10_QRDQN.py`的超参数，并查看这对训练有什么影响。

1.  调整`Chapter_10_NDQN.py`的超参数，并查看这对训练有什么影响。

1.  调整`Chapter_10_Rainbow.py`的超参数，并查看这对训练有什么影响。

1.  在另一个环境（如CartPole、FrozenLake或更复杂的环境如Atari）上运行和调整本章样本的超参数。如果您的电脑较旧且需要更努力地训练智能体，降低环境的复杂性也是有帮助的。

1.  本章还包括`Chapter_10_HDQN.py`和`Chapter_10_C51.py`示例中的分层DQNs和分类DQNs的示例代码。运行这些示例，审查代码，并自行研究这些示例为DRL带来的改进。

1.  添加从任何示例中保存/加载训练模型的功能。现在您可以使用训练好的模型来展示智能体玩游戏吗？

1.  添加将其他可能认为对训练智能体重要的训练值输出到TensorBoard的功能。

1.  将`NoisyLinear`层添加到`Chapter_10_QRDQN.py`示例中。示例中可能已经存在一些只是被注释掉的代码。

1.  将优先级回放缓冲区添加到`Chapter_10_Rainbow.py`示例中。您可以使用在`Chapter_10_NDQN.py`示例中找到的相同方法。

1.  TensorBoard允许您输出和可视化训练模型。使用TensorBoard从示例中输出训练模型。

显然，练习的数量已经增加，以反映您在DRL技能水平和/或兴趣上的增长。您当然不需要完成所有这些练习，但2-3个就足够了。在下一节中，我们将总结本章内容。

# 摘要

在本章中，我们特别关注了DeepMind在DRL领域的一项更先进的进展，称为Rainbow DQN。Rainbow在DQN的基础上结合了多项改进，这些改进显著提高了训练性能。由于我们已经涵盖了这些改进中的许多，我们只需要回顾一些新的进展。然而，在这样做之前，我们安装了TensorBoard作为调查训练性能的工具。然后，我们探讨了分布式RL的第一个进展以及如何通过理解采样分布来建模动作。继续探讨分布，我们接下来研究了有噪声的网络层——这些网络层没有单个权重，而是有描述每个权重的单个分布。基于这个例子，我们转向了Rainbow DQN，在我们的最后一个例子中完成，并快速讨论了何时确定智能体不可训练或表现停滞不前。

在下一章中，我们将从构建DRL算法/智能体转向使用Unity构建环境，并使用ML-Agents工具包在这些环境中构建智能体。
