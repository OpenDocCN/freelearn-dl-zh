- en: Building Our First Neural Network Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've had a quick refresher on Neural Networks, I thought that perhaps
    a good starting point, code-wise, would be for us to write a very simple neural
    network. We're not going to go crazy; we'll just lay the basic framework for a
    few functions so that you can get a good idea of what is behind the scenes of
    many of the APIs that you'll use. From start to finish, we'll develop this network
    application so that you are familiar with all the basic components that are contained
    in a neural network. This implementation is not perfect or all-encompassing, nor
    is it meant to be. As I mentioned, this will merely provide a framework for us
    to use in the rest of the book. This is a very basic neural network with the added
    functionality of being able to save and load networks and data. In any event,
    you will have a foundation from which to write your own neural network and change
    the world, should you so desire.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synapses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward propagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back propagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You would need to have Microsoft Visual Studio installed on the system.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see Code in Action: [http://bit.ly/2NYJa5G](http://bit.ly/2NYJa5G).
  prefs: []
  type: TYPE_NORMAL
- en: Our neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin by showing you an of what a simple neural network would look like,
    visually. It consists of an input layer with *2* inputs, a Hidden Layer with *3*
    neurons (sometimes called **nodes**), and a final output layer consisting of a
    single neuron. Of course, neural networks can consist of many more layers (and
    neurons per layer), and once you get into deep learning you will see much more
    of this, but for now this will suffice. Remember, each node, which is labeled
    as follows with an **N**, is an individual neuron – its own little processing
    brain, if you will:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a67995f1-ceca-4de8-8540-fbf25ab7843e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s break down the neural network into its three basic parts; inputs, Hidden
    Layers and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs**: This is the initial data for our network. Each input is a whose
    output to the Hidden Layer is the initial input value.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hidden Layers**: These are the heart and soul of our network, and all the
    magic happens. Neurons in this layer are assigned weights for each of their inputs.
    These weights start off randomized, and are adjusted as the network is trained
    so that the neuron''s output is closer to the expected result (if we are lucky).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outputs**: These are the output our network arrives at after it performs
    its calculations. The output in our simple case will be either true or false,
    on or off. The neurons are assigned a weight for each of their inputs, which comes
    from the previous Hidden Layer. Although it is typically common for there to be
    only a single output neuron, there''s absolutely nothing preventing you from having
    more, should you need or want more than one.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we train a neural network? Basically, we will provide the with a set
    of input data as well as the results we expect to see, which correspond to those
    inputs. That data is then run through the network until the network understands
    what we are looking for. We will train, test, train, test, train, test, on and
    on until our network understands our data (or doesn't, but that's a whole other
    conversation). We continue to do this until some designated stop condition is
    satisfied, such as an error rate threshold. Let's quickly cover some of the terminology
    we will use while training neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Back propagation**: After our data is run through the network, we to validate
    that data what we expect to be the correct output. We do this by propagating *backward* (hence
    backprop or back propagation) through each of the Hidden Layers of our network.
    The end result is that this adjusts the weights assigned to each of the neuron''s
    inputs in the Hidden Layers as well as our error rate.'
  prefs: []
  type: TYPE_NORMAL
- en: Each back propagation layer should, in a perfect world, make our network output
    closer to what we are expecting, and our error rate will get closer and closer
    to 0\. We may never get to an exact error rate of 0, so even though it may seem
    not much of a difference, an error rate of 0.0000001 could be more than acceptable
    to us.
  prefs: []
  type: TYPE_NORMAL
- en: '**Biases**: Biases allow us to modify our function so that we can generate
    better output for each neuron in our network. In short, a bias allows us to shift
    the activation function value to the left or the right. Changing the weight changes
    the steepness or vertical aspect of the Sigmoid.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Momentum**: Momentum simply adds a fraction of the previous weight update
    to the current one. Momentum is used to prevent the system from converging on
    a local minimum rather than the global minimum. High momentum can be used to help
    increase the speed of convergence of the system; however, you must be careful
    as setting this parameter too high can create a risk of overshooting the minimum,
    which will result in an unstable system. On the other hand, a momentum that is
    too low cannot reliably avoid local minima, and it can also really slow down the
    training of the system. So, getting this value correct is paramount for success
    and something you will spend a considerable amount of time doing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid function**: An activation function defines what each neuron''s output
    will be. A Sigmoid function is perhaps the most commonly used activation function.
    It converts the input into a value which lies between 0 and 1\. This function
    is used to generate our initial weights. A typical Sigmoid function will be able
    to accept an input value and, from that value, provide both an output value and
    a derivative.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate**: The learning rate will change the overall learning speed
    of the system by controlling the size of the weight and bias changes made to the
    network during the learning phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have this terminology behind us, let's start digging into the code.
    You should have downloaded the solution from the accompanying software provided
    for the book, and have it opened in Visual Studio. We use the Community Edition
    of Visual studio, but you may use whichever version you have.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to download the software, experiment with it, and embellish it if
    you need or want to. In your world, your neural network can be anything you like
    or need it to be, so make it happen. You have the source. Just because you see
    something one way doesn't make it gospel or written in stone! Learn from what
    these great open source contributors have provided for us! Remember, this neural
    network is meant only to give you some idea of the many things that you could
    do writing your own, as well as teach you some of the basics when it comes to
    a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start off by looking at some brief code snippets that will set the stage
    for the rest of the chapter. We'll start first with a little thing called a **synapse**,
    which connects one neuron to another. Next, we'll start coding exactly what an
    individual neuron is, and finally move into discussing forward and backward propagation
    and what that means to us. We'll show everything in the form of code snippets
    to make it easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Synapses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What is a synapse, you might ask? Simply put, it connects one neuron to another,
    as well as being a container to hold weight and weight delta values, depicted
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve already discussed what a neuron is, now it''s time to express it in
    code so developers like us can make more sense of it! As you can see, we have
    both our input and output synapses, our `Bias` and the `Bias Delta`, the `Gradient`,
    and the actual value of the neuron. The neuron calculates the weighted sum of
    its inputs, adds a bias, and then decides if the output should `''fire'' - ''be
    on''`- or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is our code for a basic forward propagation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To do `ForwardPropagation`, we basically sum all the inputs from each synapse
    and run the result through our Sigmoid function to get an output. The `CalculateValue`
    function does this for us.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Sigmoid function is an activation function and, as we previously, perhaps
    one of the most widely used today. Here''s what a Sigmoid function looks like
    (you do remember our section on activation functions, right?) Its sole purpose
    (very abstractly) is to bring in the values from the outside edges closer to 0
    and 1 without having to worry about values larger than this. This will prevent
    those values along the edge from running away on us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d0b3758-e073-4341-aa78-748b52735d03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'What does a `Sigmoid` function look like in C# code, you might ask? Just like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Our `Sigmoid` class will produce the output and the Derivative.
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For back propagation (backprop), we will first calculate the gradient from
    the output layers, put those values through our Hidden Layer (reversing the direction
    we took in forward propagation), update the weights, and finally put the value
    through our output layers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Calculating errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate our error, we take our actual value and subtract it from our expected
    value. The closer we are to 0, the better we will be. Note that the following
    is very little chance that we will ever hit 0, although it could conceivably happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Calculating a gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The gradient is calculated by considering the Derivative of the `Sigmoid` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Updating weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We update the weights by multiplying the learning rate times our gradient,
    and then adding in momentum and multiplying by the previous delta. This is then
    run through each input synapse to calculate the final value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Calculating values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate values, we take the output from our `Sigmoid` function and add
    to it the bias term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Neural network functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following basic list contains the functions we are going to develop n order 
    to lay down our neural network foundation:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing a network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually entering user data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training our network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing our network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that behind us, let's  start coding!
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This menu option will allow us to create a new network from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice our return value in this function. This is a fluent interface, meaning
    that various functions can be chained together into a single statement. Many people
    prefer this type of interface over the conventional one, but you can feel free
    to modify the code any way you like. The following is an example of what a fluent
    interface looks like. Believe it or not, this is a complete neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Importing an existing network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function will allow us to import a previously saved network. Again, note
    the return value that makes this a fluent interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the filename of the previously saved network. Once opened, deserialize
    it into a network structure that we will deal with. (If it didn''t work for some
    reason, abort!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `new Network` and a list of neurons to populate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy over the learning rate and the momentum that was previously saved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create input layers from our imported network data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Create our Hidden Layers from our imported network data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `OutputLayer` neurons from our imported data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, create the synapses that tie everything together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is where we manually enter the data to be used by the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Importing datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is how we our datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Deserialize the data and return it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Testing the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to test the network, we to do a simple forward and backward propagation,
    depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform forward propagation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Exporting the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Export the current network information, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Training the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways of training the network. One is to a minimum error value,
    the other is to a maximum error value. Both functions have defaults, although
    you may wish to make the threshold different for your training, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In both of the preceding function definitions, the neural network `Train` function
    is called to perform the actual training. This function in turn calls the forward
    and backward propagation functions for each dataset from within each iteration
    of the training loop, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Testing the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function allows us to test our network. Again, notice the return value,
    which makes this a fluent interface. For the most commonly used functions at a
    higher, more abstract layer, I tried to make the fluent interface available where
    it would be most beneficial, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the input data from the user, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Do the computations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Print out the results, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Computing forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function is where we `Compute` the forward propagation value based upon
    the values provided, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Exporting the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function is where we export our network. To us, exporting means serializing
    the data into a JSON human-readable format, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Exporting a dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function is where we export our dataset information. As with exporting
    the network, this will be done in JSON human readable format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With many of the ancillary, but important, functions coded, we now turn our
    attention to the meat of the neural network, the network itself. Within a neural
    network, the network part is an all-encompassing universe. Everything resides
    within it. Within this structure we will need to store the input, output, and
    Hidden Layers of neurons, as well as the learning rate and Momentum, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Neuron connection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every neuron must be connected to other neurons, and our neuron constructor
    will handle connecting all the input neurons with the synapses, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our code created, let's use a few examples to see how it can
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: Training to a minimum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will use the code we wrote to train a network to a minimum
    value or threshold. For each step, the network prompts you for the correct data,
    saving us the process of cluttering up our example code with this. In production,
    you would probably want to pass in the parameters without any user intervention,
    in case this is run as a service or microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9132c220-7211-4fbd-9e92-f7014a5bd6ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Training to a maximum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are going to train the network to reach a maximum value,
    rather than the minimum, as depicted. We manually enter the data we wish to work
    with, as well as the expected result. We then allow training to complete. Once
    completed, we type in our testing input and test the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecadbb7d-46d1-48ac-b95b-f1ec30018163.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to write a complete neural network from scratch.
    Although the following is a lot we've left out, it does the basics, and we've
    gotten to see it as pure C# code! We should now have a much better understanding
    of what a neural network is and what it comprises than when we first started.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin our journey into more complicated network
    structures such as recurrent and convolutional neural networks. There's a lot
    to cover, so hold on to your coding hats!
  prefs: []
  type: TYPE_NORMAL
