- en: Building Our First Neural Network Together
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一起构建我们的第一个神经网络
- en: Now that we've had a quick refresher on Neural Networks, I thought that perhaps
    a good starting point, code-wise, would be for us to write a very simple neural
    network. We're not going to go crazy; we'll just lay the basic framework for a
    few functions so that you can get a good idea of what is behind the scenes of
    many of the APIs that you'll use. From start to finish, we'll develop this network
    application so that you are familiar with all the basic components that are contained
    in a neural network. This implementation is not perfect or all-encompassing, nor
    is it meant to be. As I mentioned, this will merely provide a framework for us
    to use in the rest of the book. This is a very basic neural network with the added
    functionality of being able to save and load networks and data. In any event,
    you will have a foundation from which to write your own neural network and change
    the world, should you so desire.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对神经网络有了快速的复习，我认为从代码的角度来看，一个好的起点可能是我们编写一个非常简单的神经网络。我们不会做得太过分；我们只是为几个函数搭建基本框架，这样你就可以了解许多API背后的场景。从头到尾，我们将开发这个网络应用程序，以便你熟悉神经网络中包含的所有基本组件。这个实现并不完美或全面，也不是这个意思。正如我提到的，这仅仅提供了一个框架，我们可以在本书的其余部分中使用。这是一个非常基本的神经网络，增加了保存和加载网络和数据的功能。无论如何，你将有一个基础，从那里你可以编写自己的神经网络，并改变世界，如果你愿意的话。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Neural network training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络训练
- en: Terminology
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语
- en: Synapses
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 突触
- en: Neurons
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元
- en: Forward propagation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播
- en: Sigmoid function
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: Back propagation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Error calculations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误计算
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You would need to have Microsoft Visual Studio installed on the system.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在系统上安装Microsoft Visual Studio。
- en: Check out the following video to see Code in Action: [http://bit.ly/2NYJa5G](http://bit.ly/2NYJa5G).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际应用：[http://bit.ly/2NYJa5G](http://bit.ly/2NYJa5G)。
- en: Our neural network
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的神经网络
- en: 'Let''s begin by showing you an of what a simple neural network would look like,
    visually. It consists of an input layer with *2* inputs, a Hidden Layer with *3*
    neurons (sometimes called **nodes**), and a final output layer consisting of a
    single neuron. Of course, neural networks can consist of many more layers (and
    neurons per layer), and once you get into deep learning you will see much more
    of this, but for now this will suffice. Remember, each node, which is labeled
    as follows with an **N**, is an individual neuron – its own little processing
    brain, if you will:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先向您展示一个简单的神经网络的外观，从视觉上。它由一个包含*2*个输入的输入层、一个包含*3*个神经元（有时称为**节点**）的隐藏层以及一个由单个神经元组成的最终输出层组成。当然，神经网络可以包含更多层（以及每层的神经元），一旦你进入深度学习，你会看到更多这样的例子，但就目前而言，这已经足够了。记住，每个节点，用**N**标记，都是一个单独的神经元——它自己的小处理大脑，如果你愿意这样想的话：
- en: '![](img/a67995f1-ceca-4de8-8540-fbf25ab7843e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a67995f1-ceca-4de8-8540-fbf25ab7843e.png)'
- en: 'Let’s break down the neural network into its three basic parts; inputs, Hidden
    Layers and outputs:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将神经网络分解为其三个基本部分：输入、隐藏层和输出：
- en: '**Inputs**: This is the initial data for our network. Each input is a whose
    output to the Hidden Layer is the initial input value.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：这是我们的网络的初始数据。每个输入都是一个值，其输出到隐藏层的值是初始输入值。'
- en: '**Hidden Layers**: These are the heart and soul of our network, and all the
    magic happens. Neurons in this layer are assigned weights for each of their inputs.
    These weights start off randomized, and are adjusted as the network is trained
    so that the neuron''s output is closer to the expected result (if we are lucky).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏层**：这是我们网络的灵魂和核心，所有的魔法都发生在这里。这个层中的神经元为它们的每个输入分配权重。这些权重最初是随机的，并在网络训练过程中进行调整，以便神经元的输出更接近预期的结果（如果我们幸运的话）。'
- en: '**Outputs**: These are the output our network arrives at after it performs
    its calculations. The output in our simple case will be either true or false,
    on or off. The neurons are assigned a weight for each of their inputs, which comes
    from the previous Hidden Layer. Although it is typically common for there to be
    only a single output neuron, there''s absolutely nothing preventing you from having
    more, should you need or want more than one.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：这些是我们网络在执行计算后得到的结果。在我们的简单案例中，输出将是真或假，开或关。神经元为它们的每个输入分配一个权重，这些权重来自前一个隐藏层。尽管通常只有一个输出神经元是常见的，但如果需要或想要更多，你完全可以拥有更多。'
- en: Neural network training
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络训练
- en: How do we train a neural network? Basically, we will provide the with a set
    of input data as well as the results we expect to see, which correspond to those
    inputs. That data is then run through the network until the network understands
    what we are looking for. We will train, test, train, test, train, test, on and
    on until our network understands our data (or doesn't, but that's a whole other
    conversation). We continue to do this until some designated stop condition is
    satisfied, such as an error rate threshold. Let's quickly cover some of the terminology
    we will use while training neural networks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何训练神经网络？基本上，我们将提供一组输入数据以及我们期望看到的结果，这些结果与输入相对应。然后，这些数据将通过网络运行，直到网络理解我们在寻找什么。我们将进行训练、测试、训练、测试、训练、测试，如此循环，直到我们的网络理解我们的数据（或者不理解，但这又是另一个话题）。我们继续这样做，直到满足某个指定的停止条件，例如错误率阈值。让我们快速了解一下我们在训练神经网络时将使用的术语。
- en: '**Back propagation**: After our data is run through the network, we to validate
    that data what we expect to be the correct output. We do this by propagating *backward* (hence
    backprop or back propagation) through each of the Hidden Layers of our network.
    The end result is that this adjusts the weights assigned to each of the neuron''s
    inputs in the Hidden Layers as well as our error rate.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**：在数据通过网络运行后，我们需要验证这些数据，看看我们期望得到的是正确的输出。我们通过将数据反向传播（因此称为反向传播或反向传播）通过网络的每个隐藏层来实现这一点。最终结果是调整分配给隐藏层中每个神经元输入的权重以及我们的错误率。'
- en: Each back propagation layer should, in a perfect world, make our network output
    closer to what we are expecting, and our error rate will get closer and closer
    to 0\. We may never get to an exact error rate of 0, so even though it may seem
    not much of a difference, an error rate of 0.0000001 could be more than acceptable
    to us.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，每个反向传播层都应该使我们的网络输出更接近我们期望的值，我们的错误率将越来越接近0。我们可能永远无法达到精确的错误率为0，所以尽管这可能看起来差别不大，但错误率为0.0000001可能对我们来说已经足够可接受。
- en: '**Biases**: Biases allow us to modify our function so that we can generate
    better output for each neuron in our network. In short, a bias allows us to shift
    the activation function value to the left or the right. Changing the weight changes
    the steepness or vertical aspect of the Sigmoid.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏置**：偏置允许我们修改我们的函数，以便我们可以为网络中的每个神经元生成更好的输出。简而言之，偏置允许我们将激活函数的值向左或向右移动。改变权重会改变Sigmoid的陡峭程度或垂直方面。'
- en: '**Momentum**: Momentum simply adds a fraction of the previous weight update
    to the current one. Momentum is used to prevent the system from converging on
    a local minimum rather than the global minimum. High momentum can be used to help
    increase the speed of convergence of the system; however, you must be careful
    as setting this parameter too high can create a risk of overshooting the minimum,
    which will result in an unstable system. On the other hand, a momentum that is
    too low cannot reliably avoid local minima, and it can also really slow down the
    training of the system. So, getting this value correct is paramount for success
    and something you will spend a considerable amount of time doing.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**动量**：动量简单地将前一次权重更新的部分加到当前更新上。动量用于防止系统收敛到局部最小值而不是全局最小值。高动量可以用来帮助增加系统的收敛速度；然而，你必须小心，因为设置此参数过高可能导致超过最小值，这将导致系统不稳定。另一方面，动量太低可能无法可靠地避免局部最小值，它也可能真的减慢系统的训练速度。因此，正确获取此值对于成功至关重要，你将花费相当多的时间来做这件事。'
- en: '**Sigmoid function**: An activation function defines what each neuron''s output
    will be. A Sigmoid function is perhaps the most commonly used activation function.
    It converts the input into a value which lies between 0 and 1\. This function
    is used to generate our initial weights. A typical Sigmoid function will be able
    to accept an input value and, from that value, provide both an output value and
    a derivative.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sigmoid函数**：激活函数定义了每个神经元的输出。Sigmoid函数可能是最常用的激活函数。它将输入转换为介于0和1之间的值。此函数用于生成我们的初始权重。典型的Sigmoid函数能够接受一个输入值，并从这个值提供输出值和导数。'
- en: '**Learning rate**: The learning rate will change the overall learning speed
    of the system by controlling the size of the weight and bias changes made to the
    network during the learning phase.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习率**：学习率通过控制网络在学习阶段对权重和偏置所做的变化大小，从而改变系统的整体学习速度。'
- en: Now that we have this terminology behind us, let's start digging into the code.
    You should have downloaded the solution from the accompanying software provided
    for the book, and have it opened in Visual Studio. We use the Community Edition
    of Visual studio, but you may use whichever version you have.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这些术语，让我们开始深入代码。你应该已经下载了书中提供的配套软件的解决方案，并在Visual Studio中打开它。我们使用Visual Studio的社区版，但你也可以使用你有的任何版本。
- en: Feel free to download the software, experiment with it, and embellish it if
    you need or want to. In your world, your neural network can be anything you like
    or need it to be, so make it happen. You have the source. Just because you see
    something one way doesn't make it gospel or written in stone! Learn from what
    these great open source contributors have provided for us! Remember, this neural
    network is meant only to give you some idea of the many things that you could
    do writing your own, as well as teach you some of the basics when it comes to
    a neural network.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随意下载软件，实验它，如果你需要或想要的话，还可以对其进行美化。在你的世界里，你的神经网络可以是任何你喜欢或需要的样子，所以让它发生吧。你有源代码。仅仅因为你看某件事的方式不一定是真理或刻在石头上的！从这些伟大的开源贡献者为我们提供的东西中学习！记住，这个神经网络旨在给你一些想法，关于你可以做很多事情，同时也会教你一些神经网络的基础知识。
- en: Let's start off by looking at some brief code snippets that will set the stage
    for the rest of the chapter. We'll start first with a little thing called a **synapse**,
    which connects one neuron to another. Next, we'll start coding exactly what an
    individual neuron is, and finally move into discussing forward and backward propagation
    and what that means to us. We'll show everything in the form of code snippets
    to make it easier to understand.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些简短的代码片段开始，这些代码片段将为本章的其余部分奠定基础。我们首先从一个叫做**突触**的小东西开始，它连接一个神经元到另一个。接下来，我们将开始编写单个神经元的代码，最后讨论前向和反向传播以及这对我们意味着什么。我们将以代码片段的形式展示一切，以便更容易理解。
- en: Synapses
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 突触
- en: 'What is a synapse, you might ask? Simply put, it connects one neuron to another,
    as well as being a container to hold weight and weight delta values, depicted
    as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，什么是突触？简单地说，它连接一个神经元到另一个，同时也是一个容器，用来存放权重和权重增量值，如下所示：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Neurons
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经元
- en: 'We''ve already discussed what a neuron is, now it''s time to express it in
    code so developers like us can make more sense of it! As you can see, we have
    both our input and output synapses, our `Bias` and the `Bias Delta`, the `Gradient`,
    and the actual value of the neuron. The neuron calculates the weighted sum of
    its inputs, adds a bias, and then decides if the output should `''fire'' - ''be
    on''`- or not:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了什么是神经元，现在该用代码来表达了，这样像我们这样的开发者就能更好地理解它了！正如你所见，我们既有输入和输出突触，还有`偏置`和`偏置增量`，`梯度`以及神经元的实际值。神经元计算其输入的加权和，加上偏置，然后决定输出是否应该`'fire'
    - '开启'`：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Forward propagation
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'The following is our code for a basic forward propagation process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们基本前向传播过程的代码：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To do `ForwardPropagation`, we basically sum all the inputs from each synapse
    and run the result through our Sigmoid function to get an output. The `CalculateValue`
    function does this for us.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行`ForwardPropagation`，我们基本上将每个突触的所有输入相加，然后将结果通过Sigmoid函数来得到输出。`CalculateValue`函数为我们做这件事。
- en: Sigmoid function
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'The Sigmoid function is an activation function and, as we previously, perhaps
    one of the most widely used today. Here''s what a Sigmoid function looks like
    (you do remember our section on activation functions, right?) Its sole purpose
    (very abstractly) is to bring in the values from the outside edges closer to 0
    and 1 without having to worry about values larger than this. This will prevent
    those values along the edge from running away on us:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数是一个激活函数，正如我们之前所提到的，可能是今天最广泛使用的之一。下面是一个Sigmoid函数的样子（你记得我们关于激活函数的部分，对吧？）它的唯一目的（非常抽象地）是将外部边缘的值拉近0和1，而不用担心值会大于这个。这将防止边缘上的值离我们而去：
- en: '![](img/6d0b3758-e073-4341-aa78-748b52735d03.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6d0b3758-e073-4341-aa78-748b52735d03.jpg)'
- en: 'What does a `Sigmoid` function look like in C# code, you might ask? Just like
    the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，C#代码中的Sigmoid函数是什么样的？就像下面的这样：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our `Sigmoid` class will produce the output and the Derivative.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`Sigmoid`类将产生输出和导数。
- en: Backward propagation
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'For back propagation (backprop), we will first calculate the gradient from
    the output layers, put those values through our Hidden Layer (reversing the direction
    we took in forward propagation), update the weights, and finally put the value
    through our output layers, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反向传播（backprop），我们首先从输出层计算梯度，将这些值通过隐藏层（反转我们在正向传播中采取的方向），更新权重，最后将这些值通过输出层，如下所示：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Calculating errors
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算误差
- en: 'To calculate our error, we take our actual value and subtract it from our expected
    value. The closer we are to 0, the better we will be. Note that the following
    is very little chance that we will ever hit 0, although it could conceivably happen:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算我们的误差，我们从实际值中减去预期值。我们越接近0，效果越好。请注意，我们几乎不可能达到0，尽管理论上可能发生这种情况：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Calculating a gradient
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算梯度
- en: 'The gradient is calculated by considering the Derivative of the `Sigmoid` function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是通过考虑`Sigmoid`函数的导数来计算的：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Updating weights
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新权重
- en: 'We update the weights by multiplying the learning rate times our gradient,
    and then adding in momentum and multiplying by the previous delta. This is then
    run through each input synapse to calculate the final value:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将学习率乘以我们的梯度，然后加上动量并乘以之前的delta来更新权重。然后通过每个输入突触运行以计算最终值：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Calculating values
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算值
- en: 'To calculate values, we take the output from our `Sigmoid` function and add
    to it the bias term:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算值，我们从`Sigmoid`函数的输出中取出，并加上偏置项：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Neural network functions
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络函数
- en: 'The following basic list contains the functions we are going to develop n order 
    to lay down our neural network foundation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下基本列表包含我们将开发的功能，以便建立我们的神经网络基础：
- en: Creating a new network
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建新的网络
- en: Importing a network
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入网络
- en: Manually entering user data
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动输入用户数据
- en: Importing a dataset
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入数据集
- en: Training our network
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练我们的网络
- en: Testing our network
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试我们的网络
- en: With that behind us, let's  start coding!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，让我们开始编码！
- en: Creating a new network
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个新的网络
- en: 'This menu option will allow us to create a new network from scratch:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此菜单选项将允许我们从零开始创建一个新的网络：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Notice our return value in this function. This is a fluent interface, meaning
    that various functions can be chained together into a single statement. Many people
    prefer this type of interface over the conventional one, but you can feel free
    to modify the code any way you like. The following is an example of what a fluent
    interface looks like. Believe it or not, this is a complete neural network:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个函数中的返回值。这是一个流畅的接口，意味着可以将各种函数链接到单个语句中。许多人更喜欢这种接口而不是传统的接口，但您可以随意修改代码。以下是一个流畅接口的示例。信不信由你，这是一个完整的神经网络：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Importing an existing network
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入现有网络
- en: 'This function will allow us to import a previously saved network. Again, note
    the return value that makes this a fluent interface:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能将允许我们导入之前保存的网络。再次注意返回值，这使得它成为一个流畅的接口：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Get the filename of the previously saved network. Once opened, deserialize
    it into a network structure that we will deal with. (If it didn''t work for some
    reason, abort!):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 获取之前保存的网络文件名。一旦打开，将其反序列化为我们将处理的网络结构。（如果由于某种原因不起作用，则中止！）：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a `new Network` and a list of neurons to populate:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的`Network`和一个神经元列表来填充：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Copy over the learning rate and the momentum that was previously saved:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 复制之前保存的学习率和动量：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create input layers from our imported network data:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们导入的网络数据中创建输入层：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create our Hidden Layers from our imported network data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们导入的网络数据中创建我们的隐藏层：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create the `OutputLayer` neurons from our imported data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们导入的数据中创建`OutputLayer`神经元：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, create the synapses that tie everything together:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建将所有内容连接起来的突触：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following is where we manually enter the data to be used by the network:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们手动输入网络将使用的数据：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Importing datasets
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据集
- en: 'The following is how we our datasets:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何处理数据集：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Deserialize the data and return it:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 反序列化数据并返回它：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Testing the network
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试网络
- en: 'In order to test the network, we to do a simple forward and backward propagation,
    depicted as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试网络，我们需要进行简单的正向和反向传播，如下所示：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Perform forward propagation, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式进行正向传播：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Return the data, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式返回数据：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Exporting the network
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导出网络
- en: 'Export the current network information, as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式导出当前网络信息：
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Training the network
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'There are two ways of training the network. One is to a minimum error value,
    the other is to a maximum error value. Both functions have defaults, although
    you may wish to make the threshold different for your training, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络有两种方式。一种是将错误值降至最小，另一种是将错误值升至最大。这两个函数都有默认值，尽管你可能希望为你的训练设置不同的阈值，如下所示：
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In both of the preceding function definitions, the neural network `Train` function
    is called to perform the actual training. This function in turn calls the forward
    and backward propagation functions for each dataset from within each iteration
    of the training loop, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两个函数定义中，神经网络`Train`函数被调用以执行实际训练。此函数随后在训练循环的每次迭代中为每个数据集调用前向和反向传播函数，如下所示：
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Testing the network
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试网络
- en: 'This function allows us to test our network. Again, notice the return value,
    which makes this a fluent interface. For the most commonly used functions at a
    higher, more abstract layer, I tried to make the fluent interface available where
    it would be most beneficial, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数允许我们测试我们的网络。再次注意返回值，这使得它成为一个流畅的接口。对于在更高、更抽象层上最常用的函数，我尽量在最有益的地方提供流畅的接口，如下所示：
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Get the input data from the user, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户处获取输入数据，如下所示：
- en: '[PRE29]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Do the computations, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 进行计算，如下所示：
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Print out the results, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 打印结果，如下所示：
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Computing forward propagation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算前向传播
- en: 'This function is where we `Compute` the forward propagation value based upon
    the values provided, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数是根据提供的值`Compute`前向传播值的，如下所示：
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Exporting the network
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导出网络
- en: 'This function is where we export our network. To us, exporting means serializing
    the data into a JSON human-readable format, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数是我们导出网络的地方。对我们来说，导出意味着将数据序列化为JSON可读格式，如下所示：
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Exporting a dataset
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导出数据集
- en: 'This function is where we export our dataset information. As with exporting
    the network, this will be done in JSON human readable format:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数是我们导出数据集信息的地方。与导出网络一样，这将以JSON可读格式完成：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The neural network
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'With many of the ancillary, but important, functions coded, we now turn our
    attention to the meat of the neural network, the network itself. Within a neural
    network, the network part is an all-encompassing universe. Everything resides
    within it. Within this structure we will need to store the input, output, and
    Hidden Layers of neurons, as well as the learning rate and Momentum, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写了许多辅助但重要的函数之后，我们现在将注意力转向神经网络的核心，即网络本身。在神经网络中，网络部分是一个包罗万象的宇宙。一切都在其中。在这个结构中，我们需要存储输入、输出和隐藏层的神经元，以及学习率和动量，如下所示：
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Neuron connection
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经元连接
- en: 'Every neuron must be connected to other neurons, and our neuron constructor
    will handle connecting all the input neurons with the synapses, as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元都必须连接到其他神经元，我们的神经元构造函数将处理将所有输入神经元与突触连接，如下所示：
- en: '[PRE36]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Examples
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: Now that we have our code created, let's use a few examples to see how it can
    be used.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了代码，让我们用几个例子来看看它如何使用。
- en: Training to a minimum
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练至最小值
- en: 'In this example, we will use the code we wrote to train a network to a minimum
    value or threshold. For each step, the network prompts you for the correct data,
    saving us the process of cluttering up our example code with this. In production,
    you would probably want to pass in the parameters without any user intervention,
    in case this is run as a service or microservice:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将使用我们编写的代码来训练一个网络以达到最小值或阈值。对于每个步骤，网络都会提示你输入正确数据，从而避免在我们的示例代码中添加这些数据。在生产环境中，你可能希望在不进行用户干预的情况下传递参数，以防这个程序作为服务或微服务运行：
- en: '![](img/9132c220-7211-4fbd-9e92-f7014a5bd6ea.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9132c220-7211-4fbd-9e92-f7014a5bd6ea.png)'
- en: Training to a maximum
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练至最大值
- en: 'In this example, we are going to train the network to reach a maximum value,
    rather than the minimum, as depicted. We manually enter the data we wish to work
    with, as well as the expected result. We then allow training to complete. Once
    completed, we type in our testing input and test the network:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将训练网络以达到最大值，而不是最小值，如图所示。我们手动输入我们希望处理的数据以及预期的结果。然后我们允许训练完成。一旦完成，我们输入测试输入并测试网络：
- en: '![](img/ecadbb7d-46d1-48ac-b95b-f1ec30018163.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ecadbb7d-46d1-48ac-b95b-f1ec30018163.png)'
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we saw how to write a complete neural network from scratch.
    Although the following is a lot we've left out, it does the basics, and we've
    gotten to see it as pure C# code! We should now have a much better understanding
    of what a neural network is and what it comprises than when we first started.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何从头开始编写一个完整的神经网络。尽管我们省略了很多内容，但它涵盖了基础知识，并且我们已经看到了它作为纯C#代码的样子！现在，我们应该比刚开始时对神经网络是什么以及它包含什么有更深入的理解。
- en: In the next chapter, we will begin our journey into more complicated network
    structures such as recurrent and convolutional neural networks. There's a lot
    to cover, so hold on to your coding hats!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始探索更复杂的网络结构，例如循环神经网络和卷积神经网络。内容很多，所以请系好你们的编程帽子！
