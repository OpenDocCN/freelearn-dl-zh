- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Choosing an LLM for Your Application
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为您的应用选择LLM
- en: In the last chapter, we saw how pivotal it is to properly orchestrate **large
    language models** (**LLMs**) and their components within applications. In fact,
    we saw that not all LLMs are created equal. The next key decision is which LLMs
    to actually use. Different LLMs may have different architectures, sizes, training
    data, capabilities, and limitations. Choosing the right LLM for your application
    is not a trivial decision, as it can have a significant impact on the performance,
    quality, and cost of your solution.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了在应用中正确编排**大型语言模型**（**LLM**）及其组件是多么关键。事实上，我们看到了并非所有LLM都是相同的。下一个关键决策是实际使用哪些LLM。不同的LLM可能具有不同的架构、大小、训练数据、能力和局限性。为您的应用选择正确的LLM不是一个简单的决定，因为它可能会对您解决方案的性能、质量和成本产生重大影响。
- en: 'In this chapter, we will guide you through the process of choosing the right
    LLM for your application. We will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将指导你通过选择适合您应用的正确LLM的过程。我们将涵盖以下主题：
- en: An overview of the most promising LLMs in the market
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场上最有前景的LLM概述
- en: The main criteria and tools to use when comparing LLMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较LLM时使用的主要标准和工具
- en: Trade-offs between size and performance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小与性能之间的权衡
- en: By the end of this chapter, you should have a clear understanding of how to
    choose the right LLM for your application and how to use it effectively and responsibly.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该清楚地了解如何选择适合您应用的正确LLM，以及如何有效地和负责任地使用它。
- en: The most promising LLMs in the market
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 市场上最有前景的LLM
- en: The last year has witnessed an unprecedented surge in the research and development
    of LLMs. Several new models have been released or announced by different organizations,
    each with its own features and capabilities. Some of these models are the largest
    and most advanced ever created, surpassing the previous **state-of-the-art** (**SOTA**)
    by orders of magnitude. Others are lighter yet more specialized in specific tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 过去一年见证了LLM研究和开发的空前激增。不同组织发布了或宣布了几个新模型，每个模型都有其独特的特性和功能。其中一些模型是迄今为止最大、最先进的，其性能远远超过了之前的**最先进水平**（**SOTA**）。其他模型则更轻量，但在特定任务上更加专业化。
- en: In this chapter, we will review some of the most promising LLMs in the market
    as of 2024\. We will introduce their background, key findings, and main techniques.
    We will also compare their performance, strengths, and limitations on various
    benchmarks and tasks. We will also discuss their potential applications, challenges,
    and implications for the future of AI and society.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾截至2024年市场上一些最有前景的LLM（大型语言模型）。我们将介绍它们的背景、关键发现和主要技术。我们还将比较它们在各种基准和任务上的性能、优势和局限性。我们还将讨论它们在人工智能和社会未来的潜在应用、挑战和影响。
- en: Proprietary models
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专有模型
- en: Proprietary LLMs are developed and owned by private companies, and they are
    not disclosed with code. They are also typically subject to a fee for consumption.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 专有LLM是由私人公司开发和拥有的，它们不公开代码。它们通常也需付费才能使用。
- en: Proprietary models offer a series of advantages, including better support and
    maintenance as well as safety and alignment. They also tend to outperform open-source
    models in terms of generalization, because of their complexity and training datasets.
    On the other hand, they act as a “black box,” meaning that owners do not disclose
    the source code to developers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 专有模型提供一系列优势，包括更好的支持和维护，以及安全性和一致性。由于它们的复杂性和训练数据集，它们在泛化方面通常优于开源模型。另一方面，它们充当一个“黑盒”，这意味着所有者不会向开发者公开源代码。
- en: In the next sections, we will cover three of the most popular proprietary LLMs
    in the market, as of August 2023.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将介绍截至2023年8月市场上最受欢迎的三种专有LLM。
- en: GPT-4
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4
- en: Released in March 2023, GPT-4 is, together with its newly released “cousin”
    GPT-4 Turbo, one of the latest models developed by **OpenAI**, is among the top
    performers in the market at the time of writing this book (while OpenAI, as confirmed
    by its CEO Sam Altman, is already working on GPT-5).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 于2023年3月发布，GPT-4与其新发布的“堂兄弟”GPT-4 Turbo一起，是**OpenAI**开发的最新模型之一，在撰写本书时（根据其首席执行官山姆·奥特曼的确认，OpenAI已经在开发GPT-5），是市场上表现最好的模型之一。
- en: 'It belongs to the class of **generative pretrained transformer** (**GPT**)
    models, a decoder-only transformer-based architecture introduced by OpenAI. The
    following diagram shows the basic architecture:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 它属于**生成预训练转换器**（**GPT**）模型类别，这是一种由OpenAI引入的仅包含解码器的转换器架构。以下图表展示了基本架构：
- en: '![](img/B21714_03_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_03_01.png)'
- en: 'Figure 3.1: High-level architecture of a decoder-only transformer'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：仅解码器转换器的高级架构
- en: As you can see from the preceding diagram, the decoder-only architecture still
    includes the main elements that feature in transformer architecture that we covered
    in *Chapter 1*, *Positional Embeddings, Multi-Head Attention*, and *Feed Forward*
    layers. However, in this architecture, the model solely comprises a decoder, which
    is trained to predict the next token in a sequence based on the preceding tokens.
    Unlike the encoder-decoder architecture, the decoder-only design lacks an explicit
    encoder for summarizing input information. Instead, the information is implicitly
    encoded within the hidden state of the decoder, which is updated at each step
    during the generation process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，仅解码器架构仍然包括我们在*第一章*中介绍过的转换器架构的主要元素，包括*位置嵌入*、*多头注意力*和*前馈*层。然而，在这个架构中，模型仅由一个解码器组成，该解码器被训练根据前面的标记来预测序列中的下一个标记。与编码器-解码器架构不同，仅解码器设计缺少一个用于总结输入信息的显式编码器。相反，信息隐含地编码在解码器的隐藏状态中，该状态在生成过程的每一步都会更新。
- en: Now, we’ll look at some of the improvements in GPT-4 over previous versions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨GPT-4相较于先前版本的一些改进。
- en: GPT-4, like the previous models in the GPT series, has been trained on both
    publicly available and OpenAI-licensed datasets (OpenAI didn’t disclose the exact
    composition of the training set).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4与GPT系列中的先前模型一样，是在公开可用的和OpenAI许可的数据集上训练的（OpenAI没有披露训练集的确切组成）。
- en: Additionally, to make the model more aligned with the user’s intent, the training
    process also involved **reinforcement learning from human feedback** (**RLHF**)
    training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使模型更符合用户的意图，训练过程还涉及了**基于人类反馈的强化学习**（**RLHF**）训练。
- en: '**Definition**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: 'RLHF is a technique that aims at using human feedback as an evaluating metric
    for LLMs’ generated output and then using that feedback to further optimize the
    model. There are two main steps to achieve that goal:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF是一种旨在使用人类反馈作为评估LLM生成输出的指标，然后使用该反馈进一步优化模型的技术。实现该目标主要有两个步骤：
- en: Training a reward model based on human preferences.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于人类偏好训练奖励模型。
- en: Optimizing the LLM with respect to the reward model. This step is done via reinforcement
    learning and it is a type of machine learning paradigm where an agent learns to
    make decisions by interacting with an environment. The agent receives feedback
    in the form of rewards or penalties based on its actions, and its goal is to maximize
    the cumulative reward over time by continuously adapting its behavior through
    trial and error.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化LLM以适应奖励模型。这一步骤是通过强化学习完成的，它是一种机器学习范式，其中代理通过与环境的交互来学习做出决策。代理根据其行为获得奖励或惩罚的反馈，其目标是通过对行为进行试错来不断调整其行为，以最大化累积奖励。
- en: With RLHF, thanks to the reward model, the LLM is able to learn from human preferences
    and be more aligned with users’ intents.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过RLHF（强化学习与人类反馈），得益于奖励模型，LLM能够从人类偏好中学习，并更符合用户的意图。
- en: As an example, think about ChatGPT. This model integrates various training methods,
    including unsupervised pretraining, supervised fine-tuning, instruction tuning,
    and RLHF. The RLHF component involves training the model to predict human preferences
    by using feedback from human trainers. These trainers review the model’s responses
    and provide ratings or corrections, guiding the model to generate more helpful,
    accurate, and aligned responses.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以ChatGPT为例。该模型集成了各种训练方法，包括无监督预训练、监督微调、指令调整和RLHF。RLHF组件涉及通过使用来自人类训练师的反馈来训练模型预测人类偏好。这些训练师审查模型的响应并提供评分或更正，指导模型生成更有帮助、更准确和更符合预期的响应。
- en: For instance, if a language model initially produces an output that is not quite
    helpful or accurate, human trainers can provide feedback that indicates the preferred
    output. The model then uses this feedback to adjust its parameters and improve
    future responses. This process iteratively continues, with the model learning
    from a series of human judgments to better align with what is considered helpful
    or appropriate by human standards.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个语言模型最初生成的输出不太有帮助或不准确，人类训练师可以提供反馈，表明首选的输出。然后，该模型使用这些反馈来调整其参数并改进未来的响应。这个过程迭代地进行，模型从一系列人类判断中学习，以更好地符合人类标准认为的有用或适当的内容。
- en: GPT-4 demonstrated outstanding capabilities in commonsense reasoning and analytical
    skills. It has been benchmarked with SOTA systems, including the **Massive Multitask
    Language Understanding** (**MMLU**) we covered in *Chapter 1*. On MMLU, GPT-4
    outperformed previous models not only in English, but also in other languages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4在常识推理和分析技能方面表现出卓越的能力。它已经与包括我们在第一章中提到的**大规模多任务语言理解**（MMLU）在内的SOTA系统进行了基准测试。在MMLU上，GPT-4不仅在英语上，而且在其他语言上也优于之前的模型。
- en: 'The following is an illustration that shows GPT-4’s performance on MMLU:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的插图显示了GPT-4在MMLU上的表现：
- en: '![A graph with green and blue bars  Description automatically generated](img/B21714_03_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![带有绿色和蓝色条的图表  自动生成的描述](img/B21714_03_02.png)'
- en: 'Figure 3.2: GPT-4 3-shot accuracy on MMLU across languages (source: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4))'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：GPT-4在MMLU上的3次射击准确率（来源：[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)）
- en: 'In addition to MMLU, GPT-4 has been benchmarked on a variety of SOTA systems
    and academic exams, as you can see from the following graph:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了MMLU之外，GPT-4在各种最先进（SOTA）系统和学术考试上进行了基准测试，正如您可以从以下图表中看到：
- en: '![A graph of a performance  Description automatically generated](img/B21714_03_03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![性能图表  自动生成的描述](img/B21714_03_03.png)'
- en: 'Figure 3.3: GPT performance on academic and professional exams (source: [https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：GPT在学术和专业考试中的表现（来源：[https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf)）
- en: '**Note**: in the preceding graph, you can see two versions of GPT-4, vision
    and no vision (along with the GPT-3.5 for benchmarking purposes). This is because
    GPT-4 is a multi-modal model, meaning that it can take images as input, in addition
    to text. However, in this chapter, we will benchmark only its textual capabilities.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：在前面的图表中，您可以看到两个版本的GPT-4，有视觉和无视觉（以及GPT-3.5用于基准测试）。这是因为GPT-4是一个多模态模型，意味着它可以接受图像作为输入，除了文本。然而，在本章中，我们将仅对其文本能力进行基准测试。'
- en: Another great improvement of GPT-4 with respect to its predecessors (GPT-3.5
    and GPT-3) is its noticeable reduction in the risk of hallucination.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与其前辈（GPT-3.5和GPT-3）相比，GPT-4的一个显著改进是幻觉风险的明显降低。
- en: '**Definition**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Hallucination is a term that describes a phenomenon where LLMs generate text
    that is incorrect, nonsensical, or not real, but appears to be plausible or coherent.
    For example, an LLM may hallucinate a fact that contradicts the source or common
    knowledge, a name that does not exist, or a sentence that does not make sense.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉是一个描述LLMs生成不正确、无意义或不真实的文本，但看起来似乎合理或连贯的现象。例如，一个LLM可能会虚构一个与来源或常识相矛盾的事实，一个不存在的名字，或者一个没有意义的句子。
- en: Hallucination can happen because LLMs are not databases or search engines that
    store or retrieve factual information. Rather, they are statistical models that
    learn from massive amounts of text data and produce outputs based on the patterns
    and probabilities they have learned. However, these patterns and probabilities
    may not reflect the truth or the reality, as the data may be incomplete, noisy,
    or biased. Moreover, LLMs have limited contextual understanding and memory, as
    they can only process a certain number of tokens at a time and abstract them into
    latent representations. Therefore, LLMs may generate text that is not supported
    by any data or logic but is the most likely or correlated from the prompt.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉可能发生，因为大型语言模型（LLMs）并非存储或检索事实信息的数据库或搜索引擎。相反，它们是统计模型，从大量文本数据中学习并基于它们学到的模式和概率生成输出。然而，这些模式和概率可能并不反映真相或现实，因为数据可能是不完整、嘈杂或存在偏见的。此外，LLMs的上下文理解和记忆有限，因为它们一次只能处理一定数量的标记并将它们抽象为潜在表示。因此，LLMs可能会生成不支持任何数据或逻辑的文本，但这些文本是最可能或与提示最相关的。
- en: In fact, even though it is still not 100% reliable, GPT-4 made great improvements
    with TruthfulQA benchmarks, which test the model’s ability to separate fact from
    incorrect statements (we covered TruthfulQA benchmarks in *Chapter 1*, in the
    *Model evaluation* section).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，尽管它仍然不是100%可靠的，但GPT-4在TruthfulQA基准测试中取得了很大的进步，这些基准测试测试了模型区分事实和错误陈述的能力（我们在*第一章*的*模型评估*部分介绍了TruthfulQA基准测试）。
- en: Here, you can see an illustration that compares GPT-4 results in a TruthfulQA
    benchmark with those of GPT-3.5 (the model behind OpenAI’s ChatGPT) and Anthropic-LM
    (we will cover this latter model in the next sections).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到一幅插图，比较了GPT-4在TruthfulQA基准测试中的结果与GPT-3.5（OpenAI的ChatGPT背后的模型）和Anthropic-LM（我们将在下一节中介绍这个模型）的结果。
- en: '![A graph of different colored squares  Description automatically generated](img/B21714_03_04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![不同颜色方块的图表，描述由系统自动生成](img/B21714_03_04.png)'
- en: 'Figure 3.4: Model comparison in TruthfulQA benchmark (source: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4))'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：在TruthfulQA基准测试中的模型比较（来源：[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)）
- en: Finally, with GPT-4, OpenAI made an additional effort to make it safer and more
    aligned, engaging from the beginning a team of over 50 experts in domains like
    AI alignment risks, privacy, and cybersecurity, with the goal of understanding
    the extent of the risks of such a powerful model and how to prevent them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，随着GPT-4的推出，OpenAI做出了额外的努力使其更安全、更对齐，从一开始就组建了一个由50多位专家组成的团队，这些专家在AI对齐风险、隐私和网络安全等领域，目标是了解这样一个强大模型的风险程度以及如何预防这些风险。
- en: '**Definition**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Alignment is a term that describes the degree to which LLMs behave in ways that
    are useful and harmless for their human users. For example, an LLM may be aligned
    if it generates text that is accurate, relevant, coherent, and respectful. An
    LLM may be misaligned if it generates text that is false, misleading, harmful,
    or offensive.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐是指LLM（大型语言模型）在多大程度上以对人类用户有用且无害的方式行事。例如，如果一个LLM生成的文本是准确、相关、连贯且尊重的，那么它可以被认为是对齐的。如果一个LLM生成的文本是虚假的、误导性的、有害的或冒犯性的，那么它可以被认为是未对齐的。
- en: Thanks to this analysis, further data have been collected and used while training
    GPT-4 to mitigate its potential risks, resulting in a reduced risk compared to
    its predecessor, GPT-3.5.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了这次分析，在训练GPT-4时收集并使用了更多数据，以减轻其潜在风险，从而使其风险比其前身GPT-3.5有所降低。
- en: Gemini 1.5
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gemini 1.5
- en: Gemini 1.5 is a SOTA generative AI model developed by Google and released in
    December 2023\. Like GPT-4, Gemini is designed to be multimodal, meaning that
    it can process and generate content across various modalities, including text,
    images, audio, video, and code. It is based on a **mixture-of-expert** (**MoE**)
    transformer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini 1.5是由谷歌开发并于2023年12月发布的SOTA（最先进的技术）生成式AI模型。与GPT-4一样，Gemini被设计成多模态，这意味着它可以处理和生成各种模态的内容，包括文本、图像、音频、视频和代码。它基于一个**专家混合**（**MoE**）的transformer。
- en: '**Definition**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: In the context of transformer architecture, MoE refers to a model that incorporates
    multiple specialized sub-models, known as “experts,” within its layers. Each expert
    is a neural network designed to handle different types of data or tasks more efficiently.
    The MoE model uses a gating mechanism or router to determine which expert should
    process a given input, allowing the model to dynamically allocate resources and
    specialize in processing certain types of information. This approach can lead
    to more efficient training and inference, as it enables the model to scale up
    in size and complexity without a proportional increase in computational cost.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在transformer架构的背景下，MoE指的是在其层中包含多个专门子模型（称为“专家”）的模型。每个专家都是一个神经网络，旨在更有效地处理不同类型的数据或任务。MoE模型使用门控机制或路由器来确定哪个专家应该处理给定的输入，这使得模型能够动态分配资源并专门处理某些类型的信息。这种方法可以导致更高效的训练和推理，因为它使得模型在规模和复杂性上可以扩展，而不会导致计算成本的成比例增加。
- en: Gemini comes in various sizes, including Ultra, Pro, and Nano, to cater to different
    computational needs, from data centers to mobile devices. To use Gemini, developers
    can access it via the APIs provided for different model variants, allowing the
    integration of its capabilities into applications.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini有多种尺寸，包括Ultra、Pro和Nano，以满足从数据中心到移动设备的不同计算需求。要使用Gemini，开发者可以通过为不同模型变体提供的API访问它，从而将其功能集成到应用程序中。
- en: 'Compared to its previous version, Gemini 1.0, the current model outperforms
    it in text, vision, and audio tasks, as shown in the following screenshot:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与其前一代版本 Gemini 1.0 相比，当前模型在文本、视觉和音频任务上表现更优，如下面的截图所示：
- en: '![](img/B21714_03_05.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_03_05.png)'
- en: 'Figure 3.5: Gemini 1.5 Pro and Ultra compared to its previous version 1.0 (source:
    [https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)
    )'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：Gemini 1.5 Pro 和 Ultra 与其前一代版本 1.0 的比较（来源：[https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)）
- en: 'Similarly, it has demonstrated outstanding capabilities in domains such as
    math, science, and reasoning, and coding and multilinguality:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，它在数学、科学、推理以及编码和多元语言等领域也展现了卓越的能力：
- en: '![](img/B21714_03_06.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_03_06.png)'
- en: 'Figure 3.6: Gemini 1.5 Pro compared to Gemini 1.0 Pro and Ultra on different
    benchmarks (source: [https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf))'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：Gemini 1.5 Pro 与 Gemini 1.0 Pro 和 Ultra 在不同基准测试中的比较（来源：[https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)）
- en: Note that Gemini 1.5 Pro is outperforming Gemini 1.0 Ultra (which is remarkably
    bigger) in many benchmarks across the various domains. As of today, Gemini Pro
    can be tried via a web app at gemini.[google.com](https://google.com) for free,
    while Gemini Ultra is available via a premium subscription with a monthly fee.
    On the other hand, Gemini Nano, which is tailored for mobile devices, can be executed
    on capable Android devices via the Google AI Edge SDK for Android. Note that,
    as of April 2024, this SDK is still under early access preview and you can apply
    for the early access program at [https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform](https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform).
    Finally, Gemini Pro and Ultra can also be consumed by developers via the REST
    API from Google AI Studio.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Gemini 1.5 Pro 在多个领域的许多基准测试中表现优于 Gemini 1.0 Ultra（后者体积显著更大）。截至今天，您可以通过 gemini.[google.com](https://google.com)
    的网页应用程序免费试用 Gemini Pro，而 Gemini Ultra 则可通过带有月费的付费订阅获得。另一方面，专为移动设备定制的 Gemini Nano
    可以通过 Google AI Edge SDK for Android 在具备能力的 Android 设备上运行。请注意，截至 2024 年 4 月，此 SDK
    仍处于早期访问预览阶段，您可以在 [https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform](https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform)
    申请早期访问计划。最后，开发人员也可以通过 Google AI Studio 的 REST API 使用 Gemini Pro 和 Ultra。
- en: Claude 2
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Claude 2
- en: Claude 2, which stands for Constitutional Large-scale Alignment via User Data
    and Expertise, is an LLM developed by Anthropic, a research company founded by
    former OpenAI researchers and focused on AI safety and alignment. It was announced
    in July 2023.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 2，代表通过用户数据和专业知识进行宪法规模对齐，是由前 OpenAI 研究员创立的研究公司 Anthropic 开发的 LLM。它于 2023
    年 7 月宣布。
- en: Claude 2 is a transformer-based LLM that has been trained on a mix of publicly
    available information from the internet and proprietary data, via unsupervised
    learning, RLHF, and **constitutional AI** (**CAI**).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 2 是一个基于 transformer 的 LLM，它通过无监督学习、RLHF 和 **宪法 AI**（**CAI**）在互联网上公开可用的信息和专有数据混合训练而成。
- en: 'CAI is a real peculiarity of Claude. In fact, Anthropic paid extraordinary
    attention to Claude 2 alignment with safety principles. More specifically, Anthropic
    developed this unique technique called CAI, which was disclosed in December 2022
    in the paper *Constitutional AI: Harmlessness from AI Feedback*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: CAI 是 Claude 的一个真正独特之处。事实上，Anthropic 对 Claude 2 的安全原则对齐给予了极大的关注。更具体地说，Anthropic
    开发了这种独特的 CAI 技术，该技术在 2022 年 12 月发表的论文 *宪法 AI：从 AI 反馈中无害* 中被披露。
- en: CAI aims to make the model safer and more aligned with human values and intentions
    by preventing toxic or discriminatory output, not helping a human engage in illegal
    or unethical activities, and broadly creating an AI system that is helpful, honest,
    and harmless. To achieve this, it uses a set of principles to guide the model’s
    behavior and outputs, rather than relying on human feedback or data alone. The
    principles are derived from various sources, such as the UN Declaration of Human
    Rights, trust and safety best practices, principles proposed by other AI research
    labs, non-Western perspectives, and empirical research.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: CAI旨在通过防止产生有害或歧视性输出，不帮助人类参与非法或不道德的活动，以及广泛创建一个有益、诚实且无害的AI系统，来使模型更安全并与其人类价值观和意图保持一致。为了实现这一点，它使用一系列原则来指导模型的行为和输出，而不是仅仅依赖于人类反馈或数据。这些原则来自各种来源，例如联合国人权宣言、信任和安全最佳实践、其他AI研究实验室提出的原则、非西方观点和实证研究。
- en: 'CAI uses these principles in two stages of the training process:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: CAI在训练过程的两个阶段使用这些原则：
- en: First, the model is trained to critique and revise its own responses using the
    principles and a few examples.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，该模型被训练使用原则和一些示例来批评和修改自己的响应。
- en: Second, the model is trained via reinforcement learning, but rather than using
    human feedback, it uses AI-generated feedback based on the principles to choose
    the more harmless output.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，该模型通过强化学习进行训练，但不是使用人类反馈，而是使用基于原则生成的AI反馈来选择更无害的输出。
- en: 'The following illustration shows the training process according to the CAI
    technique:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下插图展示了根据CAI技术进行的训练过程：
- en: '![A diagram of a process flow  Description automatically generated](img/B21714_03_07.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![流程图示意图  自动生成描述](img/B21714_03_07.png)'
- en: 'Figure 3.7: Claude’s training process according to the CAI technique (source:
    [https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073))'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：根据CAI技术进行的Claude的训练过程（来源：[https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073))
- en: Another peculiarity of Claude 2 is the context length, which has a limit of
    100,000 tokens. This means that users can input longer prompts, namely pages of
    technical documentation or even a book, which do not need to be embedded. Plus,
    the model can also generate longer output compared to other LLMs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 2的另一个特点是上下文长度，其限制为100,000个标记。这意味着用户可以输入更长的提示，例如页面的技术文档或甚至一本书，无需嵌入。此外，与其他LLM相比，该模型还可以生成更长的输出。
- en: Finally, Claude 2 demonstrates relevant capabilities also when working with
    code, scoring 71.2% on the HumanEval benchmark.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Claude 2在处理代码时也展示了相关能力，在HumanEval基准测试中得分71.2%。
- en: '**Definition**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: HumanEval is a benchmark for evaluating the code generation ability of LLMs.
    It consists of 164 human-crafted coding problems in Python, each with a prompt,
    a solution, and a test suite. The problems cover various topics, such as data
    structures, algorithms, logic, math, and string manipulation. The benchmark can
    be used to measure the functional correctness, syntactic validity, and semantic
    coherence of the LLM’s outputs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEval是评估LLM代码生成能力的基准。它由164个由人类编写的Python编码问题组成，每个问题都有一个提示、一个解决方案和一个测试套件。这些问题涵盖了各种主题，如数据结构、算法、逻辑、数学和字符串操作。该基准可以用来衡量LLM输出的功能性正确性、语法有效性和语义一致性。
- en: Overall, Claude 2 is a very interesting model and competitor of GPT-4 to pay
    attention to. It can be consumed via the REST API or directly via the Anthropic
    beta chat experience (limited for US and UK users as of August 2023).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Claude 2是一个非常有趣的模型，也是GPT-4的竞争对手，值得关注。它可以通过REST API或直接通过Anthropic的beta聊天体验（截至2023年8月仅限美国和英国用户）进行消费。
- en: 'The following comparison table shows the main differences between the three
    models:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下比较表展示了三个模型之间的主要区别：
- en: '|  | **GPT-4** | **Gemini** | **Claude 2** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | **GPT-4** | **Gemini** | **Claude 2** |'
- en: '| **Company or institution** | OpenAI | Google | Anthropic |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **公司或机构** | OpenAI | Google | Anthropic |'
- en: '| **First release** | March 2023 | December 2023 | July 2023 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **首次发布** | 2023年3月 | 2023年12月 | 2023年7月 |'
- en: '| **Architecture** | Transformer-based, decoder only | Transformer-based |
    Transformer-based |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **架构** | 基于Transformer，仅解码器 | 基于Transformer | 基于Transformer |'
- en: '| **Sizes and variants** | Parameters not officially specifiedTwo context-length
    variants:GPT-4 8K tokensGPT-4 32K tokens | Three sizes, from smallest to largest:
    Nano, Pro, and Ultra | Not officially specified |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **大小和变体** | 参数未正式公布两种上下文长度变体：GPT-4 8K标记GPT-4 32K标记 | 从最小到最大有三个尺寸：Nano、Pro和Ultra
    | 参数未正式公布 |'
- en: '| **How to use** | REST API at OpenAI developer platformsUsing OpenAI Playground
    at [https://platform.openai.com/playground](https://platform.openai.com/playground)
    | REST API at Google AI StudioUsing Gemini at [https://gemini.google.com/](https://gemini.google.com/)
    | REST API after compiling the form at [https://www.anthropic.com/claude](https://www.anthropic.com/claude)
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **如何使用** | OpenAI开发者平台上的REST API使用OpenAI Playground在[https://platform.openai.com/playground](https://platform.openai.com/playground)
    | Google AI Studio上的REST API使用Gemini在[https://gemini.google.com/](https://gemini.google.com/)
    | 在[https://www.anthropic.com/claude](https://www.anthropic.com/claude)编译表单后的REST
    API |'
- en: 'Table 3.1: Comparison table of GPT-4, PaLM 2, and Claude 2'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1：GPT-4、PaLM 2和Claude 2的比较表
- en: In addition to proprietary models, there is a huge market for open-source LLMs
    available today. Let’s discuss some of these in the next section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了专有模型之外，目前市场上还有大量开源的LLM（大型语言模型）。让我们在下一节中讨论其中的一些。
- en: Open-source models
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源模型
- en: 'The advantage of an open-source model is that, by definition, developers have
    full visibility and access to the source code. In the context of LLMs, this implies
    the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型的优势在于，根据定义，开发者可以完全看到并访问源代码。在LLM的背景下，这意味着以下内容：
- en: You have major control over the architecture, meaning that you can also modify
    it in the local version you are going to use within your project. This also implies
    that they are not prone to potential updates to the source code made by models’
    owners.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你对架构有主要控制权，这意味着你还可以修改你将在项目中使用的本地版本。这也意味着它们不太可能受到模型所有者对源代码进行的潜在更新的影响。
- en: There is the possibility to train your model from scratch, on top of the classical
    fine-tuning, which is also available for proprietary models.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有可能从头开始训练你的模型，除了对专有模型也适用的经典微调之外。
- en: Free to use, meaning that you won’t incur any charge while using those LLMs,
    in contrast with the proprietary ones that have pay-per-use pricing.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 免费使用，这意味着在使用这些LLM时，你不会产生任何费用，与那些按使用付费的专有模型形成对比。
- en: To compare open-source models, throughout this book, we will refer to the independent
    Hugging Face Open LLM Leaderboard (you can find it at [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    a project that aims to evaluate and compare the performance of LLMs on various
    **natural language understanding** (**NLU**) tasks. The project is hosted on Hugging
    Face Spaces, a platform for creating and sharing machine-learning applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较开源模型，在这本书中，我们将参考独立的Hugging Face Open LLM排行榜（你可以在[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)找到），这是一个旨在评估和比较LLM在各种**自然语言理解**（**NLU**）任务上性能的项目。该项目托管在Hugging
    Face Spaces上，这是一个用于创建和分享机器学习应用的平台。
- en: 'The Open LLM Leaderboard uses four main evaluation benchmarks, which we covered
    in *Chapter 1*, in the *Model evaluation* section:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Open LLM排行榜使用四个主要的评估基准，我们在*第一章*的*模型评估*部分进行了介绍：
- en: '**AI2 Reasoning Challenge** (**ARC**): Grade-school science questions and complex
    NLU tasks.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI2推理挑战**（**ARC**）：小学科学问题和复杂的NLU任务。'
- en: '**HellaSwag**: Common sense reasoning.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HellaSwag**：常识推理。'
- en: '**MMLU**: Tasks in various domains, including math, computer science, and law.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MMLU**：包括数学、计算机科学和法律等多个领域的任务。'
- en: '**TruthfulQA**: An evaluation of how truthful the model is when generating
    answers.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TruthfulQA**：评估模型在生成答案时的真实性。'
- en: Even though those are just a subsample of the plethora of LLMs’ benchmarks,
    we will stick to this leaderboard as a reference evaluation framework as it being
    widely adopted.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些只是众多LLM基准测试中的一部分，但我们将坚持使用这个排行榜作为参考评估框架，因为它被广泛采用。
- en: LLaMA-2
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLaMA-2
- en: '**Large Language Model Meta AI 2** (**LLaMA-2**) is a new family of models
    developed by Meta and unveiled to the public on July 18, 2023, open source and
    for free (its first version was originally limited to researchers).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型Meta AI 2**（**LLaMA-2**）是Meta开发的新一代模型，于2023年7月18日向公众公布，开源且免费（其首个版本最初仅限于研究人员）。'
- en: It is an **autoregressive** model with an optimized, decoder-only transformer
    architecture.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个**自回归**模型，具有优化且仅包含解码器的transformer架构。
- en: '**Definition**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: The concept of autoregressive in the context of transformers refers to the fact
    that the model predicts the next token in the sequence, conditioned on all the
    previous tokens. This is done by masking the future tokens in the input so that
    the model can only attend to the past tokens. For example, if the input sequence
    is “The sky is blue,” the model would predict “The” first, then “sky,” then “is,”
    and finally “blue,” using a mask to hide the tokens that come after each prediction.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer的上下文中，自回归的概念指的是模型在给定所有先前标记的条件下预测序列中的下一个标记。这是通过在输入中屏蔽未来标记来完成的，这样模型就只能关注过去标记。例如，如果输入序列是“The
    sky is blue”，模型将首先预测“The”，然后是“sky”，接着是“is”，最后是“blue”，使用掩码隐藏每个预测之后的标记。
- en: 'LLaMA-2 models come in three sizes: 7, 13, and 70 billion parameters. All the
    versions have been trained on 2 trillion tokens and have a context length of 4,092
    tokens.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-2模型有三种大小：70亿、130亿和700亿参数。所有版本都在2000万亿标记上训练，上下文长度为4092个标记。
- en: On top of that, all model sizes come with a “chat” version, called LLaMA-2-chat,
    which is more versatile for general-purpose conversational scenarios compared
    to the base model LLama-2.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，所有模型大小都附带一个“聊天”版本，称为LLaMA-2-chat，与基础模型LLama-2相比，它在通用对话场景中更加灵活。
- en: '**Note**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: 'In the context of LLMs, the difference between **base models** and “chat” or
    **assistant models** is primarily in their training and intended use:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的背景下，**基础模型**与“聊天”或**助手模型**之间的区别主要在于它们的训练和预期用途：
- en: 'Base models: These models are trained on vast amounts of text data, often sourced
    from the internet, and their primary function is to predict the next word in a
    given context, which makes them great at understanding and generating language.
    However, they might not always be precise or focused on specific instructions.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型：这些模型在大量的文本数据上训练，通常来自互联网，其主要功能是在给定上下文中预测下一个单词，这使得它们在理解和生成语言方面非常出色。然而，它们可能并不总是精确或专注于特定指令。
- en: 'Assistant models: These models start as base LLMs but are further fine-tuned
    with input-output pairs that include instructions and the model’s attempts to
    follow those instructions. They often employ RLHF to refine the model, making
    it better at being helpful, honest, and harmless. As a result, they are less likely
    to generate problematic text and are more suitable for practical applications
    like chatbots and content generation. For example, the assistant model GPT-3.5
    Turbo (the model behind ChatGPT) is a fine-tuned version of the completion model
    GPT-3.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 助手模型：这些模型最初是基础LLM，但通过包含指令和模型尝试遵循这些指令的输入输出对进行进一步微调。它们通常使用RLHF来细化模型，使其更擅长提供帮助、诚实和无害。因此，它们不太可能生成有问题的文本，更适合聊天机器人内容和内容生成等实际应用。例如，助手模型GPT-3.5
    Turbo（ChatGPT背后的模型）是完成模型GPT-3的微调版本。
- en: In essence, while base models provide a broad understanding of language, assistant
    models are optimized to follow instructions and provide more accurate and contextually
    relevant responses.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，虽然基础模型提供了对语言的广泛理解，但助手模型经过优化以遵循指令并提供更准确和上下文相关的响应。
- en: 'LLaMA-2-chat was developed with a fine-tuning process that consisted of two
    main steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-2-chat是通过一个包含两个主要步骤的微调过程开发的：
- en: '**Supervised fine-tuning**: This step involves fine-tuning the model on publicly
    available instruction datasets and over 1 million human annotations, to make them
    more helpful and safe for conversational use cases. The fine-tuning process uses
    a selected list of prompts to guide the model outputs, and a loss function that
    encourages diversity and relevance (that’s the reason why it is “supervised”).'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监督微调**：这一步骤涉及在公开可用的指令数据集和超过100万人的标注上进行模型微调，使其在对话用例中更有帮助和安全。微调过程使用所选提示列表来引导模型输出，并使用一个鼓励多样性和相关性的损失函数（这就是为什么它被称为“监督”）。'
- en: '**RLHF**: As we saw while introducing GPT-4, RLHF is a technique that aims
    at using human feedback as an evaluating metric for LLMs’ generated output, and
    then using that feedback to further optimize the model.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**RLHF**：正如我们在介绍GPT-4时所看到的，RLHF是一种旨在使用人类反馈作为评估LLM生成输出的指标的技术，然后使用该反馈进一步优化模型。'
- en: 'The following is an illustration of how the training process for LLaMA works:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何进行LLaMA训练过程的说明：
- en: '![](img/B21714_03_08.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_03_08.png)'
- en: 'Figure 3.8: Two-step fine-tuning to obtain LLaMa-2 chat (source: [https://ai.meta.com/resources/models-and-libraries/llama/](https://ai.meta.com/resources/models-and-libraries/llama/))'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：两步微调以获得 LLaMa-2 聊天（来源：[https://ai.meta.com/resources/models-and-libraries/llama/](https://ai.meta.com/resources/models-and-libraries/llama/))
- en: 'To access the model, you need to submit a request on Meta’s website (the form
    is available at [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)).
    Once a request is submitted, you will receive an email with the GitHub repository
    where you will be able to download the following assets:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该模型，您需要在 Meta 的网站上提交一个请求（表格可在 [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
    找到）。一旦提交请求，您将收到一封电子邮件，其中包含您将能够下载以下资产的 GitHub 仓库：
- en: Model code
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型代码
- en: Model weights
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型权重
- en: README (User Guide)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: README（用户指南）
- en: Responsible Use Guide
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责任的使用指南
- en: License
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许可证
- en: Acceptable Use Policy
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可接受的使用政策
- en: Model Card
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型卡
- en: Falcon LLM
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Falcon LLM
- en: Falcon LLM is a representation of a new trend of LLMs, consisting of building
    lighter models (with fewer parameters) and focusing rather on the quality of the
    training dataset. Indeed, it is a matter of fact that complex models like GPT-4
    with trillions of parameters are extremely heavy, both in the training phase and
    inference phase. This implies the need for high and expensive computational power
    (GPU and TPU-powered) as well as a long training time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon LLM 是 LLM 的一种新趋势的表示，包括构建更轻的模型（参数更少）并专注于训练数据集的质量。事实上，这是一个事实，即像 GPT-4 这样具有万亿参数的复杂模型在训练阶段和推理阶段都非常沉重。这暗示了需要高且昂贵的计算能力（GPU
    和 TPU 驱动）以及漫长的训练时间。
- en: Falcon LLM is an open-source model launched by Abu Dhabi’s **Technology Innovation
    Institute** (**TII**) in May 2023\. It is an autoregressive, decoder-only transformer,
    trained on 1 trillion tokens, and it has 40 billion parameters (even though it
    has also been released as a lighter version with 7 billion parameters). Similarly
    to what we saw for LlaMA, Falcon LLM also comes with a fine-tuned variant, called
    “Instruct,” which is tailored toward following the user’s instructions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon LLM 是阿布扎比的 **技术创新研究所**（**TII**）于 2023 年 5 月推出的开源模型。它是一个自回归的、仅解码器的变压器，在
    1 万亿个标记上训练，拥有 40 亿个参数（尽管它也以 70 亿参数的轻量版本发布）。与 LlaMA 类似，Falcon LLM 还附带了一个经过微调的变体，称为“Instruct”，该变体旨在遵循用户的指令。
- en: '**Definition**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Instruct models are specialized for short-form instruction following. Instruction
    following is a task where the model has to execute a natural language command
    or query, such as “write a haiku about cats” or “tell me about the weather in
    Paris.” The Instruct fine-tuned models are trained on a large dataset of instructions
    and their corresponding outputs, such as the Stanford Alpaca dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Instruct 模型专门用于遵循短格式指令。指令遵循是一个任务，其中模型必须执行一个自然语言命令或查询，例如“写一首关于猫的俳句”或“告诉我巴黎的天气。”Instruct
    微调模型是在一个包含指令及其相应输出的大型数据集上训练的，例如斯坦福 Alpaca 数据集。
- en: According to the Open LLM leaderboard, since its launch, Falcon LLM has been
    among the first positions globally, second only to some versions of LlaMA.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 根据开放 LLM 排行榜，自发布以来，Falcon LLM 一直位居全球前列，仅次于一些 LlaMA 的版本。
- en: 'So, the question might be: how can a model with “only” 40 billion parameters
    perform so well? In fact, the answer is in the quality of the dataset. Falcon
    was developed using specialized tools and incorporates a unique data pipeline,
    which is capable of extracting valuable content from web data. The pipeline was
    designed to extract high-quality content by employing extensive filtering and
    deduplication techniques. The resulting dataset, called *RefinedWeb*, has been
    released by TII under the Apache-2.0 license and can be found at [https://huggingface.co/datasets/tiiuae/falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，问题可能就是：一个“仅有”40亿参数的模型是如何表现得如此出色的？实际上，答案在于数据集的质量。Falcon 是使用专用工具开发的，并集成了独特的数据管道，能够从网络数据中提取有价值的内容。该管道通过采用广泛的过滤和去重技术来设计，以提取高质量的内容。由此产生的数据集，称为
    *RefinedWeb*，已由 TII 在 Apache-2.0 许可下发布，可在 [https://huggingface.co/datasets/tiiuae/falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
    找到。
- en: By combining superior data quality with these optimizations, Falcon achieves
    remarkable performance while utilizing around 75% and 80% of the training compute
    budget of GPT-3 and PaLM-62B, respectively.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合优质数据与这些优化，Falcon在利用大约75%和80%的GPT-3和PaLM-62B的训练计算预算的同时，实现了显著的性能。
- en: Mistral
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Mistral
- en: The third and last open-source model series we are going to cover is Mistral,
    developed by Mistral AI, a company founded in April 2023 by a team of AI scientists
    who previously worked at Meta Platforms and Google DeepMind. Based in France,
    the company has quickly made a name for itself by raising significant funding
    and releasing open-source LLMs, emphasizing the importance of transparency and
    accessibility in AI development.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的第三个也是最后一个开源模型系列是Mistral，由Mistral AI开发，该公司成立于2023年4月，由一支曾在Meta Platforms和Google
    DeepMind工作的AI科学家团队创立。总部位于法国，该公司通过筹集大量资金和发布开源LLM，迅速在AI开发领域树立了声誉，强调透明性和可访问性的重要性。
- en: The Mistral model, particularly the Mistral-7B-v0.1, is a decoder-only transformer
    with 7.3 billion parameters, designed for generative text tasks. It’s known for
    its innovative architecture choices like **grouped-query attention** (**GQA**)
    and **sliding-window attention** (**SWA**), which have allowed it to outperform
    other models in benchmarks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral模型，特别是Mistral-7B-v0.1，是一个仅具有解码器的Transformer，拥有73亿参数，专为生成文本任务设计。它以其创新的架构选择而闻名，如**分组查询注意力**（**GQA**）和**滑动窗口注意力**（**SWA**），这些选择使它在基准测试中优于其他模型。
- en: '**Definition**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: GQA and SWA are mechanisms designed to improve the efficiency and performance
    of an LLM.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: GQA和SWA是旨在提高LLM效率和性能的机制。
- en: GQA is a technique that allows for faster inference times compared to standard
    full attention mechanisms. It does this by partitioning the attention mechanism’s
    query heads into groups, with each group sharing a single key head and value head.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: GQA是一种技术，与标准全注意力机制相比，它允许更快的推理时间。它是通过将注意力机制的查询头部分成组，每组共享一个单一的关键头和价值头来实现的。
- en: SWA is used to handle longer text sequences efficiently. It extends the model’s
    attention beyond a fixed window size, allowing each layer to reference a range
    of positions from the preceding layer. This means that the hidden state at a certain
    position in one layer can attend to hidden states within a specific range in the
    previous layer, thus enabling the model to access tokens at a greater distance
    and manage sequences of varying lengths with a reduced inference cost.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: SWA用于高效地处理较长的文本序列。它将模型的注意力扩展到固定窗口大小之外，允许每一层引用前一层的多个位置。这意味着某一层的隐藏状态可以关注前一层特定范围内的隐藏状态，从而使得模型能够访问更远距离的标记，并以降低推理成本的方式管理不同长度的序列。
- en: The model also provides a variant that was fine-tuned for general-purpose capabilities.
    This variant is called Mistral-7B-instruct, which outperformed all other 7 billion
    LLMs on the market (as of April 2024) on MT-Bench (an evaluation framework that
    uses an LLM as a judge).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还提供了一个针对通用能力进行了微调的变体。这个变体被称为Mistral-7B-instruct，它在MT-Bench（一个使用LLM作为评判标准的评估框架）上优于市场上所有其他70亿参数的LLM（截至2024年4月）。
- en: Like many other open-source models, Mistral can be consumed and downloaded via
    Hugging Face Hub.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他开源模型一样，Mistral可以通过Hugging Face Hub进行消费和下载。
- en: '**Note**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: In February 2024, Mistral AI and Microsoft entered a multi-year partnership
    to accelerate AI innovation. This collaboration will leverage Microsoft’s Azure
    AI supercomputing infrastructure to support the development and deployment of
    Mistral AI’s LLMs. Mistral AI’s models, including their advanced model, Mistral
    Large, will be available to customers through Azure AI Studio and Azure Machine
    Learning model catalog. The partnership aims to expand Mistral AI’s reach to global
    markets and foster ongoing research collaboration.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年2月，Mistral AI和微软签署了一项多年合作协议，以加速AI创新。这次合作将利用微软的Azure AI超级计算基础设施来支持Mistral
    AI的LLM的开发和部署。Mistral AI的模型，包括其高级模型Mistral Large，将通过Azure AI Studio和Azure机器学习模型目录提供给客户。该合作的目标是扩大Mistral
    AI在全球市场的影响力，并促进持续的研究合作。
- en: 'The following comparison table provides the main differences between the three
    models:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的比较表提供了三个模型之间的主要区别：
- en: '|  | **LlaMA** | **Falcon LLM** | **Mistral** |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | **LlaMA** | **Falcon LLM** | **Mistral** |'
- en: '| **Company or institution** | Meta | **Technology Innovation Institute** (**TII**)
    | Mistral AI |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **公司或机构** | Meta | **技术创新研究所**（**TII**） | Mistral AI |'
- en: '| **First release** | July 2023 | May 2023 | September 2023 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **首次发布** | 2023年7月 | 2023年5月 | 2023年9月 |'
- en: '| **Architecture** | Autoregressive transformer, decoder-only | Autoregressive
    transformer, decoder-only | Transformer, decoder only |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **架构** | 自回归变换器，仅解码器 | 自回归变换器，仅解码器 | 变换器，仅解码器 |'
- en: '| **Sizes and variants** | Three sizes: 7B, 13B, and 70B, alongside the fine-tuned
    version (chat) | Two sizes: 7B and 40B, alongside the fine-tuned version (instruct)
    | 7B size alongside the fine-tuned version (instruct) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **大小和变体** | 三种大小：7B、13B和70B，以及微调版本（聊天） | 两种大小：7B和40B，以及微调版本（指令） | 7B大小以及微调版本（指令）
    |'
- en: '| **Licenses** | A custom commercial license is available at [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
    | Commercial Apache 2.0 licensed | Commercial Apache 2.0 licensed |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **许可证** | 可在[https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)获取定制商业许可证
    | 商业Apache 2.0许可证 | 商业Apache 2.0许可证 |'
- en: '| **How to use** | Submit request form at [https://ai.meta.com/resources/models-and-libraries/llama-downloads/
    and download the GitHub repo](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)Also
    available in Hugging Face Hub | Download or use Hugging Face Hub Inference API/Endpoint
    | Download or use Hugging Face Hub Inference API/Endpoint or Azure AI Studio |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **如何使用** | 在[https://ai.meta.com/resources/models-and-libraries/llama-downloads/提交请求表单并下载GitHub仓库](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)，也可在Hugging
    Face Hub使用 | 下载或使用Hugging Face Hub推理API/端点 | 下载或使用Hugging Face Hub推理API/端点或Azure
    AI Studio |'
- en: 'Table 3.2: Comparison table of LLMs'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2：LLM比较表
- en: Beyond language models
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越语言模型
- en: So far, we have only been covering language-specific foundation models as they
    are the focus of this book. Nevertheless, in the context of AI-powered applications,
    it is worth mentioning that there are additional foundation models that can handle
    data that is different from text, which can be embedded and orchestrated.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只介绍了语言特定的基础模型，因为这是本书的重点。然而，在AI驱动应用的环境中，值得提及的是，还有一些可以处理不同于文本的数据的基础模型，这些数据可以被嵌入和编排。
- en: 'Here, you can find some examples of **large foundation models** (**LFMs**)
    on the market today:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以找到一些市场上**大型基础模型**（**LFMs**）的例子：
- en: '**Whisper**: It is a general-purpose speech recognition model developed by
    OpenAI that can transcribe and translate speech in multiple languages. It is trained
    on a large dataset of diverse audio and is also a multitasking model that can
    perform multilingual speech recognition, speech translation, spoken language identification,
    and voice activity detection.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Whisper**：这是一个由OpenAI开发的多语言通用语音识别模型，能够将多种语言的语音进行转录和翻译。它在一个包含多种音频的大型数据集上进行了训练，并且还是一个多任务模型，能够执行多语言语音识别、语音翻译、口语语言识别和语音活动检测。'
- en: '**Midjourney**: Developed by the independent research lab of the same name,
    Midjourney is based on a sequence-to-sequence transformer model that takes text
    prompts and outputs a set of four images that match the prompts. Midjourney is
    designed to be a tool for artists and creative professionals, who can use it for
    rapid prototyping of artistic concepts, inspiration, or experimentation.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Midjourney**：由同名独立研究实验室开发，Midjourney基于一个序列到序列的变换器模型，它接受文本提示并输出一组与提示相匹配的四张图像。Midjourney被设计成艺术家和创意专业人士的工具，他们可以使用它进行艺术概念的快速原型设计、灵感或实验。'
- en: '**DALL-E**: Similar to the previous one, DALL-E, developed by OpenAI, generates
    images from natural language descriptions, using a 12-billion parameter version
    of GPT-3 trained on a dataset of text-image pairs.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DALL-E**：与之前的类似，DALL-E是由OpenAI开发的，它使用GPT-3的120亿参数版本，在文本-图像对的数据集上训练，从自然语言描述生成图像。'
- en: 'The idea is that we can combine and orchestrate multiple LFMs within our applications
    to achieve extraordinary results. For example, let’s say we want to write a review
    about an interview with a young chef and post it on Instagram. The involved models
    might be the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 想法是我们可以在我们的应用程序中将多个LFMs组合和编排，以实现非凡的结果。例如，假设我们想要写一篇关于年轻厨师访谈的评论并发布在Instagram上。可能涉及的模型可能如下：
- en: '**Whisper** will convert the interview audio into a transcript.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Whisper**会将访谈音频转换为文本。'
- en: An **LLM**, such as Falcon-7B-instruct, with a web plugin, will extrapolate
    the name of the young chef and search it on the internet to retrieve the biography.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个带有网络插件的**LLM**，例如Falcon-7B-instruct，将推断年轻厨师的名字并在互联网上搜索以检索其传记。
- en: Another **LLM**, such as LlaMA, will process the transcript and generate a review
    with an Instagram post style. We can also ask the same model to generate a prompt
    that will ask the following model to generate a picture based on the post content.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个**LLM**，例如LlaMA，将处理转录并生成一个具有Instagram帖子风格的评论。我们也可以要求同一个模型生成一个提示，让下一个模型根据帖子内容生成图片。
- en: '**Dall-E** will generate an image based on the prompt generated by the LLM.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dall-E**将根据LLM生成的提示生成图像。'
- en: We will then provide our LFMs flow with an Instagram plugin so that the application
    is able to post the whole review, including the illustration, on our profile.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为我们的LFMs流程添加一个Instagram插件，以便应用程序能够在我们个人资料上发布整个评论，包括插图。
- en: Finally, there are emerging LFMs that are meant to be multi-modal, meaning that
    they can handle multiple data formats with just one architecture. An example is
    GPT-4 itself.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有一些新兴的LFMs旨在实现多模态，这意味着它们可以用一个架构处理多种数据格式。GPT-4本身就是一个例子。
- en: 'The following screenshot shows an example of an early OpenAI experiment with
    GPT-4 visuals, demonstrating its understanding of funny aspects within an image:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了OpenAI早期与GPT-4视觉实验的一个示例，展示了它对图像中幽默方面的理解：
- en: '![A cell phone with a cable plugged into it  Description automatically generated](img/B21714_03_09.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![手机插上充电线  自动生成的描述](img/B21714_03_09.png)'
- en: 'Figure 3.9: Early experiments with GPT-4 visuals (source: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4))'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9：GPT-4视觉的早期实验（来源：[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)）
- en: 'The following screenshot shows another example of an earlier version of GPT-4,
    demonstrating how it could understand and explain graphs in detail:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了GPT-4的早期版本的一个示例，展示了它如何能够详细理解和解释图表：
- en: '![A screenshot of a graph  Description automatically generated](img/B21714_03_10.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图表截图  自动生成的描述](img/B21714_03_10.png)'
- en: 'Figure 3.10: Early experiments with GPT-4 visuals (source: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4))'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：GPT-4视觉的早期实验（来源：[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)）
- en: 'The following example shows how an early version of GPT-4 could understand
    and solve complex mathematical problems while also providing the corresponding
    justification for its response:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例显示了GPT-4的早期版本如何理解和解决复杂的数学问题，同时提供其响应的相应理由：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_03_11.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成的描述](img/B21714_03_11.png)'
- en: 'Figure 3.11: Early experiments with GPT-4 visuals (source: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4))'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：GPT-4视觉的早期实验（来源：[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)）
- en: GPT-4 is just one example of a **large multimodal model** (**LMM**), and it
    is representative of the trend that we will probably witness in the next few years.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4只是**大型多模态模型**（LMM）的一个例子，它代表了我们在未来几年可能会见证的趋势。
- en: A decision framework to pick the right LLM
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的LLM的决策框架
- en: 'In previous paragraphs, we covered some of the most promising LLMs available
    in the market today. Now, the question is: which one should I use within my applications?
    The truth is that there is not a straightforward answer to this question.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的段落中，我们介绍了一些目前市场上最有前途的LLM。现在的问题是：在我的应用程序中我应该使用哪一个？事实是，这个问题没有直接的答案。
- en: Considerations
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑事项
- en: 'There are many factors to consider when choosing an LLM for your application.
    Those factors also need to be declined in two scenarios: proprietary and open-source
    LLMs. The following are some factors and trade-offs you might want to consider
    while choosing your LLMs:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择适用于您的应用程序的LLM时，有许多因素需要考虑。这些因素还需要在两种情况下进行说明：专有和开源LLM。以下是一些在选择LLM时您可能想要考虑的因素和权衡：
- en: '**Size and performance**: We saw that more complex models (that means, with
    a high number of parameters) tend to have better performance, especially in terms
    of parametric knowledge and generalization capabilities. Nevertheless, the larger
    the model, the more computation and memory it requires to process the input and
    generate the output, which can result in higher latency and, as we will see, higher
    costs.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大小和性能**：我们注意到，更复杂的模型（这意味着具有高参数数量）往往具有更好的性能，特别是在参数知识和泛化能力方面。然而，模型越大，处理输入和生成输出所需的计算和内存就越多，这可能导致更高的延迟，正如我们将看到的，还有更高的成本。'
- en: '**Cost and hosting strategy**: When incorporating LLMs within our applications,
    there are two types of costs we have to keep in mind:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本和托管策略**：在我们将LLM整合到我们的应用程序中时，我们必须考虑两种类型的成本：'
- en: '**Cost for model consumption**: This refers to the fee we pay to consume the
    model. Proprietary models like GPT-4 or Claude 2 require a fee, which is typically
    proportional to the number of tokens processed. On the other hand, open-source
    models like LlaMA or Falcon LLM are free to use.'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型消费成本**：这指的是我们为消费模型所支付的费用。像GPT-4或Claude 2这样的专有模型需要支付费用，这通常与处理的令牌数量成比例。另一方面，像LlaMA或Falcon
    LLM这样的开源模型是免费使用的。'
- en: '**Cost for model hosting**: This refers to your hosting strategy. Typically,
    proprietary models are hosted in a private or public hyperscaler, so that they
    can be consumed via a REST API and you don’t have to worry about the underlying
    infrastructure (for example, GPT-4 is hosted in a super-computer built in the
    Microsoft Azure cloud). With open-source models, we typically need to provide
    our own infrastructure, since those models can be downloaded locally. Of course,
    the larger the model, the more powerful the computational power needed.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型托管成本**：这指的是您的托管策略。通常，专有模型托管在私有或公共超大规模计算器中，以便通过REST API进行消费，您无需担心底层基础设施（例如，GPT-4托管在微软Azure云中构建的超计算机）。对于开源模型，我们通常需要提供自己的基础设施，因为那些模型可以本地下载。当然，模型越大，所需的计算能力就越强。'
- en: '**Note**'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意**'
- en: In the context of open-source models, another option to consume those models
    is that of using the Hugging Face Inference API. The free version allows you to
    test and evaluate, with a limited rate, all the available LLMs on a shared infrastructure
    hosted on Hugging Face. For production use cases, Hugging Face also offers Inference
    Endpoints, so that you can easily deploy your LLMs on a dedicated and fully managed
    infrastructure, with the possibility to configure parameters like region, compute
    power, and security level to accommodate your constraints in terms of latency,
    throughput, and compliance.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在开源模型的背景下，另一种消费这些模型的方式是使用Hugging Face推理API。免费版本允许您以有限的速率在Hugging Face上托管的共享基础设施上测试和评估所有可用的LLM。对于生产用例，Hugging
    Face还提供推理端点，这样您就可以轻松地将LLM部署在专用且完全管理的基础设施上，并有可能配置区域、计算能力和安全级别等参数，以满足您在延迟、吞吐量和合规性方面的限制。
- en: Pricing for the Inference Endpoint is publicly available at [https://huggingface.co/docs/inference-endpoints/pricing](https://huggingface.co/docs/inference-endpoints/pricing).
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理端点的定价信息可在[https://huggingface.co/docs/inference-endpoints/pricing](https://huggingface.co/docs/inference-endpoints/pricing)公开获取。
- en: '**Customization**: This might be a requirement you want to evaluate before
    deciding which model to adopt. In fact, not all models are equally flexible in
    terms of customization. When we talk about customization, we refer to two activities:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制**：这可能是在决定采用哪种模型之前想要评估的要求。实际上，并非所有模型在定制方面都同样灵活。当我们谈论定制时，我们指的是两种活动：'
- en: '**Fine-tuning**: This is the process of slightly adjusting LLMs’ parameters
    to better fit into a domain. All open-source models can be fine-tuned. When it
    comes to proprietary models, not all LLMs can be fine-tuned: for example, OpenAI’s
    GPT-3.5 can be fine-tuned, while the process of fine-tuning the GPT-4-0613 is
    still experimental and accessible under request to OpenAI (as per December 2023).'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：这是略微调整LLM参数以更好地适应特定领域的过程。所有开源模型都可以进行微调。当涉及到专有模型时，并非所有LLM都可以进行微调：例如，OpenAI的GPT-3.5可以进行微调，而GPT-4-0613的微调过程仍然是实验性的，并且需要根据OpenAI的要求进行（截至2023年12月）。'
- en: Henceforth, it is important to understand whether you will need fine-tuning
    in your application and decide accordingly.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从此以后，了解您是否需要在应用程序中进行微调，并据此做出决定是很重要的。
- en: '**Training from scratch**: If you really want an LLM that is super specific
    about your domain knowledge, you might want to retrain the model from scratch.
    To train an LLM from scratch, without having to reinvent an architecture, you
    can download open-source LLMs and simply re-train them on custom datasets. Of
    course, this implies that we have access to the source code, which is not the
    case when we work with proprietary LLMs.'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从头开始训练**：如果您真的需要一个对您的领域知识非常具体的LLM，您可能需要从头开始重新训练模型。要从头开始训练一个LLM，而无需重新发明架构，您可以下载开源LLM，并在自定义数据集上简单地重新训练它们。当然，这隐含着我们有权访问源代码，而我们与专有LLM合作时并非如此。'
- en: '**Domain-specific capabilities**: We saw that the most popular way of evaluating
    LLMs’ performance is that of averaging different benchmarks across domains. However,
    there are benchmarks that are tailored towards specific capabilities: if MMLU
    measures LLMs’ generalized culture and commonsense reasoning, TruthfulQA is more
    concerned with LLMs’ alignment, while HumanEval is tailored towards LLMs’ coding
    capabilities.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特定领域的能力**：我们注意到评估LLM性能最流行的方式是跨领域平均不同的基准测试。然而，也有一些基准测试是针对特定能力量身定制的：如果MMLU衡量LLM的泛化文化和常识推理，那么TruthfulQA更关注LLM的对齐，而HumanEval则针对LLM的编码能力。'
- en: Henceforth, if you have a tailored use case in mind, you might want to use a
    model that is a top performer in one specific benchmark, rather than a top performer,
    on average, across all benchmarks. Namely, you might pick Claude 2 if you are
    looking for exceptional coding capabilities, or PaLM 2 if analytical reasoning
    is what you are looking for. On the other hand, if you need a model that encompasses
    all of these capabilities, GPT-4 might be the right choice for you.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从此以后，如果您有一个特定的用例在心中，您可能希望使用在特定基准测试中表现最佳的模型，而不是在所有基准测试中平均表现最佳的模型。也就是说，如果您在寻找卓越的编码能力，可能会选择Claude
    2；如果您在寻找分析推理能力，可能会选择PaLM 2。另一方面，如果您需要一个包含所有这些能力的模型，GPT-4可能是您的正确选择。
- en: Picking a domain-specific model is also a way to make some savings in terms
    of model complexity. The thing is, it might be sufficient for you to use a relatively
    small model (for example, a LlaMA-7B-instruct) if you need to use it for a specific
    use case, which comes with all the benefits in terms of cost and performance.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 选择特定领域的模型也是节省模型复杂度的一种方式。问题是，如果您需要将其用于特定用例，可能只需要一个相对较小的模型（例如，LlaMA-7B-instruct），这样就可以带来成本和性能方面的所有好处。
- en: '**Note**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: If you are looking for LLMs that are *extremely* specific, there is a plethora
    of models that have been trained on domain-specific technical documentation. For
    example, at the beginning of 2023, the **Stanford** **Center for Research on Foundation
    Models** (**CRFM)** and MosaicML announced the release of BioMedLM, a decoder-only
    transformer-based LLM with 2.7 billion parameters, trained on biomedical abstracts
    and papers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找**极其**具体的LLM，有许多模型是在特定领域的技术文档上训练的。例如，2023年初，斯坦福大学**基础模型研究中心**（**CRFM**）和MosaicML宣布发布了BioMedLM，这是一个仅具有解码器功能的基于Transformer的2.7亿参数LLM，它在生物医学摘要和论文上进行了训练。
- en: Another example is BloombergGPT, a 50 billion parameter LLM specialized for
    the financial domain developed by Bloomberg and trained on a 363 billion token
    dataset based on Bloomberg’s extensive data sources, perhaps the largest domain-specific
    dataset yet, augmented with 345 billion tokens from general purpose datasets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是BloombergGPT，这是一个由彭博社开发的针对金融领域的50亿参数大型语言模型（LLM），它基于彭博社广泛的数据源，在包含3630亿个标记的数据集上进行训练，这可能是迄今为止最大的特定领域数据集，并额外增加了来自通用数据集的3450亿个标记。
- en: To make this decision framework more practical, let’s consider the following
    imaginary case study about the company TechGen.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个决策框架更加实用，让我们考虑以下关于TechGen公司的假设案例研究。
- en: Case study
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究
- en: 'TechGen Solutions, a leading provider of AI-driven analytics, face a decision
    between two advanced language models for their next-generation customer interaction
    system: GPT-4 and LLaMa-2\. They require a robust language model that can handle
    diverse customer queries, provide accurate technical information, and integrate
    with their proprietary software. The following are their options:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: TechGen Solutions，一家领先的AI驱动分析服务提供商，在为下一代客户交互系统选择两个高级语言模型（GPT-4和LLaMa-2）之间面临抉择。他们需要一个能够处理各种客户查询、提供准确技术信息并能与他们的专有软件集成的强大语言模型。以下是他们可选的方案：
- en: 'GPT-4: Developed by OpenAI, GPT-4 is known for its vast parameter count and
    the ability to process both text and image inputs'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4：由OpenAI开发，GPT-4以其庞大的参数数量和能够处理文本和图像输入的能力而闻名。
- en: 'LLama 2: Created by Meta AI, LLama 2 is an open-source model praised for its
    accessibility and performance on a smaller dataset.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLama 2：由Meta AI创建，LLama 2是一个开源模型，因其易用性和在小数据集上的性能而受到赞誉。
- en: 'The following are the factors that they consider when making their decision:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是他们做出决策时考虑的因素：
- en: 'Performance: TechGen evaluates the models’ performance, particularly in generating
    technical content and code, where GPT-4 has shown higher accuracy.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能：TechGen评估了这些模型的表现，特别是在生成技术内容和代码方面，GPT-4表现出了更高的准确性。
- en: 'Integration: The ease of integration with TechGen’s systems is critical, with
    GPT-4 potentially offering more seamless compatibility due to its widespread adoption.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成：与TechGen系统的集成简便性至关重要，GPT-4由于其广泛采用，可能提供更无缝的兼容性。
- en: 'Cost: While LLama 2 is free for commercial use under certain conditions, GPT-4
    comes with a cost, which TechGen must factor into their decision.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本：虽然LLama 2在特定条件下对商业用途免费，但GPT-4是有成本的，TechGen必须将其纳入他们的决策考量。
- en: 'Future-proofing: TechGen considers the long-term viability of each model, including
    the potential for updates and improvements.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来保障：TechGen考虑了每个模型的长期可行性，包括更新和改进的潜力。
- en: Based on these considerations, TechGen opts for GPT-4, swayed by its superior
    performance in generating complex, technical responses and its multilingual capabilities,
    which align with their international expansion plans. The decision is also influenced
    by GPT-4’s image processing feature, which TechGen anticipates will become increasingly
    relevant as they incorporate more multimedia content into their customer service.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些考虑，TechGen选择了GPT-4，受其生成复杂、技术性回应的优越性能和其多语言能力所吸引，这些能力与他们的国际扩张计划相吻合。这一决定还受到GPT-4图像处理功能的影响，TechGen预计随着他们将其更多的多媒体内容融入客户服务，这一功能将变得越来越重要。
- en: TechGen’s choice of GPT-4 over LLama 2 is driven by the need for a high-performing,
    versatile language model that can scale with their growing global presence and
    diverse customer needs. While LLama 2’s open-source nature and cost effectiveness
    are appealing, GPT-4’s advanced capabilities and future-proof features present
    a more compelling case for TechGen’s ambitious goals.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: TechGen选择GPT-4而非LLama 2，是由其对一个高性能、多功能的语言模型的需求所驱动，该模型能够随着其日益增长的全球影响力和多样化的客户需求而扩展。虽然LLama
    2的开源特性和成本效益颇具吸引力，但GPT-4的高级功能和未来保障特性为TechGen的雄心勃勃目标提供了更有说服力的理由。
- en: Note that these decision factors are not meant to be an exhaustive guide to
    deciding which models to embed within applications. Nevertheless, those are useful
    elements of reflection while setting up your application flow, so that you can
    determine your requirements and then shortlist those LLMs that are more suitable
    for your goals.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些决策因素并非旨在成为决定在应用中嵌入哪些模型的详尽指南。尽管如此，这些因素在设置你的应用流程时仍是有用的反思元素，这样你可以确定你的需求，然后筛选出那些更适合你目标的LLM。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter covered some of the most promising LLMs in the market. It first
    differentiated between proprietary and open-source models, with all the related
    pros and cons. It then offered a deep dive into the architecture and technical
    features of GPT-4, PaLM-2, Claude 2, LLaMa-2, Falcon LLM, and MPT, with the addition
    of a section covering some LMMs. Finally, it provided a light framework to help
    developers decide which LLMs to pick while building AI-powered applications. This
    is pivotal to get the greatest impact from your application, given your industry-specific
    scenario.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了市场上一些最有前景的LLM。它首先区分了专有模型和开源模型，并详细讨论了所有相关的优缺点。接着深入探讨了GPT-4、PaLM-2、Claude
    2、LLaMa-2、Falcon LLM和MPT的架构和技术特性，并增加了一个关于一些LMM的章节。最后，提供了一个框架，帮助开发者决定在构建AI应用时选择哪些LLM。鉴于你的行业特定场景，这对于从你的应用中获得最大影响至关重要。
- en: Starting from the next chapter, we will start working hands-on with LLMs within
    applications.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从下一章开始，我们将开始在实际应用中与LLM进行动手操作。
- en: References
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: GPT-4 Technical Report. [https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4技术报告。[https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf)
- en: 'Train short, test long: attention with linear biases enables input length extrapolation.
    [https://arxiv.org/pdf/2108.12409.pdf](https://arxiv.org/pdf/2108.12409.pdf)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练短，测试长：线性偏差的注意力使输入长度外推成为可能。[https://arxiv.org/pdf/2108.12409.pdf](https://arxiv.org/pdf/2108.12409.pdf)
- en: 'Constitutional AI: Harmlessness from AI Feedback. [https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宪法 AI：从 AI 反馈中确保无害。[https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073)
- en: Hugging Face Inference Endpoint. [https://huggingface.co/docs/inference-endpoints/index](https://huggingface.co/docs/inference-endpoints/index)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 推理端点。[https://huggingface.co/docs/inference-endpoints/index](https://huggingface.co/docs/inference-endpoints/index)
- en: Hugging Face Inference Endpoint Pricing. [https://huggingface.co/docs/inference-endpoints/pricing](https://huggingface.co/docs/inference-endpoints/pricing)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 推理端点定价。[https://huggingface.co/docs/inference-endpoints/pricing](https://huggingface.co/docs/inference-endpoints/pricing)
- en: Model Card for BioMedLM 2.7B. [https://huggingface.co/stanford-crfm/BioMedLM](https://huggingface.co/stanford-crfm/BioMedLM)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BioMedLM 2.7B 模型卡片。[https://huggingface.co/stanford-crfm/BioMedLM](https://huggingface.co/stanford-crfm/BioMedLM)
- en: PaLM 2 Technical Report. [https://ai.google/static/documents/palm2techreport.pdf](https://ai.google/static/documents/palm2techreport.pdf)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaLM 2 技术报告。[https://ai.google/static/documents/palm2techreport.pdf](https://ai.google/static/documents/palm2techreport.pdf)
- en: Solving Quantitative Reasoning Problems with Language Models. [https://arxiv.org/abs/2206.14858](https://arxiv.org/abs/2206.14858)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语言模型解决定量推理问题。[https://arxiv.org/abs/2206.14858](https://arxiv.org/abs/2206.14858)
- en: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. [https://arxiv.org/abs/2306.05685](https://arxiv.org/abs/2306.05685
    )
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MT-Bench 和聊天机器人竞技场评估 LLM 作为裁判。[https://arxiv.org/abs/2306.05685](https://arxiv.org/abs/2306.05685)
- en: Join our community on Discord
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm](https://packt.link/llm )'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm)'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code214329708533108046.png)'
