<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor014"/>1</h1>
<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Exploring Amazon Bedrock</h1>
<p>People across the globe have been amazed by the potential of generative AI, and industries across the globe are looking to innovate in their organizations and solve business use cases through generative AI.</p>
<p>This chapter will introduce you to a powerful generative AI service known as <strong class="bold">Amazon Bedrock</strong>. We’ll begin<a id="_idIndexMarker000"/> by providing an overview of the generative AI landscape. Then, we’ll examine the challenges industries face with generative AI and how Amazon Bedrock addresses those challenges effectively. After, we’ll explore the <a id="_idIndexMarker001"/>various <strong class="bold">foundation models</strong> (<strong class="bold">FMs</strong>) that are currently offered by Amazon Bedrock and help you assess which model is suitable for specific scenarios. Additionally, we’ll cover some of Amazon’s additional generative AI capabilities beyond FMs. By the end of this chapter, you will have a solid understanding of Amazon Bedrock’s generative AI offerings, model selection criteria, and the broader generative AI capabilities available from Amazon.</p>
<p>The following topics will be covered in the chapter:</p>
<ul>
<li>Understanding the generative AI landscape</li>
<li>What are FMs?</li>
<li>What is Amazon Bedrock?</li>
<li>FMs in Amazon Bedrock</li>
<li>Evaluating and selecting the right FM</li>
<li>Generative AI capabilities of Amazon</li>
<li>Generative AI use cases with Amazon Bedrock</li>
</ul>
<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Understanding the generative AI landscape</h1>
<p>Since the advent of <a id="_idIndexMarker002"/>ChatGPT, organizations across the globe have explored a plethora of use cases that generative AI can solve for them. They have built several innovation teams and teams of data scientists to build and explore various use cases, including summarizing long documents, extracting information from documents, and performing sentiment analysis to gauge satisfaction or discontent toward a product or service. If you have been<a id="_idIndexMarker003"/> working in the <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) or <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) field, you may be familiar with how a language model<a id="_idIndexMarker004"/> works – by understanding the relationship between the words in documents. The main <a id="_idIndexMarker005"/>objective of these <strong class="bold">language models</strong> is to predict the next probable word in a sentence.</p>
<p>If you look at the sentence <em class="italic">John loves to eat</em>, a natural language model is trying to predict what <a id="_idIndexMarker006"/>the next word or token in the sequence will be. Here, the next probable word seems to be <em class="italic">ice-cream</em>, with a 9.4% chance, as shown in <em class="italic">Figure 1</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 1.1 – Sentence sequencing prediction" src="img/B22045_01_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Sentence sequencing prediction</p>
<p>Language models can do this by converting every word into a numerical vector, also known as <strong class="bold">embeddings</strong>. Similar <a id="_idIndexMarker007"/>words will be closer in the vector space, while dissimilar words will be positioned spatially distant from each other. For instance, the word <em class="italic">phone</em> will be far apart from the word <em class="italic">eat</em> since the semantic meanings of these words are different.</p>
<p>Early NLP techniques <a id="_idIndexMarker008"/>such as <strong class="bold">bag-of-words models</strong> with <strong class="bold">Term Frequency - Inverse Document Frequency</strong> (<strong class="bold">TF-IDF</strong>) scoring <a id="_idIndexMarker009"/>and <strong class="bold">n-gram</strong> analysis <a id="_idIndexMarker010"/>had some limitations for language modeling tasks. TF-IDF, which determines word importance based on frequency, does not account for semantic context within sentences. N-grams, representing adjacent words or characters, do not generalize well for out-of-vocabulary terms. What was needed to advance language modeling was a method of representing words in a way that captures semantic meaning and relationships between words.</p>
<p>In neural networks, a word embedding model <a id="_idIndexMarker011"/>known as <strong class="bold">Word2Vec</strong> was able to learn associations from a large corpus of text. However, the Word2Vec model struggled to perform well with out-of-vocabulary words. Since the 2010s, researchers have been experimenting with more <a id="_idIndexMarker012"/>advanced sequence modeling techniques to address this limitation, such as <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>) and <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) networks. These models <a id="_idIndexMarker013"/>have memory cells that allow them to consider the context of previous words in a sentence when predicting the next word. RNNs and LSTMs can capture longer-range dependencies compared to models such as Word2Vec. While powerful for modeling word sequences, RNNs and LSTM are also more computationally and memory intensive, which means they can hold limited context depending on how much data is being fed to the model. Therefore, these models are unable to perform well when a whole document with several pages is provided.</p>
<p>In 2017, researchers at Google and the University of Toronto published a paper called <em class="italic">Attention Is All You Need</em> (<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>). This paper introduced the <strong class="bold">transformer architecture</strong>, which is <a id="_idIndexMarker014"/>based on a self-attention mechanism rather than recurrent or convolutional layers used in previous models. This <strong class="bold">self-attention mechanism</strong> allows <a id="_idIndexMarker015"/>the model to learn contextual relationships between all words (or a set of tokens) in the input simultaneously. It does this by calculating the importance of each word concerning other words in the sequence. This attention is applied to derive contextual representations for downstream tasks such as language modeling or machine translation. One major benefit of the transformer architecture is its ability to perform parallel computation with a long sequence of words. This enabled transformers to be effectively applied to much longer texts and documents compared to previous recurrent models.</p>
<p>Language models based on the transformer architecture <a id="_idIndexMarker016"/>exhibit <strong class="bold">state-of-the-art</strong> (<strong class="bold">SOTA</strong>) and near-human-level performance. Since the advent of transformer architecture, various models have been developed. This breakthrough paved the<a id="_idIndexMarker017"/> way <a id="_idIndexMarker018"/>for<a id="_idIndexMarker019"/> modern <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), including <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), <strong class="bold">Generative Pre-Training language model</strong> (<strong class="bold">GPT</strong>), <strong class="bold">Text-To-Text Transfer Transformer</strong> (<strong class="bold">T5</strong>), <strong class="bold">BLOOM</strong>, and <strong class="bold">Anthropic Claude</strong>.</p>
<p>Now, let’s dive<a id="_idIndexMarker020"/> into <a id="_idIndexMarker021"/>some LLMs that a powering a <a id="_idIndexMarker022"/>substantial change in the generative AI domain.</p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>What are FMs?</h1>
<p>Most of the generative<a id="_idIndexMarker023"/> AI models today are powered by the transformer-based architecture. In general, these generative AI models, also widely known as FMs, employ transformers due to their ability to process text one token at a time or entire sequences of text at once using self-attention. FMs are trained on massive amounts of data with millions or billions of parameters, allowing them to understand relationships between words in context to predict subsequent sequences. While models based on the transformer architecture currently dominate the field, not all FMs rely on<a id="_idIndexMarker024"/> this architecture. Some models are built using alternative <a id="_idIndexMarker025"/>techniques, such as <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>) or <strong class="bold">variational autoencoders</strong>.</p>
<p>GANs utilize two neural networks pitted against each other in competition. The first network is known as<a id="_idIndexMarker026"/> the <strong class="bold">generator</strong> and is tasked with generating synthetic samples that mimic real data. For example, the generator could produce new images, texts, or audio clips. The second network is <a id="_idIndexMarker027"/>called the <strong class="bold">discriminator</strong>. Its role is to analyze examples, both real and synthetic, to classify which ones are genuine and which have been artificially generated.</p>
<p>Through this adversarial process, the generator learns to produce increasingly convincing fakes that can fool the discriminator. Meanwhile, the discriminator becomes better at detecting subtle anomalies that reveal the synthetic samples. Their competing goals drive both networks to continuously improve. An example of a GAN can be found at <a href="https://thispersondoesnotexist.com/">https://thispersondoesnotexist.com/</a>. By refreshing the page endlessly, users are presented with an endless stream of novel human faces. However, none are real – all are synthetic portraits created solely by a GAN trained on vast databases of real human images. The site offers a glimpse into how GANs can synthesize highly realistic outputs across many domains.</p>
<p><strong class="bold">Variational autoencoders</strong> are<a id="_idIndexMarker028"/> simpler-to-train generative AI algorithms that also utilize two neural networks – an <strong class="bold">encoder</strong> and a <strong class="bold">decoder</strong>. Encoders learn the patterns in the data by mapping it into lower-dimensional latent space, while decoders use these patterns from the latent space and generate realistic samples.</p>
<p>While these FMs (transformer, GAN, or variational autoencoders-based) are trained on massive datasets, this makes them different from other traditional ML models, such as logistic <a id="_idIndexMarker029"/>regression, <strong class="bold">support vector machines</strong> (<strong class="bold">SVM</strong>), decision trees, and others. The term <em class="italic">foundation models</em> was coined by researchers at Stanford University at Human-Centered Artificial Intelligence to differentiate them from other ML models. The traditional ML models are trained on the labeled data and are only capable of performing narrowly defined tasks. For example, there will be one model for text generation, another model for summarization, and so on.</p>
<p>In contrast, FMs learn patterns in language by analyzing the relationships between words and sentences while training on a massive dataset containing millions or billions of parameters. Due to their enormous pre-training datasets, FMs tend to generalize well and understand contextual meaning, which allows them to solve various use cases, such as text generation, summarization, entity extraction, image generation, and others. Their pre-training <a id="_idIndexMarker030"/>enables them to serve as a highly adaptable starting point for many different applications. <em class="italic">Figure 1</em><em class="italic">.2</em> highlights some of the differences between traditional ML models and FMs:</p>
<div><div><img alt="Figure 1.2 – Traditional ML models versus FMs" src="img/B22045_01_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Traditional ML models versus FMs</p>
<p>Despite the range of FMs available, organizations face several challenges when adopting these models at <a id="_idIndexMarker031"/>scale:</p>
<ul>
<li><strong class="bold">No single model solution</strong>: There is no single model that’s optimized for all tasks and models are constantly improving with new advances in technology. To address multiple use cases, organizations may need to assemble several models that work with each other. This can take significant time and resources.</li>
<li><strong class="bold">Security concerns</strong>: Security and privacy pose a major concern as organizations want to protect their data and valuable intellectual property, and they also want control over how their data is shared and used by these models.</li>
<li><strong class="bold">Time and resource management</strong>: For applications such as document summarization and virtual assistants, specific model configuration is needed. This includes defining tasks, granting access to internal data sources, and developing APIs for the model to take action. This requires a multi-step process and complex coding.</li>
<li><strong class="bold">Lack of seamless integration</strong>: Being able to seamlessly integrate into existing applications is important to avoid managing large computational infrastructures or incurring high costs. Organizations want models to work behind the scenes<a id="_idIndexMarker032"/> without any heavy lifting or expense.</li>
</ul>
<p>Addressing these<a id="_idIndexMarker033"/> technical, operational, security, and privacy challenges is key for organizations to successfully adopt and deploy FMs at an enterprise scale.</p>
<p>These are the very problems that Amazon Bedrock is designed to solve.</p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>What is Amazon Bedrock?</h1>
<p>Amazon Bedrock is a <a id="_idIndexMarker034"/>fully managed service that offers various choices of high-performing FMs via a single API. <em class="italic">Fully managed</em> implies that users do not have to worry about creating, deploying, and operating the backend infrastructure as it has been taken care of by Amazon. So, from within your application or code, you can invoke the model on Bedrock with a single API containing your prompt. One of the key advantages of Amazon Bedrock is it provides a wide choice of leading FMs from Amazon and top AI companies such as Anthropic, AI21 Labs, Cohere, Meta, Stability AI, and Mistral.</p>
<p>Once you’ve defined your use case, the next step is to choose an FM. Amazon Bedrock provides a playground experience (a web interface for rapid experimentation) where you can experiment with different models and prompts. Additionally, there are certain techniques and suitability criteria you need to employ to choose the best-fit model for your use case. We will learn how to evaluate LLMs in the upcoming sections.</p>
<p>Once you have evaluated and identified the FM for your use case, the focus turns to enhancing its predictive <a id="_idIndexMarker035"/>capabilities. Amazon Bedrock provides the following key capabilities for refining model performance:</p>
<ul>
<li><code>Tell me the recipe for chocolate cake</code> or can be detailed prompts with multiple examples, depending on the use case that you are trying to solve. With its playground experience, Amazon Bedrock lets you effectively design and formulate prompts through rapid experimentation. We will discuss some of these techniques and practical aspects of prompt engineering in <a href="B22045_03.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>.</li>
<li><strong class="bold">Easy fine-tuning</strong>: Amazon Bedrock allows you to easily customize FMs with your dataset. This process is called <strong class="bold">fine-tuning</strong> the <a id="_idIndexMarker037"/>model and involves training the model further with your domain dataset, improving the accuracy for domain-specific tasks. Fine-tuning can be done directly from the Amazon Bedrock <a id="_idIndexMarker038"/>console or through APIs, and by providing your datasets in an Amazon <strong class="bold">Simple Storage Service</strong> (<strong class="bold">Amazon S3</strong>) bucket. We will discuss fine-tuning Amazon Bedrock FMs in detail in <a href="B22045_04.xhtml#_idTextAnchor073"><em class="italic">Chapter 4</em></a>.</li>
<li><strong class="bold">Native support for RAG</strong>: <strong class="bold">Retrieval augmented generation</strong> (<strong class="bold">RAG</strong>) is a powerful <a id="_idIndexMarker039"/>technique to fetch data from outside the language model, such as from internal knowledge bases or external sources, to provide accurate responses to domain-specific use cases. This technique is useful when large documents are needed that are beyond the context provided by the model. Amazon Bedrock provides native support for RAG, so you can connect your data source for retrieval augmentation. We will discuss RAG in greater detail in <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>.</li>
</ul>
<p>Furthermore, there are additional capabilities provided by Amazon Bedrock, such as the ability to build intelligent <strong class="bold">Agents</strong> to orchestrate and carry out multiple tasks on your behalf. Agents can call various internal and external data sources, connect to applications, and run complex tasks in multiple steps. We will dive deep into building intelligent Agents in <a href="B22045_10.xhtml#_idTextAnchor192"><em class="italic">Chapter 10</em></a>.</p>
<p>Security, privacy, and observability are some of the key capabilities of Amazon Bedrock. The data that you provide when you invoke FMs, including prompts and context, isn’t used to retain any of the FMs. In addition, all the AWS security and governance capabilities, including data encryption, IAM authentication and permission policies, VPC configuration, and others, apply to Amazon Bedrock. Hence, you can encrypt your data at rest and in transit. You can tell Amazon<a id="_idIndexMarker040"/> Bedrock to use <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) so that the traffic between AWS-hosted system components does not flow through the internet. Also, via <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>), you can <a id="_idIndexMarker041"/>provide access to certain resources or users. Furthermore, metrics, logs, and API calls are pushed to AWS<a id="_idIndexMarker042"/> CloudWatch and AWS CloudTrail, so you can have visibility and monitor the usage of Amazon Bedrock models. In <em class="italic">Part 3 </em>of the book, we will cover model evaluation, monitoring, security, privacy, and ensuring safe and responsible AI practices.</p>
<p>For now, let’s look at the different FMs offered by Amazon Bedrock.</p>
<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>FMs in Amazon Bedrock</h1>
<p>With Amazon <a id="_idIndexMarker043"/>Bedrock, you have access to six FMs <a id="_idIndexMarker044"/>from Amazon and leading AI companies – that is, AI21, Anthropic, Command, Stability AI, and Meta – as depicted in <em class="italic">Figure 1</em><em class="italic">.3</em>. Amazon Bedrock might add access to more FMs in the future:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 1.3 – FMs available on Amazon Bedrock" src="img/B22045_01_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – FMs available on Amazon Bedrock</p>
<p>Now, let’s<a id="_idIndexMarker045"/> discuss each of these models in<a id="_idIndexMarker046"/> detail.</p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Amazon Titan FMs</h2>
<p>The <strong class="bold">Amazon Titan FMs</strong> represent<a id="_idIndexMarker047"/> a suite of powerful, multipurpose<a id="_idIndexMarker048"/> models developed by AWS through extensive pretraining on vast datasets, endowing them with broad applicability across diverse domains. This FM supports use cases such as generating texts, question-answering, summarization, RAG, personalization, image generation, and more. A simple example would be generating an article/blog or writing an email.</p>
<p>Three types of Amazon Titan models are currently available on Amazon Bedrock: <em class="italic">Titan Text Generation</em>, <em class="italic">Titan Image Generator</em>, and <em class="italic">Titan Embeddings</em>.</p>
<h3>Titan Text Generation</h3>
<p><strong class="bold">Titan Text Generation</strong> is an LLM <a id="_idIndexMarker049"/>that’s <a id="_idIndexMarker050"/>designed for use cases such as generating texts, summarization, and more. Let’s assume that John has to write an email to the customer support team of his telephone operator, asking them to fix the billing issue he has been facing. We can provide a prompt to the Titan Text Generation model. The response will be generated alongside the subject, as shown in <em class="italic">Figure 1</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 1.4 – Response generated by the Titan Text G1- Express model" src="img/B22045_01_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Response generated by the Titan Text G1- Express model</p>
<p>At the time of writing, Titan Text Generation is available in three different flavors – <em class="italic">Titan Text G1 Lite,</em> <em class="italic">Titan Text G1 Express</em> and <em class="italic">Titan Text G1 Premier</em>. The main difference is that Lite is a more cost-effective and smaller model and supports up to <em class="italic">4,000</em> tokens, Express is a larger model that supports up to <em class="italic">8,000</em> tokens and is designed for complex <a id="_idIndexMarker051"/>use cases, and Premier is most<a id="_idIndexMarker052"/> advanced model by Titan that supports up to 32k tokens and is designed to provide exceptional performance.</p>
<h3>Titan Image Generator</h3>
<p><code>Generate an image of a Bunny skiing in the Swiss Alps</code>. Once the images have been generated, we can create variations of a single image, or even edit the image, as demonstrated in <em class="italic">Figure 1</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 1.5 – Titan Image Generator and its configurations" src="img/B22045_01_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Titan Image Generator and its configurations</p>
<p>In <a href="B22045_09.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, we will<a id="_idIndexMarker055"/> learn more about how image<a id="_idIndexMarker056"/> generation works and dive into various use cases.</p>
<h3>Titan Embeddings</h3>
<p>The main function<a id="_idIndexMarker057"/> of the <strong class="bold">Titan Embeddings</strong> model is <a id="_idIndexMarker058"/>to convert texts (or images) into numeric vectors. These vectors represent words mathematically so that similar words have similar vectors. You <a id="_idIndexMarker059"/>can store<a id="_idIndexMarker060"/> these embeddings in vector<a id="_idIndexMarker061"/> databases such as <strong class="bold">OpenSearch</strong>, <strong class="bold">Aurora pgvector</strong>, <strong class="bold">Amazon Kendra</strong>, or <strong class="bold">Pinecone</strong>, and <a id="_idIndexMarker062"/>these databases will be used to compare the relationship between the texts.</p>
<p>At the time of writing, the Titan Embeddings model is available in two variations – <strong class="bold">Titan Text Embeddings</strong> and <strong class="bold">Titan Multimodal Embeddings</strong>. The main difference is Titan Text <a id="_idIndexMarker063"/>Embeddings<a id="_idIndexMarker064"/> converts texts into embeddings, which makes the model a suitable fit for use cases such as RAG and clustering, while Titan Multimodal Embeddings can convert a combination of texts and images into embeddings, which makes it apt for use cases such as searching within images and providing recommendations.</p>
<p>While Titan Text Embeddings supports up to <em class="italic">8,000</em> tokens and over 25 languages, Titan Multimodal Embeddings can support up to <em class="italic">128</em> tokens with a maximum image size of 25 MB. Here, English is the only supported language.</p>
<p>In the next chapter, we will learn how to invoke these models and their input configuration parameters. For <a id="_idIndexMarker065"/>now, let’s learn about some <a id="_idIndexMarker066"/>other FMs provided by Amazon Bedrock.</p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>AI21 Labs – Jurassic-2</h2>
<p>AI21 Labs has built <a id="_idIndexMarker067"/>several FMs and task-specific models. However, at the<a id="_idIndexMarker068"/> time of writing, Amazon Bedrock provides access to <em class="italic">Jamba-Instruct</em>, <em class="italic">Jurassic 2 – Ultra</em> and <em class="italic">Jurassic 2 – </em><em class="italic">Mid</em> FMs.</p>
<p><strong class="bold">Jamba-Instruct</strong> supports <a id="_idIndexMarker069"/>only English, whereas <strong class="bold">Jurassic-2 </strong>models support multiple languages and use cases such as advanced text generation, comprehension, open book Q&amp;A, summarization and others.</p>
<p>Jamba-Instruct supports<a id="_idIndexMarker070"/> context token length of 256K, whereas, <strong class="bold">Jurassic-2 Ultra</strong> and <strong class="bold">Jurassic-2 Mid</strong> both <a id="_idIndexMarker071"/>support a context token length of 8,192.</p>
<p>An example would be the prompt <code>Give me pointers on how I should grow vegetables at home</code>. The output is depicted in <em class="italic">Figure 1</em><em class="italic">.6</em>:</p>
<div><div><img alt="Figure 1.6 – Prompting the Jurassic-2 model" src="img/B22045_01_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Prompting the Jurassic-2 model</p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Anthropic Claude</h2>
<p>Anthropic focuses <a id="_idIndexMarker072"/>on <a id="_idIndexMarker073"/>safe and responsible AI and provides a group of Claude models. These models support use cases such as Q&amp;A, removing <strong class="bold">personally identifiable information</strong> (<strong class="bold">PII</strong>), content generation, roleplay dialogues, and<a id="_idIndexMarker074"/> more. One major benefit of using Anthropic Claude is its ability to process longer sequences of text as prompts. With a maximum context window of <em class="italic">200,000</em> tokens to date, Claude can understand and respond to much more extensive prompts. This larger context allows Claude to engage in deeper discussions, understand <a id="_idIndexMarker075"/>longer narratives or documents, and generate more<a id="_idIndexMarker076"/> coherent multi-paragraph responses.</p>
<p>Amazon Bedrock currently offers access to five versions of Anthropic’s Claude language model:</p>
<ul>
<li><strong class="bold">Anthropic Claude 3.5 Sonnet</strong>: This <a id="_idIndexMarker077"/>sets new industry standards for superior intelligence, outperforming its predecessors and other top AI models in various benchmarks. Claude 3.5 Sonnet excels in areas like visual processing, content generation, customer support, data analysis, and coding. Remarkably, it achieves this enhanced performance while being 80% more cost-effective than previous Anthropic models, making it an attractive choice for businesses seeking advanced AI capabilities at a lower price point. The following link highlights the benchmarks and comparison with other models on different tasks: https://aws.amazon.com/blogs/aws/anthropics-claude-3-5-sonnet-model-now-available-in-amazon-bedrock-the-most-intelligent-claude-model-yet/.</li>
<li><strong class="bold">Anthropic Claude 3</strong>: This has three model variants – <em class="italic">Claude 3 Opus</em>, <em class="italic">Claude 3 Sonnet</em>, and <em class="italic">Claude 3 Haiku</em>. They are the recent and most advanced family of Anthropic models available on Amazon Bedrock. All these models have multimodal capabilities and can perceive and analyze images (jpeg, png), as well as other file types, such as .csv, .doc, .docx, .html, .md, .pdf, .txt, .xls, .xlsx, .gif, and text input, with a 200K context window:<ul><li><strong class="bold">Claude 3 Opus</strong>: This is Anthropic’s most capable model to date, with 175 billion parameters. Opus has advanced few-shot learning capabilities, allowing it to quickly adapt to a wide variety of tasks using just a few examples.</li><li><strong class="bold">Claude 3 Sonnet</strong>: A 60-billion-parameter multimodal AI model, Sonnet has strong few-shot learning abilities. Its parameter-efficient architecture allows it to handle complex inputs such as long documents while being more computationally efficient than Opus.</li><li><strong class="bold">Claude 3 Haiku</strong>: At 7 billion parameters, Haiku is Anthropic’s most compact and lightweight model. It is optimized for efficiency, providing high performance for its size. Its low computational requirements make it very fast to run inference.</li></ul></li>
<li><strong class="bold">Anthropic Claude 2.1</strong><strong class="bold"> and </strong><strong class="bold">Claude 2</strong>: They are also advanced additions to Anthropic’s Claude family. They provide performant reasoning capabilities and high accuracy with lower hallucination rates. They perform well on use cases such as dialogue, creative writing, information, roleplay, summarization, and others. In terms of context length, Claude 2.1 supports up to <em class="italic">200,000</em> tokens and Claude 2 supports up to <em class="italic">100,000</em> tokens.</li>
<li><strong class="bold">Anthropic Claude 1.3</strong>: This is an earlier release with capabilities typical of LLMs at that time. It demonstrated strong performance on tasks involving factual responses, summarization, and basic question-answering. In terms of context length, Claude 1.3 supports up to <em class="italic">100,000</em> tokens.</li>
<li><strong class="bold">Anthropic Claude Instant 1.2</strong>: This offers a faster and more cost-effective option compared to other Claude models. The latency of the Claude Instant model is greatly reduced at the cost of impacted performance. However, Claude Instant still demonstrates strong language skills for many common NLP applications that do not require the highest levels of reasoning or nuanced responses, and when<a id="_idIndexMarker078"/> speed or cost is a higher priority than absolute highest performance. In terms of context length, Claude Instant 1.2 supports up to <em class="italic">100,000</em> tokens.</li>
</ul>
<p>We will walk <a id="_idIndexMarker079"/>through<a id="_idIndexMarker080"/> some examples of leveraging Anthropic Claude with Bedrock in the next chapter.</p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Cohere</h2>
<p>Amazon <a id="_idIndexMarker081"/>Bedrock <a id="_idIndexMarker082"/>offers multiple models from Cohere: <em class="italic">Command</em>,<em class="italic"> Command R+</em>,<em class="italic"> Command R</em>,<em class="italic"> Command Light </em>models,<em class="italic"> Embed English, </em>and<em class="italic"> Embed Multilingual</em>. <strong class="bold">Cohere Command</strong><strong class="bold">, </strong>trained with 52 billion parameters<strong class="bold">,</strong> is an LLM useful for more complex language understanding. <strong class="bold">Command Light</strong>, with 6 billion parameters, is cost-effective and faster, making it a good option for those who need a lighter model for their applications. <strong class="bold">Command R+</strong>, trained on 104 billiion parameters, is the most powerful model by Cohere, at the time of writing this book, and is  designed for tasks with context window size of 128K tokens. <strong class="bold">Command R</strong>, trained on 35 billion parameters, is also designed for tasks with longer context window of 128K tokens.</p>
<p>Cohere Embed provides a set of models that have been trained to generate high-quality embeddings, which we already know are representations of text documents in a numerical format in vector space. Cohere offers <strong class="bold">Embed English</strong>, which has only been trained on English text, as well as <strong class="bold">Embed Multilingual</strong>, which can handle multiple (more than 100) languages. Embed models support a maximum token length of 512. These embedding models open a wide range of downstream applications, such as semantic search to find related documents, RAG, text clustering, classification, and more.</p>
<p>Take note of the following figure, which highlights a text generation example for summarizing a conversation using the Cohere Command model within Amazon Bedrock’s text playground:</p>
<div><div><img alt="Figure 1.7 – Cohere Command text generation example in Amazon Bedrock’s text playground" src="img/B22045_01_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Cohere Command text generation example in Amazon Bedrock’s text playground</p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Meta Llama 2 and Llama 3</h2>
<p>Meta offers several<a id="_idIndexMarker083"/> pre-trained LLMs under<a id="_idIndexMarker084"/> their <strong class="bold">Llama 2</strong> and <strong class="bold">Llama 3</strong> series for chatbot applications. Their base Llama2 model is pre-trained on over 2 trillion tokens of publicly available online data sources, at which point it’s fine-tuned with over 1 million examples of human annotation.</p>
<p>Four variants of Llama2 have been made available through Amazon Bedrock: <strong class="bold">Llama 2 Chat 13B</strong>, <strong class="bold">Llama 2 Chat 70B</strong>, <strong class="bold">Llama 2 13B</strong>, and <strong class="bold">Llama 2 70B</strong>. The 13B model contains 13 billion parameters and its training process took 368,640 GPU hours to complete. One of the key advantages of the Llama 13B model is its ability to process input sequences of arbitrary length, making it well-suited for tasks that require long documents or web pages to be analyzed. The larger 70B model variant contains 70 billion parameters and its training process took 1,720,320 GPU hours to complete. The 70B model can be used for multitask learning, implying it is well suited for performing multiple tasks simultaneously, such as image classification, speech recognition, and NLP. It has been shown to achieve improved performance on several tasks compared to 13B models, likely due to its relatively larger size and higher computational resources.</p>
<p>Along with Llama2, Meta Llama 3 variants are also available on Amazon Bedrock, namely <strong class="bold">Llama 3 8B Instruct</strong> and <strong class="bold">Llama 3 70B Instruct</strong>. The Llama 3 8B Instruct model is optimized for scenarios with limited computational resources, making it well-suited for edge devices and applications. It demonstrates strong performance in tasks such as text summarization, text classification, language translation, and sentiment analysis. The Llama 3 70B Instruct model is tailored for content creation, conversational AI systems, language understanding applications, and enterprise solutions. It excels in areas such as accurate text summarization, nuanced text classification, sophisticated sentiment analysis <a id="_idIndexMarker085"/>and reasoning, language <a id="_idIndexMarker086"/>modeling, dialogue systems, code generation, and following complex instructions.</p>
<p>For developers looking to utilize these models, Meta has created an open source GitHub repository called <em class="italic">llama-recipes</em> (<a href="https://github.com/facebookresearch/llama-recipes/tree/main">https://github.com/facebookresearch/llama-recipes/tree/main</a>) that includes demo code and examples of integrating the Llama2 models into chatbots and virtual assistants. This provides a starting point for researchers and practitioners to experiment with Llama2 and adapt it for their own conversational AI applications.</p>
<p><em class="italic">Figure 1</em><em class="italic">.8</em> demonstrates an entity extraction example using the Meta Llama 2 Chat 13 B model in Amazon Bedrock’s text playground:</p>
<div><div><img alt="Figure 1.8 – Entity extraction with the Llama 2 Chat 13B model in Amazon Bedrock’s text playground" src="img/B22045_01_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Entity extraction with the Llama 2 Chat 13B model in Amazon Bedrock’s text playground</p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Mistral AI</h2>
<p><strong class="bold">Mistral AI</strong> focuses on<a id="_idIndexMarker087"/> building compute-efficient, trustworthy, and<a id="_idIndexMarker088"/> powerful AI models. These are currently available in four variants on Amazon Bedrock – <em class="italic">Mistral 7B Instruct</em>, <em class="italic">Mixtral 8X7B Instruct</em>, <em class="italic">Mistral Large</em>, and <em class="italic">Mistral Small</em>:</p>
<ul>
<li><strong class="bold">Mistral 7B instruct</strong>: This is a 7-billion-parameter<a id="_idIndexMarker089"/> dense transformer language model designed for instructional tasks. It offers a compelling balance of performance and efficiency, delivering robust capabilities suitable for a wide range of use cases despite its relatively compact size. Mistral 7B instruct supports processing English natural language and code inputs, with an extended 32,000 token context window capacity. While more limited than larger models, Mistral 7B instruct provides high-quality language understanding, generation, and task execution tailored for instructional applications at a lower computational cost.</li>
<li><strong class="bold">Mixtral 8X7B</strong>: This is a 7-billion-parameter sparse Mixture-of-Experts language model that employs a highly parameter-efficient architecture. Despite its relatively compact total size, it utilizes 12 billion active parameters for any given input, enabling stronger language understanding and generation capabilities compared to similarly-sized dense models such as Mistral 7B. This sparse model supports processing inputs across multiple natural languages, as well as coding languages, catering to a wide range of multilingual and programming use cases. Additionally, Mixtral 8X7B maintains an extended context window of 32,000 tokens, allowing it to effectively model long-range dependencies within lengthy inputs.</li>
<li><strong class="bold">Mistral Large</strong>: This is capable of complex reasoning, analysis, text generation, and code generation and excels at handling intricate multilingual tasks across English, French, Italian, German, and Spanish. Mistral Large supports a maximum context window of 32,000 tokens, enabling it to process long-form inputs while delivering SOTA performance on language understanding, content creation, and coding applications demanding sophisticated multilingual capabilities.</li>
<li><strong class="bold">Mistral Small</strong>: This is an advanced language model designed for efficiency and affordability. It excels in handling high-volume, low-latency language tasks swiftly and cost-effectively. With its specialized capabilities, Mistral Small seamlessly tackles coding challenges and operates fluently across multiple languages, including <a id="_idIndexMarker090"/>English, French, German, Spanish, and Italian. Mistral Small supports a maximum context window of 32,000 tokens.</li>
</ul>
<p><em class="italic">Figure 1</em><em class="italic">.9</em> illustrates <a id="_idIndexMarker091"/>the usage of the Mistral Large model <a id="_idIndexMarker092"/>with a reasoning scenario within Amazon Bedrock’s text playground:</p>
<div><div><img alt="Figure 1.9 – Mistral Large in Amazon Bedrock’s text playground" src="img/B22045_01_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Mistral Large in Amazon Bedrock’s text playground</p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Stability AI – Stable Diffusion</h2>
<p>Stable Diffusion was<a id="_idIndexMarker093"/> developed by Stability AI to generate<a id="_idIndexMarker094"/> highly realistic images using diffusion models trained on large datasets. The core technique behind Stable Diffusion is called <strong class="bold">latent diffusion</strong>, which<a id="_idIndexMarker095"/> involves using a forward diffusion process to add noise to data over time, and a reverse diffusion process to gradually remove noise and reconstruct the original data. In the case of image generation, this allows the model to generate new images conditioned on text or image prompts provided by the user.</p>
<p>Amazon Bedrock provides <strong class="bold">SDXL 0.8</strong> and <strong class="bold">SDXL1.0</strong> Stable Diffusion models from Stability AI. The Stable Diffusion model aims to generate highly realistic images based on the text or image that’s provided as a prompt. SDXL 1.0 is particularly impressive due to its large model sizes. Its base model contains over <em class="italic">3.5 billion</em> parameters, while its ensemble pipeline uses two models totaling <em class="italic">6.6 billion</em> parameters. By aggregating results from multiple models, the ensemble approach generates even higher-quality images.</p>
<p>Through Amazon Bedrock, developers can leverage Stable Diffusion for a variety of image generation tasks. This includes generating images from text descriptions (text-to-image), generating new images based on existing images (image-to-image), as well as filling in missing areas (inpainting) or extending existing images (outpainting). We will look at these in detail in <a href="B22045_09.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a><em class="italic">.</em></p>
<p>Let’s run through a simple example of the Stable Diffusion model in Amazon Bedrock’s text playground by using this prompt: <code>a dog wearing sunglasses, riding a bike </code><code>on mars</code>.</p>
<div><div><img alt="Figure 1.10 – Image generation with the Stable Diffusion model" src="img/B22045_01_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – Image generation with the Stable Diffusion model</p>
<p>The ability to automatically create visual content has many applications across industries such as advertising, media and entertainment, and gaming. In <a href="B22045_09.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, we will explore how <a id="_idIndexMarker096"/>Stable Diffusion works under the hood. We <a id="_idIndexMarker097"/>will also discuss best practices and architecture patterns for leveraging image generation models in your applications.</p>
<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Evaluating and selecting the right FM</h1>
<p>Now that we’ve <a id="_idIndexMarker098"/>understood the different types of FMs available in Amazon Bedrock, how do we determine which one is best suited for our specific project needs? This section will help you learn how to evaluate the model fit for your use case.</p>
<p>The first step is to clearly define the problem you’re trying to solve or the use case you want to build. Get as specific as possible about the inputs, outputs, tasks involved, and any other requirements. With a well-defined use case in hand, you can research which models have demonstrated capabilities relevant to your needs. Narrowing the options upfront based on capabilities will streamline the evaluation process.</p>
<p>Once you’ve identified some potential candidate models, the next step is to examine their performance across standardized benchmarks and use cases. Amazon Bedrock provides a capability to evaluate FMs, also <a id="_idIndexMarker099"/>called <strong class="bold">model evaluation jobs</strong>. With model evaluation jobs, users have the option to use either automatic model evaluation or evaluation through the human workforce. We will cover Amazon Bedrock’s model evaluation in more detail in the upcoming chapters.</p>
<p>In addition, several <a id="_idIndexMarker100"/>leaderboards and benchmarks exist today that can help with this evaluation, such as the following:</p>
<ul>
<li>Stanford Helm leaderboard for LLMs</li>
<li>HuggingFace’s open leaderboard</li>
<li>GLUE (<a href="https://gluebenchmark.com/">https://gluebenchmark.com/</a>)</li>
<li>SuperGLUE (<a href="https://super.gluebenchmark.com/">https://super.gluebenchmark.com/</a>)</li>
<li>MMLU (<a href="https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu">https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu</a>)</li>
<li>BIG-bench (<a href="https://github.com/google/BIG-bench">https://github.com/google/BIG-bench</a>)</li>
</ul>
<p>Reviewing where each model ranks on tasks related to your use case provides an objective measure of its abilities.</p>
<p>Apart from benchmark performance, inspecting each model’s cost per query, processing latency, training parameters if fine-tuning is needed, and any other non-functional requirements need to be considered. The right model needs to not only achieve your technical objectives but also fit within your cost and timeline constraints.</p>
<p>No evaluation is complete without hands-on testing. Take advantage of Amazon Bedrock’s text playground <a id="_idIndexMarker101"/>or <strong class="bold">Amazon Partyrock</strong> to try out candidates on sample prompts, text generation tasks, or other example interactions representing your intended use case. More details regarding Amazon Bedrock’s text playground and Amazon Partyrock will be covered in the next chapter. This mechanism of model evaluation allows for a more qualitative assessment of things such as generated language quality, ability to maintain context, interpretability of responses, and the overall <em class="italic">feel</em> of interacting with each model.</p>
<p>By thoroughly researching capabilities, performance, and requirements, as well as testing multiple options, you’ll be well-equipped to select the right FM that provides the best overall fit and<a id="_idIndexMarker102"/> solution for your project needs. The right choice will help ensure your project’s success.</p>
<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Generative AI capabilities of Amazon</h1>
<p>This book is primarily <a id="_idIndexMarker103"/>focused on Amazon Bedrock, but we wanted to highlight a few other generative AI capabilities offered by Amazon that are being used in enterprises for accelerating developer productivity, innovating faster, and solving their use cases with ease.</p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Amazon SageMaker</h2>
<p><strong class="bold">Amazon SageMaker</strong> is Amazon’s<a id="_idIndexMarker104"/> fully managed<a id="_idIndexMarker105"/> ML platform for building, training, and deploying ML models at scale. One of the most powerful features of SageMaker is SageMaker Jumpstart, which provides a catalog of pre-trained open source FMs that are ready to be deployed and used.</p>
<p>Some examples of FMs available in SageMaker Jumpstart include FLAN-T5 XL, a fine-tuned XL version of the T5 transformer model optimized for natural language understanding. Additional models, such as Meta Llama2, AI21 Jurassic-2 Ultra, and Stable Diffusion models, are also available in SageMaker Jumpstart.</p>
<p>In addition to deploying these pre-trained FMs directly, SageMaker Jumpstart provides tools for customizing and fine-tuning select models for specific use cases. For instance, users can perform prompt engineering to better control model responses by adjusting text prompts. Some models also support reasoning augmentation to improve the common-sense reasoning ability of LLMs through question-answering tasks. Fine-tuning capabilities allow you to adapt the language models to domain-specific datasets.</p>
<p>This enables engineers and researchers to leverage the power of these generative AI models directly from Jumpstart so that they can build novel applications without requiring deep expertise in model training. The SageMaker platform handles all the heavy lifting of deploying, scaling, and managing ML models. When you open SageMaker Jumpstart within SageMaker Studio UI, you will see models offered by different model providers. This can be seen in <em class="italic">Figure 1</em><em class="italic">.11</em>:</p>
<div><div><img alt="Figure 1.11 – SageMaker Jumpstart" src="img/B22045_01_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – SageMaker Jumpstart</p>
<p>You can choose the <a id="_idIndexMarker106"/>model you would like to <a id="_idIndexMarker107"/>work with based on your use case and deploy it directly to a SageMaker endpoint, or you can fine-tune the model with a custom dataset. <em class="italic">Figure 1</em><em class="italic">.12</em> shows several open source models offered by HuggingFace, on SageMaker Jumpstart, exemplifying the simplicity in SageMaker to search for models of your choice suited to a particular task using the search bar or <strong class="bold">Filters</strong> options:</p>
<div><div><img alt="Figure 1.12 – SageMaker Jumpstart HuggingFace models" src="img/B22045_01_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – SageMaker Jumpstart HuggingFace models</p>
<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Amazon Q</h2>
<p><strong class="bold">Amazon Q</strong> is a Generative<a id="_idIndexMarker108"/> AI-powered assistant<a id="_idIndexMarker109"/> that is built on top of Amazon Bedrock, and has been designed to enhance productivity and accelerate decision-making across various domains. It can assist users in a multitude of tasks, ranging from software development to data analysis and decision making.</p>
<p>Here is an overview of key offerings available with Amazon Q.</p>
<h3>Amazon Q for Business</h3>
<p><strong class="bold">Amazon Q for Business</strong> is an<a id="_idIndexMarker110"/> enterprise-grade, generative <a id="_idIndexMarker111"/>AI-powered assistant designed to streamline operations and enhance productivity within organizations. With this tool you can access and interact with the company repositories of data if you have required permissions, simplifying tasks and accelerating problem-solving processes. Here are some key features of Amazon Q for Business:</p>
<ul>
<li><strong class="bold">Comprehensive Data Integration</strong>: Amazon Q for Business seamlessly connects to over 40 popular enterprise data sources, including Amazon S3, Microsoft 365, and Salesforce. It ensures secure access to content based on existing user permissions and credentials, leveraging single sign-on for a seamless experience.</li>
<li><strong class="bold">Intelligent Query Handling</strong>: You can ask questions in natural language, and Amazon Q for Business will search across all connected data sources, summarize relevant information<a id="_idIndexMarker112"/> logically, analyze trends, and engage <a id="_idIndexMarker113"/>in interactive dialogue. This empowers users to obtain accurate and comprehensive answers, eliminating the need for time-consuming manual data searches.</li>
<li><strong class="bold">Customizable and Secure</strong>: Organizations can tailor Amazon Q for Business to their specific needs by configuring administrative guardrails, document enrichment, and relevance tuning. This ensures that responses align with company guidelines while maintaining robust security and access controls.</li>
<li><strong class="bold">Task Automation</strong>: Amazon Q for Business allows users to streamline routine tasks, such as employee onboarding requests or expense reporting, through simple, natural language prompts. Additionally, users can create and share task automation applications, further enhancing efficiency and productivity.</li>
</ul>
<p>You can set up Amazon Q for Business Application in a few clicks as shown in <em class="italic">Figure 1</em><em class="italic">.13</em>.</p>
<div><div><img alt="Figure 1.13 – Setting up Amazon Q for Business" src="img/B22045_01_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – Setting up Amazon Q for Business</p>
<p>For more details<a id="_idIndexMarker114"/> on<a id="_idIndexMarker115"/> setting up Amazon Q for Business Application, you can check the link: <a href="https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/getting-started.html">https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/getting-started.html</a></p>
<div><div><img alt="Figure 1.14 – Customize web experience for Amazon Q for Business" src="img/B22045_01_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – Customize web experience for Amazon Q for Business</p>
<p>Once the application is set up, users can customize the web experience for the Q business application as shown in <em class="italic">Figure 1</em><em class="italic">.14</em></p>
<p>Let us now<a id="_idIndexMarker116"/> look at another offering Amazon Q for <a id="_idIndexMarker117"/>QuickSight.</p>
<h3>Amazon Q for QuickSight</h3>
<p><strong class="bold">Amazon Q for QuickSight</strong> is built<a id="_idIndexMarker118"/> for business users and analysts<a id="_idIndexMarker119"/> to unlock insights from their data more efficiently. It leverages the capabilities of Generative AI to streamline the process of data analysis and visualization. Here are some key features of Amazon Q for QuickSight :</p>
<ul>
<li><strong class="bold">Intuitive Storytelling</strong>: With Amazon Q for QuickSight, business users can create visually compelling narratives from their data by using simple, natural language prompts. These stories can include visuals, images, and text, making it easier to communicate insights and align stakeholders.</li>
<li><strong class="bold">Executive Summaries</strong>: Amazon Q for QuickSight can automatically generate executive summaries that highlight the most important trends and statistics from your dashboards. This feature saves time by providing a quick snapshot of key insights, eliminating the need to browse through multiple visuals.</li>
<li><strong class="bold">Natural Language Q&amp;A</strong>: Business users can confidently answer questions about their data using natural language queries. Amazon Q can understand vague or general questions, provide alternative perspectives, and offer context through narrative summaries.</li>
<li><strong class="bold">Accelerated Dashboard Building</strong>: Analysts can significantly reduce the time required to build dashboards by describing the desired visualizations using natural language. Amazon Q can interpret these prompts and generate the corresponding visuals in seconds.</li>
</ul>
<h3>Amazon Q for Developer</h3>
<p><strong class="bold">Amazon Q for Developer</strong> streamlines <a id="_idIndexMarker120"/>the software <a id="_idIndexMarker121"/>development lifecycle on AWS. Here are some key features of Amazon Q for Developers:</p>
<ul>
<li><strong class="bold">Intuitive Development Assistance</strong>: Within IDEs, Amazon Q can provide real-time code suggestions, generate new code snippets, and offer guidance on software development best practices. This accelerates the coding process and enhances productivity.</li>
<li><strong class="bold">Code Transformation</strong>: Amazon Q can help you upgrade and modernize your legacy codebases by automatically transforming and optimizing your code to the latest language versions and frameworks. This capability ensures your applications remain up-to-date and secure.</li>
<li><strong class="bold">Troubleshooting and Maintenance</strong>: Amazon Q can assist you in diagnosing and resolving<a id="_idIndexMarker122"/> errors, bugs, and issues within<a id="_idIndexMarker123"/> your AWS applications. It can also help you understand and manage your AWS resources more efficiently, minimizing the need to navigate through complex consoles.</li>
<li><strong class="bold">Cost Optimization</strong>: By analyzing your AWS cost data, Amazon Q can provide valuable insights into your cloud spending patterns, helping you identify cost-saving opportunities and optimize your cloud infrastructure for better cost efficiency.</li>
</ul>
<p><em class="italic">Figure 1</em><em class="italic">.15</em> and <em class="italic">Figure 1</em><em class="italic">.16</em> illustrate an example of Amazon Q Developer for aiding in productivity gains for software engineers or developers.</p>
<div><div><img alt="Figure 1.15 – Amazon Q Developer" src="img/B22045_01_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – Amazon Q Developer</p>
<div><div><img alt="Figure 1.16 – Amazon Q Developer Lambda function" src="img/B22045_01_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – Amazon Q Developer Lambda function</p>
<p>With Amazon Q, developers <a id="_idIndexMarker124"/>can streamline their workflows, from<a id="_idIndexMarker125"/> planning and development to testing, deployment, and maintenance, ultimately enabling them to deliver high-quality applications faster and with greater confidence.</p>
<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>Generative AI use cases with Amazon Bedrock</h1>
<p>Since the advent of <a id="_idIndexMarker126"/>generative AI, numerous organizations<a id="_idIndexMarker127"/> have benefited from the potential applications of this transformative technology in achieving their business objectives. Many of these organizations, which include Accenture, Adidas, Intuit, and Salesforce, have successfully developed prototypes and even have deployed production-ready generative AI systems using Amazon Bedroc<a id="_idTextAnchor032"/>k. Across various industries, we have<a id="_idIndexMarker128"/> seen numerous compelling<a id="_idIndexMarker129"/> use cases for generative AI with Amazon Bedrock. Let’s learn more about some of these industries in detail:</p>
<ul>
<li><strong class="bold">Finance</strong>: In the financial<a id="_idIndexMarker130"/> services sector, organizations have been working on use cases such as classifying and categorizing huge corpus of legal documents, developing systems to select optimal funding and investment plans for customers, providing insights and simplified summaries and Q&amp;As of complex financial documents, as well as detecting fraudulent activities such as forged signatures and tampered invoices. Additionally, organizations are utilizing Amazon Bedrock to understand market trends and customer behavior, aiding in informed decision-making processes.</li>
<li><strong class="bold">Healthcare</strong>: The healthcare industry has witnessed significant investment in developing generative AI applications with Amazon Bedrock. At the time of writing, AWS HealthScribe has been announced, which is powered by Amazon Bedrock (<a href="https://aws.amazon.com/healthscribe/">https://aws.amazon.com/healthscribe/</a>). These applications address a wide range<a id="_idIndexMarker131"/> of use cases, such as automating medical claims and adjudication processes, extracting valuable insights from health documents and medical research papers, and generating summaries of patient-doctor interactions. By leveraging Amazon Bedrock, healthcare providers are aiming to enhance patient care and drive innovation in the field.</li>
<li><strong class="bold">Media and entertainment</strong>: In the media and entertainment industry, organizations are actively exploring the diverse applications with Amazon Bedrock. These include generating narratives and storylines in sports and broadcasting, creating captions, images, and animations for storytelling, as well as providing personalized recommendations for TV shows, movies, and other forms of entertainment. By harnessing the capabilities of generative AI with Amazon Bedrock, media and entertainment companies aim to enhance the user experience, create engaging content, and stay ahead of the competition.</li>
</ul>
<p>These are just a few examples of the numerous use cases that various industries are working on. In later chapters, we will understand <a id="_idIndexMarker132"/>architectural patterns in building industry-specific use<a id="_idIndexMarker133"/> cases through Amazon Bedrock.</p>
<h1 id="_idParaDest-33"><a id="_idTextAnchor033"/>Summary</h1>
<p>In this chapter, we explored the various facets of the generative AI landscape: from understanding language models and the development of various NLP techniques to the invention of current SOTA transformer models. Then, we covered industrial challenges in building generative AI applications at scale and how Amazon Bedrock is seamlessly tackling those challenges.</p>
<p>Furthermore, we explored various FMs offered by Amazon Bedrock and provided insights into how you can take advantage of various frameworks and tools to evaluate and select the right FM for your use case. We also looked at alternative generative AI capabilities offered by Amazon, including Amazon SageMaker and Amazon Q. We concluded this chapter by uncovering a few generative AI use cases with Amazon Bedrock in financial services, healthcare, and media and entertainment.</p>
<p>In the next chapter, we will discover several techniques to access Amazon Bedrock and dive into various APIs via serverless services. Furthermore, we will learn about a hands-on approach toward invoking Bedrock FMs that can be integrated into enterprise-grade applications.</p>
</div>
</body></html>