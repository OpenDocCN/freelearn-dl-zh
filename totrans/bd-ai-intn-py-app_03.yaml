- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language models are computational algorithms designed to process, understand,
    and generate natural language. The study, research, and development of these algorithms
    is known as **natural language processing** (**NLP**). NLP predates the field
    of **machine learning** (**ML**) and can be traced back to the 1950s and the development
    of the first computers. While the first language models relied heavily on rule-based
    approaches, NLP shifted in the 1980s toward statistical methods and began to converge
    with ML. The increase in computational power and text corpora led to the development
    of deep learning and neural network-based language models in the early 21st century,
    which have seen significant progress over the last decade.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Language models have a variety of applications in NLP for understanding and
    generating natural languages as well as more formal languages, such as programming
    and database query languages. Their use cases include tasks such as text labeling
    and sentiment analysis, translation, summarization, information extraction, and
    question answering. With the advent of **large language models** (**LLMs**), applications
    have further expanded to develop conversational chat systems and personal assistants,
    software development agents, and general problem-solvers. In this chapter, you’ll
    deep dive into the essential concepts and implementation of LLMs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling with n-gram models to provide a probabilistic viewpoint
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial neural networks** (**ANNs**), their architecture, and training
    paradigm'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application of ANNs to the language modeling domain
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Transformer architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs in practice
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is largely theoretical, with a short code snippet in Python to
    illustrate the `tiktoken` tokenizer library. To follow along, you will need access
    to a computer with Python version 3.8 or later.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: To make the most of this chapter, you will need proficiency with Python and
    the `pip` package manager. You will also need a basic knowledge of probabilities,
    calculus, and software development concepts such as APIs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic framework
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building AI-intensive applications that interact with LLMs, you will likely
    come across API parameters relating to probabilities of tokens. To understand
    how LLMs relate to the concept of probabilities, this section introduces the probabilistic
    framework underpinning language models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling is typically done with a probabilistic view in mind, rather
    than in absolute and deterministic terms. This allows the algorithms to deal with
    the uncertainty and ambiguity often found in natural language.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'To build an intuitive understanding of probabilistic language modeling, consider
    the following start of a sentence, for which you want to predict the next word:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is obviously an ambiguous task with many possible answers. The article
    *the* is a very common and generic word in the English language, and the possibilities
    are endless. Any noun, such as *house*, *dog*, *spoon*, etc. could be a valid
    possible continuation of the sentence. Even adjectives such as *big*, *green*,
    and *lazy* are likely candidates. Conversely, there are words rarely seen after
    an article, including verbs, such as *eat*, *see*, and *learn*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with this kind of uncertainty, consider instead a slightly different
    question: “What is the probability of each word to come next?”'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer to *this* question is no longer a single word, but instead a large
    lookup table, assigning each word in the vocabulary a number, which represents
    the probability of this word following *the*. If this lookup table is representative
    of the English language, one would expect nouns and adjectives to have a higher
    probability than verbs. *Table 3.1* shows what such a table could look like, using
    made-up values for the *Probability* column. You will see shortly how these probabilities
    can be calculated from a text corpus:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '| **Previous word** | **Next word** | **Probability** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| … | … | … |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: '| the | house | 0.012% |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| the | dog | 0.013% |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| the | spoon | 0.007% |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| … | … | … |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| the | big | 0.002% |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| the | green | 0.001% |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| the | lazy | 0.001% |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| … | … | … |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| the | eat | 0.000% |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| the | see | 0.000% |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| the | learn | 0.000% |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| …. | .. | … |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: 'Table 3.1: A partial lookup table for words following the word *the*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: In this simple example, one (but not the only) way to decide which word comes
    next is to scan through this lookup table and find the word with the highest probability.
    This method, known as **greedy selection**, would suggest that the word *dog*
    is the most probable continuation of the sentence. However, it’s important to
    note that there are many possibilities, each with a different probability. For
    instance, the word *house* is also a close second in terms of probabilities, indicating
    that it could also be a likely continuation of the sentence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: To capture the flexibility and expressiveness of natural language, language
    models operate in terms of probabilities, and the process of training a language
    model means assigning probabilities for each word continuing the sentence thus
    far.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume you have gone through the process of selecting the next word several
    times, and find yourself further along in the sentence:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How does this sentence continue? What does the probability distribution look
    like now?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with this sentence[1](B22495_03.xhtml#footnote-000), you’ll
    agree that at this point, the probability for the word *lazy* will stand out above
    all others. Your internal language model can’t help but autocomplete the entire
    sentence, and the words *lazy dog* will just pop into your head.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[1](B22495_03.xhtml#footnote-000-backlink) This sentence is a pangram. A pangram
    contains every letter of the alphabet at least once. The sentence has been used
    in various contexts, such as typing practice and testing the display of text in
    computers.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: But why is that? Aren’t you in the same situation as before, asking what follows
    next after *the*? The key difference here is that you have more context; you see
    more of the sentence, which demonstrates that considering only the preceding word
    is not sufficient to build a good predictor of the next word. Yet this basic concept
    marks the very beginning of language models and can be viewed as a distant ancestor
    of the likes of ChatGPT and other modern LLMs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: n-gram language models
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first formalisms of a language model is an **n-gram model**, a simple
    statistical language model, first published in 1948 in Claude Shannon’s famous
    paper *A Mathematical Theory of* *Communication* ([https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: An n-gram language model can be described as a giant lookup table, where the
    model considers the last `n-1` words to predict the next. For `n=2`, you get a
    so-called bigram model, looking back only one word, as shown in *Table 3.1*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'As the sentence in the previous example illustrated, such a simple bigram model
    is limited and fails to capture the nuances of natural language. However, before
    exploring what happens when `n` is scaled up to larger values, let’s briefly discuss
    how you would train a bigram model, which is to say, how to calculate the probabilities
    for each pair of words in the table:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Take a large corpus of text, such as the collection of all Wikipedia pages in
    English.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scan through this text and count the occurrences of single words as well as
    observed pairs of words.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Record all counts in a lookup table.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the probability of word ![](img/B22495_03_Equations.png) following
    word ![](img/B22495_03_Equations1.png) as follows: divide the count for the word
    pair ![](img/B22495_03_Equations2.png) by the count of the single word ![](img/B22495_03_Equations1.png).'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, to calculate the probability of seeing the word *dog* following
    the word *the*, divide the pair count by the single word count in the following
    way:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_03_Equations3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Here, the term ![](img/B22495_03_Equations4.png) is pronounced as “probability
    of x given y.” In other words, the probability of seeing the word *dog* given
    we’ve just seen the word *the* is the count of seeing the words in combination
    (the numerator) divided by all counts of seeing *the* by itself (the denominator).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the training process of an n-gram language model only requires a single
    pass over the text, counting all occurring n-grams and (n-1)-grams, and storing
    the numbers in a table.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In practice, several tricks improve the quality of n-gram models, such as including
    special `<start>` and `<end>` markers at the beginning and end of each sentence,
    splitting words into smaller sub-words, such as *playing* into *play* and *-ing*,
    and many other improvements. You will review some of these techniques later in
    the *Tokenization* section, and they apply to modern LLMs as well.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now revisit the choice of `n`. As you have seen, a low value, such as
    `n=2`, doesn’t yield a very good language model. Is it just a matter of scaling
    up `n` until you reach the level of desired quality?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'A larger `n` value can capture more context and leads to a more predictive
    model. For `n=8`, the model can look back at the last seven words. The lookup
    table, as shown in *Table 3.2*, would contain a row that captures the example
    sentence:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '| **Previous** **7 words** | **Next word** | **Probability** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| … | … | … |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| the quick brown fox jumps over the | lazy | 99.381% |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| …. | .. | … |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: 'Table 3.2: A possible entry in the lookup table for an 8-gram'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: However, increasing `n` to large values has several challenges, which make this
    approach infeasible in practice.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: The size of the lookup table grows exponentially with a larger `n`. The *Oxford
    English Dictionary* contains approximately 273,000 English words ([https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words](https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words)),
    which allows for ![](img/B22495_03_Equations5.png) 74.5 billion possible combinations
    of two words (though many of these combinations would never be seen in a text).
    Increasing the n-gram model to `n=8`, the possible combinations of eight words
    grows to the astronomical number of ![](img/B22495_03_Equations6.png). Storing
    an entry in the table for each combination would be impossible as this number
    far exceeds all available hard drive storage space in the world, especially since
    the world’s collective data is estimated to reach 175 zettabytes = ![](img/B22495_03_Equations7.png)
    bytes by 2025 ([https://www.networkworld.com/article/966746/idc-expect-175-zettabytes-of-data-worldwide-by-2025.html](https://www.networkworld.com/article/966746/idc-expect-175-zettabytes-of-data-worldwide-by-2025.html)).
    Of course, most of these word combinations would never be encountered, and you
    could choose to omit unseen n-grams in the table.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: This challenge, known as the `n`, the probability of encountering any one n-gram
    shrinks exponentially. Most combinations of `n` words would never be encountered
    for any realistic size of training dataset. When processing text that is not part
    of the training corpus, the model would assign zero probability for unseen n-grams.
    The model would not be able to make meaningful predictions in this case, and this
    problem would be exacerbated the larger `n` became.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: In summary, while n-grams have their uses for certain narrow applications and
    educational purposes, the language models of today have evolved beyond purely
    statistical approaches. LLMs use machine learning techniques to deal with some
    of the issues pointed out above, which you’ll learn about in the next section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning for language modelling
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into language modeling approaches using ML, this section first
    introduces some general ML concepts and gives a high-level overview of different
    neural network architectures.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: At its core, ML is a field concerned with developing and studying algorithms
    that learn from data. Rather than executing hardcoded rules, the system is expected
    to *learn by example*, looking at provided inputs and desired outcomes (often
    referred to as **targets** in ML literature) and adjusting its behavior during
    the training process to change its outputs to closely resemble the user-provided
    targets.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'ML algorithms are roughly differentiated into three groups:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these groups has different learning objectives and problem formulations.
    For language modeling, you can mainly consider supervised (and related self-supervised)
    algorithms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One class of supervised learning algorithms is **artificial neural networks**
    (**ANNs**). All modern LLMs are variations of the basic ANN architecture. When
    you make an API call to a model such as GPT-4, your question flows through an
    ANN to produce the answer. These models have evolved in size and complexity over
    decades, but the core principles and building blocks remain the same.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The neural architectures found in human brains may have inspired the original
    design of ANNs, but ANNs are significantly different from their biological counterparts.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are made of many smaller units called neurons, which are interconnected
    with each other in various patterns, depending on the network architecture. Each
    neuron is a small processing unit, receiving numeric signals from other neurons
    and passing a (modified) signal to its successor neurons, analogous to biological
    neurons. ANNs have tunable parameters, referred to as **weights**, which sit on
    the connections between two neurons and can influence the signal passing between
    them.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: One of the most basic ANN architectures is the so-called **feed-forward network**
    (**FFN**), depicted in *Figure 3**.1*. In this architecture, neurons are arranged
    in layers, starting with an input layer, followed by one or more hidden layers,
    and finally an output layer. The layer size, which refers to the number of neurons
    per layer, can vary. Input and output layer sizes are determined by the specific
    problem domain. For example, you may want to learn a mapping from a two-dimensional
    input (say, the body mass index and age of a person) to a one-dimensional output
    (say, the daily resting calories burnt). The size of hidden layers is often chosen
    arbitrarily through experimentation in a process called **hyper-parameter tuning**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'In FFNs, every neuron in one layer connects to all neurons in the following
    layer, leading to a many-to-many relationship between two consecutive layers.
    *Figure 3**.1* shows an FFN architecture with one input layer (Layer 1), two hidden
    layers (Layers 2 and 3), and one output layer (Layer 4):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a network'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B22495_03_01.png)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: A feed-forward neural network architecture'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Zooming in on the functioning of a single neuron, *Figure 3**.2* shows a neuron
    with inputs from two other neurons (denoted ![](img/B22495_03_Equations8.png)
    and ![](img/B22495_03_Equations9.png)). The connections to the neuron contain
    the weights (denoted ![](img/B22495_03_Equations10.png) and ![](img/B22495_03_Equations.png)).
    The inputs are first multiplied with their corresponding weight and then summed
    up. The resulting sum is passed through a non-linear activation function ![](img/B22495_03_Equations11.png)
    and the result forms the output of the neuron (shown as ![](img/B22495_03_Equations12.png)).
    In mathematical terms, this is expressed as follows: ![](img/B22495_03_Equations13.png)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: While the specifics of activation functions are out of scope for this chapter,
    suffice it to say that non-linearity is important for the network to be able to
    learn complex patterns in the data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_03_02.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Activation of a single neuron with two inputs'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: During a forward pass through the network, you present the input data (for example,
    the BMI and the age of a person) at the input layer, calculate the activations
    of all neurons of the layer, pass these activations to the next layer, and so
    forth, until the output layer produces an outcome (which, in this example, you
    can interpret as the model’s prediction for calories burnt by a person).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: It may seem surprising that the simple activation functions governing individual
    neurons in a neural network can lead to complex pattern recognition capabilities.
    This phenomenon is rooted in the **universal approximation theorem**, which proves
    that a neural network with enough hidden layers and neurons can approximate any
    continuous function to any desired degree of accuracy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'You now know how data flows forward in an ANN from input to output layer. For
    an untrained model, this is only the first of three phases. In the next section,
    you’ll learn about the other two phases required to train an ANN: loss calculation
    and a backward pass.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Training an artificial neural network
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, this chapter has described the forward pass of a network, that is, how
    a response for a given input is calculated. Since the initial weights of an ANN
    are chosen randomly, the output of an untrained network is also random and nonsensical.
    The weights need to be adjusted during the training process.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The goal of training a neural network is to make its outputs match the provided
    targets for any given input. Thus, for supervised learning, a training dataset
    consists of input/target pairs of known correct responses. In the example of predicting
    the calories burnt given a person’s BMI and age, the training dataset would consist
    of many measurements of people’s BMI and age (the inputs) and their measured calories
    burnt (the targets). The more measurements the dataset contains, the better the
    model can learn patterns from the relationship between inputs and targets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process for an ANN can be broken down into three phases, as illustrated
    in *Figure 3**.3*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward pass**: Calculating the outputs from the inputs.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Loss calculation**: Calculating an error signal between the outputs and the
    desired targets.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Backward pass and weight adjustment**: Propagating the error back through
    the model and adjusting each of the weights.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B22495_03_03.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: The three phases of training an ANN'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: This process is repeated over multiple passes of the dataset, until the weight
    parameters no longer meaningfully change. At this point, the model is said to
    have converged and is ready for inference.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Training starts with a **forward pass** of the data, passing in the inputs and
    recording the network’s output. As this output may differ from the correct target
    (especially for an untrained network with random weights), it is possible to calculate
    a metric called **loss**, which is a scalar number that reflects the difference
    between actual and desired output.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The loss is required to execute the **backward pass**. This step will adjust
    all weights of the network in such a way that the network will produce an output
    closer to the target for the given input. The activation for each neuron is a
    well-formed differentiable expression with sums, products, and a differentiable
    activation function. This means that the derivative of a weight with respect to
    the loss can be calculated by the rules of calculus to determine how each weight
    parameter needs to be adjusted to minimize the loss.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: This gradient calculation is then propagated backward to previous layers using
    the chain rule of calculus, all the way to the input layer. Having calculated
    the gradients for each weight in this way, the weights can then be updated. Controlled
    by a parameter called the **learning rate**, the weights can be moved a small
    step toward the direction of minimizing the loss.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'While it’s possible to execute this loop of forward and backward passes for
    every single entry in the training set one by one, in practice the training set
    is split into small batches. A **batch** may contain tens, hundreds, or even thousands
    of data points. The **batch size** is another hyper-parameter chosen experimentally
    through hyper-parameter tuning before the actual training process. Batching up
    the data in such a manner serves these purposes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: It leads to higher efficiency as batches can be processed in parallel, especially
    on specialized hardware, such as **graphical processing** **units** (**GPUs**).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error gradients backpropagated through the network are averaged across each
    batch. This leads to more stable training as single outliers in the data have
    less impact on the weight changes.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training continues until the model no longer improves on unseen validation data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: After training, the trained model can then be applied to previously unseen inputs.
    For example, the model can be integrated into a fitness tracking app, where it
    predicts burnt calories based on a person’s BMI and age with the expectation that
    it will not only work for measurements in the training data but also generalize
    to new data points as well. This application of a trained model to new data is
    known as **inference**.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: This training procedure is at the core of every neural network, including LLMs.
    As neural networks operate on numeric data, the next section will show how language
    can be represented numerically to make it compatible with the use of ANNs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: ANNs for natural language processing
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous section showed how ANNs can learn mappings of numerical inputs
    to numerical outputs. Language, however, is inherently non-numeric: a sentence
    is a sequence of discrete words from a large vocabulary. Building a neural network-based
    word predictor poses the following challenges:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The inputs to the model are discrete words. Since ANNs operate on numeric inputs
    and outputs, a suitable mapping from words to numbers and vice versa is required.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs are further sequential. Unlike bigrams, the model should be able
    to take more than one word into account when predicting the next word.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the language model needs to be a probability distribution over
    all possible next words. To form a proper distribution, the outputs need to be
    normalized to be non-negative and sum up to one.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following sections will explain these challenges and review how they are
    addressed in modern language models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first processing step to convert text to numeric inputs is called **tokenization**.
    During this phase, words are split into common sub-words, characters, and punctuation
    marks, making up the vocabulary of tokens. Each token is then assigned a unique
    integer ID.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: When interacting with LLMs, especially when dealing with self-hosted open-source
    models, the choice of tokenizer is important and must match exactly the one used
    during the training of the model. Luckily, many common open-source tokenizers
    exist. Even commercial LLM providers, such as OpenAI, have open-sourced their
    tokenizer libraries to make it easier to interact with their models. Bindings
    of OpenAI’s `tiktoken` library are available for many popular programming languages,
    including Python, C#, Java, Go, and Rust.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example demonstrates the use of the `tiktoken` Python library.
    After installing the package with `pip install tiktoken`, you can create an `encoder`
    object and encode any text, which will return a list of token IDs. The following
    code snippet tokenizes the sentence *tiktoken is a popular tokenizer!* and decodes
    each token ID back into its byte string:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Running this code produces the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can see that the word *tiktoken* was split into three tokens, *t*, *ik*,
    and *token*, likely because the word itself is not common enough to warrant its
    own token in the vocabulary. Also of note is that whitespace is often encoded
    as part of a token, at the beginning, such as in “ *is*.”
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: When interacting with proprietary models via APIs, tokenization typically happens
    automatically and server-side. This means that you can submit prompts in text
    form without having to tokenize the inputs yourself. However, `tiktoken` and similar
    libraries are still useful tools when building AI-powered applications. For example,
    you can use them to calculate the number of tokens of a request, as API calls
    are usually charged by the number of submitted and returned tokens. Additionally,
    language models have an upper token limit for their inputs, known as their **context
    size**. Requests that are too large may fail or get truncated, which impacts the
    model’s response.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of developing applications with LLMs, it is sufficient to know
    about tokenization when it comes to the preprocessing of text. However, this is
    only the first step in making neural networks understand textual inputs. Even
    though the token IDs are numeric, the assignment from the token to its ID happens
    arbitrarily. Neural networks interpret their inputs geometrically and are not
    well suited to processing large integer numbers. In the second step, called embedding,
    these integers are converted into high-dimensional floating-point vectors, also
    known as **embedding vectors** or simply **embeddings**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Embedding
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Embedding** is the process of mapping data into a high-dimensional vector
    space. This concept is not just relevant to the training of language models but
    will also play an important role for vector databases to retrieve semantically
    similar items, which we’ll discuss later in [*Chapter 5*](B22495_05.xhtml#_idTextAnchor115),
    *Vector Databases*. Embeddings can be created for arbitrary data entities: words,
    sentences, entire documents, images, or even more abstract concepts, such as users
    or products in the context of building recommender systems.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of embeddings is twofold:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: They are fixed-length floating-point representations of their corresponding
    entities, ideally suited to be processed by neural networks.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings are coordinates in a vector space. With the right choice (or, rather,
    training), embeddings can represent semantic similarities of data entities through
    their geometric proximity. This enables the use of geometric algorithms, such
    as clustering or nearest neighbor search, to operate on the semantic meaning of
    the embedded data.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Embeddings are a fundamental concept at the core of language models and vector
    search. To understand how tokens can be embedded, let’s assume a small vector
    space with only three dimensions, as illustrated in *Figure 3**.4*. To map a token
    into this space, a random point in this space is assigned to each token. Here,
    the token is represented by its integer ID, and the random point in this space
    is indicated by its x, y, and z coordinates. The mapping is done with the help
    of an embedding matrix consisting of `n` rows and `d` columns, initialized with
    random floating-point numbers. Here, `n` is the size of the vocabulary and `d`
    is the embedding dimensionality (in this example, `d` equals `3`). To retrieve
    the coordinates for a token, the token ID is used as a row index into the embedding
    matrix, returning a d-dimensional vector. For example, the token *fox* may be
    assigned the following coordinates: `[-0.241,` `1.356, -0.7882]`.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a fox'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B22495_03_04.png)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.4: Visual representation of tokens embedded in a three-dimensional
    vector space'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Just like the weights of a neural network are assigned randomly before training,
    the values of the embedding matrix are also chosen randomly. Furthermore, and
    this is a crucial step in the training of LLMs, the embedding matrix values are
    treated as additional learnable parameters of the neural network. By allowing
    the gradients to flow all the way back into the embedding layer, the model can
    update the positions of the token coordinates during training in such a way that
    it helps the prediction task.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Studies on fully trained embedding layers of LLMs reveal that the model moves
    semantically similar tokens close together. In the earlier example, you might
    find a cluster of nouns (*fox*, *dog*) or a cluster of adjectives (*quick*, *lazy*,
    *brown*). However, with only three dimensions, similarity is limited to only three
    attributes by which tokens can be compared. LLMs use vector spaces with much larger
    dimensionality, often in the order of hundreds or even thousands of dimensions.
    In such a high-dimensional space, tokens can relate to each other (and be close
    to each other geometrically) in many ways. Some of the dimensions may have interpretable
    meanings, such as the sentiment of a word. However, most of them are likely to
    make sense only to the model internally.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you have seen how text is prepared for neural network training
    by splitting it into tokens and assigning token IDs, which can be used as an index
    to find the corresponding embedding vector in the embedding matrix. These vectors
    have geometric meaning and can be updated as part of the training phase. Next,
    you’ll learn how the outputs of the neural network can be interpreted as probabilities
    of choosing the next token.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Predicting probability distributions
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you have seen in the *n-gram language models* section, the model needs to
    output a probability distribution over the next tokens, that is, one numeric value
    for each token in the vocabulary. By choosing an output layer size matching the
    vocabulary size, the neural network will give you the right output *shape*, but
    these numbers can theoretically be any real number, including negative or very
    large positive numbers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'To form a proper probability distribution, the outputs must meet two additional
    conditions:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The outputs must be non-negative.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs must sum up to 1.0.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A special activation function called **softmax** has been designed for this
    exact purpose and is used for the output layer when probabilities are expected.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical formulation of the softmax function is as follows: ![](img/B22495_03_Equations14.png)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the application of the exponential function in the numerator maps
    the range from negative to positive infinity to that of non-negative numbers (![](img/B22495_03_Equations15.png)
    for all x). By dividing by the sum of all exponents, you normalize the values
    to ensure that the sum of outputs exactly adds up to 1.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The targets for training the model also need to contain vectors of the same
    length (one value per token). Since the next word at each step in the token sequence
    is known, you can encode the correct token with **one-hot encoding**. You can
    assign a value of 1.0 to the correct token’s position in the vector and 0.0 to
    all other positions, as shown in *Figure 3**.5*. This ensures that during the
    backward pass, the probability of seeing the correct next token is increased while
    all other probabilities are decreased.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_03_05.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Example output probabilities as predicted by the model and targets
    for the token *fox*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: With tokenization, embedding, and softmax activation, you can convert language
    into a numeric format that an ANN can understand. Further, the ANN can interpret
    the numeric outputs of the model as a discrete probability distribution over the
    next token. The final missing piece to model language with ANNs is the processing
    of sequences, which are discussed next.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with sequential data
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To produce good next-token predictions, a language model needs to be able to
    consider a sizeable context, reaching back many words or even sentences.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this, consider the following text:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '*A solitary tiger stealthily stalks its prey in the dense jungle. The underbrush
    whispers as* **it** *attacks, concealing* **its** *advance toward an* *unsuspecting
    fawn.*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'The second sentence in this example contains two pronouns, *it* and *its* (shown
    in bold above), both referring to the *tiger* from the previous sentence, many
    words apart. But without seeing the first sentence, you’d likely assume that *it*
    refers to the underbrush instead, which would have led to a very different sentence
    ending, such as this one:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '*The underbrush whispers as* **it** *sways gently in the* *soft breeze.*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: This shows long-range context matters for language modeling and next-token prediction.
    You can construct examples of arbitrary length where the pronoun resolution relies
    on the context provided many sentences earlier. These temporal dependencies and
    ambiguities are inherent to natural language, so a good language model needs to
    process long sequences of words.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: However, the FFN architecture introduced earlier is stateless and does not possess
    any memory of previously seen inputs. It is not suitable for sequential tasks,
    where future tokens depend on and refer to previous tokens.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Sequence learning is a fundamental problem in ML, not just for NLP but many
    other areas, such as time series prediction, speech recognition, video understanding,
    robot control, etc. In some cases, the inputs are sequential, in others the outputs
    are sequential, or even both. Different modifications to the FFN architecture
    have been proposed to tackle this problem.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One class of ANNs that deals with sequential data is called **recurrent neural
    networks** (**RNNs**). Unlike FFNs, RNNs include connections from a neuron to
    itself and its neighboring neurons within the same layer. These recurrent connections
    give the model an internal state, where previous activations can flow in a circular
    fashion and remain in the network when processing the next input, as depicted
    in *Figure 3**.6*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_03_06.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Recurrent connections give RNNs an internal state'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The training of RNNs remains like that of FFNs, where the RNN can be *unrolled*
    across time steps, and conceptually transformed into an FFN (albeit with many
    more layers and additional inputs corresponding to the internal states).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: However, one limitation of RNNs is that the gradients quickly diminish with
    each iteration through a recurrent connection. The network tends to *forget* activations
    that go back more than a few time steps, an issue known as the **vanishing** **gradient
    problem**.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, further architectural changes have been proposed, including
    **long short-term memory** (**LSTM**) and **gated recurrent unit** (**GRU**) networks.
    In these models, cells consisting of multiple neurons are introduced, which can
    trap the gradient signal inside over thousands of time steps, thus alleviating
    the vanishing gradient problem.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs have been applied successfully to many sequence problem domains, including
    robotics, speech and handwriting recognition, language translation, and playing
    video games.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: However, the training of recurrent networks happens sequentially along the time
    dimension, meaning that each time step requires a separate forward and backward
    pass through the network. This slows down training significantly, particularly
    for long sequences.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: RNNs have another limitation. Though the network can, in principle, remember
    previous activations due to its recurrent connections, this internal state needs
    to be carried forward for each time step. The model does not have direct access
    to the global context and its previous inputs explicitly.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Both limitations were addressed by a breakthrough discovery in 2017, which is
    discussed in the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Transformer architecture
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2017, Google published a new network architecture aimed to address some of
    the drawbacks of recurrent networks. This now famous paper, titled *Attention
    Is All You Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    introduced the **Transformer** architecture, which departed from the idea of recurrent
    connections and instead relied on an attention mechanism to consider previous
    tokens in an otherwise stateless neural network. This publication marked a significant
    shift in the field of ML and NLP and paved the way for almost all modern LLMs
    as variations of the original transformer.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Their advantages over recurrent networks—including the ability to process sequences
    in parallel, reduced computational complexity for long sequences, and superior
    handling of long-range dependencies—are key reasons why transformer architectures
    have become ubiquitous in the domain of NLP and beyond.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the original transformer model consists of two components:
    an encoder and a decoder. This architecture was proposed for the purpose of language
    translation, a sequence-to-sequence learning task with an input sequence of tokens
    in the source language processed by **the encoder**, and an output sequence of
    tokens in the target language processed by **the decoder**.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: While some LLMs still use this encoder/decoder structure, other families of
    models nowadays use simplified architectures building only on the encoder (such
    as BERT language models and variants) or the decoder (the GPT family). Generative
    models, including OpenAI’s GPT series, Meta’s Llama, Anthropic’s Claude, and Google’s
    PaLM models, all frame language modeling as next-token prediction, where the learning
    task is sequence-to-*single token*, as compared to sequence-to-sequence in the
    encoder/decoder structure. This allows for a simpler architecture, doing away
    with the encoder and only using the decoder part of a transformer.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Both the encoder and decoder of a transformer consist of many layers of so-called
    **transformer blocks**. Unlike FFNs, where each layer is simply a fully connected
    layer of neurons with the next, a transformer block has an additional attention
    layer preceding the fully connected layer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'The attention layer’s purpose is to learn which tokens in the sequence seen
    so far are most relevant when processing the current token. It assigns high attention
    weights to words that are highly relevant in the current context, and low attention
    weights to generic or irrelevant words, as you can see in *Figure 3**.7*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of food and drinks'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated with medium confidence](img/B22495_03_07.png)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: Attention maps for two sentence variations ending in *hungry* versus
    *tasty*'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.7* shows attention maps for two sentences where only the last word
    differs. Darker color shades indicate higher attention weights. A transformer
    model would learn to pay more attention to tokens related to *hungry*, such as
    *cat*, in the first example, and to tokens related to *tasty*, such as *food*,
    in the second example.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: This attention mechanism is key to transformers. The landmark paper on Transformer
    architecture demonstrated that this mechanism alone could solve the sequential
    data problem without introducing recurrent connections into the architecture.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: LLMs in practice
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, this chapter has mainly discussed the theoretical foundations of LLMs.
    Let’s close this chapter with an overview of the LLM landscape as it stands today,
    discussing some considerations for choosing an appropriate LLM as well as different
    techniques to tailor the model’s responses to your needs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The evolving field of LLMs
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI and LLMs are a rapidly changing field, with new models, frameworks,
    and research papers on the topic released frequently. Most of the know-how to
    train an LLM is publicly available, yet at the time of writing, the cost of training
    a state-of-the-art LLM from scratch is still in the order of tens to hundreds
    of millions of US dollars, due to the large amount of GPU compute resources needed.
    This cost puts training your own model out of reach of individuals and most smaller
    companies, who will have to rely on pre-trained LLMs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The most competent models as of the time of writing, namely OpenAI’s GPT-4o
    ([https://openai.com/](https://openai.com/)) and Anthropic’s Claude 3.5 Sonnet
    ([https://www.anthropic.com/](https://www.anthropic.com/)), remain closed source
    but can be accessed via APIs on a per-token cost model. Open-source models, such
    as Meta’s Llama 3 ([https://llama.meta.com/](https://llama.meta.com/)), are still
    behind on common benchmarks, but the gap is closing quickly. Depending on your
    use case and throughput requirements, it may be more cost-effective to self-host
    an open-source model or choose one of the many providers that offer model-hosting
    services.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Other considerations when choosing between open and closed models include security
    and compliance, technical support, and vendor lock-in. Commercial LLM offerings
    often come with technical support and moderation endpoints to filter illegal requests
    and harmful or objectionable content and provide detailed documentation for their
    APIs and models. Open models, in contrast, provide more flexibility and customization,
    as well as transparency and interoperability with other models, and avoid potential
    vendor lock-in.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Prompting, fine-tuning, and RAG
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs accept inputs in the form of text prompts (or simply prompts), which can
    be questions, statements, or requests that guide the model’s response. While the
    best LLMs are very capable and efficient in answering a wide range of different
    requests, chances are that a simple prompt may not lead to acceptable results
    for your application. Your use case may require special domain knowledge or responses
    in an uncommon (natural or programming) language that is under-represented in
    the original training dataset, or you may work with proprietary non-public data.
    This will not prevent you from integrating LLMs into your applications. There
    are several strategies available to deal with this scenario:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Different prompting strategies
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning an LLM on custom data
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval-augmented generation (RAG)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting an LLM is more of an art than a hard science, which has led to an
    entirely new “prompt engineer” role in software development. Common techniques
    include zero- and few-shot prompting and chain-of-thought prompting. For more
    advanced prompting techniques, you can refer to the *Prompt Engineering Guide*
    at [https://www.promptingguide.ai/](https://www.promptingguide.ai/). You’ll learn
    more about different prompting strategies in [*Chapter 9*](B22495_09.xhtml#_idTextAnchor193),
    *LLM* *Output Evaluation*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: For an even more custom-tailored response, pre-trained LLMs can be further trained
    on your own specific data through a process known as **fine-tuning**. Fine-tuning
    allows for adjustment of the language and style of the response, as well as injecting
    domain knowledge into the LLM. However, the process can be expensive depending
    on the dataset size. Fine-tuned models need to be evaluated carefully, as adjusting
    the weights may lead to overfitting, which can impact the model responses on previous
    tasks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval-augmented generation** (**RAG**) is another strategy to inject
    outside knowledge of proprietary data into an LLM. Here, an external knowledge
    base (for example, a vector database, which you will learn about in [*Chapter
    5*](B22495_05.xhtml#_idTextAnchor115), *Vector Databases*) is first queried with
    each request, and relevant information from the external data source is then included
    in the LLM prompt. While this alleviates some of the downsides of fine-tuning,
    one limiting factor is the length of the prompt (the context size) that the LLM
    can process in a single request. It is thus important to filter out irrelevant
    information to keep the prompt size manageable.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered the main components of a modern transformer-based LLM and
    a quick overview of the LLM landscape as it stands today.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: It detailed how text can be transformed into numeric data to be processed by
    ANNs. To summarize, sentences of a large text corpus are tokenized and assigned
    integer token IDs. Token IDs index into an embedding matrix, turning the integers
    into real-valued embedding vectors of fixed length. To create the targets for
    supervised training, the inputs are shifted by one token to the right, so that
    the target at each position becomes the token that follows in the sequence.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Sequential data can be learned with recurrent neural networks, but these have
    been superseded by transformers, which use an attention mechanism to learn which
    previous tokens are most relevant to predict the next. At every step in the sequence,
    the model predicts probabilities for each token in the vocabulary, which can be
    used to generate the next token.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset, consisting of inputs and targets, is split into smaller
    batches. With repeated forward and backward passes through the network, gradient
    calculation, and weight adjustments, the network learns to adjust the probabilities
    for each token given the context of previous tokens. You learned how these mechanisms
    have been put into practice by modern-day LLMs. You also got a brief introduction
    to some methods that can help you make the most of your language model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集，由输入和目标组成，被分成更小的批次。通过在网络中反复进行正向和反向传递、梯度计算和权重调整，网络学会根据先前标记的上下文调整每个标记的概率。你学习了这些机制是如何在现代大型语言模型（LLMs）中得到应用的。你还简要了解了可以帮助你最大限度地发挥语言模型作用的一些方法。
- en: In the next chapter, you will take this knowledge forward with an understanding
    of embedding models and their crucial role in machine learning.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将带着对嵌入模型及其在机器学习中的关键作用的理解，将这一知识向前推进。
