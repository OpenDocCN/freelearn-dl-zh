<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer027">
			<h1 id="_idParaDest-149" class="chapter-number"><a id="_idTextAnchor191"/>12</h1>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor192"/>Model Pruning</h1>
			<p>In this chapter, we’ll explore <strong class="bold">model pruning</strong> techniques designed to reduce model size while <span class="No-Break">maintaining performance.</span></p>
			<p>Model pruning <a id="_idIndexMarker606"/>refers to the systematic elimination of unnecessary parameters from a neural network while maintaining performance. For LLMs, this typically involves identifying and removing redundant or less important weights, neurons, or attention heads based on criteria such as magnitude, sensitivity analysis, or <span class="No-Break">gradient-based importance.</span></p>
			<p>You’ll learn how to implement various pruning methods, from magnitude-based pruning to iterative techniques, and the trade-offs involved in size reduction versus performance. Additionally, this chapter will help you decide whether to prune during or after training, ensuring your LLMs remain efficient <span class="No-Break">and effective.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Magnitude-based pruning</span></li>
				<li>Structured versus <span class="No-Break">unstructured pruning</span></li>
				<li>Iterative <span class="No-Break">pruning techniques</span></li>
				<li>Pruning during training versus <span class="No-Break">post-training pruning</span></li>
				<li>Balancing pruning and <span class="No-Break">model performance</span></li>
				<li>Combining pruning with other <span class="No-Break">compression techniques</span></li>
			</ul>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor193"/>Magnitude-based pruning</h1>
			<p><strong class="bold">Magnitude-based pruning</strong> is one <a id="_idIndexMarker607"/>of the simplest and most widely used pruning techniques. The idea behind this method is to remove weights in the neural network that contribute least to the model’s overall function, typically, these are weights with the smallest magnitude (absolute value). By pruning such weights, the model becomes more compact and faster, with minimal impact <span class="No-Break">on accuracy:</span></p>
			<pre class="source-code">
import torch
import torch.nn.utils.prune as prune
# Assume model is an instance of a pre-trained LLM
model = ...  # Load or define your LLM model
# Prune 30% of the lowest magnitude weights in all Linear layers
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.3)
# Remove the pruning reparameterization
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.remove(module, 'weight')</pre>			<p>In this code <a id="_idIndexMarker608"/>example, magnitude-based pruning removes 30% of the lowest-magnitude weights in all linear layers of an LLM. The <strong class="source-inline">prune.l1_unstructured</strong> function specifies that weights with the smallest L1 norm will <span class="No-Break">be pruned.</span></p>
			<p>The following code snippet implements the <strong class="source-inline">prune.l1_unstructured</strong> function for unstructured L1-norm-based pruning on a given parameter tensor within a PyTorch module by zeroing out weights with the smallest <span class="No-Break">absolute values:</span></p>
			<pre class="source-code">
    def prune.l1_unstructured(module, name, amount):
    """Prunes weights with lowest L1 norm magnitude in a module's tensor"""
    # Get the parameter to prune
    tensor = getattr(module, name)
    # Calculate number of parameters to prune
    n_params_to_prune = int(amount * tensor.numel())
    # Get magnitude threshold (kth smallest absolute value)
    threshold = torch.kthvalue(
        tensor.abs().view(-1), n_params_to_prune
    ).values
    # Create and apply mask (zeros out weights below threshold)
    mask = tensor.abs() &gt; threshold
    pruned_tensor = tensor.clone() * mask
    # Update parameter and register mask
    setattr(module, name, torch.nn.Parameter(pruned_tensor))
    module.register_buffer(f'{name}_mask', mask)
    # Add hook to maintain pruning during updates
    module.register_forward_pre_hook(
        lambda m, _: setattr(
            m, name,
            torch.nn.Parameter(
                getattr(m, name) * getattr(m, f'{name}_mask')
            )
        )
    )
    return mask</pre>			<p>Here, the function begins by extracting the target tensor from the module and determining how many of its elements should be pruned based on the specified proportion <strong class="source-inline">amount</strong>. It identifies a pruning threshold by computing the <em class="italic">k</em>-th smallest absolute value in the tensor, where <strong class="source-inline">k</strong> corresponds to the number of parameters to prune. A binary mask is then created, where values above the threshold are retained while those below <a id="_idIndexMarker609"/>the threshold are set to zero. This mask is applied to produce a pruned version of the tensor, which replaces the original parameter in the module. The mask is stored as a buffer to persist across model operations, and a forward pre-hook is registered to ensure that pruning is enforced before every forward pass, preserving the sparsity pattern even if the underlying weights are updated <span class="No-Break">during training.</span></p>
			<p>In model pruning, the L1 norm is used to evaluate the importance of weights or parameters in a model by summing the absolute values of their components, with lower L1 norm values often indicating less significant parameters that can be removed to reduce model size while <span class="No-Break">maintaining performance.</span></p>
			<p>After pruning, the <strong class="source-inline">prune.remove</strong> method <a id="_idIndexMarker610"/>is called to remove the pruning reparameterization and make the <span class="No-Break">changes permanent.</span></p>
			<p>Magnitude-based pruning is particularly effective for models with many small weights that contribute little to overall performance, but it may not be sufficient when applied alone for <span class="No-Break">large-scale pruning.</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor194"/>Structured versus unstructured pruning</h1>
			<p>When <a id="_idIndexMarker611"/>pruning LLMs, you can either prune <a id="_idIndexMarker612"/>weights individually (unstructured pruning) or remove entire structures, such as filters, channels, or attention heads (<span class="No-Break">structured pruning):</span></p>
			<ul>
				<li><strong class="bold">Unstructured pruning</strong>: This involves removing individual weights based on magnitude <a id="_idIndexMarker613"/>or other criteria. It provides more granularity but can result in sparse matrices, which are harder to optimize on standard hardware, as demonstrated in the <strong class="source-inline">prune.l1_unstructured</strong> function <span class="No-Break">described earlier.</span></li>
				<li><strong class="bold">Structured pruning</strong>: Entire <a id="_idIndexMarker614"/>sections of the model, such as neurons, channels, or layers, are pruned. This approach is easier to implement on modern hardware and often leads to better speedups in inference time, even though it may have a larger immediate effect on <span class="No-Break">model performance.</span></li>
			</ul>
			<p>Structured pruning in LLMs can be implemented using PyTorch’s built-in utilities, as shown in the following code. Here, we apply L2-norm structured pruning to remove 30% of neurons across linear layers, targeting entire rows of weight matrices to effectively eliminate complete neurons rather than just <span class="No-Break">individual connections:</span></p>
			<pre class="source-code">
import torch.nn.utils.prune as prune
# Structured pruning of entire neurons in a layer
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.ln_structured(
            module, name='weight', amount=0.3, n=2, dim=0
        )</pre>			<p>In this structured pruning example, the <strong class="source-inline">ln_structured</strong> function removes entire neurons from the linear layer based on the L2 norm across all weights in a given dimension. The choice of structured pruning can significantly reduce computational complexity <a id="_idIndexMarker615"/>while also making the model <a id="_idIndexMarker616"/>more suitable for deployment on standard <span class="No-Break">hardware architectures.</span></p>
			<p>Next, we’ll see how to prune a small fraction of weights at a time over multiple training steps instead of pruning large portions of the model in a <span class="No-Break">single pass.</span></p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor195"/>Iterative pruning techniques</h1>
			<p>Here, we’ll talk about <strong class="bold">iterative pruning</strong>, which allows you to prune a small fraction of weights at <a id="_idIndexMarker617"/>a time over multiple training steps. This method reduces the risk of drastic performance drops and provides more opportunities for the model to recover and adjust to <span class="No-Break">the pruning.</span></p>
			<p>The iterative <a id="_idIndexMarker618"/>approach also allows for fine-tuning after each pruning step, enabling the model to “heal” from the <span class="No-Break">weight reduction:</span></p>
			<pre class="source-code">
# Iteratively prune 10% of the model after every 10 epochs
for epoch in range(1, num_epochs+1):
    train(model, train_loader, optimizer)  # Regular training step
    if epoch % 10 == 0:
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                prune.l1_unstructured(module, name='weight',
                    amount=0.1)
                prune.remove(module, 'weight')  # Remove pruning mask after each step
    validate(model, val_loader)</pre>			<p>In this example, 10% of the weights are pruned after every 10 epochs. The gradual removal of weights ensures that the model has enough time to adjust between each pruning step. Iterative pruning, combined with validation steps, can help find a more optimal balance between model size <span class="No-Break">and <a id="_idTextAnchor196"/>performance.</span></p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor197"/>Pruning during training versus post-training pruning</h1>
			<p>A key decision <a id="_idIndexMarker619"/>in applying pruning is whether <a id="_idIndexMarker620"/>to prune the model during training or after training <span class="No-Break">is complete:</span></p>
			<ul>
				<li><strong class="bold">Pruning during training</strong>: This approach allows the model to adjust to the pruned <a id="_idIndexMarker621"/>structure over time by iteratively pruning weights as it learns. The model can compensate for pruned weights, potentially resulting in better final performance. However, it requires more computational resources and <span class="No-Break">training time.</span><p class="list-inset">Here’s an example of <span class="No-Break">this approach:</span></p><pre class="source-code">
import torch
import torch.nn.utils.prune as prune
# Assuming model is a pre-trained LLM
model = ...  # Load or define your LLM model
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()
def train(model, train_loader, optimizer):
    model.train()
    for batch in train_loader:
        inputs, targets = batch
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
# Prune 20% of the weights every 5 epochs during training
for epoch in range(1, 20):
    train(model, train_loader, optimizer)
    # Apply pruning every 5 epochs
    if epoch % 5 == 0:
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                prune.l1_unstructured(module, name='weight',
                    amount=0.2)
                prune.remove(module, 'weight')  # Remove reparameterization after each pruning</pre></li>				<li><strong class="bold">Post-training pruning</strong>: In this approach, pruning is performed after the model has <a id="_idIndexMarker622"/>been fully trained. This method is computationally efficient since it doesn’t require modifications during the training process, and you can optionally fine-tune the model afterward. However, it may result in a larger accuracy drop compared to pruning <span class="No-Break">during training.</span><p class="list-inset">Let’s take a look at an example of <span class="No-Break">post-training pruning:</span></p><pre class="source-code">
# Assuming the model has already been fully trained
model = ...  # Load or define your trained LLM model
# Prune 30% of the weights in all Linear layers after training
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.3)
# Optionally, fine-tune the model after pruning
fine_tune_epochs = 3
for epoch in range(fine_tune_epochs):
    train(model, train_loader, optimizer)  # Fine-tuning the pruned model</pre></li>			</ul>
			<p>The choice <a id="_idIndexMarker623"/>between these two <a id="_idIndexMarker624"/>depends on your performance constraints and available resources. Pruning during training often leads to more stable models, while post-training pruning is faster and <span class="No-Break">more r<a id="_idTextAnchor198"/>esource-efficient.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor199"/>Balancing pruning and model performance</h1>
			<p>Finding the right balance between pruning and model performance is critical. Aggressive pruning can <a id="_idIndexMarker625"/>lead to significant performance degradation, while too little pruning may not yield enough benefits. The key is to identify which parts of the model can be pruned with minimal impact on accuracy. This requires careful validation after each pruning step and close monitoring of key performance metrics. These metrics include parameter reduction rates, inference speed gains, memory footprint reduction, changes in perplexity, and task-specific performance. Throughout the process, it’s crucial to balance the accuracy-efficiency trade-off to ensure the pruned model retains acceptable performance despite havi<a id="_idTextAnchor200"/>ng <span class="No-Break">fewer parameters</span></p>
			<p>A common strategy is to apply fine-tuning after pruning to restore some of the lost performance. Fine-tuning allows the model to adjust to the pruned structure and recover its <span class="No-Break">original capabilities:</span></p>
			<pre class="source-code">
import torch.nn.utils.prune as prune
# Assuming model has been trained and pruned
model = ...  # Pruned LLM model
# Apply fine-tuning to restore performance after pruning
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Lower learning rate for fine-tuning
fine_tune_epochs = 5
for epoch in range(fine_tune_epochs):
    train(model, train_loader, optimizer)  # Reuse the train function from earlier
    validate(model, val_loader)  # Validation step to monitor performance</pre>			<p>In this example, after <a id="_idIndexMarker626"/>pruning a portion of the weights, the model is fine-tuned with a lower learning rate to restore performance. A lower learning rate allows the model to adjust gradually to the new pruned structure, preventing the destabilization of learned features. Validation is performed after each fine-tuning step to monitor the model’s progress and ensure that pruning has not significantly <span class="No-Break">degraded performance.</span></p>
			<p>Let’s see how we can combine pruning with other model <span class="No-Break">com<a id="_idTextAnchor201"/>pression techniques.</span></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor202"/>Combining pruning with other compression techniques</h1>
			<p>Pruning can <a id="_idIndexMarker627"/>be combined with other model compression techniques, such as quantization or distillation, to achieve even greater reductions in model size and complexity. Combining these techniques often results in more compact models that maint<a id="_idTextAnchor203"/>ain <span class="No-Break">high performance.</span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor204"/>Pruning and quantization</h2>
			<p>Pruning <a id="_idIndexMarker628"/>followed by <strong class="bold">quantization</strong> can lead to significant reductions <a id="_idIndexMarker629"/>in model size and faster inference speeds, especially for <span class="No-Break">resource-constrained environments:</span></p>
			<pre class="source-code">
import torch
import torch.nn.utils.prune as prune
import torch.quantization as quant
# Prune the model first
model = ...  # Pre-trained LLM
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.4)
        prune.remove(module, 'weight')
# Apply dynamic quantization after pruning
quantized_model = quant.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
# Check the size reduction
print("Original model size:", torch.cuda.memory_allocated())
print("Quantized model size:", torch.c<a id="_idTextAnchor205"/>uda.memory_allocated())</pre>			<h2 id="_idParaDest-158"><a id="_idTextAnchor206"/>Pruning and knowledge distillation</h2>
			<p>You can <a id="_idIndexMarker630"/>also combine <a id="_idIndexMarker631"/>pruning with <strong class="bold">knowledge distillation</strong>, where a smaller, pruned <strong class="bold">student model</strong> is trained to mimic the behavior of a larger, well-trained <span class="No-Break"><strong class="bold">teacher model</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
# Teacher and student models for knowledge distillation
teacher_model = ...  # Larger, fully trained model
student_model = ...  # Smaller model to be distilled and pruned
def distillation_loss(student_outputs, teacher_outputs, temperature):
    return torch.nn.KLDivLoss()(
        torch.nn.functional.log_softmax(
            student_outputs / temperature
        ),
        torch.nn.functional.softmax(
            teacher_outputs / temperature
        )
    )
# Train the smaller, pruned model using knowledge distillation
temperature = 2.0
optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)
for batch in train_loader:
    inputs, _ = batch
    teacher_outputs = teacher_model(inputs)
    student_outputs = student_model(inputs)
    loss = distillation_loss(
        student_outputs, teacher_outputs, temperature
    )
    loss.backward()
    optimizer.step()</pre>			<p>This <a id="_idIndexMarker632"/>approach allows the <a id="_idIndexMarker633"/>student model to achieve high performance with fewer parameters. Knowledge distillation helps compensate for accuracy loss caused by pruning by transferring high-level representations from the unpruned <span class="No-Break">teacher model.</span></p>
			<p>These examples illustrate how pruning can be applied during or after training, balanced with performance requirements, and combined with other compression techniques such as quantization and knowledge distillation to <a id="_idTextAnchor207"/>create more <span class="No-Break">efficient LLMs.</span></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor208"/>Summary</h1>
			<p>In this chapter, we explored various model pruning techniques for LLMs, including magnitude-based pruning, structured versus unstructured pruning, and iterative pruning methods. We discussed the trade-offs involved in pruning during training versus post-training, and the importance of fine-tuning after pruning to recover lost performance. By combining pruning with other compression techniques, such as quantization and distillation, you can create more efficient LLMs suitable for deployment in <span class="No-Break">resource-constrained environments.</span></p>
			<p>In the next chapter, we’ll explore quantization techniques for LLMs, focusing on reducing numerical precision to improve model efficiency while maintaining performance. You’ll learn how to apply post-training and quantization-aware training to optimize your <span class="No-Break">LLMs further.</span></p>
		</div>
	</div></div></body></html>