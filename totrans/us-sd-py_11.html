<html><head></head><body>
		<div><h1 id="_idParaDest-132" class="chapter-number"><a id="_idTextAnchor214"/>11</h1>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor215"/>Image Restore and Super-Resolution</h1>
			<p>While both Stable Diffusion V1.5 and Stable Diffusion XL have demonstrated abilities in generating images, our initial creations might not yet exhibit their utmost quality. This chapter aims to explore various techniques and strategies that can elevate image restoration, amplify image resolution, and introduce intricate details to produced visuals.</p>
			<p>The primary focus of this chapter lies in leveraging the potential of Stable Diffusion as an effective tool to enhance and upscale images. Furthermore, we will briefly introduce you to complementary cutting-edge <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) methodologies to boost image resolutions, which are distinct from diffusion-based processes.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding the terminologies</li>
				<li>Upscaling images using an image-to-image diffusion pipeline</li>
				<li>Upscaling images using ControlNet Tile</li>
			</ul>
			<p>Let’s start!</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor216"/>Understanding the terminologies</h1>
			<p>Before we begin using <a id="_idTextAnchor217"/>Stable Diffusion to enhance image quality, it is beneficial to understand the common terminologies associated with this process. Three related terms you may encounter <a id="_idIndexMarker343"/>in articles or books on Stable Diffusion are <strong class="bold">image interpolation</strong>, <strong class="bold">image upscale</strong>, and <strong class="bold">image super-resolution</strong>. These techniques aim to<a id="_idIndexMarker344"/> improve the <a id="_idIndexMarker345"/>resolution of an image, but they differ in their methods and outcomes. Familiarizing yourself with these terms will help you better understand how Stable Diffusion and other image enhancement tools work:</p>
			<ul>
				<li><strong class="bold">Image interpolation</strong> stands <a id="_idIndexMarker346"/>out as the simplest and most prevalent method to upscale images. It functions by approximating new pixel values based on the existing pixels within an image. Various interpolation methods exist, each with its strengths and weaknesses. Some of the most commonly used interpolation methods include nearest-neighbor interpolation [6], bilinear interpolation [7], bicubic interpolation [8], and Lanczos resampling [9].</li>
				<li><strong class="bold">Image upscale</strong> is a broader<a id="_idIndexMarker347"/> term encompassing any technique that augments an image’s resolution. This category includes interpolation methods, as well as more sophisticated approaches such as super-resolution.</li>
				<li><strong class="bold">Image super-resolution</strong> represents<a id="_idIndexMarker348"/> a specific subset of image upscaling aimed at enhancing an image’s resolution and finer details beyond its original dimensions while minimizing quality loss and preventing artifacts. In contrast to conventional image upscaling methods, which rely on basic interpolation, image super-resolution employs advanced algorithms, often based on deep learning techniques. These algorithms learn high-frequency patterns and details from a dataset of high-resolution images. Subsequently, these learned patterns are employed to upscale low-resolution images, producing superior-quality results.</li>
			</ul>
			<p>A solution to the aforementioned tasks (image interpolation, image upscale, and image super-resolution) is commonly<a id="_idIndexMarker349"/> referred to as <a id="_idIndexMarker350"/>an <strong class="bold">upscaler</strong> or a <strong class="bold">high-res fixer</strong>. We will use the term <em class="italic">upscaler</em> throughout the chapter.</p>
			<p>Among the deep learning-based solutions for super-resolution, various types of upscalers exist. Super-resolution solutions can be broadly classified into three types:</p>
			<ul>
				<li>GAN-based solutions such as ESRGAN [10]</li>
				<li>Swin Transformer-based solutions such as SwinIR [11],</li>
				<li>Stable Diffusion-based solutions</li>
			</ul>
			<p>In this chapter, our primary focus will be on the Stable Diffusion upscaler. This choice is driven not only by the book’s emphasis on the diffusion model but also by the potential of Stable Diffusion to provide enhanced upscale results and superior control flexibility. For instance, it allows us to direct the super-resolution process with prompts and fill in additional details. We will implement this using Python code in the latter part of this chapter.</p>
			<p>You might be eager to start utilizing the Latent Upscaler and Stable Diffusion Upscale pipeline [1] from Diffusers. However, it’s worth noting that the current Latent Upscaler and Upscale pipeline are not optimal. Both rely heavily on a specific pre-trained model, consume a substantial amount of VRAM, and exhibit relatively slower performance.</p>
			<p>This chapter will introduce two alternative solutions based on the Stable Diffusion approach:</p>
			<ul>
				<li><code>StableDiffusionPipeline</code> class. It also retains support for lengthy prompts and incorporates Textual Inversion, as introduced in previous chapters.</li>
				<li><code>ControlNet</code> model, it becomes possible to achieve image super-resolution with remarkable enhancements in detail.</li>
			</ul>
			<p>With this background, let’s delve into the intricate details of these two super-resolution m<a id="_idTextAnchor218"/>ethods.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor219"/>Upscaling images using Img2img diffusion</h1>
			<p>As we<a id="_idIndexMarker351"/> discussed in <a href="B21263_05.xhtml#_idTextAnchor097"><em class="italic">Chapter 5</em></a>, Stable Diffusion doesn’t solely rely on text as its initial guidance; it is also capable of utilizing an image as the starting point. We implemented a custom pipeline that employs an image as the foundation for image generation.</p>
			<p>By reducing the denoising strength to a certain threshold, such as <code>0.3</code>, the features and style of the initial image persist in the final generated image. This property can be exploited to employ Stable Diffusion as an image upscaler, thereby enabling image super-resolution. Let’s explore this process step by step.</p>
			<p>We will begin by introducing the concept of one-step super-resolution, followed by an exploration of multiple-step super-res<a id="_idTextAnchor220"/>olution.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor221"/>One-step super-resolution</h2>
			<p>In this section, we will cover a solution<a id="_idIndexMarker352"/> to upscale images using image-to-image diffusion once. Here are the step-by-step instructions to implement it:</p>
			<ol>
				<li>Let’s start by generating a 256x256 start image using Stable Diffusion. Instead of downloading an image from the internet or utilizing an external image as the input, let’s leverage Stable Diffusion to generate one. After all, this is the area where Stable Diffusion excels:<pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
text2img_pipe = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    "stablediffusionapi/deliberate-v2",</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
).to("cuda:0")</pre><pre class="source-code">
prompt = "a realistic photo of beautiful woman face"</pre><pre class="source-code">
neg_prompt = "NSFW, bad anatomy"</pre><pre class="source-code">
raw_image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    height = 256,</pre><pre class="source-code">
    width = 256,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(3)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(raw_image)</pre><p class="list-inset">The preceding code will generate an image, as shown in <em class="italic">Figure 11</em><em class="italic">.1</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_11_01.jpg" alt="Figure 11.1: A photo of a woman’s face sized 256x256, generated by Stable Diffusion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1: A photo of a woman’s face sized 256x256, generated by Stable Diffusion</p>
			<p class="list-inset">You may not<a id="_idIndexMarker353"/> be able to see the noise and blur in the image if you view it in a printed form (e.g., a paper book). However, if you run the preceding code and enlarge the image, you can easily notice the <em class="italic">blur</em> and <em class="italic">noise</em> in the generated image.</p>
			<p class="list-inset">Let’s save the image for further processing:</p>
			<pre class="source-code">
image_name = "woman_face"
file_name_256x256 =f"input_images/{image_name}_256x256.png"
raw_image.save(file_name_256x256)</pre>
			<ol>
				<li value="2">Resize the image to the target size. Initially, we need to establish a function for image adjustment, ensuring that the image’s width and height are both divisible by <code>8</code>:<pre class="source-code">
def get_width_height(width, height):</pre><pre class="source-code">
    width = (width//8)*8</pre><pre class="source-code">
    height = (height//8)*8</pre><pre class="source-code">
    return width,height</pre><p class="list-inset">Next, resize <a id="_idIndexMarker354"/>it to the target size using image interpolation:</p><pre class="source-code">
from diffusers.utils import load_image</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
def resize_img(img_path,upscale_times):</pre><pre class="source-code">
    img             = load_image(img_path)</pre><pre class="source-code">
    if upscale_times &lt;=0:</pre><pre class="source-code">
        return img</pre><pre class="source-code">
    width,height = img.size</pre><pre class="source-code">
    width = width * upscale_times</pre><pre class="source-code">
    height = height * upscale_times</pre><pre class="source-code">
    width,height = get_width_height(int(width),int(height))</pre><pre class="source-code">
    img = img.resize(</pre><pre class="source-code">
        (width,height),</pre><pre class="source-code">
        resample = Image.LANCZOS if upscale_times &gt; 1 \</pre><pre class="source-code">
            else Image.AREA</pre><pre class="source-code">
    )</pre><pre class="source-code">
    return img
Image.LANCZOS</strong> interpolation method.</pre><p class="list-inset">The following code employs the <code>resize_img</code> function to resize an image threefold:</p><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 3.0)</pre><p class="list-inset">Feel free to input any floating-point number greater than <code>1.0</code> into the function.</p></li>
				<li>Create an img-to-img pipeline as the upscaler. To enable guided image super-resolution, we need to provide a guided prompt, as shown here:<pre class="source-code">
sr_prompt = """8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo, """</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
a realistic photo of beautiful woman face</pre><pre class="source-code">
"""</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><p class="list-inset"><code>sr_prompt</code> means <strong class="bold">super-resolution prompt</strong> and<a id="_idIndexMarker356"/> can be resused without changing the same for any super-resolution tasks. Next, call the pipeline to upscale the image:</p><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
img2image_3x = img2img_pipe(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.3,</pre><pre class="source-code">
    num_inference_steps = 80,</pre><pre class="source-code">
    guidance_scale = 8,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
img2image_3x</pre><p class="list-inset">Note that the <code>strength</code> parameter is set to <code>0.3</code>, meaning that each denoising step applies 30% Gaussian noise to the latent. When utilizing the plain text-to-image pipeline, the strength is set to <code>1.0</code> by default. By increasing the <code>strength</code> value here, more <code>new</code> elements will be introduced to the initial image. From my testing, <code>0.3</code> seems to strike a well-balanced point. However, you have the flexibility to adjust it to values such as <code>0.25</code> or elevate it to <code>0.4</code>.</p></li>
			</ol>
			<p>For an img-to-img <a id="_idIndexMarker357"/>pipeline from Diffusers, the actual denoising steps will be a multiplication of <code>num_inference_steps</code> by <code>strength</code>. The total denoising steps will be 80 × 0.3 = 24. This is not a rule enforced by Stable Diffusion; it is sourced from the implementation of Diffusers’ Stable Diffusion pipeline.</p>
			<p>As explained in <a href="B21263_03.xhtml#_idTextAnchor064"><em class="italic">Chapter 3</em></a>, the <code>guidance_scale</code> parameter governs how closely the result aligns with the provided <code>prompt</code> and <code>neg_prompt</code>. In practice, a higher <code>guidance_scale</code> will yield a slightly clearer image but may also alter the image elements more, while a lower <code>guidance_scale</code> will lead to a more blurred image while preserving more original image elements. If you’re uncertain about the value, opt for something between 7 and 8.</p>
			<p>Once you run the preceding code, you’ll observe that not only does the size of the original image upscale to 768x768 but the image quality also experiences a significant enhancement.</p>
			<p>However, this is not the end; we can reuse the preceding process to further improve image resolution and quality.</p>
			<p>Let’s save the <a id="_idIndexMarker358"/>image for further usage:</p>
			<pre class="source-code">
file_name_768x768 = f"input_images/{image_name}_768x768.png"
img2image_3x.save(file_name_768x768)</pre>
			<p>Next, let’s upscale the image using multiple image-t<a id="_idTextAnchor222"/>o-image steps.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor223"/>Multiple-step super-resolution</h2>
			<p>Using the <a id="_idIndexMarker359"/>one-step resolution, the code upscales the image from 256x256 to 768x768. In this section, we’re taking the process a step further by increasing the image size to double its current dimensions.</p>
			<p>Note that before progressing to an even higher resolution image, you need to ensure that the utilization of VRAM might necessitate more than 8 GB.</p>
			<p>We will primarily be reusing the code from the one-step super-resolution process:</p>
			<ol>
				<li>Double the size of the image:<pre class="source-code">
resized_raw_image = resize_img(file_name_768x768, 2.0)</pre><pre class="source-code">
display(resized_raw_image)</pre></li>
				<li>Further image super-resolution code can be applied to increase the resolution of an image by six times (256x256 to 1,536x1,536), which can significantly enhance the clarity and details of the image:<pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
a realistic photo of beautiful woman face</pre><pre class="source-code">
"""</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
img2image_6x = img2img_pipe(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.3,</pre><pre class="source-code">
    num_inference_steps = 80,</pre><pre class="source-code">
    guidance_scale = 7.5,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
img2image_6x</pre></li>
			</ol>
			<p>The <a id="_idIndexMarker360"/>preceding code will produce an image that is six times the size of the original, greatly enhanc<a id="_idTextAnchor224"/>ing its quality.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor225"/>A super-resolution result comparison</h2>
			<p>Now, let’s<a id="_idIndexMarker361"/> examine the six-times-upscaled image and compare it with the original image to see how much the image quality has improved.</p>
			<p><em class="italic">Figure 11</em><em class="italic">.2</em> provides a side-by-side comparison of the original image and the six-times-upscaled super-resolution image:</p>
			<div><div><img src="img/B21263_11_02.jpg" alt="Figure 11.2: Left – the original raw image, and right – the image with the six-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2: Left – the original raw image, and right – the image with the six-times-upscaled super-resolution</p>
			<p>Kindly check<a id="_idIndexMarker362"/> out the ebook version to easily discern the finer enhancements. <em class="italic">Figure 11</em><em class="italic">.3</em> provides a clear depiction of the improvements in the mouth area:</p>
			<div><div><img src="img/B21263_11_03.jpg" alt="Figure 11.3: Left – the mouth from the original raw image, and right – the mouth from the image with six-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3: Left – the mouth from the original raw image, and right – the mouth from the image with six-times-upscaled super-resolution</p>
			<p><em class="italic">Figure 11</em><em class="italic">.4</em> shows the improvements to the eyes:</p>
			<div><div><img src="img/B21263_11_04.jpg" alt="Figure 11.4: Above – the eyes from the original raw image, and below – the eyes from the image with six-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4: Above – the eyes from the original raw image, and below – the eyes from the image with six-times-upscaled super-resolution</p>
			<p>Stable <a id="_idIndexMarker363"/>Diffusion enhances nearly every aspect of the image – from the eyebrows and eyelashes to the pupils – resulting in substantial improvements over the ori<a id="_idTextAnchor226"/>ginal raw image.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor227"/>Img-to-Img limitations</h2>
			<p>The <code>deliberate-v2</code> Stable Diffusion model is a<a id="_idIndexMarker364"/> checkpoint model built on SD v1.5, and it’s trained using 512x512 images. As a result, the img-to-img pipeline inherits all the constraints of this model. When attempting to upscale an image from 1,024x1,024 to an even higher resolution, the model might not be as efficient as it is with lower-resolution images.</p>
			<p>However, img-to-img is<a id="_idIndexMarker365"/> not the only available solution to generate exceptionally high-quality images. Next, we will explore another technique that can upscale images with ev<a id="_idTextAnchor228"/>en greater detail.</p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor229"/>ControlNet Tile image upscaling</h1>
			<p>Stable Diffusion <a id="_idIndexMarker366"/>ControlNet is a neural network architecture designed to enhance diffusion models through the incorporation of additional conditions. The concept behind this model stems from a paper titled <em class="italic">Adding Conditional Control to Text-to-Image Diffusion Models</em> [3], authored by Lvmin Zhang and Maneesh Agrawala in 2023. For more details about ControlNet, refer to <a href="B21263_13.xhtml#_idTextAnchor257"><em class="italic">Chapter 13</em></a>.</p>
			<p>ControlNet bears similarities to the image-to-image Stable Diffusion pipeline but boasts significantly greater potency.</p>
			<p>When utilizing the img2img pipeline, we input the initial image, along with conditional text, to generate an image similar to the starting guidance image. In contrast, ControlNet employs one or more supplementary UNet models that work alongside the Stable Diffusion model. These UNet models process both the input prompt and the image concurrently, with results being merged back in each step of the UNet up-stage. A comprehensive exploration of ControlNet is provided in <a href="B21263_13.xhtml#_idTextAnchor257"><em class="italic">Chapter 13</em></a>.</p>
			<p>Compared with the image-to-image pipeline, ControlNet yields superior outcomes. Among the ControlNet models, ControlNet Tile stands out for its ability to upscale images by introducing substantial detail information to the original image.</p>
			<p>In the subsequent code, we will employ the latest ControlNet version, 1.1. The authors of the paper and model affirm that they will maintain the architecture consistently up until ControlNet V1.5. At the time of reading, the most recent ControlNet iteration might surpass v1.1. There’s a likelihood that you can repurpose the v1.1 code for later vers<a id="_idTextAnchor230"/>ions of ControlNet.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor231"/>Steps to use ControlNet Tile to upscale an image</h2>
			<p>Next, let’s use ControlNet Tile to upscale an image step by step:</p>
			<ol>
				<li>Initialize <a id="_idIndexMarker367"/>ControlNet Tile model. The following code will start a ControlNet v1.1 model. Note that when ControlNet starts from v1.1, the sub-type of ControlNet is specified by <code>subfolder = '</code><code>control_v11f1e_sd15_tile'</code>:<pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import ControlNetModel</pre><pre class="source-code">
controlnet = ControlNetModel.from_pretrained(</pre><pre class="source-code">
    'takuma104/control_v11',</pre><pre class="source-code">
    subfolder = 'control_v11f1e_sd15_tile',</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
)</pre><p class="list-inset">We can’t simply use ControlNet itself to do anything; we will need to spin up a Stable Diffusion V1.5 pipeline to work together with the ControlNet model.</p></li>
				<li>Initialize a Stable Diffusion v1.5 model pipeline. The primary advantage of using ControlNet lies in its compatibility with any checkpoint model that has been fine-tuned from the Stable Diffusion base model. We will continue to use the Stable Diffusion V1.5-based model due to its outstanding quality and relatively low VRAM requirements. Given these attributes, Stable Diffusion v1.5 is expected to retain its relevance for a considerable period:<pre class="source-code">
# load controlnet tile</pre><pre class="source-code">
from diffusers import StableDiffusionControlNetImg2ImgPipeline</pre><pre class="source-code">
# load checkpoint model with controlnet</pre><pre class="source-code">
pipeline = StableDiffusionControlNetImg2ImgPipeline. \ </pre><pre class="source-code">
from_pretrained(</pre><pre class="source-code">
    "stablediffusionapi/deliberate-v2",</pre><pre class="source-code">
    torch_dtype    = torch.float16,</pre><pre class="source-code">
    controlnet     = controlnet</pre><pre class="source-code">
)</pre><p class="list-inset">In the<a id="_idIndexMarker368"/> provided code, we furnish <code>controlnet</code> which we initialized in <em class="italic">step 1</em> as a parameter for the <code>StableDiffusionControlNetImg2ImgPipeline</code> pipeline. Additionally, the code closely resembles that of a standard Stable Diffusion pipeline.</p></li>
				<li>Resize the image. This is the same step we took in the image-to-image pipeline; we need to enlarge the image to the target size:<pre class="source-code">
image_name = "woman_face"</pre><pre class="source-code">
file_name_256x256 = f"input_images/{image_name}_256x256.png"</pre><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 3.0)</pre><pre class="source-code">
resized_raw_image</pre><p class="list-inset">The preceding code upsizes the image three times using the <code>LANCZOS</code> interpolation:</p><pre class="source-code">
Image super-resolution using ControlNet Tile</pre><pre class="source-code">
# upscale</pre><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
a realistic photo of beautiful woman face</pre><pre class="source-code">
"""</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
pipeline.to("cuda")</pre><pre class="source-code">
cn_tile_upscale_img = pipeline(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    control_image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.8,</pre><pre class="source-code">
    guidance_scale = 7,</pre><pre class="source-code">
    generator = torch.Generator("cuda"),</pre><pre class="source-code">
    num_inference_steps = 50</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
cn_tile_upscale_img</pre><p class="list-inset">We reuse <a id="_idIndexMarker369"/>the positive prompt and negative prompt from the image-to-image upscaler. The distinctions are outlined here:</p><ul><li>We assign the raw upscaled image to both the initial diffusion image, denoted as <code>image = resized_raw_image</code>, and the ControlNet start image, marked as <code>control_image = </code><code>resized_raw_image</code>.</li><li>The strength is configured to <code>0.8</code> in order to leverage the influence of ControlNet<a id="_idIndexMarker370"/> on denoising, thereby enhancing the generation process.</li></ul></li>
			</ol>
			<p>Note that we can lower the strength parameter to preserve as much orig<a id="_idTextAnchor232"/>inal image as possible.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor233"/>The ControlNet Tile upscaling result</h2>
			<p>With just a single round of three-fold<a id="_idIndexMarker371"/> super-resolution, we can significantly enhance our image by introducing an abundance of intricate details:</p>
			<div><div><img src="img/B21263_11_05.jpg" alt="Figure 11.5: Left – the original raw image, and right – the ControlNet Tile three-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5: Left – the original raw image, and right – the ControlNet Tile three-times-upscaled super-resolution</p>
			<p>When compared to the image-to-image upscaler, ControlNet Tile incorporates even more details. Upon zooming into the image, you can observe the addition of individual hair strands, leading to an overall improvement in image quality.</p>
			<p>To achieve a<a id="_idIndexMarker372"/> comparable outcome, the image-to-image approach would require multiple steps to upscale the image sixfold. In contrast, ControlNet Tile accomplishes the same outcome with a single round of threefold upscaling.</p>
			<p>Furthermore, ControlNet Tile offers the advantage of relatively lower VRAM usage compared to the <a id="_idTextAnchor234"/>image-to-image solution.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor235"/>Additional ControlNet Tile upscaling samples</h2>
			<p>The <a id="_idIndexMarker373"/>ControlNet Tile super-resolution can produce remarkable results for a wide array of photos and images. Here are a few additional samples achieved by using just a few lines of code to generate, resize, and upscale images, capturing intricate details:</p>
			<ul>
				<li><strong class="bold">Man’s face</strong>: The code to generate, resize, and upscale this image is as follows:<pre class="source-code">
# step 1. generate an image</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
Raw, analog a portrait of an 43 y.o. man ,</pre><pre class="source-code">
beautiful photo with highly detailed face by greg rutkowski and magali villanueve</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = "NSFW, bad anatomy"</pre><pre class="source-code">
text2img_pipe.to("cuda")</pre><pre class="source-code">
raw_image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    height = 256,</pre><pre class="source-code">
    width = 256,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(3)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(raw_image)</pre><pre class="source-code">
image_name = "man"</pre><pre class="source-code">
file_name_256x256 = f"input_images/{image_name}_256x256.png"</pre><pre class="source-code">
raw_image.save(file_name_256x256)</pre><pre class="source-code">
# step 2. resize image</pre><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 3.0)</pre><pre class="source-code">
display(resized_raw_image)</pre><pre class="source-code">
# step 3. upscale image</pre><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
pipeline.to("cuda")</pre><pre class="source-code">
cn_tile_upscale_img = pipeline(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    control_image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.8,</pre><pre class="source-code">
    guidance_scale = 7,</pre><pre class="source-code">
    generator = torch.Generator("cuda"),</pre><pre class="source-code">
    num_inference_steps = 50,</pre><pre class="source-code">
    # controlnet_conditioning_scale = 0.8</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(cn_tile_upscale_img)</pre><p class="list-inset">The <a id="_idIndexMarker374"/>result is shown in <em class="italic">Figure 11</em><em class="italic">.6</em>:</p></li>
			</ul>
			<div><div><img src="img/B21263_11_06.jpg" alt="Figure 11.6: Left – the original raw image, and right – the ControlNet Tile three-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6: Left – the original raw image, and right – the ControlNet Tile three-times-upscaled super-resolution</p>
			<ul>
				<li><strong class="bold">Old man</strong>: Here is the code to generate, resize, and upscale the image:<pre class="source-code">
# step 1. generate an image</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
A realistic photo of an old man, standing in the garden, flower and green trees around, face view</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = "NSFW, bad anatomy"</pre><pre class="source-code">
text2img_pipe.to("cuda")</pre><pre class="source-code">
raw_image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    height = 256,</pre><pre class="source-code">
    width = 256,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(3)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(raw_image)</pre><pre class="source-code">
image_name = "man"</pre><pre class="source-code">
file_name_256x256 = f"input_images/{image_name}_256x256.png"</pre><pre class="source-code">
raw_image.save(file_name_256x256)</pre><pre class="source-code">
# step 2. resize image</pre><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 4.0)</pre><pre class="source-code">
display(resized_raw_image)</pre><pre class="source-code">
# step 3. upscale image</pre><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
pipeline.to("cuda")</pre><pre class="source-code">
cn_tile_upscale_img = pipeline(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    control_image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.8,</pre><pre class="source-code">
    guidance_scale = 7,</pre><pre class="source-code">
    generator = torch.Generator("cuda"),</pre><pre class="source-code">
    num_inference_steps = 50,</pre><pre class="source-code">
    # controlnet_conditioning_scale = 0.8</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(cn_tile_upscale_img)</pre><p class="list-inset">The<a id="_idIndexMarker375"/> result is shown in <em class="italic">Figure 11</em><em class="italic">.7</em>:</p></li>
			</ul>
			<div><div><img src="img/B21263_11_07.jpg" alt="Figure 11.7: Left – the original raw image of the old man, and right – the ControlNet Tile four-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7: Left – the original raw image of the old man, and right – the ControlNet Tile four-times-upscaled super-resolution</p>
			<ul>
				<li><strong class="bold">Royal female</strong>: Here<a id="_idIndexMarker376"/> is the code to generate, resize, and upscale the image:<pre class="source-code">
# step 1. generate an image</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
upper body photo of royal female, elegant, pretty face, majestic dress,</pre><pre class="source-code">
sitting on a majestic chair, in a grand fantasy castle hall, shallow depth of field, cinematic lighting, Nikon D850,</pre><pre class="source-code">
film still, HDR, 8k</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = "NSFW, bad anatomy"</pre><pre class="source-code">
text2img_pipe.to("cuda")</pre><pre class="source-code">
raw_image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    height = 256,</pre><pre class="source-code">
    width = 256,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(7)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(raw_image)</pre><pre class="source-code">
image_name = "man"</pre><pre class="source-code">
file_name_256x256 = f"input_images/{image_name}_256x256.png"</pre><pre class="source-code">
raw_image.save(file_name_256x256)</pre><pre class="source-code">
# step 2. resize image</pre><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 4.0)</pre><pre class="source-code">
display(resized_raw_image)</pre><pre class="source-code">
# step 3. upscale image</pre><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
pipeline.to("cuda")</pre><pre class="source-code">
cn_tile_upscale_img = pipeline(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    control_image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.8,</pre><pre class="source-code">
    guidance_scale = 7,</pre><pre class="source-code">
    generator = torch.Generator("cuda"),</pre><pre class="source-code">
    num_inference_steps = 50,</pre><pre class="source-code">
    # controlnet_conditioning_scale = 0.8</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(cn_tile_upscale_img)</pre><p class="list-inset">The <a id="_idIndexMarker377"/>result is shown in <em class="italic">Figure 11</em><em class="italic">.8</em>:</p></li>
			</ul>
			<div><div><img src="img/B21263_11_08.jpg" alt="Figure 11.8: Left – the original raw royal female, and right – the ControlNet Tile four- times﻿-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8: Left – the original raw royal female, and right – the ControlNet Tile four- times<a id="_idTextAnchor236"/>-upscaled super-resolution</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor237"/>Summary</h1>
			<p>This chapter offered a summary of contemporary image upscaling and super-resolution methodologies, emphasizing their unique characteristics. The primary focus of the chapter was on two super-resolution techniques that leverage the capabilities of Stable Diffusion:</p>
			<ul>
				<li>Utilizing the Stable Diffusion image-to-image pipeline</li>
				<li>Implementing ControlNet Tile to upscale images while enhancing details</li>
			</ul>
			<p>Furthermore, we showcased additional examples of the ControlNet Tile super-resolution technique.</p>
			<p>If your goal is to preserve as many aspects of the original image as possible during upscaling, we recommend the image-to-image pipeline. Conversely, if you prefer an AI-driven approach that generates a wealth of detail, ControlNet Tile is the more appropriate option.</p>
			<p>In <a href="B21263_12.xhtml#_idTextAnchor240"><em class="italic">Chapter 12</em></a>, we will develop a scheduled prompt parser to allow us more accurate cont<a id="_idTextAnchor238"/>rol over image generation.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor239"/>References</h1>
			<ol>
				<li>Hugging Face – Super-Resolution: <a href="https://huggingface.co/docs/diffusers/v0.13.0/en/api/pipelines/stable_diffusion/upscale&#13;">https://huggingface.co/docs/diffusers/v0.13.0/en/api/pipelines/stable_diffusion/upscale</a></li>
				<li>Hugging Face – Ultra-fast ControlNet with Diffusers: <a href="https://huggingface.co/blog/controlnet&#13;">https://huggingface.co/blog/controlnet</a></li>
				<li>Lvmin Zhang, Maneesh Agrawala, Adding Conditional Control to Text-to-Image Diffusion Models: <a href="https://arxiv.org/abs/2302.05543&#13;">https://arxiv.org/abs/2302.05543</a></li>
				<li>Lvmin Zhang, ControlNet original implementation code: <a href="https://github.com/lllyasviel&#13;">https://github.com/lllyasviel</a></li>
				<li>Lvmin Zhang, ControlNet 1.1 Tile: <a href="https://github.com/lllyasviel/ControlNet-v1-1-nightly#controlnet-11-tile&#13;">https://github.com/lllyasviel/ControlNet-v1-1-nightly#controlnet-11-tile</a></li>
				<li>Nearest-neighbor interpolation: <a href="https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation&#13;">https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation</a></li>
				<li>Bilinear interpolation: <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation&#13;">https://en.wikipedia.org/wiki/Bilinear_interpolation</a></li>
				<li>Bicubic interpolation: <a href="https://en.wikipedia.org/wiki/Bicubic_interpolation&#13;">https://en.wikipedia.org/wiki/Bicubic_interpolation</a></li>
				<li>Lanczos resampling: <a href="https://en.wikipedia.org/wiki/Lanczos_resampling&#13;">https://en.wikipedia.org/wiki/Lanczos_resampling</a></li>
				<li>ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks: <a href="https://arxiv.org/abs/1809.00219&#13;">https://arxiv.org/abs/1809.00219</a></li>
				<li>SwinIR: Image Restoration Using Swin Transformer: <a href="https://arxiv.org/abs/2108.10257&#13;">https://arxiv.org/abs/2108.10257</a></li>
				<li>Python Pillow package: <a href="https://github.com/python-pillow/Pillow">https://github.com/python-pillow/Pillow</a></li>
			</ol>
		</div>
	</body></html>