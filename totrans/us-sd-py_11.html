<html><head></head><body>
		<div id="_idContainer073">
			<h1 id="_idParaDest-132" class="chapter-number"><a id="_idTextAnchor214"/>11</h1>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor215"/>Image Restore and Super-Resolution</h1>
			<p>While both Stable Diffusion V1.5 and Stable Diffusion XL have demonstrated abilities in generating images, our initial creations might not yet exhibit their utmost quality. This chapter aims to explore various techniques and strategies that can elevate image restoration, amplify image resolution, and introduce intricate details to <span class="No-Break">produced visuals.</span></p>
			<p>The primary focus of this chapter lies in leveraging the potential of Stable Diffusion as an effective tool to enhance and upscale images. Furthermore, we will briefly introduce you to complementary cutting-edge <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) methodologies to boost image resolutions, which are distinct from <span class="No-Break">diffusion-based processes.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding <span class="No-Break">the terminologies</span></li>
				<li>Upscaling images using an image-to-image <span class="No-Break">diffusion pipeline</span></li>
				<li>Upscaling images using <span class="No-Break">ControlNet Tile</span></li>
			</ul>
			<p><span class="No-Break">Let’s start!</span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor216"/>Understanding the terminologies</h1>
			<p>Before we begin using <a id="_idTextAnchor217"/>Stable Diffusion to enhance image quality, it is beneficial to understand the common terminologies associated with this process. Three related terms you may encounter <a id="_idIndexMarker343"/>in articles or books on Stable Diffusion are <strong class="bold">image interpolation</strong>, <strong class="bold">image upscale</strong>, and <strong class="bold">image super-resolution</strong>. These techniques aim to<a id="_idIndexMarker344"/> improve the <a id="_idIndexMarker345"/>resolution of an image, but they differ in their methods and outcomes. Familiarizing yourself with these terms will help you better understand how Stable Diffusion and other image enhancement <span class="No-Break">tools work:</span></p>
			<ul>
				<li><strong class="bold">Image interpolation</strong> stands <a id="_idIndexMarker346"/>out as the simplest and most prevalent method to upscale images. It functions by approximating new pixel values based on the existing pixels within an image. Various interpolation methods exist, each with its strengths and weaknesses. Some of the most commonly used interpolation methods include nearest-neighbor interpolation [6], bilinear interpolation [7], bicubic interpolation [8], and Lanczos <span class="No-Break">resampling [9].</span></li>
				<li><strong class="bold">Image upscale</strong> is a broader<a id="_idIndexMarker347"/> term encompassing any technique that augments an image’s resolution. This category includes interpolation methods, as well as more sophisticated approaches such <span class="No-Break">as super-resolution.</span></li>
				<li><strong class="bold">Image super-resolution</strong> represents<a id="_idIndexMarker348"/> a specific subset of image upscaling aimed at enhancing an image’s resolution and finer details beyond its original dimensions while minimizing quality loss and preventing artifacts. In contrast to conventional image upscaling methods, which rely on basic interpolation, image super-resolution employs advanced algorithms, often based on deep learning techniques. These algorithms learn high-frequency patterns and details from a dataset of high-resolution images. Subsequently, these learned patterns are employed to upscale low-resolution images, producing <span class="No-Break">superior-quality results.</span></li>
			</ul>
			<p>A solution to the aforementioned tasks (image interpolation, image upscale, and image super-resolution) is commonly<a id="_idIndexMarker349"/> referred to as <a id="_idIndexMarker350"/>an <strong class="bold">upscaler</strong> or a <strong class="bold">high-res fixer</strong>. We will use the term <em class="italic">upscaler</em> throughout <span class="No-Break">the chapter.</span></p>
			<p>Among the deep learning-based solutions for super-resolution, various types of upscalers exist. Super-resolution solutions can be broadly classified into <span class="No-Break">three types:</span></p>
			<ul>
				<li>GAN-based solutions such as <span class="No-Break">ESRGAN [10]</span></li>
				<li>Swin Transformer-based solutions such as <span class="No-Break">SwinIR [11],</span></li>
				<li>Stable <span class="No-Break">Diffusion-based solutions</span></li>
			</ul>
			<p>In this chapter, our primary focus will be on the Stable Diffusion upscaler. This choice is driven not only by the book’s emphasis on the diffusion model but also by the potential of Stable Diffusion to provide enhanced upscale results and superior control flexibility. For instance, it allows us to direct the super-resolution process with prompts and fill in additional details. We will implement this using Python code in the latter part of <span class="No-Break">this chapter.</span></p>
			<p>You might be eager to start utilizing the Latent Upscaler and Stable Diffusion Upscale pipeline [1] from Diffusers. However, it’s worth noting that the current Latent Upscaler and Upscale pipeline are not optimal. Both rely heavily on a specific pre-trained model, consume a substantial amount of VRAM, and exhibit relatively <span class="No-Break">slower performance.</span></p>
			<p>This chapter will introduce two alternative solutions based on the Stable <span class="No-Break">Diffusion approach:</span></p>
			<ul>
				<li><strong class="bold">Image super-resolution using the img-to-img pipeline</strong>: This approach enables the use of any SD (v1.5 and SDXL) based models and even permits the integration of LoRA to assist in image upscaling. Additionally, this solution is built upon the foundational <strong class="source-inline">StableDiffusionPipeline</strong> class. It also retains support for lengthy prompts and incorporates Textual Inversion, as introduced in <span class="No-Break">previous chapters.</span></li>
				<li><strong class="bold">ControlNet-based Tile super-resolution</strong>: This solution also offers compatibility with arbitrary pre-trained models. Through the incorporation of an additional <strong class="source-inline">ControlNet</strong> model, it becomes possible to achieve image super-resolution with remarkable enhancements <span class="No-Break">in detail.</span></li>
			</ul>
			<p>With this background, let’s delve into the intricate details of these two <span class="No-Break">super-resolution m<a id="_idTextAnchor218"/>ethods.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor219"/>Upscaling images using Img2img diffusion</h1>
			<p>As we<a id="_idIndexMarker351"/> discussed in <a href="B21263_05.xhtml#_idTextAnchor097"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, Stable Diffusion doesn’t solely rely on text as its initial guidance; it is also capable of utilizing an image as the starting point. We implemented a custom pipeline that employs an image as the foundation for <span class="No-Break">image generation.</span></p>
			<p>By reducing the denoising strength to a certain threshold, such as <strong class="source-inline">0.3</strong>, the features and style of the initial image persist in the final generated image. This property can be exploited to employ Stable Diffusion as an image upscaler, thereby enabling image super-resolution. Let’s explore this process step <span class="No-Break">by step.</span></p>
			<p>We will begin by introducing the concept of one-step super-resolution, followed by an exploration of <span class="No-Break">multiple-step super-res<a id="_idTextAnchor220"/>olution.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor221"/>One-step super-resolution</h2>
			<p>In this section, we will cover a solution<a id="_idIndexMarker352"/> to upscale images using image-to-image diffusion once. Here are the step-by-step instructions to <span class="No-Break">implement it:</span></p>
			<ol>
				<li>Let’s start by generating a 256x256 start image using Stable Diffusion. Instead of downloading an image from the internet or utilizing an external image as the input, let’s leverage Stable Diffusion to generate one. After all, this is the area where Stable <span class="No-Break">Diffusion excels:</span><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
text2img_pipe = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    "stablediffusionapi/deliberate-v2",</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
).to("cuda:0")</pre><pre class="source-code">
prompt = "a realistic photo of beautiful woman face"</pre><pre class="source-code">
neg_prompt = "NSFW, bad anatomy"</pre><pre class="source-code">
raw_image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    height = 256,</pre><pre class="source-code">
    width = 256,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(3)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(raw_image)</pre><p class="list-inset">The preceding code will generate an image, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B21263_11_01.jpg" alt="Figure 11.1: A photo of a woman’s face sized 256x256, generated by Stable Diffusion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1: A photo of a woman’s face sized 256x256, generated by Stable Diffusion</p>
			<p class="list-inset">You may not<a id="_idIndexMarker353"/> be able to see the noise and blur in the image if you view it in a printed form (e.g., a paper book). However, if you run the preceding code and enlarge the image, you can easily notice the <em class="italic">blur</em> and <em class="italic">noise</em> in the <span class="No-Break">generated image.</span></p>
			<p class="list-inset">Let’s save the image for <span class="No-Break">further processing:</span></p>
			<pre class="source-code">
image_name = "woman_face"
file_name_256x256 =f"input_images/{image_name}_256x256.png"
raw_image.save(file_name_256x256)</pre>
			<ol>
				<li value="2">Resize the image to the target size. Initially, we need to establish a function for image adjustment, ensuring that the image’s width and height are both divisible <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">8</strong></span><span class="No-Break">:</span><pre class="source-code">
def get_width_height(width, height):</pre><pre class="source-code">
    width = (width//8)*8</pre><pre class="source-code">
    height = (height//8)*8</pre><pre class="source-code">
    return width,height</pre><p class="list-inset">Next, resize <a id="_idIndexMarker354"/>it to the target size using <span class="No-Break">image interpolation:</span></p><pre class="source-code">
from diffusers.utils import load_image</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
def resize_img(img_path,upscale_times):</pre><pre class="source-code">
    img             = load_image(img_path)</pre><pre class="source-code">
    if upscale_times &lt;=0:</pre><pre class="source-code">
        return img</pre><pre class="source-code">
    width,height = img.size</pre><pre class="source-code">
    width = width * upscale_times</pre><pre class="source-code">
    height = height * upscale_times</pre><pre class="source-code">
    width,height = get_width_height(int(width),int(height))</pre><pre class="source-code">
    img = img.resize(</pre><pre class="source-code">
        (width,height),</pre><pre class="source-code">
        resample = Image.LANCZOS if upscale_times &gt; 1 \</pre><pre class="source-code">
            else Image.AREA</pre><pre class="source-code">
    )</pre><pre class="source-code">
    return img</pre><p class="list-inset">The <strong class="bold">PIL</strong> (<strong class="bold">Pillow</strong> [12]) <strong class="source-inline">resize</strong> image function scales the pixels to the desired dimensions. In the <a id="_idIndexMarker355"/>preceding code snippet, we utilize the <strong class="source-inline">Image.LANCZOS</strong> <span class="No-Break">interpolation method.</span></p><p class="list-inset">The following code employs the <strong class="source-inline">resize_img</strong> function to resize an <span class="No-Break">image threefold:</span></p><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 3.0)</pre><p class="list-inset">Feel free to input any floating-point number greater than <strong class="source-inline">1.0</strong> into <span class="No-Break">the function.</span></p></li>
				<li>Create an img-to-img pipeline as the upscaler. To enable guided image super-resolution, we need to provide a guided prompt, as <span class="No-Break">shown here:</span><pre class="source-code">
sr_prompt = """8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo, """</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
a realistic photo of beautiful woman face</pre><pre class="source-code">
"""</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><p class="list-inset"><strong class="source-inline">sr_prompt</strong> means <strong class="bold">super-resolution prompt</strong> and<a id="_idIndexMarker356"/> can be resused without changing the same for any super-resolution tasks. Next, call the pipeline to upscale <span class="No-Break">the image:</span></p><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
img2image_3x = img2img_pipe(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.3,</pre><pre class="source-code">
    num_inference_steps = 80,</pre><pre class="source-code">
    guidance_scale = 8,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
img2image_3x</pre><p class="list-inset">Note that the <strong class="source-inline">strength</strong> parameter is set to <strong class="source-inline">0.3</strong>, meaning that each denoising step applies 30% Gaussian noise to the latent. When utilizing the plain text-to-image pipeline, the strength is set to <strong class="source-inline">1.0</strong> by default. By increasing the <strong class="source-inline">strength</strong> value here, more <strong class="source-inline">new</strong> elements will be introduced to the initial image. From my testing, <strong class="source-inline">0.3</strong> seems to strike a well-balanced point. However, you have the flexibility to adjust it to values such as <strong class="source-inline">0.25</strong> or elevate it <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">0.4</strong></span><span class="No-Break">.</span></p></li>
			</ol>
			<p>For an img-to-img <a id="_idIndexMarker357"/>pipeline from Diffusers, the actual denoising steps will be a multiplication of <strong class="source-inline">num_inference_steps</strong> by <strong class="source-inline">strength</strong>. The total denoising steps will be <span class="_-----MathTools-_Math_Number">80</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.3</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">24</span>. This is not a rule enforced by Stable Diffusion; it is sourced from the implementation of Diffusers’ Stable <span class="No-Break">Diffusion pipeline.</span></p>
			<p>As explained in <a href="B21263_03.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, the <strong class="source-inline">guidance_scale</strong> parameter governs how closely the result aligns with the provided <strong class="source-inline">prompt</strong> and <strong class="source-inline">neg_prompt</strong>. In practice, a higher <strong class="source-inline">guidance_scale</strong> will yield a slightly clearer image but may also alter the image elements more, while a lower <strong class="source-inline">guidance_scale</strong> will lead to a more blurred image while preserving more original image elements. If you’re uncertain about the value, opt for something between 7 <span class="No-Break">and 8.</span></p>
			<p>Once you run the preceding code, you’ll observe that not only does the size of the original image upscale to 768x768 but the image quality also experiences a <span class="No-Break">significant enhancement.</span></p>
			<p>However, this is not the end; we can reuse the preceding process to further improve image resolution <span class="No-Break">and quality.</span></p>
			<p>Let’s save the <a id="_idIndexMarker358"/>image for <span class="No-Break">further usage:</span></p>
			<pre class="source-code">
file_name_768x768 = f"input_images/{image_name}_768x768.png"
img2image_3x.save(file_name_768x768)</pre>
			<p>Next, let’s upscale the image using multiple <span class="No-Break">image-t<a id="_idTextAnchor222"/>o-image steps.</span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor223"/>Multiple-step super-resolution</h2>
			<p>Using the <a id="_idIndexMarker359"/>one-step resolution, the code upscales the image from 256x256 to 768x768. In this section, we’re taking the process a step further by increasing the image size to double its <span class="No-Break">current dimensions.</span></p>
			<p>Note that before progressing to an even higher resolution image, you need to ensure that the utilization of VRAM might necessitate more than <span class="No-Break">8 GB.</span></p>
			<p>We will primarily be reusing the code from the one-step <span class="No-Break">super-resolution process:</span></p>
			<ol>
				<li>Double the size of <span class="No-Break">the image:</span><pre class="source-code">
resized_raw_image = resize_img(file_name_768x768, 2.0)</pre><pre class="source-code">
display(resized_raw_image)</pre></li>
				<li>Further image super-resolution code can be applied to increase the resolution of an image by six times (256x256 to 1,536x1,536), which can significantly enhance the clarity and details of <span class="No-Break">the image:</span><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
a realistic photo of beautiful woman face</pre><pre class="source-code">
"""</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
img2image_6x = img2img_pipe(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.3,</pre><pre class="source-code">
    num_inference_steps = 80,</pre><pre class="source-code">
    guidance_scale = 7.5,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
img2image_6x</pre></li>
			</ol>
			<p>The <a id="_idIndexMarker360"/>preceding code will produce an image that is six times the size of the original, greatly enhanc<a id="_idTextAnchor224"/>ing <span class="No-Break">its quality.</span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor225"/>A super-resolution result comparison</h2>
			<p>Now, let’s<a id="_idIndexMarker361"/> examine the six-times-upscaled image and compare it with the original image to see how much the image quality <span class="No-Break">has improved.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em> provides a side-by-side comparison of the original image and the six-times-upscaled <span class="No-Break">super-resolution image:</span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B21263_11_02.jpg" alt="Figure 11.2: Left – the original raw image, and right – the image with the six-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2: Left – the original raw image, and right – the image with the six-times-upscaled super-resolution</p>
			<p>Kindly check<a id="_idIndexMarker362"/> out the ebook version to easily discern the finer enhancements. <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em> provides a clear depiction of the improvements in the <span class="No-Break">mouth area:</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B21263_11_03.jpg" alt="Figure 11.3: Left – the mouth from the original raw image, and right – the mouth from the image with six-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3: Left – the mouth from the original raw image, and right – the mouth from the image with six-times-upscaled super-resolution</p>
			<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.4</em> shows the improvements to <span class="No-Break">the eyes:</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B21263_11_04.jpg" alt="Figure 11.4: Above – the eyes from the original raw image, and below – the eyes from the image with six-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4: Above – the eyes from the original raw image, and below – the eyes from the image with six-times-upscaled super-resolution</p>
			<p>Stable <a id="_idIndexMarker363"/>Diffusion enhances nearly every aspect of the image – from the eyebrows and eyelashes to the pupils – resulting in substantial improvements over the ori<a id="_idTextAnchor226"/>ginal <span class="No-Break">raw image.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor227"/>Img-to-Img limitations</h2>
			<p>The <strong class="source-inline">deliberate-v2</strong> Stable Diffusion model is a<a id="_idIndexMarker364"/> checkpoint model built on SD v1.5, and it’s trained using 512x512 images. As a result, the img-to-img pipeline inherits all the constraints of this model. When attempting to upscale an image from 1,024x1,024 to an even higher resolution, the model might not be as efficient as it is with <span class="No-Break">lower-resolution images.</span></p>
			<p>However, img-to-img is<a id="_idIndexMarker365"/> not the only available solution to generate exceptionally high-quality images. Next, we will explore another technique that can upscale images with ev<a id="_idTextAnchor228"/>en <span class="No-Break">greater detail.</span></p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor229"/>ControlNet Tile image upscaling</h1>
			<p>Stable Diffusion <a id="_idIndexMarker366"/>ControlNet is a neural network architecture designed to enhance diffusion models through the incorporation of additional conditions. The concept behind this model stems from a paper titled <em class="italic">Adding Conditional Control to Text-to-Image Diffusion Models</em> [3], authored by Lvmin Zhang and Maneesh Agrawala in 2023. For more details about ControlNet, refer to <a href="B21263_13.xhtml#_idTextAnchor257"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">.</span></p>
			<p>ControlNet bears similarities to the image-to-image Stable Diffusion pipeline but boasts significantly <span class="No-Break">greater potency.</span></p>
			<p>When utilizing the img2img pipeline, we input the initial image, along with conditional text, to generate an image similar to the starting guidance image. In contrast, ControlNet employs one or more supplementary UNet models that work alongside the Stable Diffusion model. These UNet models process both the input prompt and the image concurrently, with results being merged back in each step of the UNet up-stage. A comprehensive exploration of ControlNet is provided in <a href="B21263_13.xhtml#_idTextAnchor257"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">.</span></p>
			<p>Compared with the image-to-image pipeline, ControlNet yields superior outcomes. Among the ControlNet models, ControlNet Tile stands out for its ability to upscale images by introducing substantial detail information to the <span class="No-Break">original image.</span></p>
			<p>In the subsequent code, we will employ the latest ControlNet version, 1.1. The authors of the paper and model affirm that they will maintain the architecture consistently up until ControlNet V1.5. At the time of reading, the most recent ControlNet iteration might surpass v1.1. There’s a likelihood that you can repurpose the v1.1 code for later vers<a id="_idTextAnchor230"/>ions <span class="No-Break">of ControlNet.</span></p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor231"/>Steps to use ControlNet Tile to upscale an image</h2>
			<p>Next, let’s use ControlNet Tile to upscale an image step <span class="No-Break">by step:</span></p>
			<ol>
				<li>Initialize <a id="_idIndexMarker367"/>ControlNet Tile model. The following code will start a ControlNet v1.1 model. Note that when ControlNet starts from v1.1, the sub-type of ControlNet is specified by <strong class="source-inline">subfolder = '</strong><span class="No-Break"><strong class="source-inline">control_v11f1e_sd15_tile'</strong></span><span class="No-Break">:</span><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import ControlNetModel</pre><pre class="source-code">
controlnet = ControlNetModel.from_pretrained(</pre><pre class="source-code">
    'takuma104/control_v11',</pre><pre class="source-code">
    subfolder = 'control_v11f1e_sd15_tile',</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
)</pre><p class="list-inset">We can’t simply use ControlNet itself to do anything; we will need to spin up a Stable Diffusion V1.5 pipeline to work together with the <span class="No-Break">ControlNet model.</span></p></li>
				<li>Initialize a Stable Diffusion v1.5 model pipeline. The primary advantage of using ControlNet lies in its compatibility with any checkpoint model that has been fine-tuned from the Stable Diffusion base model. We will continue to use the Stable Diffusion V1.5-based model due to its outstanding quality and relatively low VRAM requirements. Given these attributes, Stable Diffusion v1.5 is expected to retain its relevance for a <span class="No-Break">considerable period:</span><pre class="source-code">
# load controlnet tile</pre><pre class="source-code">
from diffusers import StableDiffusionControlNetImg2ImgPipeline</pre><pre class="source-code">
# load checkpoint model with controlnet</pre><pre class="source-code">
pipeline = StableDiffusionControlNetImg2ImgPipeline. \ </pre><pre class="source-code">
from_pretrained(</pre><pre class="source-code">
    "stablediffusionapi/deliberate-v2",</pre><pre class="source-code">
    torch_dtype    = torch.float16,</pre><pre class="source-code">
    controlnet     = controlnet</pre><pre class="source-code">
)</pre><p class="list-inset">In the<a id="_idIndexMarker368"/> provided code, we furnish <strong class="source-inline">controlnet</strong> which we initialized in <em class="italic">step 1</em> as a parameter for the <strong class="source-inline">StableDiffusionControlNetImg2ImgPipeline</strong> pipeline. Additionally, the code closely resembles that of a standard Stable <span class="No-Break">Diffusion pipeline.</span></p></li>
				<li>Resize the image. This is the same step we took in the image-to-image pipeline; we need to enlarge the image to the <span class="No-Break">target size:</span><pre class="source-code">
image_name = "woman_face"</pre><pre class="source-code">
file_name_256x256 = f"input_images/{image_name}_256x256.png"</pre><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 3.0)</pre><pre class="source-code">
resized_raw_image</pre><p class="list-inset">The preceding code upsizes the image three times using the <span class="No-Break"><strong class="source-inline">LANCZOS</strong></span><span class="No-Break"> interpolation:</span></p><pre class="source-code">
Image super-resolution using ControlNet Tile</pre><pre class="source-code">
# upscale</pre><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
a realistic photo of beautiful woman face</pre><pre class="source-code">
"""</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
pipeline.to("cuda")</pre><pre class="source-code">
cn_tile_upscale_img = pipeline(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    control_image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.8,</pre><pre class="source-code">
    guidance_scale = 7,</pre><pre class="source-code">
    generator = torch.Generator("cuda"),</pre><pre class="source-code">
    num_inference_steps = 50</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
cn_tile_upscale_img</pre><p class="list-inset">We reuse <a id="_idIndexMarker369"/>the positive prompt and negative prompt from the image-to-image upscaler. The distinctions are <span class="No-Break">outlined here:</span></p><ul><li>We assign the raw upscaled image to both the initial diffusion image, denoted as <strong class="source-inline">image = resized_raw_image</strong>, and the ControlNet start image, marked as <strong class="source-inline">control_image = </strong><span class="No-Break"><strong class="source-inline">resized_raw_image</strong></span><span class="No-Break">.</span></li><li>The strength is configured to <strong class="source-inline">0.8</strong> in order to leverage the influence of ControlNet<a id="_idIndexMarker370"/> on denoising, thereby enhancing the <span class="No-Break">generation process.</span></li></ul></li>
			</ol>
			<p>Note that we can lower the strength parameter to preserve as much orig<a id="_idTextAnchor232"/>inal image <span class="No-Break">as possible.</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor233"/>The ControlNet Tile upscaling result</h2>
			<p>With just a single round of three-fold<a id="_idIndexMarker371"/> super-resolution, we can significantly enhance our image by introducing an abundance of <span class="No-Break">intricate details:</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B21263_11_05.jpg" alt="Figure 11.5: Left – the original raw image, and right – the ControlNet Tile three-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5: Left – the original raw image, and right – the ControlNet Tile three-times-upscaled super-resolution</p>
			<p>When compared to the image-to-image upscaler, ControlNet Tile incorporates even more details. Upon zooming into the image, you can observe the addition of individual hair strands, leading to an overall improvement in <span class="No-Break">image quality.</span></p>
			<p>To achieve a<a id="_idIndexMarker372"/> comparable outcome, the image-to-image approach would require multiple steps to upscale the image sixfold. In contrast, ControlNet Tile accomplishes the same outcome with a single round of <span class="No-Break">threefold upscaling.</span></p>
			<p>Furthermore, ControlNet Tile offers the advantage of relatively lower VRAM usage compared to the <a id="_idTextAnchor234"/><span class="No-Break">image-to-image solution.</span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor235"/>Additional ControlNet Tile upscaling samples</h2>
			<p>The <a id="_idIndexMarker373"/>ControlNet Tile super-resolution can produce remarkable results for a wide array of photos and images. Here are a few additional samples achieved by using just a few lines of code to generate, resize, and upscale images, capturing <span class="No-Break">intricate details:</span></p>
			<ul>
				<li><strong class="bold">Man’s face</strong>: The code to generate, resize, and upscale this image is <span class="No-Break">as follows:</span><pre class="source-code">
# step 1. generate an image</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
Raw, analog a portrait of an 43 y.o. man ,</pre><pre class="source-code">
beautiful photo with highly detailed face by greg rutkowski and magali villanueve</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = "NSFW, bad anatomy"</pre><pre class="source-code">
text2img_pipe.to("cuda")</pre><pre class="source-code">
raw_image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    height = 256,</pre><pre class="source-code">
    width = 256,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(3)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(raw_image)</pre><pre class="source-code">
image_name = "man"</pre><pre class="source-code">
file_name_256x256 = f"input_images/{image_name}_256x256.png"</pre><pre class="source-code">
raw_image.save(file_name_256x256)</pre><pre class="source-code">
# step 2. resize image</pre><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 3.0)</pre><pre class="source-code">
display(resized_raw_image)</pre><pre class="source-code">
# step 3. upscale image</pre><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
pipeline.to("cuda")</pre><pre class="source-code">
cn_tile_upscale_img = pipeline(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    control_image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.8,</pre><pre class="source-code">
    guidance_scale = 7,</pre><pre class="source-code">
    generator = torch.Generator("cuda"),</pre><pre class="source-code">
    num_inference_steps = 50,</pre><pre class="source-code">
    # controlnet_conditioning_scale = 0.8</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(cn_tile_upscale_img)</pre><p class="list-inset">The <a id="_idIndexMarker374"/>result is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B21263_11_06.jpg" alt="Figure 11.6: Left – the original raw image, and right – the ControlNet Tile three-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6: Left – the original raw image, and right – the ControlNet Tile three-times-upscaled super-resolution</p>
			<ul>
				<li><strong class="bold">Old man</strong>: Here is the code to generate, resize, and upscale <span class="No-Break">the image:</span><pre class="source-code">
# step 1. generate an image</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
A realistic photo of an old man, standing in the garden, flower and green trees around, face view</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = "NSFW, bad anatomy"</pre><pre class="source-code">
text2img_pipe.to("cuda")</pre><pre class="source-code">
raw_image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    height = 256,</pre><pre class="source-code">
    width = 256,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(3)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(raw_image)</pre><pre class="source-code">
image_name = "man"</pre><pre class="source-code">
file_name_256x256 = f"input_images/{image_name}_256x256.png"</pre><pre class="source-code">
raw_image.save(file_name_256x256)</pre><pre class="source-code">
# step 2. resize image</pre><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 4.0)</pre><pre class="source-code">
display(resized_raw_image)</pre><pre class="source-code">
# step 3. upscale image</pre><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
pipeline.to("cuda")</pre><pre class="source-code">
cn_tile_upscale_img = pipeline(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    control_image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.8,</pre><pre class="source-code">
    guidance_scale = 7,</pre><pre class="source-code">
    generator = torch.Generator("cuda"),</pre><pre class="source-code">
    num_inference_steps = 50,</pre><pre class="source-code">
    # controlnet_conditioning_scale = 0.8</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(cn_tile_upscale_img)</pre><p class="list-inset">The<a id="_idIndexMarker375"/> result is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B21263_11_07.jpg" alt="Figure 11.7: Left – the original raw image of the old man, and right – the ControlNet Tile four-times-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7: Left – the original raw image of the old man, and right – the ControlNet Tile four-times-upscaled super-resolution</p>
			<ul>
				<li><strong class="bold">Royal female</strong>: Here<a id="_idIndexMarker376"/> is the code to generate, resize, and upscale <span class="No-Break">the image:</span><pre class="source-code">
# step 1. generate an image</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
upper body photo of royal female, elegant, pretty face, majestic dress,</pre><pre class="source-code">
sitting on a majestic chair, in a grand fantasy castle hall, shallow depth of field, cinematic lighting, Nikon D850,</pre><pre class="source-code">
film still, HDR, 8k</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = "NSFW, bad anatomy"</pre><pre class="source-code">
text2img_pipe.to("cuda")</pre><pre class="source-code">
raw_image = text2img_pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    height = 256,</pre><pre class="source-code">
    width = 256,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(7)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(raw_image)</pre><pre class="source-code">
image_name = "man"</pre><pre class="source-code">
file_name_256x256 = f"input_images/{image_name}_256x256.png"</pre><pre class="source-code">
raw_image.save(file_name_256x256)</pre><pre class="source-code">
# step 2. resize image</pre><pre class="source-code">
resized_raw_image = resize_img(file_name_256x256, 4.0)</pre><pre class="source-code">
display(resized_raw_image)</pre><pre class="source-code">
# step 3. upscale image</pre><pre class="source-code">
sr_prompt = "8k, best quality, masterpiece, realistic, photo-realistic, ultra detailed, sharp focus, raw photo,"</pre><pre class="source-code">
prompt = f"{sr_prompt}{prompt}"</pre><pre class="source-code">
neg_prompt = "worst quality, low quality, lowres, bad anatomy"</pre><pre class="source-code">
pipeline.to("cuda")</pre><pre class="source-code">
cn_tile_upscale_img = pipeline(</pre><pre class="source-code">
    image = resized_raw_image,</pre><pre class="source-code">
    control_image = resized_raw_image,</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    strength = 0.8,</pre><pre class="source-code">
    guidance_scale = 7,</pre><pre class="source-code">
    generator = torch.Generator("cuda"),</pre><pre class="source-code">
    num_inference_steps = 50,</pre><pre class="source-code">
    # controlnet_conditioning_scale = 0.8</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(cn_tile_upscale_img)</pre><p class="list-inset">The <a id="_idIndexMarker377"/>result is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B21263_11_08.jpg" alt="Figure 11.8: Left – the original raw royal female, and right – the ControlNet Tile four- times﻿-upscaled super-resolution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8: Left – the original raw royal female, and right – the ControlNet Tile four- times<a id="_idTextAnchor236"/>-upscaled super-resolution</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor237"/>Summary</h1>
			<p>This chapter offered a summary of contemporary image upscaling and super-resolution methodologies, emphasizing their unique characteristics. The primary focus of the chapter was on two super-resolution techniques that leverage the capabilities of <span class="No-Break">Stable Diffusion:</span></p>
			<ul>
				<li>Utilizing the Stable Diffusion <span class="No-Break">image-to-image pipeline</span></li>
				<li>Implementing ControlNet Tile to upscale images while <span class="No-Break">enhancing details</span></li>
			</ul>
			<p>Furthermore, we showcased additional examples of the ControlNet Tile <span class="No-Break">super-resolution technique.</span></p>
			<p>If your goal is to preserve as many aspects of the original image as possible during upscaling, we recommend the image-to-image pipeline. Conversely, if you prefer an AI-driven approach that generates a wealth of detail, ControlNet Tile is the more <span class="No-Break">appropriate option.</span></p>
			<p>In <a href="B21263_12.xhtml#_idTextAnchor240"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, we will develop a scheduled prompt parser to allow us more accurate cont<a id="_idTextAnchor238"/>rol over <span class="No-Break">image generation.</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor239"/>References</h1>
			<ol>
				<li>Hugging Face – <span class="No-Break">Super-Resolution: </span><a href="https://huggingface.co/docs/diffusers/v0.13.0/en/api/pipelines/stable_diffusion/upscale&#13;"><span class="No-Break">https://huggingface.co/docs/diffusers/v0.13.0/en/api/pipelines/stable_diffusion/upscale</span></a></li>
				<li>Hugging Face – Ultra-fast ControlNet with <span class="No-Break">Diffusers: </span><a href="https://huggingface.co/blog/controlnet&#13;"><span class="No-Break">https://huggingface.co/blog/controlnet</span></a></li>
				<li>Lvmin Zhang, Maneesh Agrawala, Adding Conditional Control to Text-to-Image Diffusion <span class="No-Break">Models: </span><a href="https://arxiv.org/abs/2302.05543&#13;"><span class="No-Break">https://arxiv.org/abs/2302.05543</span></a></li>
				<li>Lvmin Zhang, ControlNet original implementation <span class="No-Break">code: </span><a href="https://github.com/lllyasviel&#13;"><span class="No-Break">https://github.com/lllyasviel</span></a></li>
				<li>Lvmin Zhang, ControlNet 1.1 <span class="No-Break">Tile: </span><a href="https://github.com/lllyasviel/ControlNet-v1-1-nightly#controlnet-11-tile&#13;"><span class="No-Break">https://github.com/lllyasviel/ControlNet-v1-1-nightly#controlnet-11-tile</span></a></li>
				<li>Nearest-neighbor <span class="No-Break">interpolation: </span><a href="https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation&#13;"><span class="No-Break">https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation</span></a></li>
				<li>Bilinear <span class="No-Break">interpolation: </span><a href="https://en.wikipedia.org/wiki/Bilinear_interpolation&#13;"><span class="No-Break">https://en.wikipedia.org/wiki/Bilinear_interpolation</span></a></li>
				<li>Bicubic <span class="No-Break">interpolation: </span><a href="https://en.wikipedia.org/wiki/Bicubic_interpolation&#13;"><span class="No-Break">https://en.wikipedia.org/wiki/Bicubic_interpolation</span></a></li>
				<li>Lanczos <span class="No-Break">resampling: </span><a href="https://en.wikipedia.org/wiki/Lanczos_resampling&#13;"><span class="No-Break">https://en.wikipedia.org/wiki/Lanczos_resampling</span></a></li>
				<li>ESRGAN: Enhanced Super-Resolution Generative Adversarial <span class="No-Break">Networks: </span><a href="https://arxiv.org/abs/1809.00219&#13;"><span class="No-Break">https://arxiv.org/abs/1809.00219</span></a></li>
				<li>SwinIR: Image Restoration Using Swin <span class="No-Break">Transformer: </span><a href="https://arxiv.org/abs/2108.10257&#13;"><span class="No-Break">https://arxiv.org/abs/2108.10257</span></a></li>
				<li>Python Pillow <span class="No-Break">package: </span><a href="https://github.com/python-pillow/Pillow"><span class="No-Break">https://github.com/python-pillow/Pillow</span></a></li>
			</ol>
		</div>
	</body></html>