<html><head></head><body>
        

                            
                    <h1 class="header-title">Face and Motion Detection</h1>
                
            
            
                
<p>Now it is time for us to get into a really neat application. We'll start off by using the open source package <a href="http://www.aforgenet.com/">http://www.aforgenet.com/</a> to build a face and motion detection application. To do this, you'll need to have a camera installed in your system to see live streaming video. From there, we will use that camera to detect faces as well as motion. In this chapter, we are going to show two separate examples: one for facial detection, the other for motion detection. We'll show you exactly what goes on, and just how fast you can add these capabilities into your application.</p>
<p>In this chapter, we will cover such topics as:</p>
<ul>
<li>Facial detection</li>
<li>Motion detection</li>
<li>How to use the local video-integrated camera</li>
<li>Image filtering/algorithms</li>
</ul>
<p>Let's start out with facial detection. In our example, I'm going to use my friendly little French bulldog to pose for us. Before I do that, please re-read the chapter title. No matter how many times you read it, you'll probably miss the key point here. Notice it says face <em>DETECTION</em> and not face <em>RECOGNITION</em>. This is so very important I wanted to stop and re-stress it. We are not trying to identify Joe, Bob, or Sally. We are trying to verify that, out of everything we see via our camera, we can <em>detect</em> that there is a face there. We are not concerned with whose face it is, just the fact that it is a face! It is so important that we understand this before moving on, otherwise your expectations will be so incorrectly biased that you are going to make yourself confused and upset, and we don't want that!</p>
<p>Facial detection, as I will stress again later, is the first part of facial recognition, a much more complicated beast. If you can't identify, out of all the things that are on the screen, one or more faces, then you'll never be able to recognize whose face it is!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>As a prerequisite, you will need Microsoft Visual Studio(any version) installed on your system. You will also need to access the open source accord framework at <a href="https://github.com/accord-net/framework">https://github.com/accord-net/framework</a>.</p>
<p>Check out the following video to see Code in Action: <a href="http://bit.ly/2xH0thh">http://bit.ly/2xH0thh</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Facial detection</h1>
                
            
            
                
<p>Now, let's take a quick look at our application. You should have the sample solution loaded into Microsoft Visual Studio:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-453 image-border" src="img/e561a83d-af1a-49e3-9a0a-c869351ace7b.png" style="width:16.83em;height:25.67em;"/></p>
<p>And here's a look at our sample application running. Say Hi to Frenchie everyone!</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1221 image-border" src="img/76dc37eb-27d2-48d3-9f37-61ecc39691ff.png" style="width:30.17em;height:24.92em;"/></p>
<p>As you can see, we have a very simple screen that is dedicated to our video capture device. In this case, the laptop camera is our video capture device. Frenchie is kindly posing in front of the camera for us, and as soon as we enable facial tracking, look what happens:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1222 image-border" src="img/5aedfcf2-47bb-416d-bdbf-5d99efa1227a.png" style="width:16.67em;height:18.75em;"/></p>
<p>The facial features of Frenchie are now being tracked. What you see surrounding Frenchie are the tracking containers (white boxes), and our angle detector (red line) displayed. As we move Frenchie around, the tracking container and angle detector will track him. That's all well and good, but what happens if we enable facial tracking on a real human face? As you can see in the following screenshot, the tracking containers and angles are tracking the face of our guest poser, just like it did for Frenchie:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-456 image-border" src="img/7ae5fc01-5b16-431b-a337-3dae7b8f0bb1.png" style="width:47.00em;height:29.33em;"/></p>
<p>As our poser moves his head from side to side, the camera tracks this, and you can see the angle detectors adjusting to what it recognizes as the angle of the face. In this case you will notice that the color space is in black and white and not color. This is a histogram back projection and is an option that you can change:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-457 image-border" src="img/d3d38715-814f-4d2c-a11f-55b668a4e872.png" style="width:51.08em;height:31.92em;"/></p>
<p class="mce-root"/>
<p>Even as we move farther away from the camera where other objects come into view, the facial detector can keep track of our face among the noise. This is exactly how the facial recognition systems you see in movies work, albeit more simplistic, and within minutes you can be up and running with your own facial recognition application too!:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1224 image-border" src="img/a360a506-8548-4e24-b9c5-fee01dd29139.png" style="width:53.92em;height:33.50em;"/></p>
<p>Now that we've seen the outside, let's look under the hood at what is going on.</p>
<p>We need to ask ourselves exactly what the problem is that we are trying to solve here. Well, we are trying to detect (notice I did not say recognize) facial images. While easy for a human, a computer needs very detailed instruction sets to accomplish this feat. Luckily for us there is a very famous algorithm called the Viola-Jones algorithm that will do the heavy lifting for us. Why did we pick this algorithm?:</p>
<ul>
<li>Very high detection rates and very low false positives.</li>
<li>Very good at real-time processing.</li>
<li>Very good at detecting faces from non-faces. Detecting faces is the first step in facial recognition.</li>
</ul>
<p>This algorithm requires that the camera has a full-frontal upright view of the face. To be detected, the face will need to be pointing straight towards the camera, not tilted, not looking up or down. Remember, for the moment, we are just interested in facial detection.</p>
<p>To delve into the technical side of things, our algorithm will require four stages to accomplish its job. They are:</p>
<ul>
<li>Haar feature selection</li>
<li>Creating an integral image</li>
<li>Adaboost training</li>
<li>Cascading classifiers</li>
</ul>
<p>We must start by stating that all human faces share some similar properties, such as the eye being darker than the upper cheeks, the nose bridge being brighter than the eyes, your forehead may be lighter than the rest of your face, and so on. Our algorithm matches these up by using what is known as <strong>Haar Features</strong>. We can come up with matchable facial features by looking at the location and size of the eyes, mouth, and bridge of the nose, and so forth. However, here's our problem.</p>
<p>In a 24x24 pixel window, there are a total of 162,336 possible features. Obviously, to try and evaluate them all would be prohibitively expensive, if it would work at all. So, we are going to work with a technique known as <strong>adaptive boosting</strong>, or more commonly, <strong>AdaBoost</strong>. It's another one for your buzzword list, you've heard it everywhere and perhaps even read about it. Our learning algorithm will use AdaBoost to select the best features and train classifiers to use them. Let's stop and talk about it for a moment.</p>
<p>AdaBoost can be used with many types of learning algorithm and is considered the best out-of-the-box algorithm for many tasks. You usually won't notice how good and fast it is until you switch to another algorithm and time it. I have done this countless times, and I can tell you the difference is very noticeable.</p>
<p>Boosting takes the output from other weak-learning algorithms and combines them with a weighted sum that is the final output of the boosted classifier. The adaptive part of AdaBoost comes from the fact that subsequent learners are tweaked in favor of those instances that have been misclassified by previous classifiers. We must be careful with our data preparation though, as AdaBoost is sensitive to noisy data and outliers (remember how we stressed those in <a href="a48527d5-838a-4f47-a4fd-505597aeed58.xhtml">Chapter 1</a>, <em>A Quick Refresher</em>). The algorithm tends to overfit the data more than other algorithms, which is why in our earlier chapters we stressed data preparation for missing data and outliers. In the end, if <em>weak</em> learning algorithms are better than random guessing, AdaBoost can be a valuable addition to our process.</p>
<p>With that brief description behind us, let's look under the covers at what's happening. For this example, we will again use the Accord framework and we will work with the Vision Face Tracking sample. You can download the latest version of this framework from its GitHub location: <a href="https://github.com/accord-net/framework">https://github.com/accord-net/framework</a>.</p>
<p>We start by creating a <kbd>FaceHaarCascade</kbd> object. This object holds a collection of Haar-like features' weak classification stages, or stages. There will be many stages provided, each containing a set of classifier trees that will be used in the decision-making process. We are now technically working with a decision tree. The beauty of the Accord framework is that <kbd>FaceHaarCascade</kbd> automatically creates all these stages and trees for us without exposing us to the details.</p>
<p>Let's take a look at what a particular stage might look like:</p>
<pre>List&lt;HaarCascadeStage&gt; stages = new List&lt;HaarCascadeStage&gt;();<br/>List&lt;HaarFeatureNode[]&gt; nodes;<br/>HaarCascadeStage stage;<br/>stage = new HaarCascadeStage(0.822689414024353); nodes = new List&lt;HaarFeatureNode[]&gt;();<br/>nodes.Add(new[] { new HaarFeatureNode(0.004014195874333382, 0.0337941907346249, 0.8378106951713562, new int[] { 3, 7, 14, 4, -1 }, new int[] { 3, 9, 14, 2, 2 }) });<br/>nodes.Add(new[] { new HaarFeatureNode(0.0151513395830989, 0.1514132022857666, 0.7488812208175659, new int[] { 1, 2, 18, 4, -1 }, new int[] { 7, 2, 6, 4, 3 }) });<br/>nodes.Add(new[] { new HaarFeatureNode(0.004210993181914091, 0.0900492817163467, 0.6374819874763489, new int[] { 1, 7, 15, 9, -1 }, new int[] { 1, 10, 15, 3, 3 }) });<br/>stage.Trees = nodes.ToArray(); stages.Add(stage);</pre>
<p>As you can see, we are building a decision tree underneath the hood by providing the nodes for each stage with the numeric values for each feature.</p>
<p>Once created, we can use our cascade object to create our <kbd>HaarObjectDetector</kbd>, which is what we will use for detection. It takes:</p>
<ul>
<li>Our facial cascade objects</li>
<li>The minimum window size to use when searching for objects</li>
<li>Our search mode—in our case, we are searching for only a single object</li>
<li>The re-scaling factor to use when re-scaling our search window during the search</li>
</ul>
<pre>HaarCascade cascade = new FaceHaarCascade();<br/>detector = new HaarObjectDetector(cascade, 25, ObjectDetectorSearchMode.Single, 1.2f,<br/>ObjectDetectorScalingMode.GreaterToSmaller);</pre>
<p>Once created, we are ready to tackle the topic of our video collection source. In our examples, we will simply use the local camera to capture all images. However, the Accord.Net framework makes it easy to use other sources for image capture, such as <kbd>.avi</kbd> files, animated <kbd>.jpg</kbd> files, and so forth.</p>
<p>We connect to the camera, select the resolution, and are ready to go:</p>
<pre>foreach (var cap in device?.VideoCapabilities)<br/> {<br/>if (cap.FrameSize.Height == 240)<br/>return cap;<br/>if (cap.FrameSize.Width == 320)<br/>return cap;<br/> }<br/>return device?.VideoCapabilities.Last();</pre>
<p>With the application now running and our video source selected, our application will look like the following. Once again, enter Frenchie the bulldog! Please excuse the mess, Frenchie is not the tidiest of pets!:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1225 image-border" src="img/4d9d9b29-f1e7-4ab7-b41b-c8fde389d082.png" style="width:24.83em;height:20.83em;"/></p>
<p>For this demonstration, you will notice that Frenchie is facing the camera, and in the background, we have 2 x 55" monitors, as well as many other items my wife likes to refer to as <em>junk</em> (we'll be proper and call it <em>noise</em>)! This is done to show how the face detection algorithm can distinguish Frenchie's face amongst everything else! If our detector cannot handle this, it is going to get lost in the noise and will be of little use to us.</p>
<p>With our video source now coming in, we need to be notified when a new frame is received so that we can process it, apply our markers, and so on. We do this by attaching to the <kbd>NewFrameReceived</kbd> event handler of the video source player, as follows. .NET developers should be very familiar with this:</p>
<pre>this.videoSourcePlayer.NewFrameReceived += new Accord.Video.NewFrameEventHandler(this.videoSourcePlayer_NewFrame);</pre>
<p>Let's look at what happens each time we are notified that a new video frame is available.</p>
<p>The first thing that we need to do is <kbd>downsample</kbd> the image to make it easier to work with:</p>
<pre>ResizeNearestNeighbor resize = new ResizeNearestNeighbor(160, 120);<br/>UnmanagedImagedownsample = resize.Apply(im);</pre>
<p>With the image in a more manageable size, we will process the frame. If we have not found a facial region, we will stay in tracking mode waiting for a frame that has a detectable face. If we have found a facial region, we will reset our tracker, locate the face, reduce its size in order to flush away any background noise, initialize the tracker, and apply the marker window to the image. All of this is accomplished with the following code:</p>
<pre>if (regions != null&amp;&amp;regions.Length&gt;0)<br/> {<br/>tracker?.Reset();<br/>// Will track the first face found<br/>Rectangle face = regions[0];<br/>// Reduce the face size to avoid tracking background<br/>Rectangle window = new Rectangle((int)((regions[0].X + regions[0].Width / 2f) * xscale),<br/> (int)((regions[0].Y + regions[0].Height / 2f) * yscale), 1, 1);<br/>window.Inflate((int)(0.2f * regions[0].Width * xscale), (int)(0.4f * regions[0].Height * yscale));<br/>if (tracker != null)<br/> {<br/>tracker.SearchWindow = window;<br/>tracker.ProcessFrame(im);<br/> }<br/>marker = new RectanglesMarker(window);<br/>marker.ApplyInPlace(im);<br/>args.Frame = im.ToManagedImage();<br/>tracking = true;<br/> }<br/>else<br/> {<br/>detecting = true;<br/> }</pre>
<p>If a face was detected, our image frame now looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1226 image-border" src="img/1536495b-9168-4a57-a784-517d0591fe1b.png" style="width:19.42em;height:22.17em;"/></p>
<p>If Frenchie tilts his head to the side, our image frame now looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1228 image-border" src="img/ee9335bd-0879-4341-bbd9-24a00d11b2ae.png" style="width:25.83em;height:16.83em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Motion detection</h1>
                
            
            
                
<p>We will now make our focus a bit more wide-scale and detect any motion at all, not just faces. Again, we'll use Accord.Net for this and use the <kbd>Motion detection</kbd> sample. As with facial recognition, you will see just how simple it is to add this capability to your applications and instantly become a hero at work! Let's make sure you have the correct project loaded into Microsoft Visual Studio:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1229 image-border" src="img/cb0ee117-d5bf-4cbd-bd7c-30e9feb5217c.png" style="width:13.42em;height:20.25em;"/></p>
<p>With motion detection, anything that moves on the screen we will highlighted in red, so using the following screenshot you can see that the fingers are moving but everything else remains motionless:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1231 image-border" src="img/4b6f2db8-633c-40dd-a4ee-4ce16da9cfe4.png" style="width:20.58em;height:15.50em;"/></p>
<p>In the following screenshot, you can see more movement, denoted by the red blocks along this anonymous hand:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1232 image-border" src="img/3a9bc94e-95ee-49c2-83b7-9d93747dcc8a.png" style="width:24.42em;height:18.58em;"/></p>
<p>In the following screenshot, you can see that the entire hand is moving:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1233 image-border" src="img/be569970-5761-4a5a-852f-39054b3bad17.png" style="width:26.42em;height:24.17em;"/></p>
<p>If we do not wish to process the entire screen area for motion, we can define <em>motion regions</em>, where motion detection will occur only in those regions. In the following screenshot, you can see that I defined a motion region. You will notice in upcoming screenshots that this is the only area that motion will be processed from:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1234 image-border" src="img/b1e77ea7-22a7-4caa-82eb-233dc2796570.png" style="width:33.00em;height:30.08em;"/></p>
<p>Now, if we create some motion for the camera, you will see that only motion from our defined region is being processed, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1235 image-border" src="img/aa867f0c-b7af-419d-91f4-096416eb4758.png" style="width:26.08em;height:23.75em;"/></p>
<p>You can also see that, with a motion region defined and Peter the meditating Gnome in front of the region, we are still able to detect motion behind him, but his face is not part of the recognition. You could of course, combine both processes to have the best of both worlds, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1236 image-border" src="img/0e7bf3a5-7ad7-4340-a31a-aed2597fed13.png" style="width:27.50em;height:25.25em;"/></p>
<p>Another option that we can use is Grid Motion Highlighting. This highlights the motion detected region in red squares based upon a defined grid. Basically, the motion area is now a red box, as you can see:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1237 image-border" src="img/9622811a-aaa7-4bad-a7d3-b9d3b7064755.png" style="width:28.92em;height:26.58em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Code</h1>
                
            
            
                
<p>The following snippet shows a simple example of all you need to do to add video recognition to your application. As you can see, it couldn’t be any easier:</p>
<pre>// create motion detector<br/>MotionDetector detector = new MotionDetector(<br/> new SimpleBackgroundModelingDetector( ),<br/> new MotionAreaHighlighting( ) );<br/>// continuously feed video frames to motion detector<br/>while ( ... )<br/>{<br/> // process new video frame and check motion level<br/> if ( detector.ProcessFrame( videoFrame ) &gt; 0.02 )<br/> {<br/> // ring alarm or do something else<br/> }<br/>}</pre>
<p>We now open our video source:</p>
<pre>videoSourcePlayer.VideoSource = new AsyncVideoSource(source);</pre>
<p>When we receive a new video frame, that's when all the magic happens. The following are all the code it takes to make processing a new video frame a success:</p>
<pre>private void videoSourcePlayer_NewFrame(object sender, NewFrameEventArgsargs)<br/> {<br/>lock (this)<br/> {<br/>if (detector != null)<br/> {<br/>floatmotionLevel = detector.ProcessFrame(args.Frame);<br/>if (motionLevel &gt; motionAlarmLevel)<br/> {<br/>// flash for 2 seconds<br/>flash = (int)(2 * (1000 / alarmTimer.Interval));<br/> }<br/>// check objects' count<br/>if (detector.MotionProcessingAlgorithm is BlobCountingObjectsProcessing)<br/> {<br/>BlobCountingObjectsProcessing countingDetector = (BlobCountingObjectsProcessing)detector.MotionProcessingAlgorithm;<br/>detectedObjectsCount = countingDetector.ObjectsCount;<br/> }<br/>else<br/> {<br/>detectedObjectsCount = -1;<br/> }<br/>// accumulate history<br/>motionHistory.Add(motionLevel);<br/>if (motionHistory.Count&gt; 300)<br/>                    {<br/>motionHistory.RemoveAt(0);<br/>                    }<br/><br/>if (showMotionHistoryToolStripMenuItem.Checked)<br/>DrawMotionHistory(args.Frame);<br/>                }<br/>            }</pre>
<p>The key here is detecting the amount of motion that is happening in the frame, which is done with the following code. For this example, we are using a motion alarm level of 0.2, but you can use whatever you like. Once this threshold has been passed, you can do whatever logic you like such as send an email alert, text, start a video capture operation, and so forth:</p>
<pre>float motionLevel = detector.ProcessFrame(args.Frame);<br/>if (motionLevel &gt; motionAlarmLevel)<br/>{<br/>// flash for 2 seconds<br/>flash = (int)(2 * (1000 / alarmTimer.Interval));<br/>}</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned about image and motion detection (not recognition!). We used Accord.Net as an example of what open-source tools provide us with when we want to add power to our applications.</p>
<p>In the next chapter, we remain with the image theme, but work on training Convolutional Neural Networks with the open-source package ConvNetSharp.</p>


            

            
        
    </body></html>