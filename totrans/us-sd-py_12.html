<html><head></head><body>
		<div id="_idContainer079">
			<h1 id="_idParaDest-146" class="chapter-number"><a id="_idTextAnchor240"/>12</h1>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor241"/>Scheduled Prompt Parsing</h1>
			<p>In <a href="B21263_10.xhtml#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we discussed how to unlock the 77-token prompt limitation and a solution to enable prompt weighting, which paved the way for this chapter. With the knowledge from <a href="B21263_10.xhtml#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we can generate various kinds of images by leveraging the power of natural language and weighting formats. However, there are some limitations inherent in the out-of-the-box code from the Hugging Face <span class="No-Break">Diffusers package.</span></p>
			<p>For example, we cannot write a prompt to ask Stable Diffusion to generate a cat in the first five steps and then a dog in the next five steps. Similarly, we cannot write a prompt to ask Stable Diffusion to blend two concepts by alternately denoising the <span class="No-Break">two concepts.</span></p>
			<p>In this chapter, we will explore the two solutions in the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Using the <span class="No-Break">Compel package</span></li>
				<li>Building a custom scheduled <span class="No-Break">prompt pipeline</span></li>
			</ul>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor242"/>Technical requirements</h1>
			<p>To get started with the code in this chapter, you will need to install the necessary packages for running Stable Diffusion. For detailed instructions on how to set up these packages, refer to <a href="B21263_02.xhtml#_idTextAnchor037"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">.</span></p>
			<p>In addition to the packages required by Stable Diffusion, you will also need to install the <strong class="source-inline">Compel</strong> package for the <em class="italic">Using the Compel package</em> section, and the <strong class="source-inline">lark</strong> package for the <em class="italic">Building a custom scheduled prompt </em><span class="No-Break"><em class="italic">pipeline</em></span><span class="No-Break"> section.</span></p>
			<p>I will provide step-by-step instructions for installing and using these packages in <span class="No-Break">each section.</span></p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor243"/>Using the Compel package</h1>
			<p>Compel [1] is an open source text <a id="_idIndexMarker378"/>prompt-weighting and blending library developed and maintained by Damian Stewart. It is one of the easiest ways to enable blending prompts in Diffusers. This package also has the capability to apply weighting to prompts, similar to the solution we implemented in <a href="B21263_10.xhtml#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, but with a different weighting syntax. In this chapter, I will introduce the blending feature that can help us write a prompt to generate an image with two or more <span class="No-Break">concepts blended.</span></p>
			<p>Imagine that<a id="_idIndexMarker379"/> we want to create a photo that is half cat and half dog. How would we do it with prompts? Let’s say we simply give Stable Diffusion the <span class="No-Break">following prompt:</span></p>
			<pre class="source-code">
A photo with half cat and half dog</pre>
			<p>Here are the lines of Python code (without <span class="No-Break">using Compel):</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
pipeline = StableDiffusionPipeline.from_pretrained(
    "stablediffusionapi/deliberate-v2",
    torch_dtype = torch.float16,
    safety_checker = None
).to("cuda:0")
image = pipeline(
    prompt = "A photo with half cat and half dog",
    generator = torch.Generator("cuda:0").manual_seed(3)
).images[0]
image</pre>
			<p>You will see the result shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B21263_12_01.jpg" alt="Figure 12.1: Result of the A photo with half cat and half dog prompt"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1: Result of the A photo with half cat and half dog prompt</p>
			<p>The word <strong class="source-inline">half</strong> should <a id="_idIndexMarker380"/>be applied to the photo itself, rather than the image. In this case, we can use Compel to help generate a text embedding that blends cat <span class="No-Break">and dog.</span></p>
			<p>Before importing the <strong class="source-inline">Compel</strong> package, we will need to install <span class="No-Break">the package:</span></p>
			<pre class="source-code">
pip install compel</pre>
			<p>Note that the reason the <strong class="source-inline">Compel</strong> package works with Diffusers is that the package produces text embedding using <strong class="source-inline">tokenizer</strong> (type: <strong class="source-inline">transformers.models.clip.tokenization_clip.CLIPTokenizer</strong>) and <strong class="source-inline">text_encoder</strong> (type: <strong class="source-inline">transformers.models.clip.modeling_clip.CLIPTextModel</strong>) from the Stable Diffusion model file. We should also be aware of this during the initialization of the <span class="No-Break"><strong class="source-inline">Compel</strong></span><span class="No-Break"> object:</span></p>
			<pre class="source-code">
from comp
compel = Compel(
    tokenizer = pipeline.tokenizer,
    text_encoder = pipeline.text_encoder
)</pre>
			<p>The <strong class="source-inline">pipeline</strong> (type: <strong class="source-inline">StableDiffusionPipeline</strong>) is the Stable Diffusion pipeline <a id="_idIndexMarker381"/>we just created. Next, create a blend prompt using the <span class="No-Break">following format:</span></p>
			<pre class="source-code">
prompt = '("A photo of cat", "A photo of dog").blend(0.5, 0.5)'
prompt_embeds = compel(prompt)</pre>
			<p>Then, feed the text embedding into the Stable Diffusion pipeline through the <span class="No-Break"><strong class="source-inline">prompt_embeds</strong></span><span class="No-Break"> parameter:</span></p>
			<pre class="source-code">
image = pipeline(
    prompt_embeds = prompt_embeds,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
image</pre>
			<p>We will see a pet that looks like a cat and also a dog, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B21263_12_02.jpg" alt="Figure 12.2: A blended photo – half cat and half dog – using Compel"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2: A blended photo – half cat and half dog – using Compel</p>
			<p>We can <a id="_idIndexMarker382"/>change the proposition number of the blend to have more <strong class="source-inline">cat</strong> or more <strong class="source-inline">dog</strong>. Let’s change the prompt <span class="No-Break">to this:</span></p>
			<pre class="source-code">
prompt = '("A photo of cat", "A photo of dog").blend(0.7, 0.3)'</pre>
			<p>We will get a photo slightly more like a cat, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B21263_12_03.jpg" alt="Figure 12.3: A blended photo of a cat and dog using Compel – 70% cat, 30% dog"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3: A blended photo of a cat and dog using Compel – 70% cat, 30% dog</p>
			<p>Compel can do <a id="_idIndexMarker383"/>more than just prompt blending; it can also provide <strong class="source-inline">weighted</strong> and <strong class="source-inline">and</strong> conjunction prompts. You can explore more usage examples and features in its Syntax Features [<span class="No-Break">2] documentation.</span></p>
			<p>While it is easy to use Compel to blend prompts, as we have seen in the previous example, a blending prompt like the one that follows is strange and unintuitive for <span class="No-Break">day-to-day use:</span></p>
			<pre class="source-code">
prompt = '("A photo of cat", "A photo of dog").blend(0.7, 0.3)'</pre>
			<p>Upon my<a id="_idIndexMarker384"/> initial review of the sample code from the Compel repository, I was intrigued by the following line: <strong class="source-inline">("A photo of cat", "A photo of dog").blend(0.7, 0.3)</strong>. This string prompts several questions, such as how can the <strong class="source-inline">blend()</strong> function be invoked? However, it becomes clear that <strong class="source-inline">blend()</strong> is part of the prompt string and not a function that can be invoked within the <span class="No-Break">Python code.</span></p>
			<p>In contrast, the prompt blending or scheduling feature of Stable Diffusion WebUI [3] is relatively more user-friendly. The syntax allows us to achieve the same blending effect with a prompt syntax <span class="No-Break">like this:</span></p>
			<pre class="source-code">
[A photo of cat:A photo of dog:0.5]</pre>
			<p>This scheduled prompt in Stable Diffusion WebUI will render a photo of a cat during the first 50% of the steps and a photo of a dog during the last 50% of <span class="No-Break">the steps.</span></p>
			<p>Alternatively, you can use the <strong class="source-inline">|</strong> operator to alternate <span class="No-Break">the prompt:</span></p>
			<pre class="source-code">
[A photo of cat|A photo of dog]</pre>
			<p>The preceding scheduled prompt will alternate between rendering photos of a cat and a dog. In other words, it will render a photo of a cat in one step and a photo of a dog in the next step, continuing this pattern until the end of the entire <span class="No-Break">rendering process.</span></p>
			<p>These two scheduling features can also be achieved by Diffusers. In the following section, we will explore how to implement both of these advanced prompt scheduling features <span class="No-Break">for Diffuse<a id="_idTextAnchor244"/>rs.</span></p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor245"/>Building a custom scheduled prompt pipeline</h1>
			<p>As we <a id="_idIndexMarker385"/>discussed in <a href="B21263_05.xhtml#_idTextAnchor097"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, the generation process utilizes input prompt embedding to denoise an image at each step. By default, every denoising step employs the exact same embedding. However, to gain more precise control over the generation, we can modify the pipeline code to supply unique embeddings for each <span class="No-Break">denoising step.</span></p>
			<p>Take, for instance, the <span class="No-Break">following prompt:</span></p>
			<pre class="source-code">
[A photo of cat:A photo of dog:0.5]</pre>
			<p>During a<a id="_idIndexMarker386"/> total of 10 denoising steps, we hope the pipeline can remove noise in the first 5 steps to reveal <strong class="source-inline">A photo of cat</strong>, and the following 5 steps to reveal <strong class="source-inline">A photo of dog</strong>. To make this happen, we will need to implement the <span class="No-Break">following components:</span></p>
			<ul>
				<li>A prompt parser capable of extracting the scheduling number from <span class="No-Break">the prompt</span></li>
				<li>A method to embed the prompts and create a list of prompt embeddings that matches the number <span class="No-Break">of steps</span></li>
				<li>A new <strong class="source-inline">pipeline</strong> class derived from the Diffusers pipeline, enabling us to incorporate our new functionality into the pipeline while preserving all existing features of the <span class="No-Break">Diffusers pipeline</span></li>
			</ul>
			<p>Next, let’s implement the formatted <span class="No-Break">prompt pa<a id="_idTextAnchor246"/>rser.</span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor247"/>A scheduled prompt parser</h2>
			<p>The <a id="_idIndexMarker387"/>open sourced Stable Diffusion WebUI project’s source code reveals that we can use <strong class="source-inline">lark</strong> [4] – a parsing toolkit for Python. We will also use the <strong class="source-inline">lark</strong> package to parse the scheduled prompt for our own <span class="No-Break">prompt parser.</span></p>
			<p>To install <strong class="source-inline">lark</strong>, run the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
pip install -U lark</pre>
			<p>The Stable Diffusion WebUI compatible prompt is defined in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import lark
schedule_parser = lark.Lark(r"""
!start: (prompt | /[][():]/+)*
prompt: (emphasized | scheduled | alternate | plain | WHITESPACE)*
!emphasized: "(" prompt ")"
        | "(" prompt ":" prompt ")"
        | "[" prompt "]"
scheduled: "[" [prompt ":"] prompt ":" [WHITESPACE] NUMBER "]"
alternate: "[" prompt ("|" prompt)+ "]"
WHITESPACE: /\s+/
plain: /([^\\\[\]():|]|\\.)+/
%import common.SIGNED_NUMBER -&gt; NUMBER
""")</pre>
			<p>If you <a id="_idIndexMarker388"/>decide to get up to your neck in the syntax swamp to fully understand every line of the definition, the <strong class="source-inline">lark</strong> document [5] is the place <span class="No-Break">to go.</span></p>
			<p>Next, we’ll use the Python function from the SD WebUI code repository. This function utilizes the Lark <strong class="source-inline">schedule_parser</strong> syntax definition to parse the <span class="No-Break">input prompt:</span></p>
			<pre class="source-code">
def get_learned_conditioning_prompt_schedules(prompts, steps):
    def collect_steps(steps, tree):
        l = [steps]
        class CollectSteps(lark.Visitor):
            def scheduled(self, tree):
                tree.children[-1] = float(tree.children[-1])
                if tree.children[-1] &lt; 1:
                    tree.children[-1] *= steps
                tree.children[-1] = min(steps, int(tree.children[-1]))
                l.append(tree.children[-1])
            def alternate(self, tree):
                l.extend(range(1, steps+1))
        CollectSteps().visit(tree)
        return sorted(set(l))
    def at_step(step, tree):
        class AtStep(lark.Transformer):
            def scheduled(self, args):
                before, after, _, when = args
                yield before or () if step &lt;= when else after
            def alternate(self, args):
                yield next(args[(step - 1)%len(args)])
            def start(self, args):
                def flatten(x):
                    if type(x) == str:
                        yield x
                    else:
                        for gen in x:
                            yield from flatten(gen)
                return ''.join(flatten(args))
            def plain(self, args):
                yield args[0].value
            def __default__(self, data, children, meta):
                for child in children:
                    yield child
        return AtStep().transform(tree)
    def get_schedule(prompt):
        try:
            tree = schedule_parser.parse(prompt)
        except lark.exceptions.LarkError as e:
            if 0:
                import traceback
                traceback.print_exc()
            return [[steps, prompt]]
        return [[t, at_step(t, tree)] for t in collect_steps(steps, 
            tree)]
    promptdict = {prompt: get_schedule(prompt) for prompt in 
        set(prompts)}
    return [promptdict[prompt] for prompt in prompts]</pre>
			<p>Set the total denoising steps to 10, and give a shorter name, <strong class="source-inline">g</strong>, for <span class="No-Break">this function:</span></p>
			<pre class="source-code">
steps = 10
g = lambda p: get_learned_conditioning_prompt_schedules([p], steps)[0]</pre>
			<p>Now, let’s<a id="_idIndexMarker389"/> throw some prompts to the function to see the <span class="No-Break">parsing results:</span></p>
			<ul>
				<li>Test #<span class="No-Break">1: </span><span class="No-Break"><strong class="source-inline">cat</strong></span><span class="No-Break">:</span><pre class="source-code">
g("cat")</pre><p class="list-inset">The preceding code will parse the <strong class="source-inline">cat</strong> input text as the <span class="No-Break">following string:</span></p><pre class="source-code">
[[10, 'cat']]</pre><p class="list-inset">The result indicates that all 10 steps will use <strong class="source-inline">cat</strong> to generate <span class="No-Break">the image.</span></p></li>
				<li>Test #<span class="No-Break">2: </span><span class="No-Break"><strong class="source-inline">[cat:dog:0.5]</strong></span><span class="No-Break">:</span><p class="list-inset">Change the prompt <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">[cat:dog:0.5]</strong></span><span class="No-Break">:</span></p><pre class="source-code">
g('[cat:dog:0.5]')</pre><p class="list-inset">The function will generate the <span class="No-Break">following result:</span></p><pre class="source-code">
[[5, 'cat'], [10, 'dog']]</pre><p class="list-inset">The<a id="_idIndexMarker390"/> result means using <strong class="source-inline">cat</strong> for the first 5 steps, and <strong class="source-inline">dog</strong> for the last <span class="No-Break">5 steps.</span></p></li>
				<li>Test #<span class="No-Break">3: </span><span class="No-Break"><strong class="source-inline">[cat|dog]</strong></span><span class="No-Break">:</span><p class="list-inset">The function also supports alternative scheduling. Change the prompt to <strong class="source-inline">[cat | dog]</strong>, with an “or” <strong class="source-inline">|</strong> operator in the middle of the <span class="No-Break">two names:</span></p><pre class="source-code">
g('[cat|dog]')</pre><p class="list-inset">The prompt parser will generate the <span class="No-Break">following result:</span></p><pre class="source-code">
[[1, 'cat'],</pre><pre class="source-code">
 [2, 'dog'],</pre><pre class="source-code">
 [3, 'cat'],</pre><pre class="source-code">
 [4, 'dog'],</pre><pre class="source-code">
 [5, 'cat'],</pre><pre class="source-code">
 [6, 'dog'],</pre><pre class="source-code">
 [7, 'cat'],</pre><pre class="source-code">
 [8, 'dog'],</pre><pre class="source-code">
 [9, 'cat'],</pre><pre class="source-code">
 [10, 'dog']]</pre></li>
			</ul>
			<p>In other words, it alternates the prompt for each <span class="No-Break">denoising step.</span></p>
			<p>So far, it <a id="_idIndexMarker391"/>works well in terms of parsing. However, before feeding it to the pipeline, additional work needs t<a id="_idTextAnchor248"/>o <span class="No-Break">be done.</span></p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor249"/>Filling in the missing steps</h2>
			<p>In <em class="italic">Test #2</em>, the <a id="_idIndexMarker392"/>generated result includes only two elements. We need to expand the result list to cover <span class="No-Break">each step:</span></p>
			<pre class="source-code">
def parse_scheduled_prompts(text, steps=10):
    text = text.strip()
    parse_result = None
    try:
        parse_result = get_learned_conditioning_prompt_schedules(
            [text],
            steps = steps
        )[0]
    except Exception as e:
        print(e)
    if len(parse_result) == 1:
        return parse_result
    prompts_list = []
    for i in range(steps):
        current_prompt_step, current_prompt_content = \
            parse_result[0][0],parse_result[0][1]
        step = i + 1
        if step &lt; current_prompt_step:
            prompts_list.append(current_prompt_content)
            continue
        if step == current_prompt_step:
            prompts_list.append(current_prompt_content)
            parse_result.pop(0)
    return prompts_list</pre>
			<p>This <a id="_idIndexMarker393"/>Python function, <strong class="source-inline">parse_scheduled_prompts</strong>, takes two arguments: <strong class="source-inline">text</strong> and <strong class="source-inline">steps</strong> (with a default value of 10). The function processes the given text to generate a list of prompts based on a learned <span class="No-Break">conditioning schedule.</span></p>
			<p>Here’s a step-by-step explanation of what the <span class="No-Break">function does:</span></p>
			<ol>
				<li>Use a <strong class="source-inline">try-except</strong> block to call the <strong class="source-inline">get_learned_conditioning_prompt_schedules</strong> function with the processed text and the specified number of steps. The result is stored in <strong class="source-inline">parse_result</strong>. If there’s an exception – say, a syntax error, it will be caught <span class="No-Break">and printed.</span></li>
				<li>If the length of <strong class="source-inline">parse_result</strong> is 1, return <strong class="source-inline">parse_result</strong> as the <span class="No-Break">final output.</span></li>
				<li>Loop through the range of steps and perform the <span class="No-Break">following actions:</span><ol><li class="upper-roman">Get the current prompt step and content <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">parse_result</strong></span><span class="No-Break">.</span></li><li class="upper-roman">Increment the loop counter <strong class="source-inline">i</strong> by <strong class="source-inline">1</strong> and store it in the <span class="No-Break">variable step.</span></li><li class="upper-roman">If <strong class="source-inline">step</strong> is less than the current prompt step, append the current prompt content to <strong class="source-inline">prompts_list</strong> and continue to the <span class="No-Break">next iteration.</span></li><li class="upper-roman">If <strong class="source-inline">step</strong> is equal to the current prompt step, append the current prompt content to <strong class="source-inline">prompts_list</strong> and remove the first element <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">parse_result</strong></span><span class="No-Break">.</span></li></ol></li>
				<li>Return the <strong class="source-inline">prompts_list</strong> as the <span class="No-Break">final output.</span></li>
			</ol>
			<p>The<a id="_idIndexMarker394"/> function essentially generates a list of prompts based on the learned conditioning schedule, with each prompt being added to the list according to the specified number <span class="No-Break">of steps.</span></p>
			<p>Let’s call this function to test <span class="No-Break">it out:</span></p>
			<pre class="source-code">
prompt_list = parse_scheduled_prompts("[cat:dog:0.5]")
prompt_list</pre>
			<p>We will get a prompt list as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
['cat',
 'cat',
 'cat',
 'cat',
 'cat',
 'dog',
 'dog',
 'dog',
 'dog',
 'dog']</pre>
			<p>Five prompts for <strong class="source-inline">cat</strong>, and five prompts for <strong class="source-inline">dog</strong> – each denoising step will take one of<a id="_idTextAnchor250"/> <span class="No-Break">the prompts.</span></p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor251"/>A Stable Diffusion pipeline supporting scheduled prompts</h2>
			<p>Until now, all<a id="_idIndexMarker395"/> prompts are still in plain text form. We will need to use custom embedding code to encode unlimited and weighted prompts into embeddings, or we can simply use the default encoder from Diffusers to generate embeddings but with a <span class="No-Break">77-token limitation.</span></p>
			<p>To make the logic easier and more concise to follow, we will use the default text encoder in this section. Once we figure out how it works, it will be easy to swap the encoder with the more powerful one we built in <a href="B21263_10.xhtml#_idTextAnchor197"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<p>Since we will perform a minor surgical operation on the original Diffusers Stable Diffusion pipeline to support this embedding list, the operation includes creating a new pipeline class inherited from the Diffusers pipeline. We can directly reuse the tokenizer and text encoder from the initialized pipeline by using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
...
prompt_embeds = self._encode_prompt(
    prompt,
    device,
    num_images_per_prompt,
    do_classifier_free_guidance,
    negative_prompt,
    negative_prompt_embeds=negative_prompt_embeds,
)
...</pre>
			<p>I will further explain the preceding code next. We will implement the whole logic in the <strong class="source-inline">scheduler_call</strong> function (similar to the <strong class="source-inline">__call__</strong> function <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">StableDiffusionPipeline</strong></span><span class="No-Break">):</span></p>
			<pre class="source-code">
from typing import List, Callable, Dict, Any
from torch import Generator,FloatTensor
from diffusers.pipelines.stable_diffusion import (
    StableDiffusionPipelineOutput)
from diffusers import (
    StableDiffusionPipeline,EulerDiscreteScheduler)
class StableDiffusionPipeline_EXT(StableDiffusionPipeline):
    @torch.no_grad()
    def scheduler_call(
        self,
        prompt: str | List[str] = None,
        height: int | None = 512,
        width: int | None = 512,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        negative_prompt: str | List[str] | None = None,
        num_images_per_prompt: int | None = 1,
        eta: float = 0,
        generator: Generator | List[Generator] | None = None,
        latents: FloatTensor | None = None,
        prompt_embeds: FloatTensor | None = None,
        negative_prompt_embeds: FloatTensor | None = None,
        output_type: str | None = "pil",
        callback: Callable[[int, int, FloatTensor], None] | None = None,
        callback_steps: int = 1,
        cross_attention_kwargs: Dict[str, Any] | None = None,
    ):
        ...
        # 6. Prepare extra step kwargs. TODO: Logic should ideally 
        # just be moved out of the pipeline
        extra_step_kwargs = self.prepare_extra_step_kwargs(
            generator, eta)
        # 7. Denoising loop
        num_warmup_steps = len(timesteps) - num_inference_steps * \
            self.scheduler.order
        with self.progress_bar(total=num_inference_steps) as \
            progress_bar:
            for i, t in enumerate(timesteps):
                # AZ code to enable Prompt Scheduling, 
                # will only function when
                # when there is a prompt_embeds_l provided.
                prompt_embeds_l_len = len(embedding_list)
                if prompt_embeds_l_len &gt; 0:
                    # ensure no None prompt will be used
                    pe_index = (i)%prompt_embeds_l_len
                    prompt_embeds = embedding_list[pe_index]
                # expand the latents if we are doing classifier 
                #free guidance
                latent_model_input = torch.cat([latents] * 2) \
                    if do_classifier_free_guidance else latents
                latent_model_input = self.scheduler. \ 
                    scale_model_input(latent_model_input, t)
                # predict the noise residual
                noise_pred = self.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=prompt_embeds,
                    cross_attention_kwargs=cross_attention_kwargs,
                ).sample
                # perform guidance
                if do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = \
                        noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + guidance_scale * \
                        (noise_pred_text - noise_pred_uncond)
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents = self.scheduler.step(noise_pred, t, latents, 
                    **extra_step_kwargs).prev_sample
                # call the callback, if provided
                if i == len(timesteps) - 1 or ((i + 1) &gt; \
                num_warmup_steps and (i + 1) % \
                self.scheduler.order == 0):
                    progress_bar.update()
                    if callback is not None and i % callback_steps== 0:
                        callback(i, t, latents)
        if output_type == "latent":
            image = latents
        elif output_type == "pil":
            # 8. Post-processing
            image = self.decode_latents(latents)
            image = self.numpy_to_pil(image)
        else:
            # 8. Post-processing
            image = self.decode_latents(latents)
        if hasattr(self, "final_offload_hook") and \
            self.final_offload_hook is not None:
            self.final_offload_hook.offload()
        return StableDiffusionPipelineOutput(images=image)</pre>
			<p>This <a id="_idIndexMarker396"/>Python function, <strong class="source-inline">scheduler_call</strong>, is a method of the <strong class="source-inline">StableDiffusionPipeline_EXT</strong> class, which is a subclass <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">StableDiffusionPipeline</strong></span><span class="No-Break">.</span></p>
			<p>Here are the steps to implement the <span class="No-Break">whole logic:</span></p>
			<ol>
				<li>Set the <a id="_idIndexMarker397"/>default scheduler to <strong class="source-inline">EulerDiscreteScheduler</strong> for a better <span class="No-Break">generation result:</span><pre class="source-code">
if self.scheduler._class_name == "PNDMScheduler":</pre><pre class="source-code">
    self.scheduler = EulerDiscreteScheduler.from_config(</pre><pre class="source-code">
        self.scheduler.config</pre><pre class="source-code">
    )</pre></li>
				<li>Prepare the <strong class="source-inline">device</strong> and <span class="No-Break"><strong class="source-inline">do_classifier_free_guidance</strong></span><span class="No-Break"> parameters:</span><pre class="source-code">
device = self._execution_device</pre><pre class="source-code">
do_classifier_free_guidance = guidance_scale &gt; 1.0</pre></li>
				<li>Call the <strong class="source-inline">parse_scheduled_prompts</strong> function to have the <strong class="source-inline">prompt_list </strong>prompt list. This is the function we built in the previous section of <span class="No-Break">this chapter:</span><pre class="source-code">
prompt_list = parse_scheduled_prompts(prompt)</pre></li>
				<li>If no scheduled prompt is found, use the normal <span class="No-Break">single-prompt logic:</span><pre class="source-code">
embedding_list = []</pre><pre class="source-code">
if len(prompt_list) == 1:</pre><pre class="source-code">
    prompt_embeds = self._encode_prompt(</pre><pre class="source-code">
        prompt,</pre><pre class="source-code">
        device,</pre><pre class="source-code">
        num_images_per_prompt,</pre><pre class="source-code">
        do_classifier_free_guidance,</pre><pre class="source-code">
        negative_prompt,</pre><pre class="source-code">
        negative_prompt_embeds=negative_prompt_embeds,</pre><pre class="source-code">
    )</pre><pre class="source-code">
else:</pre><pre class="source-code">
    for prompt in prompt_list:</pre><pre class="source-code">
        prompt_embeds = self._encode_prompt(</pre><pre class="source-code">
            prompt,</pre><pre class="source-code">
            device,</pre><pre class="source-code">
            num_images_per_prompt,</pre><pre class="source-code">
            do_classifier_free_guidance,</pre><pre class="source-code">
            negative_prompt,</pre><pre class="source-code">
            negative_prompt_embeds=negative_prompt_embeds,</pre><pre class="source-code">
        )</pre><pre class="source-code">
        embedding_list.append(prompt_embeds)</pre><p class="list-inset">In <em class="italic">step 4</em>, the function processes the input prompt(s) to generate the prompt embeddings. The input prompt can be a single string or a list of strings. The function first parses the input prompt(s) into a list called <strong class="source-inline">prompt_list</strong>. If there is <a id="_idIndexMarker398"/>only one prompt in the list, the function directly encodes the prompt using the <strong class="source-inline">_encode_prompt</strong> method and stores the result in <strong class="source-inline">prompt_embeds</strong>. If there are multiple prompts, the function iterates through <strong class="source-inline">prompt_list</strong> and encodes each prompt separately using the <strong class="source-inline">_encode_prompt</strong> method. The resulting embeddings are stored <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">embedding_list</strong></span><span class="No-Break">.</span></p></li>
				<li>Prepare timesteps for the <span class="No-Break">diffusion process:</span><pre class="source-code">
self.scheduler.set_timesteps(num_inference_steps, device=device)</pre><pre class="source-code">
timesteps = self.scheduler.timesteps</pre></li>
				<li>Prepare latent variables to initialize the <strong class="source-inline">latents</strong> tensor (this is a <span class="No-Break">PyTorch tensor):</span><pre class="source-code">
num_channels_latents = self.unet.in_channels</pre><pre class="source-code">
batch_size = 1</pre><pre class="source-code">
latents = self.prepare_latents(</pre><pre class="source-code">
    batch_size * num_images_per_prompt,</pre><pre class="source-code">
    num_channels_latents,</pre><pre class="source-code">
    height,</pre><pre class="source-code">
    width,</pre><pre class="source-code">
    prompt_embeds.dtype,</pre><pre class="source-code">
    device,</pre><pre class="source-code">
    generator,</pre><pre class="source-code">
    latents,</pre><pre class="source-code">
)</pre></li>
				<li>Implement <a id="_idIndexMarker399"/>the <span class="No-Break">denoising loop:</span><pre class="source-code">
num_warmup_steps = len(timesteps) - num_inference_steps * \ </pre><pre class="source-code">
    self.scheduler.order</pre><pre class="source-code">
with self.progress_bar(total=num_inference_steps) as \ </pre><pre class="source-code">
    progress_bar:</pre><pre class="source-code">
    for i, t in enumerate(timesteps):</pre><pre class="source-code">
        # custom code to enable Prompt Scheduling, </pre><pre class="source-code">
        # will only function when</pre><pre class="source-code">
        # when there is a prompt_embeds_l provided.</pre><pre class="source-code">
        prompt_embeds_l_len = len(embedding_list)</pre><pre class="source-code">
        if prompt_embeds_l_len &gt; 0:</pre><pre class="source-code">
            # ensure no None prompt will be used</pre><pre class="source-code">
            pe_index = (i)%prompt_embeds_l_len</pre><pre class="source-code">
            prompt_embeds = embedding_list[pe_index]</pre><pre class="source-code">
        # expand the latents if we are doing </pre><pre class="source-code">
        # classifier free guidance</pre><pre class="source-code">
        latent_model_input = torch.cat([latents] * 2) </pre><pre class="source-code">
            if do_classifier_free_guidance else latents</pre><pre class="source-code">
        latent_model_input = </pre><pre class="source-code">
            self.scheduler.scale_model_input(</pre><pre class="source-code">
                latent_model_input, t)</pre><pre class="source-code">
        # predict the noise residual</pre><pre class="source-code">
        noise_pred = self.unet(</pre><pre class="source-code">
            latent_model_input,</pre><pre class="source-code">
            t,</pre><pre class="source-code">
            encoder_hidden_states=prompt_embeds,</pre><pre class="source-code">
            cross_attention_kwargs=cross_attention_kwargs,</pre><pre class="source-code">
        ).sample</pre><pre class="source-code">
        # perform guidance</pre><pre class="source-code">
        if do_classifier_free_guidance:</pre><pre class="source-code">
            noise_pred_uncond, noise_pred_text = \</pre><pre class="source-code">
                noise_pred.chunk(2)</pre><pre class="source-code">
            noise_pred = noise_pred_uncond + guidance_scale * \</pre><pre class="source-code">
                (noise_pred_text - noise_pred_uncond)</pre><pre class="source-code">
        # compute the previous noisy sample x_t -&gt; x_t-1</pre><pre class="source-code">
        latents = self.scheduler.step(noise_pred, t, </pre><pre class="source-code">
            latents).prev_sample</pre><pre class="source-code">
        # call the callback, if provided</pre><pre class="source-code">
        if i == len(timesteps) - 1 or ((i + 1) &gt; </pre><pre class="source-code">
            num_warmup_steps and (i + 1) % </pre><pre class="source-code">
            self.scheduler.order == 0):</pre><pre class="source-code">
            progress_bar.update()</pre><pre class="source-code">
            if callback is not None and i % callback_steps == 0:</pre><pre class="source-code">
                callback(i, t, latents)</pre><p class="list-inset">In <em class="italic">step 7</em>, the <a id="_idIndexMarker400"/>denoising loop iterates through the timesteps of the diffusion process. If prompt scheduling is enabled (i.e., there are multiple prompt embeddings in <strong class="source-inline">embedding_list</strong>), the function selects the appropriate prompt embedding for the current timestep. The length of <strong class="source-inline">embedding_list</strong> is stored in <strong class="source-inline">prompt_embeds_l_len</strong>. If <strong class="source-inline">prompt_embeds_l_len</strong> is greater than <strong class="source-inline">0</strong>, it means prompt scheduling is enabled. The function calculates the <strong class="source-inline">pe_index</strong> index for the current timestep, <strong class="source-inline">i</strong>, using the modulo operator (<strong class="source-inline">%</strong>). This ensures that the index wraps around the length of <strong class="source-inline">embedding_list</strong> and selects the appropriate prompt embedding for the current timestep. The selected prompt embedding is then assigned <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">prompt_embeds</strong></span><span class="No-Break">.</span></p></li>
				<li>The last step is <span class="No-Break">denoising post-processing:</span><pre class="source-code">
image = self.decode_latents(latents)</pre><pre class="source-code">
image = self.numpy_to_pil(image)</pre><pre class="source-code">
return StableDiffusionPipelineOutput(images=image, </pre><pre class="source-code">
    nsfw_content_detected=None)</pre><p class="list-inset">In the last step, we convert the image data from latent space to pixel space by calling the <strong class="source-inline">decode_latents()</strong> function. The <strong class="source-inline">StableDiffusionPipelineOutput</strong> class is used here for a consistent structure to be maintained when returning outputs from the pipeline. We use it here to <a id="_idIndexMarker401"/>make our pipeline compatible with the Diffusers pipeline. You can also find the complete code in the code files associated with <span class="No-Break">this chapter.</span></p><p class="list-inset">Congratulations to you if you are still here! Let’s execute it to witness <span class="No-Break">the result:</span></p><pre class="source-code">
pipeline = StableDiffusionPipeline_EXT.from_pretrained(</pre><pre class="source-code">
    "stablediffusionapi/deliberate-v2",</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    safety_checker = None</pre><pre class="source-code">
).to("cuda:0")</pre><pre class="source-code">
prompt = "high quality, 4k, details, A realistic photo of cute \ [cat:dog:0.6]"</pre><pre class="source-code">
neg_prompt = "paint, oil paint, animation, blur, low quality, \ bad glasses"</pre><pre class="source-code">
image = pipeline.scheduler_call(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
image</pre></li>
			</ol>
			<p>We should see an image like the one shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B21263_12_04.jpg" alt="Figure 12.4: A blended photo with 60% cat and 40% dog, using a custom scheduled prompt pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4: A blended photo with 60% cat and 40% dog, using a custom scheduled prompt pipeline</p>
			<p>Here’s another <a id="_idIndexMarker402"/>example, using an alternative <span class="No-Break">prompt </span><span class="No-Break"><strong class="source-inline">[cat|dog]</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
prompt = "high quality, 4k, details, A realistic photo of white \
[cat|dog]"
neg_prompt = "paint, oil paint, animation, blur, low quality, bad \
glasses"
image = pipeline.scheduler_call(
    prompt = prompt,
    negative_prompt = neg_prompt,
    generator = torch.Generator("cuda").manual_seed(3)
).images[0]
image</pre>
			<p>Our alternative prompt gives an image similar to <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B21263_12_05.jpg" alt="Figure 12.5: A photo of a blended cat and dog, using the alternative prompt scheduling from our custom-scheduled prompt pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5: A photo of a blended cat and dog, using the alternative prompt scheduling from our custom-scheduled prompt pipeline</p>
			<p>If you see<a id="_idIndexMarker403"/> half-cat, half-dog images generated, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.4</em> and <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.5</em>, you have successfully built your cust<a id="_idTextAnchor252"/>om <span class="No-Break">prompt scheduler.</span></p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor253"/>Summary</h1>
			<p>In this chapter, we introduced two solutions for conducting scheduled prompt image generation. The first solution, the <strong class="source-inline">Compel</strong> package, is the easiest one to use. Simply install the package, and you can use its prompt blend feature to blend two or more concepts in <span class="No-Break">one prompt.</span></p>
			<p>The second solution is a customized pipeline that first parses the prompt string and prepares a prompt list for each denoising step. The custom pipeline loops through the prompt list to create an embedding list. Finally, a <strong class="source-inline">scheduler_call</strong> function uses the prompt embedding from the embedding list to generate images with <span class="No-Break">precise control.</span></p>
			<p>If you successfully implement the custom scheduled pipeline, you can control generation in a more precise way. Speaking of controlling, in <a href="B21263_13.xhtml#_idTextAnchor257"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, we are going to explore another way to control image gene<a id="_idTextAnchor254"/>ration – <span class="No-Break">ControlNet.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor255"/>References</h1>
			<ol>
				<li><span class="No-Break">Compel: </span><a href="https://github.com/damian0815/compel"><span class="No-Break">https://github.com/damian0815/compel</span></a></li>
				<li>Compel Syntax <span class="No-Break">Features: </span><a href="https://github.com/damian0815/compel/blob/main/doc/syntax.md"><span class="No-Break">https://github.com/damian0815/compel/blob/main/doc/syntax.md</span></a></li>
				<li>Stable Diffusion WebUI Prompt <span class="No-Break">Editing: </span><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#prompt-editing&#13;"><span class="No-Break">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#prompt-editing</span></a></li>
				<li>Lark – a parsing toolkit for <span class="No-Break">Python: </span><a href="https://github.com/lark-parser/lark"><span class="No-Break">https://github.com/lark-parser/lark</span></a></li>
				<li>Lark usage <span class="No-Break">document: </span><a href="https://lark-parser.readthedocs.io/en/stable/"><span class="No-Break">https://lark-parser.readthedocs.io/en/stable/</span></a></li>
			</ol>
		</div>
	

		<div id="_idContainer080" class="Content">
			<h1 id="_idParaDest-156" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor256"/>Part 3 – Advanced Topics</h1>
			<p>In Parts 1 and 2, we established a solid foundation for Stable Diffusion, covering its fundamentals, customization options, and optimization techniques. Now, it’s time to venture into more advanced territories, where we’ll explore cutting-edge applications, innovative models, and expert-level strategies to generate remarkable <span class="No-Break">visual content.</span></p>
			<p>The chapters in this part will take you on a thrilling journey through the latest developments in Stable Diffusion. You’ll learn how to generate images with unprecedented control using ControlNet, craft captivating videos with AnimateDiff, and extract insightful descriptions from images using powerful vision-language models such as BLIP-2 and LLaVA. Additionally, you’ll get acquainted with Stable Diffusion XL, a newer and more capable iteration of the Stable <span class="No-Break">Diffusion model.</span></p>
			<p>To top it off, we’ll delve into the art of crafting optimized prompts for Stable Diffusion, including techniques to write effective prompts and leverage large language models to automate the process. By the end of this part, you’ll possess the expertise to tackle complex projects, push the boundaries of Stable Diffusion, and unlock new creative possibilities. Get ready to unleash your full potential and produce <span class="No-Break">breathtaking results!</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21263_13.xhtml#_idTextAnchor257"><em class="italic">Chapter 13</em></a><em class="italic">, Generating Images with ControlNet</em></li>
				<li><a href="B21263_14.xhtml#_idTextAnchor277"><em class="italic">Chapter 14</em></a><em class="italic">, Generating Video Using Stable Diffusion</em></li>
				<li><a href="B21263_15.xhtml#_idTextAnchor289"><em class="italic">Chapter 15</em></a><em class="italic">, Generating Image Descriptions Using BLIP-2 and LLaVA</em></li>
				<li><a href="B21263_16.xhtml#_idTextAnchor309"><em class="italic">Chapter 16</em></a><em class="italic">, Exploring Stable Diffusion XL</em></li>
				<li><a href="B21263_17.xhtml#_idTextAnchor335"><em class="italic">Chapter 17</em></a><em class="italic">, Building Optimized Prompts for Stable Diffusion</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer081">
			</div>
		</div>
		<div>
			<div id="_idContainer082" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer083" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer084">
			</div>
		</div>
		<div>
			<div id="_idContainer085" class="Basic-Graphics-Frame">
			</div>
		</div>
	</body></html>