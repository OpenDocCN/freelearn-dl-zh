- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unleashing Machine Learning Potentials in Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will delve into the fundamentals of **Machine Learning**
    (**ML**) and preprocessing techniques that are essential for **natural language
    processing** (**NLP**) tasks. ML is a powerful tool for building models that can
    learn from data, and NLP is one of the most exciting and challenging applications
    of ML.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained a comprehensive understanding
    of data exploration, preprocessing, and data split, know how to deal with imbalanced
    data techniques, and learned about some of the common ML models required for successful
    ML, particularly in the context of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model underfitting and overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling imbalanced data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with correlated data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior knowledge of programming languages, particularly Python, is assumed in
    this chapter and subsequent chapters of this book. It is also expected that you
    have already gone through previous chapters to become acquainted with the necessary
    linear algebra and statistics concepts that will be discussed in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working in a methodological environment, datasets are often well known
    and preprocessed, such as Kaggle datasets. However, in real-world business environments,
    one important task is to define the dataset from all possible sources of data,
    explore the gathered data to find the best method for preprocessing it, and ultimately
    decide on the ML and natural language models that fit the problem and the underlying
    data best. This process requires careful consideration and analysis of the data,
    as well as a thorough understanding of the business problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: In NLP, the data can be quite complex, as it often includes text and speech
    data that can be unstructured and difficult to analyze. This complexity makes
    preprocessing an essential step in preparing the data for ML models. The first
    step of any NLP or ML solution starts with exploring the data to learn more about
    it, which helps us decide on our path to tackle the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been preprocessed, the next step is to explore it to gain
    a better understanding of its characteristics and structure. Data exploration
    is an iterative process that involves visualizing and analyzing the data, looking
    for patterns and relationships, and identifying potential issues or outliers.
    This process can help us to determine which features are most important for our
    ML models and identify any potential biases or data quality issues. To streamline
    data and enhance analysis through ML models, preprocessing methods such as tokenization,
    stemming, and lemmatization can be employed. In this chapter, we will provide
    an overview of general preprocessing techniques for ML problems. In the following
    chapter, we will delve into preprocessing techniques specific to text processing.
    It is important to note that employing effective preprocessing techniques can
    significantly enhance the performance and accuracy of ML models, making them more
    robust and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once the data has been preprocessed and explored, we can start building
    our ML models. There is no single magical solution that works for all ML problems,
    so it’s important to carefully consider which models are best suited for the data
    and the problem at hand. Different types of NLP models exist, encompassing rule-based,
    statistical, and deep learning models. Each model type possesses unique strengths
    and weaknesses, underscoring the importance of selecting the most fitting one
    for the specific problem and dataset at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration is an important and initial step in the ML workflow that involves
    analyzing and understanding the data before building a ML model. The goal of data
    exploration is to gain insights about the data, identify patterns, detect anomalies,
    and prepare the data for modeling. Data exploration helps in choosing the right
    ML algorithm and determining the best set of features to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some common techniques that are used in data exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data visualization**: Data visualization involves depicting data through
    graphical or pictorial formats. It enables visual exploration of data, providing
    insights into its distribution, patterns, and relationships. Widely employed techniques
    in data visualization encompass scatter plots, bar charts, heatmaps, box plots,
    and correlation matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cleaning**: Data cleaning is a step of preprocessing where we identify
    the errors, inconsistencies, and missing values and correct them. It affects the
    final results of the model since ML models are sensitive to errors in the data.
    Removing duplicates and filling in missing values are some of the common data
    cleaning techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Feature engineering plays a crucial role in optimizing
    the effectiveness of machine learning models by crafting new features from existing
    data. This process involves not only identifying pertinent features but also transforming
    the existing ones and introducing novel features. Various feature engineering
    techniques, including scaling, normalization, dimensionality reduction, and feature
    selection, contribute to refining the overall performance of the models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical analysis**: Statistical analysis utilizes a range of statistical
    techniques to scrutinize data, revealing valuable insights into its inherent properties.
    Essential statistical methods include hypothesis testing, regression analysis,
    and time series analysis, all of which contribute to a comprehensive understanding
    of the data’s characteristics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain knowledge**: Leveraging domain knowledge entails applying a pre-existing
    understanding of the data domain to extract insights and make informed decisions.
    This knowledge proves valuable in recognizing pertinent features, interpreting
    results, and choosing the most suitable ML algorithm for the task at hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will explore each of these techniques in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data visualization is a crucial component of machine learning as it allows us
    to understand and explore complex datasets more easily. It involves creating visual
    representations of data using charts, graphs, and other types of visual aids.
    By visually presenting data, we can discern patterns, trends, and relationships
    that might not be readily evident when examining the raw data alone.
  prefs: []
  type: TYPE_NORMAL
- en: For NLP tasks, data visualization can help us gain insights into the linguistic
    patterns and structures in text data. For example, we can create word clouds to
    visualize the frequency of words in a corpus or use heatmaps to display the co-occurrence
    of words or phrases. We can also use scatter plots and line graphs to visualize
    changes in sentiment or topic over time.
  prefs: []
  type: TYPE_NORMAL
- en: One common type of visualization for ML is the scatter plot, which is used to
    display the relationship between two variables. By plotting the values of two
    variables on the X and Y axes, we can identify any patterns or trends that exist
    between them. Scatter plots are particularly useful for identifying clusters or
    groups of data points that share similar characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of visualization that’s frequently employed in ML is the histogram,
    a tool that illustrates the distribution of a single variable. By grouping data
    into bins and portraying the frequency of data points in each bin, we can pinpoint
    the range of values that predominate in the dataset. Histograms prove useful for
    detecting outliers or anomalies, and they aid in recognizing areas where the data
    may exhibit skewness or bias.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these basic visualizations, ML practitioners often use more advanced
    techniques, such as dimensionality reduction and network visualizations. Dimensionality
    reduction techniques, such as **principal component analysis** (**PCA**) and **t-distributed
    stochastic neighbor embedding** (**t-SNE**), are commonly used for dimensional
    reduction and to visualize or analyze the data more easily. Network visualizations,
    on the other hand, are used to display complex relationships between entities,
    such as the co-occurrence of words or the connections between social media users.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data cleaning, alternatively termed data cleansing or data scrubbing, involves
    recognizing and rectifying or eliminating errors, inconsistencies, and inaccuracies
    within a dataset. This crucial phase in data preparation for ML significantly
    influences the accuracy and performance of a model, relying on the quality of
    the data used for training. Numerous prevalent techniques are employed in data
    cleaning. Let’s take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Missing data is a common problem that occurs in many machine learning projects.
    Dealing with missing data is important because ML models cannot handle missing
    data and will either produce errors or provide inaccurate results.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several methods for dealing with missing data in ML projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropping rows**: Addressing missing data can involve a straightforward approach
    of discarding rows that contain such values. Nevertheless, exercising caution
    is paramount when employing this method as excessive row removal may result in
    the loss of valuable data, impacting the overall accuracy of the model. We usually
    use this method when we have a few rows in our dataset, and we have a few rows
    with missing values. In this case, removing a few rows can be a good and easy
    approach to training our model while the final performance will not be affected
    significantly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropping columns**: Another approach is to drop the columns that contain
    missing values. This method can be effective if the missing values are concentrated
    in a few columns and if those columns are not important for the analysis. However,
    dropping important columns can lead to a loss of valuable information. It is better
    to perform some sort of correlation analysis to see the correlation of the values
    in these columns with the target class or value before dropping these columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean/median/mode imputation**: Mean, median, and mode imputation entail substituting
    missing values with the mean, median, or mode derived from the non-missing values
    within the corresponding column. This method is easy to implement and can be effective
    when the missing values are few and randomly distributed. However, it can also
    introduce bias and affect the variability of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression imputation**: Regression imputation involves predicting the missing
    values based on the values of other variables in the dataset. This method can
    be effective when the missing values are related to other variables in the dataset,
    but it requires a regression model to be built for each column with missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple imputation**: Multiple imputation encompasses generating multiple
    imputed datasets through statistical models, followed by amalgamating the outcomes
    to produce a conclusive dataset. This approach proves efficacious, particularly
    when dealing with non-randomly distributed missing values and a substantial number
    of gaps in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-nearest neighbor imputation**: K-nearest neighbor imputation entails identifying
    the k-nearest data points to the missing value and utilizing their values to impute
    the absent value. This method can be effective when the missing values are clustered
    together in the dataset. In this approach, we can find the most similar records
    to the dataset to the record that has the missing value, and then use the mean
    of the values of those records for that specific record as the missed value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, selecting a method to handle missing data hinges on factors such
    as the nature and extent of the missing data, analysis objectives, and resource
    availability. It is crucial to thoughtfully assess the pros and cons of each method
    and opt for the most suitable approach tailored to the specific project.
  prefs: []
  type: TYPE_NORMAL
- en: Removing duplicates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Eliminating duplicates is a prevalent preprocessing measure that’s employed
    to cleanse datasets by detecting and removing identical records. The occurrence
    of duplicate records may be attributed to factors such as data entry errors, system
    glitches, or data merging processes. The presence of duplicates can skew models
    and yield inaccurate insights. Hence, it is imperative to recognize and eliminate
    duplicate records to uphold the accuracy and dependability of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There are different methods for removing duplicates in a dataset. The most common
    method is to compare all the rows of the dataset to identify duplicate records.
    If two or more rows have the same values in all the columns, they are considered
    duplicates. In some cases, it may be necessary to compare only a subset of columns
    if certain columns are more prone to duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: Another method is to use a unique identifier column to identify duplicates.
    A unique identifier column is a column that contains unique values for each record,
    such as an ID number or a combination of unique columns. By comparing the unique
    identifier column, it is possible to identify and remove duplicate records from
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: After identifying the duplicate records, the next step is to decide which records
    to keep and which ones to remove. One approach is to keep the first occurrence
    of a duplicate record and remove all subsequent occurrences. Another approach
    is to keep the record with the most complete information, or the record with the
    most recent timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial to recognize that the removal of duplicates might lead to a reduction
    in dataset size, potentially affecting the performance of ML models. Consequently,
    assessing the impact of duplicate removal on both the dataset and the ML model
    is essential. In some cases, it may be necessary to keep duplicate records if
    they contain important information that cannot be obtained from other records.
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing and transforming data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Standardizing and transforming data is a critical step in preparing data for
    ML tasks. This process involves scaling and normalizing the numerical features
    of the dataset to make them easier to interpret and compare. The main objective
    of standardizing and transforming data is to enhance the accuracy and performance
    of a ML model by mitigating the influence of features with diverse scales and
    ranges. A widely used method for standardizing data is referred to as “standardization”
    or “Z-score normalization.” This technique involves transforming each feature
    such that it has a mean of zero and a standard deviation of one. The formula for
    standardization is shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/128.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* represents the feature, *mean(x)* denotes the mean of the feature,
    *std(x)* indicates the standard deviation of the feature, and *x’* represents
    the new value assigned to the feature. By standardizing the data in this way,
    the range of each feature is adjusted to be centered around zero, which makes
    it easier to compare features and prevents features with large values from dominating
    the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another technique for transforming data is “min-max scaling.” This method rescales
    the data to a consistent range of values, commonly ranging between 0 and 1\. The
    formula for min-max scaling is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/129.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, *x* represents the feature, *min(x)* signifies the minimum
    value of the feature, and *max(x)* denotes the maximum value of the feature. Min-max
    scaling proves beneficial when the precise distribution of the data is not crucial,
    but there is a need to standardize the data for meaningful comparisons across
    different features.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data can also involve changing the distribution of the data. A
    frequently applied transformation is the log transformation, which is employed
    to alleviate the influence of outliers and skewness within the data. This transformation
    involves taking the logarithm of the feature values, which can help to normalize
    the distribution and reduce the influence of extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, standardizing and transforming data constitute a pivotal stage in the
    data preprocessing workflow for ML endeavors. Through scaling and normalizing
    features, we can enhance the accuracy and performance of the ML model, rendering
    the data more interpretable and conducive to meaningful comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Handling outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Outliers are data points that markedly deviate from the rest of the observations
    in a dataset. Their occurrence may stem from factors such as measurement errors,
    data corruption, or authentic extreme values. The presence of outliers can wield
    a substantial influence on the outcomes of ML models, introducing distortion to
    the data and disrupting the relationships between variables. Therefore, handling
    outliers is an important step in preprocessing data for ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several methods for handling outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Removing outliers**: One straightforward approach involves eliminating observations
    identified as outliers from the dataset. Nevertheless, exercising caution is paramount
    when adopting this method as excessive removal of observations may result in the
    loss of valuable information and potentially introduce bias to the analysis results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transforming data**: Applying mathematical functions such as logarithms or
    square roots to transform the data can mitigate the influence of outliers. For
    instance, taking the logarithm of a variable can alleviate the impact of extreme
    values, given the slower rate of increase in the logarithmic scale compared to
    the original values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Winsorizing**: Winsorizing is a technique that entails substituting extreme
    values with the nearest highest or lowest value in the dataset. Employing this
    method aids in maintaining the sample size and overall distribution of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imputing values**: Imputation involves replacing missing or extreme values
    with estimated values derived from the remaining observations in the dataset.
    For instance, substituting extreme values with the median or mean of the remaining
    observations is a common imputation technique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using robust statistical methods**: Robust statistical methods exhibit lower
    sensitivity to outliers, leading to more accurate results even in the presence
    of such extreme values. For instance, opting for the median instead of the mean
    can effectively diminish the influence of outliers on the final results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s crucial to emphasize that selecting an outlier-handling method should be
    tailored to the unique characteristics of the data and the specific problem at
    hand. Generally, employing a combination of methods is advisable to address outliers
    comprehensively, and assessing the impact of each method on the results is essential.
    Moreover, documenting the steps taken to manage outliers is important for reproducibility
    and to provide clarity on the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Correcting errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rectifying errors during preprocessing stands as a vital stage in readying data
    for ML. Errors may manifest due to diverse reasons such as data entry blunders,
    measurement discrepancies, sensor inaccuracies, or transmission glitches. Correcting
    errors in data holds paramount significance in guaranteeing that ML models are
    trained on dependable and precise data, consequently enhancing the accuracy and
    reliability of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several techniques exist to rectify errors in data. Here are some widely utilized
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual inspection**: An approach to rectify errors in data involves a manual
    inspection of the dataset, wherein errors are corrected by hand. This method is
    frequently employed, particularly when dealing with relatively small and manageable
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical methods**: Statistical methods prove effective in identifying
    and rectifying errors in data. For instance, when the data adheres to a recognized
    distribution, statistical techniques such as the Z-score can be employed to detect
    outliers, which can then be either removed or replaced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML methods**: Utilizing ML algorithms facilitates the detection and correction
    of errors in data. For instance, clustering algorithms prove valuable in pinpointing
    data points that markedly deviate from the broader dataset. Subsequently, these
    identified data points can undergo further examination and correction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain knowledge**: Leveraging domain knowledge is instrumental in pinpointing
    errors within data. For instance, when collecting data from sensors, it becomes
    feasible to identify and rectify errors by considering the anticipated range of
    values that the sensor is capable of producing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imputation**: Imputation serves as a method to populate missing values in
    the data. This can be accomplished through various means, including statistical
    methods such as mean or median imputation, as well as ML algorithms such as k-nearest
    neighbor imputation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a technique hinges on factors such as the nature of the data, the dataset’s
    size, and the resources at your disposal.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature selection involves choosing the most pertinent features from a dataset
    for constructing a ML model. The objective is to decrease the number of features
    without substantially compromising the model’s accuracy, resulting in enhanced
    performance, quicker training, and a more straightforward interpretation of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Several approaches to feature selection exist. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Filter methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These techniques employ statistical methods to rank features according to their
    correlation with the target variable. Common methods encompass chi-squared, mutual
    information, and correlation coefficients. Features are subsequently chosen based
    on a predefined threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Chi-squared
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The chi-squared test is a widely employed statistical method in ML for feature
    selection that’s particularly effective for categorical variables. This test gauges
    the dependence between two random variables, providing a P-value that signifies
    the likelihood of obtaining a result as extreme as or more extreme than the actual
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: In hypothesis testing, the chi-squared test assesses whether the collected data
    aligns with the expected data. A small chi-squared test statistic indicates a
    robust match, while a large statistic implies a weak match. A P-value less than
    or equal to 0.05 leads to the rejection of the null hypothesis, considering it
    highly improbable. Conversely, a P-value greater than 0.05 results in accepting
    or “failing to reject” the null hypothesis. When the P-value hovers around 0.05,
    further scrutiny of the hypothesis is warranted.
  prefs: []
  type: TYPE_NORMAL
- en: 'In feature selection, the chi-squared test evaluates the relationship between
    each feature and the target variable in the dataset. It determines significance
    based on whether a statistically significant difference exists between the observed
    and expected frequencies of the feature, assuming independence between the feature
    and target. Features with a high chi-squared score exhibit a stronger dependence
    on the target variable, making them more informative for classification or regression
    tasks. The formula for calculating the chi-squared is presented in the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>X</mi><mn>2</mn></msup><mo>=</mo><mo>∑</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>O</mi><mi>i</mi></msub><mo>−</mo><msub><mi>E</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup><msub><mi>E</mi><mi>i</mi></msub></mfrac></mrow></mrow></math>](img/130.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/131.png)
    represents the observed value and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/132.png)
    represents the expected value. The computation involves finding the difference
    between the observed frequency and the expected frequency, squaring the result,
    and then dividing by the expected frequency. The summation of these values across
    all categories of the feature yields the overall chi-squared statistic for that
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: The degrees of freedom for the test relies on the number of categories in the
    feature and the number of categories in the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: An exemplary application of chi-squared feature selection lies in text classification,
    particularly in scenarios where the presence or absence of specific words in a
    document serves as features. The chi-squared test helps identify words strongly
    associated with a particular class or category of documents, subsequently enabling
    their use as features in a ML model. In categorical data, especially where the
    relationship between features and the target variable is non-linear, chi-squared
    proves to be a valuable method for feature selection. However, its suitability
    diminishes for continuous or highly correlated features, where alternative feature
    selection methods may be more fitting.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mutual information acts as a metric to gauge the interdependence of two random
    variables. In the context of feature selection, it quantifies the information
    a feature provides about the target variable. The core methodology entails calculating
    the mutual information between each feature and the target variable, ultimately
    selecting features with the highest mutual information scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the mutual information between two discrete random variables,
    *X* and *Y*, can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>;</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/133.png)'
  prefs: []
  type: TYPE_IMG
- en: In the given equation, *p(x, y)* represents the joint probability mass function
    of *X* and *Y*, while *p(x)* and *p(y)* denote the marginal probability mass functions
    of *X* and *Y*, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of feature selection, mutual information calculation involves
    treating the feature as *X* and the target variable as *Y*. By computing the mutual
    information score for each feature, we can then select features with the highest
    scores.
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the probability mass functions needed for calculating mutual information,
    histogram-based methods can be employed. This involves dividing the range of each
    variable into a fixed number of bins and estimating the probability mass functions
    based on the frequencies of observations in each bin. Alternatively, kernel density
    estimation can be utilized to estimate the probability density functions, and
    mutual information can then be computed based on the estimated densities.
  prefs: []
  type: TYPE_NORMAL
- en: In practical applications, mutual information is often employed alongside other
    feature selection methods, such as chi-squared or correlation-based methods, to
    enhance the overall performance of the feature selection process.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation coefficients
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Correlation coefficients serve as indicators of the strength and direction of
    the linear relationship between two variables. In the realm of feature selection,
    these coefficients prove useful in identifying features highly correlated with
    the target variable, thus serving as potentially valuable predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prevalent correlation coefficient employed for feature selection is the
    Pearson correlation coefficient, also referred to as Pearson’s *r*. Pearson’s
    r measures the linear relationship between two continuous variables, ranging from
    -1 (indicating a perfect negative correlation) to 1 (indicating a perfect positive
    correlation), with 0 denoting no correlation. Its calculation involves dividing
    the covariance between the two variables by the product of their standard deviations,
    as depicted in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>r</mi><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>v</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow><mrow><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>∙</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>Y</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/134.png)'
  prefs: []
  type: TYPE_IMG
- en: In the given equation, *X* and *Y* represent the two variables of interest,
    *cov()* denotes the covariance function, and *std()* represents the standard deviation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Utilizing Pearson’s *r* for feature selection involves computing the correlation
    between each feature and the target variable. Features with the highest absolute
    correlation coefficients are then selected. A high absolute correlation coefficient
    signifies a strong correlation with the target variable, whether positive or negative.
    The interpretation of Pearson correlation values and their degree of correlation
    is outlined in *Table 3.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pearson** **Correlation Value** | **Degree** **of Correlation** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ± 1 | Perfect |'
  prefs: []
  type: TYPE_TB
- en: '| ± 0.50 - ± 1 | High degree |'
  prefs: []
  type: TYPE_TB
- en: '| ± 0.30 - ± 0.49 | Moderate degree |'
  prefs: []
  type: TYPE_TB
- en: '| < +0.29 | Low degree |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | No correlation |'
  prefs: []
  type: TYPE_TB
- en: Table 3 .1 – Pearson correlation values and their degree of correlation
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that Pearson’s *r* is only appropriate for identifying linear
    relationships between variables. If the relationship is nonlinear, or if one or
    both of the variables are categorical, other correlation coefficients such as
    Spearman’s ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ρ</mml:mi></mml:math>](img/135.png)or
    Kendall’s ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>τ</mml:mi></mml:math>](img/136.png)
    may be more appropriate. Additionally, it is important to be cautious when interpreting
    correlation coefficients as a high correlation does not necessarily imply causation.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These techniques delve into subsets of features through iterative model training
    and testing. Widely known methods encompass forward selection, backward elimination,
    and recursive feature elimination. While computationally demanding, these methods
    have the potential to significantly enhance model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: A concrete illustration of a wrapper method is **recursive feature elimination**
    (**RFE**). Functioning as a backward elimination approach, RFE systematically
    removes the least important feature until a predetermined number of features remains.
    During each iteration, a machine learning model is trained on the existing features,
    and the least important feature is pruned based on its feature importance score.
    This sequential process persists until the specified number of features is attained.
    The feature importance score can be extracted from diverse methods, including
    coefficient values from linear models or feature importance scores derived from
    decision trees. RFE is a computationally expensive method, but it can be useful
    when the number of features is very large and there is a need to reduce the feature
    space. An alternative approach is to have feature selection during the training
    process, something that’s done via embedding methods.
  prefs: []
  type: TYPE_NORMAL
- en: Embedded methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These methods select features during the training process of the model. Popular
    methods include LASSO and ridge regression, decision trees, and random forests.
  prefs: []
  type: TYPE_NORMAL
- en: LASSO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**LASSO**, an acronym for **Least Absolute Shrinkage and Selection Operator**,
    serves as a linear regression technique that’s commonly employed for feature selection
    in machine learning. Its mechanism involves introducing a penalty term to the
    standard regression loss function. This penalty encourages the model to reduce
    the coefficients of less important features to zero, effectively eliminating them
    from the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The LASSO method proves especially valuable when grappling with high-dimensional
    data, where the number of features far exceeds the number of samples. In such
    scenarios, discerning the most crucial features for predicting the target variable
    can be challenging. LASSO comes to the fore by automatically identifying the most
    relevant features while simultaneously shrinking the coefficients of others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LASSO method works by finding the solution for the following optimization
    problem, which is a minimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi>min</mi><mi
    mathvariant="bold">w</mi></munder><msubsup><mfenced open="‖" close="‖"><mrow><mi
    mathvariant="bold">y</mi><mo>−</mo><mi mathvariant="bold">X</mi><mi mathvariant="bold">w</mi></mrow></mfenced><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi
    mathvariant="bold-italic">λ</mi><msub><mfenced open="‖" close="‖"><mi mathvariant="bold">w</mi></mfenced><mn>1</mn></msub></mrow></mrow></math>](img/137.png)'
  prefs: []
  type: TYPE_IMG
- en: In the given equation, vector *y* represents the target variable, *X* denotes
    the feature matrix, *w* signifies the vector of regression coefficients, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/138.png)
    is a hyperparameter dictating the intensity of the penalty term, and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi
    mathvariant="bold">w</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/139.png)
    stands for the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/140.png)norm
    of the coefficients (that is, the sum of their absolute values).
  prefs: []
  type: TYPE_NORMAL
- en: The inclusion of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/141.png)
    penalty term in the objective function prompts the model to precisely zero out
    certain coefficients, essentially eliminating the associated features from the
    model. The degree of penalty strength is governed by the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/142.png)
    hyperparameter, which can be fine-tuned through the use of cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: LASSO has several advantages over other feature selection methods, such as its
    ability to handle correlated features and its ability to perform feature selection
    and regression simultaneously. However, LASSO has some limitations, such as its
    tendency to select only one feature from a group of correlated features, and its
    performance may deteriorate if the number of features is much larger than the
    number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the application of LASSO for feature selection in predicting house
    prices. Imagine a dataset encompassing details about houses – such as the number
    of bedrooms, lot size, construction year, and so on – alongside their respective
    sale prices. Employing LASSO, we can pinpoint the most crucial features to predict
    the sale price while concurrently fitting a linear regression model to the dataset.
    The outcome is a model that’s ready to forecast the sale price of a new house
    based on its features.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ridge regression, a linear regression method applicable to feature selection,
    closely resembles ordinary least squares regression but introduces a penalty term
    to the cost function to counter overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In ridge regression, the cost function undergoes modification with the inclusion
    of a penalty term directly proportional to the square of the coefficients’ magnitude.
    This penalty term is regulated by a hyperparameter, often denoted as ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/138.png)
    or ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/144.png)
    dictating the regularization strength. When ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/144.png)
    is set to zero, ridge regression reverts to ordinary least squares regression.
  prefs: []
  type: TYPE_NORMAL
- en: The penalty term’s impact manifests in shrinking the coefficients’ magnitude
    toward zero. This proves beneficial in mitigating overfitting, discouraging the
    model from excessively relying on any single feature. In effect, the penalty term
    acts as a form of feature selection by reducing the importance of less relevant
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for the ridge regression loss function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi>min</mi><mi
    mathvariant="bold">w</mi></munder><msubsup><mfenced open="‖" close="‖"><mrow><mi
    mathvariant="bold">y</mi><mo>−</mo><mi mathvariant="bold">X</mi><mi mathvariant="bold">w</mi></mrow></mfenced><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi
    mathvariant="bold-italic">α</mi><msub><mfenced open="‖" close="‖"><mi mathvariant="bold">w</mi></mfenced><mn>2</mn></msub></mrow></mrow></math>](img/146.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*N* is the number of samples in the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y* is the column vector of target values of size *N*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X* is the design matrix of input features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w* is the vector of regression coefficients to be estimated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/147.png)
    is the regularization parameter that controls the strength of the penalty term.
    It is a hyperparameter that needs to be tuned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first term in the loss function measures the mean squared error between
    the predicted values and the true values. The second term is the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/148.png)
    penalty term that shrinks the coefficients toward zero. The ridge regression algorithm
    finds the values of the regression coefficients that minimize this loss function.
    By tuning the regularization parameter, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/149.png),
    we can control the bias-variance trade-off of the model, with higher alpha values
    leading to more regularization and lower overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression can be used for feature selection by examining the magnitudes
    of the coefficients produced by the model. Features with coefficients that are
    close to zero or smaller are considered less important and can be dropped from
    the model. The value of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/150.png)
    can be tuned using cross-validation to find the optimal balance between model
    complexity and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of ridge regression is its ability to handle multicollinearity,
    which occurs when there are strong correlations between the independent variables.
    In such cases, ordinary least squares regression can produce unstable and unreliable
    coefficient estimates, but ridge regression can help stabilize the estimates and
    improve the overall performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing LASSO or ridge regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ridge regression and LASSO are both regularization techniques that are used
    in linear regression to prevent overfitting of the model by penalizing the model’s
    coefficients. While both methods seek to prevent overfitting, they differ in their
    approach to how the coefficients are penalized.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression adds a penalty term to the **sum of squared errors** (**SSE**)
    that is proportional to the square of the magnitude of the coefficients. The penalty
    term is controlled by a regularization parameter (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>α</mml:mi></mml:math>](img/151.png)),
    which determines the amount of shrinkage applied to the coefficients. This penalty
    term shrinks the values of the coefficients toward zero but does not set them
    exactly to zero. Therefore, ridge regression can be used to reduce the impact
    of irrelevant features in a model, but it will not eliminate them completely.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, LASSO also adds a penalty term to the SSE, but the penalty
    term is proportional to the absolute value of the coefficients. Like ridge, LASSO
    also has a regularization parameter (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/152.png))
    that determines the amount of shrinkage applied to the coefficients. However,
    LASSO has a unique property of setting some of the coefficients exactly to zero
    when the regularization parameter is sufficiently high. Therefore, LASSO can be
    used for feature selection as it can eliminate irrelevant features and set their
    corresponding coefficients to zero.
  prefs: []
  type: TYPE_NORMAL
- en: In general, if the dataset has many features and a small number of them are
    expected to be important, LASSO regression is a better choice as it will set the
    coefficients of irrelevant features to zero, leading to a simpler and more interpretable
    model. On the other hand, if most of the features in the dataset are expected
    to be relevant, ridge regression is a better choice as it will shrink the coefficients
    toward zero but not set them exactly to zero, preserving all the features in the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to note that the optimal choice between ridge and LASSO
    depends on the specific problem and dataset, and it is often recommended to try
    both and compare their performance using cross-validation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These methods transform the features into a lower-dimensional space while retaining
    as much information as possible. Popular methods include PCA, **linear discriminant
    analysis** (**LDA**), and **t-**SNE.
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PCA is a widely used technique in machine learning for reducing the dimensionality
    of large datasets while retaining most of the important information. The basic
    idea of PCA is to transform a set of correlated variables into a set of uncorrelated
    variables known as principal components.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of PCA is to identify the directions of maximum variance in the data
    and project the data in these directions, reducing the dimensionality of the data.
    The principal components are sorted in order of the amount of variance they explain,
    with the first principal component explaining the most variance in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PCA algorithm involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardize the data**: PCA requires the data to be standardized – that is,
    each feature must have zero mean and unit variance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compute the covariance matrix**: The covariance matrix is a square matrix
    that measures the linear relationships between pairs of features in the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compute the eigenvectors and eigenvalues of the covariance matrix**: The
    eigenvectors represent the primary directions of the highest variance within the
    dataset, while the eigenvalues quantify the extent of variance elucidated by each
    eigenvector.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Select the number of principal components**: The number of principal components
    to retain can be determined by analyzing the eigenvalues and selecting the top
    *k* eigenvectors that explain the most variance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Project the data onto the selected principal components**: The original data
    is projected onto the selected principal components, resulting in a lower-dimensional
    representation of the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA can be used for feature selection by selecting the top *k* principal components
    that explain the most variance in the data. This can be useful for reducing the
    dimensionality of high-dimensional datasets and improving the performance of machine
    learning models. However, it’s important to note that PCA may not always lead
    to improved performance, especially if the data is already low-dimensional or
    if the features are not highly correlated. It’s also important to consider the
    interpretability of the selected principal components as they may not always correspond
    to meaningful features in the data.
  prefs: []
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LDA is a dimensionality reduction technique that’s used for feature selection
    in machine learning. It is often used in classification tasks to reduce the number
    of features by transforming them into a lower-dimensional space while retaining
    as much class-discriminatory information as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In LDA, the goal is to find a linear combination of the original features that
    maximizes the separation between classes. The input to LDA is a dataset of labeled
    examples, where each example is a feature vector with a corresponding class label.
    The output of LDA is a set of linear combinations of the original features, which
    can be used as new features in a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: To perform LDA, the first step is to compute the mean and covariance matrix
    of each class. The overall mean and covariance matrix are then calculated from
    the class means and covariance matrices. The goal is to project the data onto
    a lower-dimensional space while still retaining the class information. This is
    achieved by finding the eigenvectors and eigenvalues of the covariance matrix,
    sorting them in descending order of the eigenvalues, and selecting the top *k*
    eigenvectors that correspond to the *k* largest eigenvalues. The selected eigenvectors
    form the basis for the new feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LDA algorithm can be summarized in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the mean vector of each class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the covariance matrix of each class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the overall mean vector and overall covariance matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the between-class scatter matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the within-class scatter matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the eigenvectors and eigenvalues of the matrix using the following
    equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi
    mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math>](img/153.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math>](img/154.png)
    is the within-class scatter matrix and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math>](img/155.png)
    is the between-class scatter matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 7. Select the top *k* eigenvectors with the highest eigenvalues as the new feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: LDA is particularly useful when the number of features is large and the number
    of examples is small. It can be used in a variety of applications, including image
    recognition, speech recognition, and NLP. However, it assumes that the classes
    are normally distributed and that the class covariance matrices are equal, which
    may not always be the case in practice.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: t-SNE is a dimensionality reduction technique that’s used for visualizing high-dimensional
    data in a low-dimensional space, often used for feature selection. It was developed
    by Laurens van der Maaten and Geoffrey Hinton in 2008.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind t-SNE is to preserve the pairwise similarities of data
    points in a low-dimensional space, as opposed to preserving the distances between
    them. In other words, it tries to retain the local structure of the data while
    discarding the global structure. This can be useful in situations where the high-dimensional
    data is difficult to visualize, but there may be meaningful patterns and relationships
    among the data points.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE starts by calculating the pairwise similarity between each pair of data
    points in the high-dimensional space. The similarity is usually measured using
    a Gaussian kernel, which gives higher weights to nearby points and lower weights
    to distant points. The similarity matrix is then converted into a probability
    distribution using a softmax function. This distribution is used to create a low-dimensional
    space, typically 2D or 3D.
  prefs: []
  type: TYPE_NORMAL
- en: In the low-dimensional space, t-SNE again calculates the pairwise similarities
    between each pair of data points, but this time using a student’s t-distribution
    instead of a Gaussian distribution. The t-distribution has heavier tails than
    the Gaussian distribution, which helps to better preserve the local structure
    of the data. t-SNE then adjusts the position of the points in the low-dimensional
    space to minimize the difference between the pairwise similarities in the high-dimensional
    space and the pairwise similarities in the low-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE is a powerful technique for visualizing high-dimensional data by reducing
    it to a low-dimensional space. However, it is not typically used for feature selection
    as its primary purpose is to create visualizations of complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, t-SNE can be used to help identify clusters of data points that share
    similar features, which may be useful in identifying groups of features that are
    important for a particular task. For example, suppose you have a dataset of customer
    demographics and purchase history, and you want to identify groups of customers
    that are similar based on their purchasing behavior. You could use t-SNE to reduce
    the high-dimensional feature space to two dimensions, and then plot the resulting
    data points on a scatter plot. By examining the plot, you might be able to identify
    clusters of customers with similar purchasing behavior, which could then inform
    your feature selection process. Here’s a sample t-SNE for the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – t-SNE on the MNIST dataset](img/B18949_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – t-SNE on the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that t-SNE is primarily a visualization tool and should not
    be used as the sole method for feature selection. Instead, it can be used in conjunction
    with other techniques, such as LDA or PCA, to gain a more complete understanding
    of the underlying structure of your data.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of feature selection method depends on the nature of the data, the
    size of the dataset, the complexity of the model, and the computational resources
    available. It is important to carefully evaluate the performance of the model
    after feature selection to ensure that important information has not been lost.
    Another important process is feature engineering, which is about transforming
    or selecting features for the machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature engineering is the process of selecting, transforming, and extracting
    features from raw data to improve the performance of machine learning models.
    Features are the individual measurable properties or characteristics of the data
    that can be used to make predictions or classifications.
  prefs: []
  type: TYPE_NORMAL
- en: One common technique in feature engineering is feature selection, which involves
    selecting a subset of relevant features from the original dataset to improve the
    model’s accuracy and reduce its complexity. This can be done through statistical
    methods such as correlation analysis or feature importance ranking using decision
    trees or random forests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another technique in feature engineering is feature extraction, which involves
    transforming the raw data into a new set of features that may be more useful for
    the model. The primary distinction between feature selection and feature engineering
    lies in their approaches: while feature selection retains a subset of the original
    features without modifying the selected features, feature engineering algorithms
    reconfigure and transform the data into a new feature space. Feature engineering
    can be done through techniques such as dimensionality reduction, PCA, or t-SNE.
    Feature selection and extraction were explained in detail in the previous subsection
    (3-1-3).'
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling is another important technique in feature engineering that involves
    scaling the values of features to the same range, typically between 0 and 1 or
    -1 and 1\. This is done to prevent certain features from dominating others in
    the model and to ensure that the algorithm can converge quickly during training.
    When the features in the dataset have different scales, this can lead to issues
    when using certain machine learning algorithms that are sensitive to the relative
    magnitudes of the features. Feature scaling can help to address this problem by
    ensuring that all features are on a similar scale. Common methods for feature
    scaling include min-max scaling, Z-score scaling, and scaling by the maximum absolute
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several common methods for feature scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Min-max scaling**: Also known as normalization, this technique scales the
    values of the feature to be between a specified range, typically between 0 and
    1 (for regular machine learning models, and sometimes -1 and 1 for deep learning
    models). The formula for min-max scaling is shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/156.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is the original feature value, *min(x)* is the minimum value of the
    feature, and *max(x)* is the maximum value of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardization**: This technique transforms the feature values to have a
    mean of 0 and a standard deviation of 1\. Standardization is less affected by
    outliers in the data than min-max scaling. The formula for standardization is
    shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/157.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is the original feature value, *mean(x)* is the mean of the feature,
    and *std(x)* is the standard deviation of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: '**Robust scaling**: This technique is similar to standardization but uses the
    median and **interquartile range** (**IQR**) instead of the mean and standard
    deviation. Robust scaling is useful when the data contains outliers that would
    significantly affect the mean and standard deviation. The formula for robust scaling
    is shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mo>(</mo><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>Q</mi><mn>3</mn><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>Q</mi><mn>1</mn><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/158.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is the original feature value, *median(x)* is the median of the feature,
    *Q1(x)* is the first quartile of the feature, and *Q3(x)* is the third quartile
    of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: '**Log transformation**: This technique is used when the data is highly skewed
    or has a long tail. By taking the logarithm of the feature values, the distribution
    can be made more normal or symmetric, which can improve the performance of some
    machine learning algorithms. The formula for log transformation is shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>x</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/159.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is the original feature value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Power transformation**: This technique is similar to log transformation but
    allows for a broader range of transformations. The most common power transformation
    is the Box-Cox transformation, which raises the feature values to a power that
    is determined using maximum likelihood estimation. The formula for the Box-Cox
    transformation is shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>x</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><msup><mi>x</mi><mi>λ</mi></msup><mo>−</mo><mn>1</mn></mrow><mi>λ</mi></mfrac></mrow></mrow></math>](img/160.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is the original feature value, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/161.png)
    is the power parameter that is estimated using maximum likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the most common methods for feature scaling in machine learning.
    The choice of method depends on the distribution of the data, the machine learning
    algorithm being used, and the specific requirements of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: One final technique in feature engineering is feature construction, which involves
    creating new features by combining or transforming existing ones. This can be
    done through techniques such as polynomial expansion, logarithmic transformation,
    or interaction terms.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial expansion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Polynomial expansion is a feature construction technique that involves creating
    new features by taking polynomial combinations of existing features. This technique
    is commonly used in machine learning to model nonlinear relationships between
    features and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind polynomial expansion is to create new features by raising the
    existing features to different powers and taking their products. For example,
    suppose we have a single feature, *x*. We can create new features by taking the
    square of *x (**![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow></math>](img/162.png)**)*.
    We can also create higher-order polynomial features by taking *x* to even higher
    powers, such as ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>3</mn></msup></mrow></math>](img/163.png)*,*
    *![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>x</mi><mn>4</mn></msup></mrow></math>](img/164.png)*,
    and so on. In general, we can create polynomial features of degree *d* by taking
    all possible combinations of products and powers of the original features up to
    degree *d*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to creating polynomial features from a single feature, we can also
    create polynomial features from multiple features. For example, suppose we have
    two features, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/165.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/166.png).
    We can create new polynomial features by taking their products (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/167.png))
    and raising them to different powers ( ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/168.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/169.png),
    and so on). Again, we can create polynomial features of any degree by taking all
    possible combinations of products and powers of the original features.
  prefs: []
  type: TYPE_NORMAL
- en: One important consideration when using polynomial expansion is that it can quickly
    lead to a large number of features, especially for high degrees of polynomials.
    This can make the resulting model more complex and harder to interpret, and can
    also lead to overfitting if the number of features is not properly controlled.
    To address this issue, it is common to use regularization techniques or feature
    selection methods to select a subset of the most informative polynomial features.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, polynomial expansion is a powerful feature construction technique that
    can help capture complex nonlinear relationships between features and the target
    variable. However, it should be used with caution and with appropriate regularization
    or feature selection to avoid overfitting and maintain model interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a regression problem, you might have a dataset with a single
    feature, say *x*, and you want to fit a model that can capture the relationship
    between *x* and the target variable, *y*. However, the relationship between *x*
    and *y* may not be linear, and a simple linear model may not be sufficient. In
    this case, polynomial expansion can be used to create additional features that
    capture the non-linear relationship between *x* and *y*.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, let’s say you have a dataset with a single feature, *x*, and
    a target variable, *y*, and you want to fit a polynomial regression model. The
    goal is to find a function, *f(x)*, that minimizes the difference between the
    predicted and actual values of *y*.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial expansion can be used to create additional features based on *x*,
    such as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/170.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>](img/171.png),
    and so on. This can be done using libraries such as `scikit-learn`, which has
    a `PolynomialFeatures` function that can automatically generate polynomial features
    of a specified degree.
  prefs: []
  type: TYPE_NORMAL
- en: By adding these polynomial features, the model becomes more expressive and can
    capture the non-linear relationship between *x* and *y*. However, it’s important
    to be careful not to overfit the data as adding too many polynomial features can
    lead to a model that is overly complex and performs poorly on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Logarithmic transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logarithmic transformation is a common feature engineering technique that’s
    used in data preprocessing. The goal of logarithmic transformation is to make
    data less skewed and more symmetric by applying a logarithmic function to the
    features. This technique can be particularly useful for features that are skewed,
    such as those with a long tail of high values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logarithmic transformation is defined as an equation taking the natural
    logarithm of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>y</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/172.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is the transformed data and *x* is the original data. The logarithmic
    function maps the original data to a new space, where the relationship between
    the values is preserved but the scale is compressed. The logarithmic transformation
    is particularly useful for features with large ranges or that are distributed
    exponentially, such as the prices of products or the incomes of individuals.
  prefs: []
  type: TYPE_NORMAL
- en: One of the benefits of the logarithmic transformation is that it can help normalize
    data and make it more suitable for certain machine learning algorithms that assume
    normally distributed data. Additionally, logarithmic transformation can reduce
    the impact of outliers on the data, which can help improve the performance of
    some models.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the logarithmic transformation is not appropriate
    for all types of data. For example, if the data includes zero or negative values,
    the logarithmic transformation cannot be applied directly. In these cases, a modified
    logarithmic transformation, such as adding a constant before taking the logarithm,
    may be used. Overall, logarithmic transformation is a useful technique for feature
    engineering that can help improve the performance of machine learning models,
    especially when dealing with skewed or exponentially distributed data.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, feature engineering is a critical step in the machine learning pipeline
    as it can significantly impact the performance and interpretability of the resulting
    models. Effective feature engineering requires domain knowledge, creativity, and
    an iterative process of testing and refining different techniques until the optimal
    set of features is identified.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction terms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In feature construction, interaction terms refer to creating new features by
    combining two or more existing features in a dataset through multiplication, division,
    or other mathematical operations. These new features capture the interaction or
    relationship between the original features, and they can help improve the accuracy
    of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a dataset of real estate prices, you might have features such
    as the number of bedrooms, the number of bathrooms, and the square footage of
    the property. By themselves, these features provide some information about the
    price of the property, but they do not capture any interaction effects between
    the features. However, by creating an interaction term between the number of bedrooms
    and the square footage, you can capture the idea that larger properties with more
    bedrooms tend to be more expensive than smaller ones with the same number of bedrooms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, interaction terms are created by multiplying or dividing two or
    more features together. For example, if we have two features, *x* and *y*, we
    can create an interaction term by multiplying them together: *xy*. We can also
    create interaction terms by dividing one feature by another: *x/y*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating interaction terms, it is important to consider which features
    to combine and how to combine them. Here are some common techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain knowledge**: Use domain knowledge or expert intuition to identify
    which features are likely to interact and how they might interact.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pairwise combinations**: Create interaction terms by pairwise combining all
    pairs of features in the dataset. This can be computationally expensive, but it
    can help identify potential interaction effects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PCA**: Use PCA to identify the most important combinations of features, and
    create interaction terms based on these combinations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, interaction terms are a powerful tool in feature construction that
    can help capture complex relationships between features and improve the accuracy
    of machine learning models. However, it is important to be careful when creating
    interaction terms as too many or poorly chosen terms can lead to overfitting or
    decreased model interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Common machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will explain some of the most common machine learning models, as well
    as their advantages and disadvantages. Knowing this information will help you
    pick the best model for the problem and be able to improve the implemented model.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression is a type of supervised learning algorithm that’s used to
    model the relationship between a dependent variable and one or more independent
    variables. It assumes a linear relationship between the input features and the
    output. The goal of linear regression is to find the best-fit line that predicts
    the value of the dependent variable based on the independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for a simple linear regression with one independent variable (also
    called a **simple linear equation**) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mi>m</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow></mrow></math>](img/173.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* is the dependent variable (the variable we want to predict)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the independent variable (the input variable)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*m* is the slope of the line (how much *y* changes when *x* changes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* is the y-intercept (where the line intercepts the *Y*-axis when *x* = 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of linear regression is to find the values of *m* and *b* that minimize
    the difference between the predicted values and the actual values of the dependent
    variable. This difference is typically measured using a cost function, such as
    mean squared error or mean absolute error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple linear regression is an extension of simple linear regression, where
    there are multiple independent variables. The equation for multiple linear regression
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>b</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/174.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* is the dependent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/175.png)
    are the independent variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/176.png)
    is the y-intercept (when all the independent variables are equal to 0)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/177.png)
    are the coefficients (how much *y* changes when each independent variable changes)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: Similar to simple linear regression, the goal of multiple linear regression
    is to find the values of ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/178.png)![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></math>](img/179.png)
    that minimize the difference between the predicted values and the actual values
    of the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of linear regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s simple and easy to understand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used to model a wide range of relationships between the dependent
    and independent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s computationally efficient, making it fast and suitable for large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides interpretable results, allowing for the analysis of the impact of
    each independent variable on the dependent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of linear regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It assumes a linear relationship between the input features and the output,
    which may not always be the case in real-world data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may not capture complex non-linear relationships between the input features
    and the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s sensitive to outliers and influential observations, which can affect the
    accuracy of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It assumes that the errors are normally distributed with constant variance,
    which may not always hold true in practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is a popular machine learning algorithm that’s used for
    classification problems. Unlike linear regression, which is used for predicting
    continuous values, logistic regression is used for predicting discrete outcomes,
    typically binary outcomes (0 or 1).
  prefs: []
  type: TYPE_NORMAL
- en: The goal of logistic regression is to estimate the probability of a certain
    outcome based on one or more input variables. The output of logistic regression
    is a probability score, which can be converted into a binary class label by applying
    a threshold value. The threshold value can be adjusted to balance between precision
    and recall based on the specific requirements of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic regression model assumes that the relationship between the input
    variables and the output variable is linear in the logit (log odds) space. The
    logit function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo>(</mo><mi>p</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>p</mi><mo>/</mo><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/180.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p* is the probability of the positive outcome (that is, the probability
    of the event occurring).
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic regression model can be represented mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo>(</mo><mi>p</mi><mo>)</mo><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mo>..</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi><mi>n</mi></msub><mi
    mathvariant="normal">*</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></mrow></math>](img/181.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>,</mo><msub><mi>β</mi><mn>1</mn></msub><mo>,</mo><msub><mi>β</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>β</mi><mi>n</mi></msub></mrow></mrow></math>](img/182.png)
    are the coefficients of the model, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mrow></math>](img/183.png)
    are the input variables, and *logit(p)* is the logit function of the probability
    of a positive outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression model is trained using a dataset of labeled examples,
    where each example consists of a set of input variables and a binary label indicating
    whether the positive outcome occurred or not. The coefficients of the model are
    estimated using maximum likelihood estimation, which seeks to find the values
    of the coefficients that maximize the likelihood of the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of logistic regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretable**: The coefficients of the model can be interpreted as the
    change in the log odds of the positive outcome associated with a unit change in
    the corresponding input variable, making it easy to understand the impact of each
    input variable on the predicted probability of the positive outcome'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computationally efficient**: Logistic regression is a simple algorithm that
    can be trained quickly on large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Works well with small datasets**: Logistic regression can be effective even
    with a small number of observations, provided that the input variables are relevant
    to the prediction task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of logistic regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumes linearity**: Logistic regression assumes a linear relationship between
    the input variables and the logit of the probability of the positive outcome,
    which may not always be the case in real-world datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**May suffer from overfitting**: If the number of input variables is large
    compared to the number of observations, the model may suffer from overfitting,
    leading to poor generalization performance on new data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not suitable for non-linear problems**: Logistic regression is a linear algorithm
    and is not suitable for problems where the relationship between the input variables
    and the output variable is non-linear'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees are a type of supervised learning algorithm used for classification
    and regression analysis. A decision tree consists of a series of nodes that represent
    decision points, each of which has one or more branches that lead to other decision
    points or a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In a classification problem, each leaf node of the tree represents a class label,
    while in a regression problem, each leaf node represents a numerical value. The
    process of building a decision tree involves choosing a sequence of attributes
    that best splits the data into subsets that are more homogenous concerning the
    target variable. This process is typically repeated recursively for each subset
    until a stopping criterion is met, such as a minimum number of instances in each
    subset or a maximum depth of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equations for decision trees involve calculating the information gain (or
    another splitting criterion, such as Gini impurity or entropy) for each potential
    split at each decision point. The attribute with the highest information gain
    is selected as the split criterion for that node. The conceptual formula for information
    gain is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>I</mi><mi>n</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>g</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo>=</mo><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>(</mo><mi>p</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>)</mo><mo>−</mo><mo>[</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>e</mi><mi>d</mi><mi>a</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi
    mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>i</mi><mi>e</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>p</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>n</mi><mrow><mi>t</mi><mo>′</mo></mrow><mi>s</mi><mi>c</mi><mi>h</mi><mi>i</mi><mi>l</mi><mi>d</mi><mi>r</mi><mi>e</mi><mi>n</mi><mo>]</mo></mrow></mrow></mrow></math>](img/184.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *entropy* is a measure of the impurity or randomness of a system. In the
    context of decision trees, entropy is used to measure the impurity of a node in
    the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *entropy* of a node is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><msub><mrow><mi>l</mi><mi>o</mi><mi>g</mi></mrow><mn>2</mn></msub><msub><mi>p</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math>](img/185.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *c* is the number of classes and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/186.png)
    is the proportion of the samples that belong to class *i* in the node.
  prefs: []
  type: TYPE_NORMAL
- en: The entropy of a node ranges from 0 to 1, with 0 indicating a pure node (that
    is, all samples belong to the same class) and 1 indicating a node that is evenly
    split between all classes.
  prefs: []
  type: TYPE_NORMAL
- en: In a decision tree, the entropy of a node is used to determine the splitting
    criterion for the tree. The idea is to split the node into two or more child nodes
    such that the entropy of the child nodes is lower than the entropy of the parent
    node. The split with the lowest entropy is chosen as the best split.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the choice of the next node in the decision tree differs based
    on the underlying algorithm – for example, CART, ID3, or C4.5\. What we explained
    here was CART, which uses Gini impurity and entropy to split the data.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using entropy as a splitting criterion is that it can handle
    both binary and multi-class classification problems. It is also relatively computationally
    efficient compared to other splitting criteria. However, one disadvantage of using
    entropy is that it tends to create biased trees in favor of attributes with many
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the advantages of decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy to understand and interpret, even for non-experts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle both categorical and numerical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle missing data and outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used for feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be combined with other models in ensemble methods, such as random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some of the disadvantages of decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: Can be prone to overfitting, especially if the tree is too deep or complex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be sensitive to small changes in the data or the way the tree is built
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be biased toward features with many categories or high cardinality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can have problems with rare events or imbalanced datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Random forest is an ensemble learning method that’s versatile and can perform
    classification and regression tasks. It operates by generating multiple decision
    trees during training, predicting the target class for classification based on
    the majority of the trees, and the predicted value based on the mean prediction
    by trees for regression tasks. The algorithm for constructing a random forest
    can be summarized in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap sampling**: Randomly select a subset of the data with replacement
    to create a new dataset that’s the same size as the original dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature selection**: Randomly select a subset of the features (columns) for
    each split when building a decision tree. This helps to create diversity in the
    trees and reduce overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tree building**: Construct a decision tree for each bootstrap sample and
    feature subset. The decision tree is constructed recursively by splitting the
    data based on the selected features until a stopping criterion is met (for example,
    maximum depth or minimum number of samples in a leaf node).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ensemble learning**: Combine the predictions of all decision trees to make
    a final prediction. For classification, the class that receives the most votes
    from the decision trees is the final prediction. For regression, the average of
    the predictions from all decision trees is the final prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The random forest algorithm can be expressed mathematically as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Given a dataset, *D*, with *N* samples and *M* features, we create *T* decision
    trees {Tre e 1, Tre e 2, … , Tre e T} by applying the preceding steps. Each decision
    tree is constructed using a bootstrap sample of the data, *D’*, with size *N’
    (N’ <= N)* and a subset of the features, *F’*, with size *m (m <= M)*. For each
    split in the decision tree, we randomly select *k (k < m)* features from *F’*
    and choose the best feature to split the data based on an impurity measure (for
    example, Gini index or entropy). The decision tree is built until a stopping criterion
    is met (for example, the maximum depth or minimum number of samples in a leaf
    node).
  prefs: []
  type: TYPE_NORMAL
- en: The final prediction, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/187.png),
    for a new sample, *x*, is obtained by aggregating the predictions from all decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'For classification, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/188.png)
    is the class that receives the most votes from all decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><mi
    mathvariant="bold">y</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>j</mi></msub><mrow><munder><mo>∑</mo><mi>i</mi></munder><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/189.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/190.png)
    is the prediction of the *j-th* decision tree for the *i-th* sample, and *I()*
    is the indicator function that returns 1 if the condition is true and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'For regression, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/188.png)
    is the average of the predictions from all decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mover><mi
    mathvariant="bold">y</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mo>(</mo><mn>1</mn><mo>/</mo><mi>T</mi><mo>)</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></math>](img/192.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/193.png)
    is the prediction of the *i-th* decision tree for the new sample, *x*.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, random forest is a powerful machine learning algorithm that can
    handle high-dimensional and noisy datasets. It works by constructing multiple
    decision trees using bootstrap samples of the data and feature subsets, and then
    aggregating the predictions of all decision trees to make a final prediction.
    The algorithm is scalable, easy to use, and provides a measure of feature importance,
    making it a popular choice for many machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of random forests are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Robustness**: Random forest is a very robust algorithm that can handle a
    variety of input data types, including numerical, categorical, and ordinal data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: Random forest can rank the importance of features, allowing
    users to identify the most important features for classification or regression
    tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: Random forest has a built-in mechanism for reducing overfitting,
    called bagging, which helps to generalize well on new data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Random forest can handle large datasets with a high number
    of features, making it a good choice for big data applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers**: Random forest is robust to the presence of outliers as it is
    based on decision trees, which can handle outliers effectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of random forests are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretability**: Random forest models can be difficult to interpret as
    they are based on an ensemble of decision trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training time**: The training time of a random forest can be longer than
    other simpler algorithms, especially when the number of trees in the ensemble
    is large'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory usage**: Random forest requires more memory than some other algorithms
    as it has to store the decision trees in memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias**: Random forest can suffer from bias if the data is imbalanced or if
    the target variable has a high cardinality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: Although random forest is designed to prevent overfitting,
    it is still possible to overfit the model if the hyperparameters are not properly
    tuned'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, random forest is a powerful machine learning algorithm that has many
    advantages, but it is important to carefully consider its limitations before applying
    it to a particular problem.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines (SVMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SVMs are considered robust supervised learning algorithms that can perform
    both classification and regression tasks. They excel in scenarios with intricate
    decision boundaries, surpassing the limitations of linear models. At their core,
    SVMs aim to identify a hyperplane within a multi-dimensional space that maximally
    segregates the classes. This hyperplane is positioned to maximize the distance
    between itself and the closest points from each class, known as support vectors.
    Here’s how SVMs work for a binary classification problem. Given a set of training
    data, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>)</mo><mo>,</mo><mo>(</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>)</mo><mo>,</mo><mo>...</mo><mo>,</mo><mo>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><msub><mi>y</mi><mi>n</mi></msub><mo>)</mo><mo>}</mo></mrow></mrow></mrow></math>](img/194.png),
    where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/195.png)
    is a d-dimensional feature vector and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    is the binary class label (+1 or -1), the goal of an SVM is to find a hyperplane
    that separates the two classes with the largest margin. The margin is defined
    as the distance between the hyperplane and the closest data points from each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – SVM margins](img/B18949_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – SVM margins
  prefs: []
  type: TYPE_NORMAL
- en: 'The hyperplane is defined by a weight vector, *w*, and a bias term, *b*, such
    that for any new data point, *x*, the predicted class label, *y*, is given by
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi
    mathvariant="bold">x</mml:mi></mml:math>](img/197.png)+b)'
  prefs: []
  type: TYPE_IMG
- en: Here, *sign* is the sign function, which returns +1 if the argument is positive
    and -1 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function of an SVM is to minimize the classification error subject
    to the constraint that the margin is maximized. This can be formulated as an optimization
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>z</mi><mi>e</mi><mn>1</mn><mo>/</mo><mn>2</mn><msup><mrow><mo>|</mo><mo>|</mo><mi
    mathvariant="bold">w</mi><mo>|</mo><mo>|</mo></mrow><mn>2</mn></msup></mrow></mrow></math>](img/198.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>s</mi><mi>u</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>t</mi><mi>o</mi><msub><mi>y</mi><mi>i</mi></msub><mo>(</mo><msup><mi
    mathvariant="bold">w</mi><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi><mo>)</mo><mo>≥</mo><mn>1</mn><mi>f</mi><mi>o</mi><mi>r</mi><mi>i</mi><mo>=</mo><mn>1,2</mn><mo>,</mo><mo>...</mo><mo>,</mo><mi>n</mi></mrow></mrow></mrow></math>](img/199.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mrow><mo>|</mo><mo>|</mo><mi
    mathvariant="bold">w</mi><mo>|</mo><mo>|</mo></mrow><mn>2</mn></msup></mrow></math>](img/200.png)
    is the squared Euclidean norm of the weight vector, *w*. The constraints ensure
    that all data points are correctly classified and that the margin is maximized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the advantages of SVMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Effective in high-dimensional spaces, which is useful when the number of features
    is large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used for both classification and regression tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works well with both linearly separable and non-linearly separable data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle outliers well due to the use of the margin concept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has a regularization parameter that allows you to control overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some of the disadvantages of SVMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Can be sensitive to the choice of kernel function, which can greatly affect
    the performance of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally intensive for large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be difficult to interpret the results of an SVM model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires careful tuning of parameters to achieve good performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks and transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks and transformers are both powerful machine learning models that
    are used for a variety of tasks, such as image classification, NLP, and speech
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Neural networks draw inspiration from the structure and functioning of the
    human brain. They represent a category of machine learning models that are proficient
    in various tasks such as classification, regression, and more. Comprising multiple
    layers of interconnected nodes known as neurons, these networks adeptly process
    and manipulate data. The output of each layer is fed into the next layer, creating
    a hierarchy of feature representations. The input to the first layer is the raw
    data, and the output of the final layer is the prediction. A simple neural network
    for detecting the gender of a person based on their height and weight is shown
    in *Figure 3**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Simple neural network](img/B18949_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Simple neural network
  prefs: []
  type: TYPE_NORMAL
- en: 'The operation of a single neuron in a neural network can be represented by
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow></mrow></mfenced></mrow></mrow></math>](img/201.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/202.png)
    is the input values, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/203.png)
    is the weights of the connections between the neurons, *b* is the bias term, and
    *f* is the activation function. The activation function applies a non-linear transformation
    to the weighted sum of the inputs and bias term.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network involves adjusting the weights and biases of the neurons
    to minimize a loss function. This is typically done using an optimization algorithm
    such as stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of neural networks include their ability to learn complex non-linear
    relationships between input and output data, their ability to automatically extract
    meaningful features from raw data, and their scalability to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantages of neural networks include their high computational and memory
    requirements, their sensitivity to hyperparameter tuning, and the difficulty of
    interpreting their internal representations.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers are a type of neural network architecture that is particularly
    well suited to sequential data such as text or speech. They were introduced in
    the context of NLP and have since been applied to a wide range of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The core component of a transformer is the self-attention mechanism, which allows
    the model to attend to different parts of the input sequence when computing the
    output. The self-attention mechanism is based on a dot product between a query
    vector, a set of key vectors, and a set of value vectors. The resulting attention
    weights are used to weight the values, which are then combined to produce the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The self-attention operation can be represented by the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></mrow></mrow></math>](img/204.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></mrow></mrow></math>](img/205.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></mrow></mrow></math>](img/206.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>A</mi><mo>(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced
    open="(" close=")"><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>K</mi></msub></msqrt></mfrac></mfenced><mi>V</mi></mrow></mrow></mrow></math>](img/207.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *X* is the input sequence, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:math>](img/208.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math>](img/209.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math>](img/210.png)
    are learned projection matrices for the query, key, and value vectors, respectively,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:math>](img/211.png)
    is the dimensionality of the key vectors, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:math>](img/212.png)
    is a learned projection matrix that maps the output of the attention mechanism
    to the final output.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of transformers include their ability to handle variable-length
    input sequences, their ability to capture long-range dependencies in the data,
    and their state-of-the-art performance on many NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantages of transformers include their high computational and memory
    requirements, their sensitivity to hyperparameter tuning, and their difficulty
    in handling tasks that require explicit modeling of sequential dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few of the most popular machine learning models. The choice
    of model depends on the problem at hand, the size and quality of the data, and
    the desired outcome. Now that we have explored the most common machine learning
    models, we will explain model underfitting and overfitting, which happens during
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Model underfitting and overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, the ultimate goal is to build a model that can generalize
    well on unseen data. However, sometimes, a model can fail to achieve this goal
    due to either underfitting or overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Underfitting occurs when a model is too simple to capture the underlying patterns
    in the data. In other words, the model can’t learn the relationship between the
    features and the target variable properly. This can result in poor performance
    on both the training and testing data. For example, in *Figure 3**.4*, we can
    see that the model is underfitted, and it cannot present the data very well. This
    is not what we like in machine learning models, and we usually like to see a precise
    model, as shown in *Figure 3**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – The machine learning model underfitting on the training data](img/B18949_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – The machine learning model underfitting on the training data
  prefs: []
  type: TYPE_NORMAL
- en: 'Underfitting happens when the model is not trained well, or the model complexity
    is not enough to catch the underlying pattern in the data. To solve this problem,
    we can use more complex models, and continue the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Optimal fitting of the machine learning model on the training
    data](img/B18949_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Optimal fitting of the machine learning model on the training data
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimal fitting happens when the model captures the pattern in the data pretty
    well but does not overfit every single sample. This helps the model work better
    on unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Overfitting the model on the training data](img/B18949_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Overfitting the model on the training data
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, overfitting occurs when a model is too complex and fits the
    training data too closely, which can lead to poor generalization on new, unseen
    data, as shown in *Figure 3**.6*. This happens when the model learns the noise
    or random fluctuations in the training data, rather than the underlying patterns.
    In other words, the model becomes too specialized for the training data and does
    not perform well on the testing data. As shown in the preceding figure, the model
    is overfitted, and the model tried to predict every single sample very precisely.
    The problem with this model is that it does not learn the general pattern, and
    learns the pattern of each individual sample, which makes it work poorly when
    facing new, unseen records.
  prefs: []
  type: TYPE_NORMAL
- en: 'A useful way to understand the trade-off between underfitting and overfitting
    is through the bias-variance trade-off. Bias refers to the difference between
    the predicted values of the model and the actual values in the training data.
    A high bias means that the model is not complex enough to capture the underlying
    patterns in the data and underfits the data (*Figure 3**.7*). An underfit model
    has poor performance on both the training and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – High bias](img/B18949_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – High bias
  prefs: []
  type: TYPE_NORMAL
- en: 'Variance, on the other hand, refers to the sensitivity of the model to small
    fluctuations in the training data. A high variance means that the model is overly
    complex and overfits the data, which leads to poor generalization performance
    on new data. An overfit model has good performance on the training data but poor
    performance on the testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Just right (not high bias, not high variance)](img/B18949_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Just right (not high bias, not high variance)
  prefs: []
  type: TYPE_NORMAL
- en: 'To strike a balance between bias and variance, we need to choose a model that
    is neither too simple nor too complex. As mentioned previously, this is often
    referred to as the bias-variance trade-off (*Figure 3**.8*). A model with a high
    bias and low variance can be improved by increasing the complexity of the model,
    while a model with a high variance and low bias can be improved by decreasing
    the complexity of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – High variance](img/B18949_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – High variance
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods to reduce bias and variance in a model. One common
    approach is regularization, which adds a penalty term to the loss function to
    control the complexity of the model. Another approach is to use ensembles, which
    combine multiple models to improve the overall performance by reducing the variance.
    Cross-validation can also be used to evaluate the model’s performance and tune
    its hyperparameters to find the optimal balance between bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, understanding bias and variance is crucial in machine learning as it
    helps us to choose an appropriate model and identify the sources of error in the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Bias refers to the error that is introduced by approximating a real-world problem
    with a simplified model. Variance, on the other hand, refers to the error that
    is introduced by the model’s sensitivity to small fluctuations in the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: When a model has high bias and low variance, it is underfitting. This means
    that the model is not capturing the complexity of the problem and is making overly
    simplistic assumptions. When a model has low bias and high variance, it is overfitting.
    This means that the model is too sensitive to the training data and is fitting
    the noise instead of the underlying patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome underfitting, we can try increasing the complexity of the model,
    adding more features, or using a more sophisticated algorithm. To prevent overfitting,
    several methods can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-validation**: Assessing the performance of machine learning models
    is essential. Cross-validation serves as a method for assessing the effectiveness
    of a machine learning model. It entails training the model on one portion of the
    data and testing it on another. By employing distinct subsets for training and
    evaluation, cross-validation mitigates the risk of overfitting. Further elaboration
    on this technique will be provided in the subsequent section on data splitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: Regularization is a technique that’s used to add a penalty
    term to the loss function during training, which helps to reduce the complexity
    of the model and prevent overfitting. There are different types of regularization,
    including L1 regularization (LASSO), L2 regularization (ridge), and elastic net
    regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: Early stopping is a technique that’s used to stop the training
    process when the performance of the model on the validation data starts to degrade.
    This helps to prevent overfitting by stopping the model from continuing to learn
    from the training data when it has already reached its maximum performance. This
    technique is usually used in iterative algorithms such as deep learning methods,
    where the model is being trained for multiple iterations (epochs). To use early
    stopping, we usually train the model while evaluating the model performance on
    the training and validation subsets. The model’s performance usually improves
    on the training set with more training, but since the model has not seen the validation
    set, the validation error usually decreases initially and at some point, starts
    increasing again. This point is where the model starts overfitting. By visualizing
    the training and validation error of the model during training, we can identify
    and stop the model at this point (*Figure 3**.10*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Early stopping](img/B18949_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Early stopping
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropout**: Dropout is a technique in deep learning models that is used to
    randomly drop out some neurons during training, which helps to prevent the model
    from relying too heavily on a small subset of features or neurons and overfitting
    the training data. By dropping the weight of neurons in the model during the process,
    we make the model learn the general pattern and prevent it from memorizing the
    training data (overfitting).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data augmentation**: Data augmentation is a method that we can use to artificially
    expand the training data size by applying transformations, such as rotation, scaling,
    and flipping, to the existing dataset, which helps us to extend our training data.
    This strategy aids in mitigating overfitting by offering the model a more diverse
    set of examples to learn from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Ensemble methods are techniques that are used to combine
    multiple models to improve their performance and prevent overfitting. This can
    be done by using techniques such as bagging, boosting, or stacking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using these techniques, it is possible to prevent overfitting and build models
    that generalize well to new, unseen data. In practice, it is important to monitor
    both the training and testing performance of the model and make adjustments accordingly
    to achieve the best possible generalization performance. We will explain how to
    split our data into training and testing in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing a machine learning model, it’s important to split the data into
    training, validation, and test sets; this is called data splitting. This is done
    to evaluate the performance of the model on new, unseen data and to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common method for splitting the data is the train-test split, which
    splits the data into two sets: the training set, which is used to train the model,
    and the test set, which is used to evaluate the performance of the model. The
    data is randomly divided into two sets, with a typical split being 80% of the
    data for training and 20% for testing. Using this approach the model will be trained
    using the majority of the data (training data) and then tested on the remaining
    data (test set). Using this approach, we can ensure that the model’s performance
    is based on new, unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time in machine learning model development, we have a set of hyperparameters
    for our model that we like to tune (we will explain hyperparameter tuning in the
    next subsection). In this case, we like to make sure that the performance that
    we get on the test set is reliable and not just by chance based on a set of hyperparameters.
    In this case, based on the size of our training data, we can divide the data into
    60%, 20%, and 20% (or 70%, 15%, and 15%) for training, validation, and testing.
    In this case, we train the model on the training data and select the set of hyperparameters
    that give us the best performance on the validation set. We then report the actual
    model performance on the test set, which has not been seen or used before during
    model training or hyperparameter selection.
  prefs: []
  type: TYPE_NORMAL
- en: A more advanced method for splitting the data, especially when the size of our
    training data is limited, is k-fold cross-validation. In this method, the data
    is split into *k* equally sized “folds,” and the model is trained and tested *k*
    times, with each fold being used as the test set once and the remaining folds
    used as the training set. The results of each fold are then averaged to get an
    overall measure of the model’s performance. K-fold cross-validation is useful
    for small datasets where the train-test split may result in a large variance in
    performance evaluation. In this case, we report the average, minimum, and maximum
    performance of the model on each of the *k* folds, as shown in *Figure 3**.11*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – K-fold cross-validation](img/B18949_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – K-fold cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: Another variant of k-fold cross-validation is stratified k-fold cross-validation,
    which ensures that the distribution of the target variable is consistent across
    all folds. This is useful when dealing with imbalanced datasets, where the number
    of instances of one class is much smaller than the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series data requires special attention when splitting. In this case, we
    typically use a method called time series cross-validation, which preserves the
    temporal order of the data. In this method, the data is split into multiple segments,
    with each segment representing a fixed time interval. The model is then trained
    on the past data and tested on the future data. This helps to evaluate the performance
    of the model in real-world scenarios. You can see an example of how to split the
    data in a time series problem in *Figure 3**.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Time series data splitting](img/B18949_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Time series data splitting
  prefs: []
  type: TYPE_NORMAL
- en: In all cases, it’s important to ensure that the split is done randomly but with
    the same random seed each time to ensure the reproducibility of the results. It’s
    also important to ensure that the split is representative of the underlying data
    – that is, the distribution of the target variable should be consistent across
    all sets. Once we have split the data into different subsets for training and
    testing our model, we can try to find the best set of hyperparameters for our
    model. This process is called hyperparameter tuning and will be explained next.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyperparameter tuning is an important step in the machine learning process that
    involves selecting the best set of hyperparameters for a given model. Hyperparameters
    are values that are set before the training process begins and can have a significant
    impact on the model’s performance. Examples of hyperparameters include learning
    rate, regularization strength, number of hidden layers in a neural network, and
    many others.
  prefs: []
  type: TYPE_NORMAL
- en: The process of hyperparameter tuning involves selecting the best combination
    of hyperparameters that results in the optimal performance of the model. This
    is typically done by searching through a predefined set of hyperparameters and
    evaluating their performance on a validation set.
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods for hyperparameter tuning, including grid search,
    random search, and Bayesian optimization. Grid search involves creating a grid
    of all possible hyperparameter combinations and evaluating each one on a validation
    set to determine the optimal set of hyperparameters. Random search, on the other
    hand, randomly samples hyperparameters from a predefined distribution and evaluates
    their performance on a validation set.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random search** and **grid search** are methods that are used to search the
    search space, entirely or randomly, without considering previous hyperparameter
    results. Thus, these methods are inefficient. An alternative Bayesian optimization
    method has been proposed that iteratively computes the posterior distribution
    of the function and considers past evaluations to find the best hyperparameters.
    Using this approach, we can find the best set of hyperparameters with less iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian optimization utilizes past evaluations to probabilistically map hyperparameters
    to objective function scores, as demonstrated in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>|</mo><mi>h</mi><mi>y</mi><mi>p</mi><mi>e</mi><mi>r</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo>)</mo></mrow></mrow></mrow></math>](img/213.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the steps Bayesian optimization undertakes:'
  prefs: []
  type: TYPE_NORMAL
- en: It develops a surrogate probabilistic model for the objective function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It identifies the optimal hyperparameters based on the surrogate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It utilizes these hyperparameters in the actual objective function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It updates the surrogate model to integrate the latest results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It reiterates *steps 2* to *4* until it reaches the maximum iteration count
    or time limit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sequential model-based optimization** (**SMBO**) methods are a formalization
    of Bayesian optimization, with trials run one after another, trying better hyperparameters
    each time and updating a probability model (surrogate). SMBO methods differ in
    *steps 3* and *4* – specifically, how they build a surrogate of the objective
    function and the criteria used to select the next hyperparameters. These variants
    include Gaussian processes, random forest regressions, and tree-structured Parzen
    estimators, among others.'
  prefs: []
  type: TYPE_NORMAL
- en: In low-dimensional problems with numerical hyperparameters, Bayesian optimization
    is considered the best available hyperparameter optimization method. However,
    it is restricted to problems of moderate dimension.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these methods, there are also several libraries available that
    automate the process of hyperparameter tuning. Examples of these libraries include
    scikit-learn’s `GridSearchCV` and `RandomizedSearchCV`, `Keras Tuner`, and `Optuna`.
    These libraries allow for efficient hyperparameter tuning and can significantly
    improve the performance of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperparameter optimization in machine learning can be a complex and time-consuming
    process. Two primary complexity challenges arise in the search process: the trial
    execution time and the complexity of the search space, including the number of
    evaluated hyperparameter combinations. In deep learning, these challenges are
    especially pertinent due to the extensive search space and the utilization of
    large training sets.'
  prefs: []
  type: TYPE_NORMAL
- en: To address these issues and reduce the search space, some standard techniques
    may be used. For example, reducing the size of the training dataset based on statistical
    sampling or applying feature selection techniques can help reduce the execution
    time of each trial. Additionally, identifying the most important hyperparameters
    for optimization and using additional objective functions beyond just accuracy,
    such as the number of operations or optimization time, can help reduce the complexity
    of the search space.
  prefs: []
  type: TYPE_NORMAL
- en: By combining accuracy with visualization through a deconvolution network, researchers
    have achieved superior results. However, it’s important to note that these techniques
    are not exhaustive, and the best approach may depend on the specific problem at
    hand.
  prefs: []
  type: TYPE_NORMAL
- en: Another common approach for improving model performance is to use multiple models
    in parallel; these are called ensemble models. They are very useful in dealing
    with machine learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble modeling is a technique in machine learning that combines the predictions
    of multiple models to improve overall performance. The idea behind ensemble models
    is that multiple models can be better than a single model as different models
    may capture different patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: There are several types of ensemble models, all of which we’ll cover in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bootstrap aggregating**, also known as **bagging**, is an ensemble method
    that combines multiple independent models trained on different subsets of the
    training data to reduce variance and improve model generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The bagging algorithm can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a training dataset of size *n*, create *m* bootstrap samples of size *n*
    (that is, sample *n* instances with replacement *m* times).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a base model (for example, a decision tree) on each bootstrap sample independently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate the predictions of all base models to obtain the ensemble prediction.
    This can be done by either taking the majority vote (in the case of classification)
    or the average (in the case of regression).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bagging algorithm is particularly effective when the base models are unstable
    (that is, have high variance), such as decision trees, and when the training dataset
    is small.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for aggregating the predictions of the base models depends on
    the type of problem (classification or regression). For classification, the ensemble
    prediction is obtained by taking the majority vote:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>Y</mi><mrow><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><msub><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mi>j</mi></msub><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mrow><mo>=</mo><mi>j</mi><mo>)</mo></mrow></mrow></mrow></math>](img/214.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/215.png)
    is the predicted class of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/216.png)
    base model for the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/217.png)
    instance and *I()* is the indicator function (equal to 1 if x is true, and 0 otherwise).
  prefs: []
  type: TYPE_NORMAL
- en: 'For regression, the ensemble prediction is obtained by taking the average score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>Y</mi><mrow><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow></mrow></math>](img/218.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/193.png)
    is the predicted value of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    base model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of bagging are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Improved model generalization by reducing variance and overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to handle high-dimensional datasets with complex relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used with a variety of base models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of bagging are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Increased model complexity and computation time due to the use of multiple base
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can sometimes lead to overfitting if the base models are too complex or the
    dataset is too small
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not work well when the base models are highly correlated or biased
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boosting is another popular ensemble learning technique that aims to improve
    the performance of weak classifiers by combining them into a stronger classifier.
    Unlike bagging, boosting focuses on iteratively improving the accuracy of the
    classifier by adjusting the weights of the training examples. The basic idea behind
    boosting is to learn from the mistakes of the previous weak classifiers and to
    put more emphasis on the examples that were incorrectly classified in the previous
    iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several boosting algorithms, but one of the most popular ones is
    AdaBoost (short for adaptive boosting). The AdaBoost algorithm works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it initializes the weights of the training examples to be equal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it trains a weak classifier on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, it computes the weighted error rate of the weak classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, it computes the importance of the weak classifier based on its weighted
    error rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it increases the weights of the examples that were misclassified by the
    weak classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once it’s done this, it normalizes the weights of the examples so that they
    sum up to one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It repeats *steps 2* to *6* for a predetermined number of iterations or until
    the desired accuracy is achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it combines the weak classifiers into a strong classifier by assigning
    weights to them based on their importance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final classifier is a weighted combination of the weak classifiers. The
    importance of each weak classifier is determined by its weighted error rate, which
    is computed as an equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mrow><mi>E</mi><mi>R</mi><mi>R</mi><mi>O</mi><mi>R</mi></mrow><mi>m</mi></msub><mo>=</mo><mfrac><mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>w</mi><mi>i</mi></msub><mi>I</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac></mrow></mrow></math>](img/221.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *m* is the index of the weak classifier, *N* is the number of training
    examples, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/222.png)
    is the weight of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    training example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    is the true label of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    training example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math>](img/226.png)
    is the prediction of the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/227.png)
    weak classifier for the ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math>](img/220.png)
    training example, and ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>I</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced><mo>)</mo></mrow></mrow></mrow></math>](img/229.png)
    is an indicator function that returns 1 if the prediction of the weak classifier
    is incorrect and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The importance of the weak classifier is computed by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="normal">α</mi><mi>m</mi></msub><mo>=</mo><mi>ln</mi><mfrac><mrow><mn>1</mn><mo>−</mo><msub><mrow><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><mi>m</mi></msub></mrow><msub><mrow><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><mi>m</mi></msub></mfrac></mrow></mrow></math>](img/230.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The weights of the examples are updated based on their importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>i</mi><mrow><msub><mi
    mathvariant="normal">α</mi><mi>m</mi></msub><mi>I</mi><mfenced open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>h</mi><mi>m</mi></msub><mfenced
    open="(" close=")"><msub><mi>x</mi><mi>i</mi></msub></mfenced></mrow></mfenced></mrow></msubsup></mrow></mrow></math>](img/231.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final classifier is then obtained by combining the weak classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">α</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:math>](img/232.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *M* is the total number of weak classifiers, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/233.png)
    is the prediction of the *m-th* weak classifier, and `sign()` is a function that
    returns +1 if its argument is positive and -1 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some of the advantages of boosting:'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting can improve the accuracy of weak classifiers and can lead to a significant
    improvement in performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting is relatively easy to implement and can be applied to a wide range
    of classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting can handle noisy data and reduce the risk of overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some of the disadvantages of boosting:'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting can be sensitive to outliers and can overfit to noisy data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting can be computationally expensive, especially when dealing with large
    datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting can be difficult to interpret as it involves combining multiple weak
    classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stacking is another popular ensemble learning technique that combines the predictions
    of multiple base models by training a higher-level model on their predictions.
    The idea behind stacking is to leverage the strengths of different base models
    to achieve better predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how stacking works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the training data into two parts: the first part is used to train the
    base models, while the second part is used to create a new dataset of predictions
    from the base models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train multiple base models on the first part of the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the trained base models to make predictions on the second part of the training
    data to create a new dataset of predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a higher-level model (also known as a metamodel or blender) on the new
    dataset of predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the trained higher-level model to make predictions on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The higher-level model is typically a simple model such as a linear regression,
    logistic regression, or a decision tree. The idea is to use the predictions of
    the base models as input features for the higher-level model. This way, the higher-level
    model learns to combine the predictions of the base models to make more accurate
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most commonly known ensemble models is random forest, where the model
    combines the predictions of multiple decision trees and outputs the predictions.
    This is usually more accurate and prone to overfitting. We elaborated on Random
    Forest earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient boosting is another ensemble model that can be used for classification
    and regression tasks. It works by getting a weak classifier (such as a simple
    tree), and in each step tries to improve this weak classifier to build a better
    model. The main idea here is that the model tries to focus on its mistakes in
    each step and improve itself by fitting the model by correcting the errors made
    in previous trees.
  prefs: []
  type: TYPE_NORMAL
- en: During each iteration, the algorithm computes the negative gradient of the loss
    function concerning the predicted values, followed by fitting a decision tree
    to these negative gradient values. The predictions of the new tree are then combined
    with the predictions of the previous trees, using a learning rate parameter that
    controls the contribution of each tree to the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The overall prediction of the gradient boosting model is obtained by summing
    up the predictions of all the trees, which are weighted by their respective learning
    rates.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the equation for the gradient boosting algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize the model with a constant value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>F</mi><mn>0</mn></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mi>c</mi></msub><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mi>c</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/234.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *c* is a constant, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/196.png)
    is the true label of the *i-th* sample, *N* is the number of samples, and *L*
    is the loss function, which is used to measure the error between the predicted
    and true labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each iteration, *m*, the algorithm fits a decision tree to the negative
    gradient values of the loss function concerning the predicted values, ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msub><mi>r</mi><mi>m</mi></msub><mo>=</mo><mo>−</mo><mo>∇</mo><mi>L</mi><mo>(</mo><mi>y</mi><mo>,</mo><mi>F</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/236.png).
    The decision tree predicts the negative gradient values, which are then used to
    update the predictions of the model via the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>F</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msub><mi>F</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>η</mi><mi
    mathvariant="normal">*</mi><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/237.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><msub><mi>F</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/238.png)
    is the prediction of the model at the previous iteration, *η* is the learning
    rate, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/239.png)
    is the prediction of the decision tree at the current iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final prediction of the model is obtained by combining the predictions
    of all the trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>F</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>η</mi><mi>m</mi></msub><mi
    mathvariant="normal">*</mi><msub><mi>h</mi><mi>m</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math>](img/240.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *M* is the total number of trees in the model and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math>](img/241.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/242.png)
    are the learning rate and prediction of the *m-th* tree, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some of the advantages of gradient boosting:'
  prefs: []
  type: TYPE_NORMAL
- en: High prediction accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handles both regression and classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle missing data and outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used with various loss functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle high-dimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s look at some of the disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive to overfitting, especially when the number of trees is large
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally expensive and time-consuming to train, especially for large
    datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires careful tuning of hyperparameters, such as the number of trees, the
    learning rate, and the maximum depth of the trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, we have reviewed the ensemble models that can help us improve our
    model performance. However, sometimes, our dataset has some features that we need
    to consider before we apply machine learning models. One common case is when we
    have an imbalanced dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Handling imbalanced data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most real-world problems, our data is imbalanced, which means that the distribution
    of records from different classes (such as patients with and without cancer) is
    different. Handling imbalanced datasets is an important task in machine learning
    as it is common to have datasets with uneven class distribution. In such cases,
    the minority class is often under-represented, which can cause poor model performance
    and biased predictions. The reason behind this is that machine learning methods
    are trying to optimize their fitness function to minimize the error in the training
    set. Now, let’s say that we have 99% of the data from the positive class and 1%
    from the negative class. In this case, if the model predicts all records as positive,
    the error will be 1%; however, this model is not useful for us. That’s why, if
    we have an imbalanced dataset, we need to use various methods to handle imbalanced
    data. In general, we can have three categories of methods to handle imbalanced
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Undersampling**: A very simple method that comes to mind is to use fewer
    training records from the majority class. This method works, but we need to consider
    that by using less training data, we are feeding less information to the model
    causes to have a less robust training and final model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resampling**: Resampling methods involve modifying the original dataset to
    create a balanced distribution. This can be achieved by either oversampling the
    minority class (creating more samples of the minority class) or undersampling
    the majority class (removing samples from the majority class). Oversampling techniques
    include **random oversampling**, **Synthetic Minority Oversampling Technique**
    (**SMOTE**), and **Adaptive Synthetic Sampling** (**ADASYN**). Undersampling techniques
    include **random undersampling**, **Tomek links**, and **cluster centroids**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling imbalanced datasets in machine learning models**: Such as modifying
    cost function, or modified batching in deep learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SMOTE is a widely used algorithm for handling imbalanced datasets in machine
    learning. It is a synthetic data generation technique that creates new, synthetic
    samples in the minority class by interpolating between existing samples. SMOTE
    works by identifying the k-nearest neighbors of a minority class sample and then
    generating new samples along the line segments that connect these neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps of the SMOTE algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a minority class sample, *x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose one of its k-nearest neighbors, *x’*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate a synthetic sample by interpolating between *x* and *x’*. To do this,
    choose a random number, *r*, between 0 and 1, and then calculate the synthetic
    sample, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>r</mi><mi
    mathvariant="normal">*</mi><mfenced open="(" close=")"><mrow><mrow><mi>x</mi><mo>′</mo></mrow><mo>−</mo><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/243.png)'
  prefs: []
  type: TYPE_IMG
- en: This creates a new sample that is somewhere between *x* and *x’*, but not the
    same as either one.
  prefs: []
  type: TYPE_NORMAL
- en: 4. Repeat *steps 1* to *3* until the desired number of synthetic samples has
    been generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the advantages and disadvantages of SMOTE:'
  prefs: []
  type: TYPE_NORMAL
- en: It helps to address the problem of class imbalance by creating synthetic samples
    in the minority class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE can be combined with other techniques, such as random undersampling or
    Tomek links, to further improve the balance of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE can be applied to both categorical and numerical data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE can sometimes create synthetic samples that are unrealistic or noisy,
    leading to overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE can sometimes cause the decision boundary to be too sensitive to the minority
    class, leading to poor performance of the majority class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE can be computationally expensive for large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of SMOTE in action. Suppose we have a dataset with two classes:
    the majority class (class 0) has 900 samples, and the minority class (class 1)
    has 100 samples. We want to use SMOTE to generate synthetic samples for the minority
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: We select a minority class sample, *x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We choose one of its k-nearest neighbors, *x’*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We generate a synthetic sample by interpolating between *x* and *x’* using
    a random number, *r*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>r</mi><mi
    mathvariant="normal">*</mi><mo>(</mo><mi>x</mi><mo>′</mo><mo>−</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>](img/244.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, suppose *x* is (*1, 2*), *x’* is (*3, 4*), and *r* is *0.5*. In
    this case, the new sample is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>n</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo>=</mo><mo>(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>)</mo><mo>+</mo><mn>0.5</mn><mi
    mathvariant="normal">*</mi><mo>(</mo><mo>(</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>)</mo><mo>−</mo><mo>(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>)</mo><mo>)</mo><mo>=</mo><mo>(</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>)</mo></mrow></mrow></mrow></math>](img/245.png)'
  prefs: []
  type: TYPE_IMG
- en: 4. We repeat *steps 1* to *3* until we have generated the desired number of
    synthetic samples. For example, suppose we want to generate 100 synthetic samples.
    We repeat *steps 1* to *3* for each of the 100 minority class samples and then
    combine the original minority class samples with the synthetic samples to create
    a balanced dataset with 200 samples in each class.
  prefs: []
  type: TYPE_NORMAL
- en: The NearMiss algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `NearMiss` algorithm is a technique for balancing class distribution by
    undersampling (removing) the records from the major class. When two classes have
    records that are very close to each other, eliminating some of the records from
    the majority class increases the distance between the two classes, which helps
    the classification process. To avoid information loss problems in the majority
    of undersampling methods, near-miss methods are widely used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The working of nearest-neighbor methods is based on the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the distances between all records from the major class and minor class.
    Our goal is to undersample the records from the major class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose *n* records from the major class that are closest to the minor class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are *k* records in the minor class, the nearest method will return
    *k*n* records from the major class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are three variations of applying the `NearMiss` algorithm that we can
    use to find the *n* closest records in the major class:'
  prefs: []
  type: TYPE_NORMAL
- en: We can select the records of the major class for which the average distances
    to the k-closest records of the minor class are the smallest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can select the records of the major class for which the average distances
    to the k-farthest records of the minor class are the smallest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can implement two steps. In the first step, for each record from the minor
    class, their *M* nearest neighbors will be stored. Then, the records from the
    major class are selected such that the average distance to the *N* nearest neighbors
    is the largest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cost-sensitive learning is a method that’s used to train machine learning models
    on imbalanced datasets. In imbalanced datasets, the number of examples in one
    class (usually the minority class) is much lower than in the other class (usually
    the majority class). Cost-sensitive learning involves assigning misclassification
    costs to the model that differ based on the class being predicted, which can help
    the model focus more on correctly classifying the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we have a binary classification problem with two classes, positive
    and negative. In cost-sensitive learning, we assign different costs to different
    types of errors. For example, we may assign a higher cost to misclassifying a
    positive example as negative because in an imbalanced dataset, the positive class
    is the minority class, and misclassifying positive examples can have a greater
    impact on the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can assign costs in the form of a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted Positive** | **Predicted Negative** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual Positive** | `TP_cost` | `FN_cost` |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual Negative** | `FP_cost` | `TN_cost` |'
  prefs: []
  type: TYPE_TB
- en: Table 3.2 – Confusion matrix costs
  prefs: []
  type: TYPE_NORMAL
- en: Here, `TP_cost`, `FN_cost`, `FP_cost`, and `TN_cost` are the costs associated
    with true positives, false negatives, false positives, and true negatives, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'To incorporate the cost matrix into the training process, we can modify the
    standard loss function that the model optimizes during training. One common cost-sensitive
    loss function is the weighted cross-entropy loss, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>−</mo><mo>(</mo><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mi
    mathvariant="normal">*</mi><mi>y</mi><mi mathvariant="normal">*</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mover><mi
    mathvariant="normal">y</mi><mo stretchy="true">ˆ</mo></mover><mo>)</mo><mo>+</mo><msub><mi>w</mi><mrow><mi>n</mi><mi>e</mi><mi>g</mi></mrow></msub><mi
    mathvariant="normal">*</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>)</mo><mi
    mathvariant="normal">*</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>1</mn><mo>−</mo><mover><mi
    mathvariant="normal">y</mi><mo stretchy="true">ˆ</mo></mover><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/246.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is the true label (either 0 or 1), ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/247.png)
    is the predicted probability of the positive class, and ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/248.png)and
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/249.png)
    are weights that are assigned to the positive and negative classes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The weights, ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/250.png)and
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/251.png),
    can be determined by the costs assigned in the confusion matrix. For example,
    if we assign a higher cost to false negatives (that is, misclassifying a positive
    example as negative), we may set ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>](img/252.png)to
    a higher value than ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>](img/249.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Cost-sensitive learning can also be used with other types of models, such as
    decision trees and SVMs. The concept of assigning costs to different types of
    errors can be applied in various ways to improve the performance of a model on
    imbalanced datasets. However, it’s important to carefully select the appropriate
    cost matrix and loss function based on the specific characteristics of the dataset
    and the problem being solved:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensemble techniques**: Ensemble techniques combine multiple models to improve
    predictive performance. In imbalanced datasets, an ensemble of models can be trained
    on different subsets of the dataset, ensuring that each model is trained on both
    the minority and majority classes. Examples of ensemble techniques for imbalanced
    datasets include bagging and boosting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Anomaly detection techniques can be used to identify
    the minority class as an anomaly in the dataset. These techniques aim to identify
    rare events that are significantly different from the majority class. The identified
    samples can then be used to train the model on the minority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea behind data augmentation is to generate new examples by applying transformations
    to the original ones, while still retaining the label. These transformations can
    include rotation, translation, scaling, flipping, and adding noise, among others.
    This can be particularly useful for imbalanced datasets, where the number of examples
    in one class is much smaller than in the other.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of imbalanced datasets, data augmentation can be used to create
    new examples of the minority class, effectively balancing the dataset. This can
    be done by applying the same set of transformations to the minority class examples,
    creating a new set of examples that are still representative of the minority class
    but are slightly different from the original ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equations that are involved in data augmentation are relatively simple
    as they are based on applying transformation functions to the original examples.
    For example, to rotate an image by a certain angle, we can use a rotation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mi>x</mi><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>−</mo><mi>y</mi><mi>s</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/254.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>y</mi><mo>′</mo><mo>=</mo><mi>x</mi><mi>s</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>+</mo><mi>y</mi><mi>c</mi><mi>o</mi><mi>s</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></math>](img/255.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* and *y* are the original coordinates of a pixel in the image, *x’*
    and *y’* are the new coordinates after rotation, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>θ</mml:mi></mml:math>](img/256.png)
    is the angle of rotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, to apply translation, we can simply shift the image by a certain
    number of pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>x</mi><mo>′</mo><mo>=</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/257.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mo>′</mo><mo>=</mo><mi>y</mi><mo>+</mo><mi>d</mi><mi>y</mi></mrow></mrow></math>](img/258.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *dx* and *dy* are the horizontal and vertical shifts, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation can be a powerful technique for addressing imbalanced datasets
    as it can create new examples that are representative of the minority class, while
    still preserving the label information. However, it is important to be careful
    when applying data augmentation as it can also introduce noise and artifacts in
    the data, and can lead to overfitting if not done properly.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, handling imbalanced datasets is an important aspect of machine
    learning. There are several techniques available to handle imbalanced datasets,
    each with its advantages and disadvantages. The choice of technique depends on
    the dataset, the problem, and the available resources. Besides having imbalanced
    data, in the case of working on time series data, we might face correlated data.
    We’ll take a closer look at this next.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with correlated data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dealing with correlated time series data in machine learning models can be
    challenging as traditional techniques such as random sampling can introduce biases
    and overlook dependencies between data points. Here are some approaches that can
    help:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time series cross-validation**: Time series data is often dependent on past
    values and it’s important to preserve this relationship during model training
    and evaluation. Time series cross-validation involves splitting the data into
    multiple folds, with each fold consisting of a continuous block of time. This
    approach ensures that the model is trained on past data and evaluated on future
    data, which better simulates how the model will perform in real-world scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Correlated time series data can be difficult to model
    with traditional machine learning algorithms. Feature engineering can help transform
    the data into a more suitable format. Examples of feature engineering for time
    series data include creating lags or differences in the time series, aggregating
    data into time buckets or windows, and creating rolling statistics such as moving
    averages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series-specific models**: There are several models specifically designed
    for time series data, such as **AutoRegressive Integrated Moving Average** (**ARIMA**),
    **Seasonal ARIMA** (**SARIMA**), **Prophet**, and **Long Short-Term Memory** (**LSTM**)
    networks. These models are designed to capture the dependencies and patterns in
    time series data and may outperform traditional machine learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series preprocessing techniques**: Time series data can be preprocessed
    to remove correlations and make the data more suitable for machine learning models.
    Techniques such as differencing, detrending, and normalization can help remove
    trends and seasonal components from the data, which can help reduce correlations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction techniques**: Correlated time series data can have
    a high dimensionality, which can make modeling difficult. Dimensionality reduction
    techniques such as PCA or autoencoders can help reduce the number of variables
    in the data while preserving the most important information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, it’s important to approach time series data with techniques that
    preserve the temporal dependencies and patterns in the data. This can require
    specialized modeling techniques and preprocessing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about various concepts related to machine learning,
    starting with data exploration and preprocessing techniques. We then explored
    various machine learning models, such as logistic regression, decision trees,
    support vector machines, and random forests, along with their strengths and weaknesses.
    We also discussed the importance of splitting data into training and test sets,
    as well as techniques for handling imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also covered the concepts of model bias, variance, underfitting,
    and overfitting, and how to diagnose and address these issues. We also explored
    ensemble methods such as bagging, boosting, and stacking, which can improve model
    performance by combining the predictions of multiple models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned about the limitations and challenges of machine learning,
    including the need for large amounts of high-quality data, the risk of bias and
    unfairness, and the difficulty of interpreting complex models. Despite these challenges,
    machine learning offers powerful tools for solving a wide range of problems and
    has the potential to transform many industries and fields.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss text preprocessing, which is required for
    text to be used by machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., de Freitas, N.: *Taking
    the human out of the loop: A review of Bayesian optimization. Proceedings of the
    IEEE 104(1), 148–175 (2016).* *DOI 10.1109/JPROC.2015.2494218*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
