- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Creating and Connecting a Knowledge Graph to an AI Agent
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和连接知识图谱到AI代理
- en: In the previous two chapters, we discussed the RAG framework in detail. We started
    with naïve RAG and then saw how we could add different components, replace others,
    or modify the entire pipeline for our needs. The whole system is extremely flexible,
    but some concepts remain the same. First, we start with a corpus (or multiple
    corpora of texts) and conduct embedding of these texts to obtain a database of
    vectors. Once the user query arrives, we conduct a similarity search on this database
    of vectors. Regardless of the scope or type of texts, our pipeline is based on
    the concept of vectorizing these texts in some way and then providing the information
    contained in the discovered texts to the LLM.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们详细讨论了RAG框架。我们从简单的RAG开始，然后看到了我们如何添加不同的组件，替换其他组件，或修改整个管道以满足我们的需求。整个系统非常灵活，但有些概念保持不变。首先，我们从语料库（或多个文本语料库）开始，对这些文本进行嵌入以获得向量数据库。一旦用户查询到达，我们就对这个向量数据库进行相似度搜索。无论文本的范围或类型如何，我们的管道都基于以某种方式对文本进行矢量化，然后向LLM提供发现的文本中包含的信息。
- en: Texts are often full of redundant information, and in the previous chapter,
    we saw that LLMs are sensitive to the amount of noise in the input. Most people
    have seen the benefit of creating schematic notes or mind maps. These schematics
    are concise because of the principle that underlining everything in a book is
    like underlining nothing. The principle of these diagrams is to extract the key
    information to remember that will enable us to answer questions in the future.
    Schematics should present the fundamental information and the relationships that
    connect them. These schemas can be represented as a graph and, more precisely,
    as a knowledge graph. The advantage of these graphs is that they are compact,
    represent knowledge as entities and relationships, and we can conduct analyses
    and use graph search algorithms on them. Over the years, these **knowledge graphs**
    (**KGs**) have been built by major companies or institutions and are now available
    for use. Many of these KGs have been used for information extraction, where information
    is extracted with a series of queries to answer questions. This extracted information
    is a series of entities and relationships, rich in knowledge but less understandable
    to us humans. The natural step is to use this information for the context of an
    LLM and then generate a natural language response. This paradigm is called **GraphRAG,**
    and we will discuss it in detail in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 文本通常充满了冗余信息，在上一章中，我们看到了LLMs对输入中噪声量的敏感性。大多数人已经看到了创建图表笔记或思维导图的好处。这些图表之所以简洁，是因为在书中划线一切就像没有划线一样。这些图表的原则是提取关键信息以便我们能够记住，这将使我们能够回答未来的问题。图表应该展示基本信息和连接它们的关系。这些模式可以表示为图形，更精确地说，是知识图谱。这些图形的优点是它们紧凑，将知识表示为实体和关系，并且我们可以对它们进行分析和使用图搜索算法。多年来，这些**知识图谱**（**KGs**）由主要公司或机构构建，现在可供使用。许多这些KGs已被用于信息提取，其中信息通过一系列查询来提取以回答问题。这些提取的信息是一系列实体和关系，知识丰富但对我们人类来说不太容易理解。自然的下一步是使用这些信息为LLM的上下文生成自然语言响应。这种范式被称为**GraphRAG**，我们将在本章中详细讨论。
- en: In any case, nothing prohibits us from using an LLM for all the steps in KG.
    In fact, LLMs have a number of innate capabilities that make them useful even
    for tasks for which they are not trained. This is precisely why we will see that
    we can use LLMs to extract relationships and entities and build our KGs. LLMs,
    though, also possess reasoning capabilities, and in this chapter, we will discuss
    how we can use these models to reason both about the information contained in
    graphs and about the structure of the graphs themselves. Finally, we will discuss
    what perspectives and questions remain open, and the advantages and disadvantages
    of the proposed approaches.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，没有什么阻止我们使用LLM在KG的所有步骤中。事实上，LLMs具有许多固有的能力，即使对于它们未经过训练的任务也很有用。这正是我们将看到我们可以使用LLMs来提取关系和实体并构建我们的KGs的原因。然而，LLMs也具备推理能力，在本章中，我们将讨论我们如何使用这些模型来推理图中包含的信息以及图的结构本身。最后，我们将讨论哪些观点和问题仍然是开放的，以及所提出方法的优缺点。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Introduction to knowledge graphs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识图谱简介
- en: Creating a knowledge graph with your LLM
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你的LLM创建知识图谱
- en: Retrieving information with a knowledge graph and an LLM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用知识图谱和大型语言模型检索信息
- en: Understanding graph reasoning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解图推理
- en: Ongoing challenges in knowledge graphs and GraphRAG
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识图谱和GraphRAG的持续挑战
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Most of this code can be run on a CPU, but it is preferable to be run on a GPU.
    The code is written in PyTorch and uses standard libraries for the most part (PyTorch,
    Hugging Face Transformers, LangChain, ChromaDB, `sentence-transformer`, `faiss-cpu`,
    and so on).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码都可以在CPU上运行，但更倾向于在GPU上运行。代码是用PyTorch编写的，大部分使用标准库（PyTorch、Hugging Face Transformers、LangChain、ChromaDB、`sentence-transformer`、`faiss-cpu`等）。
- en: 'In this chapter, we also use Neo4j as the database for the graph. Although
    we will do all operations with Python, Neo4j must be installed and you must be
    registered to use it. The code can be found on GitHub: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们也将使用Neo4j作为图数据库。尽管我们将使用Python进行所有操作，但Neo4j必须安装，并且您必须注册才能使用它。代码可以在GitHub上找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7)。
- en: Introduction to knowledge graphs
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识图谱简介
- en: 'Knowledge representation is one of the open problems of AI and has very ancient
    roots (Leibniz believed that the whole knowledge could be represented and used
    to conduct calculations). The interest in knowledge representation is based on
    the fact that it represents the first step in conducting computer reasoning. Once
    this knowledge is organized in an orderly manner, it can be used to design inference
    algorithms and solve reasoning problems. Early studies focused on using deduction
    to solve problems about organized entities (e.g., through the use of ontologies).
    This has worked well for many toy problems, but it is laborious, often requires
    a whole set of hardcoded rules, and risks succumbing to combinatorial explosion.
    Because search in these spaces could be extremely computationally expensive, an
    attempt was made to define two concepts:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 知识表示是人工智能的开放问题之一，并且有着非常悠久的历史（莱布尼茨认为所有知识都可以被表示并用于计算）。对知识表示的兴趣基于这样一个事实，即它是进行计算机推理的第一步。一旦这些知识被有序地组织起来，就可以用来设计推理算法和解决推理问题。早期的研究主要集中在使用演绎法来解决有关有序实体的问题（例如，通过使用本体）。这对于许多玩具问题来说效果很好，但它很费力，通常需要一套硬编码的规则，并且存在陷入组合爆炸的风险。因为这些空间中的搜索可能极其计算密集，因此尝试定义了两个概念：
- en: '**Limited rationality**: Finding a solution but also considering the cost of
    it'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限理性**：找到解决方案的同时，也要考虑其成本'
- en: '**Heuristic search**: Limiting the search in space, thus finding a semi-optimal
    solution (a local but not global optima)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启发式搜索**：限制搜索空间，从而找到半优解（局部但非全局最优解）'
- en: 'These principles have inspired a whole series of algorithms that have since
    allowed information searches to be conducted more efficiently and tractably. Interest
    in these algorithms grew strongly in the late 1990s with the advent of the World
    Wide Web and the need to conduct internet searches quickly and accurately. Regarding
    data, the World Wide Web is also based on three technological principles:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则激发了一系列算法的产生，这些算法随后使得信息搜索变得更加高效和可行。在20世纪90年代末，随着万维网的诞生和快速准确进行互联网搜索的需求，对这些算法的兴趣急剧增长。关于数据，万维网也基于三个技术原则：
- en: '**Distributed data**: Data is distributed across the world and accessible from
    all parts of the world'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式数据**：数据分布在全球各地，可以从世界各地的任何地方访问'
- en: '**Connected data**: Data is interconnected and not isolated; the data’s meaning
    is a function of its connection with other data'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接数据**：数据是相互关联的，而不是孤立的；数据的意义是其与其他数据连接的函数'
- en: '**Semantic metadata**: In addition to the data itself, we have information
    about its relationships, and this metadata allows us to search efficiently'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义元数据**：除了数据本身之外，我们还有关于其关系的信息，并且这些元数据使我们能够高效地搜索'
- en: For this reason, we began to search for a technology that could respect the
    nature of this new data. It came naturally to turn to representations of a graphical
    nature. In fact, by definition, graphs model relationships between different entities.
    This approach began to be used in web searches in 2012 when Google began adding
    knowledge cards for each concept searched. These knowledge cards can be seen as
    graphs of name entities in which the connections are the graph links. These cards
    then allow for more relevant search and user facilitation. Subsequently, the term
    *knowledge graph* came to mean any graph that connects entities through a series
    of meaningful relationships. These relationships generally represent semantic
    relationships between entities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们开始寻找一种能够尊重这种新型数据本质的技术。自然而然地，我们转向了图形表示。实际上，根据定义，图模型化了不同实体之间的关系。这种方法始于 2012
    年，当时 Google 开始为每个搜索的概念添加知识卡片。这些知识卡片可以被视为名称实体的图，其中连接是图链接。这些卡片随后允许进行更相关的搜索和用户便利化。随后，“知识图谱”一词开始指任何通过一系列有意义的连接连接实体的图。这些关系通常表示实体之间的语义关系。
- en: '![Figure 7.1 – Knowledge card in Google](img/B21257_07_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – Google 中的知识卡片](img/B21257_07_01.jpg)'
- en: Figure 7.1 – Knowledge card in Google
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – Google 中的知识卡片
- en: A formal definition of graphs and knowledge graphs
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图和知识图谱的正式定义
- en: 'Since KGs are a subtype of graphs, we will start with a brief introduction
    of graphs. Graphs are data structures composed of nodes (or vertices) that are
    connected by relationships (or edges) to represent a model of a domain. A graph
    can then represent knowledge in a compact manner while trying to reduce noise.
    There are different types of graphs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于知识图谱是图的一种子类型，我们将从图的简要介绍开始。图是由节点（或顶点）组成的数据结构，这些节点通过关系（或边）连接，以表示一个领域的模型。图可以以紧凑的方式表示知识，同时试图减少噪声。存在不同类型的图：
- en: '**Undirected**: Edges have no direction'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无向图**: 边没有方向'
- en: '**Directed**: Edges have a defined direction (there is a defined beginning
    and end)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有向图**: 边有定义的方向（有明确的开始和结束）'
- en: '**Weighted**: Edges carry weights, representing the strength or cost of the
    relationship'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权图**: 边带有权重，表示关系的强度或成本'
- en: '**Labeled**: Nodes are associated with features and labels'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记图**: 节点与特征和标签相关联'
- en: '**Multigraph**: Multiple edges (relationships) exist between the same pair
    of nodes'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多重图**: 同一对节点之间存在多个边（关系）'
- en: 'The following figure shows a visual representation of these graphs:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这些图的视觉表示：
- en: '![Figure 7.2 – Different types of graph architecture](img/B21257_07_02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 不同类型的图架构](img/B21257_07_02.jpg)'
- en: Figure 7.2 – Different types of graph architecture
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 不同类型的图架构
- en: 'A KG is thus a subgraph with three main properties:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，知识图谱是一个具有三个主要属性的子图：
- en: '**Nodes represent real-world entities**: These entities can represent people,
    places, or domain-specific entities (genes, proteins, diseases, financial products,
    and so on)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点代表现实世界实体**: 这些实体可以代表人、地点或特定领域的实体（基因、蛋白质、疾病、金融产品等）'
- en: '**Relationships define semantic connections between nodes**: For example, two
    people may be linked by a relationship that represents friendship, or a specific
    gene is associated with a particular disease'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系定义节点之间的语义连接**: 例如，两个人可能通过代表友谊的关系相连，或者一个特定的基因与某种特定疾病相关联'
- en: '**Nodes and edges may have associated properties**: For example, all people
    will have as a property that they are human beings (a label) but they can also
    have quantitative properties (date of birth, a specific identifier, and so on)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点和边可能具有关联属性**: 例如，所有人都会有一个属性，即他们是人类（一个标签），但他们也可以有定量属性（出生日期、特定标识符等）'
- en: So, a little more formally, we can say that we have a knowledge base (a database
    of facts) represented in the form of factual triplets. A factual triples has the
    form of `(head, relation, tail)` or `(subject, predicate, object)`, or more succinctly,
    `(e1,r1,e2)`, such as `(Napoleon, BornIn, Ajaccio)`. The KG is a representation
    of this knowledge base that allows us to conduct interpretation. Given the structure
    of these triplets, a KG is a directed graph where the nodes are these entities
    and the edges are factual relationships.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更正式地说，我们可以认为我们有一个以事实三元组形式表示的知识库（事实数据库）。事实三元组的形式为`(头，关系，尾)`或`(主体，谓词，对象)`，或者更简洁地，`(e1,r1,e2)`，例如`(拿破仑，出生地，阿雅克肖)`。知识图谱是这种知识库的表示，它允许我们进行解释。鉴于这些三元组的结构，知识图谱是一个有向图，其中节点是这些实体，边是事实关系。
- en: '![Figure 7.3 – Example of a knowledge base and knowledge graph (https://arxiv.org/pdf/2002.00388)](img/B21257_07_03.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 知识库和知识图谱的示例 (https://arxiv.org/pdf/2002.00388)](img/B21257_07_03.jpg)'
- en: Figure 7.3 – Example of a knowledge base and knowledge graph ([https://arxiv.org/pdf/2002.00388](https://arxiv.org/pdf/2002.00388))
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 知识库和知识图谱的示例 ([https://arxiv.org/pdf/2002.00388](https://arxiv.org/pdf/2002.00388))
- en: 'A KG is defined as a graph consisting of a set of entities, *E*, relations,
    *R*, and facts, *F*, where each fact *f* is a triplet:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱定义为由一组实体*E*、关系*R*和事实*F*组成的图，其中每个事实*f*是一个三元组：
- en: <mrow><mrow><mrow><mi mathvariant="script">G</mi><mo>=</mo><mfenced close="}"
    open="{"><mrow><mi mathvariant="script">E</mi><mo>,</mo><mi mathvariant="script">R</mi><mo>,</mo><mi
    mathvariant="script">F</mi></mrow></mfenced><mo>;</mo><mi>f</mi><mo>=</mo><mo>(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mrow></mrow>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mrow><mrow><mi mathvariant="script">G</mi><mo>=</mo><mfenced close="}"
    open="{"><mrow><mi mathvariant="script">E</mi><mo>,</mo><mi mathvariant="script">R</mi><mo>,</mo><mi
    mathvariant="script">F</mi></mrow></mfenced><mo>;</mo><mi>f</mi><mo>=</mo><mo>(</mo><mi>h</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mrow></mrow>
- en: As you can see, a KG is an alternative representation of knowledge. The same
    kind of data can be represented in either a table or a graph. We can directly
    create triplets from a table and then directly represent them in a KG. We do not
    need table headers, and it is easier to conduct the update of such a structure.
    Graphs are considered universal data representations because they can be applied
    to any type of data. In fact, not only can we map tabular data to a KG, but we
    can also get data from other formats (JSON, XML, CSV, and so on). Graphs also
    allow us to nimbly represent recursive structures (such as trees and documents)
    or cyclical structures (such as social networks). Also, if we do not have the
    information for all properties, the table representation will be full of missing
    values; in a KG, we do not have this problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，知识图谱是知识的另一种表示形式。相同类型的数据可以以表格或图的形式表示。我们可以直接从表格中创建三元组，然后直接在知识图谱中表示它们。我们不需要表头，并且更新这种结构更容易。图被认为是通用数据表示，因为它们可以应用于任何类型的数据。实际上，我们不仅可以将表格数据映射到知识图谱，还可以从其他格式（如JSON、XML、CSV等）获取数据。图还允许我们灵活地表示递归结构（如树和文档）或循环结构（如社交网络）。此外，如果我们没有所有属性的信息，表格表示将充满缺失值；而在知识图谱中，我们不会遇到这个问题。
- en: 'Graphs *per se* represent network structures. This is very useful for many
    business cases (e.g., in finance and medicine) where a lot of data is already
    structured as networks. Another advantage is that it is much easier to conduct
    a merge of graphs than of tables. Merging tables is usually a complicated task
    (where you have to choose which columns to merge, avoid creating duplicates, and
    other potential problems). If the data is in triplets, it is extremely easy to
    merge two KG databases. For example, look how simple it is to transform this table
    into a graph; they are equivalent:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图本身代表网络结构。这在许多业务案例（例如，在金融和医学领域）中非常有用，因为这些数据已经以网络的形式结构化。另一个优点是，合并图比合并表格要容易得多。合并表格通常是一个复杂的过程（你必须选择要合并的列，避免创建重复项，以及其他潜在问题）。如果数据以三元组形式存在，合并两个知识图谱数据库就极其简单。例如，看看将这个表格转换为图是多么简单；它们是等价的：
- en: '![Figure 7.4 – Table and graph are equivalent](img/B21257_07_04.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 表格和图表是等价的](img/B21257_07_04.jpg)'
- en: Figure 7.4 – Table and graph are equivalent
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 表格和图表是等价的
- en: 'A KG should not be seen as a static entity. In fact, knowledge evolves; this
    causes new entities or relationships to be added. One of the most widely used
    tasks in **knowledge graph reasoning** (**KGR**) is to predict new relationships.
    For example, if A is the husband of B and father of C, this implies that B is
    the mother of C, which could be derived with logical ruling: *(A, husband of,
    B) ^ (A, father of C) -> (B, mother of, C)*. In this pre-existing datum, we have
    inferred a missing relationship.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱不应被视为一个静态实体。实际上，知识是不断演变的；这会导致新的实体或关系被添加。在**知识图谱推理**（KGR）中最广泛使用的任务之一是预测新的关系。例如，如果A是B的丈夫和C的父亲，这暗示B是C的母亲，这可以通过逻辑推理得出：*(A,
    husband of, B) ^ (A, father of C) -> (B, mother of, C)*。在这个预存数据中，我们推断出一个缺失的关系。
- en: Another very important task is how to conduct the update of a KG once we have
    obtained new triplets (this may require complex preprocessing). This is very important
    because once we have integrated some new knowledge, we can conduct further analysis
    and further reasoning. Also, being a graph, we can use graph analysis algorithms
    (such as centrality measures, connectivity, clustering, and so on) for our business
    cases. Leveraging these algorithms makes it much easier to conduct searches or
    complex queries in a KG than in relational databases.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的任务是，一旦我们获得了新的三元组（这可能需要复杂的预处理），如何更新知识图谱。这非常重要，因为一旦我们整合了一些新的知识，我们就可以进行进一步的分析和推理。此外，作为一个图，我们可以使用图分析算法（如中心性度量、连通性、聚类等）来处理我们的业务案例。利用这些算法使得在知识图谱中进行搜索或复杂查询比在关系数据库中要容易得多。
- en: '![Figure 7.5 – A KG is a dynamic entity](img/B21257_07_05.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 知识图谱是一个动态实体](img/B21257_07_05.jpg)'
- en: Figure 7.5 – A KG is a dynamic entity
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 知识图谱是一个动态实体
- en: 'In addition, KGs are much more flexible and adaptable than people think. KGs
    can be adapted for different tasks. For example, there are extensions of KGs such
    as **hierarchical KGs** where we have multiple levels. In hierarchical KGs, entities
    from one level can be connected to the next level (this, for example, is very
    useful when we have ontologies). Entities can also be **multimodal**, so a node
    can represent an image to which other entities (textual or other images, or other
    types of modalities) are connected. Another type of KG is a **temporal KG**, in
    which we incorporate a temporal dimension. This type of KG can be very useful
    for predictive analysis. We can see these KGs in the following figure:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，知识图谱比人们想象的要灵活和适应性强得多。知识图谱可以适应不同的任务。例如，有知识图谱的扩展，如**层次知识图谱**，其中我们有多级。在层次知识图谱中，一个级别的实体可以连接到下一个级别（例如，当我们有本体时，这非常有用）。实体也可以是**多模态的**，因此一个节点可以代表一个图像，其他实体（文本或其他图像，或其他类型的模态）与之相连。另一种类型的知识图谱是**时序知识图谱**，其中我们引入了时间维度。这种类型的知识图谱对于预测分析非常有用。我们可以在以下图中看到这些知识图谱：
- en: '![Figure 7.6 – Different types of knowledge graphs](img/B21257_07_06.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 不同类型的知识图谱](img/B21257_07_06.jpg)'
- en: Figure 7.6 – Different types of knowledge graphs
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 不同类型的知识图谱
- en: Taxonomies and ontologies
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类法和本体
- en: 'The main difference between a graph and a KG is that the former is a given
    structure representing relationships between entities, while the latter makes
    semantic relationships explicit by allowing reasoning and inference to humans
    and machines. So, the advantage of a KG is that we can use algorithms for both
    graphs and specific reasoning algorithms (we will see later, in the *Understanding
    graph reasoning* section, some approaches in detail). These capabilities are enhanced
    by incorporating metadata. Indeed, we can construct `dog` and `cat` entities might
    be grouped under `mammals`). In addition, multiple taxonomies can be integrated
    if necessary (thus having multiple trees to allow for more refined searching).
    These taxonomies help in searching or when we need to filter and work with very
    large KGs. `maximum_speed` property of `100` km, so Bob will not be able to get
    there in less than an hour because his job has a `distance_from_home` property
    of `120` km). Rules allow us to be able to improve search and solve tasks that
    were too complex before (for example, we can assign different properties to relations:
    if `married_to` is transitive, we can automatically infer information about a
    person without the relation being specified). Thanks to ontologies, we can conduct
    certain types of reasoning effectively and quickly, such as deductive reasoning,
    class inference, transitive reasoning, and so on.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图和KG之间的主要区别在于，前者是一个给定的结构，表示实体之间的关系，而后者通过允许推理和推理使语义关系明确，对人类和机器都是如此。因此，KG的优势在于我们可以使用图算法和特定的推理算法（我们将在“理解图推理”部分中稍后详细说明）。通过整合元数据，这些能力得到了增强。实际上，我们可以构建`dog`和`cat`实体可能被分组在`mammals`下）。此外，如果需要，可以整合多个分类法（因此拥有多个树，以便进行更精细的搜索）。这些分类法有助于搜索或在我们需要过滤和操作非常大的KG时。例如，`maximum_speed`属性为`100`
    km，因此鲍勃将无法在不到一个小时的时间内到达那里，因为他的工作有一个`distance_from_home`属性为`120` km）。规则使我们能够改进搜索并解决以前过于复杂的任务（例如，我们可以为关系分配不同的属性：如果`married_to`是传递性的，我们可以在没有指定关系的情况下自动推断有关某人的信息）。多亏了本体，我们可以有效地快速进行某些类型的推理，例如演绎推理、类推理、传递推理等。
- en: 'Ontologies are generally grouped into two groups:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本体通常分为两组：
- en: '**Domain-independent ontologies**: Ontologies that provide fundamental concepts
    that are not tied to a particular domain. They provide a high-level view that
    helps with data integration, especially when there are several domains. Commonly,
    these are a small number, they represent the first level, and they are the first
    ones built.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立于领域的本体**：这些本体提供的是与特定领域无关的基本概念。它们提供了一个高级视图，有助于数据集成，尤其是在存在多个领域的情况下。通常，这些本体数量较少，它们代表第一级，并且是首先构建的。'
- en: '**Domain ontologies**: These are focused on a domain and are used to provide
    the fundamental terminology. They are most useful for specialized domains such
    as medicine and finance. They are usually found at levels below the domain-independent
    one and are a subclass of it.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域本体**：这些本体专注于一个领域，用于提供基本术语。它们对于医学和金融等专业化领域非常有用。它们通常位于独立于领域的本体之下，并且是它的子类。'
- en: In this section, we have seen how KGs are flexible systems that can store data
    and be able to easily find knowledge. This flexibility makes them powerful tools
    for subsequent analysis, but at the same time, does not make them easy to build.
    In the next section, we will see how we can build a KG from a collection of texts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了KG是如何作为灵活的系统来存储数据并能轻松找到知识的。这种灵活性使它们成为后续分析的有力工具，但同时也使得构建它们并不容易。在下一节中，我们将看到如何从一组文本中构建KG。
- en: Creating a knowledge graph with your LLM
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用你的大型语言模型（LLM）创建知识图谱
- en: 'The construction of a KG is generally a multistep process consisting of the
    following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: KG的构建通常是一个多步骤的过程，包括以下步骤：
- en: '**Knowledge creation**: The first step, in which we define the purpose of this
    KG, is to gather the sources from which to extract knowledge. In this step, we
    have to decide how we build our KG but also where we maintain it. Once built,
    the KG has to be stored, and we have to have an efficient structure to query it.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识创造**：第一步，我们定义这个知识图谱（KG）的目的，即收集提取知识的来源。在这个步骤中，我们必须决定如何构建我们的KG以及在哪里维护它。一旦构建完成，KG就必须被存储，并且我们必须有一个高效的查询结构。'
- en: '**Knowledge assessment**: In this step, we assess the quality of the KG obtained.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识评估**：在这个步骤中，我们评估获得的KG的质量。'
- en: '**Knowledge cleaning**: There are several steps and procedures to make sure
    there are no errors and then correct them. This step can be conducted at the same
    time as knowledge assessment, and some pipelines conduct them together.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识清洗**：有多个步骤和程序来确保没有错误，然后进行纠正。这一步可以与知识评估同时进行，并且一些流程将它们一起进行。'
- en: '**Knowledge enrichment**: This involves a series of steps to identify whether
    there are gaps in knowledge. We can also integrate additional sources (extract
    information from other datasets, integrate databases, or merge multiple KGs).'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识增强**：这涉及一系列步骤来识别知识中是否存在空白。我们还可以整合额外的来源（从其他数据集中提取信息、整合数据库或合并多个知识图谱）。'
- en: '**Knowledge deployment**: In this final step, the KG is deployed either as
    a standalone application (e.g., as a graph database) or used within another application.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识部署**：在这一最终步骤中，知识图谱可以作为独立的应用程序（例如，作为图数据库）部署，或者用于其他应用程序中。'
- en: 'We can see the process in the following figure:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图中看到这个过程：
- en: '![Figure 7.7 – KG construction pipeline](img/B21257_07_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 知识图谱构建流程](img/B21257_07_07.jpg)'
- en: Figure 7.7 – KG construction pipeline
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 知识图谱构建流程
- en: Knowledge creation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识创建
- en: 'In general, when building a KG from scratch, the definition of ontologies is
    the first step. There are several guides on how to build them (both libraries
    and tools to visualize them). Efforts are made to build ontologies that are clear,
    verifiable, and reusable. Defining ontologies should be done with a purpose in
    mind, discussing what the purpose of a KG is with various stakeholders, and then
    defining ontologies accordingly. The most relevant ones should be chosen (the
    first level of the KG), following which the hierarchy and properties should be
    defined. There are two approaches: top-down (define core ontologies first and
    then more specialized ones) or bottom-up (define specialized ontologies and then
    group them into superclasses). Especially for specialized domains, we could start
    from ontologies that have already been built (there are several defined for finance,
    medicine, and academic research) and this ensures better interoperability.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从头开始构建知识图谱时，本体定义是第一步。有关于如何构建它们的几个指南（包括库和可视化工具）。努力构建清晰、可验证和可重用的本体。定义本体时应有一个目的，与各种利益相关者讨论知识图谱的目的，然后相应地定义本体。最相关的本体应该被选择（知识图谱的第一级），然后定义层次结构和属性。有两种方法：自上而下（首先定义核心本体，然后是更专业的本体）或自下而上（首先定义专业本体，然后将它们分组到超类中）。特别是对于专业领域，我们可以从已经构建的本体开始（有几种是为金融、医学和学术研究定义的），这确保了更好的互操作性。
- en: 'The next step is to extract knowledge from our sources. In this step, we have
    to extract triplets (or a set of facts) from a text corpus or another source (a
    database, or structured and unstructured data). Two tasks can be defined:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从我们的来源中提取知识。在这个步骤中，我们必须从文本语料库或其他来源（数据库、结构化和非结构化数据）中提取三元组（或一组事实）。可以定义两个任务：
- en: '**Named entity recognition** (**NER**): NER is the task of extracting entities
    from text and classifying them'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别（NER**）：NER是从文本中提取实体并将它们分类的任务'
- en: '**Relation extraction** (**RE**): RE is the task of identifying connections
    between various entities in a context'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系抽取（RE**）：RE是识别上下文中各种实体之间联系的任务'
- en: NER is one of the most common tasks in **natural language processing** (**NLP**)
    and is used not only for KG creation but also as a key step when we want to move
    from unstructured text to structured data. It generally requires a pipeline consisting
    of several steps (text preprocessing, entity identification and classification,
    contextual analysis, and data post-processing). During NER, we try to identify
    entities by first conducting a preprocessing step to avoid errors in the pipeline
    (e.g., proper tokenization). Once entities are identified, they are usually classified
    (e.g., by adding a label such as people, organizations, or places). In addition,
    surrounding text is attempted to be used to disambiguate them (e.g., trying to
    recognize whether *Apple* in the text represents the fruit or the company). A
    preprocessing step is then conducted to resolve ambiguities or merge multi-token
    entities.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: NER是自然语言处理（NLP）中最常见的任务之一，不仅用于知识图谱的创建，而且在我们想要从非结构化文本转换为结构化数据时也是一个关键步骤。它通常需要一个由多个步骤组成的管道（文本预处理、实体识别和分类、上下文分析以及数据后处理）。在NER过程中，我们首先进行预处理步骤以避免管道中的错误（例如，正确的标记化），然后尝试识别实体。一旦识别出实体，它们通常会被分类（例如，通过添加如人物、组织或地点等标签）。此外，还尝试使用周围文本来消除歧义（例如，试图识别文本中的*Apple*代表的是水果还是公司）。然后进行预处理步骤以解决歧义或合并多标记实体。
- en: '![Figure 7.8 – Example of NER (https://arxiv.org/pdf/2401.10825)](img/B21257_07_08.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – NER示例 (https://arxiv.org/pdf/2401.10825)](img/B21257_07_08.jpg)'
- en: Figure 7.8 – Example of NER ([https://arxiv.org/pdf/2401.10825](https://arxiv.org/pdf/2401.10825))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – NER示例 ([https://arxiv.org/pdf/2401.10825](https://arxiv.org/pdf/2401.10825))
- en: RE is the task in which we understand the relationships between the various
    extracted entities. More formally, we use a model to identify and categorize the
    connections between entities in a text (e.g., in the sentence *Bob works at Apple*,
    we extract the relationship *works at*). It can be considered a separate task
    or, in some cases, conducted together with NER (e.g., with a single model). Also,
    RE is a key step for KG creation and, at the same time, useful for several other
    NLP tasks (such as question answering, information retrieval, and so on).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RE是我们理解各种提取实体之间关系的任务。更正式地说，我们使用一个模型来识别和分类文本中实体之间的连接（例如，在句子*Bob works at Apple*中，我们提取关系*works
    at*）。它可以被视为一个单独的任务，或者在某些情况下与NER一起进行（例如，使用单个模型）。此外，RE是知识图谱创建的关键步骤，同时对于其他几个NLP任务（如问答、信息检索等）也很有用。
- en: There are several methods to be able to conduct NER and RE. The earliest and
    most laborious methods were knowledge-based or rule-based. For example, one of
    the simplest approaches to identifying company names in financial documents was
    to use indicators such as capital letters (identify `Mr.` and `Ms.` elements to
    extract people, and so on). Rule-based worked very well for standardized documents
    (such as clinical notes or official documents) but demonstrated little scalability.
    These methods require establishing laborious upstream rules and specific knowledge,
    risking missing many entities in different datasets.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以进行NER和RE。最早且最费力的方法是基于知识或基于规则的。例如，在财务文件中识别公司名称的最简单方法之一是使用诸如大写字母等指标（识别`Mr.`和`Ms.`元素以提取人物，等等）。基于规则的方法在标准化文档（如临床笔记或官方文件）中效果很好，但可扩展性较差。这些方法需要建立繁琐的上游规则和特定知识，风险是错过不同数据集中许多实体。
- en: Statistical methods based on the hidden Markov model, conditional random fields,
    or maximum entropy (methods that rely on predicting the entity based on likelihood
    learned from training data) have allowed for greater scalability. These methods,
    though, require large, quality datasets that have defined labels. Other supervised
    learning algorithms have been used to predict entities and then extract them.
    These algorithms have worked well with high computational costs and especially
    the need for labels. Obtaining labels is expensive and these datasets quickly
    become outdated (new companies, new products, and so on are created).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基于隐马尔可夫模型、条件随机字段或最大熵（基于从训练数据中学习到的可能性预测实体的方法）的统计方法，使得更大的可扩展性成为可能。然而，这些方法需要大量、高质量的已定义标签的数据集。其他监督学习算法已被用于预测实体然后提取它们。这些算法在高计算成本和特别需要标签的情况下表现良好。获取标签是昂贵的，这些数据集很快就会过时（新公司、新产品等不断出现）。
- en: Recently, given the advances in unsupervised learning (models such as the transformer),
    it has been decided to use LLMs also for NER and RE and for constructing KGs (in
    some studies, these are called **LLM-augmented KGs**). The ability to process
    large corpora of text, the knowledge acquired during pre-training, and their versatility
    make LLMs useful for the construction of KGs (and other related tasks that we
    will see later).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，鉴于无监督学习（如transformer模型）的进步，决定使用LLM进行NER和RE以及构建KG（在某些研究中，这些被称为**LLM增强的KG**）。处理大量文本语料库的能力、在预训练期间获得的知识以及它们的通用性使LLM在构建KG（以及我们稍后将看到的其他相关任务）中变得非常有用。
- en: Due to their ability to leverage contextual information and linguistic abilities,
    state-of-the-art methods generally employ transformer-based models for NER tasks.
    Previous methods had problems with texts that had complex structures (a token
    that belongs to several entities, or entities that are not contiguous in the text)
    while transformers are superior in solving these cases. BERT-based models were
    previously used, which were then later fine-tuned for different tasks. Today,
    however, we exploit the capabilities of an LLM that does not need to be fine-tuned
    but can learn a task without training through in-context learning. An LLM can
    then directly extract entities from text without the need for labels and provide
    them in the desired format (for example, we might want the LLM to provide a list
    of triplets or the triplet plus a given label). To avoid disambiguation problems,
    we can ask the LLM to provide additional information when conducting the extraction.
    For example, in music, *Apple* can refer to Apple Music, the British psychedelic
    rock band Apple, or the singer Fiona Apple. LLMs can help us disambiguate which
    of these entities it refers to based on the context of the period. At the same
    time, the flexibility of LLMs allows us to tie entities to various ontologies
    during extraction.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们能够利用上下文信息和语言能力，最先进的方法通常使用基于transformer的模型进行NER任务。以前的方法在处理具有复杂结构的文本（属于多个实体的标记，或文本中不连续的实体）时存在问题，而transformer在这些情况下表现更优。以前曾使用基于BERT的模型，这些模型后来被微调用于不同的任务。然而，如今，我们利用LLM的能力，它不需要微调，但可以通过上下文学习来学习任务。然后LLM可以直接从文本中提取实体，无需标签，并以所需格式提供它们（例如，我们可能希望LLM提供三元组列表或三元组加上给定的标签）。为了避免歧义问题，我们可以要求LLM在提取时提供额外的信息。例如，在音乐领域，*Apple*可以指苹果音乐、英国迷幻摇滚乐队Apple或歌手Fiona
    Apple。LLM可以帮助我们根据时期上下文区分这些实体中它指的是哪一个。同时，LLM的灵活性允许我们在提取过程中将实体与各种本体联系起来。
- en: Similarly, an LLM can help with the RE task. There are several ways to do this.
    One of the simplest is to conduct sentence-level RE, in which you provide the
    model with a sentence and it must extract the relationship between the two entities.
    The extension of this approach is to extract all the relationships between entities
    at the level of an entire document. Since this is not an easy task, more sophisticated
    approaches with more than one LLM can be used to make sure that we can understand
    relationships at the local and global levels of the document (for example, in
    a document, a local relationship between two entities is in the same sentence,
    but we can also have global relationships where an entity mentioned at the beginning
    of the document is related to an entity that is present at the end of the document).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，LLM也可以帮助进行RE任务。有几种方法可以实现这一点。其中最简单的一种是进行句子级别的RE，即你向模型提供一个句子，它必须提取两个实体之间的关系。这种方法的扩展是在整个文档级别提取实体之间的所有关系。由于这并非易事，因此可以使用更复杂的、包含多个LLM的方法来确保我们能够理解文档的局部和全局关系（例如，在文档中，两个实体之间的局部关系位于同一句子中，但我们也可以有全局关系，其中一个实体位于文档的开头，而另一个实体位于文档的结尾）。
- en: '![Figure 7.9 – General framework of LLM-based KG construction (This information
    was taken from an article published in 2023; https://arxiv.org/pdf/2306.08302)](img/B21257_07_09.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 基于LLM的知识图谱构建的一般框架（此信息来源于2023年发表的一篇文章；https://arxiv.org/pdf/2306.08302)](img/B21257_07_09.jpg)'
- en: Figure 7.9 – General framework of LLM-based KG construction (This information
    was taken from an article published in 2023; [https://arxiv.org/pdf/2306.08302](https://arxiv.org/pdf/2306.08302))
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 基于LLM的知识图谱构建的一般框架（此信息来源于2023年发表的一篇文章；[https://arxiv.org/pdf/2306.08302](https://arxiv.org/pdf/2306.08302))
- en: As mentioned, the two tasks need not be conducted separately (NER and RE), but
    an LLM provides the flexibility to conduct them in a single step. In this case,
    it is of great importance to define the right prompt in which we instruct the
    model in extracting entities and relationships and in which format we want the
    output. We can then proceed iteratively, extracting entities and relationships
    for a large body of text. Alternatively, we can use a set of prompts for different
    tasks (one for entity extraction, one for relation extraction, and so on) and
    scan the corpus and these prompts automatically with the LLM. In some approaches,
    to maintain more flexibility, one LLM is used for extraction and then a smaller
    LLM is used for correction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，两个任务不需要分别进行（NER 和 RE），但 LLM 提供了在单步中执行它们的灵活性。在这种情况下，定义正确的提示非常重要，其中我们指导模型提取实体和关系，并指定我们想要的输出格式。然后我们可以迭代地进行，从大量文本中提取实体和关系。或者，我们可以使用一组针对不同任务的提示（一个用于实体提取，一个用于关系提取等）并使用
    LLM 自动扫描语料库和这些提示。在某些方法中，为了保持更大的灵活性，使用一个 LLM 进行提取，然后使用一个更小的 LLM 进行校对。
- en: Another interesting perspective is that an LLM is enough to create a KG. In
    fact, LLMs are trained with a huge amount of text (the latest LLMs are trained
    with trillions of tokens that include scraping the internet and thousands of books).
    Several studies today show that even small LLMs (around 7 billion parameters)
    have considerable knowledge, especially about facts (the definition of knowledge
    in an LLM is also complicated because this is not associated with a single parameter
    but widespread). Therefore, some authors have proposed distilling knowledge directly
    from the LLM. In this case, by exploiting prompts constructed for the task, we
    conduct what is called a **knowledge search** of the LLM to extract triplets.
    In this way, by extracting facts directly from the LLM, we can then directly construct
    our KG. KGs constructed in this way are competitive in quality, diversity, and
    novelty with those constructed with large text datasets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的视角是，一个 LLM 就足以创建一个 KG。实际上，LLMs 是用大量文本训练的（最新的 LLMs 使用了包含从互联网抓取和数千本书的万亿个标记进行训练）。今天，有几项研究表明，即使是小型
    LLMs（大约 70 亿个参数）也具有相当的知识，特别是关于事实（在 LLM 中的知识定义也很复杂，因为它不是与单个参数相关联，而是广泛分布的）。因此，一些作者提出了直接从
    LLM 中蒸馏知识。在这种情况下，通过利用为任务构建的提示，我们进行所谓的 LLM **知识搜索**以提取三元组。通过这种方式，直接从 LLM 中提取事实，然后我们可以直接构建我们的
    KG。以这种方式构建的 KG 在质量、多样性和新颖性方面与使用大型文本数据集构建的 KG 具有竞争力。
- en: '![Figure 7.10 – General framework of distilling KGs from LLMs (https://arxiv.org/pdf/2306.08302)](img/B21257_07_10.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 从 LLM 中蒸馏 KG 的一般框架 (https://arxiv.org/pdf/2306.08302)](img/B21257_07_10.jpg)'
- en: Figure 7.10 – General framework of distilling KGs from LLMs ([https://arxiv.org/pdf/2306.08302](https://arxiv.org/pdf/2306.08302))
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 从 LLM 中蒸馏 KG 的一般框架 ([https://arxiv.org/pdf/2306.08302](https://arxiv.org/pdf/2306.08302))
- en: Creating a knowledge graph with an LLM
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LLM 创建知识图谱
- en: 'In this tutorial, I will use Neo4j and LangChain to create a KG with an LLM.
    LangChain allows us to use LLMs efficiently to extract information from a text
    corpus, while Neo4j is a program for analyzing and visualizing graphs. The complete
    code is in the book’s GitHub repository ([https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7));
    here, we will describe the general process and the most important code snippets.
    We can have two methods:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我将使用 Neo4j 和 LangChain 创建一个由 LLM 构建的 KG。LangChain 允许我们高效地使用 LLM 从文本语料库中提取信息，而
    Neo4j 是一个用于分析和可视化图的程序。完整的代码在本书的 GitHub 仓库中（[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr7)）；在这里，我们将描述一般过程和最重要的代码片段。我们可以有两种方法：
- en: '**Custom method**: LLMs have innate abilities to be able to accomplish tasks;
    we can take advantage of these generalist abilities'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义方法**：LLMs 天生具备完成任务的能力；我们可以利用这些通用能力'
- en: '**LangChain graph transformers**: Today, there are libraries that make the
    job easier and allow just a few lines of code to achieve the same result'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LangChain 图转换器**：如今，有库可以简化工作，只需几行代码就能达到相同的效果'
- en: 'The custom method is simply to define a prompt that allows the model to understand
    the task and execute it efficiently. In this case, our prompt is structured with
    the following elements:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义方法简单来说就是定义一个提示，让模型能够理解任务并高效执行。在这种情况下，我们的提示由以下元素构成：
- en: A clear definition of the task with a set of bullet points. The task description
    can contain both what the model must do and what it must not do.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个清晰的带有项目符号的任务定义。任务描述可以包含模型必须做什么以及不能做什么。
- en: Additional context that allows the model to better understand how to perform
    the task. Since these models are trained for dialogic tasks, providing them with
    information about what role they should play helps the performance.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供额外的上下文，帮助模型更好地理解如何执行任务。由于这些模型是针对对话任务训练的，提供它们关于应该扮演什么角色的信息有助于提高性能。
- en: Some examples to explain how to perform the task.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些示例来解释如何执行任务。
- en: 'This approach builds on what we learned in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042).
    The model we are using is instruction-tuned (trained to perform tasks) so providing
    clear instructions helps the model understand the task and perform it. The addition
    of some examples leverages in-context learning. Using a crafted prompt allows
    us to be flexible and be able to adapt the prompt to our needs:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法建立在我们在 [*第 3 章*](B21257_03.xhtml#_idTextAnchor042) 中学到的知识之上。我们使用的模型是指令微调的（训练用于执行任务），因此提供清晰的指令有助于模型理解任务并执行它。添加一些示例利用了情境学习。使用精心制作的提示使我们能够灵活地调整提示以适应我们的需求：
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Executing the preceding should yield the following result:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码应该产生以下结果：
- en: '![Figure 7.11 – Screenshot of the results](img/B21257_07_11.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 结果截图](img/B21257_07_11.jpg)'
- en: Figure 7.11 – Screenshot of the results
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 结果截图
- en: The results show how a crafted prompt succeeds in generating triplets (which
    we can then use to construct our KG). This highlights the great flexibility of
    LLMs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，精心制作的提示如何成功生成三元组（然后我们可以使用这些三元组来构建我们的 KG）。这突出了 LLM 的巨大灵活性。
- en: 'We do not always want a custom approach but might want to use a more established
    pipeline. LangChain provides the ability to do this with just a few lines of code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不总是想要一个定制的解决方案，但可能想要使用一个更成熟的流程。LangChain 只需几行代码就能提供这种能力：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: LangChain gives us the same result in an already structured format that simplifies
    our work.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 以一种已经结构化的格式给我们相同的成果，这简化了我们的工作。
- en: The graph can then be visualized in Neo4j and we can work on the graph, conduct
    searches, select nodes, and so on.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图形可以在 Neo4j 中进行可视化，我们可以在图上工作，进行搜索，选择节点等操作。
- en: '![Figure 7.12 – Screenshot of the graph from Neo4j](img/B21257_07_12.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 从 Neo4j 中获取的图形截图](img/B21257_07_12.jpg)'
- en: Figure 7.12 – Screenshot of the graph from Neo4j
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 从 Neo4j 中获取的图形截图
- en: 'Once the graph is generated, we can use it for our queries. Obviously, we can
    conduct these queries in Neo4j, but it is also possible to do it in Python. For
    example, LangChain allows us to conduct queries of our KG:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成了图形，我们就可以用它来进行查询。显然，我们可以在 Neo4j 中进行这些查询，但也可以在 Python 中进行。例如，LangChain 允许我们对我们的
    KG 进行查询：
- en: '[PRE2]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once executed the code, you should obtain these results:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，你应该获得以下结果：
- en: '![Figure 7.13 – Querying the KG](img/B21257_07_13.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 查询 KG](img/B21257_07_13.jpg)'
- en: Figure 7.13 – Querying the KG
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 查询 KG
- en: As we can see, LangChain in this case generates a corresponding query in Cypher
    and then conducts the graph query. In this way, we are using an LLM to generate
    a query in Cypher, while we can write directly in natural language.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，LangChain 在这种情况下生成相应的 Cypher 查询，然后执行图查询。这样，我们使用一个 LLM 在 Cypher 中生成查询，同时我们也可以直接用自然语言编写。
- en: Knowledge assessment
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识评估
- en: 'Once the KG has been created, it is necessary to check for errors and the overall
    quality of the KG. The quality of a KG is assessed by a set of dimensions (there
    are metrics associated with each of these dimensions) that are used to monitor
    the KG in terms of accessibility, representation, context, and intrinsic quality.
    Some of the metrics are as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦知识图谱（KG）创建完成，就需要检查错误以及 KG 的整体质量。KG 的质量是通过一系列维度（每个维度都有相应的指标）来评估的，这些维度用于监控 KG
    在可访问性、表示、上下文和内在质量方面的表现。以下是一些指标：
- en: '**Accuracy**: This metric assesses accuracy in syntactic and semantic terms.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：这个指标从语法和语义的角度评估准确性。'
- en: '**Completeness**: This metric measures how much knowledge a KG contains with
    respect to a certain domain or task. It usually measures whether a KG contains
    all the necessary entities and relationships for a domain (sometimes a comparison
    with a golden standard KG is used).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**：这个指标衡量 KG 在某个领域或任务中的知识量。它通常衡量 KG 是否包含一个领域所需的所有实体和关系（有时会与黄金标准 KG 进行比较）。'
- en: '**Conciseness**: KGs allow knowledge to be expressed efficiently, but they
    risk scaling quickly. Blank nodes (specific types of nodes that represent anonymous
    or unnamed entities, used when a node is needed in the graph but is not precisely
    indicated) can often be generated during the creation process. If care is not
    taken, one risks filling the KG with blank nodes.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁性**：知识图谱允许知识以高效的方式表达，但它们也面临着快速扩展的风险。空白节点（代表匿名或未命名的实体的一种特定类型的节点，在图中需要节点但未精确指示时使用）在创建过程中经常被生成。如果不加注意，就有可能使知识图谱充满空白节点。'
- en: '**Timeliness**: Knowledge should also be updated regularly because it can change
    and become outdated. Therefore, it is important to decide the frequency of updates.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时效性**：知识也应定期更新，因为知识可能会变化并变得过时。因此，决定更新的频率很重要。'
- en: '**Accessibility, ease of manipulation, and operation**: KGs are used for searches
    or other tasks; metrics exist today that measure the usefulness of KGs. In fact,
    for a KG to be useful, it must be easily accessible, be able to be manipulated,
    and be able to conduct research and updates.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性、操作简便性和操作性**：知识图谱（KGs）用于搜索或其他任务；目前存在一些度量标准来衡量知识图谱的有用性。实际上，对于一个知识图谱来说，要变得有用，它必须易于访问、能够被操作，并且能够进行研究和更新。'
- en: '**Ease of understanding**: Since the KG is meant to be used to represent knowledge
    for humans, some authors have proposed measuring the degree to which the KG is
    interpretable for humans. Indeed, today, there is a greater emphasis on transparency
    and interpretation of models in AI.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于理解**：由于知识图谱旨在用于代表人类的知识，一些作者提出了衡量知识图谱对人类可解释程度的方法。确实，今天，在人工智能中，对模型的透明度和可解释性的重视程度更高。'
- en: '**Security, privacy, and traceability**: Metrics also exist today to control
    who accesses the KG and whether it is secure from outside access. Similarly, knowledge
    also needs to be tracked, as we need to be sure which sources it comes from. Traceability
    also allows us to be in privacy compliance. For example, our KG may contain sensitive
    data about users or come from erroneous or problematic documents. Traceability
    allows us to correct these errors, delete data from users who require their data
    to be deleted, and so on.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性、隐私性和可追溯性**：目前也存在一些度量标准来控制谁可以访问知识图谱以及它是否能够抵御外部访问。同样，知识也需要被追踪，因为我们需要确保它来自哪个来源。可追溯性还允许我们遵守隐私法规。例如，我们的知识图谱可能包含关于用户的敏感数据，或者来自错误或有问题的文档。可追溯性允许我们纠正这些错误，删除需要删除其数据的用户的数据，等等。'
- en: Knowledge cleaning
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识清洗
- en: 'Having assessed the quality of our KG, we can see that there are errors. In
    general, error detection and correction are together called **knowledge cleaning**.
    Different types of errors can occur in a KG:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 评估了我们的知识图谱的质量后，我们可以看到其中存在错误。一般来说，错误检测和纠正统称为**知识清洗**。知识图谱中可能发生不同类型的错误：
- en: We may have entities or relationships that have syntactic errors
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能存在语法错误的实体或关系
- en: Some errors may be ontology-related (assigning to ontologies that do not exist,
    connecting them to the wrong ontologies, wrong properties of ontologies, and so
    on)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些错误可能与本体相关（分配给不存在的本体、将它们连接到错误的本体、本体的错误属性等等）
- en: Some may be semantic and may be more difficult to identify
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些可能是语义的，可能更难识别
- en: There are also knowledge errors that may result from errors in the sources for
    creating the knowledge (symptom *x* is not a symptom of disease *y*, person *x*
    is not the CEO of company *y*, and so on)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识来源的错误也可能导致知识错误（症状 *x* 不是疾病 *y* 的症状，人 *x* 不是公司 *y* 的首席执行官等等）
- en: There are several methods to detect these errors. The simplest methods are statistical
    and seek to identify outliers in a KG by exploiting probabilistic and statistical
    modeling. There are also more sophisticated variations that exploit simple machine-learning
    models. These models are not particularly accurate. Since we can use logical reasoning
    and ontologies with KGs, there are knowledge-based reasoning methods to identify
    outliers (e.g., an instance of a person cannot also be an instance of a place,
    so by exploiting similar rules, we can identify outliers). Finally, there are
    methods based on AI, and one can also use an LLM to check for errors. An LLM possesses
    both knowledge and reasoning skills so it can be used to verify that facts are
    correct. For example, if for some error, we have the triplet `(Vienna, CapitalOf,
    Hungary)`, an LLM can identify the error). Then, there are similar methods for
    conducting KG correction. However, several frameworks have already been built
    and established to conduct detection and correction.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以检测这些错误。最简单的方法是统计方法，通过利用概率和统计建模来识别KG中的异常值。也有更复杂的变化，利用简单的机器学习模型。这些模型并不特别准确。由于我们可以使用逻辑推理和本体与KG，因此有基于知识的推理方法来识别异常值（例如，一个人的实例不能也是地点的实例，因此通过利用类似的规则，我们可以识别异常值）。最后，有基于AI的方法，也可以使用一个LLM来检查错误。LLM拥有知识和推理技能，因此可以用来验证事实是否正确。例如，如果我们对于某个错误，我们有三元组
    `(Vienna, CapitalOf, Hungary)`，LLM可以识别这个错误）。然后，还有类似的KG修正方法。然而，已经建立了几个框架来进行检测和修正。
- en: Knowledge enrichment
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识丰富化
- en: '**Knowledge enrichment** (or KG completion) is generally the next step. KGs
    are notoriously incomplete, so several rounds of KG completion and correction
    can occur. The completeness of a KG is sometimes complicated to define and is
    contextual to the domain and application tasks. The first step in completing a
    KG is usually to identify additional sources of information (e.g., for a medical
    KG, it could be an additional biomedical database or an additional corpus of scientific
    articles). Often, in the first step of building the KG, we use only one data type
    (unstructured text) and then extend the extraction in a second step to other data
    types (CSV, XML, JSON, images, PDF, and so on). Each of these data types presents
    different challenges, so we should modify our pipeline. The more sources we use,
    the more crucial KG cleaning and alignment tasks become. For example, the more
    heterogeneous the sources, the greater the importance of entity resolution (identifying
    duplicate entities at the KG level).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识丰富化**（或KG补全）通常是下一步。KGs通常是不完整的，因此可能需要进行几轮KG补全和修正。KG的完整性有时很难定义，并且与领域和应用任务相关。完成KG的第一步通常是识别额外的信息来源（例如，对于一个医学KG，这可能是一个额外的生物医学数据库或额外的科学文章语料库）。通常，在构建KG的第一步中，我们只使用一种数据类型（非结构化文本），然后在第二步中扩展提取到其他数据类型（CSV、XML、JSON、图像、PDF等）。每种数据类型都提出了不同的挑战，因此我们应该修改我们的管道。我们使用的来源越多，KG清理和对齐任务就越重要。例如，来源越异构，实体解析（在KG级别识别重复实体）的重要性就越大。'
- en: 'An interesting alternative is to infer knowledge using an LLM (or other transformer
    models). For example, three possible approaches have been explored:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的替代方案是使用一个LLM（或其他transformer模型）来推断知识。例如，已经探索了三种可能的方法：
- en: '`(h,r,t)` is given to a model as a transformer model to predict the probability
    that it exists (`0` represents that the triplet is invalid, while `1` is a valid
    triplet). A variation of this approach is to take the final hidden state from
    the model and train a linear classifier to predict in a binary fashion whether
    the triplet is valid or not.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(h,r,t)` 被作为一个transformer模型输入以预测其存在的概率（`0` 表示三元组无效，而 `1` 是一个有效的三元组）。这种方法的变体是从模型中提取最终隐藏状态并训练一个线性分类器以二进制方式预测三元组是否有效。'
- en: '`(h,r,?)`, we can try to complete the gap.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(h,r,?)`，我们可以尝试填补这个空缺。'
- en: '`(h,r)` and `(t)`. In this way, we get two representations from the model (the
    final hidden state of the model is used). After that, we use a scoring function
    to predict whether this triplet is valid. This approach is definitely more accurate
    but risks a combinatorial explosion. As you can see, in this approach, we are
    trying to calculate the similarity between two textual representations (the representation
    of `(h,r)` and `(t)`).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(h,r)` 和 `(t)`。这样，我们从模型中获得了两种表示（模型的最终隐藏状态被使用）。之后，我们使用评分函数来预测这个三元组是否有效。这种方法确实更准确，但存在组合爆炸的风险。正如你所见，在这个方法中，我们试图计算两个文本表示之间的相似性（`(h,r)`
    和 `(t)` 的表示）。'
- en: These approaches are, in fact, very similar to those we have seen in previous
    chapters when trying to calculate the similarity between two sentences.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法实际上与我们之前章节中尝试计算两个句子之间相似性时看到的方法非常相似。
- en: '![Figure 7.14 – LLMs as encoders for KG completion (https://arxiv.org/pdf/2306.08302)](img/B21257_07_14.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – LLMs作为KG补全的编码器（https://arxiv.org/pdf/2306.08302）](img/B21257_07_14.jpg)'
- en: Figure 7.14 – LLMs as encoders for KG completion ([https://arxiv.org/pdf/2306.08302](https://arxiv.org/pdf/2306.08302))
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – LLMs作为KG补全的编码器（[https://arxiv.org/pdf/2306.08302](https://arxiv.org/pdf/2306.08302)）
- en: Alternatively, one can use few-shot examples or other prompting techniques and
    ask an LLM to complete them directly. In addition, this approach allows you to
    be able to provide additional items in the prompt. In previous approaches, we
    provided only the triplet `(h,r,t)`; with prompt engineering, we can also provide
    other contextual elements (relationship descriptions, entity descriptions, etc.)
    or add instructions to better complete the task.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以使用少量示例或其他提示技术，并要求一个LLM直接完成它们。此外，这种方法还允许你在提示中提供额外的项目。在以前的方法中，我们只提供了三元组 `(h,r,t)`；通过提示工程，我们还可以提供其他上下文元素（关系描述、实体描述等）或添加指令以更好地完成任务。
- en: '![Figure 7.15 – Prompt-based KG completion (https://arxiv.org/pdf/2306.08302)](img/B21257_07_15.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – 基于提示的KG补全（https://arxiv.org/pdf/2306.08302）](img/B21257_07_15.jpg)'
- en: Figure 7.15 – Prompt-based KG completion ([https://arxiv.org/pdf/2306.08302](https://arxiv.org/pdf/2306.08302))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 基于提示的KG补全（[https://arxiv.org/pdf/2306.08302](https://arxiv.org/pdf/2306.08302)）
- en: Knowledge hosting and deployment
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识托管和部署
- en: 'The last step is the hosting and deployment of the KG. The KG is a set of nodes
    and relationships, so we can use a graph-specific paradigm to be able to store
    the data. Of course, hosting a KG is not without challenges:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是KG的托管和部署。KG是一组节点和关系，因此我们可以使用图特定的范式来存储数据。当然，托管KG并非没有挑战：
- en: '**Size**: The larger the KG, the more complex its management becomes'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大小**：KG越大，其管理就越复杂'
- en: '**Data model**: We have to choose the system that allows us to optimally access
    the information for our tasks, as different systems have different advantages
    and disadvantages'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据模型**：我们必须选择一个系统，使我们能够最优地访问我们任务所需的信息，因为不同的系统有不同的优缺点'
- en: '**Heterogeneity**: The graph may contain multiple modes, thus making storage
    more complex'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异质性**：图可能包含多种模式，这使得存储更加复杂'
- en: '**Speed**: The more it grows, the more complex knowledge updates become'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：随着其增长，知识更新的复杂性也随之增加'
- en: '**User needs**: Users may have heterogeneous needs that may be conflicting,
    requiring us to have to implement rules and constraints'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户需求**：用户可能有异质的需求，这些需求可能相互冲突，需要我们实施规则和约束'
- en: '**Deployment**: The system must be accessible to users and allow easy inference'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署**：系统必须对用户可访问，并允许轻松推理'
- en: 'There are different alternatives for the storage of a KG:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: KG的存储有不同选择：
- en: KGs can be hosted in classic **relational databases** (e.g., **Structured Query
    Language** (**SQL**) where entities and relationships are stored in tables). From
    these tables, one can then reconstruct the graph’s relational structure using
    projections. Using a relational database to store a large KG can result in large
    tables that are impractical or a multitude of tables with a complex hierarchy.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KG可以托管在经典的**关系数据库**（例如，**结构化查询语言**（SQL）中，实体和关系存储在表中）。然后，可以从这些表中使用投影重建图的关系结构。使用关系数据库存储大型KG可能导致大型表不切实际，或者大量具有复杂层次结构的表。
- en: An alternative is the **document model** where data is stored as tuples (key-value
    pairs) and then organized into collections. This structure can be convenient for
    searching; it is a schematic system that allows the speed of writing, but updating
    knowledge in nested collections can be a nightmare.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种选择是**文档模型**，其中数据以元组（键值对）的形式存储，然后组织成集合。这种结构在搜索时可能很方便；它是一个允许快速写入的示意图系统，但更新嵌套集合中的知识可能是一场噩梦。
- en: '**Graph databases** are databases that are optimized for storing and searching
    graphs and data transformation. The graph data model then has nodes and edges
    with various metadata that are attached. The query language is also adapted to
    this structure (and is vaguely reminiscent of SQL). Graph databases also have
    the advantage of allowing heterogeneity and supporting speed. Neo4j is one of
    the most widely used and uses an adapted query language (Cypher).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图数据库**是针对存储和搜索图以及数据转换进行优化的数据库。图数据模型包含节点和边，并附加了各种元数据。查询语言也适应了这种结构（并且有些类似于SQL）。图数据库还有允许异构性和支持速度的优势。Neo4j是最广泛使用的之一，并使用了一种修改后的查询语言（Cypher）。'
- en: '**Triplet stores** are where the database is made up directly of triplets.
    Databases exist today that save information in triplets and allow queries to be
    conducted in the database. Typically, these databases also have native support
    for ontologies and for conducting logical reasoning.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**三元组存储**是数据库直接由三元组组成的地方。今天存在一些数据库，它们以三元组的形式保存信息，并允许在数据库中执行查询。通常，这些数据库也具有对本体和进行逻辑推理的原生支持。'
- en: Hosting comes with its own set of challenges, and the choice of data model should
    be conducted with subsequent applications in mind. For example, if our KG needs
    to retrieve data from our relational database, using this system has advantages
    for integration. On the other hand, though, in this case, we will sacrifice performance
    for heterogeneity and speed. A graph database handles performance and the structural
    nature of the graph better, but it may integrate poorly with other components
    of the system. Whatever system we use for storage, we can either build hybrid
    systems depending on the applications or create a KG as a layer on top of the
    database.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 托管带来了一系列挑战，选择数据模型时应考虑到后续应用。例如，如果我们的KG需要从我们的关系型数据库中检索数据，使用这个系统在集成方面有优势。然而，在这种情况下，我们将为了异构性和速度而牺牲性能。图数据库在处理性能和图的结构的结构性质方面做得更好，但它可能与其他系统组件的集成不佳。无论我们使用什么系统进行存储，我们都可以根据应用构建混合系统，或者创建一个作为数据库之上的层的KG。
- en: Deployment is the last step in the pipeline. That doesn’t mean it’s the end
    of the story, though. A KG can become outdated easily, so we need to have pipelines
    in mind for knowledge updates or to be able to handle new applications. Similarly,
    the entry of new knowledge means that we must have pipelines for knowledge assessment
    (monitoring the quality of the KG, ensuring that no errors are entered or that
    there are no conflicts). Some knowledge may be outdated or need to be deleted
    for legal or privacy issues; therefore, we should have pipelines for cleaning
    the KG. Other pipelines should instead focus on controlling access and security
    of our system.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 部署是管道中的最后一步。但这并不意味着故事的结束。KG很容易过时，因此我们需要考虑知识更新或能够处理新应用的管道。同样，新知识的进入意味着我们必须有知识评估的管道（监控KG的质量，确保没有错误输入或没有冲突）。某些知识可能过时或需要删除，以解决法律或隐私问题；因此，我们应该有清理KG的管道。其他管道应专注于控制系统的访问和安全。
- en: In this section, we have seen all the steps necessary to create and deploy a
    KG. Now that we have our KG, we can use it; in the next section, we will discuss
    how to find information and use it as a context for our LLM.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了创建和部署KG所需的所有步骤。现在我们有了我们的KG，我们可以使用它；在下一节中，我们将讨论如何查找信息并将其作为我们LLM的上下文。
- en: Retrieving information with a knowledge graph and an LLM
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用知识图谱和LLM检索信息
- en: 'In the previous two chapters, we discussed the capabilities of RAG and its
    role in reducing hallucinations generated by LLMs. Although RAG has been widely
    used in both research and industrial applications, there are still limitations:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们讨论了RAG的能力及其在减少LLM生成的幻觉中的作用。尽管RAG在研究和工业应用中已被广泛使用，但仍存在局限性：
- en: '**Neglecting relationships**: The text in the databases is interconnected and
    not isolated. For example, a document is divided into chunks; since these chunks
    belong to a single document, there is a semantic connection between them. RAG
    fails to capture structured relational knowledge when this cannot be captured
    by semantic similarity. Some authors point out that, in science, there are important
    relationships between an article and previous works, and these relationships are
    usually highlighted with a citation network. Using RAG, we can find articles that
    are similar to the query but we cannot find this citation network, losing this
    relational information.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忽略关系**：数据库中的文本是相互关联的，而不是孤立的。例如，一个文档被分割成块；因为这些块属于单个文档，它们之间存在语义联系。当无法通过语义相似性捕获时，RAG无法捕捉结构化的关系知识。一些作者指出，在科学中，一篇文章与先前作品之间存在重要的关系，这些关系通常通过引用网络来突出显示。使用RAG，我们可以找到与查询相似的文档，但我们无法找到这个引用网络，从而丢失了这些关系信息。'
- en: '**Redundant information**: The context that comes to the LLM is a series of
    concatenated chunks. Today with LLMs, we can add more and more context (the context
    length of models is getting longer and longer) but they struggle with the presence
    of redundant information. The more chunks we add to the context, the greater the
    amount of redundant or non-essential information to answer the query. The presence
    of this redundant information reduces the performance of the model.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗余信息**：传递给LLM的上下文是一系列连接的块。今天，随着LLM的发展，我们可以添加越来越多的上下文（模型的上下文长度越来越长），但它们在冗余信息的存在上遇到了困难。我们添加到上下文中的块越多，用于回答查询的冗余或非必要信息就越多。这种冗余信息的存在降低了模型的表现。'
- en: '**Lacking global information**: RAG finds a set of documents but fails to find
    global information because the set of documents is not representative of global
    information. This is a problem, especially for summarization tasks.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏全局信息**：RAG找到一组文档，但无法找到全局信息，因为文档集不能代表全局信息。这是一个问题，尤其是在摘要任务中。'
- en: '**Graph retrieval-augmented generation** (**GraphRAG**) has emerged as a new
    paradigm to try to solve these challenges. In traditional RAG, we find text chunks
    by conducting a similarity analysis on the embedded vectors. In GraphRAG, we conduct
    the search on the knowledge graph, and the found triplets are provided to the
    context. So, the main difference is that upon arrival of a query from a user,
    we conduct the search in the KG and use the information contained in the graph
    to answer the query.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**图检索增强生成**（**GraphRAG**）作为一种新的范式出现，试图解决这些挑战。在传统的RAG中，我们通过对嵌入向量进行相似性分析来找到文本块。在GraphRAG中，我们在知识图谱上进行搜索，并将找到的三元组提供给上下文。因此，主要区别在于，当用户查询到达时，我们在知识图谱中进行搜索，并使用图中包含的信息来回答查询。'
- en: '![Figure 7.16 – Comparison between direct LLM, RAG, and GraphRAG (https://arxiv.org/pdf/2408.08921)](img/B21257_07_16.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图7.16 – 直接LLM、RAG和GraphRAG的比较](https://arxiv.org/pdf/2408.08921)(img/B21257_07_16.jpg)'
- en: Figure 7.16 – Comparison between direct LLM, RAG, and GraphRAG ([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 – 直接LLM、RAG和GraphRAG的比较([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
- en: 'Formally, we can define GraphRAG as a framework that exploits a KG to provide
    context to an LLM and produce a better response. The system, therefore, is very
    similar to classical RAG; to avoid confusion, in this context, we will call it
    *vector RAG*. In GraphRAG, the KG is the knowledge base, and from this, we find
    information on entities and relationships. GraphRAG consists of three main steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，我们可以将GraphRAG定义为一个利用知识图谱（KG）为语言模型（LLM）提供上下文并生成更好响应的框架。因此，该系统与经典的RAG非常相似；为了避免混淆，在此语境中，我们将称之为*向量RAG*。在GraphRAG中，知识图谱是知识库，从中我们可以找到实体和关系的信息。GraphRAG由三个主要步骤组成：
- en: '**Graph-based indexing** (**G-indexing**): In this initial phase, the goal
    is to build a graph database and index it correctly.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于图的索引**（**G-indexing**）：在这个初始阶段，目标是构建一个图数据库并正确索引它。'
- en: '`q`, in natural language, we want to extract a subgraph that we can use to
    correctly answer the query.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`q`，在自然语言中，我们希望提取一个子图，我们可以用它来正确回答查询。'
- en: '**Graph-enhanced generation** (**G-generation**): The last step concerns using
    the found knowledge for generation. This step is conducted with an LLM that receives
    the context and generates the answer.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图增强生成**（**G-generation**）：最后一步是使用找到的知识进行生成。这一步使用一个LLM来完成，该LLM接收上下文并生成答案。'
- en: '![Figure 7.17 – Overview of the GraphRAG framework for a question-answering
    task (https://arxiv.org/pdf/2408.08921)](img/B21257_07_17.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图7.17 – 图RAG框架在问答任务中的概述 (https://arxiv.org/pdf/2408.08921)](img/B21257_07_17.jpg)'
- en: Figure 7.17 – Overview of the GraphRAG framework for a question-answering task
    ([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – 图RAG框架在问答任务中的概述 ([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
- en: In the following subsections, we will talk about each step in detail.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将详细讨论每个步骤。
- en: Graph-based indexing
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于图的索引
- en: 'In the first step, we need to choose what our graph data will be. Generally,
    two types of KGs are used: open KGs or self-constructed KGs. In the first case,
    we can use a KG that is already available and adapt it to our GraphRAG. Today,
    many KGs have already been built and are available (for example, Wikidata is a
    knowledge base that collects data from various Wikipedia-related projects). Several
    KGs are specialized in a particular domain; these KGs have a greater understanding
    of a particular domain (some of these KGs are open and usable). Alternatively,
    it is possible to build your own KG.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步，我们需要选择我们的图数据将是什么。通常，使用两种类型的知识图谱：开放的知识图谱或自构建的知识图谱。在前一种情况下，我们可以使用已经可用的知识图谱，并将其适应到我们的GraphRAG中。今天，已经建立了许多知识图谱，并且可供使用（例如，Wikidata是一个收集来自各种维基百科相关项目的数据的知识库）。一些知识图谱专注于特定领域；这些知识图谱对特定领域有更深入的理解（其中一些是开放的且可用的）。或者，你也可以构建自己的知识图谱。
- en: When building it or before using it in GraphRAG, you should pay attention to
    indexing. Proper indexing allows us to have a faster and more efficient GraphRAG.
    Although we can imagine the KG visually as a graph, it is still stored in a database.
    Indexing allows us access to the information we want to find. Thus, there are
    several types of indexing. For example, we can have text descriptions associated
    with nodes, triplets, or ontologies that are then used during the search. Another
    way is to transform graph data into vectors and conduct the search on these vector
    spaces (embedding). We can also use indexing that better respects the graph nature
    of the data or hybrid versions.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建它或在使用它之前，你应该注意索引。适当的索引使我们能够拥有更快、更高效的GraphRAG。虽然我们可以将知识图谱可视化为一个图，但它仍然存储在数据库中。索引使我们能够访问我们想要找到的信息。因此，存在几种索引类型。例如，我们可以将文本描述与节点、三元组或本体关联起来，然后在搜索中使用这些描述。另一种方式是将图数据转换为向量，并在这些向量空间上进行搜索（嵌入）。我们还可以使用更尊重数据图性质的索引或混合版本。
- en: '![Figure 7.18 – Overview of graph-based indexing (https://arxiv.org/pdf/2408.08921)](img/B21257_07_18.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图7.18 – 基于图的索引概述 (https://arxiv.org/pdf/2408.08921)](img/B21257_07_18.jpg)'
- en: Figure 7.18 – Overview of graph-based indexing ([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – 基于图的索引概述 ([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
- en: Graph-guided retrieval
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图引导检索
- en: 'In GraphRAG, retrieval is crucial for the quality of response generation (similar
    to vector RAG). The search for a KG has two challenges that need to be solved:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在GraphRAG中，检索对于生成高质量响应至关重要（类似于向量RAG）。对知识图谱的搜索面临两个需要解决的问题：
- en: '**Explosive candidate subgraphs**: As the graph grows, the number of subgraphs
    in the KG increases exponentially. This means we need efficient algorithms to
    explore the KG and find the relevant subgraphs. Some of these algorithms use heuristic
    methods to be more efficient.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**爆炸性候选子图**：随着图的增长，知识图谱中的子图数量呈指数增长。这意味着我们需要高效的算法来探索知识图谱并找到相关的子图。其中一些算法使用启发式方法以提高效率。'
- en: '**Insufficient similarity measurement**: Our query is in text form, but we
    want to conduct our similarity search on a graph. This means that our algorithm
    must be able to understand both textual and structural information and be able
    to succeed in comparing the similarity between data from different sources.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似度测量不足**：我们的查询是文本形式，但我们希望在图上执行相似度搜索。这意味着我们的算法必须能够理解文本和结构信息，并且能够成功比较来自不同来源的数据之间的相似度。'
- en: We can have different types of retrievers. The simplest are `Obama` entity,
    in KG, we take neighboring entities, `k-hop=1`, or even neighbors of neighbors,
    `k-hop=2`, and so on). Non-parametric retrievers are the simplest and also the
    fastest systems, but they suffer from inaccurate retrieval (they can be improved
    by learning). There are machine and deep learning models that are natively trained
    on graphs. GNN-based retrievers are one example. **Graph neural networks** (**GNNs**)
    are neural networks that natively handle graphs and can be used for many tasks
    on graphs (node classification, edge prediction, and so on) so they search the
    graph for subgraphs similar to the query.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以有不同类型的检索器。最简单的是`奥巴马`实体，在KG中，我们取相邻实体，`k-hop=1`，或者甚至邻居的邻居，`k-hop=2`，等等）。非参数检索器是最简单且也是最快的系统，但它们在检索准确性方面存在问题（可以通过学习来改进）。有一些机器和深度学习模型是原生在图上训练的。基于GNN的检索器是一个例子。**图神经网络**（**GNNs**）是原生处理图的神经网络，可以用于图上的许多任务（节点分类、边预测等），因此它们搜索与查询相似的子图。
- en: Alternatively, we can use an LLM-based retriever where we have a transformer-based
    model that conducts the search. The model then processes and interprets the query
    to conduct the search. Several of these LLMs are models that have been trained
    on the text, and then fine-tuning is conducted to search the graphs. One advantage
    is that an LLM can be used as an agent and use different tools or functions to
    search the graph. Both LLM-based retrievers and GNN-based retrievers significantly
    improve retrieval accuracy but at a high computational cost. There are also alternatives
    today that use different methods (conduct with both a GNN and LLM, or use heuristic
    methods together with an LLM) or the process can be multistage (e.g., conduct
    an initial search with an LLM and then refine the results).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用基于LLM的检索器，其中我们有一个基于transformer的模型来进行搜索。然后模型处理和解释查询以进行搜索。这些LLM中有一些是在文本上训练过的模型，然后进行微调以搜索图。一个优点是LLM可以用作代理，并使用不同的工具或函数来搜索图。基于LLM的检索器和基于GNN的检索器都显著提高了检索精度，但计算成本很高。今天也有使用不同方法（同时使用GNN和LLM，或与LLM一起使用启发式方法）的替代方案，或者过程可以是多阶段的（例如，先用LLM进行初始搜索，然后细化结果）。
- en: As was done for the vector RAG, we can add additional components to conduct
    enhancement. For example, in the previous chapter, we saw that we can rewrite
    a query or decompose queries that are too complex. Query modification helps to
    better capture the meaning of the query (because sometimes the query does not
    capture the implicit meaning intended by the user). Retrieval can also be a flexible
    process. In the previous chapter, we saw that in naïve RAG, retrieval was conducted
    only once, but then variations in advanced and modular RAG were established where
    retrieval can be multistage or iterative. Even more sophisticated variations make
    the process adaptive depending on the query, so for simpler queries, only one
    retrieval is conducted, and for more complex queries, multiple iterations may
    be conducted. Similarly, the results obtained after retrieval can also be modified.
    For example, even with GraphRAG, we can conduct a compression of the retrieved
    knowledge. In fact, we may also find redundant information if we conduct multiple
    retrieval stages and thus it is convenient to filter out irrelevant information.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 就像为向量RAG所做的那样，我们可以添加额外的组件来进行增强。例如，在前一章中，我们看到我们可以重写查询或分解过于复杂的查询。查询修改有助于更好地捕捉查询的含义（因为有时查询没有捕捉到用户意图的隐含含义）。检索也可以是一个灵活的过程。在前一章中，我们看到在简单的RAG中，检索只进行了一次，但在高级和模块化RAG中建立了检索可以多阶段或迭代的变体。更复杂的变体使过程根据查询自适应，因此对于简单的查询，只进行一次检索，而对于更复杂的查询，可能需要进行多次迭代。同样，检索后获得的结果也可以进行修改。例如，即使在使用GraphRAG的情况下，我们也可以对检索到的知识进行压缩。实际上，如果我们进行多个检索阶段，我们可能也会发现冗余信息，因此方便过滤掉不相关信息。
- en: Today, there are also reranking approaches to reorder the retrieved results
    with GraphRAG. One example is to reorder the various subgraphs found and perhaps
    choose the top *k* subgraphs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，使用GraphRAG也有重新排序检索结果的方法。一个例子是对找到的各种子图进行排序，并可能选择前*k*个子图。
- en: '![Figure 7.19 – General architectures of graph-based retrieval (https://arxiv.org/pdf/2408.08921)](img/B21257_07_19.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图7.19 – 基于图的检索的一般架构](https://arxiv.org/pdf/2408.08921)](img/B21257_07_19.jpg)'
- en: Figure 7.19 – General architectures of graph-based retrieval ([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 基于图的检索的一般架构 ([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
- en: 'Another difference with vector RAG is how we control search granularity. In
    vector RAG, granularity is controlled by deciding the size of the chunks. In the
    case of GraphRAG, we do not conduct chunking or find chunks. We can, however,
    control granularity during retrieval by choosing what we find:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与向量RAG相比，另一个区别在于我们如何控制搜索粒度。在向量RAG中，粒度是通过决定块的大小来控制的。在GraphRAG的情况下，我们不进行分块或寻找块。然而，我们可以通过选择我们找到的内容来在检索过程中控制粒度：
- en: '**Nodes**: In GraphRAG, we can retrieve individual entities. Nodes can have
    properties associated with them and then only add entities and their properties
    to the context. This can be useful for target queries.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**: 在GraphRAG中，我们可以检索单个实体。节点可以与它们关联属性，然后只将实体及其属性添加到上下文中。这对于目标查询可能很有用。'
- en: '**Triplets**: By expanding the search granularity, we choose to retrieve triplets
    (so not only nodes, but also their relationships). This is useful when we are
    interested not only in the entity itself but also in their relationships.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**三元组**: 通过扩展搜索粒度，我们选择检索三元组（因此不仅包括节点，还包括它们的关系）。当我们不仅对实体本身感兴趣，还对它们的关系感兴趣时，这很有用。'
- en: '**Paths**: In this case, we still expand the retrieval. A path is a chain of
    nodes and relationships, so starting from entity *X* and arriving at entity *Y*,
    the path is all the chain of entities and relationships that connect them. Obviously,
    there are multiple paths between different entities, and these grow exponentially
    as the size of the graph increases. So, we generally define rules, use GNNs, or
    choose the shortest path.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**路径**: 在这种情况下，我们仍然扩展检索。路径是一系列节点和关系，因此从实体 *X* 到实体 *Y* 的路径是连接它们的实体和关系的所有链。显然，不同实体之间存在多条路径，并且随着图的大小增加，这些路径呈指数增长。因此，我们通常定义规则、使用GNN或选择最短路径。'
- en: '**Subgraphs**: A subgraph can be defined as a subset of nodes and relationships
    internal to the KG. Extracting a subgraph allows us to answer complex queries
    because it allows us to analyze complex patterns and dependencies between entities.
    There are several ways to extract a subgraph: we can use specific patterns or
    conduct a merge of different paths.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子图**: 子图可以被定义为KG内部节点和关系的子集。提取子图使我们能够回答复杂查询，因为它允许我们分析实体之间的复杂模式和依赖关系。提取子图有几种方法：我们可以使用特定的模式或进行不同路径的合并。'
- en: '**Hybrid granularities**: We can use different granularities at the same time
    or choose an adaptive system.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合粒度**: 我们可以同时使用不同的粒度或选择一个自适应系统。'
- en: 'In the case of GraphRAG, balancing granularity and efficiency is important.
    We do not want to saturate the context with elements to prevent later LLM struggles
    with irrelevant information. It also depends on the complexity of the query: for
    simple queries, even low granularity is enough, whereas complex queries benefit
    from higher granularity. An adaptive approach can make the system more efficient
    while maintaining nuances when needed.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在GraphRAG的情况下，平衡粒度和效率很重要。我们不希望用元素填满上下文，以防止后续LLM与无关信息作斗争。这也取决于查询的复杂性：对于简单查询，即使低粒度也足够，而对于复杂查询，更高的粒度更有益。自适应方法可以使系统更高效，同时在需要时保持细微差别。
- en: '![Figure 7.20 – Different levels of retrieval granularity](img/B21257_07_20.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图7.20 – 不同级别的检索粒度](img/B21257_07_20.jpg)'
- en: Figure 7.20 – Different levels of retrieval granularity
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 – 不同级别的检索粒度
- en: 'Once the knowledge is found and cleaned up, we can provide it to the LLM to
    generate a response to the query. This knowledge enters the prompt provided to
    the LLM and the model generates a response. Alternatively, this found knowledge
    can be used for certain types of models that are used for some specific tasks
    (e.g., a GNN to answer multiple-choice questions). The main problem is that the
    graph has a non-Euclidean nature and integration with textual information is not
    optimal. For this, graph translators that convert the found graph information
    into more digestible information for an LLM can be used. This conversion increases
    the LLM’s ability to understand the information. So, once we find the information
    in graph form (nodes, relationships, path, or subgraph), we put it in the context
    of the LLM. There are some alternatives:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到并清理了知识，我们就可以将其提供给LLM以生成对查询的响应。这种知识进入提供给LLM的提示中，模型生成响应。或者，这种找到的知识可以用于某些特定任务的模型（例如，一个用于回答多项选择题的GNN）。主要问题是图具有非欧几里得性质，与文本信息的集成并不理想。为此，可以使用图翻译器将找到的图信息转换成LLM更容易理解的信息。这种转换提高了LLM理解信息的能力。因此，一旦我们在图形式（节点、关系、路径或子图）中找到信息，我们就将其置于LLM的上下文中。有一些替代方案：
- en: '**Graph formats**: We can directly add the set of relationships and nodes in
    the prompt, or we can use a form of graph structure representation such as adjacency
    or edge tables. The latter compactly conveys a better relational structure. Another
    idea is a node sequence, which is generated according to a predeterminate rule.
    This is a compact representation that contains the order of the nodes in the graph.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图形格式**：我们可以在提示中直接添加关系和节点的集合，或者我们可以使用一种图形结构表示形式，如邻接表或边表。后者紧凑地传达了更好的关系结构。另一种想法是节点序列，它是根据预定规则生成的。这是一种紧凑的表示，包含了图中节点的顺序。'
- en: '**Natural language**: There are specific graph languages that can be used to
    transform information into natural language, a representation that is more congenial
    to LLM. In this process, we convert the found subgraph into a descriptive form.
    Templates can be used to transform the graph where it is filled with nodes and
    relationships. In some templates, you can define which are the nearest neighbors
    of a node and which are the most distant (1-hop and 2-hop in the graph), or you
    can use an LLM to transform this subgraph into a natural language description.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言**：存在特定的图语言，可以将信息转换成自然语言，这种表示对LLM来说更为亲切。在这个过程中，我们将找到的子图转换成描述性形式。可以使用模板将图转换，其中节点和关系被填充。在某些模板中，您可以定义节点的最近邻和最远邻（图中的1-hop和2-hop），或者您可以使用LLM将此子图转换成自然语言描述。'
- en: '**Syntax tree**: The graph is flattened and represented as a syntax tree. These
    trees have the advantage of a hierarchical structure and maintain the topological
    order of the graph. This approach maintains the properties of the graph but makes
    the information more digestible for the LLM.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语法树**：图被展平并表示为语法树。这些树具有层次结构的优势，并保持了图的拓扑顺序。这种方法保持了图的属性，但使信息对LLM来说更容易消化。'
- en: '**Code-like forms**: Retrieved graphs can be converted into a standard format
    such as **Graph Markup Language** (**GraphML**). These languages are specifically
    designed for graphs but are a structural and textual hybrid.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码形式**：检索到的图可以被转换成标准格式，例如**图形标记语言**（**GraphML**）。这些语言专门为图设计，但它们是结构和文本的混合体。'
- en: Conversion still presents difficulties because it must ensure that the result
    is concise and complete but, at the same time, understandable by the LLM. Optimally,
    this representation should also include the structural information of the graph.
    Whether the retrieved subgraph is converted or not, the result is entered into
    the LLM prompt and a response is generated.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 转换仍然存在困难，因为它必须确保结果是简洁且完整的，同时，对LLM来说是可理解的。理想情况下，这种表示还应包括图的结构的详细信息。无论检索到的子图是否转换，结果都输入到LLM提示中，并生成响应。
- en: '![Figure 7.21 – Subgraph transformation to enhance generation (https://arxiv.org/pdf/2408.08921)](img/B21257_07_21.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图7.21 – 通过子图转换增强生成 (https://arxiv.org/pdf/2408.08921)](img/B21257_07_21.jpg)'
- en: Figure 7.21 – Subgraph transformation to enhance generation ([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 – 通过子图转换增强生成([https://arxiv.org/pdf/2408.08921](https://arxiv.org/pdf/2408.08921))
- en: GraphRAG applications
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphRAG应用
- en: GraphRAG has several applications. The first is question answering (so the same
    application as RAG) where we extract the subgraphs and the LLM uses them for subsequent
    reasoning and answering. A sub-branch of question answering is commonsense reasoning
    question answering where it often takes the format of multiple choice questions.
    For this subtask, we often do not use an LLM but a GNN or other machine learning
    (ML) model instead. However, KGs (and therefore also GraphRAG) have an extensive
    application for information retrieval, for example, if we want to investigate
    the relationships between some entities of interest. A KG can be used by itself
    to extract relationships between entities, but the addition of generation with
    an LLM allows us to explore these relationships and contextual nuances better.
    This is an attractive factor for academic and literature research. In fact, in
    academia, an article is authored by multiple authors who are part of different
    institutions. An article builds on previous research, and so for each article,
    there is a network of citations. These structural elements are easily modeled
    for a graph. An interesting application recently published shows how Ghafarollahi
    et al. used multiple KG agents to analyze published literature and propose new
    research hypotheses. In short, they extracted paths or subgraphs from a KG (constructed
    from 1,000 articles), then an agent analyzed the ontologies, and then a new research
    hypothesis was generated from this. In this interesting application, a number
    of agents collaborate to create new potential searches for new materials.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: GraphRAG有多个应用。第一个是问答（与RAG相同的应用）中，我们提取子图，而LLM使用它们进行后续推理和回答。问答的一个子分支是常识推理问答，它通常以多选题的形式出现。对于这个子任务，我们通常不使用LLM，而是使用GNN或其他机器学习（ML）模型。然而，KG（因此也是GraphRAG）在信息检索方面有广泛的应用，例如，如果我们想研究一些感兴趣实体之间的关系。KG可以单独用来提取实体之间的关系，但添加LLM的生成功能使我们能够更好地探索这些关系和上下文细微差别。这是学术和文献研究的一个吸引因素。事实上，在学术界，一篇文章是由来自不同机构的多个作者共同撰写的。一篇文章建立在先前研究的基础上，因此对于每篇文章，都有一个引用网络。这些结构元素很容易用图来建模。最近发表的一个有趣的应用展示了Ghafarollahi等人如何使用多个KG智能体来分析已发表的文献并提出新的研究假设。简而言之，他们从一个由1,000篇文章构建的KG中提取路径或子图，然后一个智能体分析本体，然后从这个本体中生成新的研究假设。在这个有趣的应用中，多个智能体协作以创建新的潜在材料搜索。
- en: '![Figure 7.22 – Overview of the multi-agent graph-reasoning system for scientific
    discovery assistance (https://arxiv.org/pdf/2409.05556v1)](img/B21257_07_22.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图7.22 – 科学发现辅助的多智能体图推理系统概述 (https://arxiv.org/pdf/2409.05556v1)](img/B21257_07_22.jpg)'
- en: Figure 7.22 – Overview of the multi-agent graph-reasoning system for scientific
    discovery assistance ([https://arxiv.org/pdf/2409.05556v1](https://arxiv.org/pdf/2409.05556v1))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22 – 科学发现辅助的多智能体图推理系统概述 ([https://arxiv.org/pdf/2409.05556v1](https://arxiv.org/pdf/2409.05556v1))
- en: One of the reasons for interest in GraphRAG is that KGs used to be used for
    fact verification (after all, a KG is a collection of facts) so providing facts
    to an LLM should reduce LLM hallucinations. This aspect makes it particularly
    attractive for biomedical applications. In fact, hallucinations are a serious
    problem for medical decision-making applications. In medicine, if the vector RAG
    can reduce hallucinations, it does not allow for a holistic view, especially when
    an overview is needed to answer a question. Therefore, Wu et al. suggest using
    a GraphRAG-based approach called **MedGraphRAG**. In this work, they use several
    medical sources to create their system and take advantage of the hierarchical
    nature of KGs. They construct three levels for their KG. At the first level, there
    are user-provided documents (medical reports from a hospital). Entities at this
    level are then connected to a more foundational level of commonly accepted information.
    The second level is constructed from medical textbooks and scientific articles.
    Finally, at the third level, there are well-defined medical terms and knowledge
    relationships that have been obtained from standardized and reliable sources.
    Leveraging retrieval from this KG obtains state-of-the-art results on major medical
    question-answering benchmark datasets. The advantage is that this system also
    outperforms models that are fine-tuned on medical knowledge, thus with large computational
    savings.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对GraphRAG感兴趣的一个原因是，知识图谱过去常用于事实核查（毕竟，知识图谱是一系列事实的集合），因此向大型语言模型（LLM）提供事实应该会减少LLM的幻觉。这一特性使其特别适合生物医学应用。事实上，幻觉是医疗决策应用中的一个严重问题。在医学中，如果RAG向量可以减少幻觉，它不允许有全面的视角，尤其是在需要概述来回答问题时。因此，Wu等人建议使用基于GraphRAG的方法，称为**MedGraphRAG**。在这项工作中，他们使用几个医学来源来创建他们的系统，并利用知识图谱的层次性质。他们为他们的知识图谱构建了三个层级。在第一层级，有用户提供的文档（医院的医疗报告）。这一层级的实体随后连接到一个更基础的信息层级。第二层级由医学教科书和科学文章构建。最后，在第三层级，有从标准化和可靠来源获得的明确定义的医学术语和知识关系。利用从这个知识图谱中的检索获得了在主要医学问答基准数据集上的最先进结果。其优势在于，这个系统也优于在医学知识上微调的模型，从而实现了大量的计算节省。
- en: '![Figure 7.23 – MedGraphRAG framework (https://arxiv.org/pdf/2408.04187)](img/B21257_07_23.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图7.23 – MedGraphRAG框架 (https://arxiv.org/pdf/2408.04187)](img/B21257_07_23.jpg)'
- en: Figure 7.23 – MedGraphRAG framework ([https://arxiv.org/pdf/2408.04187](https://arxiv.org/pdf/2408.04187))
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23 – MedGraphRAG框架 ([https://arxiv.org/pdf/2408.04187](https://arxiv.org/pdf/2408.04187))
- en: A further interest in GraphRAG comes from the use of KGs to propose recommendations
    to users. In e-commerce platforms, recommendation systems are used to predict
    the future purchasing intentions of users and suggest other products of interest.
    Thus, it was proposed that the system matches a new user to subgraphs derived
    from past users with similar behavior, and leverages these to predict likely future
    purchases and suggest appropriate products. In addition, this approach can also
    be useful for the legal and financial fields. In the legal field, there are extensive
    citations between cases and judicial opinions, and judges use past cases and opinions
    to make new decisions. Given a legal case, GraphRAG could suggest previous legal
    cases and help in decision-making. In finance, GraphRAG might suggest previous
    financial transactions or customer cases.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对GraphRAG的进一步兴趣源于使用知识图谱（KGs）向用户提出推荐。在电子商务平台上，推荐系统被用来预测用户的未来购买意图，并建议其他感兴趣的产品。因此，有人提出，该系统将新用户与从过去具有相似行为的用户中派生出的子图相匹配，并利用这些信息来预测可能的未来购买并建议合适的产品。此外，这种方法对于法律和金融领域也很有用。在法律领域，案例和司法意见之间存在广泛的引用，法官使用过去的案例和意见来做出新的裁决。给定一个法律案例，GraphRAG可以建议先前的法律案例并帮助决策。在金融领域，GraphRAG可能会建议先前的金融交易或客户案例。
- en: Finally, up to this point, we have suggested that GraphRAG and vector RAG are
    antagonistic. Actually, both systems have advantages and disadvantages so it would
    be more useful to use them in synergy. Sarmah et al., proposed **HybridRAG**,
    where both GraphRAG and vector RAG are used in one system. Their system shows
    advantages in financial responses. In the future, there may be systems that exploit
    both one and the other approach with the addition of a router that can choose
    whether to search for the KG or the vector database. Alternatively, there could
    be more complex systems for knowledge fusion in context (especially if KG search
    and chunks provide some redundant information).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，到目前为止，我们建议GraphRAG和向量RAG是相互对立的。实际上，这两个系统都有优点和缺点，因此将它们协同使用会更有用。Sarmah等人提出了**HybridRAG**，在该系统中同时使用了GraphRAG和向量RAG。他们的系统在金融响应方面显示出优势。未来，可能会有系统利用这两种方法，并增加一个路由器来选择是否搜索知识图谱或向量数据库。或者，可能会有更复杂的系统用于上下文中的知识融合（特别是如果知识图谱搜索和块提供了一些冗余信息）。
- en: In this section, we discussed how an LLM can be connected to a KG, and how it
    can be used to find information that enriches the context of the LLM. In the next
    section, we will discuss other tasks for which the synergy of LLMs and KGs is
    useful.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了如何将LLM连接到知识图谱，以及如何使用它来查找丰富LLM上下文的信息。在下一节中，我们将讨论LLMs和KGs协同有用的其他任务。
- en: Understanding graph reasoning
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解图推理
- en: 'This section is devoted to a discussion of how to solve graph data tasks. In
    this section, we will discuss some of the approaches used to solve tasks on knowledge
    graphs: KG embeddings, GNNs, and LLMs. KG embeddings and GNNs would require at
    least one chapter each; hence, these topics are outside the scope of the book,
    but we believe that an introduction to them would be beneficial to a practitioner.
    In fact, both embedding and GNNs can be used synergistically with LLMs and agents.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本节致力于讨论如何解决图数据任务。在本节中，我们将讨论一些用于解决知识图谱上任务的途径：知识图谱嵌入、图神经网络（GNNs）和大型语言模型（LLMs）。知识图谱嵌入和GNNs各需要至少一章内容；因此，这些主题超出了本书的范围，但我们相信对这些内容的介绍对实践者是有益的。实际上，嵌入和GNNs都可以与LLMs和代理协同使用。
- en: There are many tasks in which a model is required to understand the structure
    to solve, and these are collectively called **graph structure understanding tasks**.
    Many of these tasks are solved using algorithms or models designed specifically
    to learn these tasks. Today, a new paradigm is being developed in which we try
    to use LLMs to solve these tasks; we will discuss this in depth at the end of
    this section. Examples of tasks might be degree calculation (how many neighbors
    a node has), path search (defining a path between two nodes, calculating which
    is the minimum path, and so on), Hamilton path (identifying a path that visits
    each node only once), topological sorting (identifying whether nodes can be visited
    in topological order), and many others. Some of these tasks are simple (degree
    calculation and path search) but others are much more complex (topological sorting
    and Hamilton path).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多任务需要模型理解结构来解决，这些任务统称为**图结构理解任务**。许多这些任务都是使用专门设计来学习这些任务的算法或模型来解决的。今天，我们正在开发一种新范式，我们试图使用LLMs来解决这些任务；我们将在本节末尾深入讨论这一点。任务的例子可能包括度计算（一个节点有多少邻居）、路径搜索（定义两个节点之间的路径，计算哪条是最短路径等）、哈密顿路径（识别一个只访问每个节点一次的路径）、拓扑排序（识别节点是否可以按拓扑顺序访问）以及许多其他任务。其中一些任务很简单（度计算和路径搜索），但其他任务则要复杂得多（拓扑排序和哈密顿路径）。
- en: '![Figure 7.24 – Graph structure understanding tasks (https://arxiv.org/pdf/2404.14809)](img/B21257_07_24.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图7.24 – 图结构理解任务 (https://arxiv.org/pdf/2404.14809)](img/B21257_07_24.jpg)'
- en: Figure 7.24 – Graph structure understanding tasks ([https://arxiv.org/pdf/2404.14809](https://arxiv.org/pdf/2404.14809))
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24 – 图结构理解任务 ([https://arxiv.org/pdf/2404.14809](https://arxiv.org/pdf/2404.14809))
- en: Graph learning tasks, on the other hand, require the model to include not only
    the structure of the graph but also the attributes of the graph (features of nodes,
    edges, and the graph), thus understanding the semantic information of graphs.
    Examples of some of the tasks are node classification (classify the node according
    to its attributes and according to the attributes of its neighbors), graph classification
    (you have to understand the whole graph to classify it), edge classification,
    and node feature explanation (explain a feature of the node). **Knowledge graph
    question answering** (**KGQA**) is a task that falls into this group, as we need
    to understand both the structure and meaning of entities and relationships to
    answer questions. A similar task is conducting KG queries to generate text (this
    can also be seen as a subtask). KG embeddings capture multi-relational semantics
    and latent patterns in the graph, making them particularly useful for relational
    reasoning and symbolic link prediction tasks (KG link prediction, for example).
    GNNs, on the other hand, capture graph structure and node/edge features; these
    make them perform well for tasks that require inductive reasoning, use of features,
    or local/global representation of graph structure (node or graph classification/regression).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，图学习任务要求模型不仅包括图的结构，还包括图的属性（节点、边和图的特征），从而理解图的语义信息。一些任务的例子包括节点分类（根据节点的属性及其邻居的属性进行分类）、图分类（你必须理解整个图才能对其进行分类）、边分类和节点特征解释（解释节点的某个特征）。**知识图谱问答**（**KGQA**）是这类任务之一，因为我们需要理解实体和关系的结构和意义来回答问题。一个类似的任务是执行知识图谱查询以生成文本（这也可以被视为一个子任务）。知识图谱嵌入捕获图中的多关系语义和潜在模式，使它们在关系推理和符号链接预测任务（例如KG链接预测）中特别有用。另一方面，图神经网络（GNNs）捕获图结构和节点/边特征；这使得它们在需要归纳推理、使用特征或图结构的局部/全局表示（节点或图分类/回归）的任务中表现良好。
- en: '![Figure 7.25 – Graph learning tasks (https://arxiv.org/pdf/2404.14809)](img/B21257_07_25.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图7.25 – 图学习任务 (https://arxiv.org/pdf/2404.14809)](img/B21257_07_25.jpg)'
- en: Figure 7.25 – Graph learning tasks ([https://arxiv.org/pdf/2404.14809](https://arxiv.org/pdf/2404.14809))
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.25 – 图学习任务 ([https://arxiv.org/pdf/2404.14809](https://arxiv.org/pdf/2404.14809))
- en: Knowledge graph embeddings
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识图谱嵌入
- en: KGs are effective in representing knowledge, but they are complex to manipulate
    at scale. This complicates their use when we are interested in particular tasks
    such as link prediction and entity classification. Therefore, **knowledge graph
    embedding** (**KGE**) was proposed to be able to simplify these tasks. We have
    already discussed the concept of embedding in the first chapter. An embedding
    is a projection of data into a low-dimensional and continuous vector space, which
    is especially useful when our data has a sparse representation (such as text and
    graphs). For a KG, an embedding is the projection of the graph (nodes, edges,
    and their feature vectors) in this reduced space. A KGE model then tries to learn
    a projection that preserves both structure and information, so that it can then
    be used for downstream tasks.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱（KGs）在表示知识方面非常有效，但它们在规模上的操作却很复杂。这在我们对特定任务感兴趣时，例如链接预测和实体分类，会使得它们的使用变得复杂。因此，**知识图谱嵌入**（**KGE**）被提出以简化这些任务。我们已经在第一章中讨论了嵌入的概念。嵌入是将数据投影到低维且连续的向量空间中，这在我们的数据具有稀疏表示（如文本和图）时特别有用。对于一个知识图谱，嵌入是将图（节点、边及其特征向量）投影到这个降低的空间中。然后，KGE模型试图学习一个既保留结构又保留信息的投影，以便它可以用于下游任务。
- en: Learning this representation is not an easy task, and several types of algorithms
    have been proposed. For example, some KGEs try to preserve relational patterns
    between entities. TransE is an approach that embeds KGs in Euclidean space where
    the relationships between entities are vectors. TransE is based on the idea that
    two entities connected in the triplet should be as close as possible in space.
    Furthermore, from a triplet `(h, r, t)`, it tries to learn a space where *h +
    r ≈ t*, thus allowing us to do various mathematical operations. RotatE, another
    approach, also tries to preserve other relational patterns (symmetry, antisymmetry,
    inversion, and composition) using a complex vector space. This is quite useful
    when we want to answer questions that require this notion of symmetry (*marriage*
    is symmetric) or composition (*my nephew is my brother’s son*). Other methods,
    however, try to preserve structural patterns. In fact, larger KGs contain complex
    and compound structures that are lost during the embedding process. For example,
    hierarchical, chain structure, and ring structure are lost during classical embeddings.
    These structures are important when we want to conduct reasoning or extract subgraphs
    for some tasks. ATTH (another KG embedding method) uses hyperbolic space to preserve
    hierarchical structures and logical patterns at the same time. Other methods,
    however, try to model the uncertainties of entities and relationships, making
    tasks such as link prediction easier.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 学习这种表示方法并非易事，已经提出了几种类型的算法。例如，一些KGEs试图保留实体之间的关系模式。TransE是一种将KGs嵌入欧几里得空间的方法，其中实体之间的关系是向量。TransE基于这样的想法，即三元组中连接的两个实体在空间中应该尽可能接近。此外，从三元组
    `(h, r, t)` 中，它试图学习一个空间，其中 *h + r ≈ t*，从而允许我们进行各种数学运算。RotatE，另一种方法，也试图使用复杂的向量空间来保留其他关系模式（对称性、反对称性、逆运算和组合）。当我们想要回答需要这种对称性概念（*婚姻*是对称的）或组合（*我的侄子是我哥哥的儿子*）的问题时，这非常有用。然而，其他方法试图保留结构模式。实际上，较大的KGs包含在嵌入过程中丢失的复杂和复合结构。例如，层次结构、链结构和环结构在经典嵌入过程中丢失。当我们想要进行推理或为某些任务提取子图时，这些结构很重要。ATTH（另一种KG嵌入方法）使用双曲空间来同时保留层次结构和逻辑模式。然而，其他方法试图模拟实体和关系的不确定性，使诸如链接预测等任务更容易。
- en: '![Figure 7.26 – Illustration of three typical structures in KGs (https://arxiv.org/pdf/2211.03536)](img/B21257_07_26.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图7.26 – KGs中三种典型结构的示意图](https://arxiv.org/pdf/2211.03536)(img/B21257_07_26.jpg)'
- en: Figure 7.26 – Illustration of three typical structures in KGs ([https://arxiv.org/pdf/2211.03536](https://arxiv.org/pdf/2211.03536))
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.26 – KGs中三种典型结构的示意图([https://arxiv.org/pdf/2211.03536](https://arxiv.org/pdf/2211.03536))
- en: KGEs have been used extensively for several tasks such as link prediction. In
    this case, exploiting the small space is used to try to identify the most likely
    missing links. Similarly, continuous space allows models to be used to conduct
    triple classification. A further application is to use learned embeddings to recommender
    systems.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: KGEs已被广泛用于诸如链接预测等任务。在这种情况下，利用小空间试图识别最有可能缺失的链接。同样，连续空间允许模型用于三元分类。进一步的应用是使用学习到的嵌入来构建推荐系统。
- en: Graph neural networks
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图神经网络
- en: 'There are several challenges in using graphs natively with ML algorithms. First,
    classical ML models take data that is in a rectangular or grid-like form, making
    it non-intuitive how to apply it to graphs. In addition, for a graph, there are
    several pieces of information that we want to use to solve tasks: nodes, edges,
    global context, and connectivity. The last one is particularly difficult to represent,
    and we usually use an adjacency matrix. This representation is sparse, grows largely
    with the number of nodes in the graph, and thus is space inefficient. Also, since
    there is no order in the graph, we can get several adjacency matrices that convey
    the same information but may not be recognized by a model.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用图形与ML算法原生结合时存在几个挑战。首先，经典的ML模型接受的是矩形或网格状的数据，这使得将其应用于图形非直观。此外，对于图形，我们想要使用的信息有：节点、边、全局上下文和连通性。最后一个尤其难以表示，我们通常使用邻接矩阵。这种表示是稀疏的，随着图中节点数量的增加而大幅增长，因此空间效率低下。此外，由于图中没有顺序，我们可以得到几个传递相同信息的邻接矩阵，但可能不会被模型识别。
- en: 'A GNN is a deep learning model that natively takes a graph and also exploits
    its structure during its learning process. There are different types of GNNs (in
    the *Further reading* section, there are some reviews so you can go into more
    detail on this topic) but here we will focus on the main framework: message passing.
    Most GNNs can be seen as graph convolution networks in which we aggregate for
    each node the information coming from its neighbors. One of the advantages of
    a GNN is that at the same time as it is being trained for a task, it learns an
    embedding for each node. At each step, this node embedding is updated with information
    from its neighbors.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 是一种深度学习模型，它原生地接受一个图，并在其学习过程中利用其结构。有不同类型的 GNN（在 *进一步阅读* 部分有一些综述，你可以深入了解这个主题），但在这里我们将关注主要框架：消息传递。大多数
    GNN 可以被视为图卷积网络，其中我们为每个节点聚合来自其邻居的信息。GNN 的一个优点是，在训练任务的同时，它还为每个节点学习一个嵌入。在每一步，这个节点嵌入都会用来自邻居的信息进行更新。
- en: 'The message-passing framework is in a sense very similar to a neural network,
    as we saw earlier. In this case, there are two main steps: gather the embeddings
    of the various neighboring nodes and then follow by an aggregation function (which
    can be different depending on various architectures) and a nonlinearity. Then,
    at each step, we conduct an update of the embedding of each node, learning a new
    representation of the graph. A classical GNN can be composed of a series of GNN
    blocks and then a final layer that exploits the learned representation to accomplish
    a task. This can be written in a formula like this:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递框架在某种程度上与神经网络非常相似，正如我们之前所看到的。在这种情况下，有两个主要步骤：收集各个邻居节点的嵌入，然后跟随一个聚合函数（这取决于不同的架构而不同）和非线性函数。然后，在每一步，我们更新每个节点的嵌入，学习图的新表示。一个经典的
    GNN 可以由一系列 GNN 块和一个最终层组成，该层利用学习到的表示来完成任务。这可以写成如下公式：
- en: <mml:math display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>∙</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>⋃</mml:mo><mml:mo>{</mml:mo><mml:mi>v</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math>
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: <mml:math display="block"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi
    mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>∙</mml:mo><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi
    mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>⋃</mml:mo><mml:mo>{</mml:mo><mml:mi>v</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math>
- en: Here, at layer *l+1*, we learn a representation, *h*, based on the previous
    embedding. *W* is a layer-specific weight matrix, *v* is a node, *w* is the set
    of neighbors, and *c* is the normalization coefficient.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在第 *l+1* 层，我们基于之前的嵌入学习一个表示，*h*。*W* 是一个特定于层的权重矩阵，*v* 是一个节点，*w* 是邻居集合，*c*
    是归一化系数。
- en: '![Figure 7.27 – GNNs](img/B21257_07_27.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.27 – GNNs](img/B21257_07_27.jpg)'
- en: Figure 7.27 – GNNs
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.27 – GNNs
- en: In this case, we are assuming that each neighbor’s contribution is the same.
    This may not be the case, so inspired by the attention mechanism of RNNs and transformers,
    **graph attention networks** (**GATs**) have been proposed. In this type of GNN,
    the model learns different levels of importance for each neighbor. Several models
    of GNN layers exist today, but basically, the principle does not change much.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们假设每个邻居的贡献是相同的。这可能并不成立，因此受到RNN和transformers的注意力机制的启发，提出了**图注意力网络**（GATs）。在这种类型的GNN中，模型学习为每个邻居学习不同的重要性级别。目前存在几种GNN层模型，但基本原理变化不大。
- en: GNNs have been successfully used for several graph tasks but still have some
    limitations such as difficult scalability, problems with batching, and so on.
    They have also been applied to KGs, but they increase complexity.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs 已成功应用于多个图任务，但仍存在一些局限性，如难以扩展、批处理问题等。它们也已被应用于知识图谱，但增加了复杂性。
- en: LLMs reasoning on knowledge graphs
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM在知识图谱上的推理
- en: 'LLMs have the advantage that they are not trained for a specific task but acquire
    a broad spectrum of skills during training. In addition, LLMs have reasoning skills
    that can be improved with specific approaches. Therefore, several researchers
    have suggested conducting graph reasoning with LLMs. The main method of approaching
    an LLM is to use a prompt as input. There are three approaches:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的优势在于它们不是为特定任务训练的，而是在训练过程中获得了一组广泛技能。此外，LLM具有可以通过特定方法改进的推理技能。因此，一些研究人员建议使用LLM进行图推理。接近LLM的主要方法是用提示作为输入。有三种方法：
- en: '**Manual prompt**: The simplest approach is to provide a prompt to an LLM in
    which they are asked to solve a task on a graph. In the prompt, the graph is entered
    and additional information can be added (e.g., if we want the LLM to conduct a
    **depth-first search** (**DFS**) algorithm to solve the task, we provide a succinct
    explanation of how this algorithm works). A limitation to these prompts is that
    it is not possible to insert wide graphs within a prompt (limitation due to the
    context length of the LLM).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动提示**：最简单的方法是向LLM提供一个提示，要求它们在图上解决一个任务。在提示中，输入图，并可以添加额外信息（例如，如果我们希望LLM使用**深度优先搜索**（DFS）算法来解决任务，我们提供该算法如何工作的简要说明）。这些提示的局限性在于，无法在提示中插入宽图（这是由于LLM的上下文长度限制造成的）。'
- en: '**Self-prompting**: The LLM conducts a continuous update of the prompt to make
    it easier for the LLM to solve tasks. In other words, given an original prompt,
    the LLM conducts prompt updates to better define tasks and how to resolve them.
    Then, based on the output of the LLM, a new prompt is generated and fed back to
    the LLM. This process can be conducted multiple times to refine the output.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自我提示**：LLM 通过持续更新提示来使任务解决更加容易。换句话说，给定一个原始提示，LLM 进行提示更新以更好地定义任务及其解决方式。然后，基于LLM的输出，生成一个新的提示并反馈给LLM。这个过程可以多次进行以优化输出。'
- en: '**API call prompts**: This type of prompt is inspired by agents, in which the
    LLM is provided with a set of APIs that it can invoke to conduct reasoning about
    graphs or other external tools.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API调用提示**：这种提示类型受到代理的启发，其中LLM被提供一组它可以调用的API，用于对图或其他外部工具进行推理。'
- en: '![Figure 7.28 – Prompting methods for the LLM applied to graph tasks (https://arxiv.org/pdf/2404.14809)](img/B21257_07_28.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图7.28 – 将LLM应用于图任务的提示方法 (https://arxiv.org/pdf/2404.14809)](img/B21257_07_28.jpg)'
- en: Figure 7.28 – Prompting methods for the LLM applied to graph tasks ([https://arxiv.org/pdf/2404.14809](https://arxiv.org/pdf/2404.14809))
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.28 – 将LLM应用于图任务的提示方法 ([https://arxiv.org/pdf/2404.14809](https://arxiv.org/pdf/2404.14809))
- en: The alternative to finding complex prompting strategies is the **supervised
    fine-tuning** (**SFT**) method. In this case, a dataset with graph tasks and their
    solutions is used to train the model to improve its reasoning skills.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找复杂提示策略的替代方法是**监督微调**（SFT）方法。在这种情况下，使用包含图任务及其解决方案的数据集来训练模型，以提高其推理能力。
- en: Another interesting aspect of LLMs is that they can also be used in combination
    with other models. This allows their qualities to be exploited with models that
    are specialized and better suited for certain tasks. For example, we can combine
    LLMs with GNNs, in which case LLMs can function as enhancers of GNNs. The GNN
    handles graph structure much better than the LLM, but the latter handles textual
    attributes much better. We can then capitalize on the strengths of the two models
    to have a much stronger synergistic model. LLMs possess greater semantic and syntactic
    capacity than other models, and this allows them to create powerful textual embeddings.
    An LLM can then generate numerical embeddings that are then used by the GNN as
    node features. For example, we have a citation network between scientific articles
    (our graph where each node is an article) and we want to classify articles into
    various topics. We can take the abstract for each article and use an LLM to create
    an embedding of the abstract. These number vectors will be the node features for
    the articles. At this point, we can train our GNN with better results than without
    features. Alternatively, if the node features are textual, we can use an LLM to
    generate labels. For example, for our article network, we have the article titles
    associated with each node, and we use the LLM in a zero-shot setting to generate
    a set of labels. This is useful because manual annotation is expensive, so we
    can get labels much more quickly. When we have obtained the labels, we can train
    a GNN on the graph. Alternatively, we can also think of conducting fine-tuning
    of the LLM and GNN at the same time for tasks.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）的另一个有趣方面是它们也可以与其他模型结合使用。这使得它们的特性可以与专门针对某些任务而优化的模型相结合。例如，我们可以将LLMs与GNNs（图神经网络）结合，在这种情况下，LLMs可以作为GNNs的增强器。GNN在处理图结构方面比LLM做得更好，但LLM在处理文本属性方面做得更好。然后我们可以利用这两个模型的优势，以获得一个更强的协同模型。LLMs比其他模型具有更大的语义和句法能力，这使得它们能够创建强大的文本嵌入。然后LLM可以生成数值嵌入，这些嵌入随后被GNN用作节点特征。例如，我们有一个科学论文之间的引用网络（我们的图中每个节点是一篇论文），我们希望将文章分类到各种主题中。我们可以取每篇文章的摘要，并使用LLM创建摘要的嵌入。这些数字向量将成为文章的节点特征。在这个阶段，我们可以训练我们的GNN，比没有特征时获得更好的结果。或者，如果节点特征是文本的，我们可以使用LLM来生成标签。例如，对于我们的文章网络，我们每个节点都有与之关联的文章标题，我们使用LLM在零样本设置下生成一组标签。这很有用，因为手动标注很昂贵，所以我们可以更快地获得标签。当我们获得标签后，我们可以在图上训练一个GNN。或者，我们也可以考虑同时进行LLM和GNN的微调。
- en: '![Figure 7.29 – LLM and GNN synergy](img/B21257_07_29.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图7.29 – LLM和GNN协同](img/B21257_07_29.jpg)'
- en: Figure 7.29 – LLM and GNN synergy
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.29 – LLM和GNN协同
- en: 'Another interesting approach is graph-formed reasoning. Several of the prompting
    techniques that have been used for reasoning do not take into account that human
    thinking is not linear, so according to some, this method of reasoning can be
    approximated using graphs. There are two types of approaches that take advantage
    of this idea:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有趣的方法是图形成推理。一些用于推理的提示技术没有考虑到人类思维不是线性的，因此根据某些观点，这种推理方法可以用图来近似。有两种利用这种想法的方法：
- en: '`Distance = 60 km`, `Time = 1.5 hours`, `Use speed = distance ÷ time`, and
    `Speed = 40 km/h`, with edges showing how each thought leads to the next. This
    graph structure enables the model to reason step by step, explore alternatives,
    or verify calculations.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`距离 = 60 km`，`时间 = 1.5小时`，`使用速度 = 距离 ÷ 时间`，`速度 = 40 km/h`，边表示每个思想如何导致下一个。这种图结构使模型能够逐步推理，探索替代方案或验证计算。'
- en: '**Verify on the graph**: In this approach, we use a graph to verify the correctness
    and consistency of the reasoning. For example, if different paths should lead
    to a logical conclusion, they should be the same or similar. So, if there is a
    contradiction, it means the reasoning is wrong. Generally, one generates several
    reasonings for a question, structures them as a graph, and analyzes them to improve
    the final answer. This approach requires a verifier who analyzes this graph, usually
    another LLM.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在图上验证**：在这种方法中，我们使用图来验证推理的正确性和一致性。例如，如果不同的路径应该导致一个逻辑结论，它们应该是相同的或相似的。所以，如果有矛盾，这意味着推理是错误的。通常，人们会为一个问题生成几个推理，将它们结构化为一个图，并分析它们以改进最终答案。这种方法需要一个验证者来分析这个图，通常是另一个LLM。'
- en: '![Figure 7.30 – Think on graphs and verify on graphs (https://arxiv.org/pdf/2404.14809)](img/B21257_07_30.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图7.30 – 在图上思考和在图上验证（https://arxiv.org/pdf/2404.14809）](img/B21257_07_30.jpg)'
- en: Figure 7.30 – Think on graphs and verify on graphs ([https://arxiv.org/pdf/2404.14809](https://arxiv.org/pdf/2404.14809))
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.30 – 在图上思考并在图上验证（[https://arxiv.org/pdf/2404.14809](https://arxiv.org/pdf/2404.14809)）
- en: In this section, we discussed the intricate relationship between graphs and
    LLMs and how they can enable us to solve some tasks that were previously conducted
    with graph ML algorithms. In the next section, we will discuss exciting perspectives
    in the field of some questions that remain open.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了图和LLM之间错综复杂的关系以及它们如何使我们能够解决一些以前使用图ML算法执行的任务。在下一节中，我们将讨论一些尚未解决的问题领域的令人兴奋的视角。
- en: Ongoing challenges in knowledge graphs and GraphRAG
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识图谱和GraphRAG的持续挑战
- en: KGs are a powerful medium for storing and organizing information, but there
    are still limitations and open questions. Especially for large KGs, scalability
    is important; a balance must be struck between expressiveness and computational
    efficiency. Plus building a KG requires a lot of computational effort (using an
    LLM to extract triplets from a large corpus of text can be expensive and require
    adequate infrastructure). In addition, once the KG is built, it must be evaluated
    and cleaned, which also requires some effort (manual or computational). Moreover,
    growth in the KG also means growth in the infrastructural cost to enable access
    or use. Querying large KGs requires having optimized algorithms to avoid the risk
    of increasingly large latency times. Industrial KGs can contain billions of entities
    and relationships, representing an intricate and complex scale. Many of the algorithms
    are designed for small-scale KGs (up to thousands of entities) so retrieval in
    large-scale KGs still remains challenging.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: KGs是存储和组织信息的有力媒介，但仍存在局限性和未解决的问题。特别是对于大型KGs，可扩展性很重要；必须在表达性和计算效率之间取得平衡。此外，构建KG需要大量的计算工作（使用LLM从大量文本语料库中提取三元组可能很昂贵，并且需要足够的基础设施）。此外，一旦KG构建完成，就必须对其进行评估和清理，这也需要一些努力（手动或计算）。此外，KG的增长也意味着支持访问或使用的基础设施成本的增长。查询大型KGs需要拥有优化算法以避免越来越大的延迟时间风险。工业KGs可以包含数十亿个实体和关系，代表着复杂和庞大的规模。许多算法是为小型KGs（多达数千个实体）设计的，因此在大规模KGs中的检索仍然具有挑战性。
- en: In addition, KGs are notoriously incomplete, which means that one must have
    pipelines in order to complete the KG. This means having both pipelines to add
    additional sources and pipelines to conduct the update of data sources. Most databases
    are static, so creating dynamic systems is challenging. This is critical to make
    the best use of KGs for domains such as finance, where we want to account for
    rapid market changes. On a side note, KGs can be multimodal, but integrating these
    modes is not easy at all. While adding modalities significantly improves the reasoning
    process, the understanding of the nuances of stored knowledge, and the richness
    of the KG, it significantly increases management complexity (more storage required,
    more sophisticated pipelines, more complex knowledge harmonization, and so on).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，知识图谱（KGs）众所周知是不完整的，这意味着必须拥有管道来完善KG。这意味着需要既有添加额外来源的管道，也有更新数据源的管道。大多数数据库都是静态的，因此创建动态系统具有挑战性。这对于在金融等领域充分利用KG至关重要，在这些领域，我们希望考虑到市场的快速变化。顺便提一下，KGs可以是多模态的，但整合这些模式并不容易。虽然添加模态可以显著提高推理过程，对存储知识的细微差别和KG的丰富性的理解，但它也显著增加了管理复杂性（需要更多存储，更复杂的管道，更复杂的知识协调等等）。
- en: GraphRAG is a relatively new technology and still not fully optimized. For one
    thing, information retrieval could be improved, especially the transition between
    the user’s text query and retrieval on the KG. The more the KG grows, the more
    there is a risk of finding redundant information that harms the generation process.
    After retrieval, we could then end up with a long context that is provided to
    the LLM for generation. To reduce noise and reduce computation, we can compress
    the context, but this carries the risk of information loss. At present, lossless
    compression is an active field of research, while current methods allow, at most,
    a trade-off between compression and information preservation. There is also currently
    a lack of benchmark standards to evaluate new GraphRAG approaches; this does not
    allow for an easy comparison of either current or future methods. GraphRAG allows
    for considering inter-entity relationships and structural knowledge information,
    reducing redundant text information, and being able to find global information
    again. At the same time, though, the nuances of the text are lost, and GraphRAG
    underperforms in abstractive question-answering tasks or when there is no explicit
    entity mentioned in the question. So, the union of vector and GraphRAG (or HybridRAG)
    is an exciting prospect for the future. It remains interesting to understand how
    these two technologies will be integrated in an optimal way.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: GraphRAG 是一种相对较新的技术，并且尚未完全优化。一方面，信息检索可以改进，尤其是在用户的文本查询和知识图谱检索之间的转换。随着知识图谱的不断扩大，找到冗余信息并损害生成过程的风险也随之增加。检索之后，我们可能会得到一个很长的上下文，这个上下文被提供给大型语言模型进行生成。为了减少噪声和降低计算量，我们可以压缩上下文，但这会带来信息丢失的风险。目前，无损压缩是一个活跃的研究领域，而现有方法最多只能允许在压缩和信息保留之间进行权衡。目前还缺乏基准标准来评估新的
    GraphRAG 方法；这不允许对当前或未来的方法进行轻松比较。GraphRAG 允许考虑实体间关系和结构化知识信息，减少冗余文本信息，并能够再次找到全局信息。然而，与此同时，文本的细微差别却丢失了，GraphRAG
    在抽象问答任务或当问题中没有明确提到实体时表现不佳。因此，向量和 GraphRAG（或 HybridRAG）的结合对未来来说是一个令人兴奋的前景。了解这两种技术如何以最佳方式集成仍然很有趣。
- en: An important note is that LLMs are not specifically trained for graph tasks.
    As we mentioned in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042), LLMs are trained
    to predict the next word in a sequence. By optimizing this simple goal, they acquire
    most of their skills. Obviously, it is difficult to assimilate a spatial understanding
    simply from text. This means that LLMs generally struggle with structural data.
    This is highlighted with tabular data, where LLMs have problems with understanding
    tables and relationships. The first problem is that LLMs struggle with numerical
    representation since the tokenization step makes it difficult for an LLM to understand
    the whole number (lack of consistent decimal representation, and problems with
    numerical operation). This then impacts the execution of graph tasks where this
    numerical understanding is necessary. Specific studies on graph understanding
    show that LLMs have a basic understanding of graph structure. LLMs understand
    these graphs in linear form and better understand the labels associated with the
    nodes more than the topological structure of the graph. According to these studies,
    LLMs have a basic understanding, which is strongly impacted by prompt design,
    prompt techniques, semantic information provided, and the presence of examples.
    Next-generation, multi-parameter LLMs succeed in solving simple tasks on small
    graphs, but their performance decays rapidly as both graph and task complexity
    increase. There are two reasons for this lack of understanding of structural data.
    The first is that in the large text corpora used for training LLMs, there is not
    much graph-derived data. So, LLMs can only learn basic spatial relationships because
    these are described in the texts. Therefore, SFT on graph datasets allows for
    results that are better than much larger models.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的一点是，LLM并没有专门训练用于图任务。正如我们在**第3章**中提到的（B21257_03.xhtml#_idTextAnchor042），LLM被训练来预测序列中的下一个单词。通过优化这个简单的目标，它们获得了大部分技能。显然，仅从文本中吸收空间理解是困难的。这意味着LLM通常在结构数据上遇到困难。这一点在表格数据中得到了突出，LLM在理解表格和关系上存在问题。第一个问题是LLM在数值表示上遇到困难，因为标记化步骤使得LLM难以理解整个数字（缺乏一致的十进制表示，以及数值运算问题）。这随后影响了需要这种数值理解的图任务的执行。对图理解的具体研究表明，LLM对图结构有基本理解。LLM以线性形式理解这些图，并且比图的结构拓扑结构更好地理解与节点相关的标签。根据这些研究，LLM有基本理解，这强烈受到提示设计、提示技术、提供的语义信息和示例存在的影响。下一代多参数LLM在解决小图上的简单任务时取得成功，但随着图和任务复杂性的增加，它们的性能迅速下降。这种对结构数据缺乏理解有两个原因。第一个原因是，在用于训练LLM的大文本语料库中，没有多少来自图的数据。因此，LLM只能学习基本的时空关系，因为这些关系在文本中有所描述。因此，在图数据集上进行的SFT可以产生比许多更大的模型更好的结果。
- en: '![Figure 7.31 – SFT on graph data allows better performance for small LLMs
    than larger LLMs (https://arxiv.org/pdf/2403.04483)](img/B21257_07_31.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图7.31 – 在图数据上使用SFT比大LLM对小LLM有更好的性能](https://arxiv.org/pdf/2403.04483)(img/B21257_07_31.jpg)'
- en: Figure 7.31 – SFT on graph data allows better performance for small LLMs than
    larger LLMs ([https://arxiv.org/pdf/2403.04483](https://arxiv.org/pdf/2403.04483))
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.31 – 在图数据上使用SFT比大LLM对小LLM有更好的性能([https://arxiv.org/pdf/2403.04483](https://arxiv.org/pdf/2403.04483))
- en: The second reason, on the other hand, stems from why humans understand spatial
    structures well. Humans learn spatial relationships from their experiences in
    the outside world. The brain creates mental maps that allow us to orient ourselves
    in space. These maps also enable us to better understand abstract spatial concepts
    such as graphs. LLMs do not have a mental map nor can they have the experience
    of the outside world, thus making them disadvantaged in understanding abstract
    spatial concepts.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，第二个原因源于人类理解空间结构良好的原因。人类从他们在外部世界的经验中学习空间关系。大脑创建心理地图，使我们能够在空间中定位自己。这些地图还使我们能够更好地理解抽象的空间概念，如图形。LLM没有心理地图，也不能有外部世界的经验，这使得它们在理解抽象空间概念方面处于不利地位。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In *Chapters 5* and *6*, the main question was how to find information and how
    to use this information to generate an answer to users’ questions. Finding information
    dynamically allows us to reduce the hallucinations of our model and keep its knowledge
    up to date.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在**第5章**和**第6章**中，主要问题是如何找到信息以及如何使用这些信息来生成对用户问题的答案。动态地查找信息使我们能够减少我们模型的幻觉并保持其知识更新。
- en: In this chapter, we started with a text corpus and created a system to find
    the most relevant information for generating an answer (naïve RAG). Next, we created
    a more sophisticated system to try to extract only the relevant information and
    avoid redundant information or noise. For some researchers, by its nature, text
    contains relevant information intermixed with background noise. What matters are
    the entities present and their relationships. From this reductionist approach
    comes the idea of representing essential knowledge in a knowledge graph. The graph
    allows us to use algorithms to search for information or explore possible connections.
    For a long time, graph reasoning and LLMs have run on parallel tracks, but recently,
    their stories have begun to intertwine. We have seen how this interaction between
    LLM and KG can be conducted in various ways. For example, an LLM can be used to
    extract relationships and entities for our graph construction, or an LLM can be
    used to conduct reasoning about the KG. Similarly, we can use the KG to find knowledge
    and enrich the context of the LLM, thus enabling it to effectively answer a user
    question.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从一个文本语料库开始，创建了一个系统来找到生成答案的最相关信息（朴素 RAG）。接下来，我们创建了一个更复杂的系统，试图提取仅相关的信息，避免冗余信息或噪声。对于一些研究人员来说，文本本质上包含与背景噪声混合的相关信息。重要的是实体及其关系。从这个还原论方法中产生了在知识图谱中表示基本知识的想法。图使我们能够使用算法来搜索信息或探索可能的连接。长期以来，图推理和
    LLM 一直在平行轨道上运行，但最近，它们的故事开始交织在一起。我们已经看到了 LLM 和 KG 之间如何以各种方式进行交互。例如，LLM 可以用来提取用于图构建的关系和实体，或者
    LLM 可以用来对 KG 进行推理。同样，我们可以使用 KG 来寻找知识并丰富 LLM 的上下文，从而使其能够有效地回答用户问题。
- en: 'Right now, there is a sort of a Manichean definition: either the vector RAG
    or the GraphRAG. Both have merits and demerits, and the research points toward
    a unification of these two worlds (HybridRAG). In the future, we will find more
    sophisticated ways of uniting KGs and vectors. Also, the understanding of the
    graph structure on one side of an LLM is still immature. With the growth of training
    datasets, the new generation LLMs are exposed to more examples of graphs. However,
    understanding spatial relationships in an abstract concept such as a graph also
    means understanding them in problems with greater real-world relevance. Therefore,
    this is an active field of research, especially for robots that must interact
    in space and use AI.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，存在一种类似摩尼教式的定义：要么是向量 RAG，要么是 GraphRAG。两者都有优点和缺点，研究趋势指向这两个世界的统一（HybridRAG）。在未来，我们将找到更多将知识图谱和向量结合在一起的高级方法。此外，对于
    LLM 的一侧的图结构理解仍然不成熟。随着训练数据集的增长，新一代 LLM 暴露于更多图示例。然而，在抽象概念如图中的空间关系理解，也意味着在具有更大现实相关性的问题中理解它们。因此，这是一个活跃的研究领域，特别是对于必须在太空中互动并使用人工智能的机器人。
- en: Moving into space is one of the next frontiers of AI. Interaction in space presents
    peculiar challenges, such as balancing exploration and exploitation. In the next
    chapter, we will discuss this concept more abstractly. We will focus on reinforcement
    learning and agent behavior in the relationship to space. Whether chess, a video
    game, or a real-world environment, an agent must learn how to interact with space
    to achieve a goal. In the next chapter, we will look at how to enable an agent
    to explore the world without losing sight of the aim.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 进入太空是人工智能的下一个前沿领域之一。太空中的交互带来了独特的挑战，例如平衡探索和开发。在下一章中，我们将更抽象地讨论这一概念。我们将关注强化学习和代理在太空关系中的行为。无论是棋类游戏、视频游戏还是现实世界环境，代理必须学习如何与太空互动以实现目标。在下一章中，我们将探讨如何使代理能够在不失去目标的情况下探索世界。
- en: Further reading
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Ghafarollahi, *SciAgents: Automating Scientific Discovery through Multi-agent
    Intelligent Graph Reasoning*, 2024, [https://arxiv.org/pdf/2409.05556v1](https://arxiv.org/pdf/2409.05556v1)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ghafarollahi, 《SciAgents：通过多智能体智能图推理自动化科学发现》](https://arxiv.org/pdf/2409.05556v1)
    2024'
- en: 'Raieli, *A Brave New World for Scientific Discovery: Are AI Research Ideas
    Better?*, 2024, [https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182](https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Raieli, 《科学发现的勇敢新世界：AI 研究想法是否更好？》](https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182)
    2024'
- en: Raieli, *How the LLM Got Lost in the Network and Discovered Graph Reasoning*,
    2024, [https://towardsdatascience.com/how-the-llm-got-lost-in-the-network-and-discovered-graph-reasoning-e2736bd04efa](https://towardsdatascience.com/how-the-llm-got-lost-in-the-network-and-discovered-graph-reasoning-e2736bd04efa)
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *LLM如何在网络中迷失并发现图推理*, 2024, [https://towardsdatascience.com/how-the-llm-got-lost-in-the-network-and-discovered-graph-reasoning-e2736bd04efa](https://towardsdatascience.com/how-the-llm-got-lost-in-the-network-and-discovered-graph-reasoning-e2736bd04efa)
- en: 'Wu, *Medical Graph RAG: Towards Safe Medical Large Language Model via Graph
    Retrieval-Augmented Generation*, 2024, [https://arxiv.org/abs/2408.04187](https://arxiv.org/abs/2408.04187)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu, *医学图RAG：通过图检索增强生成实现安全的医学大型语言模型*, 2024, [https://arxiv.org/abs/2408.04187](https://arxiv.org/abs/2408.04187)
- en: 'Raieli, *The Convergence of Graph and Vector RAGs: A New Era in Information
    Retrieval*, 2024, [https://medium.com/gitconnected/the-convergence-of-graph-and-vector-rags-a-new-era-in-information-retrieval-b5773a723615](https://medium.com/gitconnected/the-convergence-of-graph-and-vector-rags-a-new-era-in-information-retrieval-b5773a723615)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *图和向量RAGs的收敛：信息检索的新时代*, 2024, [https://medium.com/gitconnected/the-convergence-of-graph-and-vector-rags-a-new-era-in-information-retrieval-b5773a723615](https://medium.com/gitconnected/the-convergence-of-graph-and-vector-rags-a-new-era-in-information-retrieval-b5773a723615)
- en: 'Sarmah, *HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented
    Generation for Efficient Information Extraction*, 2024, [https://arxiv.org/pdf/2408.04948](https://arxiv.org/pdf/2408.04948)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarmah, *HybridRAG：整合知识图谱和向量检索增强生成以实现高效信息提取*, 2024, [https://arxiv.org/pdf/2408.04948](https://arxiv.org/pdf/2408.04948)
- en: Liang, *Survey of Graph Neural Networks and Applications*, 2022, [https://onlinelibrary.wiley.com/doi/10.1155/2022/9261537](https://onlinelibrary.wiley.com/doi/10.1155/2022/9261537)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang, *图神经网络及其应用综述*, 2022, [https://onlinelibrary.wiley.com/doi/10.1155/2022/9261537](https://onlinelibrary.wiley.com/doi/10.1155/2022/9261537)
- en: Arora, *A Survey on Graph Neural Networks for Knowledge Graph Completion*, 2020,
    [https://arxiv.org/pdf/2007.12374](https://arxiv.org/pdf/2007.12374)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora, *关于图神经网络在知识图谱补全中的应用综述*, 2020, [https://arxiv.org/pdf/2007.12374](https://arxiv.org/pdf/2007.12374)
- en: Huang, *Can LLMs Effectively Leverage Graph Structural Information through Prompts,
    and Why?*, 2023, [https://arxiv.org/abs/2309.16595](https://arxiv.org/abs/2309.16595)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang, *LLM能否通过提示有效地利用图结构信息，为什么？*, 2023, [https://arxiv.org/abs/2309.16595](https://arxiv.org/abs/2309.16595)
- en: 'Liu, *Evaluating Large Language Models on Graphs: Performance Insights and
    Comparative Analysis*, 2023, [https://arxiv.org/abs/2308.11224](https://arxiv.org/abs/2308.11224)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu, *在图上评估大型语言模型：性能洞察和比较分析*, 2023, [https://arxiv.org/abs/2308.11224](https://arxiv.org/abs/2308.11224)
